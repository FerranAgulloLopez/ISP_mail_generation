{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generator_fine_tune_new.ipynb","provenance":[],"authorship_tag":"ABX9TyM0O1eDuxqoMaG7hJjQ8Hou"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Generator fine tuning"],"metadata":{"id":"6rFiJznwTFvS"}},{"cell_type":"markdown","source":["### set up"],"metadata":{"id":"13s53w6dTKq2"}},{"cell_type":"code","source":["%%time\n","%%capture\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1eyO4ZeNUle","executionInfo":{"status":"ok","timestamp":1642462445785,"user_tz":-60,"elapsed":2906,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"472d9663-89b4-4978-9c62-5d0249a6b770"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 22.2 ms, sys: 4.07 ms, total: 26.2 ms\n","Wall time: 2.63 s\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSmmfM36NVWf","executionInfo":{"status":"ok","timestamp":1642462445785,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"eec5ccc6-6973-4faa-b1da-fb4f2aecb8c6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Jan 17 23:34:05 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import os\n","import io\n","import requests\n","import numpy as np\n","import pandas as pd\n","import re\n","import zipfile\n","import random\n","import time\n","import csv\n","import datetime\n","from itertools import compress\n","from collections import Counter, defaultdict\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n","                         AdamW, get_linear_schedule_with_warmup, \\\n","                         TrainingArguments, BeamScorer, Trainer\n","\n","import torch\n","from torch.utils.data import Dataset, random_split, DataLoader, \\\n","                             RandomSampler, SequentialSampler\n","\n","from IPython.display import clear_output\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import json\n","\n","print(f\"PyTorch version: {torch.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfrEhuIbNWv-","executionInfo":{"status":"ok","timestamp":1642462450277,"user_tz":-60,"elapsed":4494,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"141750f2-f12e-4b38-9340-ffafb3ff0dfb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","PyTorch version: 1.10.0+cu111\n"]}]},{"cell_type":"markdown","source":["### config"],"metadata":{"id":"y-3eGICbTRXl"}},{"cell_type":"code","source":["DEBUG           = False\n","\n","USE_APEX        = True\n","APEX_OPT_LEVEL  = 'O1'\n","\n","MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n","\n","UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n","\n","SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n","                    \"eos_token\": \"<|EOS|>\",\n","                    \"unk_token\": \"<|UNK|>\",                    \n","                    \"pad_token\": \"<|PAD|>\",\n","                    \"sep_token\": \"<|SEP|>\"}\n","                    \n","MAXLEN          = 768  #{768, 1024, 1280, 1600}\n","\n","TRAIN_SIZE      = 0.8\n","\n","if USE_APEX:\n","    TRAIN_BATCHSIZE = 4\n","    BATCH_UPDATE    = 16\n","else:\n","    TRAIN_BATCHSIZE = 2\n","    BATCH_UPDATE    = 32\n","\n","EPOCHS          = 7\n","LR              = 5e-4\n","EPS             = 1e-8\n","WARMUP_STEPS    = 1e2\n","\n","SEED            = 2020"],"metadata":{"id":"mntKBPaaNZE_","executionInfo":{"status":"ok","timestamp":1642462450278,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(SEED)"],"metadata":{"id":"eBe3wlVeNgFF","executionInfo":{"status":"ok","timestamp":1642462450278,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### auxiliary functions"],"metadata":{"id":"jhSHO1lYTWlm"}},{"cell_type":"code","source":["class mailsDataset(Dataset):\n","\n","    def __init__(self, mails, tokenizer, randomize=True):\n","        self.randomize = randomize\n","        self.tokenizer = tokenizer \n","        self.mails     = mails\n","\n","    def __len__(self):\n","        return len(self.mails)\n","    \n","    def __getitem__(self, i):\n","        \n","        input = SPECIAL_TOKENS['bos_token'] + self.mails[i]['subject'] + \\\n","                SPECIAL_TOKENS['sep_token'] + self.mails[i]['abstractive_summary'] + SPECIAL_TOKENS['sep_token'] + \\\n","                self.mails[i]['content'] + SPECIAL_TOKENS['eos_token']\n","\n","        encodings_dict = tokenizer(input,                                   \n","                                   truncation=True, \n","                                   max_length=MAXLEN, \n","                                   padding=\"max_length\")   \n","        \n","        input_ids = encodings_dict['input_ids']\n","        attention_mask = encodings_dict['attention_mask']\n","        \n","        return {'label': torch.tensor(input_ids),\n","                'input_ids': torch.tensor(input_ids), \n","                'attention_mask': torch.tensor(attention_mask)}"],"metadata":{"id":"lGYIEOgjNi60","executionInfo":{"status":"ok","timestamp":1642462450278,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def split_data(mails, S=TRAIN_SIZE):\n","    # shuffle list\n","    random.shuffle(mails)\n","\n","    # split into training and validation sets    \n","    train_size = int(S * len(mails))\n","\n","    train_mails = mails[:train_size]\n","    val_mails = mails[train_size:]\n","\n","    return train_mails, val_mails"],"metadata":{"id":"ULbD8W0mNt5E","executionInfo":{"status":"ok","timestamp":1642462450278,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def get_tokenier(special_tokens=None):\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n","\n","    if special_tokens:\n","        tokenizer.add_special_tokens(special_tokens)\n","        print(\"Special tokens added\")\n","    return tokenizer\n","\n","def get_model(tokenizer, special_tokens=None, load_model_path=None):\n","\n","    #GPT2LMHeadModel\n","    if special_tokens:\n","        config = AutoConfig.from_pretrained(MODEL, \n","                                            bos_token_id=tokenizer.bos_token_id,\n","                                            eos_token_id=tokenizer.eos_token_id,\n","                                            sep_token_id=tokenizer.sep_token_id,\n","                                            pad_token_id=tokenizer.pad_token_id,\n","                                            output_hidden_states=False)\n","    else: \n","        config = AutoConfig.from_pretrained(MODEL,                                     \n","                                            pad_token_id=tokenizer.eos_token_id,\n","                                            output_hidden_states=False)    \n","\n","    #----------------------------------------------------------------#\n","    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n","\n","    if special_tokens:\n","        #Special tokens added, model needs to be resized accordingly\n","        model.resize_token_embeddings(len(tokenizer))\n","\n","    if load_model_path:\n","        model.load_state_dict(torch.load(load_model_path))\n","\n","    model.cuda()\n","    return model"],"metadata":{"id":"yVVidbTgQNdG","executionInfo":{"status":"ok","timestamp":1642462450279,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### load model and tokenizer"],"metadata":{"id":"a20tlZwpTZ79"}},{"cell_type":"code","source":["%%time\n","\n","tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n","model = get_model(tokenizer, \n","                  special_tokens=SPECIAL_TOKENS,\n","                #   load_model_path='pytorch_model.bin'\n","                 )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6ePmhlFQTAk","executionInfo":{"status":"ok","timestamp":1642462461409,"user_tz":-60,"elapsed":11136,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"cdc9385e-f647-4384-9fc4-20440fa16bc6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Special tokens added\n","CPU times: user 4.85 s, sys: 2.21 s, total: 7.06 s\n","Wall time: 11 s\n"]}]},{"cell_type":"code","source":["# - Freeze selective layers:\n","# - Freeze all layers except last n:\n","for parameter in model.parameters():\n","    parameter.requires_grad = False\n","\n","for i, m in enumerate(model.transformer.h):        \n","    #Only un-freeze the last n transformer blocks\n","    if i+1 > 12 - UNFREEZE_LAST_N:\n","        for parameter in m.parameters():\n","            parameter.requires_grad = True \n","\n","for parameter in model.transformer.ln_f.parameters():        \n","    parameter.requires_grad = True\n","\n","for parameter in model.lm_head.parameters():        \n","    parameter.requires_grad = True"],"metadata":{"id":"NrJOMUbEQU_y","executionInfo":{"status":"ok","timestamp":1642462461410,"user_tz":-60,"elapsed":21,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### load data"],"metadata":{"id":"20gCEGo0Tc8d"}},{"cell_type":"code","source":["with open('/content/drive/My Drive/master/isp/summarized_bc3_email_corpus_dataset_t5_large.json', 'r') as file:\n","    json_data_1 = json.load(file)\n","with open('/content/drive/My Drive/master/isp/summarized_bc3_email_corpus_dataset_pegasus.json', 'r') as file:\n","    json_data_2 = json.load(file)\n","json_data = json_data_1\n","json_data['mails'] += json_data_2['mails'] "],"metadata":{"id":"KIg4T1m_QcHj","executionInfo":{"status":"ok","timestamp":1642462461410,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["train_data, val_data = split_data(json_data['mails'])\n","\n","train_dataset = mailsDataset(train_data, tokenizer)\n","val_dataset = mailsDataset(val_data, tokenizer, randomize=False)\n","\n","f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"TEvPvZJwQYni","executionInfo":{"status":"ok","timestamp":1642462461410,"user_tz":-60,"elapsed":18,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"2856d8ce-4c81-477f-be7f-12aa244ee8fe"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'There are 416 samples for training, and 104 samples for validation testing'"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### fine tune model"],"metadata":{"id":"p4z3xz5PRggB"}},{"cell_type":"code","source":["%%time\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/content/\",\n","    num_train_epochs=EPOCHS,\n","    per_device_train_batch_size=TRAIN_BATCHSIZE,\n","    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n","    gradient_accumulation_steps=BATCH_UPDATE,\n","    evaluation_strategy=\"epoch\",\n","    fp16=True,\n","    fp16_opt_level=APEX_OPT_LEVEL,\n","    warmup_steps=WARMUP_STEPS,    \n","    learning_rate=LR,\n","    adam_epsilon=EPS,\n","    weight_decay=0.01,        \n","    save_total_limit=1   \n",")\n","\n","#---------------------------------------------------#\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,    \n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer\n",")\n","\n","#---------------------------------------------------#\n","trainer.train()\n","trainer.save_model()    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":989},"id":"2iR8A8GuRhVU","executionInfo":{"status":"ok","timestamp":1642462804353,"user_tz":-60,"elapsed":342949,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"9eaffa84-385d-4b59-891e-5fa412b4881a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Using amp half precision backend\n","***** Running training *****\n","  Num examples = 416\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 16\n","  Total optimization steps = 42\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 05:33, Epoch 6/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>29.730238</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>18.907234</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>4.335934</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>1.866698</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>1.440816</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>1.227566</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>1.087682</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 104\n","  Batch size = 4\n","***** Running Evaluation *****\n","  Num examples = 104\n","  Batch size = 4\n","***** Running Evaluation *****\n","  Num examples = 104\n","  Batch size = 4\n","***** Running Evaluation *****\n","  Num examples = 104\n","  Batch size = 4\n","***** Running Evaluation *****\n","  Num examples = 104\n","  Batch size = 4\n","***** Running Evaluation *****\n","  Num examples = 104\n","  Batch size = 4\n","***** Running Evaluation *****\n","  Num examples = 104\n","  Batch size = 4\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to /content/\n","Configuration saved in /content/config.json\n","Model weights saved in /content/pytorch_model.bin\n","tokenizer config file saved in /content/tokenizer_config.json\n","Special tokens file saved in /content/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 5min 7s, sys: 33.6 s, total: 5min 41s\n","Wall time: 5min 42s\n"]}]},{"cell_type":"code","source":["!cp -r 'pytorch_model.bin' '/content/drive/My Drive/master/isp/fine_tuned_gpt2_768.bin'"],"metadata":{"id":"9zPolsFzT0UG","executionInfo":{"status":"ok","timestamp":1642462954940,"user_tz":-60,"elapsed":2135,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### generation"],"metadata":{"id":"EXnEg_AaTpEt"}},{"cell_type":"code","source":["tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n","model = get_model(tokenizer, \n","                  special_tokens=SPECIAL_TOKENS,\n","                  load_model_path='pytorch_model.bin')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SEbcxtJToX3","executionInfo":{"status":"ok","timestamp":1642462813026,"user_tz":-60,"elapsed":6615,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"932210bc-1383-4148-b881-b9f794c0c020"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.15.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n","loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n","loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.15.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","Assigning <|BOS|> to the bos_token key of the tokenizer\n","Assigning <|EOS|> to the eos_token key of the tokenizer\n","Assigning <|UNK|> to the unk_token key of the tokenizer\n","Assigning <|PAD|> to the pad_token key of the tokenizer\n","Assigning <|SEP|> to the sep_token key of the tokenizer\n"]},{"output_type":"stream","name":"stdout","text":["Special tokens added\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50257,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50258,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 50260,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"sep_token_id\": 50261,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.15.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["mail = json_data['mails'][0]\n","\n","prompt = SPECIAL_TOKENS['bos_token'] + mail['subject'] + \\\n","         SPECIAL_TOKENS['sep_token'] + mail['abstractive_summary'] + SPECIAL_TOKENS['sep_token']\n","         \n","generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","device = torch.device(\"cuda\")\n","generated = generated.to(device)\n","\n","model.eval();"],"metadata":{"id":"TB64Me7SUqr_","executionInfo":{"status":"ok","timestamp":1642462813027,"user_tz":-60,"elapsed":20,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Beam-search text generation:\n","sample_outputs = model.generate(generated, \n","                                do_sample=True,   \n","                                max_length=MAXLEN,                                                      \n","                                num_beams=5,\n","                                repetition_penalty=5.0,\n","                                early_stopping=True,      \n","                                num_return_sequences=1\n","                                )\n","for i, sample_output in enumerate(sample_outputs):\n","    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n","    a = len(mail['subject']) + len(mail['abstractive_summary'])   \n","    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWgLmdSIUSdN","executionInfo":{"status":"ok","timestamp":1642462853085,"user_tz":-60,"elapsed":6664,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"612e1a0a-9bce-403f-ed7c-1c6ce45fc109"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:2259: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  next_indices = next_tokens // vocab_size\n"]},{"output_type":"stream","name":"stdout","text":["1: I believe that there is a need for more information about the accessibility of these sites and how they can be used by people with disabilities. If you have any questions or comments, please feel free to email me at: info@w3c.org\n","\n","\n"]}]},{"cell_type":"code","source":["mail"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXyINByVVOsX","executionInfo":{"status":"ok","timestamp":1642462862557,"user_tz":-60,"elapsed":287,"user":{"displayName":"Ferran Agulló","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04197250542131316927"}},"outputId":"c2f2e472-87b7-43df-8e8e-f06838338abc"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'abstractive_summary': 'The World Wide Web Consortium (W3C) has published its guidelines for making web pages accessible to people with disabilities.',\n"," 'content': 'Hello Education and Outreach Colleagues, I was talking to a friend today -- an amateur web developer and professional disability rights advocate -- who complained that the W3C guidelines are overly technical for her needs. She wants a plain language version of the guidelines. As she is fairly technologically savvy, she expressed frustration at having to work so hard to understand what must be done to make accessible web pages. To illustrate her point, she read me the Quick Tip card description of Image map. I agree with her, the tone is definitely geeky. But not everyone who develops web pages speaks the language of client-side servers and hotspots. I would guess that most people who develop web pages are amateurs (in the original sense of the word: from amore or amour: an activity done out of love.) Will these people freeze when they read &quot;make line by line reading sensible?&quot; or &quot;Use CSS?&quot; How about we create a &quot;user-friendly&quot; version of the Web Content guidelines? Maybe a primer. Alan',\n"," 'extractive_summary': 'I was talking to a friend today -- an amateur web developer and professional disability rights advocate -- who complained that the W3C guidelines are overly technical for her needs. She wants a plain language version of the guidelines. As she is fairly technologically savvy, she expressed frustration at having to work so hard to understand what must be done to make accessible web pages. I agree with her, the tone is definitely geeky. How about we create a &quot;user-friendly&quot; version of the Web Content guidelines? Maybe a primer. ',\n"," 'subject': 'Non-geek version of guidelines'}"]},"metadata":{},"execution_count":20}]}]}