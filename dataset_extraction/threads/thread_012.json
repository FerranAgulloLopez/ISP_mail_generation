[{"subject": "Re: Security considerations from RE-AUTHENTICATIONREQUESTE", "content": "On Fri, 13 Feb 1998, Jim Gettys wrote:\n\n> Here's my revision, given Ted and Koen's comments...\n> - Jim\n> \n> 15.6 Authentication Credentials and Idle Clients\n> \n> Existing HTTP clients and user agents typically retain authentication \n> information indefinately. HTTP/1.1. does not provide a method for an origin \n> server or proxy to force reauthentication. Since clients may be idle for \n> extended periods between use (and unauthorized users may have access to \n> the user agent during these idle periods), this is a significant defect \n> that requires further extensions to HTTP. This is currently under separate \n> study. For user agents, there are a number of work-arounds to parts of \n> this problem, and we enourage the use of password protection in screen \n> savers, idle time-outs, and other methods which mitigate the security \n> problems inherent in this problem.\n\n  I believe that the problem is somewhat more general than\nreauthentication.  There are times when the web application developer\nwould like to force the client to discard credentials whether or not they\nshould then be reaquired.  The simplest example is the often-asked\nquestion: \"How do I get the browser to discard the credentials when the\nuser presses my 'logout' form submit button?\".\n\n  We've been going back and forth on the http-ext list about whether this\nis the same requirement or not...\n\n\n\n", "id": "lists-012-0000000"}, {"subject": "Re: Security considerations from RE-AUTHENTICATIONREQUESTE", "content": "> View: Browse?HTML????Browse?Raw?Text\n> From: \"Scott Lawrence\" <lawrence@agranat.com>\n> Date: Mon, 16 Feb 1998 11:18:48 -0500\n> To: jg@pa.dec.com (Jim Gettys)\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Security considerations from RE-AUTHENTICATION-REQUESTED\n> \n> ? I've attempted to provide a more general discussion of the issue of\n> ? cached credentials, appended below.\n> \n> >>>>> \"JG\" == Jim Gettys <jg@pa.dec.com> writes:\n> \n> JG> 15.6 Authentication Credentials and Idle Clients\n> \n> JG> Existing HTTP clients and user agents typically retain authentication\n> JG> information indefinately. HTTP/1.1. does not provide a method for an origin\n> JG> server or proxy to force reauthentication. Since clients may be idle for\n> JG> extended periods between use (and unauthorized users may have access to\n> JG> the user agent during these idle periods), this is a significant defect\n> JG> that requires further extensions to HTTP. This is currently under separate\n> JG> study. For user agents, there are a number of work-arounds to parts of\n> JG> this problem, and we enourage the use of password protection in screen\n> JG> savers, idle time-outs, and other methods which mitigate the security\n> JG> problems inherent in this problem.\n> \n> 15.6 Caching Authentication Credentials\n> \n> ? Existing HTTP clients and user agents typically retain authentication\n> ? information indefinately. HTTP/1.1. does not provide a method for a\n> ? server to direct clients to dicard these cached credentials.? This is a\n> ? significant defect that requires further extensions to HTTP.\n> ? Circumstances under which this should be possible include but are not\n> ? limited to:\n> \n> ??? - Clients which have been idle for an extended period following which\n> ????? the server may wish to cause the client to reprompt the user for\n> ????? credentials.\n> \n> ??? - Applications which include a session termination indication (such as\n> ????? a 'logout' or 'commit' button on a page) after which the server side\n> ????? of the application 'knows' that there is no further reason for the\n> ????? client to retain the credentials.\n> \n> ? This is currently under separate study.? For user agents, there are a\n> ? number of work-arounds to parts of this problem, and we enourage the use\n> ? of password protection in screen savers, idle time-outs, and other\n> ? methods which mitigate the security problems inherent in this problem.\n> ? In particular, user agents which cache credentials are encouraged to\n> ? provide a readily accessible mechanism for discarding cached credentials\n> ? under user control.\n> \n>\nOK, I've adopted this rewrite, with the exception of the \"For user agents\";\nsince the work arounds are often server side rather than having anything\nto do with user agents.\n- Jim\n\n\n\n", "id": "lists-012-0009933"}, {"subject": "Re: Security considerations from RE-AUTHENTICATIONREQUESTE", "content": "  I've attempted to provide a more general discussion of the issue of\n  cached credentials, appended below.\n\n>>>>> \"JG\" == Jim Gettys <jg@pa.dec.com> writes:\n\nJG> 15.6 Authentication Credentials and Idle Clients\n\nJG> Existing HTTP clients and user agents typically retain authentication\nJG> information indefinately. HTTP/1.1. does not provide a method for an origin\nJG> server or proxy to force reauthentication. Since clients may be idle for\nJG> extended periods between use (and unauthorized users may have access to\nJG> the user agent during these idle periods), this is a significant defect\nJG> that requires further extensions to HTTP. This is currently under separate\nJG> study. For user agents, there are a number of work-arounds to parts of\nJG> this problem, and we enourage the use of password protection in screen\nJG> savers, idle time-outs, and other methods which mitigate the security\nJG> problems inherent in this problem.\n\n15.6 Caching Authentication Credentials\n\n  Existing HTTP clients and user agents typically retain authentication\n  information indefinately. HTTP/1.1. does not provide a method for a\n  server to direct clients to dicard these cached credentials.  This is a\n  significant defect that requires further extensions to HTTP.\n  Circumstances under which this should be possible include but are not\n  limited to:\n\n    - Clients which have been idle for an extended period following which\n      the server may wish to cause the client to reprompt the user for\n      credentials.\n\n    - Applications which include a session termination indication (such as\n      a 'logout' or 'commit' button on a page) after which the server side\n      of the application 'knows' that there is no further reason for the\n      client to retain the credentials.\n\n  This is currently under separate study.  For user agents, there are a\n  number of work-arounds to parts of this problem, and we enourage the use\n  of password protection in screen savers, idle time-outs, and other\n  methods which mitigate the security problems inherent in this problem.\n  In particular, user agents which cache credentials are encouraged to\n  provide a readily accessible mechanism for discarding cached credentials\n  under user control.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n", "id": "lists-012-0021735"}, {"subject": "Re: Security considerations from RE-AUTHENTICATIONREQUESTE", "content": "There is another problem with the re-write ... I'll comment below ...\n\nOn Mon, 16 Feb 1998, Jim Gettys wrote:\n\n> \n> \n> > View: Browse?HTML????Browse?Raw?Text\n> > From: \"Scott Lawrence\" <lawrence@agranat.com>\n> > Date: Mon, 16 Feb 1998 11:18:48 -0500\n> > To: jg@pa.dec.com (Jim Gettys)\n> > Cc: http-wg@cuckoo.hpl.hp.com\n> > Subject: Re: Security considerations from RE-AUTHENTICATION-REQUESTED\n> > \n> > ? I've attempted to provide a more general discussion of the issue of\n> > ? cached credentials, appended below.\n> > \n> > >>>>> \"JG\" == Jim Gettys <jg@pa.dec.com> writes:\n> > \n> > JG> 15.6 Authentication Credentials and Idle Clients\n> > \n> > JG> Existing HTTP clients and user agents typically retain authentication\n> > JG> information indefinately. HTTP/1.1. does not provide a method for an origin\n> > JG> server or proxy to force reauthentication. Since clients may be idle for\n> > JG> extended periods between use (and unauthorized users may have access to\n> > JG> the user agent during these idle periods), this is a significant defect\n> > JG> that requires further extensions to HTTP. This is currently under separate\n> > JG> study. For user agents, there are a number of work-arounds to parts of\n> > JG> this problem, and we enourage the use of password protection in screen\n> > JG> savers, idle time-outs, and other methods which mitigate the security\n> > JG> problems inherent in this problem.\n> > \n> > 15.6 Caching Authentication Credentials\n> > \n> > ? Existing HTTP clients and user agents typically retain authentication\n> > ? information indefinately. HTTP/1.1. does not provide a method for a\n> > ? server to direct clients to dicard these cached credentials.? This is a\n> > ? significant defect that requires further extensions to HTTP.\n> > ? Circumstances under which this should be possible include but are not\n> > ? limited to:\n\nBeginning with \"Circumstances\", the wording of this sentence doesn't fit\nsemantically in the HTTP specification as it is justification for\nthe further extensions and not worded as a security concern. Perhaps\ntry:\n           Circumstances under which credential caching can interfere\n           with the application's security model include but are not\n           limited to:\n\n\n> > \n> > ??? - Clients which have been idle for an extended period following which\n> > ????? the server may wish to cause the client to reprompt the user for\n> > ????? credentials.\n> > \n> > ??? - Applications which include a session termination indication (such as\n> > ????? a 'logout' or 'commit' button on a page) after which the server side\n> > ????? of the application 'knows' that there is no further reason for the\n> > ????? client to retain the credentials.\n> > \n> > ? This is currently under separate study.? For user agents, there are a\n> > ? number of work-arounds to parts of this problem, and we enourage the use\n> > ? of password protection in screen savers, idle time-outs, and other\n> > ? methods which mitigate the security problems inherent in this problem.\n> > ? In particular, user agents which cache credentials are encouraged to\n> > ? provide a readily accessible mechanism for discarding cached credentials\n> > ? under user control.\n> > \n> >\n> OK, I've adopted this rewrite, with the exception of the \"For user agents\";\n> since the work arounds are often server side rather than having anything\n> to do with user agents.\n> - Jim\n> \n> \n\n\n\n", "id": "lists-012-0032510"}, {"subject": "FW: I-D ACTION:draft-cohen-http-ext-postal00.tx", "content": "-----Original Message-----\nFrom: Internet-Drafts@ns.ietf.org [mailto:Internet-Drafts@ns.ietf.org] \nSent: Tuesday, February 17, 1998 4:10 AM\nTo: IETF-Announce@ns.ietf.org\nSubject: I-D ACTION:draft-cohen-http-ext-postal-00.txt\n\n\nA New Internet-Draft is available from the on-line Internet-Drafts\ndirectories.\n\n\nTitle           : Don't Go Postal - An argument against\n  improperly overloading the HTTP POST Method\nAuthor(s): J. Cohen et al.\nFilename: draft-cohen-http-ext-postal-00.txt\nPages: \nDate: 16-Feb-98\n\nAs time goes on, more and more groups are extending HTTP's functionality. In\nusing HTTP, a decision is made to either use a new method name for new\nfunctionality or to overload an existing one such as POST.  Our belief is\nthat in most cases, overloading existing method names, with POST as a\nparticularly troublesome example, is a bad idea.  We, as a group of\nindividuals, suggest that the default requirement for new HTTP functionality\nmust be to create a new method name.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-cohen-http-ext-postal-00.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-cohen-http-ext-postal-00.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-cohen-http-ext-postal-00.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n-----\nTo: \nSubject: \nDate: Tue, 17 Feb 1998 10:06:31 -0800\nX-Mailer: Internet Mail Service (5.5.1960.3)\n\n\n\nbegin 600 ATT25606.txt\nM0V]N=&5N=\"UT>7!E.B!M97-S86=E+V5X=&5R;F%L+6)O9'D[#0H)86-C97-S\nM+71Y<&4](FUA:6PM<V5R=F5R(CL-\"@ES97)V97(](FUA:6QS97)V0&1S+FEN\nM=&5R;FEC+FYE=\"(-\"@T*0V]N=&5N=\"U4>7!E.B!T97AT+W!L86EN#0I#;VYT\nM96YT+4E$.@D\\,3DY.#`R,38Q-#$W,3,N22U$0&EE=&8N;W)G/@T*#0I%3D-/\nM1$E.1R!M:6UE#0I&24Q%(\"]I;G1E<FYE=\"UD<F%F=',O9')A9G0M8V]H96XM\n8:'1T<\"UE>'0M<&]S=&%L+3`P+G1X=`T*\n`\nend\n\nbegin 600 draft-cohen-http-ext-postal-00.url\nM6TEN=&5R;F5T4VAO<G1C=71=#0I54DP]9G1P.B\\O9G1P+FEE=&8N;W)G+VEN\nM=&5R;F5T+61R869T<R]D<F%F=\"UC;VAE;BUH='1P+65X=\"UP;W-T86PM,#`N\n%='AT#0H=\n`\nend\n\n\n\n", "id": "lists-012-0045655"}, {"subject": "Test Day 1998-021", "content": "  This is the invitation for participation in HTTP/1.1\n  interoperability testing on Thursday, 1998-02-19.\n\n  The W3C HTTP/1.1 Implementors Forum page\n\n           <http://www.w3.org/Protocols/HTTP/Forum/>\n\n  lists many participants in the ongoing testing, and some resources\n  of interest to implementors.\n\n  ================================================================\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n\n", "id": "lists-012-0058261"}, {"subject": "Expectation Failed MUST or SHOULD", "content": "  I noticed this in the course of trying to work through documenting\n  what features we implement (I guess that means that it's working\n  :-).  I think this is just an editorial issue - there is a SHOULD\n  here that may leave implementors with the idea that the Expect\n  header can be ignored.\n\n  draft-ietf-http-v11-spec-rev-01.txt says:\n\n    14.47 Expect\n\n    The Expect request-header field is used to indicate that particular\n    server behaviors are required by the client.  A server that does not\n    understand or is unable to comply with any of the expectation values in\n    the Expect field of a request MUST respond with appropriate error\n    status.\n       [...ABNF...]\n    The server SHOULD respond with a 417 (Expectation Failed) status if any\n    of the expectations cannot be met.\n\n    This header field is defined with extensible syntax to allow for future\n    extensions.  If a server receives a request containing an Expect field\n    that includes an expectation-extension that it does not support, it MUST\n    respond with a 417 (Expectation Failed) status.\n\n  I'm not sure why the SHOULD in the second paragraph above is not a\n  MUST other than to allow for the possibility of responding with some\n  other 4xx status because of another problem with the request.  If\n  so, this should be clarified, perhaps by rewording it as something\n  like:\n\n    The server MUST respond with 417 (Expectation Failed) if any of\n    the expectations cannot be met or, if there are other problems\n    with the request, some other 4xx status.\n\n  If the thinking was/is that the SHOULD is to allow for\n  2068-compliant implementations then it should be changed - a\n  2068-complaint implementation may not be compliant with the Draft\n  Standard, and that is OK.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n", "id": "lists-012-0066929"}, {"subject": "Re:  This may be a stupid question ..", "content": "hitchhikers guide to the galaxy -zaphod- so long and thanks for all the fish!\nauthor dent\n\n\n\n", "id": "lists-012-0075659"}, {"subject": "experimental status cod", "content": "Hi,\n\n We're looking to implement something along the lines of the\n\"reauthentication\"\nor server provoked client retry that I've been going on about on the\nhttp-ext\nlist.\n\nWe intend to publish it as an experimental track document.\n\nI spoke with some of the editors this morning, and have read through \n draft-schulzrinne-http-status-00.txt\n\nBeing that there isnt an active IANA registry yet, and we need \na response code number, we'd like to pick one and move forward.\n\nAccording to schulzrinne, x00-x49 are HTTP managed for http stuff.\nSince this is a potential http extension, I think it can fit into \nthat definition.\n\nWhat I propose is that\n we will use 449 as a response code. \n So, in the absence of a IANA registry, I would like to announce our use\n of 449.\n\n\n---\nJosh Cohen <josh@microsoft.com>\nProgram Manager IE - Networking Protocols \n\n\n\n", "id": "lists-012-0082198"}, {"subject": "I-D ACTION:draft-ietf-http-state-man-mec08.txt,.p", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-08.txt,.ps\nPages: 22\nDate: 17-Feb-98\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal, but it can interoperate with HTTP/1.0 user\nagents that use Netscape's method.  (See the HISTORICAL section.)\n\nThis document reflects implementation experience with RFC 2109 and\nobsoletes it.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-08.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-08.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-08.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-0090117"}, {"subject": "editorial issue TRANSPARENTPROXY", "content": "Here are some clarifications around proxies that Roy was kind enough\nto draft up, to clarify when requirements apply to transparent proxies\nvs. proxies performing other actions (e.g. annotation).\n- Jim\n\n\nView: Browse?HTML????Browse?Raw?Text\nFrom: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\nDate: Tue, 17 Feb 1998 15:34:23 -0800\nTo: jg@w3.org\nCc: masinter@parc.xerox.com, lawrence@agranat.com,\ndmk@research.bell-labs.com,\n??????? joshco@microsoft.com\nSubject: transparent proxy changes\n\nAs per our discussion this morning, I wrote up a definition for\ntransparent and non-transparent proxies.? I also went through the rev-01\nspec and found all places where there is any difference between transparent\nand non-transparent behavior.? Changes are below.\n\n....Roy\n\n*** draft-ietf-http-v11-spec-rev-01.txtThu Feb 12 15:17:03 1998\n--- tproxy.txtTue Feb 17 15:27:15 1998\n***************\n*** 609,614 ****\n--- 609,622 ----\n??? are serviced internally or by passing them on, with possible\n??? translation, to other servers. A proxy must implement both the client\n??? and server requirements of this specification.\n+?? A \"transparent proxy\" is a proxy that does not modify the request or\n+?? response beyond what is required for proxy authentication and\n+?? identification.? A \"non-transparent proxy\" is a proxy that modifies the\n+?? request or response in order to provide some added service to the user\n+?? agent, such as group annotation services, media type transformation,\n+?? protocol reduction, or anonymity filtering.? Except where either\n+?? transparent or non-transparent behavior is explicitly stated, the\n+?? HTTP proxy requirements apply to both types of proxies.\n\n? gateway\n??? A server which acts as an intermediary for some other server. Unlike\n***************\n*** 1023,1029 ****\n? Proxy and gateway applications must be careful when forwarding messages\n? in protocol versions different from that of the application. Since the\n? protocol version indicates the protocol capability of the sender, a\n! proxy/gateway MUST never send a message with a version indicator which\n? is greater than its actual version. If a higher version request is\n? received, the proxy/gateway MUST either downgrade the request version,\n? or respond with an error, or switch to tunnel behavior.\n--- 1031,1037 ----\n? Proxy and gateway applications must be careful when forwarding messages\n? in protocol versions different from that of the application. Since the\n? protocol version indicates the protocol capability of the sender, a\n! proxy/gateway MUST NOT send a message with a version indicator which\n? is greater than its actual version. If a higher version request is\n? received, the proxy/gateway MUST either downgrade the request version,\n? or respond with an error, or switch to tunnel behavior.\n***************\n*** 1964,1970 ****\n? interpret the request. Servers SHOULD respond to invalid Request-URIs\n? with an appropriate status code.\n\n! In requests that they forward, proxies MUST NOT rewrite the \"abs_path\"\n? part of a Request-URI in any way except as noted above to replace a null\n? abs_path with \"*\", no matter what the proxy does in its internal\n? implementation.\n--- 1972,1979 ----\n? interpret the request. Servers SHOULD respond to invalid Request-URIs\n? with an appropriate status code.\n\n! In requests that they forward, transparent proxies MUST NOT rewrite the\n! \"abs_path\"\n? part of a Request-URI in any way except as noted above to replace a null\n? abs_path with \"*\", no matter what the proxy does in its internal\n? implementation.\n***************\n*** 2245,2251 ****\n? The extension-header mechanism allows additional entity-header fields to\n? be defined without changing the protocol, but these fields cannot be\n? assumed to be recognizable by the recipient. Unrecognized header fields\n! SHOULD be ignored by the recipient and MUST be forwarded by proxies.\n\n\n? 7.2 Entity Body\n--- 2254,2261 ----\n? The extension-header mechanism allows additional entity-header fields to\n? be defined without changing the protocol, but these fields cannot be\n? assumed to be recognizable by the recipient. Unrecognized header fields\n! SHOULD be ignored by the recipient and MUST be forwarded by transparent\n! proxies.\n\n\n? 7.2 Entity Body\n***************\n*** 5029,5039 ****\n? 13.5.2 Non-modifiable Headers\n\n? Some features of the HTTP/1.1 protocol, such as Digest Authentication,\n! depend on the value of certain end-to-end headers. A cache or non-\n! caching proxy SHOULD NOT modify an end-to-end header unless the\n? definition of that header requires or specifically allows that.\n\n! A cache or non-caching proxy MUST NOT modify any of the following fields\n? in a request or response, nor may it add any of these fields if not\n? already present:\n\n--- 5039,5049 ----\n? 13.5.2 Non-modifiable Headers\n\n? Some features of the HTTP/1.1 protocol, such as Digest Authentication,\n! depend on the value of certain end-to-end headers. A transparent proxy\n! SHOULD NOT modify an end-to-end header unless the\n? definition of that header requires or specifically allows that.\n\n! A transparent proxy MUST NOT modify any of the following fields\n? in a request or response, nor may it add any of these fields if not\n? already present:\n\n***************\n*** 5049,5055 ****\n? INTERNET-DRAFT??????????? HTTP/1.1 Friday, November 21, 1997\n\n\n! A cache or non-caching proxy MUST NOT modify any of the following fields\n? in a response:\n\n??? .? Expires\n--- 5059,5065 ----\n? INTERNET-DRAFT??????????? HTTP/1.1 Friday, November 21, 1997\n\n\n! A transparent proxy MUST NOT modify any of the following fields\n? in a response:\n\n??? .? Expires\n***************\n*** 5063,5069 ****\n??? Note: a typical reason for adding the Content-Length header is that\n??? the origin server sent the content chunked encoded.\n\n! A cache or non-caching proxy MUST NOT modify or add any of the following\n? fields in a response that contains the no-transform Cache-Control\n? directive, or in any request:\n\n--- 5073,5079 ----\n??? Note: a typical reason for adding the Content-Length header is that\n??? the origin server sent the content chunked encoded.\n\n! A proxy MUST NOT modify or add any of the following\n? fields in a response that contains the no-transform Cache-Control\n? directive, or in any request:\n\n***************\n*** 5071,5077 ****\n??? .?\n??? .? Content-Range\n??? .? Content-Type\n! A cache or non-caching proxy MAY modify or add these fields in a\n? response that does not include no-transform, but if it does so, it MUST\n? add a Warning 114 (Transformation applied) if one does not already\n? appear in the response.\n--- 5081,5088 ----\n??? .?\n??? .? Content-Range\n??? .? Content-Type\n!\n! A non-transparent proxy MAY modify or add these fields in a\n? response that does not include no-transform, but if it does so, it MUST\n? add a Warning 114 (Transformation applied) if one does not already\n? appear in the response.\n***************\n*** 6201,6207 ****\n? 14.9.5 No-Transform Directive\n\n? Implementers of intermediate caches (proxies) have found it useful to\n! convert the media type of certain entity bodies. A proxy might, for\n\n? Fielding, et al?????????????????????????????????? [Page 107]\n\n--- 6212,6219 ----\n? 14.9.5 No-Transform Directive\n\n? Implementers of intermediate caches (proxies) have found it useful to\n! convert the media type of certain entity bodies. A non-transparent proxy\n! might, for\n\n? Fielding, et al?????????????????????????????????? [Page 107]\n\n***************\n*** 6210,6220 ****\n\n\n? example, convert between image formats in order to save cache space or\n! to reduce the amount of traffic on a slow link. HTTP has to date been\n! silent on these transformations.\n\n! Serious operational problems have already occurred, however, when these\n! transformations have been applied to entity bodies intended for certain\n? kinds of applications. For example, applications for medical imaging,\n? scientific data analysis and those using end-to-end authentication, all\n? depend on receiving an entity body that is bit for bit identical to the\n--- 6222,6231 ----\n\n\n? example, convert between image formats in order to save cache space or\n! to reduce the amount of traffic on a slow link.\n\n! Serious operational problems might occur, however, when these\n! transformations are applied to entity bodies intended for certain\n? kinds of applications. For example, applications for medical imaging,\n? scientific data analysis and those using end-to-end authentication, all\n? depend on receiving an entity body that is bit for bit identical to the\n***************\n*** 6354,6363 ****\n???????? Content-Encoding: gzip\n? The Content-Encoding is a characteristic of the entity identified by the\n? Request-URI. Typically, the entity-body is stored with this encoding and\n! is only decoded before rendering or analogous usage. However, a proxy\n! MAY modify the content-coding if the new coding is known to be\n! acceptable to the recipient, unless the \"no-transform\" Cache-Control\n! directive is present in the message.\n\n? If the content-coding of an entity is not \"identity\", then the response\n? MUST including a Content-Encoding entity-header (section 14.12) that\n--- 6365,6374 ----\n???????? Content-Encoding: gzip\n? The Content-Encoding is a characteristic of the entity identified by the\n? Request-URI. Typically, the entity-body is stored with this encoding and\n! is only decoded before rendering or analogous usage. However, a\n! non-transparent proxy MAY modify the content-coding if the new coding is\n! known to be acceptable to the recipient and the \"no-transform\"\n! Cache-Control directive is not present in the message.\n\n? If the content-coding of an entity is not \"identity\", then the response\n? MUST including a Content-Encoding entity-header (section 14.12) that\n***************\n*** 6838,6844 ****\n? Internet host which issued the request. For example, when a request is\n? passed through a proxy the original issuer's address SHOULD be used.\n\n!?? Note: The client SHOULD not send the From header field without the\n??? user's approval, as it may conflict with the user's privacy\n\n? Fielding, et al?????????????????????????????????? [Page 118]\n--- 6849,6855 ----\n? Internet host which issued the request. For example, when a request is\n? passed through a proxy the original issuer's address SHOULD be used.\n\n!?? Note: The client SHOULD NOT send the From header field without the\n??? user's approval, as it may conflict with the user's privacy\n\n? Fielding, et al?????????????????????????????????? [Page 118]\n\n\n\n", "id": "lists-012-0099040"}, {"subject": "505 response a MUST", "content": "  Another issue found while trying to document supported features\n  (there appears to be some value to this rather tedious exercise :)\n\n  draft-ietf-http-v11-spec-rev-01 defines:\n\n    10.5.6 505 HTTP Version Not Supported\n\n    The server does not support, or refuses to support, the HTTP protocol\n    version that was used in the request message. The server is indicating\n    that it is unable or unwilling to complete the request using the same\n    major version as the client, as described in section 3.1, other than\n    with this error message. The response SHOULD contain an entity\n    describing why that version is not supported and what other protocols\n    are supported by that server.\n\n  ... and section 3.1 spells out various rules about version number\n  usage, but does not specify that a server MUST send a 505 response\n  if it receives a major version number higher than the highest\n  version it implements.\n\n  I've tried a few of the servers out there, and they all return\n  success when I send HTTP/2.0 requests (which were 1.1 requests with\n  2.0 labels).\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n", "id": "lists-012-0117704"}, {"subject": "New MHTML draft", "content": "Two new MHTML drafts have recently been published. Since we have rather\nlate discovered and had to handle discrepancies between MHTML and HTTP 1.1,\nit would be valuable if someone in the HTTP group had time to review\nthese drafts.\n\nIf you write comments on them, send the comments to\nIETF working group on HTML in e-mail <mhtml@segate.sunet.se>\nor to both that mailing list and the HTTP mailing list. Do not send\ncomments only to the HTTP mailing list.\n\nBelow are excerpts from the announcements of the two new drafts:\n\n--- --- ---\n\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the MIME Encapsulation of Aggregate HTML\nDocuments Working Group of the IETF.\n\n        Title           : MIME Encapsulation of Aggregate Documents, such as\n                          HTML (MHTML)\n        Author(s)       : N. Shelness, A. Hopmann, J. Palme\n        Filename        : draft-ietf-mhtml-rev-05.txt\n        Pages           : 27\n        Date            : 16-Feb-98\n\nHTML [RFC 1866] defines a powerful means of specifying multimedia\ndocuments. These multimedia documents consist of a text/html root\nresource (object)and other subsidiary resources (image, video clip,\napplet, etc. objects) referenced by Uniform Resource Identifiers (URIs)\nwithin the text/html root resource. When an HTML multimedia document is\nretrieved by a browser, each of these component resources is\nindividually retrieved in real time from a location, and using a\nprotocol, specified by each URI.\n\nIn order to transfer a complete HTML multimedia document in a single e-\nmail message, it is necessary to:- a) aggregate a text/html root\nresource and all of the subsidiary resources it references into a\nsingle composite message structure, and b) define a means by which URIs\nin the text/html root can reference subsidiary resources within that\ncomposite message structure.\n\nThis document does both. It a) defines the use of a MIME\nmultipart/related structure to aggregate a text/html root resource and\nthe subsidiary resources it references, and b) specifies two MIME\ncontent-headers (Content-Base and Content-Location) that allow URIs in\na multipart/related text/html root body part to reference subsidiary\nresources in other body parts of the same multipart/related structure.\n\nWhile initially designed to support e-mail transfer of complete multi-\nresource HTML multimedia documents, these conventions can also be\nemployed by other transfer protocols such as HTTP and FTP to retrieve a\ncomplete multi-resource HTML multimedia document in a single transfer\nor for storage and archiving of complete HTML-documents.\n\nDifferences between this and a previous version of this standard, which\nwas published as RFC 2110, are summarized in chapter 13.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n        \"get draft-ietf-mhtml-rev-05.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-mhtml-rev-05.txt\n\n--- --- ---\n\nA New Internet-Draft is available from the on-line Internet-Drafts\ndirectories.  This draft is a work item of the MIME Encapsulation of\nAggregate HTML Documents Working Group of the IETF.\n\n        Title           : Sending HTML in MIME, an informational\n                          supplement to the RFC:  MIME Encapsulation of\n                          Aggregate HTML Documents (MHTML)\n        Author(s)       : J. Palme\n        Filename        : draft-ietf-mhtml-info-09.txt\n        Pages           : 20\n        Date            : 16-Feb-98\n\nThe memo ''MIME Encapsulation of Aggregate HTML Documents (MHTML)''\n(draft-ietf-mhtml-rev-05.txt) specifies how to send packaged aggregate\nHTML objects in MIME format. This memo is an accompanying informational\ndocument, intended to be an aid to developers. This document is not an\nInternet standard.\n\nIssues discussed are implementation methods, caching strategies,\nproblems with rewriting of URIs, making messages suitable both for\nmailers which can and which cannot handle Multipart/related and handling\nrecipients which do not have full Internet connectivity.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n        \"get draft-ietf-mhtml-info-09.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-mhtml-info-09.txt\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n", "id": "lists-012-0126168"}, {"subject": "HTTP Rev02 production has started..", "content": "I've started the physical production of the draft (rev-02) at this point.\n\nRev-02 represents a milestone, with all issues resolved, and lots of \neditorial cleanup work.  It does do some reorganization of sections, so \nmany section numbers will change for the first time since RFC 2068.  It \nis concievable that this draft could be the draft standard, so it will \nbe the draft to read carefully, when I've issued it.\n\nIf people can hold off new issues until after I issue the draft\nin a day or so (and escape for a bit of vacation), it may help me\n(and you) to keep things straight.\n- Jim\n\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n", "id": "lists-012-0138483"}, {"subject": "draft-cohen-http-ext-postal0", "content": "I was confused by a number of things.\n\n1) What exactly is the threat?\n\n2) I disagree that POST by itself defines an action.  It's really POST +\nthe host connected to + the URL accessed.\n\n3) The Introduction points to the problem as being the Internet Printing\nProtocol's binary payload.  Doesn't the same problem already occur with\nfile upload?\n\n4) Which direction poses the threat?  Does the threat to security (to an\nenterprise) derive from passing from the enterprise, out through a PFB,\nto the outside world?  Or is there a threat from, in some fashion,\npassing from the outside world, through a PFB, into an enterprise\nserver?\n\n5) Suppose we add a new method to HTTP for the IPP.  Do we also have to\nadd a new method to HTTP for each such application, with each method\nhaving tightly constrained semantics?\n\n6) I think the \"cat is out of the bag\" on POST.  Assume new,\napplication-specific methods.  Should a restrictive administrator ban\nPOST and insist that all applications migrate to new POST-like methods\nwith exposed semantics?\n\n7) Isn't IPP a \"red herring\"?  What restrictive administrator can\nconfigure to \"deny all, selectively allow\" even now, given the huge\nrange of forms that users inside the enterprise may wish to use.\n\n8) It seems to me that the paper presumes a particular firewall\narchitecture and then describes how protocols should be designed to\naccommodate it.  That seems folly in the absence of a standard to which\nPFBs are designed.\n\nDave Kristol\n\n\n\n", "id": "lists-012-0145909"}, {"subject": "Rev02 of the HTTP/1.1 specification has been submitted..", "content": "At http://www.w3.org/Protocols/HTTP/Issues/.  As usual, it is\navailable there in plain-text, postscript and Microsoft Word with and\nwithout change bars, either uncompressed or gzip'ed.\n\nAs of when I submitted this draft, there were no outstanding issues\nwith the specification I knew of, either technical or editorial.  This\nrepresents a significant milestone for the specification; my thanks to\nyou all for your help.  \n\nWhile it is unlikely that this document will be the final draft before\ndraft standard given previous experience, that we have not finished\nthe interoperability report, and that this draft depends both on the\nauthentication specification and the URI specification, there is a\nnon-zero chance it is the final draft. If you have been putting off a\nfinal read, now is the time to read the specification; we hope to\nfinish the interoperability report in a timely fashion.\n\nI also expect everyone who drafted text to resolve issues for the\nspecification to check that I correctly edited those changes into this\ndraft, and to let me know of any problems.\n\nI will be on vacation all next week.\n\nYour HTTP/1.1 Editor,\nJim Gettys\n\n\n\n", "id": "lists-012-0153692"}, {"subject": "Re: Rev02 of the HTTP/1.1 specification has been submitted..", "content": "jg@zorch.w3.org wrote:\n> \n> At http://www.w3.org/Protocols/HTTP/Issues/.  As usual, it is\n> available there in plain-text, postscript and Microsoft Word with and\n> without change bars, either uncompressed or gzip'ed.\n\nBravo Jim, and the rest of the editorial group, for all your hard work!\n\nDave Kristol\n\n\n\n", "id": "lists-012-0162417"}, {"subject": "Re: editorial issue TRANSPARENTPROXY", "content": "Jim Gettys wrote:\n> \n> Here are some clarifications around proxies that Roy was kind enough\n> to draft up, to clarify when requirements apply to transparent proxies\n> vs. proxies performing other actions (e.g. annotation).\n>                                 - Jim\n> \n> View: Browse HTML    Browse Raw Text\n> From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n> Date: Tue, 17 Feb 1998 15:34:23 -0800\n> To: jg@w3.org\n> Cc: masinter@parc.xerox.com, lawrence@agranat.com,\n> dmk@research.bell-labs.com,\n>         joshco@microsoft.com\n> Subject: transparent proxy changes\n> \n> As per our discussion this morning, I wrote up a definition for\n> transparent and non-transparent proxies.  I also went through the rev-01\n> spec and found all places where there is any difference between transparent\n> and non-transparent behavior.  Changes are below.\n\nI've read this fairly carefully. It looks OK to me.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n", "id": "lists-012-0170200"}, {"subject": "editorial issue TRANSPARENTPROXY", "content": "** Reply to note from jg@pa.dec.com (Jim Gettys) Wed, 18 Feb 1998 07:39:42 -0800\n\nPerhaps related, Ronald Tschalaer was testing our proxy implementation\nfor us last week ;-), and discovered that we will (if configured to),\nadd a From header-field to outbound requests.\n(the From header we add is administrator configured, intended to be\nanalogous to mailto:webmaster@host.org)\n\nRonald had a concern that this could words around \"From\" say client,\nand that this could somehow cause trouble (not the least of which would\nbe two From header-fields in our implementation.\n\nIt had not occurred to me before, but I think this could mess up\nDigest.\n\nComments?\nIf no-one else volunteers, I can write up some text to clarify this.\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-0180068"}, {"subject": "draft-cohen-http-ext-postal00.tx", "content": "draft-cohen-http-ext-postal-00.txt makes a number of points.  I'll\ndiscuss three of them.\n\nPoint 1: `New protocols should be firewall-friendly, i.e. allow for\neasy blocking/filtering'\n\nI agree with point 1.\n\nPoint 2: `In order to be firewall-friendly, a new protocol should not\nuse the POST method plus a new MIME type, but a completely new method'\n\nI disagree with point 2.  The draft argues that using a new method,\ninstead of a new MIME type, allows the firewall to be more efficient,\nbecause it has to inspect a smaller part of the messages through it.\n\nHowever, in the current internet environment, any decent firewall must\nalso inspect the MIME type if it is to be effective at enforcing\nsecurity policies, because a lot of insecure things, like GET\nresponses which carry native-code applets, are only detectable by\ninspecting the MIME type.  The draft acknowledges this in section 8\nbut argues that this trend should be reversed.  However, I argue that\nthis trend is impossible to reverse.  Some software developers\nindependent of the IETF will continue to use POST (and GET) in an\neffort to get the highest possible market penetration for their new\ninsecure and/or bandwidth-hogging multimedia formats and applications.\nIn fact, I will expect that future firewalls will become increasingly\nsophisticated in order to filter out the stuff produced by above\nsoftware developers: there is an arms race going on here.\n\nSo, because of all the non-IETF protocols and data formats developers\nout there, `don't use POST' will not allow firewalls to be any\nsimpler, and any efficiency gains will be limited to the case of\nrejecting messages only.\n\nIn short, I would term any a new protocol as sufficiently\nfirewall-friendly if it uses either a new method or a new MIME type.\n\nPoint 3: `Any new (IETF) protocol should have the property that a\nfirewall will block it by default, and that explicit action is needed\nby the firewall administrator to enable its use'.\n\n[This point is stated most clearly in section 2:\n  `While the designers of a new\n   protocol may feel that their new protocol introduces no new risks,\n   they do not have the right to decide what a PFB will support.'\n]\n\nI could not disagree more.  I feel that there are many cases in which\nit would be quite legitimate for the IETF to decide that, for a\ncertain protocol, the default mode should be that `the average\nliberally configured firewall' accepts the protocol.\n\nAlso, one should realise that the IETF does not operate in a vacuum.\nIf it makes deployment of its open standards much more difficult by\nalways requiring explicit action by all firewall administrators, then\nproprietary de-facto standards, which were developed to work through\nfirewalls by default, will take over in many cases.\n\nI feel that the IETF would shoot itself in the foot if it were to\nadopt point 3 above as an absolute principle.  Adopting point 3 would\nresult in the IETF being unable to participate in, compete with, or\npre-empt some types of internet protocol efforts by independent\nparties.  Also, it would make it much more difficult upgrade some of\nthe current `streaming protocols', which are built on top of\napplication/something GET responses, to real streaming protocols which\nare more internet-friendly.\n\n\nNow for some specific nitpicks:\n\nSection 5: \n\n\"Unfortunately, when the letters P.O.S.T. came into existence as an\nHTTP method, the operational meaning was fairly specific in that it\nwas for HTTP form data submission.\"  This is not historically accurate\nas far as I know.  In a 5 Nov 1993 Tim BL draft of the HTTP spec, POST\nis defined as `Creates a new object linked to the specified\nobject. [...]  The new document is the data part of the\nrequest. [...]'.  In fact, I believe that forms were first defined a\nfew years after POST.\n\nApart from that, I find the `preserve the purity of the original\ndesign' arguments in section 5 and 6 not very compelling anyway.\n\nSection 7:\n\nPossible editing error: I believe that the `not' in the last line on\npage 6 should not have been there.\n\nSection 9:\n\nAre you talking about apache as an origin server or as a proxy?\n\n\nSome final comments:\n\nThis draft touches on the highly political issue of whether a software\ndeveloper has the `right' to try to bypass the security/bandwidth\nallocation policies set by firewall administrators.  The draft answers\nthis question with a resounding `no', and I tend to agree.  However,\nwe have to realise that not all people will agree with this: there are\nbusiness models which depend on the current de facto `yes' staying a\n`yes'.  Seen in this light, the whole issue bears a striking\nresemblance to the issue of whether advertising servers have the\n`right' to set persistent cookies in order to gather cross-site\nstatistics.  So I can predict with some confidence that, if this draft\ngoes forward, we will at some point have press releases in which\npeople ask the IETF not to endorse this draft because it would kill\nthe profitability of all kinds of useful content sites.\n\nAlso, though I agree with the `no', I do not want to take the extreme\nposition that all new protocols, even those which have no big new\nsecurity/bandwidth impact, should be created to be automatically\nrejected by all firewalls.\n\nAs for the specific case of IPP: I reviewed the IPP security\nconsiderations and I think that it is perfectly legitimate for IPP to\nuse POST.\n\nKoen.\n\n\n\n", "id": "lists-012-0187614"}, {"subject": "Age calculations in HTTP 1.1 11/21/97 documen", "content": "Greetings HTTP Working Group,\n\nI am hereby forwarding a note that I had sent to some colleagues. They agreed\nthat the response_delay is counted twice. My colleagues felt that the\napparent_age definition should stay as is, but that the corrected_initial_age\nshould equal the corrected_received_age exactly.\n\nCan you look at this issue? Thank you.\n\nRegards,\n\nBurt Silverman, IBM Networking Division\n---------------------- Forwarded by Burt Silverman/Watson/IBM on 23/02/98 15:58\n---------------------------\n\n\nBurt Silverman\n20/02/98 16:45\n\nTo:\ncc:\nFrom: Burt Silverman/Watson/IBM @ IBMUS\nSubject: Age calculations in HTTP 1.1 11/21/97 document\n\nHi,\n\nLooking at section 13.2.3, I found that I could only make sense of the formulas\nif I switched the definition of apparent_age from\n\napparent_age = max(0,response_time - date_value);\n\nto\n\napparent_age = max(0,request_time - date_value);\n\nFor example, if in year\n\n1981, the user sent a request\n1989, date_value\n1990, request from the cache to the server\n2000, resonse_time\n2001, now\n\nthen,\napparent_age = 1 year\nresponse_delay = 10 years\ncorrected_initial_age = 11 years\nresident_time = 1 year\nand,\ncurrent_age = 12 years. This seems to make sense.\n\nBut with the official definitions,\napparent_age = 11 years\nresponse_delay = 10 years\ncorrected_initial_age = 21 years\nresident_time = 1 year\nand\ncurrent_age = 22 years. This brings us back to 1979, and doesn't make much\nsense.\n\nAm I correct?\n\nBurt\n\n\n\n", "id": "lists-012-0199885"}, {"subject": "comments on HTTP/1.1 Rev02 20Feb9", "content": "First:  the alphabetized index is fantastic.  Thanks!!\n\nThese comments apply to the Rev-02 draft, 20 Feb 1998, available off\n<http://www.w3.org/Protocols/HTTP/Issues/>.  I've divided my remarks\nbelow into possible issues, nits, and nitty nits.\n\nDave Kristol\n==================================\nPossible Issues\n\n1) 9.2 OPTIONS\n    \"The response body, if any, SHOULD also include....\"\n\n    The spec. says the body itself is undefined.  Can we at least state\n    what the media type is?\n\n2) 10.2.7, 206 Partial Content\n    \"... the Content-Length header field in the response MUST match...\"\n\n    Is there some reason why the entity couldn't be sent with chunked\n    transfer coding instead and *without* a Content-Length?\n\n    It wasn't clear to me, until sect. 19.6.3, that a server could\n    *limit* itself to sending these headers in response to If-Range.\n    Could we make that clearer?\n\n3) 13, Caching in HTTP\n    Last paragraph:  I think something like the following needs to be\n    added at the end:\n    \"In such cases, the design should also provide a way to inform the\n    end user of a break in transparency.\"\n\n4) 13.11 Write-Through Mandatory\n    Suppose I want to add a custom method to HTTP, one of whose\n    side-effects would be to invalidate a cache entry.  (Suppose\n    I were adding something comparable to DELETE, for example.)\n    How would the cache know it must invalidate the associated\n    resource?  I don't think any of the Cache-Control directives\n    can tell the cache to discard an entry, can they?  (And if they\n    could, that would be an interesting denial of service attack.)\n\n5) 14.8 Authorization\n    \"HTTP access authentication is described in section 11.\"\n    Not anymore, it isn't!  At least, not in any detail.\n\n6) 14.9 Cache-Control\n    I have three editorial suggestions to the reference utility of the\n    spec.\n\n    1) It would be nice if the various cache-control directives were\n    in the document's index.\n\n    2) It would be nice if each of the c-c directives had its own\n    paragraph, similar to public/private/no-cache in 14.9.1.  The\n    hanging indents make it real easy to find descriptions.\n\n    3) Provide a very short description (one sentence would be nice)\n    for each of the c-c directives just to give the sense of what each\n    one does.  With each one I'd also like a list of the sections under\n    14.9 where the details could be found.\n\n7) 14.39 TE\n    Isn't the wording of the Note backwards?  Shouldn't it be:\n\n    \"Note: Because of backward compatibility considerations with RFC\n    2068, neither parameter nor accept-params can be used with the\n    \"chunked\" transfer-coding.\n==================================\nNits\n\n1) 2.1, implied *LWS\n    I would feel better if \"... between any two tokens\" made it clear\n    we're talking about the grammatical non-terminal \"token\".  In\n    compiler-speak, ';' is also a token, but that's not what's meant\n    here.\n\n2) 8.1.1\n    \"often require[s] a client to make\"\n       ^-> add\n    \"Analys[i]s of these...\"\n            ^-> change\n\n3) 8.2.4, Requirements for HTTP/1.1 clients\n    \"... the client should not wait for a[n indefinite or] lengthy period\" ->\n                                          --------------- -> delete\n    (\"indefinite\" qualifies already as lengthy :-)\n\n4) 10.3, Redirection 3xx\n    \"A client SHOULD implement detect infinite...\"\n         --------- -> delete\n\n5) 10.3.6, 305 Use Proxy\n    \"Note: ... a single request, or [to be] generated by...\"\n         ----- -> add\n6) 11, Access Authentication\n    \"HTTP provides a several optional challenge-response authentication\n       ^-> delete\n    mechanisms which MAY be used ...\"\n         ^-> add\n\n    \"The general framework for access authentication, and the specifications\n      --- -> add<- -\n    of \"basic\" and \"digest\" authentication, are specified ...\"\n      ^-> add\n\n7) 12.1, Server-driven Negotiation\n    \"The Vary header field can be used to express the parameters\n    [the server uses] to select a representation that is subject to ...\"\n    ================ -> change ------- -> add\n\n8) 12.3, Transparent Negotiation (last paragraph)\n    \"... does not prevent any such mechanism from being developed as an\n    extension [that could be] used ...\"\n           ============= -> change\n9) 13.1.2, Warnings\n    \"2. ... entity headers that [is] not rectified by a revalidation[,]\"\n == -> change  <- =\n    [Verb refers back to \"aspect\".]\n\n10) 13.2.1 Server-Specified Expiration\n    [last sentence]\n    \"See section 13.13 for [an] explanation of ...\"\n        -- -> add\n11) 13.2.3, Age Calculations\n    \"Because of network-imposed delays, some significant interval may\n    pass [between] the time ...\n  ======= -> change\n\n12) 13.2.4, Expiration Calculations\n    \"If neither Expires nor Cache-Control: max-age or s-maxage ...\" ->\n    \"If none of Expires, Cache-Control: max-age, or Cache-Control: s-maxage ...\"\n\n13) 14.3, Accept Encoding\n    [first line]\n    \" ... but restricts the content-coding[s] ...\"\n       ^- add\n\n14) 14.9.3, Modifications of ...\n    \"If a response includes a[n] s-maxage directive, ...\"\n          ^- add\n\n    \" ... and the fact that [pre]-HTTP/1.1-compliant caches ...\"\n         --- -> change (from \"non\"; \"non\" would\n     also apply to HTTP/1.2)\n\n15) 14.9.6, Cache Control Extensions\n    Third paragraph beginning \"For example, ...\"\n    [Style] In this paragraph, the items in double-quotes, community,\n    private, UCI, would appear elsewhere in Courier typeface and\n    without quotes.  Be consistent.\n\n    Next paragraph:  \"private\"\"\n          ^- delete (at least)\n\n16) 14.16, Content-Range\n    \"... unless this length [this] is unknown ...\"\n         ---- -> delete\n\n    \"If the server ignores a byte-range-spec because ...\"\n     --------------- -> should this be Courier?\n\n17) 14.23, Host\n    \"... servers MUST respond with a 400 [(Bad Request)] status code ...\"\n      ------------- -> add\n\n18) 14.39, TE\n    14.40, Trailer\n[Style] \"chunked\" is rendered three different ways to mean the\nsame thing\n- chunked (Roman, no quotes)\n- \"chunked\" (Roman, double quotes)\n- \"chunked\" (Courier, double quotes)\n\nPick one and use it consistently.\n\n[in two places]\n  \"... for other header fields than Content-MD5\" ->\n  \"... for header fields other than Content-MD5\"\n\n19) 14.44, Vary\n    [3rd paragraph]\n    \"... for the duration of time [for] which the response is fresh.\"\n       === -> change\n\n20) 14.46, Warning\n    [next-to-last paragraph after 299]\n    \"... the sender MUST include a warn-date in each warning-value.\"\n    ->\n    \"... the sender MUST include in each warning-value a warn-date that\n    matches the date in the response.\"\n\n21) 15.1, Personal Information\n    [last sentence, reword as follows]\n    \"History shows that errors in this area often create serious\n    security and/or privacy problems and generate highly adverse\n    publicity for the implementer's company.\"\n\n22) 15.3, DNS Spoofing\n    \"... the deliberate mis[s]-association of ...\"\n        ^- delete\n\n23) 15.6, Authentication Credentials...\n    \"dicard\" -> \"discard\"\n    \"enourage\" -> \"encourage\"\n\n24) 17, References\n    [38] ... RFC 2279 [(]obsoletes RFC 2044), ...\n           ^- add\n\n25) 19.4.4, Introduction of Content-Encoding\n    \"... to perform [a function equivalent to] Content-Encoding.\"\n         ======================== ->\n     change from \"an equivalent function as\"\n\n26) 19.4.7, MHTML ...\n    \"... including line length limitations an[d] folding, ...\"\n          ^- add\n\n27) 19.5.1, Content-Disposition\n    \"... seem to be present in the filename-parm parameter, ...\"\n       -------------\n    Should be Courier typeface?\n\n28) 19.6, Compatibility ...\n    \"It is worth noting that[,] at the time of composing this specification, we ...\"\n         ^- add\n\n    \"... described in section 19.6.2.0.\"\n    That's 19.6.2 now, I think.\n\n29) 19.6.1.1, Changes to Simplify ...\n    \"... allocation of many IP addresses to a single host created serious\n    problems.\"\n    I think we need to be more specific.  How about:\n\n    \"... allocation of many IP addresses to a single host unnecessarily\n    depleted the IP address space at a time when the community feared\n    address space exhaustion.\"\n\n    [Bullet list]\n    \"Host request-headers are required in HTTP/1.1 requests.\"\n    change to\n    \"A client that sends an HTTP/1.1 request MUST send a Host header.\"\n\n30) 19.6.3.1, Significant Changes From ...\n    \"Require proxies [to] upgrade requests ...\"\n          -- -> add\n\n    \"The Cache-Control: max-age directive [was] not properly defined...\"\n       --- -> add\n\n    \"A new error code (416)[ ] was needed to indicate an error for a byte\n        ^- add\n    range request [that falls] outside...\"\n       ---------- -> add\n\n    [Last paragraph]\n    Is it transfer *encoding* or transfer *coding*?\n\n    \"... The solution is [that] transfer codings become...\"\n      ---- -> change\n\n    \"... and enables trailer headers in the future.\" ->\n    \"... and enabling headers in the trailer in the future.\"\n\n31) 19.6.3.2, Clarifications of the Specification\n    two instances of \"t-specials\" should be \"tspecials\".\n\n    \"Fix chunked transfer [en]coding to allow...\"\n       -- -> delete\n==================================\nNitty nits\n\n1) 10.2.7, 206 Partial Content\n    \"...indicating the desired range , and...\n        ^- delete\n\n2) 15.1.3, Encoding Sensitive Information in URL's\n    [last sentence]\n    \"Servers can use POST[-]based form submission instead[.]\"\n      ^- add  ^- add\n\n\n\n", "id": "lists-012-0208082"}, {"subject": "new issue: 206-CONTENTLENGTH[Was: comments on HTTP/1.1 Rev02 20Feb98", "content": "Dave Kristol writes:\n    2) 10.2.7, 206 Partial Content\n    \"... the Content-Length header field in the response MUST match...\"\n\n    Is there some reason why the entity couldn't be sent with chunked\n    transfer coding instead and *without* a Content-Length?\n\n    It wasn't clear to me, until sect. 19.6.3, that a server could\n    *limit* itself to sending these headers in response to If-Range.\n    Could we make that clearer?\n\nI probably should have caught this when I was cleaning up the other\nstuff related to Content-Length.\n\nProposed resolution:\n\nChange this:\n\n  .  Either a Content-Range header field (section 14.16) indicating the\n     range included with this response, or a multipart/byteranges\n     Content-Type including Content-Range fields for each part. If\n     multipart/byteranges is not used, the Content-Length header field\n     in the response MUST match the actual number of OCTETs transmitted\n     in the message-body.\n\nto this:\n\n  .  Either a Content-Range header field (section 14.16) indicating the\n     range included with this response, or a multipart/byteranges\n     Content-Type including Content-Range fields for each part. If\n|    a Content-Length header field is present in the response, its value\n|    MUST match the actual number of OCTETs transmitted\n     in the message-body.\n\n-Jeff\n\nP.S.: it's not clear to me why this says \"OCTETs\" instead of \"octets\".\n\"OCTET\" is a non-terminal in the BNF, but both forms are used in\nnon-BNF contexts in the current draft.\n\n\n\n", "id": "lists-012-0226284"}, {"subject": "Re: comments on HTTP/1.1 Rev02 20Feb98 (related to caching", "content": "Dave Kristol writes:\n\n    3) 13, Caching in HTTP\nLast paragraph:  I think something like the following needs to be\nadded at the end:\n\"In such cases, the design should also provide a way to inform the\nend user of a break in transparency.\"\n    \nAre you refering to this paragraph:\n   A basic principle is that it must be possible for the clients to detect\n   any potential relaxation of semantic transparency.\nor to the \"Note\" that follows it?  At any rate, just before the short\nparagraph I quoted, we already have:\n\n  3. Protocol features that allow a cache to attach warnings to\n     responses that do not preserve the requested approximation of\n     semantic transparency.\n\nWhat exactly are we missing here?\n\n    4) 13.11 Write-Through Mandatory\nSuppose I want to add a custom method to HTTP, one of whose\nside-effects would be to invalidate a cache entry.  (Suppose\nI were adding something comparable to DELETE, for example.)\nHow would the cache know it must invalidate the associated\nresource?  I don't think any of the Cache-Control directives\ncan tell the cache to discard an entry, can they?  (And if they\ncould, that would be an interesting denial of service attack.)\n    \nNote that 13.11 per se is not about invalidation (this is discussed in\nother places); it's about requiring the cache to forward \"all methods\nthat may be expected to cause modifications\" to the origin server.\nWhich is another way of saying \"if it's not GET or HEAD, a proxy must\nforward it.\"\n\nOne could certainly attack the intellectual basis for the\ndiscussions (elsewhere) about invalidation.  It's basically\nimpossible to do anything \"correct\", but we've added a few\nstop-gap measures so that in the places where we know what is\ngoing on, we can avoid obvious incoherencies.  A custom method\nis by definition not part of HTTP/1.1, so it's hard for us to\nspecify what it would do to a cache entry, but one could imagine\nimplementing a rule that \"if you are forwarding a method that\nyou don't understand, you should also invalidate any cache\nentries that might possibly be related to the Request-URI.\"\n\nThe possibility of a denial-of-service attack (really, a \"denial\nof cache performance\" attack) is not limited to this particular\ncase.  My own preference is that we can't engineer the protocol\nto prevent them, so the right thing to do is to detect them\n(at the proxy, by log analysis) and then call in the lawyers.\n\nAnyway, it seems unlikely that we could change the HTTP/1.1\nspec, at this late date, to do anything new w.r.t. cache\ninvalidation.\n\n-Jeff\n\n\n\n", "id": "lists-012-0234981"}, {"subject": "Re: comments on HTTP/1.1 Rev02 20Feb98 (related to caching", "content": "Jeffrey Mogul wrote:\n> \n> Dave Kristol writes:\n> \n>     3) 13, Caching in HTTP\n>         Last paragraph:  I think something like the following needs to be\n>         added at the end:\n>         \"In such cases, the design should also provide a way to inform the\n>         end user of a break in transparency.\"\n> \n> Are you refering to this paragraph:\n>    A basic principle is that it must be possible for the clients to detect\n>    any potential relaxation of semantic transparency.\n> or to the \"Note\" that follows it?  At any rate, just before the short\n> paragraph I quoted, we already have:\n\nThat latter.\n\n> \n>   3. Protocol features that allow a cache to attach warnings to\n>      responses that do not preserve the requested approximation of\n>      semantic transparency.\n> \n> What exactly are we missing here?\n\nMy marbles. :-)  The warnings should suffice.\n\n> \n>     4) 13.11 Write-Through Mandatory\n>         Suppose I want to add a custom method to HTTP, one of whose\n>         side-effects would be to invalidate a cache entry.  (Suppose\n>         I were adding something comparable to DELETE, for example.)\n>         How would the cache know it must invalidate the associated\n>         resource?  I don't think any of the Cache-Control directives\n>         can tell the cache to discard an entry, can they?  (And if they\n>         could, that would be an interesting denial of service attack.)\n> \n> Note that 13.11 per se is not about invalidation (this is discussed in\n> other places); it's about requiring the cache to forward \"all methods\n> that may be expected to cause modifications\" to the origin server.\n> Which is another way of saying \"if it's not GET or HEAD, a proxy must\n> forward it.\"\n> \n> One could certainly attack the intellectual basis for the\n> discussions (elsewhere) about invalidation.  It's basically\n> impossible to do anything \"correct\", but we've added a few\n> stop-gap measures so that in the places where we know what is\n> going on, we can avoid obvious incoherencies.  A custom method\n> is by definition not part of HTTP/1.1, so it's hard for us to\n> specify what it would do to a cache entry, but one could imagine\n> implementing a rule that \"if you are forwarding a method that\n> you don't understand, you should also invalidate any cache\n> entries that might possibly be related to the Request-URI.\"\n\nWould it make sense to add words to that effect to the spec.?  Or at\nleast advice to implementers?\n\n> [...]\n\nDave Kristol\n\n\n\n", "id": "lists-012-0245151"}, {"subject": "Re: comments on HTTP/1.1 Rev02 20Feb98 (related to caching", "content": "Jeff Mogul wrote:\n> One could certainly attack the intellectual basis for the\n> discussions (elsewhere) about invalidation.  It's basically\n> impossible to do anything \"correct\", but we've added a few\n> stop-gap measures so that in the places where we know what is\n> going on, we can avoid obvious incoherencies.  A custom method\n> is by definition not part of HTTP/1.1, so it's hard for us to\n> specify what it would do to a cache entry, but one could imagine\n> implementing a rule that \"if you are forwarding a method that\n> you don't understand, you should also invalidate any cache\n> entries that might possibly be related to the Request-URI.\"\n\nDMK wrote:\nWould it make sense to add words to that effect to the spec.?  Or at\nleast advice to implementers?\n\nJeff suggested I propose words.  Here goes.\n\nAdd at the end of 13.10, Invalidation After Updates or Deletes:\n\nA cache that passes through requests for methods it does not understand\nshould invalidate any entities referred to by the Request-URI.\n\nDave Kristol\n\n\n\n", "id": "lists-012-0255345"}, {"subject": "Test Day 1998-022", "content": "  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n\n", "id": "lists-012-0264231"}, {"subject": "I-D ACTION:draft-ietf-http-v11-spec-rev02.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: Hypertext Transfer Protocol -- HTTP/1.1\nAuthor(s): J. Mogul, T. Berners-Lee, L. Masinter, P. Leach, \n                          R. Fielding, H. Nielsen, J. Gettys\nFilename: draft-ietf-http-v11-spec-rev-02.txt\nPages: 155\nDate: 24-Feb-98\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol\nfor distributed, collaborative, hypermedia information systems. It is a\ngeneric, stateless, protocol which can be used for many tasks, such as\nname servers and distributed object management systems, through\nextension of its request methods. A feature of HTTP is the typing and\nnegotiation of data representation, allowing systems to be built\nindependently of the data being transferred.\n \nHTTP has been in use by the World-Wide Web global information initiative\nsince 1990. This specification defines the protocol referred to as\n''HTTP/1.1'', and is an update to RFC2068 [33].\n \nAt the time of this revision's submission, there were no known outstanding\ntechnical or editorial issues.  The HTTP/1.1 issues list, along with\nmany representations of this specification including Postscript, Microsoft\nword, with and without change bars, with or without gzip compression\ncan be found at http://www.w3.org/Protocols/HTTP/Issues/.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-v11-spec-rev-02.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-v11-spec-rev-02.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-v11-spec-rev-02.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-0272520"}, {"subject": "Why a chain", "content": "[I forget whether you're open to such questions now.  I presume someone will\ntell me if not.]\n\nI just fetched rev-02.  Let me ask a question that's been bothering me for\nquite a while now.  In section 1.4, HTTP is defined wrt a request/response pair\npropagating along a chain of intermediaries (not just proxies).  That seems too\nspecific in multiple ways; let me focus here on the chain part.  Why a chain?\nI understand the need for a distinction between hop-by-hop and end-to-end\nheaders; I'm just concerned that the definition of \"end-to-end\" is a bit too\nspecialized.  In particular, the restriction to the chain shape seems\nspurrious.  Surely there are reasons for constructing non-chain shapes.  And I\nsuspect the HTTP protocol doesn't need (or perhaps could easily be made to not\nneed) the restriction to chain shapes.\n\nLet me illustrate my question by asking for an analysis of the following\nsituation.  Consider a rendering server whose job is to accept GET requests for\narbitrary URIs and always respond with entities whose MIME type is some\ncompressed bitmap type suitable for quick blasting onto a TV screen.  When\nasked to GET a URI, this server acts as a client to GET both the requested URI\nand any inlined images, then renders them all together to create the compressed\nbitmap response.  This is a non-chain shape: a request into a server causes\nmultiple requests to go out of that server.  Among the questions I wonder about\nare: (1) is it fair to say this server is an \"HTTP/1.1 server\"?  (2) is this\nserver an \"HTTP/1.1 proxy\"?  (3) what happens with the Last-Modified headers in\nthe responses?  (4) whose job is it (HTTP/1.1's?  the intermediary's vendor's?)\nto define what happens with the Last-Modified headers in the responses?  (5)\nWhat is the \"origin server\" for the request submitted to this rendering server?\n\n\n\n", "id": "lists-012-0282264"}, {"subject": "Why gateways", "content": "Why does the HTTP/1.1 spec need to talk about gateways?  As a spec of protocol,\nnot server implementation, it goes without saying that a server implementation\ncan call on other servers via other protocols.  I note that the entry for\n\"gateway\" in the index (of rev-02) lists only one occurrence: the entry for\n\"gateway\" in the glossary.  (There is at least one more use of \"gateway\" in the\nspec: on page 12, where gateways are allowed in the chain of intermediaries.\nDo you really want to say as much as 1.4 says about the ways that these other\nprotocols can work?)\n\n\n\n", "id": "lists-012-0291298"}, {"subject": "Re: Why a chain", "content": "On Wed, 25 Feb 1998 Mike_Spreitzer.PARC@xerox.com wrote:\n\n> Let me illustrate my question by asking for an analysis of the following\n> situation.  Consider a rendering server whose job is to accept GET requests for\n> arbitrary URIs and always respond with entities whose MIME type is some\n> compressed bitmap type suitable for quick blasting onto a TV screen.  When\n> asked to GET a URI, this server acts as a client to GET both the requested URI\n> and any inlined images, then renders them all together to create the compressed\n> bitmap response.  This is a non-chain shape: a request into a server causes\n> multiple requests to go out of that server.  Among the questions I wonder about\n> are: (1) is it fair to say this server is an \"HTTP/1.1 server\"?\n\n  It certainly could be.  \n\n> (2) is this server an \"HTTP/1.1 proxy\"?  \n\n  No.  It is originating the response - I suppose that one could\ncharacterize this as an HTTP-to-HTTP (H2H) gateway if the sources of the\ncomposite image were obtained using HTTP, but that is essentially\nirrelevant - no other server has the response that is being sent to the\nuser agent, so it is not acting as a proxy for another server.\n\n> (3) what happens with the Last-Modified headers in the responses?  \n\n  If I were creating such a thing, I would set the cache validators (both\nL-M and Etag) such that they reflected the state of the composite; L-M to\nthe most recent L-M of the components, and the Etag some composite of the\nEtag values for the components.\n\n  \n\n\n\n", "id": "lists-012-0299038"}, {"subject": "Re: Why a chain", "content": "On Wed, 25 Feb 1998 Mike_Spreitzer.PARC@xerox.com wrote:\n\n> [I forget whether you're open to such questions now.  I presume someone will\n> tell me if not.]\n> \n> I just fetched rev-02.  Let me ask a question that's been bothering me for\n> quite a while now.  In section 1.4, HTTP is defined wrt a\n> request/response pair\n> propagating along a chain of intermediaries (not just proxies).  That\n> seems too\n> specific in multiple ways; let me focus here on the chain part.  Why a chain?\n> I understand the need for a distinction between hop-by-hop and end-to-end\n> headers; I'm just concerned that the definition of \"end-to-end\" is a bit too\n> specialized.  In particular, the restriction to the chain shape seems\n> spurrious.  Surely there are reasons for constructing non-chain shapes.\n> And I\n> suspect the HTTP protocol doesn't need (or perhaps could easily be made\n> to not\n> need) the restriction to chain shapes.\n> \n> Let me illustrate my question by asking for an analysis of the following\n> situation.  Consider a rendering server whose job is to accept GET\n> requests for\n> arbitrary URIs and always respond with entities whose MIME type is some\n> compressed bitmap type suitable for quick blasting onto a TV screen.  When\n> asked to GET a URI, this server acts as a client to GET both the\n> requested URI\n> and any inlined images, then renders them all together to create the\n> compressed\n> bitmap response.  This is a non-chain shape: a request into a server causes\n> multiple requests to go out of that server.  Among the questions I\n> wonder about\n> are: (1) is it fair to say this server is an \"HTTP/1.1 server\"?  (2) is this\n> server an \"HTTP/1.1 proxy\"?  (3) what happens with the Last-Modified\n> headers in\n> the responses?  (4) whose job is it (HTTP/1.1's?  the intermediary's\n> vendor's?)\n> to define what happens with the Last-Modified headers in the responses?  (5)\n> What is the \"origin server\" for the request submitted to this rendering\n> server?\n\nIt feels to me that this hypothetical program is neither an HTTP Server \n(though it has such characteristics) nor Proxy ... rather it is an HTTP\nclient or application. That it happens to also be an HTTP server in the\nvague sense that it uses the HTTP protocol to communicate with its \npartner in delivering the application is sort of moot in terms\nof analyzing how the HTTP specification applies.  The composite of\nthis application and its set-top partner forms what we have generally\nrefered to as a user-agent.\n\nLooking then to the HTTP protocol for guidance as to how to impelement\nthe application connection, I think Scott's suggestions match what I would\nhave suggested, assuming the desire to leave responsiblity for tracking\nchanges with the set-top end of the partnership.  Reality however is\nthat for the rendering half of the partner ship to correctly respond in\nthe network optimum fashion, it needs to check each of the pieces which\nformed the rendered composite with the origin servers, etc. The only\nreally useful thing the settop client could make use of would be a \ncomputed composite expires value which was the earliest of any of the\noriginal expires. This expires value would represent when it was\nnecessary for the settop part to ask the rendering part to check\nthe individual components. Knowledge which the rendering component\nhas about the composition of the object returned to the set-top part\nwould allow it to check the status of which ever components had\nactually expired.\n\nDave Morris\n\n\n\n", "id": "lists-012-0308026"}, {"subject": "Re: Why gateways", "content": "... and why are there gateways to servers but not gateways to clients?\n\n\n\n", "id": "lists-012-0319119"}, {"subject": "Re: Why gateways", "content": "On Wed, 25 Feb 1998 Mike_Spreitzer.PARC@xerox.com wrote:\n\n> ... and why are there gateways to servers but not gateways to clients?\n\nBecause with the HTTP protocol, clients initiate all communications.\nIf the client doesn't connect to the gateway, it can't be used.\n\nDave Morris\n\n\n\n", "id": "lists-012-0326453"}, {"subject": "Rev02: seek clarification on conditional header", "content": "I've been trying to figure out how to add If-Match (I-M) and\nIf-None-Match (I-N-M) to my server.  I'm getting a severe headache as I\nread and reread the relevant sections and try to understand the\ninteractions between them and If-Modified-Since (I-M-S) and\nIf-Unmodified-Since (I-U-S).  (I haven't even got to If-Range yet!)\n\nThe remarks below apply to a simple server that's trying to respond to\na GET.\n\n14.25 If-Match\n\n    If the request would, without the If-Match header field, result in\n    anything other than a 2xx status, then the If-Match header MUST be\n    ignored.\n\nThis paragraph implies to me that I should test I-M-S before I-M,\nbecause it's possible that I-M-S could produce a 304.  (Note that\nApache 1.2.5 does the I-M test first, on the theory that I-M is a more\naccurate test.  I'm using Apache as a reference because I'm assuming\nthat Roy had a strong hand in the code.  So I would expect it to reflect\nthe desired behavior.)\n\n14.26 If-None-Match\n\n    If any of the entity tags match the entity tag of the entity that would\n    have been returned in the response to a similar GET request (without the\n    If-None-Match header) on that resource, or if \"*\" is given and any\n    current entity exists for that resource, then the server MUST NOT\n    perform the requested method, unless required to do so because the\n    resource's modification date fails to match that supplied in an If-\n    Modified-Since header field in the request. Instead, if the request\n    method was GET or HEAD, the server SHOULD respond with a 304 (Not\n    Modified) response, including the cache-related entity-header fields\n    (particularly ETag) of one of the entities that matched. For all other\n    request methods, the server MUST respond with a status of 412\n    (Precondition Failed).\n\nSame remarks here.  Same remarks about Apache.\n\nThe problem is, if I test I-M-S first, then I-N-M, I can run afoul of this\npart of 14.26:\n\n    If none of the entity tags match, then the server MAY perform the\n    requested method as if the If-None-Match header field did not exist, but\n    MUST also ignore any If-Modified-Since header field(s) in the request.\n    That is, if no entity tags match, then the server MUST not return a 304\n    (Not Modified) response.\n\nAlso, I'm uncertain what the proper behavior is for I-N-M if an Etag\nmatch succeeds and there's no I-M-S, or if there's an I-M-S and the\nfile has been modified since the I-M-S date.  Return 412 (Precondition\nFailed)?  The response isn't well specified.\n\nI'm too confused to ask any further coherent questions, so I'll wait\nfor responses to these.\n\nDave Kristol\n\n\n\n", "id": "lists-012-0334253"}, {"subject": "Re: comments on HTTP/1.1 Rev02 20Feb9", "content": "I've found some more problems.\n\n13.3.3 Weak and Strong Validators\n\n    A cache or origin server receiving a cache-conditional request, other\n    than a full-body GET request, MUST use the strong comparison function to\n    evaluate the condition.\n\nThe term \"cache-conditional request\" is nowhere defined.\n\n14.25 If-Match\n\n    A server MUST use the strong comparison function (see section 3.11) to\n    compare the entity tags in If-Match.\n\nActually the \"strong comparison function\" is described in 13.3.3.\n\nDave Kristol\n\n\n\n", "id": "lists-012-0343985"}, {"subject": "Re: Why gateways", "content": "I'm asking why the thing that initiates communication has to do so over HTTP,\nwhereas the thing that ultimately serves it does not have to do so over HTTP.\n\n\n\n", "id": "lists-012-0352019"}, {"subject": "Re: Why gateways", "content": "I believe the original question was:\n   ... and why are there gateways to servers but not gateways to clients?\n\n\nOn Wed, 25 Feb 1998 Mike_Spreitzer.PARC@xerox.com wrote:\n\n> I'm asking why the thing that initiates communication has to do so over HTTP,\n> whereas the thing that ultimately serves it does not have to do so over HTTP.\n\nThe point of a gateway is that it allows an HTTP client to access a \nnon-HTTP service.\n\nTHis is the HTTP protocol. HTTP clients initiate the connection. That\na program which functions as an HTTP client also acts as the server for\nanother protocol would seem to be outside of the scope of the HTTP\nprotocol.  Likewise a client which uses some other protocol to a\ngateway which then issues HTTP requests on its behalf. This specification\ndeals with what the HTTP protocol is expected to do and the result of\nusing a gateway from the HTTP perspective.\n\nDave Morris\n\n\n\n", "id": "lists-012-0359862"}, {"subject": "new editorial issue CACHE_CONDITIONAL: [was: comments on Rev02", "content": "Dave Kristol writes:\n    13.3.3 Weak and Strong Validators\n\nA cache or origin server receiving a cache-conditional request,\nother than a full-body GET request, MUST use the strong\ncomparison function to evaluate the condition.\n\n    The term \"cache-conditional request\" is nowhere defined.\n    \nProbably the phrase should simply be \"conditional request\" (it\nappears several other places in the draft).  The term \"conditional\nrequest\" is informally defined in 13.3.\n\n-Jeff\n\n\n\n", "id": "lists-012-0368283"}, {"subject": "Re: Why gateways", "content": ">I'm asking why the thing that initiates communication has to do so over HTTP,\n>whereas the thing that ultimately serves it does not have to do so over HTTP.\n\nNeither \"has to do\" anything.  The introduction introduces a bunch of\nterms that are necessary for common communication about the HTTP\nenvironment and its participants.  There are many things that can\nbe abstracted together to form the \"user agent\", just as there are\nmany things that can be abstracted together to form an origin server.\nI included \"gateway\" specifically because people kept confusing\nsuch things with a proxy.\n\nI have a ton of ideas on how to rewrite the introduction to make the\nabstractions a little more clear, and another ton of ideas on how to\nreorganize the entire specification to make it readable, but have neither\nthe time nor the patience to restart the standards process.\n\nBTW, the chain of communication is the only form described because\nall other forms can be reduced to a chain for the purpose of discussing\nthe HTTP semantics.\n\n....Roy\n\n\n\n", "id": "lists-012-0375747"}, {"subject": "Re: Rev02: seek clarification on conditional header", "content": "[Warning: lengthy analysis follows.]\n\nDave Kristol writes:\n\n    14.25 If-Match\n    \nIf the request would, without the If-Match header field, result in\nanything other than a 2xx status, then the If-Match header MUST be\nignored.\n    \n    This paragraph implies to me that I should test I-M-S before I-M,\n    because it's possible that I-M-S could produce a 304.  (Note that\n    Apache 1.2.5 does the I-M test first, on the theory that I-M is a more\n    accurate test.  I'm using Apache as a reference because I'm assuming\n    that Roy had a strong hand in the code.  So I would expect it to reflect\n    the desired behavior.)\n    \nFirst of all, I think it's a mistake to refer to the Apache\nimplementation to try to figure out what your implementation should\ndo.  It's quite possible that Apache does the right thing; however,\nthe IESG expects us to document multiple \"independent\" implementations\nbased on the specification, and it seems to me that basing your\ndecision on how Apache did it subverts this goal.\n\nIf the spec is wrong or ambiguous, we need to fix it, but not\non the basis that any particular existing implementation has already\nchosen one option.\n\nI should also point out that this specific topic was discussed\non the HTTP-WG mailing list in mid-November (1997), resulting in\nsome changes that might not have appeared in a release of Apache\nthat came out in early January.  So that version of Apache wouldn't\nnecessarily agree with the specification anyway (although the Apache\npeople know better than I do what kind of schedule they followed).\n\nAnyway, the specific \"issue\" to look at is IMS_INM_MISMATCH,\nwhich cites as a \"fix\" my message:\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0117.html\nand a follow-up:\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0160.html\n\nLet me start by addressing your questions regarding If-None-Match,\nsince that is what we have already discussed and resolved (?):\n\n    14.26 If-None-Match\n    \nIf any of the entity tags match the entity tag of the entity\nthat would have been returned in the response to a similar GET\nrequest (without the If-None-Match header) on that resource, or\nif \"*\" is given and any current entity exists for that\nresource, then the server MUST NOT perform the requested\nmethod, unless required to do so because the resource's\nmodification date fails to match that supplied in an If-\nModified-Since header field in the request. Instead, if the\nrequest method was GET or HEAD, the server SHOULD respond with\na 304 (Not Modified) response, including the cache-related\nentity-header fields (particularly ETag) of one of the entities\nthat matched. For all other request methods, the server MUST\nrespond with a status of 412 (Precondition Failed).\n    \n    The problem is, if I test I-M-S first, then I-N-M, I can run afoul of this\n    part of 14.26:\n    \nIf none of the entity tags match, then the server MAY perform\nthe requested method as if the If-None-Match header field did\nnot exist, but MUST also ignore any If-Modified-Since header\nfield(s) in the request.  That is, if no entity tags match,\nthen the server MUST not return a 304 (Not Modified) response.\n    \nI'm not sure there is a contradiction.  The first paragraph you cite\nfrom 14.26 says (if I'm not confused by my own words!)\n\nif (any entity tags match) {\n    if (IMS-date != last-modified) {\nMUST perform method\n    }\n    else {\nMUST NOT perform method\nif (method == GET or HEAD)\n    return 304\nelse\n    return 412\n    }\n}\nelse {\n    unspecified behavior\n}\n\nThe second paragraph says, in effect\n\nif (no entity tags match) {\n    process request, ignoring any IMS header\n}\nelse {\n    unspecified behavior\n    MUST NOT return 304\n}\n\nThe composition of these two conditionals should be obvious ...\n\nDave asks:\n    Also, I'm uncertain what the proper behavior is for I-N-M if an Etag\n    match succeeds and there's no I-M-S, or if there's an I-M-S and the\n    file has been modified since the I-M-S date.  Return 412 (Precondition\n    Failed)?  The response isn't well specified.\n\nIt is possible to edit the first quoted paragraph, by deleting\ninapplicable clauses, to answer these two questions:\n\n    for I-N-M if an Etag match succeeds and there's no I-M-S, \n\n\"If any of the entity tags match ... the server MUST NOT perform\nthe requested method\" (since the \"unless\" clause in that sentence\nisn't triggered).  \"[If] the request method was GET or HEAD, the\nserver SHOULD respond with a 304 ... [for] all other methods,\n... respond with a status of 412.\"\n\n    if there's an I-M-S and the file has been modified since the I-M-S date.\n\n\"the server ... is required to do so [i.e., perform the method]\nbecause the resource's modification date fails to match that supplied\nin an If-Modified-Since header field in the request.\"\n\nI suspect the english here could be a bit clearer, but I don't\nthink it's actually ambiguous or contradictory.\n\nNow, back to If-Match.   You're basically asking what to do\nif you see this request:\n\nGET /foo.html HTTP/1.1\nHost: research.bell-labs.com\nIf-Match: \"xyzzy\"\nIf-Modified-Since: Wed, 25 Feb 98 14:51:40 GMT\n\nTo me, this looks like a meaningless (or at best, highly unlikely)\nrequest.  What it appears to mean is that the client only wants the\nentity-body of /foo.html if the resource HAS been changed since the\nlast-modified date of the client's current cache entry for /foo.html,\nAND if the (strong) entity tag HAS NOT changed.\n\nIn other words, I can't see much of a reason for combining\nIf-Match and IMS.\n\nIt might make more sense to combine If-Match and If-Unmodified-Since,\ne.g.:\n\nGET /foo.html HTTP/1.1\nHost: research.bell-labs.com\nIf-Match: \"xyzzy\"\nIf-Unmodified-Since: Wed, 25 Feb 98 14:51:40 GMT\n\nbut then I think the only question is whether this means that\nthe request is performed if all of the \"If-*\" conditions are\nmet, or if at least one of them is met.  The language in the\nspec for If-Match is:\n\n    If none of the entity tags match ..., the server MUST NOT perform\n    the requested method, and MUST return a 412 (Precondition Failed)\n    response. [...]\n\n    If the request would, without the If-Match header field, result in\n    anything other than a 2xx status, then the If-Match header MUST be\n    ignored.\n\nand the language for If-Unmodified-Since is:\n\n    If the requested variant has been modified since the specified\n    time, the server MUST NOT perform the requested operation, and MUST\n    return a 412 (Precondition Failed).\n\n    If the request normally (i.e., without the If-Unmodified-Since\n    header) would result in anything other than a 2xx status, the\n    If-Unmodified- Since header should be ignored.\n\nIt turns out that there *is* a contradition here: if none of\nthe entity tags match AND if the variant has been modified, then\neach header \"wants\" to cause us to return 412, but the rules\nfor each header individually says \"ignore me because my buddy\nwants us to return 412\".  So I think the phrase \"2xx status\"\nin both quotes above should be \"2xx or 412 status\".\n\nWith that fix applied (I'll submit this as an \"issue\"), I think the\nmeaning of a request with both IM and IUS is straightforward:\n\nif ((no entity tags match the If-Match value)\nor\n    (last-modified date not same as IUS value)) {\n    return 412\n}\nelse {\n    process request, ignoring If-Match and If-Unmodified-Since hdrs\n}\n\nClear?\n\n-Jeff\n\n\n\n", "id": "lists-012-0383559"}, {"subject": "new editorial issue IM_IUS_412 [Was: seek clarification on conditional hdrs", "content": "[This is an excerpt of the relevant parts of\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0408.html\nwith some slight rephrasings to make it self-contained, but does\ncontain any new material.]\n\nIn this example\n\nGET /foo.html HTTP/1.1\nHost: research.bell-labs.com\nIf-Match: \"xyzzy\"\nIf-Unmodified-Since: Wed, 25 Feb 98 14:51:40 GMT\n\nthere is a question as to whether this means that the request is\nperformed if all of the \"If-*\" conditions are met, or if at least one\nof them is met.  The language in the spec for If-Match is:\n\n    If none of the entity tags match ..., the server MUST NOT perform\n    the requested method, and MUST return a 412 (Precondition Failed)\n    response. [...]\n\n    If the request would, without the If-Match header field, result in\n    anything other than a 2xx status, then the If-Match header MUST be\n    ignored.\n\nand the language for If-Unmodified-Since is:\n\n    If the requested variant has been modified since the specified\n    time, the server MUST NOT perform the requested operation, and MUST\n    return a 412 (Precondition Failed).\n\n    If the request normally (i.e., without the If-Unmodified-Since\n    header) would result in anything other than a 2xx status, the\n    If-Unmodified-Since header should be ignored.\n\nIt turns out that there is a contradiction here: if none of\nthe entity tags match AND if the variant has been modified, then\neach header \"wants\" to cause us to return 412, but the rules\nfor each header individually says \"ignore me because my buddy\nwants us to return 412\".  So I think the phrase \"2xx status\"\nin both quotes above should be \"2xx or 412 status\".\n\nWith this contradiction resolved, the meaning of the example is clear:\nthe method is performed if at least one entity tag from the If-Match\nheader field matches the current entity tag, AND if the resource's\nmodification date matches the value of the If-Unmodified-Since header\nfield.\n\nSpecifically, in rev-02:\n\n(1) In section 14.25 (If-Match), change:\n\n    If the request would, without the If-Match header field, result in\n    anything other than a 2xx status, then the If-Match header MUST be\n    ignored.\n    \nto\n\n    If the request would, without the If-Match header field, result in\n    anything other than a 2xx or 412 status, then the If-Match header\n    MUST be ignored.\n\n(2) In section 14.28 (If-Unmodified-Since), change:\n\n    If the request normally (i.e., without the If-Unmodified-Since\n    header) would result in anything other than a 2xx status, the\n    If-Unmodified-Since header should be ignored.\n    \nto\n\n    If the request normally (i.e., without the If-Unmodified-Since\n    header) would result in anything other than a 2xx or 412 status,\n    the If-Unmodified-Since header should be ignored.\n\n-Jeff\n\n\n\n", "id": "lists-012-0398273"}, {"subject": "Re: Rev02: seek clarification on conditional header", "content": "[More lengthy commentary follows.]\n\nMy thanks to Jeff Mogul for his clarifications.  I won't copy the whole\ndiscourse here, but I do have follow-up comments.\n\nDespite the IESG's need for independent implementations, I resorted to\nlooking at the Apache code in the hope that that code would help me to\nunderstand what the spec. was trying to say.  It didn't help.\n\nThe reasons I didn't follow the tangled discussions in November on this\nissue were multiple.  The most obvious is that, with something this\ntangled, it's hard to follow in the abstract.  Now that I'm actually\ntrying to implement, my interest is higher, and my attention is\ngreater.\n\nJeff's comments, taken piece by piece, make sense.  While I'm not the\nsharpest person in HTTP-WG, I think I should have been able to follow\nthe spec. and I couldn't.  I'm concerned that my difficulty suggests it\nwill be far too easy for others to \"get it wrong\" too, which is the\nantithesis of what the spec. is meant to do.  I commend Jeff's little\nsnippets of pseudo-code, and I wonder whether they aren't a whole lot\nmore helpful that the paragraphs of confusing words.  Maybe they should\nbe in the spec.  (And, see below.)\n\nThe overall problem I have with Jeff's explanation is that it assumes a\nreasonable client.  But a good implementation has to deal with an\narbitrary combination of headers, even those that seem to make no\nsense.  Of course the spec. itself should also be consistent even in\nthe face of \"unreasonable\" combinations of headers.  So I find myself\ntrying to deal with all combinations of I-M-S, I-U-S, I-M, and I-N-M.\n\nArchitecturally I want to have a single routine to handle all the\nconditional headers.  At the very least it would have to look something\nlike\nif (header1 exists)\n    process header1\nif (header2 exists)\n    process header2\n...\n\nThose might be \"else if\", but you get the idea.  The \"process\" step\nmight end with exiting the routine.  So the next question is, which\nheader is header1, which is header2, etc.?\n\nLet me try the following pseudo-code on the group.  Do you think it\nreflects the spec. (as revised by Jeff's remarks about 412)?\n\n=============\n    /* Test conditional headers. */\n\n    suppress-IMS = 0;\n\n    /* If-None-Match */\n    if (I-N-M is present) {\nif (any entity tags match) {\n    if (IMS-date == last-modified) {\n/* MUST NOT perform method */\nif (method == GET or HEAD)\n    return 304\nelse\n    return 412\n}\n    }\n    /* Note: fall through to test other conditionals. */\n}\nelse {\n    /* no tags match; must suppress 304 (Not Modified) */\n    suppress-IMS = 1;\n}\n    }\n\n    /* If-Modified-Since */\n    if (   suppress-IMS == 0\n&& I-M-S is present\n&& method == GET\n&& IMS-date != last-modified)\nreturn 304\n\n    /* If-Match */\n    if (I-M is present && no entity tags match)\nreturn 412\n\n    /* If-Unmodified-Since */\n    if (I-U-S is present && IUS-date != last-modified)\nreturn 412\n\n    return process method\n================\n\nOne nit from 14.26 If-None-Match\n       Instead, if the request\n    method was GET or HEAD, the server SHOULD respond with a 304 (Not\n    Modified) response, including the cache-related entity-header fields\n    (particularly ETag) of one of the entities that matched.\n\nThe description of If-Modified-Since says it applies to GET, but when\nit's used in conjunction with If-None-Match it evidently also applies\nto HEAD.  Is that what is intended?  Should something in that regard\nbe said in the I-M-S description?\n\nDave Kristol\n\n\n\n", "id": "lists-012-0407753"}, {"subject": "Streaming Excel &amp; Word Files to I", "content": "Hello,\nWe have a cgi program that streams documents across the web using HTTP 1.0. \n When we stream an HTTP Document to Internet Explorer 3.02 that has a \ncontent type of application/msword or application/vnd.ms-excel the plugins \nfor Word and Excel both try to load the URL a second time after it has been \nstreamed to the browser.  In essence the document is sent twice.\n\nDoes anyone know of a solution or reason for this behavior?  We have one \nmachine that this does NOT happen on, so there seems that there may be a \nway to configure IE or Word to not call the URL twice.\n\nThanks in advance.\n\nMark\n\n\n\n", "id": "lists-012-0418501"}, {"subject": "Re: Age calculations in HTTP 1.1 11/21/97 documen", "content": "    Looking at section 13.2.3, I found that I could only make sense of\n    the formulas if I switched the definition of apparent_age from\n    \n    apparent_age = max(0,response_time - date_value);\n    \n    to\n    \n    apparent_age = max(0,request_time - date_value);\n    \n    For example, if in year\n    \n    1981, the user sent a request\n    1989, date_value\n    1990, request from the cache to the server\n    2000, resonse_time\n    2001, now\n    \n    then,\n    apparent_age = 1 year\n    response_delay = 10 years\n    corrected_initial_age = 11 years\n    resident_time = 1 year\n    and,\n    current_age = 12 years. This seems to make sense.\n    \n    But with the official definitions,\n    apparent_age = 11 years\n    response_delay = 10 years\n    corrected_initial_age = 21 years\n    resident_time = 1 year\n    and\n    current_age = 22 years. This brings us back to 1979, and doesn't make\n    much sense.\n\nIt's misleading to try to discuss this in terms of years, since\nit's highly unlikely that the difference between request_time\nand response_time would be larger than a few minutes at most.\n\nAnyway, the problem we're trying to solve here is that in this\ntimeline\n\nrequest_time        date_value          response_time\n      |                |                       |\n------|----------------|-----------------------|------->\n    \n\nit really makes no sense to compute\n\n    apparent_age = max(0, request_time - date_value);\n\nbecause if the clocks are synchronized, then (with non-zero\nspeed-of-light delay) the value (request_time - date_value)\nis always negative, and so the max(0, request_time - date_value)\nwould always be zero.\n\nThe fact that the response_delay is counted twice in the\nfinal result is an attempt to correct for clock skew,\nsince if you can't guarantee that the client and server\nclocks are synchronized (and we can't!), then the apparent_age\ncould be wrong by a significant amount.  (Trace studies show\nthat a lot of the clocks in HTTP servers are set wrong, and\nwe suspect that the situation for clients could be even\nworse.)  Adding in the response_delay helps to avoid underestimating\nthe Age of a response in the presence of clock skew.\n\nThe actual magnitude of the error is normally quite small,\nsince response_delay ought to be on the order of several\nseconds, or several minutes at most.\n\n-Jeff\n\n\n\n", "id": "lists-012-0425671"}, {"subject": "Re: Age calculations in HTTP 1.1 11/21/97 documen", "content": "In the course of my testing, I've found that those proxies (the few we've\nhad to play with) which attempt to adjust for clock skew generally get it\nwrong, especially when there is no Date header (noclock11.agranat.com).\n\nI don't want to restart the debate, but if it were me I'd just record my\nlocal time of reception, and construct my outgoing Age header based on how\nlong I'd held it plus any Age that I received.  Forget about trying for\nany absolute truth - that's for philosophers - go for Good Enough.\n\n\n\n", "id": "lists-012-0435794"}, {"subject": "Re: 505 response a MUST", "content": "I'll go for MUST 505. Any counter-opinions?\n\n\n-----Original Message-----\nFrom: Scott Lawrence <lawrence@agranat.com>\nTo: HTTP Working Group <http-wg@cuckoo.hpl.hp.com>\nCc: jg@w3.org <jg@w3.org>\nDate: Wednesday, February 18, 1998 9:24 AM\nSubject: 505 response a MUST?\n\n\n>\n>  Another issue found while trying to document supported features\n>  (there appears to be some value to this rather tedious exercise :)\n>\n>  draft-ietf-http-v11-spec-rev-01 defines:\n>\n>    10.5.6 505 HTTP Version Not Supported\n>\n>    The server does not support, or refuses to support, the HTTP protocol\n>    version that was used in the request message. The server is indicating\n>    that it is unable or unwilling to complete the request using the same\n>    major version as the client, as described in section 3.1, other than\n>    with this error message. The response SHOULD contain an entity\n>    describing why that version is not supported and what other protocols\n>    are supported by that server.\n>\n>  ... and section 3.1 spells out various rules about version number\n>  usage, but does not specify that a server MUST send a 505 response\n>  if it receives a major version number higher than the highest\n>  version it implements.\n>\n>  I've tried a few of the servers out there, and they all return\n>  success when I send HTTP/2.0 requests (which were 1.1 requests with\n>  2.0 labels).\n>\n>--\n>Scott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\n>Agranat Systems, Inc.        Engineering            http://www.agranat.com/\n>\n>\n\n\n\n", "id": "lists-012-0443663"}, {"subject": "Re: 505 response a MUST", "content": ">>  ... and section 3.1 spells out various rules about version number\n>>  usage, but does not specify that a server MUST send a 505 response\n>>  if it receives a major version number higher than the highest\n>>  version it implements.\n\nThat isn't why 505 was created.  It allows a future server to deny\nservice to older protocols.  Since we cannot know whether HTTP/2.0\nis incompatible with HTTP/1.1 (the major version change does not imply\nincompatibility, it just removes the requirement for compatibility),\nit is inappropriate for a server to be required to respond in error\nto a message it might be able to respond to normally.  That's why it\nis not a MUST, and why Apache responds normally to an HTTP-version of\nHTTP/2.0 if it can interpret the request as an HTTP/1.1 server.\n\n....Roy\n\n\n\n", "id": "lists-012-0454300"}, {"subject": "looking for a browser that pipelines request", "content": "I am looking for a Web browser that pipelines multiple requests over\na persistent connection. The browsers that I have checked (Netscape\nNavigator and Communicator, and W3C Amaya) do not pipeline requests\neven when using a persistent connection (at least that's what appears\nto be the case based on a tcpdump trace).\n\nI would appreciate any relevant information/pointers.\n\nThanks.\n\n-Venkat\n\n\n\n", "id": "lists-012-0462824"}, {"subject": "Re: 505 response a MUST", "content": "I wrote:\n\n> >>  ... and section 3.1 spells out various rules about version number\n> >>  usage, but does not specify that a server MUST send a 505 response\n> >>  if it receives a major version number higher than the highest\n> >>  version it implements.\n\nOn Fri, 27 Feb 1998, Roy T. Fielding wrote:\n\n> That isn't why 505 was created.  It allows a future server to deny\n> service to older protocols.  Since we cannot know whether HTTP/2.0\n> is incompatible with HTTP/1.1 (the major version change does not imply\n> incompatibility, it just removes the requirement for compatibility),\n> it is inappropriate for a server to be required to respond in error\n> to a message it might be able to respond to normally.  That's why it\n> is not a MUST, and why Apache responds normally to an HTTP-version of\n> HTTP/2.0 if it can interpret the request as an HTTP/1.1 server.\n\n  That logic does not seem sound to me - if changing the major version\nnumber means that the protocol _may_ be not backward compatible, then a\nrecipient that does not implement 2.0 cannot know whether or not it can\n_correctly_ interpret the message as a 1.1 message.  Your logic assumes\nthat the authors of 2.0 will not make any semantic changes to existing\nprotocol elements.\n\n  The well-known problems with intermediate systems altering the response\nversion also, I think, argue in favor of the more restrictive usage.\n\n\n\n", "id": "lists-012-0470969"}, {"subject": "Re: 505 response a MUST", "content": "At 10:18 3/1/98 -0500, Scott Lawrence wrote:\n>  That logic does not seem sound to me - if changing the major version\n>number means that the protocol _may_ be not backward compatible, then a\n>recipient that does not implement 2.0 cannot know whether or not it can\n>_correctly_ interpret the message as a 1.1 message.  Your logic assumes\n>that the authors of 2.0 will not make any semantic changes to existing\n>protocol elements.\n\nYes indeed - this is quite important for something like Mandatory, if we\nwant ever to be able to not use it without the method name fix. If there is\nany use of the version number at all then an application must assume that\nit can't understand a major version higher than what it knows about.\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-0480872"}, {"subject": "Re: looking for a browser that pipelines request", "content": "At 21:40 2/28/98 -0800, Venkat Padmanabhan wrote:\n>\n>I am looking for a Web browser that pipelines multiple requests over\n>a persistent connection. The browsers that I have checked (Netscape\n>Navigator and Communicator, and W3C Amaya) do not pipeline requests\n>even when using a persistent connection (at least that's what appears\n>to be the case based on a tcpdump trace).\n\nJose Kahan is currently integrating the libwww pipelining code into Amaya\nbut until then you can try out the libwww robot [1] which does the trick\npretty well. This is the application that we have used for the HTTP/1.1\nperformance paper [2].\n\nHenrik\n\n[1] http://www.w3.org/Robot/\n[2] http://www.w3.org/Protocols/HTTP/Performance/Pipeline.html\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-0490466"}, {"subject": "Re: 505 response a MUST", "content": ">  That logic does not seem sound to me - if changing the major version\n>number means that the protocol _may_ be not backward compatible, then a\n>recipient that does not implement 2.0 cannot know whether or not it can\n>_correctly_ interpret the message as a 1.1 message.  Your logic assumes\n>that the authors of 2.0 will not make any semantic changes to existing\n>protocol elements.\n\nNo, it assumes a client won't send a 2.0 message to a 1.x server unless\nthe 2.0 message is sufficiently semantically and syntactically compatible\nsuch that there is some reasonable assurance that the 1.x server will\nrespond in an anticipated (and safe) manner, even if that means a 400\nresponse.  Forcing a server to respond in error just for the sake of\nan error only guarantees the worst-case behavior of an extra round-trip,\nand offers no extra safety in any case because deployed HTTP/1.x servers\nwill never respond with 505.  That code is intended for future 2.x\nservers to say \"piss-off\" to older clients in the format of an HTTP/1.1\nmessage, since they will have to respond in the same major version as\nthe old client.\n\n....Roy\n\n\n\n", "id": "lists-012-0499188"}, {"subject": "Re: 505 response a MUST", "content": ">Yes indeed - this is quite important for something like Mandatory, if we\n>want ever to be able to not use it without the method name fix. If there is\n>any use of the version number at all then an application must assume that\n>it can't understand a major version higher than what it knows about.\n\nBut Henrik, both are demonstrably false.  The \"method name fix\" does not work\nwith existing proxies, since many existing proxies will forward an unknown\nmethod name even if they do not understand it.  Requiring that servers\npuke on each new major version number does not work because HTTP/1.x\nservers do not puke on each new major version number, nor should they\nsince the decision of when and why to bump the major version number is\nessentially a political one.\n\nIn order for Mandatory to work, you must find a mechanism that works with\nexisting systems, and the only one that works right now is to probe the\nconnection with an OPTIONS request.  We can't standardize on something\nthat is based on a prerequisite contrary to reality.\n\n....Roy\n\n\n\n", "id": "lists-012-0508300"}, {"subject": "Re: 505 response a MUST", "content": "At 22:44 3/1/98 -0800, Roy T. Fielding wrote:\n\n>But Henrik, both are demonstrably false.  The \"method name fix\" does not work\n>with existing proxies, since many existing proxies will forward an unknown\n>method name even if they do not understand it.\n\nRoy,\n\nI think we have different ideas of what Mandatory is supposed to do: The\nsemantics of a mandatory extension as written in the current draft [1] is\nthat the \"ultimate recipient\" is required to deal with it, not necessarily\nany intermediate proxy. Therefore, tunnelling is less of a problem for at\nleast end-to-end mandatory extensions. Hop-by-hop extensions can be dealt\nwith as described below.\n\nIn both situations the correct answer is \"some do and some don't\" leaving\nthe following questions:\n\n1) Do we want a \"right answer\" (do or don't) for new apps?\n2) Can Mandatory work with both the do's and the don'ts?\n\nFor Mandatory, 1) is strictly a question of esthetics and whether we want\nto integrate something like Mandatory into new versions of HTTP or not. New\napplications (and new HTTP revisions) that do not do Mandatory by\ndefinition fall into the 2) category and hence it should be possible to\nlive without it (although acceptable, I do not particularly like this).\n\nThe more interesting (essential, in fact) question is whether Mandatory can\nbe made to work with both the do's and the don'ts. This should be possible\nto check out by looking at the possible outcomes from a non-mandatory-aware\nproxy:\n\n                               HTTP Proxy Server\n     Scope   |          Hop-by-hop          |        End-to-end\n    Strength | Optional   |   Required      | Optional   |   Required\n -----------------------------------------------------------------------\n Intended    | Strip      | 501 or tunnel   | Forward    | 501 or tunnel\n       \n Behavior    | extension  | extension       | extension  | extension\n             |            |                 |            | \n Unintended  | Forward    | Forward         | Strip      | Strip\n Behavior    | extension  | extension       | extension  | extension\n\nOptional hop-by-hop extensions do not require any method name change, so\nthis only involves the Connection header. HTTP/1.1 already have\nrestrictions on how to deal with the interactions between Connection\nheaders and HTTP/1.0 applications and they apply to Mandatory as well.\n\nMandatory hop-by-hop extensions do require a method name change as well as\nthe Connection header field. If the request unintentionally is forwarded\ninstead of properly processed or tunnelled then the M- method name prefix\nwould be all that would be left in the forwarded request forcing the origin\nserver to respond with a 501.\n\nIf all the mandatory extension headers of an end-to-end extension were\nstripped by a proxy then this would be the same situation with a lonely M-\nmethod name that can not be correctly dealt with by any origin server.\n\n>Requiring that servers\n>puke on each new major version number does not work because HTTP/1.x\n>servers do not puke on each new major version number, nor should they\n>since the decision of when and why to bump the major version number is\n>essentially a political one.\n>\n>In order for Mandatory to work, you must find a mechanism that works with\n>existing systems, and the only one that works right now is to probe the\n>connection with an OPTIONS request.\n\nNo, the OPTIONS method has exactly the same problems as any M- method for\nall the existing proxies that pass through unknown methods.\n\nHowever, the main reason why OPTIONS can't substitute the method name hack\nis that an application has no guarantee that two requests will travel over\nthe same message path, nor that they will be handled by the same instance\nof the origin server, nor that the information contained in an OPTIONS\nresponse is still valid for subsequent requests.\n\nThe information returned in a OPTIONS response can be informational only\nand hence is complementary to the Mandatory spec.\n\n>We can't standardize on something\n>that is based on a prerequisite contrary to reality.\n\nI don't think we disagree on that but I also think that the Mandatory spec\nis sound wrt weird, real-life proxies.\n\nHenrik\n\nPS: I think this discussion is more appropriate on the\n<ietf-http-ext@w3.org>  mailing list, so I cc that list. Please continue\nthere!\n\n[1]\n\"http://www.w3.org/Protocols/HTTP/ietf-http-ext/draft-frystyk-http-mandatory\"\n\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-0516579"}, {"subject": "Re: looking for a browser that pipelines request", "content": "> I am looking for a Web browser that pipelines multiple requests over\n> a persistent connection. The browsers that I have checked (Netscape\n> Navigator and Communicator, and W3C Amaya) do not pipeline requests\n> even when using a persistent connection (at least that's what appears\n> to be the case based on a tcpdump trace).\n> \n> I would appreciate any relevant information/pointers.\n\nSince MSIE 4.x does HTTP/1.1, that is one place to look at. Also,\nHotJava in combination with HTTPClient will also pipeline. Pointers\nfor the later combination:\n\nhttp://www.javasoft.com/products/hotjava/1.1.2/index.html\nhttp://www.innovation.ch/java/HTTPClient/\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-0528475"}, {"subject": "Re: looking for a browser that pipelines request", "content": "On Tue, 3 Mar 1998, Life is hard... and then you die. wrote:\n\n> \n> > I am looking for a Web browser that pipelines multiple requests over\n> > a persistent connection. The browsers that I have checked (Netscape\n> > Navigator and Communicator, and W3C Amaya) do not pipeline requests\n> > even when using a persistent connection (at least that's what appears\n> > to be the case based on a tcpdump trace).\n> > \n> > I would appreciate any relevant information/pointers.\n> \n> Since MSIE 4.x does HTTP/1.1, that is one place to look at. Also,\n\nHas anyone verified if MSIE 4.x actually pipelines when talking to a 1.1\nserver?\n\nHTTP/1.1 clients don't have to.\n\n\n\n", "id": "lists-012-0536783"}, {"subject": "Re: looking for a browser that pipelines request", "content": "On Tue, 3 Mar 1998, Marc Slemko wrote:\n\n> Has anyone verified if MSIE 4.x actually pipelines when talking to a 1.1\n> server?\n\nYes, it does, and the performance difference is quite noticable even on a\nlocal connection.\n\n\n\n", "id": "lists-012-0545452"}, {"subject": "RE: looking for a browser that pipelines request", "content": "IE 4.x does not pipeline requests.\nYaron\n\n> -----Original Message-----\n> From:Scott Lawrence [SMTP:lawrence@agranat.com]\n> Sent:Tuesday, March 03, 1998 5:38 AM\n> To:Marc Slemko\n> Cc:HTTP-WG@cuckoo.hpl.hp.com\n> Subject:Re: looking for a browser that pipelines requests\n> \n> \n> \n> On Tue, 3 Mar 1998, Marc Slemko wrote:\n> \n> > Has anyone verified if MSIE 4.x actually pipelines when talking to a 1.1\n> > server?\n> \n> Yes, it does, and the performance difference is quite noticable even on a\n> local connection.\n> \n\n\n\n", "id": "lists-012-0553371"}, {"subject": "RE: looking for a browser that pipelines request", "content": "I just watched it do two GETs on one connection:\n\nRequests:\nGET / HTTP/1.1\nAccept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, */*\nAccept-Language: en-us\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\nHost: localhost\nConnection: Keep-Alive\n\nGET /Admin/lgmast.gif HTTP/1.1\nAccept: */*\nReferer: http://localhost/\nAccept-Language: en-us\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\nHost: localhost\nConnection: Keep-Alive\n\nResponses:\nHTTP/1.1 200 Document follows\nServer: Domino-Go-Webserver/4.6\nDate: Tue, 03 Mar 1998 22:35:05 GMT\nConnection: Keep-Alive\nAccept-Ranges: bytes\nVary: *\nContent-Location: Frntpage.html\nContent-Type: text/html\nContent-Length: 2414\nLast-Modified: Mon, 11 Aug 1997 17:39:59 GMT\n\n...\n\n\nHTTP/1.1 200 Document follows\nServer: Domino-Go-Webserver/4.6\nDate: Tue, 03 Mar 1998 22:35:06 GMT\nConnection: Keep-Alive\nAccept-Ranges: bytes\nContent-Type: image/gif\nContent-Length: 14865\nLast-Modified: Mon, 11 Aug 1997 17:40:17 GMT\n\n...\n\n\nThere were three GETs altogether, and one went by itself on a second connection.\nI haven't seen MSIE put more than two requests on a connection.\n\nCarl Kugler\n\n\n\nyarong@microsoft.com on 03/03/98 02:44:23 PM\nPlease respond to yarong@microsoft.com @ internet\nTo: marcs@znep.com @ internet, lawrence@agranat.com @ internet\ncc: HTTP-WG@cuckoo.hpl.hp.com @ internet\nSubject: RE: looking for a browser that pipelines requests\n\n\nIE 4.x does not pipeline requests.\n Yaron\n\n> -----Original Message-----\n> From: Scott Lawrence [SMTP:lawrence@agranat.com]\n> Sent: Tuesday, March 03, 1998 5:38 AM\n> To: Marc Slemko\n> Cc: HTTP-WG@cuckoo.hpl.hp.com\n> Subject: Re: looking for a browser that pipelines requests\n>\n>\n>\n> On Tue, 3 Mar 1998, Marc Slemko wrote:\n>\n> > Has anyone verified if MSIE 4.x actually pipelines when talking to a 1.1\n> > server?\n>\n> Yes, it does, and the performance difference is quite noticable even on a\n> local connection.\n>\n\n\n\n", "id": "lists-012-0562664"}, {"subject": "RE: looking for a browser that pipelines request", "content": "Pipelining != persistant connection ... are you saying that the\nsecond get was sent before the first response was completed? Or that\none connection was used for two gets in sequence with the second GET\non the connection issued after the first response was received?\n\nI can't speak to pipelining, but I recently had to watch IE's use\nof connections 'externally' (I watched current connection stats, not\na wire trace) and performance wise it looked like IE was using \nconnections > 2 times based on knowing the total number of objects\nretrieved vs. current connections and netstat and perfmon stats.\n\nDave Morris\n\nOn Tue, 3 Mar 1998, Carl Kugler wrote:\n\n> I just watched it do two GETs on one connection:\n> \n> Requests:\n> GET / HTTP/1.1\n> Accept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, */*\n> Accept-Language: en-us\n> Accept-Encoding: gzip, deflate\n> User-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\n> Host: localhost\n> Connection: Keep-Alive\n> \n> GET /Admin/lgmast.gif HTTP/1.1\n> Accept: */*\n> Referer: http://localhost/\n> Accept-Language: en-us\n> Accept-Encoding: gzip, deflate\n> User-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\n> Host: localhost\n> Connection: Keep-Alive\n> \n> Responses:\n> HTTP/1.1 200 Document follows\n> Server: Domino-Go-Webserver/4.6\n> Date: Tue, 03 Mar 1998 22:35:05 GMT\n> Connection: Keep-Alive\n> Accept-Ranges: bytes\n> Vary: *\n> Content-Location: Frntpage.html\n> Content-Type: text/html\n> Content-Length: 2414\n> Last-Modified: Mon, 11 Aug 1997 17:39:59 GMT\n> \n> ...\n> \n> \n> HTTP/1.1 200 Document follows\n> Server: Domino-Go-Webserver/4.6\n> Date: Tue, 03 Mar 1998 22:35:06 GMT\n> Connection: Keep-Alive\n> Accept-Ranges: bytes\n> Content-Type: image/gif\n> Content-Length: 14865\n> Last-Modified: Mon, 11 Aug 1997 17:40:17 GMT\n> \n> ...\n> \n> \n> There were three GETs altogether, and one went by itself on a second connection.\n> I haven't seen MSIE put more than two requests on a connection.\n> \n> Carl Kugler\n> \n> \n> \n> yarong@microsoft.com on 03/03/98 02:44:23 PM\n> Please respond to yarong@microsoft.com @ internet\n> To: marcs@znep.com @ internet, lawrence@agranat.com @ internet\n> cc: HTTP-WG@cuckoo.hpl.hp.com @ internet\n> Subject: RE: looking for a browser that pipelines requests\n> \n> \n> IE 4.x does not pipeline requests.\n>  Yaron\n> \n> > -----Original Message-----\n> > From: Scott Lawrence [SMTP:lawrence@agranat.com]\n> > Sent: Tuesday, March 03, 1998 5:38 AM\n> > To: Marc Slemko\n> > Cc: HTTP-WG@cuckoo.hpl.hp.com\n> > Subject: Re: looking for a browser that pipelines requests\n> >\n> >\n> >\n> > On Tue, 3 Mar 1998, Marc Slemko wrote:\n> >\n> > > Has anyone verified if MSIE 4.x actually pipelines when talking to a 1.1\n> > > server?\n> >\n> > Yes, it does, and the performance difference is quite noticable even on a\n> > local connection.\n> >\n> \n> \n> \n> \n> \n> \n\n\n\n", "id": "lists-012-0574521"}, {"subject": "RE: looking for a browser that pipelines request", "content": "> Pipelining != persistant connection ...\n\nOops, I forgot that distinction.\n\n> are you saying that the\n> second get was sent before the first response was completed? Or that\n> one connection was used for two gets in sequence with the second GET\n> on the connection issued after the first response was received?\n\nI really can't tell with the crude trace tool I'm using.\n\n  -Carl\n\n\n\ndwm@xpasc.com on 03/03/98 04:26:19 PM\nPlease respond to dwm@xpasc.com @ internet\nTo: Carl Kugler/Boulder/IBM@ibmus\ncc: http-wg@hplb.hpl.hp.com @ internet, marcs@znep.com @ internet,\nlawrence@agranat.com @ internet, HTTP-WG@cuckoo.hpl.hp.com @ internet,\nyarong@microsoft.com @ internet\nSubject: RE: looking for a browser that pipelines requests\n\n\n\nPipelining != persistant connection ... are you saying that the\nsecond get was sent before the first response was completed? Or that\none connection was used for two gets in sequence with the second GET\non the connection issued after the first response was received?\n\nI can't speak to pipelining, but I recently had to watch IE's use\nof connections 'externally' (I watched current connection stats, not\na wire trace) and performance wise it looked like IE was using\nconnections > 2 times based on knowing the total number of objects\nretrieved vs. current connections and netstat and perfmon stats.\n\nDave Morris\n\nOn Tue, 3 Mar 1998, Carl Kugler wrote:\n\n> I just watched it do two GETs on one connection:\n>\n> Requests:\n> GET / HTTP/1.1\n> Accept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, */*\n> Accept-Language: en-us\n> Accept-Encoding: gzip, deflate\n> User-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\n> Host: localhost\n> Connection: Keep-Alive\n>\n> GET /Admin/lgmast.gif HTTP/1.1\n> Accept: */*\n> Referer: http://localhost/\n> Accept-Language: en-us\n> Accept-Encoding: gzip, deflate\n> User-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\n> Host: localhost\n> Connection: Keep-Alive\n>\n> Responses:\n> HTTP/1.1 200 Document follows\n> Server: Domino-Go-Webserver/4.6\n> Date: Tue, 03 Mar 1998 22:35:05 GMT\n> Connection: Keep-Alive\n> Accept-Ranges: bytes\n> Vary: *\n> Content-Location: Frntpage.html\n> Content-Type: text/html\n> Content-Length: 2414\n> Last-Modified: Mon, 11 Aug 1997 17:39:59 GMT\n>\n> ...\n>\n>\n> HTTP/1.1 200 Document follows\n> Server: Domino-Go-Webserver/4.6\n> Date: Tue, 03 Mar 1998 22:35:06 GMT\n> Connection: Keep-Alive\n> Accept-Ranges: bytes\n> Content-Type: image/gif\n> Content-Length: 14865\n> Last-Modified: Mon, 11 Aug 1997 17:40:17 GMT\n>\n> ...\n>\n>\n> There were three GETs altogether, and one went by itself on a second\nconnection.\n> I haven't seen MSIE put more than two requests on a connection.\n>\n> Carl Kugler\n>\n>\n>\n> yarong@microsoft.com on 03/03/98 02:44:23 PM\n> Please respond to yarong@microsoft.com @ internet\n> To: marcs@znep.com @ internet, lawrence@agranat.com @ internet\n> cc: HTTP-WG@cuckoo.hpl.hp.com @ internet\n> Subject: RE: looking for a browser that pipelines requests\n>\n>\n> IE 4.x does not pipeline requests.\n>  Yaron\n>\n> > -----Original Message-----\n> > From: Scott Lawrence [SMTP:lawrence@agranat.com]\n> > Sent: Tuesday, March 03, 1998 5:38 AM\n> > To: Marc Slemko\n> > Cc: HTTP-WG@cuckoo.hpl.hp.com\n> > Subject: Re: looking for a browser that pipelines requests\n> >\n> >\n> >\n> > On Tue, 3 Mar 1998, Marc Slemko wrote:\n> >\n> > > Has anyone verified if MSIE 4.x actually pipelines when talking to a 1.1\n> > > server?\n> >\n> > Yes, it does, and the performance difference is quite noticable even on a\n> > local connection.\n> >\n>\n>\n>\n>\n>\n>\n\n\n\n", "id": "lists-012-0588620"}, {"subject": "new editorial(?) issue HEAD_WITH_IM", "content": "Dave Kristol and I have been working in private to nail down\na precise description of how the various conditional headers\n(If-Modified-Since, If-None-Match, etc.) interact.\n\nDESCRIPTION OF PROBLEM:\n\nOne new question came up: is it \"legal\" to send an\nIf-Modified-Since header with a HEAD method?\n\nThe current revision of the specification does not address\nthis question directly.  However, it makes two somewhat\ncontradictory statements:\n\nSection 9.4 (HEAD) says:\n    The HEAD method is identical to GET except that the server MUST NOT\n    return a message-body in the response.\n\n\"identical\" implies that HEAD+IMS should be the same as GET+IMS,\nexcept that a 200 response would carry no body (a 304 response\nnever has a body, of course).\n\nOn the other hand, section 14.24 (or 14.25, depending on whether\nthe alphabetical ordering has been fixed in your copy of -rev-02)\nsays\nThe If-Modified-Since request-header field is used with the\nGET method to make it conditional ...\nbut does not mention the possibility of using this header with\nany other method.\n\n(The specifications of the other four If-* headers do not mention\na specific method, although If-Range is introduced as a way\nto avoid an extra GET request.)\n\nPROPOSED SOLUTION:\n\nOur intuition is that, based on the language in 9.4, any headers\nallowed with GET should be allowed with HEAD (and interpreted\nthe same way by the server).  I.e., the language in 14.24\nshould be changed to\nThe If-Modified-Since request-header field is used with the\nGET (or HEAD) method to make it conditional ...\nor even\nThe If-Modified-Since request-header field is used with a\nmethod to make it conditional ...\n\nWHAT SERVERS ALREADY DO:\n\nAlthough I'm not a fan of testing existing implementations to\nsee what the specification should say, I'll note that Apache/1.2.5\nseems to allow and support HEAD+IMS:\n\n    % telnet www.freebsd.org 80\n    Trying 204.216.27.18...\n    Connected to hub.freebsd.org.\n    Escape character is '^]'.\n    HEAD / HTTP/1.0\n    \n    HTTP/1.1 200 OK\n    Date: Wed, 04 Mar 1998 22:20:33 GMT\n    Server: Apache/1.2.5\n    Last-Modified: Wed, 04 Mar 1998 12:02:49 GMT\n    ETag: \"2742a6-2fa4-34fd42e9\"\n    Content-Length: 12196\n    Accept-Ranges: bytes\n    Connection: close\n    Content-Type: text/html\n    \n    Connection closed by foreign host.\n    % telnet www.freebsd.org 80 \n    Trying 204.216.27.18...\n    Connected to hub.freebsd.org.\n    Escape character is '^]'.\n    HEAD / HTTP/1.0\n    If-Modified-Since: Wed, 04 Mar 1998 12:02:49 GMT\n    \n    HTTP/1.1 304 Not Modified\n    Date: Wed, 04 Mar 1998 22:20:44 GMT\n    Server: Apache/1.2.5\n    Connection: close\n    ETag: \"2742a6-2fa4-34fd42e9\"\n    \n    Connection closed by foreign host.\n    %\n    \nBy the way, Apache/1.2.5 also returns \"304\" when the request includes\n    If-None-Match: \"2742a6-2fa4-34fd42e9\"\ninstead of\n    If-Modified-Since: Wed, 04 Mar 1998 12:02:49 GMT\n\n-Jeff\n\n\n\n", "id": "lists-012-0603908"}, {"subject": "Rigorously defining the interaction of conditional header", "content": "Dave Kristol in\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0402.html\nbrought up the problem that the current draft of the HTTP/1.1\nspecification is ambiguous or hard to interpret in several ways.\n\nI believe we have now identified all of the actual ambiguities\nand created \"issues\" for them:\n    issue IM_IUS_412\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0409.html\n    issue CACHE_CONDITIONAL\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0406.html\n    issue HEAD_WITH_IMS\n(not visible in the archive yet)\n\nThis leaves the problem that it is still somewhat hard to read\nthe specification, especially if one is trying to figure out\nhow the various headers interact.\n\nDave and I have spent several days coming up with pseudo-code\nthat, we believe, correctly represents the specified behavior\nin all possible cases.  It was agreed during this week's editorial\nteleconference that the next draft should include such pseudo-code,\nas an explanation of how to interpret the existing specification.\n\nSince neither Dave nor I was able to get the right code on our\nfirst tries, we would appreciate some review of this!\n\nBecause the cutoff date for Internet-Draft submission is Friday,\nMarch 13, and Jim Gettys will need several days to actually generate\nthe draft in the required format, any comments must be received\nas soon as possible (i.e., \"read this today\").\n\n-Jeff (and Dave Kristol)\n\nP.S.: Note that the somewhat repetitive format of this code\nis intentional.  I found that the only way that I could be\nsure that we had covered every possible case was to fully\nexpand the ELSE clauses of every IF statement.  Also, Dave\nand I (mildly) disagreed on whether it is clearer to use\na few GOTO statements, or an auxilliary state variable which\nis set in one place and tested in another.  My preference\nwas to use the GOTOs, to make the control flow as clear as possible.\n\n\n\nNEW SECTION 13.3.5  Interpreting combinations of conditional headers\n\n    HTTP/1.1 includes four request-header fields that can be used to\n    make a request conditional: If-Match (section 14.24),\n    If-Modified-Since (section 14.25), If-None-Match (section 14.26),\n    and If-Unmodified-Since (section 14.28).  The protocol allows\n    combinations of these four headers, and the proper interpretation\n    of the meaning of the possible combinations has proved to be\n    difficult to extract from the language of the specification.\n\n    We therefore include a description, in pseudo-code, of one possible\n    correct implementation of the logic for processing HTTP/1.1\n    conditional headers in order to decide whether to perform the\n    request, or to return a status code of 304 (Not Modified) or 412\n    (Predoncondition Failed).\n\n    Many other implementations are allowed by the HTTP/1.1\n    specification, but their external behavior is required to conform\n    to the behavior of the pseudo-code below, in all cases where it\n    defines a specific behavior.\n\n    Note that there is a fifth conditional header, If-Range (section\n    14.26) that is not described here.  It does not make sense to\n    combine the If-Range request-header field with other conditional\n    request-header fields, and so there is no need to define its\n    interaction with other such fields.\n    \n    /* Logic for processing HTTP/1.1 conditional headers\n     *(not including If-Range)\n     *\n     * Implicit inputs:\n     *The request message\n     *The selected resource\n     *\n     * Outputs:\n     *  the response status code, if its value is defined by this logic\n     *  otherwise, the decision to continue processing the request \n     */\n    \n    /*\n     * Uses the following functions:\n     *\n     * BOOLEAN reqhdrs_include(STRING field_name)\n     *returns true if the request headers include the given field-name.\n     *\n     * STRING fieldval(STRING field_name)\n     *returns the request's field-value for the given field-name.\n     *(May involve combining multiple instances of the same header.)\n     *\n     * INT count_match_current_etag(STRING field_value)\n     *returns the number of entity tags in the field-value that\n     *match the current entity tag of the selected resource.\n     *\n     * BOOLEAN matches_lastmod(STRING field_value)\n     *returns true if the timestamp in the field-value matches\n     *the current last-modified timestamp of the selected resource.\n     */\n    \n    /*\n     * NOTE: all branches of the IF-THEN-ELSE statements are included\n     * in the following nested conditional, to ensure that no cases\n     * are omitted\n     */\nIF reqhdrs_include(\"If-None-Match\") THEN {\n  IF count_match_current_etag(fieldval(\"If-None-Match\")) > 0 THEN {\n    IF reqhdrs_include(\"If-Modified-Since\") THEN {\n      IF matches_lastmod(fieldval(\"If-Modified-Since\")) THEN {\nIF req_method == \"GET\" OR req_method == \"HEAD\" THEN {\n  status_code = 304; RETURN;\n} ELSE {\n  /* WARNING: meaningless If-Modified-Since ? */\n  GOTO next_step;\n} ENDIF\n      } ELSE {/* If-Modified-Since mismatch */\nGOTO next_step;\n      } ENDIF\n    } ELSE {/* entity tags match, no If-Modified-Since */\n      /* 14.26 says MUST NOT perform method */\n      IF req_method == \"GET\" OR req_method == \"HEAD\" THEN {\nstatus_code = 304; RETURN;\n      } ELSE {\nstatus_code = 412; RETURN;\n      } ENDIF\n    } ENDIF \n  } ELSE {/* no entity tag matches, ignore If-Modified-Since */\n    GOTO next_step;\n  } ENDIF\n} ELSE {\n  /* Check for vanilla If-Modified-Since */\n  IF reqhdrs_include(\"If-Modified-Since\") THEN {\n    IF matches_lastmod(fieldval(\"If-Modified-Since\")) THEN {\n      IF req_method == \"GET\" OR req_method == \"HEAD\" THEN {\nstatus_code = 304; RETURN;\n      } ELSE {\n/* WARNING: meaningless If-Modified-Since ? */\nGOTO next_step;\n      } ENDIF\n    } ELSE {/* If-Modified-Since mismatch */\n      GOTO next_step;\n    } ENDIF\n  } ELSE {\n    GOTO next_step;\n  } ENDIF\n} ENDIF\n    \n    next_step:\n/* If-Match */\nIF reqhdrs_include(\"If-Match\") THEN {\n  IF count_match_current_etag(fieldval(\"If-Match\")) == 0 THEN {\n    status_code = 412; RETURN;\n  } ENDIF\n} ENDIF\n    \n/* If-Unmodified-Since */\nIF reqhdrs_include(\"If-Unmodified-Since\") THEN {\n  IF NOT matches_lastmod(fieldval(\"If-Unmodified-Since\")) THEN {\n    status_code = 412; RETURN;\n  } ENDIF\n} ENDIF\n    \n/* continue processing the request */\n    \n\n\n\n", "id": "lists-012-0613609"}, {"subject": "Intro &amp; Questio", "content": "Hi\n\nI am new to the list and I am currently doing some work for a local \ncompany (Activate Ltd. Malta) that provides web services.\n\nI have had a problem recently with the HTTP server. (I hope it is not\noff-topic on this list).  My question is:\n\nWhen trying to submit a form to a CGI script (in Perl), I am getting\n        HTTP Error 405: Method Not Allowed.\nI have tried searching the RFC specs for HTTP protocol, but could not\nfind 405 among the status codes.  Could please someone tell me what this\nerror means?\n\nRegards \n\nMark\n\n\n\n", "id": "lists-012-0627647"}, {"subject": "Re: new editorial(?) issue HEAD_WITH_IM", "content": ">Our intuition is that, based on the language in 9.4, any headers\n>allowed with GET should be allowed with HEAD (and interpreted\n>the same way by the server).  I.e., the language in 14.24\n>should be changed to\n>The If-Modified-Since request-header field is used with the\n>GET (or HEAD) method to make it conditional ...\n\nThat is what we decided the last time this issue was discussed,\nwhich is why Apache was changed accordingly.\n\n>or even\n>The If-Modified-Since request-header field is used with a\n>method to make it conditional ...\n\nThat would lead to complications.  The semantics of If-Modified-Since\n(return 200 if true, 304 if false) do not make any sense for other methods.\n\n....Roy\n\n\n\n", "id": "lists-012-0634259"}, {"subject": "Re: Rigorously defining the interaction of conditional header", "content": "We already had an Issues list item for this (IMS_INM_MISMATCH)\nbased on\n\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0110.html\n\nfrom last November.  The ambiguities regarding multiple conditionals\nwere first mentioned (by me) at the last LA IETF, and a solution was\nposted a year ago in\n\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q1/0251.html\n\n>Dave and I have spent several days coming up with pseudo-code\n>that, we believe, correctly represents the specified behavior\n>in all possible cases.  It was agreed during this week's editorial\n>teleconference that the next draft should include such pseudo-code,\n>as an explanation of how to interpret the existing specification.\n\nI am less interested in specifying pseudocode that represents strictly\nwhat the specification (incompletely) says than I am in simply specifying\nthe precedence according to what the specification *should* say and then\nfixing the contradictions in the spec.  This is not an easy problem, as I\nwarned when the WG insisted on separate conditional fields instead of\na single IF header field.  However, I cannot believe that the right\nsolution is to specify it in unreadable pseudocode -- at least choose\na language with structure.  It is in fact far less readable than the\nactual code from Apache (and I know that the code works).\n\nIn any case, the pseudocode given does not do the right thing.\n\nThe order of the checks is important, since ETag checks are supposed\nto be more accurate than checks relative to the modification time.\nNone of the checks are made if the response would already be an error.\nFurthermore, some of the conditionals (If-Match and If-Unmodified-Since)\ndemand stronger action (higher precedence) due to their semantics\n(i.e., a response of 412 always overrides a check of 200/304).\nFinally, if a strong validation check is available, we do not care about\nthe result of any weaker validation check (and thus the weaker check must\nnot be performed).  These observations need to be added to the current\nspecification, preferably by grouping all of the conditional fields\nunder a single section and defining the precedence at the start.\n\nThe right procedure to follow is described in the Apache code+comments:\n\n=======================================================================\n   if (the server has already decided to respond with an error)\n   {\n      return with the error status;\n   }\n\n   SRV := the selected resource variant\n\n   if ((an If-Match request-header field was given) AND\n       ((SRV does not have an ETag defined) OR\n        ((SRV's ETag does not match any of the entity tags in that field) AND\n         (the field value is not \"*\" [meaning match anything])))\n   {\n      return with a status of 412 (Precondition Failed);\n   }\n   else if ((a valid If-Unmodified-Since request-header field was given) AND\n            (SRV has been modified since the IUS time))\n   {\n      return with a status of 412 (Precondition Failed);\n   }\n\n   if ((an If-None-Match request-header field was given) AND\n       ((the field value is \"*\" [meaning match anything]) OR\n        ((SRV has an ETag defined) AND \n         (SRV's ETag matches any of the entity tags in that field))))\n   {\n      if (the request method was GET or HEAD)\n      {\n         return with a status of 304 (Not Modified);\n      }\n      else\n      {\n         return with a status of 412 (Precondition Failed);\n      }\n   }\n   else if ((a valid If-Modified-Since request-header field was given) AND\n            (the request method was GET or HEAD) AND\n            (SRV has not been modified since the IMS time))\n   {\n      return with a status of 304 (Not Modified);\n   }\n\n   return SRV with the appropriate success status;\n=======================================================================\n\nThat can be added to the spec if desired, but the important thing is that\nthe specification must define a precedence in order to avoid contradictions.\nSince precondition checks >> validation checks and etag checks > last_mod,\nthe only sensible precedence is\n\n    If-Match > If-Unmodified-Since > If-None-Match > If-Modified-Since\n\n.....Roy\n\n\n\n", "id": "lists-012-0641863"}, {"subject": "Re: Rigorously defining the interaction of conditional header", "content": "On Thu, 5 Mar 1998, Roy T. Fielding wrote:\n\n> \n> That can be added to the spec if desired, but the important thing is that\n> the specification must define a precedence in order to avoid contradictions.\n> Since precondition checks >> validation checks and etag checks > last_mod,\n> the only sensible precedence is\n> \n>     If-Match > If-Unmodified-Since > If-None-Match > If-Modified-Since\n> \n\nThis is certainly clear and simple.  But, why do precondition checks \ntake precedence over validation checks?  While we are on the subject,\nwhat is the purpose of the precondition checks?\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-0653964"}, {"subject": "Re: Rigorously defining the interaction of conditional header", "content": "At 03:36 05/03/98 -0800, Roy T. Fielding wrote:\n>[...] However, I cannot believe that the right\n>solution is to specify it in unreadable pseudocode -- at least choose\n>a language with structure.  It is in fact far less readable than the\n>actual code from Apache (and I know that the code works).\n\nI find decision tables are a useful way to analyse such problems.\n\n(My reference is over 25 years old: \"Programs from Decision Tables\", E.\nHumbey, Macdonald/American Elsevier computer monographs, ISBN\n0-444-19569-6/0-356-04126-3.  It's probably not still in print.  It does\ncontain a number of references to decision table articles published in the\n1960s in Commications of the ACM:  August 1967, September 1970, January\n1966, November 1966, October 1968, January 1965, February 1964, June 1970,\nNovember 1965, June 1965, February 1971.  All of these pre-date even my\nlibrary of \"Communications\".  Strangely, I'm not aware of any more recent\nreferences.)\n\nGK.\n---\n\n------------\nGraham Klyne\n\n\n\n", "id": "lists-012-0663455"}, {"subject": "Re: Rigorously defining the interaction of conditional header", "content": "At the risk of saying the obvious, Roy Fielding's remarks about\nconditional headers in\n\n<http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0434.html>\n\npropose a different algorithm from what Jeff Mogul and I hammered\nout based on the current specification:\n\n<http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0431.html>\n\nI don't know which is the better answer, but, as an implementer, I find\nmyself stuck, uncertain about what exactly I should implement.  Since I\nhaven't seen any clarifying noise on http-wg, I would feel better if\nsomeone said (on the list) that they were working behind the scenes to\nresolve the discrepancy.\n\nDave Kristol\n\n\n\n", "id": "lists-012-0672283"}, {"subject": "Re: Rigorously defining the interaction of conditional header", "content": ">This is certainly clear and simple.  But, why do precondition checks \n>take precedence over validation checks?  While we are on the subject,\n>what is the purpose of the precondition checks?\n\nA precondition gives a client the opportunity to require that the\nresource (or at least some aspect of the resource) has not changed\nbefore applying a method.  Preconditions are necessary to support\natomic check+set actions which are common in distributed authoring\nand version control situations.\n\nActually, validation checks are also preconditions, but I didn't have a\nconvenient word for preconditions-that-return-412-when-false vs\n                    preconditions-that-return-304-when-false.\nThe latter is only used for cache validation checks.\n\nThe precedence is due to the requirement that 304 be returned only\nif the response would otherwise be 200.  Since a 412 is not a 200,\nthose preconditions win.  Checking them first is a performance issue,\nbut also simplifies the algorithm description.\n\n....Roy\n\n\n\n", "id": "lists-012-0680131"}, {"subject": "Re: http acceleration and Date header", "content": "On Sun, 8 Mar 1998, Andrew Daviel wrote:\n\n> I have been doing a survey of clock accuracy\n> (http://vancouver-webpages.com/time/) and have found that, while the\n> majority of Web server clocks (as derived from the http Date header) \n> are very accurate, a few are way off. \n\nI've seen this issue arise in a different aspect of Squid.  Perhaps it\nshould be part of the Apache documentation to instruct that the Webmaster\ninstall a Network Time Protocol client before ore installing pages on the\nweb server.\n\nMy problem was that the remote server was about five to ten minutes off,\nhence the modification date of a file was off.  So, when an\nif-modified-since request was sent from my Squid server, whose clock is\nsynchronized with the world time standard, the file would show up as not\nmodified until enough minutes had passed to make up for the clock error.\n\n-Mike Pelletier.\n\n\n\n", "id": "lists-012-0688697"}, {"subject": "http acceleration and Date header", "content": "I have been doing a survey of clock accuracy\n(http://vancouver-webpages.com/time/) and have  found that, while the\nmajority of Web server clocks (as derived from the http Date header)\nare very accurate, a few  are way off.\n\nI then wondered what the effect was of running an http accelerator (Squid\nin httpd_accel mode, for instance) and tried it. It seems that the\nServer header is unmodified, but that the Date header remains set at\nwhatever the origin server said until someone does an unconditional GET\n(shift-reload in Netscape) (or presumeably until the original page is\nmodified...).\n\nI can't tell (as far as I know) if this is actually the reason for\nseeing clock errors.\n\nI wondered if this was the way that httpd accelerators were intended to\nwork.\n\nAndrew Daviel\nVancouver Webpages & TRIUMF\n\n\n\n", "id": "lists-012-0697788"}, {"subject": "Re: http acceleration and Date header", "content": "On Mon, 9 Mar 1998, Michael Pelletier wrote:\n\n> My problem was that the remote server was about five to ten minutes off,\n> hence the modification date of a file was off.  So, when an\n> if-modified-since request was sent from my Squid server, whose clock is\n> synchronized with the world time standard, the file would show up as not\n> modified until enough minutes had passed to make up for the clock error.\n\n  Proxies (squid) should not use its own time for the response when doing\na conditional get request; it should use the time in the Last-Modified (or\nif that is not there, then Date) header from the original response.  This \navoids the clock syncronization problem because the origin server is\nalways using timestamps from its own clock.\n\n\n\n", "id": "lists-012-0705964"}, {"subject": "I-D ACTION:draft-ietf-http-trust-state-mgt02.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: HTTP Trust Mechanism for State Management\nAuthor(s): D. Jaye\nFilename: draft-ietf-http-trust-state-mgt-02.txt\nPages: 11\nDate: 06-Mar-98\n\nThis document specifies an addition to the state management protocol\nspecified in draft-ietf-http-state-man-mec-08[Kristol].  The intent is\nto provide a mechanism that allows user agents to determine the privacy\npractices of a server and to accept or reject cookies based on those\npractices.  Allowing the user to establish preferences for how to handle\ncookies based on the server's practices provides a practical mechanism\nto provide users control over the privacy implications of cookies.\n \nTo provide verification of server privacy practices, we assume the\nexistence of one or more independent Trust Authorities.  The authority\nestablishes PICS ratings representing server privacy practices. It then\nissues trust-labels, in the form of digitally signed PICS labels, to\norganizations for specific domains and paths based on the server privacy\npractices.  The Trust Authority must be able to audit domains to\nverify their adherence to a given level.  Passing these trust-labels\nalong with cookies allows the user agent to support cookie handling\npreferences based on trusted privacy practices.\n \nThis document describes how PICS-headers are used in conjunction with\nSet-Cookie or Set-Cookie2 headers in [Kristol] to provide trust-labels\nto communicate the privacy practices of servers regarding cookies.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-trust-state-mgt-02.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-trust-state-mgt-02.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-trust-state-mgt-02.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-0714028"}, {"subject": "issue &amp; resolution: ETAG_CLARIT", "content": "Some private discussion, including a lengthy one during today's\neditorial-group teleconference, has resulted in this proposed\nchange to the language in section 14.19 (ETag).  We believe that\nthis is really just a clarification, rather than a substantive\nchange to the specification:\n\n(1) Change\n\nThe ETag entity-header field defines the entity tag for the\nassociated entity. The headers used with entity tags are\ndescribed in sections 14.19, 14.25, 14.26 and 14.44. The entity\ntag may be used for comparison with other entities from the\nsame resource (see section 13.2.3).\n\nTo\n\nThe ETag response-header field provides the current value of\nthe entity tag for the requested variant. The headers used with\nentity tags are described in sections 14.25, 14.26 and 14.44.\nThe entity tag may be used for comparison with other entities\nfrom the same resource (see section 13.2.3).\n\nRationale: ETag is only meaningful in a response, because it's not\npossible for a client to tell a server what ETag to assign to a\nresource or variant  Also, the entity-tag in an ETag header refers to\nthe requested resource or variant, not necessarily the contents\n(message-body) of the response.  This allows (for example) a server\nresponding to PUT to provide the current entity-tag value for the\nresource (or variant) affected by the PUT, which allows the client to\nknow if its cache entry for that resource is still valid.\n\nThe cross-reference to section 14.19 was superfluous in 14.19,\nand is deleted.\n\n*** The rest of these changes are a consequence of making ETag\n*** a response-header field, rather than an entity-header field.\n\n(2) In section 7.1 (Entity Header Fields), remove\n\n                      | ETag                     ; Section 14.19\n\nfrom the BNF for entity-header.\n\n(3) In section 6.2 (Response Header Fields), add\n\n                      | ETag                     ; Section 14.19\n\nto the BNF for response-header.\n\n(4) In 13.3.2 (Entity Tag Cache Validators), change\n\nThe ETag entity-header field value, an entity tag, provides\nfor an \"opaque\" cache validator.\n\nTo\n\nThe ETag response-header field value, an entity tag, provides\nfor an \"opaque\" cache validator.\n\n(5) In section 14.26 (If-None-Match), change\n\n    Instead, if the request method was GET or HEAD, the server SHOULD\n    respond with a 304 (Not Modified) response, including the\n    cache-related entity-header fields (particularly ETag) of one of\n    the entities that matched. For all other request methods, the\n    server MUST respond with a status of 412 (Precondition Failed).\n\nTo\n\n    Instead, if the request method was GET or HEAD, the server SHOULD\n    respond with a 304 (Not Modified) response, including the\n    cache-related header fields (particularly ETag) of one of the\n    entities that matched. For all other request methods, the server\n    MUST respond with a status of 412 (Precondition Failed).\n\n-Jeff\n\n\n\n", "id": "lists-012-0723776"}, {"subject": "Editorial Issue: HTTP Trademarked by MIT or W3", "content": "I just noticed a couple of days ago that the W3C claims to own HTTP \nas a trademark. (This is on the W3C web site:\n  http://www.w3.org/Consortium/Legal/ipr-notice.html#W3C \nis one list). Assuming that notice is represents a valid claim to a\ntrademark, aren't we required to acknowledge the trademark?\n\nDave Morris\n\n\n\n", "id": "lists-012-0732940"}, {"subject": "issue &amp; resolution: IF_PRECEDENC", "content": "During today's editorial-group teleconference, we discussed\nwhether the HTTP/1.1 specification should or should not specify\nprecedences between If-* headers other than what is already\nspecified in the draft-ietf-http-v11-spec-rev-02 document.\n\nSince this would represent a substantive change to the specification,\nwe agreed that such a change requires specific justification.  That\nis, we would like to see a concrete and plausible scenario described\nin which the lack of a precedence between (for example) If-Match\nand If-None-Match results in a failure of interoperability.\n\nWe agreed that we would reopen this issue if someone provides such a\nscenario.  In the absence of such a scenario, we agreed to make the\nspecification more explicit about the lack of a precedence.  The\nchanges below, therefore, are editorial clarifications, and are not\nsubstantive changes to the specification.\n\n(1) To the end of section 14.24 (or 14.25, depending on which\nalphabetization you have), If-Match, add:\n\n    This specification does not define a precedence between the\n    If-Match header field and the If-None-Match or If-Modified-Since\n    header fields.\n\n(2) To the end of section 14.25 (or 14.24, depending on which\nalphabetization you have), If-Modified-Since, add:\n\n    This specification does not define a precedence between the\n    If-Modified-Since header field and the If-Match or\n    If-Unmodified-Since header fields.\n\n(3) To the end of section 14.26 (If-None-Match), add:\n\n    This specification does not define a precedence between the\n    If-None-Match header field and the If-Match or If-Unmodified-Since\n    header fields.\n\n(4) To the end of section 14.28 (If-Unmodified), add:\n\n    This specification does not define a precedence between the\n    If-Unmodified-Since header field and the  If-None-Match or\n    If-Modified-Since header fields.\n\n-Jeff\n\n\n\n", "id": "lists-012-0740518"}, {"subject": "Re: issue &amp; resolution: IF_PRECEDENC", "content": "On Tue, 10 Mar 1998, Jeffrey Mogul wrote:\n\n> During today's editorial-group teleconference, we discussed\n> whether the HTTP/1.1 specification should or should not specify\n> precedences between If-* headers other than what is already\n> specified in the draft-ietf-http-v11-spec-rev-02 document.\n> \n...\n> \n> We agreed that we would reopen this issue if someone provides such a\n> scenario.  In the absence of such a scenario, we agreed to make the\n> specification more explicit about the lack of a precedence.\n\n\nExcellent decision!\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-0749253"}, {"subject": "Re: Editorial Issue: HTTP Trademarked by MIT or W3", "content": ">I just noticed a couple of days ago that the W3C claims to own HTTP \n>as a trademark. (This is on the W3C web site:\n>  http://www.w3.org/Consortium/Legal/ipr-notice.html#W3C \n>is one list). Assuming that notice is represents a valid claim to a\n>trademark, aren't we required to acknowledge the trademark?\n\n\nI'll find out what the status is and report back; until then, let's not deal with this.\nIf something is necessary, we'll catch it before the RFC issues.\n\nLarry\n\n\n\n", "id": "lists-012-0757666"}, {"subject": "http acceleration and Date headers (fwd", "content": "Apologies for any duplicates - we'd had a network outage\nand I hadn't seen this turn up on the Squid list...\n---------- Forwarded message ----------\nDate: Sun, 8 Mar 1998 13:44:31 -0800 (PST)\nFrom: Andrew Daviel <andrew@andrew.triumf.ca>\nTo: squid-users@nlanr.net, http-wg@cuckoo.hpl.hp.com\nSubject: http acceleration and Date headers\n\n\nI have been doing a survey of clock accuracy\n(http://vancouver-webpages.com/time/) and have  found that, while the\nmajority of Web server clocks (as derived from the http Date header)\nare very accurate, a few  are way off.\n\nI then wondered what the effect was of running an http accelerator (Squid\nin httpd_accel mode, for instance) and tried it. It seems that the\nServer header is unmodified, but that the Date header remains set at\nwhatever the origin server said until someone does an unconditional GET\n(shift-reload in Netscape) (or presumeably until the original page is\nmodified...).\n\nI can't easily tell  if this is actually the reason for\nthe clock errors I've seen.\n\nI wondered if this was the way that httpd accelerators were intended to\nwork.\n\nAndrew Daviel\nVancouver Webpages & TRIUMF\n\n\n\n", "id": "lists-012-0765556"}, {"subject": "Re: issue &amp; resolution: IF_PRECEDENC", "content": "It is impossible to tell right now whether failing to address\nthe issue will leave several contradictions in the next draft, or if\nthose contradictions will be fixed by other changes.  Either way, there\nis only one order of precedence (and thus implementation) which will\nresult in a conservative and efficient test of conditional requirements\ngiven the presence of both last-modified and etag.  Spending effort\nto not specify that in the draft is a bit silly, but won't change\nthe correct implemenations one iota.\n\nSpecifying the interaction between mutually contradictory conditionals\n(If-Match/If-None-Match, If-Modified-Since/If-Unmodified-Since) does\nnot describe a plausible scenario, but it is just as easy to say that\nthe result of combining such conditionals is explicitly undefined,\nrather than saying that there is no precedence (which says nothing).\n\nSpecifying the interaction between etag-based conditionals and\nlast-modified-based conditionals is plausible (and indeed specified\nelsewhere in the draft, just not where the reader can easily find it)\nsince such combinations will be the norm when using a proxy.\n\n....Roy\n\n\n\n", "id": "lists-012-0774486"}, {"subject": "issue &amp; resolution: 204UNCLEA", "content": "The definition of \"204 No Content\" makes reference to updated\nmetainformation being returned about the requested variant. It does so in\nold language that refers to the \"document in the user agent's active view\",\nrather than the requested variant, making it seem that it only had UI\nconsequences. Some server implementations were returning updated\nmetainformation, e.g. after a PUT, and expecting that clients could use it,\nand some client implementations wanted updated information after a PUT and\ndidn't realize this was the way to get it.\n\nThe following is proposed as the replacement for 10.2.4 204 No Content:\n\n> 10.2.5 204 No Content \n> The server has fulfilled the request but does not need to return an\n> entity-body, and may want to return updated metainformation.  The response\n> MAY include new or updated metainformation in the form of entity-headers,\n> which if present SHOULD be associated with the requested variant.\n> \n> If the client is a user agent, it SHOULD NOT change its document view from\n> that which caused the request to be sent. This response is primarily\n> intended to allow input for actions to take place without causing a change\n> to the user agent's active document view, although any new or updated\n> metainformation SHOULD be applied to the document currently in the user\n> agent's active view.\n> \n> The 204 response MUST NOT include a message-body, and thus is always\n> terminated by the first empty line after the header fields.\n> \npaul\n\n\n\n", "id": "lists-012-0782518"}, {"subject": "issue &amp; resolution: LOCATIONPU", "content": "> I propose to replace:\n> \n> 14.14 Content-Location\n> The Content-Location entity-header field MAY be used to supply the\n> resource location for the entity enclosed in the message when that entity\n> is accessible from a location separate from the requested resource's URI.\n> In the case where a resource has multiple entities associated with it, and\n> those entities actually have separate locations by which they might be\n> individually accessed, the server should provide a Content-Location for\n> the particular variant which is returned. In addition, a server SHOULD\n> provide a Content-Location for the resource corresponding to the response\n> entity.\n>        Content-Location = \"Content-Location\" \":\" \n>                          ( absoluteURI | relativeURI )\n> The value of Content-Location also defines the base URL for the entity\n> (see section Error! Reference source not found.).\n> \n> The Content-Location value is not a replacement for the original requested\n> URI; it is only a statement of the location of the resource corresponding\n> to this particular entity at the time of the request. Future requests MAY\n> use the Content-Location URI if the desire is to identify the source of\n> that particular entity. \n> \n> A cache cannot assume that an entity with a Content-Location different\n> from the URI used to retrieve it can be used to respond to later requests\n> on that Content-Location URI. However, the Content-Location can be used to\n> differentiate between multiple entities retrieved from a single requested\n> resource, as described in section 13.6.\n> \n> If the Content-Location is a relative URI, the relative URI is interpreted\n> relative to the Request-URI.\n> \n> with\n> 14.14 Content-Location\n> The Content-Location entity-header field MAY be used to supply the\n> resource location for the entity enclosed in the message when that entity\n> is accessible from a location separate from the requested resource's URI.\n> A server SHOULD provide a Content-Location for the variant corresponding\n> to the response entity; especially in the case where a resource has\n> multiple entities associated with it, and those entities actually have\n> separate locations by which they might be individually accessed, the\n> server SHOULD provide a Content-Location for the particular variant which\n> is returned.\n>        Content-Location = \"Content-Location\" \":\" \n>                          ( absoluteURI | relativeURI )\n> The value of Content-Location also defines the base URL for the entity\n> (see section Error! Reference source not found.).\n> \n> The Content-Location value is not a replacement for the original requested\n> URI; it is only a statement of the location of the resource corresponding\n> to this particular entity at the time of the request. Future requests MAY\n> specify the Content-Location URI as the request-uri if the desire is to\n> access that particular entity. \n> \n> A cache cannot assume that an entity with a Content-Location different\n> from the URI used to retrieve it can be used to respond to later requests\n> on that Content-Location URI. However, the Content-Location can be used to\n> differentiate between multiple entities retrieved from a single requested\n> resource, as described in section 13.6.\n> \n> If the Content-Location is a relative URI, the relative URI is interpreted\n> relative to the Request-URI.\n> \n> The meaning of the Content-Location header in requests is undefined;\n> servers are free to ignore it in those cases.\n> \n> To save hunting, the changes are from\n> In the case where a resource has multiple entities associated with it, and\n> those entities actually have separate locations by which they might be\n> individually accessed, the server should provide a Content-Location for\n> the particular variant which is returned. In addition, a server SHOULD\n> provide a Content-Location for the resource corresponding to the response\n> entity.\n> to\n> A server SHOULD provide a Content-Location for the variant corresponding\n> to the response entity; especially in the case where a resource has\n> multiple entities associated with it, and those entities actually have\n> separate locations by which they might be individually accessed, the\n> server SHOULD provide a Content-Location for the particular variant which\n> is returned.\n> \n> and from\n> Future requests MAY use the Content-Location URI if the desire is to\n> identify the source of that particular entity. \n> to\n> Future requests MAY specify the Content-Location URI as the request-uri if\n> the desire is to access that particular entity. \n> \n> and adds\n> The meaning of the Content-Location header in PUT or POST requests is\n> undefined; servers are free to ignore it in those cases.\n> \n> Rationale:\n> The first change makes it clear that the response can only have one\n> Content-Location URI; the old wording made it sound like there might be\n> two -- one of there are multiple entities, and one for the response\n> entity.\n> \n> The second change clarifies the vague \"to identify the source\" to\n> something operationally meaningful: to access that particular entity.\n> \n> The third change makes it clear that expecting Content-Location to do\n> anything on PUT or POST is hopeless.\n> \n> Paul\n> \n\n\n\n", "id": "lists-012-0790572"}, {"subject": "issue &amp; resolution: PUT-ENTITYHEADER", "content": "It doesn't say anywhere in the definition of PUT that the entity-headers, in\naddition to the entity-body, should be associated with the request URI.\n\nI proposed to add the following paragraph near the end of section 9.6\n\nUnless otherwise specified for a particular entity-header, the\nentity-headers in the PUT request SHOULD be applied to the resource created\nor modified by the PUT.\n\nPaul\n\n\n\n", "id": "lists-012-0802677"}, {"subject": "Re: comments on HTTP/1.1 Rev02 20Feb9", "content": "Thanks as usual for your excellent read!\n\n\n> From: dmk@research.bell-labs.com (Dave Kristol)\n> Date: Tue, 24 Feb 98 11:01:00 EST\n> To: http-wg@cuckoo.hpl.hp.com\n> Cc: jg@w3.org\n> Subject: comments on HTTP/1.1 Rev02 20Feb98\n> \n\n> \n> 1) 9.2 OPTIONS\n>     \"The response body, if any, SHOULD also include....\"\n> \n>     The spec. says the body itself is undefined.  Can we at least state\n>     what the media type is?\n> \n\nMy memory is that the media type is deliberately left undefined for later\nuse.  I can anticipate a number of possible media types that might make\nsense, including an HTML page that just explains to a user what the options\nmight be.\n\n> 2) 10.2.7, 206 Partial Content\n>     \"... the Content-Length header field in the response MUST match...\"\n> \n>     Is there some reason why the entity couldn't be sent with chunked\n>     transfer coding instead and *without* a Content-Length?\n> \n>     It wasn't clear to me, until sect. 19.6.3, that a server could\n>     *limit* itself to sending these headers in response to If-Range.\n>     Could we make that clearer?\n\nDealt with in Jeff's message \nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0390.html (added\nto issue CONTENT-LENGTH).\n\n> \n> 3) 13, Caching in HTTP\n>     Last paragraph:  I think something like the following needs to be\n>     added at the end:\n>     \"In such cases, the design should also provide a way to inform the\n>     end user of a break in transparency.\"\n> \n> 4) 13.11 Write-Through Mandatory\n>     Suppose I want to add a custom method to HTTP, one of whose\n>     side-effects would be to invalidate a cache entry.  (Suppose\n>     I were adding something comparable to DELETE, for example.)\n>     How would the cache know it must invalidate the associated\n>     resource?  I don't think any of the Cache-Control directives\n>     can tell the cache to discard an entry, can they?  (And if they\n>     could, that would be an interesting denial of service attack.)\n> \n\nAlready dealt with in the thread with Jeff Mogul.\n\n> 5) 14.8 Authorization\n>     \"HTTP access authentication is described in section 11.\"\n>     Not anymore, it isn't!  At least, not in any detail.\n\nI've fixed the reference to point to the authentication document, here,\nand in other places where a similar problem occurs.\n\n> \n> 6) 14.9 Cache-Control\n>     I have three editorial suggestions to the reference utility of the\n>     spec.\n> \n>     1) It would be nice if the various cache-control directives were\n>     in the document's index.\n> \n\nThey are in the index: under Cache-Control, strangely enough :-).\n\n>     2) It would be nice if each of the c-c directives had its own\n>     paragraph, similar to public/private/no-cache in 14.9.1.  The\n>     hanging indents make it real easy to find descriptions.\n\nThanks for the suggestion; I hope you like the new formatting...  I think\nthis is a good idea.\n\n> \n>     3) Provide a very short description (one sentence would be nice)\n>     for each of the c-c directives just to give the sense of what each\n>     one does.  With each one I'd also like a list of the sections under\n>     14.9 where the details could be found.\n\nNo to the first request, but I've added section cross references in the\nBNF for the directives to help you find them more easily.\n> \n> 7) 14.39 TE\n>     Isn't the wording of the Note backwards?  Shouldn't it be:\n> \n>     \"Note: Because of backward compatibility considerations with RFC\n>     2068, neither parameter nor accept-params can be used with the\n>     \"chunked\" transfer-coding.\n\nYup.\n\n> ==================================\n> Nits\n> \n> 1) 2.1, implied *LWS\n>     I would feel better if \"... between any two tokens\" made it clear\n>     we're talking about the grammatical non-terminal \"token\".  In\n>     compiler-speak, ';' is also a token, but that's not what's meant\n>     here.\n\nDunno quite what you want here....\n\n> \n> 2) 8.1.1\n>     \"often require[s] a client to make\"\n>        ^-> add\n>     \"Analys[i]s of these...\"\n>             ^-> change\n> \n\nDone\n\n> 3) 8.2.4, Requirements for HTTP/1.1 clients\n>     \"... the client should not wait for a[n indefinite or] lengthy period\" ->\n>                                           --------------- -> delete\n>     (\"indefinite\" qualifies already as lengthy :-)\n\nDone\n\n> \n> 4) 10.3, Redirection 3xx\n>     \"A client SHOULD implement detect infinite...\"\n>          --------- -> delete\n\nDone\n\n> \n> 5) 10.3.6, 305 Use Proxy\n>     \"Note: ... a single request, or [to be] generated by...\"\n>          ----- -> add\n\nDone\n\n> 6) 11, Access Authentication\n>     \"HTTP provides a several optional challenge-response authentication\n>        ^-> delete\n>     mechanisms which MAY be used ...\"\n>          ^-> add\n> \n>     \"The general framework for access authentication, and the specifications\n>       --- -> add<- -\n>     of \"basic\" and \"digest\" authentication, are specified ...\"\n>       ^-> add\n> \n\nDone\n\n> 7) 12.1, Server-driven Negotiation\n>     \"The Vary header field can be used to express the parameters\n>     [the server uses] to select a representation that is subject to ...\"\n>     ================ -> change ------- -> add\n> \n\nDone\n\n> 8) 12.3, Transparent Negotiation (last paragraph)\n>     \"... does not prevent any such mechanism from being developed as an\n>     extension [that could be] used ...\"\n>            ============= -> change\n\nDone.\n\n> 9) 13.1.2, Warnings\n>     \"2. ... entity headers that [is] not rectified by a revalidation[,]\"\n>                                  == -> change  <- =\n>     [Verb refers back to \"aspect\".]\n\nDone\n\n> \n> 10) 13.2.1 Server-Specified Expiration\n>     [last sentence]\n>     \"See section 13.13 for [an] explanation of ...\"\n>         -- -> add\n\nDone\n\n> 11) 13.2.3, Age Calculations\n>     \"Because of network-imposed delays, some significant interval may\n>     pass [between] the time ...\n>           ======= -> change\n\nDone.\n\n> \n> 12) 13.2.4, Expiration Calculations\n>     \"If neither Expires nor Cache-Control: max-age or s-maxage ...\" ->\n>     \"If none of Expires, Cache-Control: max-age, or Cache-Control: s-maxage ...\"\n\nDone.\n\n> \n> 13) 14.3, Accept Encoding\n>     [first line]\n>     \" ... but restricts the content-coding[s] ...\"\n>        ^- add\n> \n\nDone\n\n> 14) 14.9.3, Modifications of ...\n>     \"If a response includes a[n] s-maxage directive, ...\"\n>           ^- add\n\nDone \n\n> \n>     \" ... and the fact that [pre]-HTTP/1.1-compliant caches ...\"\n>          --- -> change (from \"non\"; \"non\" would\n>                                      also apply to HTTP/1.2)\n\nDone\n\n> \n> 15) 14.9.6, Cache Control Extensions\n>     Third paragraph beginning \"For example, ...\"\n>     [Style] In this paragraph, the items in double-quotes, community,\n>     private, UCI, would appear elsewhere in Courier typeface and\n>     without quotes.  Be consistent.\n\nHobgoblins of small minds :-).\n\n> \n>     Next paragraph:  \"private\"\"\n>           ^- delete (at least)\n> \n\nDone\n\n> 16) 14.16, Content-Range\n>     \"... unless this length [this] is unknown ...\"\n>          ---- -> delete\n> \n\nDone\n\n>     \"If the server ignores a byte-range-spec because ...\"\n>                              --------------- -> should this be Courier?\n\nDone\n\n> \n> 17) 14.23, Host\n>     \"... servers MUST respond with a 400 [(Bad Request)] status code ...\"\n>   \n\nDone\n    ------------- -> add\n> \n> 18) 14.39, TE\n>     14.40, Trailer\n>         [Style] \"chunked\" is rendered three different ways to mean the\n>         same thing\n>         - chunked (Roman, no quotes)\n>         - \"chunked\" (Roman, double quotes)\n>         - \"chunked\" (Courier, double quotes)\n> \n>         Pick one and use it consistently.\n\nMore hobgoblins :-). Done.\n\n> \n>         [in two places]\n>                   \"... for other header fields than Content-MD5\" ->\n>                   \"... for header fields other than Content-MD5\"\n> \n\nDone\n\n> 19) 14.44, Vary\n>     [3rd paragraph]\n>     \"... for the duration of time [for] which the response is fresh.\"\n>        === -> change\n\nDone\n\n> \n> 20) 14.46, Warning\n>     [next-to-last paragraph after 299]\n>     \"... the sender MUST include a warn-date in each warning-value.\"\n>     ->\n>     \"... the sender MUST include in each warning-value a warn-date that\n>     matches the date in the response.\"\n> \n\nDone\n\n> 21) 15.1, Personal Information\n>     [last sentence, reword as follows]\n>     \"History shows that errors in this area often create serious\n>     security and/or privacy problems and generate highly adverse\n>     publicity for the implementer's company.\"\n> \n\nDone\n\n> 22) 15.3, DNS Spoofing\n>     \"... the deliberate mis[s]-association of ...\"\n>         ^- delete\n> \n\nDone\n\n> 23) 15.6, Authentication Credentials...\n>     \"dicard\" -> \"discard\"\n>     \"enourage\" -> \"encourage\"\n> \n\nDone\n\n> 24) 17, References\n>     [38] ... RFC 2279 [(]obsoletes RFC 2044), ...\n>            ^- add\n> \n\nDone\n\n> 25) 19.4.4, Introduction of Content-Encoding\n>     \"... to perform [a function equivalent to] Content-Encoding.\"\n>          ======================== ->\n>                      change from \"an equivalent function as\"\n\nDone\n\n> \n> 26) 19.4.7, MHTML ...\n>     \"... including line length limitations an[d] folding, ...\"\n>           ^- add\n> \nDone\n\n> 27) 19.5.1, Content-Disposition\n>     \"... seem to be present in the filename-parm parameter, ...\"\n>        -------------\n>     Should be Courier typeface?\n> \n\nDone\n\n> 28) 19.6, Compatibility ...\n>     \"It is worth noting that[,] at the time of composing this specification, we ...\"\n>          ^- add\n> \n\nDone, but also added (1996) to make it clearer these recommendations may\nbecome stale.\n\n>     \"... described in section 19.6.2.0.\"\n>     That's 19.6.2 now, I think.\n> \n\nDone.\n\n> 29) 19.6.1.1, Changes to Simplify ...\n>     \"... allocation of many IP addresses to a single host created serious\n>     problems.\"\n>     I think we need to be more specific.  How about:\n> \n>     \"... allocation of many IP addresses to a single host unnecessarily\n>     depleted the IP address space at a time when the community feared\n>     address space exhaustion.\"\n\nNo, a bigger problem is the routing problems rather than address space\nexhaustion. (Think about adding the 257th virtual host on a server, when\nyour subnet has 256 addresses, for example).\nI think leaving it as it is now is enough said, because the explanation\ngets much too involved.\n\n> \n>     [Bullet list]\n>     \"Host request-headers are required in HTTP/1.1 requests.\"\n>     change to\n>     \"A client that sends an HTTP/1.1 request MUST send a Host header.\"\n\nDone.\n\n> \n> 30) 19.6.3.1, Significant Changes From ...\n>     \"Require proxies [to] upgrade requests ...\"\n>           -- -> add\n\nDone.\n\n> \n>     \"The Cache-Control: max-age directive [was] not properly defined...\"\n>        --- -> add\n> \n\nDone\n\n>     \"A new error code (416)[ ] was needed to indicate an error for a byte\n>         ^- add\n>     range request [that falls] outside...\"\n>   \n\nDone\n     ---------- -> add\n> \n>     [Last paragraph]\n>     Is it transfer *encoding* or transfer *coding*?\n> \nTransfer coding\n\n>     \"... The solution is [that] transfer codings become...\"\n>       ---- -> change\n> \nDone\n>     \"... and enables trailer headers in the future.\" ->\n>     \"... and enabling headers in the trailer in the future.\"\n> \nDone\n\n> 31) 19.6.3.2, Clarifications of the Specification\n>     two instances of \"t-specials\" should be \"tspecials\".\n> \n\nDone\n\n>     \"Fix chunked transfer [en]coding to allow...\"\n>        -- -> delete\n\nDone\n\n> ==================================\n> Nitty nits\n> \n> 1) 10.2.7, 206 Partial Content\n>     \"...indicating the desired range , and...\n>         ^- delete\n\nDone\n\n> \n> 2) 15.1.3, Encoding Sensitive Information in URL's\n>     [last sentence]\n>     \"Servers can use POST[-]based form submission instead[.]\"\n>       ^- add  ^- add\n\nDone\n\nThanks again for your read!\n- Jim\n\n\n\n", "id": "lists-012-0809624"}, {"subject": "OPTIONS and TRACE vis a vis CGI application", "content": "It is my sense from list postings that a TRACE or OPTIONS request which\nreferenced a specific resource which a server handed to an external\napplication such as a CGI program should be handed to that program \nfor handling.\n\nThat is:\n   OPTIONS /cgi-bin/somescript HTTP/1.1\nor\n   TRACE /cgi-bin/somescript HTTP/1.1\nshould be handled by the application which would respond to a GET or HEAD\nof the same resource.\n\nIs that a fair summary of the WG's expectations in this case? A long scan\nof my memory and a check of the archives and latest draft didn't pop up\nanything. But I was sure I'd seen something on the list in the last few\nweeks to this effect.\n\nDave Morris\n\n\n\n", "id": "lists-012-0831395"}, {"subject": "Re: comments on HTTP/1.1 Rev02 20Feb9", "content": ">> 1) 2.1, implied *LWS\n>>     I would feel better if \"... between any two tokens\" made it clear\n>>     we're talking about the grammatical non-terminal \"token\".  In\n>>     compiler-speak, ';' is also a token, but that's not what's meant\n>>     here.\n>\n>Dunno quite what you want here....\n\nI guess something like \"... between any two tokens (for the definition of\n\"token\" above).\"\n\nMaybe it's just my compiler background getting in the way.  Or maybe my\nremark is irrelevant.  (And it's late, and I'm tired.... :-)\n\nDave Kristol\n\n\n\n", "id": "lists-012-0839569"}, {"subject": "Editorial issue 304SHOUL", "content": "I think Henry is right...  Please correct me if I'm wrong.\n\n- Jim\n\n\nattached mail follows:\nAs the title says, this is pretty minor....\n\nThe wording for handling the If-Modified-Since header uses a MUST when\ndescribing when to send back a 304 response. My understanding of\nMUST/SHOULD/MAY is that MUST is to be used for requirements neccessary for\ninteroperability or correct functioning of the protocol. Since the sending\nof a 304 response doesn't impact interoperability or correctness (the\nprotocol is unharmed but less efficient if a server ignores I-M-S and always\nsends back the data), this should probably be a SHOULD. \n\n(And no, IIS doesn't ignore I-M-S. This was brought up to me by a\nnon-Microsoft person writing an ISAPI app). \n\nHenry\n\n\n\n", "id": "lists-012-0847398"}, {"subject": "Re: new editorial (?) issue: IDENTITYT", "content": "> View: Browse HTML    Browse Raw Text\n> From: Jeffrey Mogul <mogul@pa.dec.com>\n> Resent-From: http-wg@cuckoo.hpl.hp.com\n> Date: Tue, 10 Feb 98 18:53:47 PST\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: new editorial (?) issue: IDENTITY-TE\n> \n> This is implicit in the current (rev-01) draft, but I think it\n> could be made explicit:\n> \n> In section 3.6 (Transfer Codings), after:\n>        The Internet Assigned Numbers Authority (IANA) acts as a\n>        registry for transfer-coding value tokens. Initially, the\n>        registry contains the following tokens: \"chunked\" (section\n>        3.6.1), \"identity\" (section 3.6.2), \"gzip\" (section 3.5),\n>        \"compress\" (section 3.5), and \"deflate\" (section 3.5).\n> \n> Add this paragraph:\n> \n>        The \"identity\" transfer-coding is used only in the TE\n>        header, and SHOULD NOT be used in the Transfer-Encoding\n>        header.\n> \n\nThis is already stated in the same way in 3.6.2 Identity Transfer Coding\n\n> Also, in section 3.5, there's a minor grammatical error in the\n> existing analogous statement:\n> \n>        identity\n>             The default (identity) encoding; the use of no\n>             transformation whatsoever. This content-coding is used only\n>             in the Accept-Encoding header, and SHOULD NOT be used in\n>             Content-Encoding header.\n> \n> should be:\n> \n>        identity\n>             The default (identity) encoding; the use of no\n>             transformation whatsoever. This content-coding is used only\n>             in the Accept-Encoding header, and SHOULD NOT be used in\n>             the Content-Encoding header.\n>             ^^^\n> \n\nDone.\n- Jim\n\n> -Jeff\n\n\n\n", "id": "lists-012-0854757"}, {"subject": "Re: OPTIONS and TRACE vis a vis CGI application", "content": "On Wed, 11 Mar 1998, David W. Morris wrote:\n\n> It is my sense from list postings that a TRACE or OPTIONS request which\n> referenced a specific resource which a server handed to an external\n> application such as a CGI program should be handed to that program \n> for handling.\n> \n> That is:\n>    OPTIONS /cgi-bin/somescript HTTP/1.1\n> or\n>    TRACE /cgi-bin/somescript HTTP/1.1\n> should be handled by the application which would respond to a GET or HEAD\n> of the same resource.\n\n  I would agree for OPTIONS, since it is probably the capabilities of the\nCGI or other sub-server that the requestor is interested in (although my\nexperience has usually been that CGIs often respond as though the method\nwere GET or POST because the authors don't check).\n\n  I can't think of any reason why TRACE would require the participation of\nthe resource - it's purpose is just to reflect the headers as received,\nand the server itself can do that just fine.\n \n\n\n\n", "id": "lists-012-0864179"}, {"subject": "Re: 505 response a MUST", "content": "> From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n> Date: Sun, 01 Mar 1998 22:30:52 -0800\n> To: Scott Lawrence <lawrence@agranat.com>\n> Cc: Larry Masinter <masinter@parc.xerox.com>,\n>         HTTP Working Group <http-wg@cuckoo.hpl.hp.com>,\n> jg@w3.org, http-wg@hplb.hpl.hp.com\n> Subject: Re: 505 response a MUST?\n> \n> >  That logic does not seem sound to me - if changing the major version\n> >number means that the protocol _may_ be not backward compatible, then a\n> >recipient that does not implement 2.0 cannot know whether or not it can\n> >_correctly_ interpret the message as a 1.1 message.  Your logic assumes\n> >that the authors of 2.0 will not make any semantic changes to existing\n> >protocol elements.\n> \n> No, it assumes a client won't send a 2.0 message to a 1.x server unless\n> the 2.0 message is sufficiently semantically and syntactically compatible\n> such that there is some reasonable assurance that the 1.x server will\n> respond in an anticipated (and safe) manner, even if that means a 400\n> response.  Forcing a server to respond in error just for the sake of\n> an error only guarantees the worst-case behavior of an extra round-trip,\n> and offers no extra safety in any case because deployed HTTP/1.x servers\n> will never respond with 505.  That code is intended for future 2.x\n> servers to say \"piss-off\" to older clients in the format of an HTTP/1.1\n> message, since they will have to respond in the same major version as\n> the old client.\n> \n> ....Roy\n\nI think Roy is right on this one; I'm removing the change from Rev-03.\n- Jim\n\n\n\n", "id": "lists-012-0873058"}, {"subject": "Production of HTTP/1.1 Rev03 has started", "content": "As I have started production of Rev-03, if you have anything you haven't \nsent it, you should hold off until it is issued.\n\n- Jim Gettys\n\n\n\n", "id": "lists-012-0884150"}, {"subject": "Re: Editorial issue 304SHOUL", "content": "Right, this ought to be a SHOULD.\n\nInterestingly enough, this \"bug\" basically goes back to RFC1945,\nwhere it says:\n\n      c) If the resource has not been modified since a valid\n         If-Modified-Since date, the server shall return a 304 (not\n         modified) response.\n\nRFC2119 says that \"SHALL\" and \"MUST\" mean the same thing, although\nI'm not sure that was the intention in RFC1945.\n\n-Jeff\n\n\n\n", "id": "lists-012-0890486"}, {"subject": "Re: OPTIONS and TRACE vis a vis CGI application", "content": "Scott Lawrence wrote:\n    On Wed, 11 Mar 1998, David W. Morris wrote:\n    \n    > It is my sense from list postings that a TRACE or OPTIONS request which\n    > referenced a specific resource which a server handed to an external\n    > application such as a CGI program should be handed to that program \n    > for handling.\n    > \n    > That is:\n    >    OPTIONS /cgi-bin/somescript HTTP/1.1\n    > or\n    >    TRACE /cgi-bin/somescript HTTP/1.1\n    > should be handled by the application which would respond to a GET or HEAD\n    > of the same resource.\n    \n    I would agree for OPTIONS, since it is probably the capabilities of\n    the CGI or other sub-server that the requestor is interested in\n    (although my experience has usually been that CGIs often respond as\n    though the method were GET or POST because the authors don't\n    check).\n\n    I can't think of any reason why TRACE would require the\n    participation of the resource - it's purpose is just to reflect the\n    headers as received, and the server itself can do that just fine.\n\nNote that the spec, in section 9.1.2 (Idempotent Methods) says\n\nAlso, the methods OPTIONS and TRACE should\nhave no side effects, and so are inherently idempotent.\n\n(I'm not sure if that \"should\" should be a \"SHOULD\" or a \"MUST\",\nor if it just means \"normally.\")\n\nLater, in 9.2 (OPTIONS) the spec says:\n    This method allows the client to determine the\n    options and/or requirements associated with a resource, or the\n    capabilities of a server, without implying a resource action or\n    initiating a resource retrieval.\n\nAnd in section 9.8 (TRACE) the spec says:\n    The TRACE method is used to invoke a remote, application-layer\n    loop-back of the request message.\n\nThe implication seems to me that the spec doesn't care if the\nCGI (or other sub-server) is invoked for TRACE or OPTIONS ...\nbut you can't let this invocation cause any side-effects.\n\nI'm not sure whether the CGI specification is sufficient\nto guarantee that if the method is OPTIONS or TRACE, then\nthe CGI script will never cause side-effects.  So it would\nseem to me that a cautious implementation of TRACE, at least,\nwouldn't risk invoking the CGI script.\n\nE.g., if the client sends\n\nTRACE /cgi-bin/action=buy&stock=sunw&quantity=100000 HTTP/1.1\nHost: stockbroker.com\n\nI think it would be a mistake to excecute the CGI script.\n\nIt's trickier with OPTIONS, since the capabilities of the CGI script\nmight be the target of the OPTIONS request.  I suspect this means\nthat we're not done figuring out how OPTIONS is supposed to work ....\n\n-Jeff\n\n\n\n", "id": "lists-012-0897626"}, {"subject": "Rev03 of the HTTP/1.1 specification has been submitted..", "content": "At http://www.w3.org/Protocols/HTTP/Issues/.  As usual, it is\navailable there in plain-text, postscript and Microsoft Word with and\nwithout change bars, either uncompressed or gzip'ed.\n\nAs of when I submitted this draft, there were no outstanding issues\nwith the specification I knew of, either technical or editorial.  This\nis the second draft for which this is true.\n\nWe expect the next draft of the Access Authentication specification by\ntomorrow (Friday).\n\nIt is likely this draft will be the Last-Call draft for draft standard \nof HTTP/1.1, though experience reminds me that it is possible there will \nbe one final draft to react to any comments and/or deletions required \nby interoperability reports (to satisfy IETF procedural requirements), or \nto resolve dependencies between the HTTP specification, the access \nauthentication and URI specifications.  \n\nWorking group members should have already read Rev-02.  If you haven't,\nyou have been negligent.\n\nI also expect everyone who drafted text to resolve issues for the\nspecification to check that I correctly edited those changes into this\ndraft, and to let me know of any problems.\n\nAt some (unpredictable) time in the next two weeks or so, I will become\nunavailable for a week or so. Nothing like completely unpredictable deadlines\nto cause one to lose sleep (whew! dodged that bullet).\n                        Your HTTP/1.1 Editor,\n                                Jim Gettys\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n", "id": "lists-012-0907294"}, {"subject": "URI Reference jg27", "content": "Apparently a decision, the discussion about I missed, was made to\nremove the syntactical description of URIs from the HTTP specification.\n\nProbably a good move, but that leaves the draft dependant on what is\ncurrently recorded as a private reference. See ref [42] in draft 03.\n\nI assume this is an internet draft and not just a private document.\nWhat is the name/status of the latest draft?\n\nDave Morris\n\n\n\n", "id": "lists-012-0915912"}, {"subject": "I-D ACTION:draft-ietf-http-v11-spec-rev03.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: Hypertext Transfer Protocol -- HTTP/1.1\nAuthor(s): J. Mogul, T. Berners-Lee, L. Masinter, P. Leach, \n                          R. Fielding, H. Nielsen, J. Gettys\nFilename: draft-ietf-http-v11-spec-rev-03.txt\nPages: 156\nDate: 12-Mar-98\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol\nfor distributed, collaborative, hypermedia information systems. It is a\ngeneric, stateless, protocol which can be used for many tasks, such as\nname servers and distributed object management systems, through\nextension of its request methods. A feature of HTTP is the typing and\nnegotiation of data representation, allowing systems to be built\nindependently of the data being transferred.\n \nHTTP has been in use by the World-Wide Web global information initiative\nsince 1990. This specification defines the protocol referred to as\n''HTTP/1.1'', and is an update to RFC2068 [33].\n \nAt the time of this revision's submission, there were no known outstanding\ntechnical or editorial issues.  The HTTP/1.1 issues list, along with\nmany representations of this specification including Postscript, Microsoft\nWord, with and without change bars, with or without gzip compression\ncan be found at http://www.w3.org/Protocols/HTTP/Issues/.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-v11-spec-rev-03.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-v11-spec-rev-03.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-v11-spec-rev-03.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-0923219"}, {"subject": "Re: OPTIONS and TRACE vis a vis CGI application", "content": "** Reply to note from Jeffrey Mogul <mogul@pa.dec.com> Thu, 12 Mar 98 12:04:04 PST\n\nCurrently we pass neither OPTIONS nor TRACE to the CGI, though we do\nallow configuration of plug-ins to override any method.\n\nI think the compelling argument here is that you cannont guarantee that\nthe CGI will not cause side-effects.\n\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-0932782"}, {"subject": "Re: URI Reference jg27", "content": "Duplication of most the specification of URI's in the HTTP document is\nclearly the wrong thing for us to do.  They came out a while ago (either\nRev-01 or Rev-02; I don't remember which).\n\nThe area directors would like the URI spec to go to draft in finite time;\nthe HTTP spec is making the presumption it will.  If it doesn't, I'll have\nto issue another draft with the URI BNF back in.\n\nThis isn't an issue until we get interoperability reports in, in any case;\nthat is required for us to progress to draft standard.\n- Jim\n\n\n\n", "id": "lists-012-0940564"}, {"subject": "The last LAST CALL: HTTP/1.1, Authentication, State Managemen", "content": "We have reached the point in the HTTP Working Group where we can make the last\n\"LAST CALL\" on working group documents:\n\n<draft-ietf-http-v11-spec-rev-03.txt>, March 13, 1998, Hypertext Transfer Protocol -- HTTP/1.1\n<draft-ietf-http-authentication-01.txt>, March 13, 1998, HTTP Authentication: Basic and Digest Access Authentication\n<draft-ietf-http-state-man-mec-08.txt>, February 16, 1998, HTTP State Management Mechanism\n\nIn order to actually move these forward as DRAFT STANDARD, we have to document at least\ntwo interoperable implementations of each feature. We are working on completing that task\nin the next few weeks.\n\nYour careful review of the documents and the status of implementations of these protocols\n is requested.\n\nThe working group LAST CALL on these documents will end on Friday, March 27, 1998; barring\nany major issues arising, we will then forward them to the IESG for consideration.\n\nOther documents that were considered in this working group are either being considered\nfor publication as Experimental Protocols, or have been taken up by other working groups\n(primarily HTTPEXT and CONNEG). Once we have completed these, we will be done.\n\nThank you very much for your cooperation.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-0947882"}, {"subject": "Re: The last LAST CALL: HTTP/1.1, Authentication, State Managemen", "content": "Larry Masinter:\n>\n>We have reached the point in the HTTP Working Group where we can make the last\n>\"LAST CALL\" on working group documents:\n\n[...]\n><draft-ietf-http-state-man-mec-08.txt>, February 16, 1998, HTTP State Management Mechanism\n[...]\n\nFor state management, is this a last call for informational status or\nfor something else?\n\n>Larry\n\nKoen.\n\n\n\n", "id": "lists-012-0956546"}, {"subject": "I-D ACTION:draft-ietf-http-ext-mandatory00.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: Mandatory Extensions in HTTP\nAuthor(s): P. Leach, H. Nielsen, S. Lawrence\nFilename: draft-ietf-http-ext-mandatory-00.txt\nPages: 12\nDate: 13-Mar-98\n\n       HTTP is used increasingly in applications that need more facilities\n       than the standard version of the protocol provides, ranging from\n       distributed authoring, collaboration, and printing, to various remote\n       procedure call mechanisms. This document proposes the use of a\n       mandatory extension mechanism designed to address the tension between\n       private agreement and public specification and to accommodate\n       extension of applications such as HTTP clients, servers, and proxies.\n       The proposal associates each extension with a URI[2], and use a few\n       new RFC 822[1] style header fields to carry the extension identifier\n       and related information between the parties involved in an extended\n       transaction.\n\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-ext-mandatory-00.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-ext-mandatory-00.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-ext-mandatory-00.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-0964373"}, {"subject": "I-D ACTION:draft-ietf-http-authentication01.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: HTTP Authentication: Basic and Digest \n                          Access Authentication\nAuthor(s): J. Franks, E. Sink, P. Leach, J. Hostetler, \n                          P. Hallam-Baker, L. Stewart, S. Lawrence, A. Luotonen\nFilename: draft-ietf-http-authentication-01.txt\nPages: 26\nDate: 13-Mar-98\n\n''HTTP/1.0'' includes the specification for a Basic Access Authentication\nscheme. This scheme is not considered to be a secure method of user\nauthentication (unless used in conjunction with some external secure\nsystem such as SSL [5]), as the user name and password are passed over\nthe network as cleartext.\n \nThis document also provides the specification for HTTP's authentication\nframework, the original Basic authentication scheme and a scheme based\non cryptographic hashes, referred to as ''Digest Access Authentication''.\nIt is therefore also intended to serve as a replacement for RFC 2069\n[6].  Some optional elements specified by RFC 2069 have been removed\nfrom this specification due to problems found since its publication;\nother new elements have been added -for compatibility, those new\nelements have been made optional, but are strongly recommended.\n \nLike Basic, Digest access authentication verifies that both parties to a\ncommunication know a shared secret (a password); unlike Basic, this\nverification can be done without sending the password in the clear,\nwhich is Basic's biggest weakness. As with most other authentication\nprotocols, the greatest sources of risks are usually found not in the\ncore protocol itself but in policies and procedures surrounding its use.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-authentication-01.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-authentication-01.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-authentication-01.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-0973671"}, {"subject": "RE: OPTIONS and TRACE vis a vis CGI application", "content": "I think a compelling case can be made for passing OPTIONS to CGI\nprograms.  I always try to check the method in my CGI programs, but I\ndon't doubt that there are a lot of them out there that don't. Still, my\nhard-nosed approach is that CGI programs are really sub-servers and have\njust as much of an obligation to comply with the protocol as any\nservrer. (Okay, okay, maybe \"obligation\" isn't the right word.)\n\nThis obviously isn't an IETF issue, but it seems to me that a reasonable\nstrategy would be for servers to only pass GET and POST requests to a\nCGI program unless some affirmative action is taken (e.g., via\n.htaccess) to indicate that they can properly handle other requests.\n\n==\nGregory Woodhouse <gregory.woodhouse@med.va.gov>\nSan Francisco CIO Field Office - Infrastructure\n+1 415 744 6362\nStandards are wonderful; everyone should have one of their own.\n\n\n> ----------\n> From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n> Sent: Thursday, March 12, 1998 12:04 PM\n> To: http working group\n> Subject: Re: OPTIONS and TRACE vis a vis CGI applications \n> \n> Scott Lawrence wrote:\n>     On Wed, 11 Mar 1998, David W. Morris wrote:\n>     \n>     > It is my sense from list postings that a TRACE or OPTIONS\n> request which\n>     > referenced a specific resource which a server handed to an\n> external\n>     > application such as a CGI program should be handed to that\n> program \n>     > for handling.\n>     > \n>     > That is:\n>     >    OPTIONS /cgi-bin/somescript HTTP/1.1\n>     > or\n>     >    TRACE /cgi-bin/somescript HTTP/1.1\n>     > should be handled by the application which would respond to a\n> GET or HEAD\n>     > of the same resource.\n>     \n>     I would agree for OPTIONS, since it is probably the capabilities\n> of\n>     the CGI or other sub-server that the requestor is interested in\n>     (although my experience has usually been that CGIs often respond\n> as\n>     though the method were GET or POST because the authors don't\n>     check).\n> \n>     I can't think of any reason why TRACE would require the\n>     participation of the resource - it's purpose is just to reflect\n> the\n>     headers as received, and the server itself can do that just fine.\n> \n> Note that the spec, in section 9.1.2 (Idempotent Methods) says\n> \n> Also, the methods OPTIONS and TRACE should\n> have no side effects, and so are inherently idempotent.\n> \n> (I'm not sure if that \"should\" should be a \"SHOULD\" or a \"MUST\",\n> or if it just means \"normally.\")\n> \n> Later, in 9.2 (OPTIONS) the spec says:\n>     This method allows the client to determine the\n>     options and/or requirements associated with a resource, or the\n>     capabilities of a server, without implying a resource action or\n>     initiating a resource retrieval.\n> \n> And in section 9.8 (TRACE) the spec says:\n>     The TRACE method is used to invoke a remote, application-layer\n>     loop-back of the request message.\n> \n> The implication seems to me that the spec doesn't care if the\n> CGI (or other sub-server) is invoked for TRACE or OPTIONS ...\n> but you can't let this invocation cause any side-effects.\n> \n> I'm not sure whether the CGI specification is sufficient\n> to guarantee that if the method is OPTIONS or TRACE, then\n> the CGI script will never cause side-effects.  So it would\n> seem to me that a cautious implementation of TRACE, at least,\n> wouldn't risk invoking the CGI script.\n> \n> E.g., if the client sends\n> \n> TRACE /cgi-bin/action=buy&stock=sunw&quantity=100000 HTTP/1.1\n> Host: stockbroker.com\n> \n> I think it would be a mistake to excecute the CGI script.\n> \n> It's trickier with OPTIONS, since the capabilities of the CGI script\n> might be the target of the OPTIONS request.  I suspect this means\n> that we're not done figuring out how OPTIONS is supposed to work ....\n> \n> -Jeff\n> \n\n\n\n", "id": "lists-012-0983632"}, {"subject": "Upgrading to TLS Within HTT", "content": "Upgrading to TLS Within HTTP\n\n   Rohit Khare, UC Irvine, March 15, 1998\n     _________________________________________________________________\n   \n  0. Motivation\n  \n   At the [1]Washington DC IETF meeting last year, the Applications Area\n   Directors indicated they would like to see a mechanism for applying\n   Transport Layer Security [TLS] within an [2]HTTP connection, at the\n   same port, instead of only being able to recommend a distinct port\n   (443) and scheme (https). The TLS working group has moved forward with\n   an extensive draft on properly implementing https\n   ([3]draft-ietf-tls-https-00), but there is alternate precedent in SMTP\n   and other applications of TLS ([4]draft-hoffman-smtp-ssl,\n   [5]draft-newman-tls-imappop-03, [6]murray-auth-ftp-ssl-00).\n   \n   There has already been extensive debate on the [7]http-wg ,\n   [8]ietf-tls and [9]ietf-apps-tls mailing lists about the advisability\n   of permitting optional 'upgrades' to secure connections within the\n   same channel, primarily focusing on the thread of man-in-the-middle\n   attacks. Our intent here is not to engage in this debate, but merely\n   to document a proposed mechanism for doing either with HTTP. Several\n   applications being built upon HTTP might use this mechanism, such as\n   the [10]Internet Printing Protocol; we look to them for implementation\n   guidance.\n   \n  1. Introduction\n  \n   TLS, a/k/a SSL (Secure Sockets Layer) establishes a private end-to-end\n   connection, optionally including strong mutual authentication, using a\n   variety of cryptosystems. Initially, a handshake phase uses three\n   subprotocols to set up a record layer, authenticate endpoints, set\n   parameters, as well as report errors. Then, there is an ongoing\n   layered record protocol that handles encryption, compression, and\n   reassembly for the remainder of the connection. The latter is intended\n   to be completely transparent. For example, there is no dependency\n   between TLS's record markers and or certificates and HTTP/1.1's\n   chunked encoding or authentication.\n   \n   The need to 'secure' running connections is not merely 'running SSL\n   over port 80', an early challenge for firewall developers answered by\n   Ari Luotonen's [11]ssl-tunneling-02 draft in 1995. The HTTP/1.1 spec\n   reserves CONNECT for future use, deferring to the more recent\n   [12]draft-luotonen-web-proxy-tunneling-00 proposal. This technique\n   perpetuates the concept that security is indicated by a magic port\n   number -- CONNECT establishes a generic TCP tunnel, so port number is\n   the only way to specify the layering of TLS with HTTP (https) or with\n   NTTP (snews).\n   \n   Instead, the preferred mechanism to initiate and insert TLS in an\n   HTTP/1.1 session should be the Upgrade: header, as defined in section\n   14.42 of rev-03. Ideally, TLS-capable clients should add \"Upgrade:\n   TLS/1.0\" to their initial request, and TLS-capable servers may reply\n   with \"101 Switching Protocol\", complete the handshake, and continue\n   with the \"normal\" response to the original request. However, the\n   specification quoth:\n   \n     \"The Upgrade header field only applies to switching\n     application-layer protocols upon the existing transport-layer\n     connection.\"\n     \n   Aside from this minor semantic difference -- invoking TLS indeed\n   changes the existing transport-layer connection -- this is an ideal\n   application of Upgrade. This technique overlays the TLS-request on an\n   HTTP method; requires client-initiation, and allows servers to choose\n   whether or not to make the switch. Like the other examples of\n   TLS-enabled application protocols, the original session is preserved\n   across the TLS handshake; secured communications resumes with a\n   servers' reply.\n   \n   The potential for a man-in-the-middle attack (wherein the \"TLS/1.0\"\n   upgrade token is stripped out) is precisely the same as for mixed\n   http/https use:\n   \n    1. Removing the token is similar to rewriting web pages to change\n       https:// links to http:// links.\n    2. The risk is only present if the server is willing to vend that\n       information over an insecure channel in the first place\n    3. If the client knows for a fact that a server is TLS-compliant, it\n       can insist on it by only connecting as https:// or by only sending\n       an upgrade request on a no-op method like OPTIONS.\n       \n   Furthermore, for clients which do not actively try to invoke TLS,\n   servers can use Upgrade: to advertise TLS compliance, too. Since\n   TLS-compliance should be considered a feature of the server and not\n   the resource at hand, it should be sufficient to send it once, and let\n   clients cache that fact.\n   \n  2. Potential Solution\n  \n   Define \"TLS/x.y\" as a reference to the TLS specification\n   ([13]draft-ietf-tls-protocol-03), with x and y bound to its major and\n   minor version numbers. Section 6.2.1 of the current draft explains why\n   the TLS version would currently be defined as 1.0, not the actual\n   parameters on the wire (which is \"3.1\" for backwards compatibility\n   with SSL3).\n   \n   An HTTP client may initiate an upgrade by sending \"TLS/x.y\" as one of\n   the field-values of the Upgrade: header. The origin-server MAY respond\n   with \"101 Switching Protocols\"; if so it MUST include the header\n   \"Upgrade: TLS/x.y\" to indicate what it is switching to.\n   \n   Servers which can upgrade to TLS MAY include the header \"TLS/x.y\" in\n   an Upgrade response header to inform the client; servers SHOULD\n   include such indication in response to any OPTIONS request.\n   \n   Similarly, servers MAY require clients to switch to TLS first by\n   responding with a new error code \"418: Upgrade Required\", which MUST\n   specify the protocol to be supported, \"TLS/x.y\" in this case.\n   \n   While proxy servers MAY be able to initiate a TLS-secured connection,\n   e.g. the outgoing firewall for a trusted subnetwork, proxy servers\n   MUST NOT remove \"TLS/x.y\" from Upgrade header in the request or\n   response and MUST NOT reply on behalf of the origin server (i.e.\n   \"Cache-control: no-cache\" must be implied).\n   \n  3. Next Steps\n  \n   I could proceed by formalizing Section 2 as an Internet-Draft, but\n   under the jurisdiction of which IETF working group? Furthermore, I do\n   not have access nor personal interest in a TLS-capable client/server\n   pair to experiment with.\n   \n   N.B. I believe this work is completely separate from HTTP-extension\n   work proceeding in the web evolution / http-extension working group.\n   This uses Upgrade for its stated purpose -- to switch to an entirely\n   different protocol -- not to define or modify HTTP methods and\n   semantics.\n   \n   Please watch [14]http://www.ics.uci.edu/~rohit/http-tls for updates of\n   this document and any Internet-Drafts relating to this proposal.\n   \n  4. Acknowledgments\n  \n   Thanks to Paul Hoffman for his work on the STARTTLS command extension\n   for ESMTP. Thanks to Roy Fielding for assistance with the rationale\n   behind Upgrade: and OPTIONS.\n   \n  5. References\n\n   1. http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0495.html\n   2. http://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-rev-03.txt\n   3. http://ds.internic.net/internet-drafts/draft-ietf-tls-https-00.txt\n   4. http://www.imc.org/ietf-apps-tls/draft-hoffman-smtp-ssl\n   5. http://ds.internic.net/internet-drafts/draft-newman-tls-imappop-03.txt\n   6. http://www.consensus.com/ietf-tls/murray-auth-ftp-ssl-00.txt\n   7. http://www.ics.uci.edu/pub/ietf/http/\n   8. http://www.consensus.com/ietf-tls/\n   9. http://www.imc.org/ietf-apps-tls/\n  10. http://www.pwg.org/ipp/index.html\n  11. http://www.consensus.com/ietf-tls/ssl-tunneling-02.txt\n  12. http://ds.internic.net/internet-drafts/draft-luotonen-web-proxy-tunneling-00.txt\n  13. http://www.consensus.com/ietf-tls/tls-protocol-03.txt\n  14. http://www.ics.uci.edu/~rohit/http-tls [soon!]\n\n\n\n", "id": "lists-012-0995393"}, {"subject": "Correct behavior of reverse proxies WRT host field", "content": "I'm wondering what the correct behavior of a reverse proxy is with respect\nto the host field in the request. At least one of the commercially available\nproxies modifies this field as the request is forwarded, setting it to the\naddress of the destination machine behind it.\n\nThis causes problems in the following scenario (and probably elsewhere as\nwell):\n1) user sends a request for /<directory name>\n2) reverse proxy converts the host address in the request header (to an IP\naddress known only on the internal network).\n2) web server (IIS or Netscape) issues a redirect with Location: <internal\nhost>://<directory name>/\n3) browser tries to hit an address that is not available to it.\n\nThe section of RFC 2068 that I believe is relevant (the first paragraph\nunder 14.23) is, to my interpretation, could be ambiguous on the issue.\nSpecifically: \"The Host field value MUST represent the network location of\nthe origin server or gateway given by the original URL\".\n\nWhat should the proxy be doing to the host field (if anything)?\n\nWilliam Wallace                  We rede of ane rycht famouss of renowne, \nInterWorld Corporation      Of worthi blude that ryngis in this regioune, \nPhone: 212-301-2428                 And hensfurth I will my process hald, \nFax:   212-301-2345            Of Wilyham Wallas yhe haf hard beyne tald.\nEmail: williamw@interworld.com        Blind Harry, 15th century minstrel.\n\n\n\n", "id": "lists-012-10004147"}, {"subject": "Re: Semantics of chunkexts", "content": "> Sender: francis@barrayar.appoint.lan\n> From: John Stracke <francis@ecal.com>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Wed, 28 Apr 1999 13:51:14 +0000\n> To: http-wg@hplb.hpl.hp.com\n> Subject: Semantics of chunk-exts?\n> -----\n> I'm implementing Transfer-Encoding: chunked, and I'm trying to\n> figure out what to do if I get a chunk-ext I don't understand\n> (which is to say, any chunk-ext, since RFC-2068 doesn't define\n> any).  Currently I ignore it and log a warning, on the theory\n> that any future extension should be backwards compatible--but\n> 2068 is silent on whether I can expect compatibility, and I'm not\n> sure what kind of compatible extensions might make sense anyway.\n> Any advice?\n> \n\nYes, in general you can expect compatibility.  You must ignore the\nextension and continue.\n\nAt this date, you should be implementing to the draft standard;\npick up the last internet draft (the RFC editor has not yet issued\nthe RFC, though the IESG has approved it).\n- Jim\n\n\n\n", "id": "lists-012-10012473"}, {"subject": "Re: Semantics of chunkexts", "content": "Jim Gettys wrote:\n\n> At this date, you should be implementing to the draft standard;\n> pick up the last internet draft (the RFC editor has not yet issued\n> the RFC, though the IESG has approved it).\n\nThanks; I wasn't aware that there was anything more recent.\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-10021351"}, {"subject": "RE: Correct behavior of reverse proxies WRT host field", "content": "> From: Wallace, William\n> Sent: Wednesday, April 28, 1999 10:53\n\n> I'm wondering what the correct behavior of a reverse proxy is with respect\n> to the host field in the request. At least one of the\n> commercially available\n> proxies modifies this field as the request is forwarded, setting it to the\n> address of the destination machine behind it.\n\nFirst, there isn't any definition for a 'reverse' proxy, so there cannot be\nany rule for how it should behave.\n\nThe functionality you describe sounds more like a gateway to me (an\nhttp-http gateway) in that it is not transparent (it hides from the user\nagent the fact that it is forwarding the request).\nWith respect to the real (internal) origin server, it is behaving as a\nclient, and so its translation of the Host header is appropriate (it is\nsetting it to the origin server as it sees it).\n\n> This causes problems in the following scenario (and probably elsewhere as\n> well):\n> 1) user sends a request for /<directory name>\n> 2) reverse proxy converts the host address in the request header (to an IP\n> address known only on the internal network).\n> 2) web server (IIS or Netscape) issues a redirect with Location: <internal\n> host>://<directory name>/\n\nThis step appears to me to be where the problem actually occurs.  Either the\ngateway server should realize that the redirect is to an internal server and\njust do the redirected request (hiding the fact that it occurred from the\noutside user agent), or it should translate the Location header in the\nresponse that it forwards to that outside user so that the URL is useable\n(presumably to direct the request to itself with whatever recognizer it uses\nto translate to the internal server again).\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10109606"}, {"subject": "HTTP &amp; SHTT", "content": "Hi ,\nI am working  on a paper that deals with HTTP and SHTTP , but i don't know\nwhat\nare the latest versions of both protocols , neither where i can look for \nthem \nall the versions i have found are old ones .\nAnything related to the subject will be helpful .\nBest regards  \n\n\n\n", "id": "lists-012-10118572"}, {"subject": "Re: HTTP &amp; SHTT", "content": "i think for HTTP, the latest version is HTTP 1.1 expires may 1999. you can\nsee it \n  http://www.w3.org\nfor shttp, right now i also searching it too.\n\nOn Wed, 5 May 1999, Asmaa Mourhir wrote:\n\n> Hi ,\n> I am working  on a paper that deals with HTTP and SHTTP , but i don't know\n> what\n> are the latest versions of both protocols , neither where i can look for \n> them \n> all the versions i have found are old ones .\n> Anything related to the subject will be helpful .\n> Best regards  \n> \n\nThanks For All Attention\nRonny\n\n\n\n", "id": "lists-012-10125654"}, {"subject": "Re: Upgrading to TLS Within HTT", "content": "Mea culpa --\n\nSection 13.5.1 clearly indicates that Upgrade is a hop-by-hop header.\nAnd that's a pretty embarassing oversight in my proposal. \n\nIn fact, the behavior *is* hop-by-hop, since intervening proxies have\nto convert to tunnels upon receiving a \"101 Switching to TLS\"\nresponse. Deploying this solution requires updated support in the\nentire chain, even if it's as minor as:\n\n\"Proxies which support TLS-tunneling MUST relay any TLS/x.y\nUpgrade request onward in its subsequent request\"\n\nIn other words, unlike other Upgrade tokens that might be defined, this \none requires you to pass along the request in all cases. \n\nI have updated the online version of that paper accordingly\n     http://www.ics.uci.edu/~rohit/http-tls\nChanges are in red; note also that the new error code 418, Upgrade Required,\nis a general-purpose HTTP response.\n\nRohit\n\n(This discussion concerns HTTP, so that's the one list replies are directed to)\n\n\n\n", "id": "lists-012-1012650"}, {"subject": "Htt", "content": "While parsing some HTTP/1.1 requests and responses using Netscape4.5, I found some contradictions with the protocol (RFC 2068)\n\nfor example:\nUser-Agent: Mozilla/4.51 [en] (Win95; I)\n\nThe - [en] is not permited in this line.\nIs there anywhere I can find these new additions?\nWhat would you suggest I do with such a parameter?\n\nthanks,\n\nHugo.\n\nBTW - \nThere is a problem with the search mechanism of the message list.\nIt is ordered from last to first, so when getting 50 search results, it\ngives the last 13 results (not first), so when clicking the arrow to get \nthe next 13 results, it returns to the full list.\nIf it worked properly, maybe I wouldn't need to ask my question.\n\n\n\n", "id": "lists-012-10133099"}, {"subject": "Re: Htt", "content": "hugo@spearhead.net wrote:\n\n> While parsing some HTTP/1.1 requests and responses using Netscape4.5, I found some contradictions with the protocol (RFC 2068)\n\nI don't think Netscape emits HTTP/1.1; it's 1.0 only (at least, I've never seen a request from it that claimed to be 1.1; there\nmight conceivably be some way to make it do 1.1).  RFC-1945 (HTTP/1.0) describes a similar syntax for User-Agent, but RFC-1945 is\nnot normative; it's Informational, describing the behavior of existing clients rather than attempting to specify a protocol.  And\nit does say:\n\n     Note: Some existing clients fail to restrict themselves to the product token syntax within the User-Agent field.\n\nSo the short answer is, you're out of luck.  If you want to parse HTTP/1.0 User-Agent fields, you have to build in knowledge of\nspecific browsers, cross your fingers, and hope.\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-10140048"}, {"subject": "want to set my pages to expire but can't find info on date format expected ..", "content": "I've looked at the HTTP specifications (on www.w3.org) but couldn't find any\nmention on the datetime format expected when setting the \"expire\" tag.\n\nRandy.\n\n\n\n", "id": "lists-012-10147873"}, {"subject": "RE: want to set my pages to expire but can't find info on date format expected ..", "content": "> I've looked at the HTTP specifications (on www.w3.org) but \n> couldn't find any\n> mention on the datetime format expected when setting the \"expire\" tag.\n\nSection 3.3.1 of draft-ietf-http-v11-spec-rev-06.txt\n(it's the same as the Date header).\n\nIt is better to specify Max-Age though and avoid clock skew problems.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10156127"}, {"subject": "General confusion about CacheControl header", "content": "Just to bring it to people's attention, and perhaps stimulate some\ndiscussion;\n\nI've noticed in dealing with many cache implementors, on this list and in\ndocumentation around the net (including my own) that there's a lot of\nconfusion about the exact meaning of the various Cache-Control HTTP headers.\n\nSome of this may have been cause by differences between RFC2068 and the\nDraft Standard (rev-06), but I think it is more to do with their names.\n\nParticularly:\n\n* must-revalidate (response header), according to rev-06, does not mean that\nthe client (whether browser or cache) must revalidate on every request; it\nmeans that a client cannot take liberties with the object's freshness.\nproxy-revalidate is similar, but only applies to shared caches.\n\n* no-cache as a response header does not mean that the object cannot be\nstored in a cache; rather, it means that it must be revalidated upon every\nrequest. As a request header, it means that a cached copy cannot be used.\nIMHO this is unfortunately named, because of the different meanings in\ndifferent contexts.\n\nThe latest place I've noticed this is the HTTP State Management\ndocumentation.\n\nRegards,\n\n\n\nMark Nottingham\nInternet Project Manager\nMerrill Lynch Australasia (Melbourne)\n\n\n\n", "id": "lists-012-10165540"}, {"subject": "Re: want to set my pages to expire but can't find info on date format expected ..", "content": "Scott Lawrence:\n[...]\n>It is better to specify Max-Age though and avoid clock skew problems.\n\nNote however that just setting max-age will not work for most HTTP/1.0\nclients, which don't know about max-age.  On the current internet I\nwould still recommend the use of Expires, though adding a max-age too\nwon't hurt.\n\n>Scott Lawrence           Director of R & D        <lawrence@agranat.com>\n\nKoen.\n\n\n\n", "id": "lists-012-10173690"}, {"subject": "Re: want to set my pages to expire but can't find info on date format  expected ..", "content": "Koen Holtman wrote:\n\n> Scott Lawrence:\n> [...]\n> >It is better to specify Max-Age though and avoid clock skew problems.\n>\n> Note however that just setting max-age will not work for most HTTP/1.0\n> clients, which don't know about max-age.  On the current internet I\n> would still recommend the use of Expires, though adding a max-age too\n> won't hurt.\n\nNaive question: can you specify Max-Age on a 1.1 response and Expires on a\n1.0 response? Or are you likely to run into problems with proxies giving the\n1.1 response to 1.0 clients? (I know they're not supposed to.)\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-10183098"}, {"subject": "Re: want to set my pages to expire but can't find info on date format expected ..", "content": ">Naive question: can you specify Max-Age on a 1.1 response and Expires on a\n>1.0 response? Or are you likely to run into problems with proxies giving the\n>1.1 response to 1.0 clients? (I know they're not supposed to.)\n\nBoth are 1.x header fields.  If the recipient understands cache-control\nand max-age, then it will ignore Expires.  Otherwise, it will ignore the\ncache-control.  Therefore, it is best to send both.\n\nThere is nothing wrong with sending an HTTP/1.1 response to an HTTP/1.0\nclient.  There are some features of HTTP/1.1 that cannot be used with\nan HTTP/1.0 client, but that's different (and aren't related to caching).\n\n....Roy\n\n\n\n", "id": "lists-012-10192223"}, {"subject": "ContentRange Problem !", "content": "Is there anyone know how to use \"Content-Range:\" and \"Range:\" in the HTTP \nversion 1.1 ? Because i already write like this In Visual Basic 6. and it \ndidn't work :\n\nstrcommand = \"GET /index.html\" + \" HTTP/1.1\" + vbCrLf\nstrcommand = strcommand + \"Host: www.stts.ac.id\" \nstrcommand = strcommand + \"Accept: */*\" + vbCrLf\nstrcommand = strcommand + \"Accept: text/html\" + vbCrLf\nstrcommand = strcommand + \"Content-Range: bytes 0-50/*\" + vbCrLf\nstrcommand = strcommand + vbCrLf\n\nWhen i invoke this program into a webserver that support HTTP 1.1, it\nstill download all the content of the webpage ( not just the first 50\nbytes ). So, i need help from all of you that can help me solve my problem\n!!\n\nThanks \nRonny Widjaja\nInformatic And Engineering School\n\n\n\n", "id": "lists-012-10200355"}, {"subject": "Repost: Upgrading to TLS within HTT", "content": "Upgrading to TLS Within HTTP\n\n   _Rohit Khare_, UC Irvine, March 16, 1998\n   \n     _________________________________________________________________\n                                      \n  0. Motivation\n  \n   At the [1]Washington DC IETF meeting last year, the Applications Area\n   Directors indicated they would like to see a mechanism for applying\n   Transport Layer Security [TLS] within an [2]HTTP connection, at the\n   same port, instead of only being able to recommend a distinct port\n   (443) and scheme (https). The TLS working group has moved forward with\n   an extensive draft on properly implementing https\n   ([3]draft-ietf-tls-https-00), but there is alternate precedent in SMTP\n   and other applications of TLS ([4]draft-hoffman-smtp-ssl,\n   [5]draft-newman-tls-imappop-03, [6]murray-auth-ftp-ssl-00).\n   \n   There has already been extensive debate on the [7]http-wg ,\n   [8]ietf-tls and [9]ietf-apps-tls mailing lists about the advisability\n   of permitting optional 'upgrades' to secure connections within the\n   same channel, primarily focusing on the thread of man-in-the-middle\n   attacks. Our intent here is not to engage in this debate, but merely\n   to document a proposed mechanism for doing either with HTTP. Several\n   applications being built upon HTTP might use this mechanism, such as\n   the [10]Internet Printing Protocol; we look to them for implementation\n   guidance.\n   \n  1. Introduction\n  \n   TLS, a/k/a SSL (Secure Sockets Layer) establishes a private end-to-end\n   connection, optionally including strong mutual authentication, using a\n   variety of cryptosystems. Initially, a handshake phase uses three\n   subprotocols to set up a record layer, authenticate endpoints, set\n   parameters, as well as report errors. Then, there is an ongoing\n   layered record protocol that handles encryption, compression, and\n   reassembly for the remainder of the connection. The latter is intended\n   to be completely transparent. For example, there is no dependency\n   between TLS's record markers and or certificates and HTTP/1.1's\n   chunked encoding or authentication.\n   \n   The need to 'secure' running connections is not merely 'running SSL\n   over port 80', an early challenge for firewall developers answered by\n   Ari Luotonen's [11]ssl-tunneling-02 draft in 1995. The HTTP/1.1 spec\n   reserves CONNECT for future use, deferring to the more recent\n   [12]draft-luotonen-web-proxy-tunneling-00 proposal. This technique\n   perpetuates the concept that security is indicated by a magic port\n   number -- CONNECT establishes a generic TCP tunnel, so port number is\n   the only way to specify the layering of TLS with HTTP (https) or with\n   NTTP (snews).\n   \n   Instead, the preferred mechanism to initiate and insert TLS in an\n   HTTP/1.1 session should be the Upgrade: header, as defined in section\n   14.42 of rev-03. Ideally, TLS-capable clients should add \"Upgrade:\n   TLS/1.0\" to their initial request, and TLS-capable servers may reply\n   with \"101 Switching Protocol\", complete the handshake, and continue\n   with the \"normal\" response to the original request. However, the\n   specification quoth:\n   \n     \"The Upgrade header field only applies to switching\n     application-layer protocols upon the existing transport-layer\n     connection.\"\n     \n   Aside from this minor semantic difference -- invoking TLS indeed\n   changes the existing transport-layer connection -- this is an ideal\n   application of Upgrade. This technique overlays the TLS-request on an\n   HTTP method; requires client-initiation, and allows servers to choose\n   whether or not to make the switch. Like the other examples of\n   TLS-enabled application protocols, the original session is preserved\n   across the TLS handshake; secured communications resumes with a\n   servers' reply.\n   \n   The potential for a man-in-the-middle attack (wherein the \"TLS/1.0\"\n   upgrade token is stripped out) is precisely the same as for mixed\n   http/https use:\n   \n    1. Removing the token is similar to rewriting web pages to change\n       https:// links to http:// links.\n    2. The risk is only present if the server is willing to vend that\n       information over an insecure channel in the first place\n    3. If the client knows for a fact that a server is TLS-compliant, it\n       can insist on it by only connecting as https:// or by only sending\n       an upgrade request on a no-op method like OPTIONS.\n       \n   Furthermore, for clients which do not actively try to invoke TLS,\n   servers can use Upgrade: to advertise TLS compliance, too. Since\n   TLS-compliance should be considered a feature of the server and not\n   the resource at hand, it should be sufficient to send it once, and let\n   clients cache that fact.\n   \n  2. Potential Solution\n  \n   Define \"TLS/x.y\" as a reference to the TLS specification\n   ([13]draft-ietf-tls-protocol-03), with x and y bound to its major and\n   minor version numbers. Section 6.2.1 of the current draft explains why\n   the TLS version would currently be defined as 1.0, not the actual\n   parameters on the wire (which is \"3.1\" for backwards compatibility\n   with SSL3).\n   \n   An HTTP client may initiate an upgrade by sending \"TLS/x.y\" as one of\n   the field-values of the Upgrade: header. The origin-server MAY respond\n   with \"101 Switching Protocols\"; if so it MUST include the header\n   \"Upgrade: TLS/x.y\" to indicate what it is switching to.\n   \n   Servers which can upgrade to TLS MAY include the header \"TLS/x.y\" in\n   an Upgrade response header to inform the client; servers SHOULD\n   include such indication in response to any OPTIONS request.\n   \n   Similarly, servers MAY require clients to switch to TLS first by\n   responding with a new error code \"418: Upgrade Required\", which MUST\n   specify the protocol to be supported. @@ This is a change to 'core'\n   HTTP; if, processwise, it's too difficult to slip in a general-purpose\n   error code, we may have to fall-back to \"418: TLS Required\".\n   \n   Upgrade is a hop-by-hop header (Section 13.5.1), so each intervening\n   proxy which supports TLS MUST also request the same version of TLS/x.y\n   on its subsequent request. Furthermore, any caching proxy which\n   supports TLS MUST NOT reply from its cache when TLS/x.y has been\n   requested (although clients are still recommended to explicitly\n   include \"Cache-control: no-cache\").\n   \n   Note: proxy servers may be able to request or initiate a TLS-secured\n   connection, e.g. the outgoing or incoming firewall of a trusted\n   subnetwork.\n   \n  3. Next Steps\n  \n   I could proceed by formalizing Section 2 as an Internet-Draft, but\n   under the jurisdiction of which IETF working group? Furthermore, I do\n   not have access nor personal interest in a TLS-capable client/server\n   pair to experiment with.\n   \n   N.B. I believe this work is completely separate from HTTP-extension\n   work proceeding in the web evolution / http-extension working group.\n   This uses Upgrade for its stated purpose -- to switch to an entirely\n   different protocol -- not to define or modify HTTP methods and\n   semantics.\n   \n   Please watch [14]http://www.ics.uci.edu/~rohit/http-tls for updates of\n   this document and any Internet-Drafts relating to this proposal.\n   \n  4. Acknowledgments\n  \n   Thanks to Paul Hoffman for his work on the STARTTLS command extension\n   for ESMTP. Thanks to Roy Fielding for assistance with the rationale\n   behind Upgrade: and OPTIONS.\n   \n  5. References\n\nReferences\n\n   1. http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0495.html\n   2. http://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-rev-03.txt\n   3. http://ds.internic.net/internet-drafts/draft-ietf-tls-https-00.txt\n   4. http://www.imc.org/ietf-apps-tls/draft-hoffman-smtp-ssl\n   5. http://ds.internic.net/internet-drafts/draft-newman-tls-imappop-03.txt\n   6. http://www.consensus.com/ietf-tls/murray-auth-ftp-ssl-00.txt\n   7. http://www.ics.uci.edu/pub/ietf/http/\n   8. http://www.consensus.com/ietf-tls/\n   9. http://www.imc.org/ietf-apps-tls/\n  10. http://www.pwg.org/ipp/index.html\n  11. http://www.consensus.com/ietf-tls/ssl-tunneling-02.txt\n  12. http://ds.internic.net/internet-drafts/draft-luotonen-web-proxy-tunneling-00.txt\n  13. http://www.consensus.com/ietf-tls/tls-protocol-03.txt\n  14. http://www.ics.uci.edu/~rohit/http-tls\n\n\n\n", "id": "lists-012-1020413"}, {"subject": "Re: ContentRange Problem !", "content": "In message <Pine.BSF.4.10.9905291041580.9863-100000@elang.stts.edu>, Kid writes\n:\n>Is there anyone know how to use \"Content-Range:\" and \"Range:\" in the HTTP \n>version 1.1 ? Because i already write like this In Visual Basic 6. and it \n>didn't work :\n>\n>strcommand = \"GET /index.html\" + \" HTTP/1.1\" + vbCrLf\n>strcommand = strcommand + \"Host: www.stts.ac.id\" \n>strcommand = strcommand + \"Accept: */*\" + vbCrLf\n>strcommand = strcommand + \"Accept: text/html\" + vbCrLf\n>strcommand = strcommand + \"Content-Range: bytes 0-50/*\" + vbCrLf\n>strcommand = strcommand + vbCrLf\n>\n>When i invoke this program into a webserver that support HTTP 1.1, it\n>still download all the content of the webpage ( not just the first 50\n>bytes ). So, i need help from all of you that can help me solve my problem\n\nThat's because Content-Range is what you get in the response, not how\nyou request a range.  To do that, use the Range header field, like\n\nstrcommand = \"GET /index.html\" + \" HTTP/1.1\" + vbCrLf\nstrcommand = strcommand + \"Host: www.stts.ac.id\" \nstrcommand = strcommand + \"Accept: */*\" + vbCrLf\nstrcommand = strcommand + \"Accept: text/html\" + vbCrLf\nstrcommand = strcommand + \"Range: bytes=0-50\" + vbCrLf\nstrcommand = strcommand + vbCrLf\n\n....Roy\n\n\n\n", "id": "lists-012-10208067"}, {"subject": "Thanks ( Content Range ) !", "content": "Thanks for all of you that already help me with your email !!\n\nRonny Widjaja\nInformatic And Engineering School\n\n\n\n", "id": "lists-012-10216362"}, {"subject": "Server Which Didn't Send ContentLength !", "content": "hi, all\nNow i have a really serious problem. When i invoke a GET command to get a\nfile from one webserver, that webserver will send the header for that file \nwith the entity for that file too. In my program i can check the\ntotalbytes that i already received. If that totalbytes is greater or equal\nto the content length that i already have ( when i got the header ), i can\nstop the download process. But what about server that didn't send a\ncontent-length, How can i detect that my download process is already done\nbecause i didn't get content-length for this remote server ??\n\nThanks For All Attention\nRonny\nInformatic And Engineering School\n\n\n\n", "id": "lists-012-10223301"}, {"subject": "Server Didn't Send ContentLength !", "content": "hi, all\nNow i have a really serious problem. When i invoke a GET command to get a\nfile from one webserver, that webserver will send the header for that file\nwith the entity for that file too. In my program i can check the\ntotalbytes that i already received. If that totalbytes is greater or equal\nto the content length that i already have ( when i got the header ), i can\nstop the download process. But what about server that didn't send a\ncontent-length, How can i detect that my download process is already done\nbecause i didn't get content-length for this remote server ??\n\nThanks For All Attention\nRonny\nInformatic And Engineering School \n\n\n\n", "id": "lists-012-10230922"}, {"subject": "Re: Server Which Didn't Send ContentLength !", "content": "Dear sir:\n\nAs I know, Webserver after sending the answer\nwill close the connection. So the end is clear\n(disconnecting !)\n\nMasoud.\nStatistical Center of Iran\n\n\n\n______________________________________________________\nGet Your Private, Free Email at http://www.hotmail.com\n\n\n\n", "id": "lists-012-10238332"}, {"subject": "Http Programming In Proxy !", "content": "May be i am just a beginner in this topic, so may be my question is so\nfunny. \nI make a http program with Visual Basic 6 to download a resource from a\nremote server. I use GET, HEAD and the other function inside HTTP\nprogramming. But now i want to change my program ( actually to give one\nadvantage again ). And that advantage is : i can download a resource from\nproxy server. \nThe things that i want to ask is :\nIs there anything different that i have to change or add between\nprogramming without proxy and with proxy ?? And also is there any\ndocumentation or may be any site that i can visit to learn about this\ntopic, because i didn't found this topic in any RFC.\n\nThanks For All Attention\nRonny\nInformatic And Engineering School\n\n\n\n", "id": "lists-012-10245187"}, {"subject": "Chunk size definition problem", "content": "Hi all,\n\nLooking at the definition of Chunked tranfer coding\nin draft-ietf-http-v11-rev-06.txt (section 3.6.1), one\nreads:\n\n\"The chunk-size field is a string of hex digits indicating the size\nof the chunk.\"\n\nShouldn't this read \"the size of the chunk-data\"?  The chunk\nitself is defined as\n\n       chunk          = chunk-size [ chunk-extension ] CRLF\n                        chunk-data CRLF\n\nHowever, looking at how Web servers (Apache, IIS) implement chunked\ntransfer coding, it realy looks like chunk-size represents the size\nof the chunk-data itself...\n\nAm I missing something?\n\nThanks for any clarification,\n===\nWham! <wham_bang@yahoo.com>\n\n\n\n_________________________________________________________\nDo You Yahoo!?\nGet your free @yahoo.com address at http://mail.yahoo.com\n\n\n\n", "id": "lists-012-10252301"}, {"subject": "RE: Chunk size definition problem", "content": "I think this is a typo, and that we have the opportunity to correct\nit soon. If there's no objection, we'll try to make this correction\nas the RFC Editor processes the document. (We're almost next\non the RFC editor queue.)\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n> Looking at the definition of Chunked transfer coding\n> in draft-ietf-http-v11-rev-06.txt (section 3.6.1), one\n> reads:\n> \n> \"The chunk-size field is a string of hex digits indicating the size\n> of the chunk.\"\n> \n> Shouldn't this read \"the size of the chunk-data\"?  The chunk\n> itself is defined as\n> \n>        chunk          = chunk-size [ chunk-extension ] CRLF\n>                         chunk-data CRLF\n> \n> However, looking at how Web servers (Apache, IIS) implement chunked\n> transfer coding, it realy looks like chunk-size represents the size\n> of the chunk-data itself...\n> \n> Am I missing something?\n> \n> Thanks for any clarification,\n\n\n\n", "id": "lists-012-10260228"}, {"subject": "RE: Chunk size definition problem", "content": "At 11.32 -0700 1999-06-02, Larry Masinter wrote:\n>If there's no objection, we'll try to make this correction\n>as the RFC Editor processes the document. (We're almost next\n>on the RFC editor queue.)\n\nThat should be possible.\n\nJust pass detailed editorial notes to the RFC editor, and I am sure \nthey can handle this -- hopefully.\n\n    paf\n\n\n\n", "id": "lists-012-10269462"}, {"subject": "Caching input  known problem", "content": "Hi,\nI am working in the IETF Web Replication and Caching working\ngroup to create a report describing the current Known Problems with\nproxy and cache technologies.  The purpose is to document the current\nstate of technology and understand where further work needs to be\napplied.  This is a companion to the caching research issues document\nthat Joe Touch is editing.\n\nThe Known Problems draft is not something I am writing; I have\nvolunteered to edit the document.  I do not believe any one person can\n(or indeed, should) be responsible for describing all known problems\nwith web proxying, caching and replication.  Instead, this must be a\ncollaborative process within the Internet community, drawing upon the\nexperience of researchers and especially practitioners in the field.\nThe I-D will acknowledge each contributor (unless he or she wishes to\nremain anonymous).\n\nI am sending this letter to solicit your input.  I have created\na problem template (based upon the TCP-IMPL and cache research issues\ntemplates) to help frame the known problems.  If you can please use the\ntemplate to describe the issues; if not, it is still more important to\nget the issues documented than to have them correctly formatted.  Let me\nknow if you feel the template needs to be improved, too.\n\nPlease take a look at the template included below and try to\nsend me your input by Friday 9th July.  I will summarize the input I\nhave received by that date at the IETF WREC working group meeting the\nfollowing week.  The WREC working group will distribute meeting minutes\nto the wrec@cs.utk.edu mailing list.\n\nRemember, this activity cannot be successful without input from\nthe community.  Lack of input implies there are no perceived problems.\nIf you feel otherwise, speak up so we can work towards resolving them.\nThank you!\n\n     --       jad       --\n\nJohn Dilley <jad@hpl.hp.com>\nHewlett-Packard Laboratories\n1501 Page Mill Road MS 1U-17\nPalo Alto, CA  94304  // USA\n\n\nKnown Problems Working Draft -- on the web:\n\nhttp://www.hpl.hp.com/personal/John_Dilley/caching/draft-wrec-known-prob-00.html\n\n\nText-only Template -- \n\nName\n     A short, descriptive name.\nClassification\n     Please choose a standard category if possible (see list above).\nDescription\n     A succinct definition of the problem.\nSignificance (High, Medium, Low)\n     High Medium Low. Supplimental information\nImplications\n     Why the problem is viewed as a problem.\nIndications\n     How to detect the presence of the problem.\nSolution(s)\n     Solutions that permanently fix the problem, if such are known.\nWorkaround\n     Practical workaround if no solution is available or usable.\nContact\n     email@host.org.domain (Your Name) or Anonymous\n\n================================================================\n\nTemplate Instructions\n\nName\n     A short, descriptive name (3-5 words) name associated with the problem.\n     In this memo, the name is used as a subsection heading.\nClassification\n     Problems are grouped into categories of similar problems for ease of\n     reading of this memo. Choose the category that best describes the\n     problem. Suggested categories are as follows:\n        o Specification: the spec is ambiguous, incomplete, or incorrect\n        o Implementation: the implementation of the specification is\n          incorrect\n          ------------------------------------------------------------------\n        o Performance: perceived latency or server demand exceed reasonable\n          bounds\n        o Administration: care and feeding of caches is or causes a problem\n        o Network: impact on local, global networks from cache behavior\n        o Security: privacy, authentication concerns\n     This is the first draft of this memo. The classification structure is\n     in revision. In the published drafts of the memo the classification\n     structure should be fixed but may be revised from time to time.\nDescription\n     A definition of the problem, succinct but including necessary\n     background information.\nSignificance (High, Medium, Low)\n     May include a brief summary of the environments for which the problem\n     is significant.\nImplications\n     Why the problem is viewed as a problem. What inappropriate behavior\n     results from it? This section should substantiate the magnitude of any\n     problem indicated with High significance.\nIndications\n     How to detect the presence of the problem. This may include references\n     to one or more substantiating documents that demonstrate the problem.\n     This should include the network configuration that led to the problem\n     such that it can be reproduced. Problems that are irreproduceable will\n     not appear in this memo.\nSolution(s)\n     Solutions that permanently fix the problem, if such are known. For\n     example, what version of the software does not exhibit the problem?\n     Indicate if the solution is accepted by the community, one of several\n     solutions pending agreement, or open possibly with experimental\n     solutions.\nWorkaround\n     Practical workaround if no solution is available or usable. The\n     workaround should have sufficient detail for someone experiencing the\n     problem to get around it.\nContact\n     Contact name and email address of the person who supplied the\n     information for this section. If you would prefer to remain anonymous\n     the editor's name will appear here instead, but we believe in credit\n     where credit is due.\n\n\n\n", "id": "lists-012-10277275"}, {"subject": "To Know Filesize Without ContentLength !", "content": "I make a program to know every filesize from remote server. But i have a\nproblem when there is many remote server that didn't send Content-Length.\nThe thing that i want to ask is : \n\" Is there any way to know how many is the filesize for a file when the\nremote server didn't send Content-Length header ?? And also is there any\nfile from remote server that i couldn't get the filesize with all method\n?? \" \n\n\nThanks For All Attention\nRonny\n\n\n\n", "id": "lists-012-10290000"}, {"subject": "Password change via HTT", "content": "This seems like such an obvious issue that it's hard to believe it's\nnot addressed by the existing protocol but, for the life of me, I can't\nsee it, so I'll \"open my mouth and remove all doubt\" :\n\nMany password oriented security systems have password expiration mechanisms\nthat force users to change passwords on a semi-regular basis. When a user's\npassword has not been changed within a system-specific time interval the\npassword is considered \"expired\" and the user is prompted for a new password\nthe next time she/he logs on. Furthermore, many security systems allow a user\nto change his or her logon password at any time. Yet both of these features\nare, at best, clunky to implement via HTTP.\n\nOur current hack for allowing users to change password, is to allow users\nusing our software to change passwords by specify a new password after their\nexisting password by separating old password from new password with a colon\n(:). This deeply unsatisfactory on many accounts.\n\n1. When our server accepts the old-pass:new-pass password, the browser sends\n   this same pair the next time it requests a URL from the same server. But,\n   on subsequent attempts, old-pass is no longer valid. Unless we do something\n   fancy, this results in a 401 going back to the browser, forcing the user\n   to just enter new-pass. Yes, it might be possible to check if new-pass is\n   now the current password but not until after old-pass is rejected and a\n   password violation logged (we can't all control the web server *and* the\n   access control facility). Perhaps we can keep a history of successful password\n   changes but is a lot of work if one wishes to make this history survive\n   server outages. From an aesthetic point of view, it's just plain grody\n   that the browser is sending old-pass:new-pass on every request to the\n   server.\n\n2. If new-pass is invalid (not enough characters, contains an invalid\n   character, whatever), we have no decent way of identifying the problem\n   to the user. Perhaps if browsers would display the text of the 401 message\n   in the password pop-up windows ? Doing so, however, would produce some\n   serious backword compatibility problems. We can also put up a page\n   explaining the problem with an indication that the user should hit\n   \"reload\" however that's done on whatever browser the user is using\n   (mumble, mumble) and then correct the problem. Unfortunately, when the\n   user hits reload, the browser will believe that the user had enterred\n   a valid password (after all, it got an HTML page didn't it ?) so will\n   re-send it producing the same page indicating the same problem.\n   Argggggghhhhh ! Re-directing with a back link produces the same\n   result. So what's a poor server to do ? Send an explanation page\n   every other time, alternating with 401's ? Horrors ! Another alternative\n   is to provide a password change page but this can be a major pain\n   depending on the access control facility and would result in the\n   loss of any POST data unless many hoops are jumped through.\n\n3. There is no decent way to tell the user that his/her password has\n   expired and a new one is required. Once again, the current technology\n   is to send down a page that explains the situation to the user,\n   explaining that the user must hit reload (mumble, mumble) and then\n   respond to the password expiry. But then you're back to the reload\n   loop problem outlined in 2.\n\nThere are other problems with the current mechanisms but these 3 are\nthe major ones.\n\nClearly, I must be missing something because dealing with expired\npasswords can't possibly be this bad, can it ? It certainly seems that\nthe Digest Access Authentication Scheme won't help much because sending\nany passwords (new or old) in clear text is antithetical to its\noperation. But a new password *must* be sent in clear text (though\nadvisedly over an SSL connection) unless one can control the encryption\nof the passwords on the access control facility and change it to\nsomething the browser understands (we don't and can't).\n\nOn the off chance that I'm not missing anything, I would think a new\nresponse header field such as \"WWW-Authnewpass:\" with values such as\n\"disallow\", \"allow\", \"require\" would do the trick. Perhaps a new\nresponse message indicating \"Invalid new password\" would be useful\nto notify the browser of a common error. The browser would need a\nnew request header (\"Authnewpass:\" ?) that contains any new password\nprovided. If the new password appears to be accepted, the browser\nwould use the new password in subsequent accesses to the server.\nPerhaps any browser that supports \"Authnewpass:\" would send it every\ntime followed by a null string to indicate that it supports this\nfacility so that the server can drop back to old/clunky mode of\noperation, otherwise.\n\nYes, yes, I understand HTTP 1.1 is frozen, so I suppose a reasonable\nresponse is to tell me to go bark up another tree. I'm actually kinda\nhoping someone will respond and tell me how stupid I am and how\npassword expirations are easily and elegantly handled by the current\nprotocols (a little public humiliation never hurt anyone).\n\nSorry about the long post and thanks for your time.\n\nAlex Kodat\nSirius Software\nCambridge, MA\n\n\n\n", "id": "lists-012-10296904"}, {"subject": "Re: Password change via HTT", "content": "Some history.\n\n    The HTTP Authentication metchanism was invented back in 1993. The\nprinciple constraint on the design was the patent encumberances on all known\nforms of public key cryptography. I would much have preferred to have been\nable to propose a public key based scheme at that time.\n\n    Today the Diffie Helleman patent has expired and the RSA patent will\nexpire in very short order. There is no reason to propose another password\nbased scheme. We should look to phase out the use of passwords entirely -\nexcept for passphrases used to secure private keys.\n\n    The PKIX group has proposed a complete set of standards for use and\nmanagement of PKI. Commercial products provide a complete infrastructure for\ndeployment in enterprises both large and small.\n\n        Phill\n\n\n\n", "id": "lists-012-10308961"}, {"subject": "Re: Password change via HTT", "content": "In-Reply-To:  Message of Sat, 12 Jun 1999 23:24:20 -0400 from <hallam@ai.mit.>\n\nWhile I wholeheartedly agree that PKCS is *far* superior to password based\nschemes, I suspect passwords will be around for some time to come. The idea\nthat every workstation out there will be equipped with smart-card readers\nand all users will be walking around with smart cards that contain their\npersonal client certificate is lovely but not one I think we're likely\nto see everywhere for many years to come.\n\nPassword based systems are just too easy to manage and can be trivially\nused with existing legacy systems. It's kinda like the https vs. shttp\nissue or electronic wallets vs. credit card numbers over SSL: the\nobviously superior technology is adopted slowly because the easier to\nmanage technology is considered \"good enough\" (BTS) and has virtually\nno administrative overhead whereas the newer superior technology has\nconsiderable administrative overhead.\n\nJust a prediction that 10 years from now people will still be using\npasswords with we-based applications and will still be sending credit\ncard numbers over SSL. If there's a way I can help our customers using\npassword based systems I'd like to be able to do so.\n\nAlex Kodat\nSirius Software\nCambridge, MA\n\n\n\n", "id": "lists-012-10316772"}, {"subject": "Re:      Re: Password change via HTT", "content": ">While I wholeheartedly agree that PKCS is *far* superior to password based\n>schemes, I suspect passwords will be around for some time to come. The idea\n>that every workstation out there will be equipped with smart-card readers\n>and all users will be walking around with smart cards that contain their\n>personal client certificate is lovely but not one I think we're likely\n>to see everywhere for many years to come.\n\nSmartcards are not a requirement for PKI. I have installed many PKIs\nand very few use smartcards.\n\n\n>Password based systems are just too easy to manage and can be trivially\n>used with existing legacy systems.\n\nActually management of password systems in a large enterprise is far\nfrom easy.\n\nManagement of passwords in a small system is no simpler than\nlocally issued certificates.\n\nEither way, I don't think that the HTTP working group should spend\nany more time trying to make passwords work when applications such\nas SSH have demonstrated that public key based systems are more\nfeasible and easier to manage.\n\n\n        Phill\n\n\n\n", "id": "lists-012-10325220"}, {"subject": "Re:      Re: Password change via HTT", "content": "In-Reply-To:  Message of Sun, 13 Jun 1999 19:21:13 -0400 from <hallam@ai.mit.>\n\nPromise I'll take this thread off-line after this note unless anyone\nelse is at all interested.\n\n> Smartcards are not a requirement for PKI. I have installed many PKIs\n> and very few use smartcards.\n\nYes, but in many situations involving \"public\"/shared PC's there doesn't\nseem to be much of an alternative. Walking around with diskettes containing\nclient certificates won't fly at a lot of places even if they are password\nprotected. Not only are the diskettes easily copied, but they'll often\nsit on networked PC's that have virtually no real security built into\nthem (or at the very least configured). Now the protection of the client\ncerts depends on passwords that are probably easily guessed in many instances.\n\n> Actually management of password systems in a large enterprise is far\n> from easy.\n>\n> Management of passwords in a small system is no simpler than\n> locally issued certificates.\n\nYes, but in either case it's already considered a \"solved\" problem\nor at least one that's relatively under control. Password systems\nare the devil one already knows and the devil one is stuck with anyway\nwhile supporting legacy systems. While I'm sure a lot of forward\nlooking companies are using PKI there are a lot more that aren't.\nI notice the W3C members only archives are password protected (and\nnot even over SSL it seems ?).\n\n> Either way, I don't think that the HTTP working group should spend\n> any more time trying to make passwords work when applications such\n> as SSH have demonstrated that public key based systems are more\n> feasible and easier to manage.\n\nMine is not to say what the working group should be working on\nand once again profound apologies if I've wasted anyone's time on\nthis. I just wanted to point out that since HTTP *does* support\npassword authentication there *seemed* to be a bit of a hole in\nits support of password change/expiration. I was hoping (and am\nstill hoping) someone would point out how that hole is not there.\nI also still believe that people will be using centrally stored\npasswords at least 10 years from now (Y2K programmers now know that\n10 years is not such a long time, after all).\n\nAssuming I'm not missing anything, my recommendation to our\ncustomers would be PKI (though, they'll laugh at me: we have a hospital\ncustomer with 10,000 more or less public PC's with twice the number\nof users), use some change password HTML page technology with all\nits incumbent problems or use a hacked version of the publicly\navailable Netscape Communicator that supports the sort of headers\nI had suggested in my first note. I suspect most will go with the\nsecond alternative as being the least attractive but requiring the\nleast work and/or money. Less work almost always seems to trump\nmost other considerations.\n\nThanks again for your comments.\n\nAlex Kodat\nSirius Software\nCambridge, MA\n\n\n\n", "id": "lists-012-10333533"}, {"subject": "RE: Re: Password change via HTT", "content": "Unfortunately, there are problems with certificate security.\nShamir recently demonstrated how easy it is find the private key\nin a PC because of different entropy of the objects.\n\nAlso, how can I be sure that the \"client\" serving up the \ncertificate is the endpoint? A toolkit like WIDL would appear to\nprovide a screen scraping capability for http which effectively\ncreates a potential proxy, of which I, at the server end have\nno knowledge. Even if I have a cryptographically secure tunnel,\nand have a certificate, how do I know that someone hasn't added\ntheir own plumbing to the client?\n\nThere are times when it pays to use both belt and suspenders ...\nand even that may not be enough.\n\nSteve\n\n> -----Original Message-----\n> From: Alex Kodat [mailto:ALEX@SIRIUS.sirius-software.com]\n> Sent: Sunday, June 13, 1999 6:10 PM\n> To: hallam@ai.mit.edu\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: Re: Password change via HTTP\n> \n> \n> In-Reply-To:  Message of Sun, 13 Jun 1999 19:21:13 -0400 from \n> <hallam@ai.mit.>\n> \n> Promise I'll take this thread off-line after this note unless anyone\n> else is at all interested.\n> \n> > Smartcards are not a requirement for PKI. I have installed many PKIs\n> > and very few use smartcards.\n> \n> Yes, but in many situations involving \"public\"/shared PC's \n> there doesn't\n> seem to be much of an alternative. Walking around with \n> diskettes containing\n> client certificates won't fly at a lot of places even if they \n> are password\n> protected. Not only are the diskettes easily copied, but they'll often\n> sit on networked PC's that have virtually no real security built into\n> them (or at the very least configured). Now the protection of \n> the client\n> certs depends on passwords that are probably easily guessed \n> in many instances.\n> \n> > Actually management of password systems in a large enterprise is far\n> > from easy.\n> >\n> > Management of passwords in a small system is no simpler than\n> > locally issued certificates.\n> \n> Yes, but in either case it's already considered a \"solved\" problem\n> or at least one that's relatively under control. Password systems\n> are the devil one already knows and the devil one is stuck with anyway\n> while supporting legacy systems. While I'm sure a lot of forward\n> looking companies are using PKI there are a lot more that aren't.\n> I notice the W3C members only archives are password protected (and\n> not even over SSL it seems ?).\n> \n> > Either way, I don't think that the HTTP working group should spend\n> > any more time trying to make passwords work when applications such\n> > as SSH have demonstrated that public key based systems are more\n> > feasible and easier to manage.\n> \n> Mine is not to say what the working group should be working on\n> and once again profound apologies if I've wasted anyone's time on\n> this. I just wanted to point out that since HTTP *does* support\n> password authentication there *seemed* to be a bit of a hole in\n> its support of password change/expiration. I was hoping (and am\n> still hoping) someone would point out how that hole is not there.\n> I also still believe that people will be using centrally stored\n> passwords at least 10 years from now (Y2K programmers now know that\n> 10 years is not such a long time, after all).\n> \n> Assuming I'm not missing anything, my recommendation to our\n> customers would be PKI (though, they'll laugh at me: we have \n> a hospital\n> customer with 10,000 more or less public PC's with twice the number\n> of users), use some change password HTML page technology with all\n> its incumbent problems or use a hacked version of the publicly\n> available Netscape Communicator that supports the sort of headers\n> I had suggested in my first note. I suspect most will go with the\n> second alternative as being the least attractive but requiring the\n> least work and/or money. Less work almost always seems to trump\n> most other considerations.\n> \n> Thanks again for your comments.\n> \n> Alex Kodat\n> Sirius Software\n> Cambridge, MA\n> \n> \n\n\n\n", "id": "lists-012-10343883"}, {"subject": "Re: Password change via HTT", "content": "While I'm sure this thread is off-topic for http-wg,\nI agree with Alex that passwords are here to stay,\nand I agree with Phillip that PKI is too.\n\nThe big question is:  How will passwords be used?\n\nI sincerely doubt that the dominant form will be\nPINs for smart cards, and I know we can do better\nthan simple local key files encrypted with a \npassword/phrase.\n\nFor another vision of strong password + PKI systems, visit\n<http://www.IntegritySciences.com>.\n\n-- dpj\n\n\nAt 09:55 AM 6/13/99 EDT, Alex Kodat wrote:\n>In-Reply-To:  Message of Sat, 12 Jun 1999 23:24:20 -0400 from\n<hallam@ai.mit.>\n>\n>While I wholeheartedly agree that PKCS is *far* superior to password based\n>schemes, I suspect passwords will be around for some time to come. The idea\n>that every workstation out there will be equipped with smart-card readers\n>and all users will be walking around with smart cards that contain their\n>personal client certificate is lovely but not one I think we're likely\n>to see everywhere for many years to come.\n>\n>Password based systems are just too easy to manage and can be trivially\n>used with existing legacy systems. It's kinda like the https vs. shttp\n>issue or electronic wallets vs. credit card numbers over SSL: the\n>obviously superior technology is adopted slowly because the easier to\n>manage technology is considered \"good enough\" (BTS) and has virtually\n>no administrative overhead whereas the newer superior technology has\n>considerable administrative overhead.\n>\n>Just a prediction that 10 years from now people will still be using\n>passwords with we-based applications and will still be sending credit\n>card numbers over SSL. If there's a way I can help our customers using\n>password based systems I'd like to be able to do so.\n>\n>Alex Kodat\n>Sirius Software\n>Cambridge, MA\n\n\n\n", "id": "lists-012-10356627"}, {"subject": "Re: Password change via HTT", "content": "Steve Parker wrote:\n> \n> Unfortunately, there are problems with certificate security.\n> Shamir recently demonstrated how easy it is find the private key\n> in a PC because of different entropy of the objects.\n\nErr? And who leaves their private key lying around unencrypted?\n\n> Also, how can I be sure that the \"client\" serving up the\n> certificate is the endpoint? A toolkit like WIDL would appear to\n> provide a screen scraping capability for http which effectively\n> creates a potential proxy, of which I, at the server end have\n> no knowledge. Even if I have a cryptographically secure tunnel,\n> and have a certificate, how do I know that someone hasn't added\n> their own plumbing to the client?\n\nWhy do you care?\n\n> There are times when it pays to use both belt and suspenders ...\n> and even that may not be enough.\n\nWhat were you planning to add to certs+crypto to make it more secure?\n\nCheers,\n\nBen.\n\n--\nhttp://www.apache-ssl.org/ben.html\n\n\"My grandfather once told me that there are two kinds of people: those\nwho work and those who take the credit. He told me to try to be in the\nfirst group; there was less competition there.\"\n     - Indira Gandhi\n\n\n\n", "id": "lists-012-10365714"}, {"subject": "Document Action: The Safe Response Header Field to Experimenta", "content": "The IESG has approved the Internet-Draft 'The Safe Response Header\nField' <draft-holtman-http-safe-03.txt> as an Experimental Protocol.\nThis document is the product of the HyperText Transfer Protocol Working\nGroup.  The IESG contact persons are Harald Alvestrand and Keith\nMoore.\n\n\nNote to RFC Editor:\n\nThe text in the fourth paragraph of Section 3 currently reads:\n\nthey must be assumed unsafe by default. This document adds a\nmechanism to HTTP, the Save header field, for telling if a POST\nrequest is safe.\n\nThe text should reference a Safe header field (not a Save header field).\n\n\n\n", "id": "lists-012-1036971"}, {"subject": "RE: Password change via HTT", "content": "> From: Ben Laurie [mailto:ben@algroup.co.uk]\n> Err? And who leaves their private key lying around unencrypted?\nThat's the question I would have asked myself until recently.\nDoesn't help (well, just a slight delay) - see Shamir and van\nSomeren's paper \"Playing hide and seek with stored keys\", delivered\nto this year's Financial Cryptography conference: \"We describe efficient\nalgebraic attacks which can locate secret RSA keys in long bit strings,\nand more general statistical attacks which can find arbitrary cryptographic\nkeys embedded in large programs. These techniques can be used to apply\nlunchtime attacks on signature keys used by financial institutes, or to\ndefeat authenticode mechanisms in software packages.\" Shamir is the S in\nRSA.\nUseful tips on how to recover cryptographic keys from Windows NT can be\nfound at Peter Gutmann's pages:\nhttp://www.cs.auckland.ac.nz/~pgut001/index.html\n>\n> > Also, how can I be sure that the \"client\" serving up the\n> > certificate is the endpoint? A toolkit like WIDL would appear to\n> > provide a screen scraping capability for http which effectively\n> > creates a potential proxy, of which I, at the server end have\n> > no knowledge. Even if I have a cryptographically secure tunnel,\n> > and have a certificate, how do I know that someone hasn't added\n> > their own plumbing to the client?\n>\n> Why do you care?\n\nIf I trust the certificate alone, then I am mistakenly trusting a\nprogram, not an individual ... then I have delegated authentication\nto that program.\n\n> What were you planning to add to certs+crypto to make it more secure?\nI don't have a perfect answer. I would at least add passwords. And not\nuse NT.\n\nUnfortunately, there is no plateau with security, and no soundbyte\nsolution.\n\n> Cheers,\n>\n> Ben.\n>\nRegards,\n\nSteve\n\n\n\n", "id": "lists-012-10374651"}, {"subject": "Re: Password change via HTT", "content": "Steve Parker wrote:\n\n> Doesn't help (well, just a slight delay) - see Shamir and van\n> Someren's paper \"Playing hide and seek with stored keys\", delivered\n> to this year's Financial Cryptography conference: \"We describe efficient\n> algebraic attacks which can locate secret RSA keys in long bit strings,\n> and more general statistical attacks which can find arbitrary cryptographic\n> keys embedded in large programs.\n\nI take it this requires access to the process's memory space?\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-10384740"}, {"subject": "RE: Password change via HTT", "content": "> I take it this requires access to the process's memory space?\nSince this usually gets swapped out at some point, the swap file\nwould seem a much easier point of attack. How easy this is depends\non implementations. Windows would appear to be easy. Systems with\nprotected memory space such as AS/400 or various specialized trusted\nsystems would appear to be immune - but Shamir was exploring the\npossibility of \"lunchtime attacks\" on client systems, which is\nvirtually synonymous with Windows.\n\nSteve\n\n> -----Original Message-----\n> From: francis@ariel.local.thibault.org\n> [mailto:francis@ariel.local.thibault.org]On Behalf Of John Stracke\n> Sent: Tuesday, June 15, 1999 8:35 AM\n> To: http-wg@hplb.hpl.hp.com\n> Subject: Re: Password change via HTTP\n> \n> \n> Steve Parker wrote:\n> \n> > Doesn't help (well, just a slight delay) - see Shamir and van\n> > Someren's paper \"Playing hide and seek with stored keys\", delivered\n> > to this year's Financial Cryptography conference: \"We \n> describe efficient\n> > algebraic attacks which can locate secret RSA keys in long \n> bit strings,\n> > and more general statistical attacks which can find \n> arbitrary cryptographic\n> > keys embedded in large programs.\n> \n> I take it this requires access to the process's memory space?\n> \n> --\n> /=============================================================\\\n> |John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n> |francis@ecal.com|============================================|\n> |Chief Scientist | NT's lack of reliability is only surpassed |\n> |eCal Corp.      |  by its lack of scalability. -- John Kirch |\n> \\=============================================================/\n> \n> \n> \n> \n\n\n\n", "id": "lists-012-10392662"}, {"subject": "Asynchronous updates/notification", "content": "For some long-running operations, it would be nice if a server could send asynchronous notifications to a client, so the client doesn't have to poll (or refresh periodically) to get progress updates.  I'm wondering if something similar to the 100-Continue mechanism could be used for this.  Maybe with a new status code like 101-Notification.  A client could send an Expect (or Accept or similar) header to indicate willingness to accept notifications.  A server could send any number of interim 101-Notification responses, with message-bodies, before sending a final status code.  101s would not be sent until the entire request arrives.  \n\nComments?\n\n    -Carl Kugler\n\n\n\n", "id": "lists-012-10402311"}, {"subject": "RE: Asynchronous updates/notification", "content": "> From: kugler@us.ibm.com\n\n> For some long-running operations, it would be nice if a server\n> could send asynchronous notifications to a client, so the client\n> doesn't have to poll (or refresh periodically) to get progress\n> updates.  I'm wondering if something similar to the 100-Continue\n> mechanism could be used for this.\n\nYou can do this now by sending a single response with a multipart/replace\nbody.  All it costs is keeping a connection open.  The more general need is\nfor notifications that do not require an open connection, but that is a\ndifferent story.\n\n\n\n", "id": "lists-012-10410109"}, {"subject": "RE: Asynchronous updates/notification", "content": "Great!  Is the current MIME type still \"multipart/x-mixed-replace\" (i.e., is it\nstill considered experimental?)?  Is multipart/replace widely implemented?\n\n     -Carl\n\n\n\n\"Scott Lawrence\" <lawrence@agranat.com> on 06/17/99 08:21:45 AM\n\nTo:   Carl Kugler/Boulder/IBM@IBMUS, http-wg@cuckoo.hpl.hp.com\ncc:\nSubject:  RE: Asynchronous updates/notifications\n\n\n\n\n\n\n\n> From: kugler@us.ibm.com\n\n> For some long-running operations, it would be nice if a server\n> could send asynchronous notifications to a client, so the client\n> doesn't have to poll (or refresh periodically) to get progress\n> updates.  I'm wondering if something similar to the 100-Continue\n> mechanism could be used for this.\n\nYou can do this now by sending a single response with a multipart/replace\nbody.  All it costs is keeping a connection open.  The more general need is\nfor notifications that do not require an open connection, but that is a\ndifferent story.\n\n\n\n", "id": "lists-012-10418247"}, {"subject": "MIME type settin", "content": "I am not sure whether my Q is relevant to this forum. But as I read all the\nQ's posted in it, I am seeking help here -\nMy Question\n--------------\nI have to run a dos .bat file as a cgi script in Microsoft Personal Web\nServer 4.0(I have HTML form's action set to this .bat file). How do I\nassociate at the web server end, that .bat is an executable script  ? When\nI submit the form, web server reports  \"MIME type not properly set\".\nBasically it does,nt recognise .bat extension for execution.\nRegards\n--Mukul\n\n\n\n", "id": "lists-012-10427227"}, {"subject": "RE: MIME type settin", "content": "> I am not sure whether my Q is relevant to this forum.\n>But as I read all the Q's posted in it, I am seeking help here -\n\nThank you for your polite inquiry. Perhaps someone\nhere will be kind enough to help you, but in truth,\nhttp-wg@cuckoo.hpl.hp.com is _not_ a forum for\nasking questions about how to build web applications.\n\nwww-talk@w3.org is often recommended, as are the\nlarge number of news groups dedicated to the topic.\n\nWe've intended to have http-wg mailing list remain open\neven after the HTTP working group closes, in order to\ndiscuss issues and ambiguities with RFC 2616 & 2617.\nI wouldn't want to see it become just another forum for\ngeneral \"how to\" web questions, though.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n> \n\n\n\n", "id": "lists-012-10434389"}, {"subject": "Slides available from my USENIX tal", "content": "Last week, I gave a talk at the 1999 USENIX Conference\non \"What's wrong with HTTP, and why it doesn't matter.\"\n\nLarry Masinter suggested that \"many members of the HTTP\nworking group would enjoy seeing your talk slides.\"  Or\nperhaps many members of the HTTP working group will\nflame me for them.\n\nAt any rate, sooner or later these will be online\nat www.usenix.org; meanwhile, you can find them at\n     ftp://ftp.digital.com/pub/DEC/WRL/mogul/usenix1999it_mogul.ps\n     ftp://ftp.digital.com/pub/DEC/WRL/mogul/usenix1999it_mogul.pdf\ndepending on what format you prefer.\n\nThere are a few slides at the end of the file, on caching,\nthat I brought in case they were needed to fill some time --\nthey weren't.\n\nPlease:\n   (1) do not send me email if you have trouble retrieving,\n   printing, or displaying these files.  I'll ignore such email.\n   \n   (2) do not expect me to respond to critiques of these slides,\n   or questions about what I really meant, or what I said\n   that isn't on the slides.\n   \n   (3) remember that these slides reflect my personal opinions,\n   and not necessarily anyone else's.\n\n-Jeff\n\n\n\n", "id": "lists-012-10443012"}, {"subject": "RE: MIME type settin", "content": "I am sorry for posting a Q not relevant to this forum. But, I got the\nanswer from someone here. And, thanks for pointing me to the correct forum.\nIf this is a moderated mailing list, pl let me a member of it. I like the\ntechnical stuff here.\nThanks\n--Mukul Gandhi\n\nAt 10:07 AM 6/18/99 PDT, Larry Masinter wrote:\n>> I am not sure whether my Q is relevant to this forum..\n>>But as I read all the Q's posted in it, I am seeking help here -\n>\n>Thank you for your polite inquiry. Perhaps someone\n>here will be kind enough to help you, but in truth,\n>http-wg@cuckoo.hpl.hp.com is _not_ a forum for\n>asking questions about how to build web applications..\n>\n>www-talk@w3.org is often recommended, as are the\n>large number of news groups dedicated to the topic..\n>\n>We've intended to have http-wg mailing list remain open\n>even after the HTTP working group closes, in order to\n>discuss issues and ambiguities with RFC 2616 & 2617..\n>I wouldn't want to see it become just another forum for\n>general \"how to\" web questions, though..\n>\n>Larry\n>-- \n>http://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-10450386"}, {"subject": "Re: The last LAST CALL: HTTP/1.1, Authentication, Stat", "content": "On Fri, 13 Mar 1998, Larry Masinter wrote:\n> We have reached the point in the HTTP Working Group where we can make\n> the last \"LAST CALL\" on working group documents:\n[snip]\n> <draft-ietf-http-authentication-01.txt>, March 13, 1998, HTTP Authentication: Basic and Digest Access Authentication\n[snip]\n> In order to actually move these forward as DRAFT STANDARD, we have to\n> document at least two interoperable implementations of each feature. We\n> are working on completing that task in the next few weeks.\n[snip]\n> The working group LAST CALL on these documents will end on Friday, March\n> 27, 1998; barring any major issues arising, we will then forward them to\n> the IESG for consideration.\n\nJust a comment from the sidelines: there have been significant changes\nin the authentication draft, and yet a LAST CALL is issued only hours\nafter it is issued?\n\nAre any servers going to implement the new stuff by next week? Otherwise\nI won't bother hurrying on my side.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1045083"}, {"subject": "RE: Asynchronous updates/notification", "content": ">The more \n> general need is\n> for notifications that do not require an open connection, but \n> that is a\n> different story.\n\n\n\n", "id": "lists-012-10458893"}, {"subject": "RE: Asynchronous updates/notification", "content": ">The more \n> general need is\n> for notifications that do not require an open connection, but \n> that is a\n> different story.\n\nSurendra Reddy and I prepared some Internet-Drafts on a possible Event\nNotification Protocol, but I think the effort got mired down in trying to\ncome up with the support for a general workflow protocol.  I'm still\ninterested in a fairly lightweight ENP (TCP/IP-only, request-response only\n(not store-and-forward also)), as it would help optimize a project we have\nhere at work.  I don't know when I'll have time to get back to it -- maybe\nin the 2nd half of this year.\n===============================================\nMark Leighton Fisher            fisherm@tce.com\nThomson Consumer Electronics    Indianapolis IN\n\"Browser Torture Specialist, First Class\" \n\n\n\n", "id": "lists-012-10466681"}, {"subject": "Re: Asynchronous updates/notification", "content": "I am working in the Internet Presence and Instant Messages field. As you know,\npresence needs asynchonous notifications as well. \n\nWe are using a protocol for 2 years now which does:\n1) subscriptions from client to server\n2) notifications from server to client w/o open connection\n3) if 2) is not possible, it uses a Unix-select-like mechanism thru HTTP \n   with open connection. Notifications travel from server to client while \n   TCP connections are established from client to sever. \n\nYou might have a look into the Internet-Draft: draft-wolf-vpp-01.txt. \nSearch for NOTIFY and SELECT. \nOr: go to http://rr-vs.informatik.uni-ulm.de/rr/ and watch the TCP traffic\nbetween Java applet and server. Everything is encapsulated into HTTP\nrequest-response transactions to be HTTP-proxy compatible. We are very happy\nwith the select-mechanism. Tell me, if you need more information. I can write\nan Internet Draft. Planned this anyway since the HTTP-SELECT experience could\nbe valuable for others as well. \n-- \nKlaus H. Wolf                                   Voice: +49 (731) 502 4145\nDistributed Systems Dept.                     Ethernet: 08:00:20:12:2a:01\nUniversity of Ulm                          Cobrow: http://www.cobrow.com/\n89069 Ulm, Germany     Live: http://www.cobrow.com/pages/people/wolf.html\n\n\n\n", "id": "lists-012-10475666"}, {"subject": "Last Call: Applicability Statement for HTTP State Management to BC", "content": "The IESG has received a request from the IETF Steering Group Working\nGroup to consider Applicability Statement for HTTP State Management\n<draft-iesg-http-cookies-00.txt> as a BCP.\n\nThe IESG will also consider HTTP State Management Mechanism\n<draft-ietf-http-state-man-mec-10.txt> as a Proposed Standard.\n\nThe IESG plans to make a decision in the next few weeks, and solicits\nfinal comments on this action.  Please send any comments to the\niesg@ietf.org or ietf@ietf.org mailing lists by July 23, 1999.\n\nFiles can be obtained via\nhttp://www.ietf.org/internet-drafts/draft-iesg-http-cookies-00.txt\nhttp://www.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-10.txt\n\n\n\n", "id": "lists-012-10484411"}, {"subject": "Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "Rohit Khare and I have revised an updated his draft on how to use the 1.1\nUpgrade header to signal a change to HTTP over TLS on the current\nconnection, allowing secured operation without using a separate port.\n\nThe I-D announcement is at:\n  http://www.ietf.org/mail-archive/ietf-announce/msg04379.html\n\nThe I-D itself is at:\n  http://www.ietf.org/internet-drafts/draft-ietf-tls-http-upgrade-01.txt\n\nor in HTML at:\n\nhttp://www.agranat.com/fs/public/lawrence/draft-ietf-tls-http-upgrade-01.htm\nl\n\n\nIn addition to the material on how to use Upgrade, this draft requests the\ncreation of an IANA registry for Upgrade tokens and one for HTTP error\ncodes.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10493049"}, {"subject": "Re: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "Scott Lawrence wrote:\n\n> Rohit Khare and I have revised an updated his draft on how to use the 1.1\n> Upgrade header to signal a change to HTTP over TLS on the current\n> connection, allowing secured operation without using a separate port.\n\nIs this necessary? Can you use the CONNECT method instead? (I know it's not\nactually defined in RFC2616--it's just reserved--but it is implemented, and\nit'd be easier to codify existing practice than to come up with a new\nmechanism.)\n\nThe CONNECT implementation in Apache's mod_proxy (at least, the one I'm\nrunning, version 1.2.4) takes a host:port specifier as the Request-URI and\nopens up a connection to that host:port.  (I assume that at least Netscape's\nproxy server does the same thing, since Netscape Communicator works through\nthis proxy.) This does require a separate port on the destination machine; but,\nif the destination machine is the machine processing the CONNECT, then you\ndon't have to worry about firewalls in the way.\n\nFurthermore, if the server processing the CONNECT is the same process as the\nsecure server, it doesn't even actually have to use a loopback connection; it\ncan just pass the incoming connection to its TLS code.   Of course, making the\nsecure server a separate process is likely to be safer, since there's less\nchance of information leaking across connections; in that case, the insecure\nserver may be able to transfer the connection to the secure server process (if\nthe OS allows, and if they're implemented so as to cooperate).\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-10501796"}, {"subject": "RE: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "> From: John Stracke\n> Subject: Re: Upgrading to TLS Within HTTP/1.1 draft available\n>\n> Scott Lawrence wrote:\n>\n> > Upgrade header to signal a change to HTTP over TLS on the current\n> > connection, allowing secured operation without using a separate port.\n>\n> Is this necessary? Can you use the CONNECT method instead? (I\n> know it's not\n> actually defined in RFC2616--it's just reserved--but it is\n> implemented, and\n> it'd be easier to codify existing practice than to come up with a new\n> mechanism.)\n\nIt really isn't a new mechanism in the sense that this kind of change from\none protocol to another was exactly what the Upgrade header was included in\nHTTP to accomplish; it's just that the specifics of how to use it were not\nspelled out in the spec.\n\nThe CONNECT mechanism is really trying to do something different.  It\ncreates a tunnel through an existing proxy, but doesn't signal the protocol\nto be used end-to-end on that connection.\n\nPart of the goal here is to show how secured and unsecured traffic in any\nprotocol can share a TCP well known port, so that we can get away from\nassigning two ports to each protocol.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10511234"}, {"subject": "I-D ACTION:draft-hnrs-rmt-avt-feedback00.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: A Taxonomy of Feedback for Multicast\nAuthor(s): M. Hofmann, J. Nonnenmacher, J. Rosenberg, \n                          H. Schulzrinne\nFilename: draft-hnrs-rmt-avt-feedback-00.txt\nPages: 11\nDate: 25-Jun-99\n\nWhen distributing information through multicast, it is often\nneccessary to solicit feedback from receivers. This feedback can be\nto report on reception quality, indicate loss of packets, or\nacknowledge receipt of data. There is a range of feedback mechanisms\npossible, ranging from polling to periodic multicast feedback. This\ndocument presents a taxonomy of these various mechanisms, with a\ndiscussion of the pros and cons of each approach and a discussion of\napplicability.\n\nA URL for this Internet-Draft is:\nhttp://www.ietf.org/internet-drafts/draft-hnrs-rmt-avt-feedback-00.txt\n\nInternet-Drafts are also available by anonymous FTP. Login with the username\n\"anonymous\" and a password of your e-mail address. After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-hnrs-rmt-avt-feedback-00.txt\".\n\nA list of Internet-Drafts directories can be found in\nhttp://www.ietf.org/shadow.html \nor ftp://ftp.ietf.org/ietf/1shadow-sites.txt\n\n\nInternet-Drafts can also be obtained by e-mail.\n\nSend a message to:\nmailserv@ietf.org.\nIn the body type:\n\"FILE /internet-drafts/draft-hnrs-rmt-avt-feedback-00.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-10520437"}, {"subject": "304 response with ContentLength header == OK", "content": "Hi all,\n\nWhile testing an HTTP/1.1 proxy server I am working on,\nI found a page on a major site that returns the following\nresponse on a conditional GET:\n\n> HTTP/1.0 304 Not Modified\n> Content-type: image/gif\n> Content-length: 4672\n> Expires: Thu, 15 Apr 2010 20:00:00 GMT\n> Last-modified: Fri, 15 Apr 1994 00:00:00 GMT\n>\n\nThe response didn't contain a body, but the presence\nof a Content-Length threw off the proxy.  I'll be fixing\nthat in the name of tolerance in any case. But I was\nwondering if including a Content-Length on a response\nthat must NOT include a body and which isn't produced by\na HEAD request should be considered a bug?  That is,\nshould I point out to the site operator that his/her\nserver is generating bad HTTP or is this OK since 304\ntells us there won't be a body anyway?\n\nAny opinions?\n===\nWham! <wham_bang@yahoo.com>\n\n\n\n_________________________________________________________\nDo You Yahoo!?\nGet your free @yahoo.com address at http://mail.yahoo.com\n\n\n\n", "id": "lists-012-10529360"}, {"subject": "External bnf references in http1.1/v0", "content": "Hello,\n\nI've taken a look at the BNF in the HTTP 1.1 / v03 document, and there \nno internal inconsistencies left. But I would suggest to state explicitely\nin the text, that the following BNF-identifier refererences to other\ndocuments, like you have done it in the URI-Section:\n  challenge\n  credentials\n\nThis is not i big issue, since one can guess that from the text, but for\nthe sake of consistency I suggest the following changes:\n\n1)\nAdd this (for instance) to section 11 (Access Authentication\")\n    This specification adopts the definitions of \"challenge\" and \n    \"credentials\" from that specification.\n\n2) Typographic change\nReplace in 14.22 (From)\n    The address SHOULD be machine-usable, as defined by mailbox in RFC 822\n    [9] (as updated by RFC 1123 [8]):\nby\n    The address SHOULD be machine-usable, as defined by \"mailbox\" in RFC 822\n    [9] (as updated by RFC 1123 [8]):\nor even\n    The address SHOULD be machine-usable, as defined by \"mailbox\" in RFC 822\n    [9] (as updated by RFC 1123 [8]). This specification adopts the \n    definition of \"mailbox\" from that specification.\n\n-- \n                                             Jacob Schroeder\n                                             Dipl. Informatiker\n                                             eMail: jschroeder@becomsys.de\n\n\n\n", "id": "lists-012-1053157"}, {"subject": "Re: 304 response with ContentLength header == OK", "content": "In a previous episode Wham Bang said...\n:: \n:: While testing an HTTP/1.1 proxy server I am working on,\n:: I found a page on a major site that returns the following\n:: response on a conditional GET:\n:: \n:: > HTTP/1.0 304 Not Modified\n:: > Content-type: image/gif\n:: > Content-length: 4672\n:: > Expires: Thu, 15 Apr 2010 20:00:00 GMT\n:: > Last-modified: Fri, 15 Apr 1994 00:00:00 GMT\n:: >\n:: \n:: The response didn't contain a body, but the presence\n:: of a Content-Length threw off the proxy.  I'll be fixing\n:: that in the name of tolerance in any case. But I was\n:: wondering if including a Content-Length on a response\n:: that must NOT include a body and which isn't produced by\n:: a HEAD request should be considered a bug?  That is,\n\nmy reasonably literate (but not super expert) reading makes this look\nlike a lose-lose (you're both wrong ;).. story of my life some days..)\n\nas for your proxy:\n\n   4.4 Message Length\n\n   The transfer-length of a message is the length of the message-body\n   as it appears in the message; that is, after any transfer-codings\n   have been applied. When a message-body is included with a message,\n   the transfer-length of that body is determined by one of the\n   following (in order of precedence):\n\n   1.Any response message which \"MUST NOT\" include a message-body\n     (such as the 1xx, 204, and 304 responses and any response to a\n     HEAD request) is always terminated by the first empty line after\n     the header fields, regardless of the entity-header fields present\n     in the message.\n\nso ignoring content-length isn't just tolerant, it's required.\n\nand as for the content provider:\n\n14.13 Content-Length\n\n   The Content-Length entity-header field indicates the size of the\n   entity-body, in decimal number of OCTETs, sent to the recipient or,\n   in the case of the HEAD method, the size of the entity-body that\n   would have been sent have been sent had the request been a GET.\n\nthey're definitely in the wrong as their entity body is size 0 and\nthey aren't responding to a HEAD.\n\n-P\n\n\n\n", "id": "lists-012-10537984"}, {"subject": "Re: 304 response with ContentLength header == OK", "content": "Hello again,\n\nHmmmm, in retrospect I'm a little disapointed at myself for\ntroubling the list with stuff that turns out to be\npretty clearly spelled out in the spec...  Sorry.\n\n--- Patrick McManus <mcmanus@appliedtheory.com> wrote:\n> In a previous episode Wham Bang said...\n> :: [...]\n> :: response on a conditional GET:\n> :: \n> :: > HTTP/1.0 304 Not Modified\n> :: > Content-type: image/gif\n> :: > Content-length: 4672\n> :: > Expires: Thu, 15 Apr 2010 20:00:00 GMT\n> :: > Last-modified: Fri, 15 Apr 1994 00:00:00 GMT\n> :: >\n> :: \n> :: The response didn't contain a body, but the presence\n> :: of a Content-Length threw off the proxy. [...]\n>\n> [...]\n> as for your proxy:\n> [...]\n> \n>    1.Any response message which \"MUST NOT\" include a message-body\n>      (such as the 1xx, 204, and 304 responses and any response to a\n>      HEAD request) is always terminated by the first empty line after\n>      the header fields, regardless of the entity-header fields\n>      present in the message.\n> \n> so ignoring content-length isn't just tolerant, it's required.\n> \n\nYes, the quote makes it quite clear. Hey, at least I was going to\nfix it. :)\n\n> and as for the content provider:\n> \n> 14.13 Content-Length\n> \n>    The Content-Length entity-header field indicates the size of the\n>    entity-body, in decimal number of OCTETs, sent to the recipient\n>    or, in the case of the HEAD method, the size of the entity-body\n>    that would have been sent have been sent had the request been a\n>    GET.\n> \n> they're definitely in the wrong as their entity body is size 0 and\n> they aren't responding to a HEAD.\n> \n\nSomeone disagreed with this opinion via email, but I'd tend\nto agree. The language is (again) quite clear. Putting in\nContent-Length: 0 would've been OK though.\n\nThanks for the clarifications!\n\n===\nWham! <wham_bang@yahoo.com>\n\n\n\n_________________________________________________________\nDo You Yahoo!?\nGet your free @yahoo.com address at http://mail.yahoo.com\n\n\n\n", "id": "lists-012-10547544"}, {"subject": "Re: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "Scott Lawrence wrote:\n\n> Part of the goal here is to show how secured and unsecured traffic in any\n> protocol can share a TCP well known port, so that we can get away from\n> assigning two ports to each protocol.\n\nBut aren't there security benefits to having separate ports (e.g., making it\npossible to run your secure server in a separate process)?\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-10557576"}, {"subject": "Re: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "> Sender: francis@ariel.local.thibault.org\n> From: John Stracke <francis@ecal.com>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Tue, 29 Jun 1999 16:47:55 +0000\n> To: \"Http-Wg@Hplb. Hpl. Hp. Com\" <http-wg@hplb.hpl.hp.com>\n> Subject: Re: Upgrading to TLS Within HTTP/1.1 draft available\n> -----\n> Scott Lawrence wrote:\n> \n> > Part of the goal here is to show how secured and unsecured traffic in any\n> > protocol can share a TCP well known port, so that we can get away from\n> > assigning two ports to each protocol.\n> \n> But aren't there security benefits to having separate ports (e.g., making it\n> possible to run your secure server in a separate process)?\n> \n>\n\nNo: the problem is that establishing a connection to a separate port\nallows for man-in-the-middle attacks at connection establishment times;\nyou are just making attacks easier using different port numbers.\n\nThe new IESG/IANA policy is therefore to no longer allocate independent \nport numbers for secure connections.  This is the stronger motivation\nthan conserving port numbers.\n- Jim Gettys\n\n\n\n", "id": "lists-012-10566008"}, {"subject": "Re: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "Jim Gettys wrote:\n\n> > But aren't there security benefits to having separate ports (e.g., making it\n> > possible to run your secure server in a separate process)?\n>\n> No: the problem is that establishing a connection to a separate port\n> allows for man-in-the-middle attacks at connection establishment times;\n\nOK, got it.  Thanks for the explanation.\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-10575816"}, {"subject": "RE: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "> From: francis@ariel.local.thibault.org On Behalf Of John Stracke\n>\n> Scott Lawrence wrote:\n>\n> > Part of the goal here is to show how secured and unsecured\n> traffic in any\n> > protocol can share a TCP well known port, so that we can get away from\n> > assigning two ports to each protocol.\n>\n> But aren't there security benefits to having separate ports\n> (e.g., making it\n> possible to run your secure server in a separate process)?\n\nThere is nothing about the proposal that prevents that; I may run my secure\nserver at http://www.example.com/ and the secure one at\nhttp://www.example.com:2000/ or the other way around.  My server may use the\nIP address or a Host header value, or a part of the URL path to determine\nthat the request needs to be upgraded to a secure connection.\n\nIn any event, separate ports are orthogonal to separate processes; one does\nnot imply or require the other.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10583863"}, {"subject": "Re: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "Scott Lawrence wrote:\n> In any event, separate ports are orthogonal to separate processes; one does\n> not imply or require the other.\n\nI think the point is the reverse one: a single port requires a single\nprocess (or at least a family of related processes). I cannot run two\ndifferent pieces of software on one port _and_ get reasonable\nperformance (if I care not about performance, its no problem, of\ncourse).\n\nCheers,\n\nBen.\n\n--\nhttp://www.apache-ssl.org/ben.html\n\n\"My grandfather once told me that there are two kinds of people: those\nwho work and those who take the credit. He told me to try to be in the\nfirst group; there was less competition there.\"\n     - Indira Gandhi\n\n\n\n", "id": "lists-012-10593497"}, {"subject": "Re: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "Ben Laurie wrote:\n\n> Scott Lawrence wrote:\n> > In any event, separate ports are orthogonal to separate processes; one does\n> > not imply or require the other.\n>\n> I think the point is the reverse one: a single port requires a single\n> process (or at least a family of related processes). I cannot run two\n> different pieces of software on one port _and_ get reasonable\n> performance\n\nIt's possible on most Unices, where you can pass file descriptors from one\nprocess to another.  They need to cooperate, though, and the receiving process\nhas to trust the sending process not to play man-in-the-middle games.\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-10602199"}, {"subject": "http 1.1. Cache Response Codes..", "content": "Gentlemen:\n\nAs a web measurement analyst, one of my worst problems is dealing with\ncaching.  Why do we use the code 200 for everything?  Why not design the\nspec in a graduated fashion:\n\nIf browser has url in its local cache, it still sends a get request to the\nserver, but an option says \"I already have it, just letting you log the\nrequest\". The success code is a 209, \"user has non-expired data in cache\".\n\nIf a non local cache has the data, same system.  The option can be\ndifferent, and we can even use a different code (210), but for the most\npart, we should just let the 209 mean \"cached request\".\n\nThis solves many problems:\n1) path analysis of a user's visit\n2) advertising requests (not perfectly, given the IAB's recent standards,\nbut better than nothing for smaller sites)\n3) pages per visit calculations are accurate\n4) this is a minor increase in bandwidth compared to not sending the\nrequest at all, and is far superior to eliminating caching.\n\nThanks for your time,\n\nMichael\n\n___\n\nMichael Wexler\nDirector -- Research and Measurement\nmwexler@nny.com\n\nNicholson | NY\n295 Lafayette Street\nNew York, NY 10012\n212.274.0470 x178\n212.274.0380 fax\n\nhttp://www.nny.com/\n\n\n\n", "id": "lists-012-1061060"}, {"subject": "Re: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "John Stracke wrote:\n> \n> Ben Laurie wrote:\n> \n> > Scott Lawrence wrote:\n> > > In any event, separate ports are orthogonal to separate processes; one does\n> > > not imply or require the other.\n> >\n> > I think the point is the reverse one: a single port requires a single\n> > process (or at least a family of related processes). I cannot run two\n> > different pieces of software on one port _and_ get reasonable\n> > performance\n> \n> It's possible on most Unices, where you can pass file descriptors from one\n> process to another.  They need to cooperate, though, and the receiving process\n> has to trust the sending process not to play man-in-the-middle games.\n\nIt is possible, but not particularly efficient or portable. ISTR one of\nthe very first things Apache did was to get rid of that nonsense.\n\nCheers,\n\nBen.\n\n--\nhttp://www.apache-ssl.org/ben.html\n\n\"My grandfather once told me that there are two kinds of people: those\nwho work and those who take the credit. He told me to try to be in the\nfirst group; there was less competition there.\"\n     - Indira Gandhi\n\n\n\n", "id": "lists-012-10610710"}, {"subject": "Re: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "Ben Laurie writes:\n > It is possible, but not particularly efficient or portable. ISTR one of\n > the very first things Apache did was to get rid of that nonsense.\n\nMore precisely, Apache chose not to adopt it.  NCSA 1.4 used an\nfd-passing scheme in order to handle accepts on its persistent child\nprocesses; the parent server accepted connections, and then fd-passed\nthem to the children.  When we were trying to figure out what to do\nabout producing a non-fork-per-request version of Apache, Rob Hartill\ndid some experiments which pretty quickly showed that a simpler\napproach could yield results at least as good.  \n\nIf you really need fd-passing behavior, there's sample code in one of\nStevens' books (\"Advanced Programming in the Unix Environment\"), but\nit's messy; IIRC, there are multiple variants to deal with the\nvariety of interfaces in different Unix variants.  \n\nrst\n\n\n\n", "id": "lists-012-10619560"}, {"subject": "RE: Last Call: Applicability Statement for HTTP State Management  to BC", "content": "The following comments are regarding:\nhttp://www.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-10.txt\n\nFIRST OFF:\n\nI would like to congratulate Dave Kristol. Although, as I discuss below, I\nhave significant reservations about this specification Dave has spent an\nenormous amount of time trying to be reasonable. He has worked very hard to\nput in text that actually addresses the issues, isn't wish washy but at the\nsame time avoids the strident tone that has often entered into the cookie\ndebate. In addition Dave has done a really good job of addressing the\nvarious problems with the early specification and coming up with the most\nreasonable solutions one can probably expect given the circumstances.\n\nThat having been said.... =)\n\nOBJECTION:\n\nThe key failure in cookie security is authentication, the ability to know\nexactly with whom you are dealing. As we all know, domains can not provide\nthis information which is the core of the cookie security problem. It will\nobviously take some time for a proper cookie authentication mechanism to be\nagreed upon and it is quite reasonable for people to seek some sort of\ninterim solution which will provide at least some protection in the\nmeantime.\n\nI do not believe that draft-ietf-http-state-man-mec-10.txt provides this\ninterim solution. While the draft does provide some improvements in regards\nto the handling of cookies I personally do not believe that those\nimprovements are sufficient to merit changing the existing client and server\ninfrastructure. As such I believe that this draft should be blocked from\nprogressing to proposed standard status until it can demonstrate a\nsufficiently high level of security to either qualify as a robust interim\nmeasure or until it provides a workable solution to the underlying problem.\n\nI realize that such judgments are completely subjective and am uncomfortable\nmaking the previous argument. I prefer arguments based on indisputable\nfacts, but that doesn't appear possible here. On the positive side the\nspecification is well written and certainly achieves its modest goals. No\ngreat harm will come to the internet community through its publication. I do\nbelieve, however, that it would be unfortunate for the IETF to lend its\ncredibility to this specification in the specification's current state.\n\nGENERAL COMMENTS:\n\nI did not find internationalization considerations for cookie comments.\nShouldn't they be in UTF-8?\n\nIf no version is specified in a Set-Cookie2 header is one to assume that it\nis version 1?\n\nIf one always returns the version number exactly then servers have no idea\nif the client understood any enhanced semantics associated with a greater,\nbut still backwards compatible, version. Shouldn't the client only return\nthe highest version number it supports?\n\nNIT:\n\nIn section 6 there is mention of 'speculating'. I would suggest rephrasing\nwith the phrase \"provide guidance.\"\n\nTYPOS:\n\nIn section 2, 6th paragraph, the sentence begins \"ost names can be\"\n\nSection 4.2.3 talks about TTP/1.1 and TTP/1.0 servers.\n\nI think there is a return missing after the title for section 4.3.5.\n\nThere is a \"owever\" in section 4.3.5.\n\nSection 6 contains a \"ere\".\n\nI think a return got lost after the title of section 6.3.1.\n\n> -----Original Message-----\n> From: The IESG [mailto:iesg-secretary@ietf.org]\n> Sent: Wed, June 23, 1999 2:00 PM\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Last Call: Applicability Statement for HTTP State \n> Management to\n> BCP\n> \n> \n> \n> The IESG has received a request from the IETF Steering Group Working\n> Group to consider Applicability Statement for HTTP State Management\n> <draft-iesg-http-cookies-00.txt> as a BCP.\n> \n> The IESG will also consider HTTP State Management Mechanism\n> <draft-ietf-http-state-man-mec-10.txt> as a Proposed Standard.\n> \n> The IESG plans to make a decision in the next few weeks, and solicits\n> final comments on this action.  Please send any comments to the\n> iesg@ietf.org or ietf@ietf.org mailing lists by July 23, 1999.\n> \n> Files can be obtained via\n> http://www.ietf.org/internet-drafts/draft-iesg-http-cookies-00.txt\n> http://www.ietf.org/internet-drafts/draft-ietf-http-state-man-\nmec-10.txt\n\n\n\n", "id": "lists-012-10628279"}, {"subject": "RE: Upgrading to TLS Within HTTP/1.1 draft availabl", "content": "John,\n\nWe have been over this subject with the IETF Application Area Directors.\nThey don't want to see any extra ports for security. Full stop.\n\nCarl-Uno\n\n> -----Original Message-----\n> From: John Stracke [mailto:francis@ecal.com]\n> Sent: Tuesday, June 29, 1999 9:48 AM\n> To: Http-Wg@Hplb. Hpl. Hp. Com\n> Subject: Re: Upgrading to TLS Within HTTP/1.1 draft available\n> \n> \n> Scott Lawrence wrote:\n> \n> > Part of the goal here is to show how secured and unsecured \n> traffic in any\n> > protocol can share a TCP well known port, so that we can \n> get away from\n> > assigning two ports to each protocol.\n> \n> But aren't there security benefits to having separate ports \n> (e.g., making it\n> possible to run your secure server in a separate process)?\n> \n> --\n> /=============================================================\\\n> |John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n> |francis@ecal.com|============================================|\n> |Chief Scientist | NT's lack of reliability is only surpassed |\n> |eCal Corp.      |  by its lack of scalability. -- John Kirch |\n> \\=============================================================/\n> \n> \n> \n\n\n\n", "id": "lists-012-10641602"}, {"subject": "Re: Asynchronous updates/notification", "content": "\"Manros, Carl-Uno B\" wrote:\n> \n> Klaus,\n> \n> Ja bitte, ein Internet-Draft w?re sehr willkommen!\n\nPosted just before the deadline, here is the Internet-Draft on our \n\"Experiences with HTTP-SELECT and Asynchronous Updates\" through\nrequest-response protocols and firewalls:\n  http://www.ietf.org/internet-drafts/draft-wolf-http-select-00.txt\n\nBTW: The traffic on the ietf-announce list is amazing. I feel sorry for the\nInternet-Drafts Administrator(s) who process all those documents. I suppose\nthe procedure must be modified soon to keep up with the flood of\nInternet-Drafts. \n--\nKlaus H. Wolf                                   Voice: +49 (731) 502 4145\nDistributed Systems Dept.                     Ethernet: 08:00:20:12:2a:01\nUniversity of Ulm                          Cobrow: http://www.cobrow.com/\n89069 Ulm, Germany     Live: http://www.cobrow.com/pages/people/wolf.html\n\n\n\n", "id": "lists-012-10651176"}, {"subject": "RE: Last Call: Applicability Statement for HTTP State Management to BC", "content": "\"Yaron Goland (Exchange)\" <yarong@Exchange.Microsoft.com> wrote:\n  > The following comments are regarding:\n  > http://www.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-10.txt\n\nDespite the Subject line, Yaron's comments apply to the above I-D,\nnot to the Applicability Statement.\n\n  > \n  > [Complimentary remarks about me accepted.  And deleted.]\n  >\n\nI would like to redirect any follow-up discussion to\nhttp-state@lists.research.bell-labs.com, where we have been conducting\nthe discussions about cookies.  Also, please forgive me, but I am on\nvacation for a couple more weeks, and my brain is shut down.  I will\ntry to address the issues Yaron raises when I return.\n\nPlease note that the draft in question is dated July 24, 1998 [sic!].\n\nDave Kristol\n\n\n\n", "id": "lists-012-10659404"}, {"subject": "rfc2617: BNF for 'domain", "content": "Section 3.2.1.: Does \n\n  domain = \"domain\" \"=\" <\"> URI ( 1*SP URI ) <\">\n\nreally describe \"A quoted, space-separated list of URIs\"? Should there\nbe a '*' before the '('?\n\njoe\n\n-- \nJoe Orton\njoe@orton.demon.co.uk ... jeo101@york.ac.uk\nhttp://www.orton.demon.co.uk/\n\n\n\n", "id": "lists-012-10669423"}, {"subject": "RE: Last Call: Applicability Statement for HTTP State Management  to BC", "content": "Koen Holtman wrote:\n> There has never been a real consensus for 2109 or state-man-mec. \n\n\nAt the time these documents were forwarded, it was my belief that\nthere was rough consensus for 2109 and, subsequently, state-man-mec.\nThe objections were to the mandatory default behavior necessary to\nachieve the desired protections of the security considerations\nsection.\n\nI believe it is still in the best interest of the Internet to publish\nthese documents, and that (unless there have been significant reversals\nof previously strongly-held opinion) they still represent the\n\"rough consensus\" of the community.\n\nAs for the specific comments:\n\n> - I believe that the '2.2.1. Leakage of Information to Third Parties'\n> requirements in iesg-http-cookies are good practice.  However, I think\n> that the '2.2.1. Use as an Authentication Mechanism' requirements go\n> too far: for some cases where the security concerns of authentication\n> are minor and the usability concerns are major, I consider the use of\n> cookies for authentication to be a valid engineering solution.\n\nThis is covered in the last paragraph of section 2.2.1.\n\n> - (section 2:) The HTTP Content-Location header field cannot currently\n> be used in a scheme to maintain state, except when it is part of a\n> HTTP redirect.\n\nyes, that's the context. What's the problem?\n\n> - (section 2:) I do not agree to the statement that state management\n> represents a 'marginal (but still useful) increase in functionality\n> over ordinary HTTP and HTML.'  The increase is not marginal for people\n> who want to build reliable services that maintain state.  A major\n> advantage of cookies not mentioned in section 2 is that cookie state\n> is insensitive to users navigating the site using the 'back' button or\n> history functions.\n\nYou object to the word \"marginal\", but its usage in this context\nis consistent with the intent and your statements. And the list\nof advantages need not be listed exhaustively.\n\n> - (section 3:) 'While such exposure is possible, this is not a flaw in\n> the protocol itself'.  It _is_ a flaw in the protocol itself, one that\n> was inherited from Netscape cookies.  Other less flawed state\n> management protocols would have been possible, but were pre-empted by\n> cookies.  If state-man-mec is called flawless in an IESG BCP document,\n> this implies to me that new protocols with similar flaws would have a\n> good chance of passing an IETF/IESG security considerations review,\n> which they clearly should not.\n\nWell, its hard to equate the lack of a single flaw into \"flawless\",\nbut perhaps a \"primarily\", as in \"it is not primarily a flaw in the\nprotocol itself\" would satisfy your concern.\n\n\n\n", "id": "lists-012-10677300"}, {"subject": "RE: Last Call: Applicability Statement for HTTP State Management  to BC", "content": "> It is my understanding that a lack of consensus on the security\n> considerations (in this case a lack of consensus on how much privacy is\n> needed by default) implies a lack of consensus on the whole specification.\n\nKoen, a careful reading of RFC 2026 notes that \"consensus\" is not\ninterpreted as \"without disagreement\", and that it outlines the\nnature of the types of disagreements that might hold within a working\ngroup. Although the specified default for cookie privacy was\nin dispute in the working group, it was my judgement that the objections\nwere neither that\n\n> (a) his or her own views have not been adequately considered\n>   by the Working Group,\n\nsince the views on the default privacy behavior were considered\nat length, and\n\n> (b) the Working Group has made an incorrect technical choice\n>   which places the quality and/or integrity of the Working Group's \n>   product(s) in significant jeopardy. \n\nsince the disagreement was not about quality or integrity, but\nrather about privacy policy and commercial viability, in an\nadvertising-dominated Internet, of the specified default behavior. \n\nAnyone who is not satisfied with the resolution of this dispute\ncan follow the dispute reoslution procedures outlined in RFC 2026,\nbut I maintain that, as far as the IETF process is concerned, the\nworking group reached \"consensus\" (albeit rough) on the entire\ndocument.\n\nLarry Masinter\n(as HTTP working group chair)\n\n\n\n", "id": "lists-012-10689371"}, {"subject": "Mobile IP and HTTP prox", "content": "Hi,\n\nI just want to know that will mobile IP (in which situation hosts can\nattach to different networks without changing their ip addresses) bring\nsome problems to HTTP proxy, since it may or may not save bandwidth if a\nhost keeps on using the same proxy after it is connected to a new\nnetwork Also since the ip address of the host is not changed how can it\nfind out the proper proxy it should use just through application layer\nprotocol.\n\nJianxin Zhao\n----------------------------------------------------------------------\nJianxin Zhao                      E-mail: jxzhao@csgrad.cs.vt.edu\nGraduate student                  Tel:    (540)961-9324\nComputer Science Department       Address: 1203A UNIVERSITY TERRACE\nVirginia Tech.                             BLACKSBURG VA 24060\nWWW: http://csgrad.cs.vt.edu/~jxzhao\n----------------------------------------------------------------------\n\n\n\n", "id": "lists-012-1069434"}, {"subject": "Cookie doub", "content": "Dear respected members ,\nI have some doubt about how cookies operate. Say I have invoked a server\napplication running from the browser. That application sets a cookie on the\nclient M/C. Supposing almost without time lag, another request wen from the\nsame client to the server. This time also, a cookie will be set on the\nclient M/C.\nNow my Qs are -\n\n1. Will the new cookie overwrites the old one ? Or is it a seperate cookie ?\n\n2. Is it possible to design a server application that overwrites the old\ncookie if request from the client comes almost immediately ? \n\nCan somebody pl answer to my query ? When are cookies expected to be\nformally adopted by IETF ? Will it benifit Netscape also ? Or Netscape is\ngiving this technology to the standardising body free of cost ?\n\nRegards\n-Mukul Gandhi\nhttp://members.tripod.com/~mukulgandhi\n\n\n\n", "id": "lists-012-10699790"}, {"subject": "rfc2617: responseauth calculatio", "content": "Just after a clarification:\n\nIn the calculation of the response-auth digest for the\n'Authentication-Info' header, is the qop-value used the one which is\nsent by the client in the 'Authorization' header, or the one sent by\nthe server in the Auth-Info header itself?\n\nExample: the client sends, e.g. a GET request, with no entity-body, so\nuses \"qop=auth\" in the 'Authorization' header. The server response then\nhas an entity-body, and uses \"qop=auth-int\" in the 'Authentication-Info'\nheader.\n\nThe sentence\n \n                              The \"response-digest\" value is calculated\n   as for the \"request-digest\" in the Authorization header, except that\n   if \"qop=auth\" or is not specified in the Authorization header for the\n   request, A2 is\n       ...\n\nimplies that the qop-value the client sent is used, but the paragraph\n\n   message-qop\n     Indicates the \"quality of protection\" options applied to the\n     response by the server.  The value \"auth\" indicates authentication;\n     the value \"auth-int\" indicates authentication with integrity\n     protection. The server SHOULD use the same value for the message-\n     qop directive in the response as was sent by the client in the\n     corresponding request.\n\nseems to implies that the qop-value the server sends is used.\n\nRegards,\n\njoe\n\n-- \nJoe Orton\njoe@orton.demon.co.uk ... jeo101@york.ac.uk\nhttp://www.orton.demon.co.uk/\n\n\n\n", "id": "lists-012-10706890"}, {"subject": "Date header handlin", "content": "14.18 says that the Date header \"represents the date and time at which the\nmessage was originated, having the same semantics as orig-date in RFC 822.\n[...] The HTTP-date sent in a Date header SHOULD NOT represent a date and\ntime subsequent to the generation of the message.\"\n\nIs the term 'message' used here as it is defined in 1.3? The reference to\n822 seems to indicate it is bound to the entity generation date, not the\nmessage generation date.\n\nAlso to support this,\n* 13.5.1 implies that Date is a end-to-end header, and SHOULD NOT be changed\nby a cache (although it isn't specifically forbidden in 13.5.2).\n* 13.2.3 calculates entity age based on the Date header; if Date were\nmodified after entity generation, this would be invalid.\n\nI've noticed that some Web caches will change the response's Date header to\nreflect the time that they generate the message. Based on the above, this\nseems to be incorrect. Others will use the original Date, but not update the\nheader upon validation (as in 13.5.3).\n\nIs this correct? What's the proper behaviour here?\n\n\n\nRegards,\n\nMark Nottingham\nInternet Project Manager\nMerrill Lynch Australasia (Melbourne)\n\n\n\n", "id": "lists-012-10715823"}, {"subject": "RE: rfc2617: responseauth calculatio", "content": "> In the calculation of the response-auth digest for the\n> 'Authentication-Info' header, is the qop-value used the one which is\n> sent by the client in the 'Authorization' header, or the one sent by\n> the server in the Auth-Info header itself?\n\nThe intent was that they should be the same.  The server presents\nalternatives it is willing to support in the WWW-Authenticate challenge, and\nthe client chooses one in its Authorization.  The server should then use\nthat value in the response.  If it is not willing to use 'auth', then it\nshould not present that alternative in the challenge.\n\nIf you did switch between request and response, you would want the server to\nuse the value it is sending in calculating the digest - the point of\nincluding it in the digest is that it be protected from modification.\n\nAs a practical matter, changing qop wouldn't work at all today, since the\nonly commercial browser that does digest at all doesn't support 'auth-int'\nyet.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10724128"}, {"subject": "RE: rfc2617: BNF for 'domain", "content": "> From: Joe Orton [mailto:joe@orton.demon.co.uk]\n\n> Section 3.2.1.: Does \n> \n>   domain = \"domain\" \"=\" <\"> URI ( 1*SP URI ) <\">\n> \n> really describe \"A quoted, space-separated list of URIs\"? Should there\n> be a '*' before the '('?\n\nYes.  One URI is perfectly ok.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10733119"}, {"subject": "RE: rfc2617: responseauth calculatio", "content": "> > In the calculation of the response-auth digest for the\n> > 'Authentication-Info' header, is the qop-value used the one which is\n> > sent by the client in the 'Authorization' header, or the one sent by\n> > the server in the Auth-Info header itself?\n> \n> The intent was that they should be the same.  The server presents\n> alternatives it is willing to support in the WWW-Authenticate challenge, and\n> the client chooses one in its Authorization.  The server should then use\n> that value in the response.  If it is not willing to use 'auth', then it\n> should not present that alternative in the challenge.\n\nAh, can auth-int be used for messages with no body (zero-length), e.g.\nGET requests? I presumed it couldn't, maybe this is the source of my\nconfusion.\n\n> If you did switch between request and response, you would want the server to\n> use the value it is sending in calculating the digest - the point of\n> including it in the digest is that it be protected from modification.\n\nOkay, thanks.\n\n> As a practical matter, changing qop wouldn't work at all today, since the\n> only commercial browser that does digest at all doesn't support 'auth-int'\n> yet.\n\n(I'm writing client code).\n\nRegards,\n\njoe\n\n-- \nJoe Orton\njoe@orton.demon.co.uk ... jeo101@york.ac.uk\nhttp://www.orton.demon.co.uk/\n\n\n\n", "id": "lists-012-10741111"}, {"subject": "Re: Last Call: Applicability Statement for HTTP State Management  to BC", "content": "Larry Masinter:\n>\n[....]\n>Anyone who is not satisfied with the resolution of this dispute\n>can follow the dispute reoslution procedures outlined in RFC 2026,\n>but I maintain that, as far as the IETF process is concerned, the\n>working group reached \"consensus\" (albeit rough) on the entire\n>document.\n\nDear Larry,\n\nI am sorry if you took my assertion about a lack of consensus on 2109\nsomewhat personally in your capacity as HTTP working group chair.\nReading your message, with all its invocation of procedural machinery,\nI am unsure whether you believe I was trying to somehow dispute old\nactions of the chair.  I was not.  I have no strong interest in\nexamining the question of whether or not the HTTP wg chair applied any\nrules correctly some N years ago.  For what it is worth, I certainly\nrecall that, after 2109 was published, it became much more\ncontroversial than its draft had ever been in the working group.\n\nThe main point I have been trying to make here is that there is\n*currently*, in my considered opinion, no consensus, rough or\notherwise, on the correct default for the third party cookie filter in\nthe state-man-mec which is now in IETF last call.  After publication\nof 2109 there has been a lot of discussion, but little convergence.  I\ntrust the IESG to sort out the process implications of all this when\nthey make a decision on the draft.\n\nNow, as you were also somewhat involved in the post-2109 discussions,\nI would be interested in your view of the level of agreement reached\ntherein.\n\n\n>Larry Masinter\n>(as HTTP working group chair)\n\nKoen.\n\n\n\n", "id": "lists-012-10750765"}, {"subject": "RE: rfc2617: responseauth calculatio", "content": "> From: Joe Orton\n> Subject: RE: rfc2617: response-auth calculation\n>\n> Ah, can auth-int be used for messages with no body (zero-length), e.g.\n> GET requests? I presumed it couldn't, maybe this is the source of my\n> confusion.\n\nI'm not sure where you would have gotten that impression, but yes, a GET\nrequest can use auth-int.  The hash of an empty message body is\n'd41d8cd98f00b204e9800998ecf8427e'.\n\n> (I'm writing client code).\n\nFeel free to test against 'digest-test.agranat.com'; send feedback to me.\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10760751"}, {"subject": "Re: http 1.1. Cache Response Codes..", "content": ">>>>> \"MW\" == Michael Wexler <mwexler@nny.com> writes:\n\nMW> If browser has url in its local cache, it still sends a get request to the\nMW> server, but an option says \"I already have it, just letting you log the\nMW> request\". The success code is a 209, \"user has non-expired data in cache\".\n\nMW> If a non local cache has the data, same system.  The option can be\nMW> different, and we can even use a different code (210), but for the most\nMW> part, we should just let the 209 mean \"cached request\".\n\n  How does this differ from the use of conditional requests and the\n  304 Not Modified response?\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n", "id": "lists-012-1076848"}, {"subject": "Re: Date header handlin", "content": "\"Nottingham, Mark (Australia)\" <mark_nottingham@exchange.au.ml.com> writes\n\n    14.18 says that the Date header \"represents the date and time at\n    which the message was originated, having the same semantics as\n    orig-date in RFC 822.  [...] The HTTP-date sent in a Date header\n    SHOULD NOT represent a date and time subsequent to the generation\n    of the message.\"\n\n    Is the term 'message' used here as it is defined in 1.3? The\n    reference to 822 seems to indicate it is bound to the entity\n    generation date, not the message generation date.\n\n    Also to support this,\n    * 13.5.1 implies that Date is a end-to-end header, and SHOULD NOT\n    be changed by a cache (although it isn't specifically forbidden in\n    13.5.2).\n    * 13.2.3 calculates entity age based on the Date header; if Date\n    were modified after entity generation, this would be invalid.\n\nOuch.  The text from 14.18 probably should have said \"end-to-end\nmessage\", since it does seem to be ambiguous (although one might\nbelieve that the word \"originated\" implies \"origin server\",\nthis is clearly open to misinterpretation).\n\nThere is also text in 13.2.3 saying:\n\n   HTTP/1.1 requires origin servers to send a Date header, if possible,\n   with every response, giving the time at which the response was\n   generated (see section 14.18). We use the term \"date_value\" to denote\n   the value of the Date header, in a form appropriate for arithmetic\n   operations.\n\nI think the lack of clarity may reflect the assumption that\nall responses carry a Date from the origin server, and so\nwe just assumed that proxies wouldn't be adding them except\nwhen specifically required by 14.18.\n\n    I've noticed that some Web caches will change the response's Date\n    header to reflect the time that they generate the message. Based on\n    the above, this seems to be incorrect. Others will use the original\n    Date, but not update the header upon validation (as in 13.5.3).\n\n    Is this correct? What's the proper behaviour here?\n\nIt's hard to define \"proper behaviour\" for HTTP/1.0 caches,\nsince the HTTP/1.1 spec doesn't formally apply to them, and\nthe HTTP/1.0 spec wasn't very detailed in this area.  HTTP/1.1\ncompliant proxies, based on 13.5.1, SHOULD NOT modify Date\nheaders (i.e., not without a very good reason, and I can't\nthink of any right now).  And 13.5.3 clearly says that\nHTTP/1.1-compliant proxies MUST update the Date of a stored\ncache entry upon validation.\n\nSo if the \"some Web caches\" you refer to are claiming to\nbe HTTP/1.1 implementations, these are in violation of\nthe spec.  Otherwise, we're stuck with the history of HTTP/1.0.\n\n-Jeff\n\n\n\n", "id": "lists-012-10769374"}, {"subject": "RE: Date header handlin", "content": "[...]\n> So if the \"some Web caches\" you refer to are claiming to\n> be HTTP/1.1 implementations, these are in violation of\n> the spec.  Otherwise, we're stuck with the history of HTTP/1.0.\n\nDepends on if you read the request header or their marketing material.\n*grin*\n\nMost cache vendors will say that they implement \"all of the important parts\nof 1.1\" without actually claiming compliance. In reality, they support a few\n1.1 features, while formally stating that they're 1.0. To be fair, Caches\nprobably have the hardest job to implement 1.1. \n\n\n\n", "id": "lists-012-10778976"}, {"subject": "Frame", "content": "Hello!\n\nI?m new to the list. I joined it because I?ve a urgent question. I start a\ncgi-script from a frame which is part of a frameset. Is there any\npossibility to redirect the output of this script to the _top-frame? I found\na solution which uses JavaScript, but I don?t want to force my Users to turn\nJavaScript on.\nI think a clean solution would be to include a line like\n\nTarget-Frame: _top\n\nin the HTTP-response header of my cgi-script, but I couldn?t find such a\ncommand in the HTTP-RFC?s.\n\nThanks in advance for any helpful comment on this!\n\nGreetings, Stefan Borggraefe!\n----\nStefan Borggraefe, University of Dortmund, Student of Computer Science\nPermanent : borggraefe@bigfoot.com     ICQ: 41716666\nCurrent   : stefan@lottinet.ping.de\n\n\n\n", "id": "lists-012-10786335"}, {"subject": "RE: Frame", "content": "> I?m new to the list. I joined it because I?ve a urgent question. I start a\n> cgi-script from a frame which is part of a frameset. Is there any\n> possibility to redirect the output of this script to the\n> _top-frame? I found\n> a solution which uses JavaScript, but I don?t want to force my\n> Users to turn\n> JavaScript on.\n\nYou can put a 'target=\"_top\"' attribute in either an anchor <a ...> tag or\nan HTML form <form ...> tag; the results will go to the indicated frame.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10793929"}, {"subject": "RE: rfc2617: responseauth calculatio", "content": "> > Ah, can auth-int be used for messages with no body (zero-length), e.g.\n> > GET requests? I presumed it couldn't, maybe this is the source of my\n> > confusion.\n> \n> I'm not sure where you would have gotten that impression,\n\nOnly my imagination... I just kind of presumed that H(entity-body) would\nnot be defined if there was 'no' entity-body... I see this is wrong now.\n\n> but yes, a GET request can use auth-int. \n\nGreat - \"I geddit now\". Thanks.\n\n> > (I'm writing client code).\n> \n> Feel free to test against 'digest-test.agranat.com'; send feedback to me.\n\nWill do.\n\nRegards,\n\njoe\n\n-- \nJoe Orton\njoe@orton.demon.co.uk ... jeo101@york.ac.uk\nhttp://www.orton.demon.co.uk/\n\n\n\n", "id": "lists-012-10801545"}, {"subject": "Expires and 'noncacheable", "content": "In 14.9.3,\n[...]\nMany HTTP/1.0 cache implementations will treat an Expires value that is less\nthan or equal to the response Date value as being equivalent to the\nCache-Control response directive 'no-cache'. If an HTTP/1.1 cache receives\nsuch a response, and the response does not include a Cache-Control header\nfield, it SHOULD consider the response to be non-cacheable in order to\nretain compatibility with HTTP/1.0 servers.\n\nWould it be safe to assume 'non-cacheable' can be interpreted as 'stale'\nhere? (everything else says it is)\n\n\n\nMark Nottingham\nInternet Project Manager\nMerrill Lynch Australasia (Melbourne)\n\n\n\n", "id": "lists-012-10809970"}, {"subject": "RE: Expires and 'noncacheable", "content": "> From: Nottingham, Mark (Australia)\n\n> In 14.9.3,\n> [...]\n> Many HTTP/1.0 cache implementations will treat an Expires value\n> that is less\n> than or equal to the response Date value as being equivalent to the\n> Cache-Control response directive 'no-cache'. If an HTTP/1.1 cache receives\n> such a response, and the response does not include a Cache-Control header\n> field, it SHOULD consider the response to be non-cacheable in order to\n> retain compatibility with HTTP/1.0 servers.\n>\n> Would it be safe to assume 'non-cacheable' can be interpreted as 'stale'\n> here? (everything else says it is)\n\nI usually use 'stale' to mean something that it was ok to cache for some\namount of time, but that time has passed, so it is no longer ok to use the\ncached copy.  'non-cacheable' means that it should not get into the cache at\nall.\n\n\n\n", "id": "lists-012-10817807"}, {"subject": "RequestStartSec and RequestStartUse", "content": "Hello all,\n\nThis may be slightly off topic, but I've recently been\nrunning across a server that returns the HTTP headers\nRequestStartSec and RequestStartUsec. I've been unable\nto find any description of how to interpret these \nheaders in any rfc, discussion group, web search, etc.\n\nBelow is an example:\nRequestStartUsec: 12344\nRequestStartSec: 992929229\n\nThese headers are contained within an otherwise rfc\ncompliant HTTP1.1 response. I was hoping someone on \nthis list may know something about these rouge headers.\nI'd like my client to do more than ignore them if they\nhave some useful purpose.\n\nRegards,\n\n\n\n--\n-----------------------------------------------------\nDoug Reed\nmailto:douglas.e.reed@worldnet.att.net\nhome: 561-738-6922\nmobile: 561-350-7051\n-----------------------------------------------------\n\n\n\n", "id": "lists-012-10826257"}, {"subject": "RE: Last Call: Applicability Statement for HTTP State Management  to BC", "content": "[ietf@ietf.org removed from distribution]\n\nOn Thu, 15 Jul 1999, Larry Masinter wrote:\n\n> Koen Holtman wrote:\n> > There has never been a real consensus for 2109 or state-man-mec. \n> \n> \n> At the time these documents were forwarded, it was my belief that\n> there was rough consensus for 2109 and, subsequently, state-man-mec.\n> The objections were to the mandatory default behavior necessary to\n> achieve the desired protections of the security considerations\n> section.\n\nYes, exacly, there have always been objections to the default behavior of\nthe third party cookie filter, that was my main point.  The level of\nconsensus on the rest of the specification has been considerably higher.\n\n> I believe it is still in the best interest of the Internet to publish\n> these documents, and that (unless there have been significant reversals\n> of previously strongly-held opinion) they still represent the\n> \"rough consensus\" of the community.\n\nIt is my understanding that a lack of consensus on the security\nconsiderations (in this case a lack of consensus on how much privacy is\nneeded by default) implies a lack of consensus on the whole specification.\n\n> \n> As for the specific comments:\n\nNote: these are my specific nitpicks of iesg-http-cookies.  My main\nobjection to publishing the current draft as a BCP lies somewhere else,\nsee my original message.\n\n> \n> > - I believe that the '2.2.1. Leakage of Information to Third Parties'\n> > requirements in iesg-http-cookies are good practice.  However, I think\n> > that the '2.2.1. Use as an Authentication Mechanism' requirements go\n> > too far: for some cases where the security concerns of authentication\n> > are minor and the usability concerns are major, I consider the use of\n> > cookies for authentication to be a valid engineering solution.\n> \n> This is covered in the last paragraph of section 2.2.1.\n\nThe first SHOULD NOT in that section is fairly definite, and I am mainly\nobjecting to that one.  The second SHOULD NOT weakens it, and one can\nindeed argue that the last paragraphs gives even more wiggle room, even if\nit does not contain any uppercase keywords.  A bit of rebalancing of\nSHOULDs will probably resolve my problem. \n \n> > - (section 2:) The HTTP Content-Location header field cannot currently\n> > be used in a scheme to maintain state, except when it is part of a\n> > HTTP redirect.\n> \n> yes, that's the context. What's the problem?\n\nThe section implies that Content-Location can be used to maintain state on\nits own in some way, outside the context of redirects.  In fact, I just\nrecall that HTTP redirection uses 'location', not 'content-location', so\nreally content-location has little to do with any currently feasible\nmethod of maintaining state. \n\n> \n> > - (section 2:) I do not agree to the statement that state management\n> > represents a 'marginal (but still useful) increase in functionality\n> > over ordinary HTTP and HTML.'  The increase is not marginal for people\n> > who want to build reliable services that maintain state.  A major\n> > advantage of cookies not mentioned in section 2 is that cookie state\n> > is insensitive to users navigating the site using the 'back' button or\n> > history functions.\n> \n> You object to the word \"marginal\", but its usage in this context\n> is consistent with the intent and your statements. And the list\n> of advantages need not be listed exhaustively.\n\nI am suggesting a way to improve the presentation.  \"marginal\" does not\nsound right to me, even if you believe it to be consistent with what \nI want.\n\n> \n> > - (section 3:) 'While such exposure is possible, this is not a flaw in\n> > the protocol itself'.  It _is_ a flaw in the protocol itself, one that\n> > was inherited from Netscape cookies.  Other less flawed state\n> > management protocols would have been possible, but were pre-empted by\n> > cookies.  If state-man-mec is called flawless in an IESG BCP document,\n> > this implies to me that new protocols with similar flaws would have a\n> > good chance of passing an IETF/IESG security considerations review,\n> > which they clearly should not.\n> \n> Well, its hard to equate the lack of a single flaw into \"flawless\",\n> but perhaps a \"primarily\", as in \"it is not primarily a flaw in the\n> protocol itself\" would satisfy your concern.\n\nI said that I believe it _is_ a flaw in the protocol, your proposed\nrestatement would not work to satisfy my concern, which is about weakening\nthe level of what is acceptable in an ietf standards track protocol. \n\nKoen.\n\n\n\n", "id": "lists-012-10833560"}, {"subject": "Re: Last Call: Applicability Statement for HTTP State Management  to BC", "content": "On Wed, 23 Jun 1999, The IESG wrote:\n\n> \n> The IESG has received a request from the IETF Steering Group Working\n> Group to consider Applicability Statement for HTTP State Management\n> <draft-iesg-http-cookies-00.txt> as a BCP.\n> \n> The IESG will also consider HTTP State Management Mechanism\n> <draft-ietf-http-state-man-mec-10.txt> as a Proposed Standard.\n[...]\n\n\n\nDear IESG,\n\nFirst, let me join in thanking David Kristol for the amount of work done. \nIt would be nice to declare victory, publish RFCs, and move on.  However\nthings are not that simple. \n\nI believe that the current cookie situation on the internet is\nseriously broken as far as privacy is concerned.  However, I don't\nbelieve that moving the above two drafts to BCP and proposed standard\nwill benefit the internet community.  I would prefer it if either\n\n- the IESG did nothing and let the two drafts disappear\n  silently from the internet draft repository, or\n\n- state-man-mec were moved to experimental, superseding 2109,\n  with iesg-http-cookies disappearing silently.\n\nBasically state-man-mec is too controversial to end up on the standards\ntrack.  The controversy surrounding the state management work had some\ngood effects, so I don't feel that the effort on it was wasted, but going\non to publish proposed standards would be a bit strange given all the\ncontroversy. \n\n\n* On cookies and the IETF\n\nI believe that the cookie situation that we live with now would\nbenefit from both a) some tight spec writing and b) a lot of social\nengineering.  I don't expect many people to disagree to a).  The\nNetscape cookie specification has a lot of vague parts and it would be\nnice to have a better spec that clears them up.  Point b) is more\ncontentious: there are people out there who believe that the current\ncookie situation is perfectly OK.  From my state management\ndiscussions with them in e-mail over the last few years, I conclude\nthat their number is significant.  These are not just one or two\ndisagreeable individuals who can be ignored in declaring an IETF\nconsensus.\n\nWhat we really have here is a political struggle between the\n'commercial complex' which wants to obtain and resell a lot of\nprivacy-related information from end users as a means of supporting\ncontent, and a 'consumer complex' which believes that this way of\nsupporting content is either inherently wrong, or that the consumer is\ngetting far too little value out of the current deal.\n\nI believe that this political struggle cannot be resolved through the\nIETF consensus process.  (It might be possible to silently complete an\nIETF last call without anybody in the 'commercial complex' noticing,\nbut that won't resolve anything.)\n\nWhat the IESG and IETF can feasibly do with respect to cookies:\n\n1. the IESG could release policy statements with respect to cookies,\n   which are clearly independent from the IETF consensus/standards\n   process (draft-iesg-http-cookies-00 is not very good as such a\n   statement, see below)\n\n2. the IETF can write an informational spec which tightens up the\n   definition of Netscape cookies, but which does not add any new\n   security/privacy restrictions beyond those in the\n   Netscape spec (2109/state-man-mec is not such a spec)\n\n3. the IETF could try to change the playing field of the political\n   struggle by developing credible specifications of *optional*\n   protocol-level 'cookie filters' to enhance privacy, or privacy\n   negotiation mechanisms.  (2109/state-man-mec contains the\n   `unverifiable transaction' or 'third party cookie' mechanism, which\n   is an example of what I mean, by a 'cookie filter', though the use\n   of it is not optional in these specs, the filter should be switched\n   on by default.)  Of course a number of browser implementers already\n   provide various forms of cookie filters, and this is a good thing.\n   Public specifications of such filters would be very valuable to the\n   designers of stateful sites, to ensure that their sites will keep\n   working, or fail gracefully, if certain filters are switched on or\n   off.\n\nActions 2. and 3. would require significant new editorial work, it is\nunclear whether we would see it being done even if there is a broad\nfeeling that this is the way to go.  In any case both 2. and 3. are\ncompatible with dropping state-man-mec from the repository or moving\nit to experimental.\n\n\n* Comments on draft-ietf-http-state-man-mec-10.txt:\n\nThere has never been a real consensus for 2109 or state-man-mec.  The\nmain breaking point has always been that the draft specifies that the\n'cookie filter' for third party cookies (cookies in unverifiable\ntransactions) should be switched 'on' by default.  Basically there is\na large group of web advertisers and people depending on web\nadvertising who believe that the filter should be 'off' by default.\nMost significantly, the two major browser implementors believe (or I\nshould say their representatives implied this at one point, and I have\nnot seen a reversal since) that their interests are best served by\nproviding implementations which do not filter out third party cookies\nby default.\n\nPushing state-man-mec onto the standards track without a good IETF\nconsensus may be something the IESG can 'legally' do (I would not\nknow), but I don't see how the internet community would be served by\nthis.\n\nYaron Goland commented recently that state-man-mec has too few\nimproved elements over the old Netscape cookies to warrant a\nchangeover of the infrastructure.  I agree to some extent: I would say\nthat the changes to browser implementations and site policies that I\nwant could be made without changing the Netscape cookie wire protocol.\n\nYaron also said that:\n|The key failure in cookie security is authentication, the ability to\n|know exactly with whom you are dealing. As we all know, domains can\n|not provide this information which is the core of the cookie security\n|problem.\n\nI'm not sure if Yaron is referring to the need for some kind of\ncross-domain identification scheme, or to the need for signed\ncertificates or similar IDs.  In either case I disagree: the key\nfailure in cookie security is inadequate support and defaults for user\nprivacy in most deployed browsers.  Adding more authentication to the\nwire protocol won't solve this core problem.\n\n\n* Comments on draft-iesg-http-cookies-00.txt:\n\nMy main problem with iesg-http-cookies is that it sets out to define a\nstandard that does not have an RFC number but is in stead is made up\nof two RFCs on different tracks.  Some quotes:\n\n|Uses in the latter category should be considered violations of the\n|standard, even though the actual protocol implementations may conform\n|to the standard.  This memo, along with RFC- XXXX, is considered part\n|of the HTTP State Management protocol specification.\n\n|The following uses of HTTP State Management are deemed inappropriate\n|and a violation of the standard:\n\niesg-http-cookies can be read as a kind of appendix that introduces\nnew MUST-level requirements into the standards-track IETF\nstate-man-mec wire protocol specification through the back door.\nWhile this is not a very friendly interpretation, it is a plausible\none.  As such, iesg-http-cookies exposes the IESG and IETF to the risk\nof some rather effective attacks should the mass media flamewar over\ncookies flare up again, attacks which could lower the credibility of\nthe whole IETF process.\n\nI am not against the IESG entering the cookie privacy debate,\nespecially if it is on the side that I belong to.  But if it does,\nmuch more care should be taken to separate IESG policy statements from\nthe IETF consensus process.  Also a pure policy statement, if made,\nshould apply to both Netscape and state-man-mec cookies.\n\nComments on details:\n\n- I believe that the '2.2.1. Leakage of Information to Third Parties'\nrequirements in iesg-http-cookies are good practice.  However, I think\nthat the '2.2.1. Use as an Authentication Mechanism' requirements go\ntoo far: for some cases where the security concerns of authentication\nare minor and the usability concerns are major, I consider the use of\ncookies for authentication to be a valid engineering solution.\n\n- (section 2:) The HTTP Content-Location header field cannot currently\nbe used in a scheme to maintain state, except when it is part of a\nHTTP redirect.\n\n- (section 2:) I do not agree to the statement that state management\nrepresents a 'marginal (but still useful) increase in functionality\nover ordinary HTTP and HTML.'  The increase is not marginal for people\nwho want to build reliable services that maintain state.  A major\nadvantage of cookies not mentioned in section 2 is that cookie state\nis insensitive to users navigating the site using the 'back' button or\nhistory functions.\n\n- (section 3:) 'While such exposure is possible, this is not a flaw in\nthe protocol itself'.  It _is_ a flaw in the protocol itself, one that\nwas inherited from Netscape cookies.  Other less flawed state\nmanagement protocols would have been possible, but were pre-empted by\ncookies.  If state-man-mec is called flawless in an IESG BCP document,\nthis implies to me that new protocols with similar flaws would have a\ngood chance of passing an IETF/IESG security considerations review,\nwhich they clearly should not.\n\nKoen.\n\n\n\n", "id": "lists-012-10848087"}, {"subject": "Re: http 1.1. Cache Response Codes..", "content": "Have you reviewed RFC 2227 -- \"Simple Hit-Metering and Usage-Limiting for\nHTTP\"? Seems like this RFC is trying to solve your problem. \n\nDave Morris\n\nOn Thu, 19 Mar 1998, Michael Wexler wrote:\n\n> As a web measurement analyst, one of my worst problems is dealing with\n> caching.  Why do we use the code 200 for everything?  Why not design the\n> spec in a graduated fashion:\n> \n> If browser has url in its local cache, it still sends a get request to the\n> server, but an option says \"I already have it, just letting you log the\n> request\". The success code is a 209, \"user has non-expired data in cache\".\n> \n> If a non local cache has the data, same system.  The option can be\n> different, and we can even use a different code (210), but for the most\n> part, we should just let the 209 mean \"cached request\".\n> \n> This solves many problems:\n> 1) path analysis of a user's visit\n> 2) advertising requests (not perfectly, given the IAB's recent standards,\n> but better than nothing for smaller sites)\n> 3) pages per visit calculations are accurate\n> 4) this is a minor increase in bandwidth compared to not sending the\n> request at all, and is far superior to eliminating caching.\n\n\n\n", "id": "lists-012-1085327"}, {"subject": "RE: Expires and 'noncacheable", "content": "On Wed, 21 Jul 1999, Scott Lawrence wrote:\n\n> > From: Nottingham, Mark (Australia)\n> \n> > In 14.9.3,\n> > [...]\n> > Many HTTP/1.0 cache implementations will treat an Expires value\n> > that is less\n> > than or equal to the response Date value as being equivalent to the\n> > Cache-Control response directive 'no-cache'. If an HTTP/1.1 cache receives\n> > such a response, and the response does not include a Cache-Control header\n> > field, it SHOULD consider the response to be non-cacheable in order to\n> > retain compatibility with HTTP/1.0 servers.\n> >\n> > Would it be safe to assume 'non-cacheable' can be interpreted as 'stale'\n> > here? (everything else says it is)\n> \n> I usually use 'stale' to mean something that it was ok to cache for some\n> amount of time, but that time has passed, so it is no longer ok to use the\n> cached copy.  'non-cacheable' means that it should not get into the cache at\n> all.\n\nI agree ... 'non-cacheable' is supposed to preclude any possibility that\nthe data would be found in a cache and in particular a disk cache where\nit might be examined. 'non-cacheable' is supposed to mean that it would\nnever be valid for a caching proxy to return in response to a request.\nNo second guessing, ever.\n\nDave Morris\n\n\n\n", "id": "lists-012-10866651"}, {"subject": "RE: Expires and 'noncacheable", "content": "I don't dispute the meaning of 'non-cacheable'. However, I don't know that\nit's the correct term to use here.\n\nThe use of non-cacheable here is internally inconsistent; Cache-Control:\nno-cache means that the object must be revalidated (it's stale), not that it\ncan never enter the cache (that's no-store).\n\n14.9.3:\nMany HTTP/1.0 cache implementations will treat an Expires value that is less\nthan or equal to the response Date value as being equivalent to the\nCache-Control response directive 'no-cache'. If an HTTP/1.1 cache receives\nsuch a response, and the response does not include a Cache-Control header\nfield, it SHOULD consider the response to be non-cacheable in order to\nretain compatibility with HTTP/1.0 servers.\n\nAdditionally, it contradicts:\n\n13.2.1:\nIf an origin server wishes to force a semanticlly transparent cache to\nvalidate every request, it MAY assign an explicit expiration time in the\npast. This means that the response is always stale, and so the cache SHOULD\nvalidate it vefore using it for subsequent requests.\n\n13.4:\nUnless specifically constrained by a cache-control (section 14.9) directive,\na caching system MAY always store a successful response (see section 13.8)\nas a cache entry, MAY return it without validation if it is fresh, and MAY\nreturn it after successful validation.\n\n\n\n\n\n\n\n\n\n\n> > I usually use 'stale' to mean something that it was ok to \n> > cache for some\n> > amount of time, but that time has passed, so it is no \n> > longer ok to use the\n> > cached copy.  'non-cacheable' means that it should not get \n> > into the cache at all.\n> \n> I agree ... 'non-cacheable' is supposed to preclude any \n> possibility that\n> the data would be found in a cache and in particular a disk \n> cache where\n> it might be examined. 'non-cacheable' is supposed to mean \n> that it would\n> never be valid for a caching proxy to return in response to a request.\n> No second guessing, ever.\n\n\n\n", "id": "lists-012-10875681"}, {"subject": "Comments on HTTP TLS Upgrade draf", "content": "Here are some comments on the draft-ietf-tls-http-upgrade-01.txt draft.\nFirst, I would like to say that I think it is crucial to the integrity of\nthe Web that it become possible to negotiate transport stacks dynamically\ninstead of using new URI schemes. Now to the comments:\n\n* Chapter 5.1: There is no link between the use of the Upgrade header field\nand the cachability of a response as you indicate in the second last\nparagraph. Upgrade is optional and it is fully ok for a cache to serve an\nalready cached HTTP response either using HTTP or HTTP/TLS (if the cache\nchooses to honor the Upgrade header field). Obviously, it is not OK to do\nit the other way.\n\nIf the client starts with an OPTIONS request and the cache honors the\nupgrade request, subsequent requests on that secure connection must of\ncourse be served on top of the same secure connection.\n\n* Chapter 7: Saying that the protocol is HTTP doesn't say a whole lot more\nabout the service than saying that it is TLS.\n\nMore importantly, the Upgrade header field is also intended to upgrade HTTP\nitself, for example from HTTP/1.1 to some future HTTP/3.0. However, when\nyou collapse HTTP and TLS into one token, you loose the capability (or run\ninto a multiplicative explosion) expressing that the application would like\nto upgrade to both HTTP/3.0 *and* TLS/1.0. Instead it is more useful to do\nit like this:\n\nUpgrade: HTTP/3.0, TLS/1.0\n\nIn other words, there is no need for the \"clustering\" of Upgrade header\nfield tokens.\n\nA general problem with the Upgrade header field tokens is, however, that\nthey do not allow for parameters to be passed along: How do I specify other\nparameters such as required encryption strength, the scope of resources\navailable through that secure link, etc? It should not be part of the\nextension mechanism to describe such parameters directly but it must be\npossible for the application to at least pass that information through.\n\n* Chapter 9: There is no guarantee that \"http://w3.org\" and\n\"https://w3.org\" point to the same resource just as well as there is no\nguarantee that \"ftp://w3.org\" and \"http://w3.org\" point to the same\nresource. In general, it is not possible to declare that the http namespace\ncan take over the https namespace as you write in chapter 9.\n\nWhat instead you *can* do is to provide a mechanism for making http: URIs\nsecure and for resolving https: URIs using HTTP just as well as ftp: URIs\ncan be resolved using an HTTP gateway. \n\n* I think there is a problem with the OPTIONS method and how you specify\nthat it should interact with the upgrade header field. The OPTIONS request\ncan be forwarded all the way to the origin server as described in HTTP/1.1\nsection 5.1.2. However, the Upgrade header field can't (which you also\npoint out) as it is a hop-by-hop header field. For example, the user agent\nsends an OPTIONS request like this:\n\nOPTIONS http://w3.org HTTP/1.1\nUpgrade : HTTP+TLS/1.0\nConnection: Upgrade\n\nthis will be forwarded through the first proxy as\n\nOPTIONS http://w3.org HTTP/1.1\nor\nOPTIONS * HTTP/1.1\nHost: w3.org\n\nNote that the proxy hasn't agreed on tunnelling yet - it has only forwarded\nthe request according to normal HTTP/1.1 rules. If the user agent instead\nsends a request like this:\n\nOPTIONS * HTTP/1.1\nHost: w3.org\nUpgrade : HTTP+TLS/1.0\nConnection: Upgrade\n\nthen the request is intended for the proxy only (in the role as a server)\nand won't get forwarded. In order for the proxy to forward the request for\na tunnel, it must itself send a new OPTIONS * request which isn't sent on\nbehalf of the user agent but of the proxy because the two OPTIONS *\nrequests (the one from the user agent and the one from the proxy) are not\nrelated in any way.\n\nI believe the intended semantics (I call it \"repeated hop-by-hop\") can be\nmodelled by the HTTP extension framework like this (and is sort of the idea\nof the CONNECT draft [1]):\n\nM-OPTIONS http://w3.org HTTP/1.1\nHost: w3.org\nMan: \"http://www.iana.org/http/ext/tls\"\n\nThis request will be tunneled through the proxy because it doesn't know the\nextension (or if it does then it will know what it must tunnel), or it will\nfail and refuse to cooperate (which also is the case for the OPTIONS\nmethod). As the extension is not hop-by-hop, it will be forwarded all the\nway to the ultimate recipient.\n\nThe response could of course still be a 101 Switching Protocols which would\nthen be propagated all the way back to the user agent and all\nintermediaries will already be in tunnel mode.\n\nFurthermore the extension framework would allow for additional parameters\nto be sent in the request, like for example the required key length etc:\n\nM-OPTIONS http://w3.org HTTP/1.1\nHost: w3.org\nMan: \"http://www.iana.org/http/ext/tls\"; ns=65\n65-keylength: 128\n\nSomething to consider,\n\nHenrik\n\nps: Reference 7 ([7]) doesn't seem to be used anywhere\n\n[1]\nhttp://www.kashpureff.org/nic/drafts/draft-luotonen-web-proxy-tunneling-00.t\nxt.html\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-10884599"}, {"subject": "RE: Comments on HTTP TLS Upgrade draf", "content": "> From: Henrik Frystyk Nielsen [frystyk@w3.org]\n> Subject: Comments on HTTP TLS Upgrade draft\n\n> [...]\n> First, I would like to say that I think it is crucial to the integrity of\n> the Web that it become possible to negotiate transport stacks dynamically\n> instead of using new URI schemes. [...]\n\nWe'll accept that as a supporting comment :-)\n\n> * Chapter 5.1: There is no link between the use of the Upgrade header\nfield\n> and the cachability of a response as you indicate in the second last\n> paragraph. Upgrade is optional and it is fully ok for a cache to serve an\n> already cached HTTP response either using HTTP or HTTP/TLS (if the cache\n> chooses to honor the Upgrade header field). Obviously, it is not OK to do\n> it the other way.\n\nI believe that you are referring to:\n\n   Furthermore a caching proxy SHOULD not reply to a request with\n   Upgrade tokens from its cache.\n\nThe intent here is just to remind the proxy that Upgrade, as a\nhop-by-hop header, is not appropriate to cache with a response and\nforward as part of that response; it is not a property of the returned\nentity, but of the immediate (in this case origin-proxy) connection.\n\n> If the client starts with an OPTIONS request and the cache honors the\n> upgrade request, subsequent requests on that secure connection must of\n> course be served on top of the same secure connection.\n\nThe rules that govern what happens after any upgrade are those of the\nprotocol to which the connection has changed, and are therefor beyond\nthe scope of this document.\n\n> * Chapter 7: Saying that the protocol is HTTP doesn't say a whole lot more\n> about the service than saying that it is TLS.\n\n> More importantly, the Upgrade header field is also intended to upgrade\nHTTP\n> itself, for example from HTTP/1.1 to some future HTTP/3.0. However, when\n> you collapse HTTP and TLS into one token, you loose the capability (or run\n> into a multiplicative explosion) expressing that the application would\nlike\n> to upgrade to both HTTP/3.0 *and* TLS/1.0. Instead it is more useful to do\n> it like this:\n\n> Upgrade: HTTP/3.0, TLS/1.0\n\n> In other words, there is no need for the \"clustering\" of Upgrade header\n> field tokens.\n\nWe considered this alternative at some length, and at one point had\nwritten the draft that way.  We liked the idea of the Upgrade response\nheader being able to specify a series of protocol tokens so as to\nspecify a stack to be used after the change.  We were not sure that\nthis would be understood, however.  RFC 2616 is vague on just what\nmultiple tokens could mean in an Upgrade response.  We elected for the\nsimpler set of expectations.\n\n> A general problem with the Upgrade header field tokens is, however, that\n> they do not allow for parameters to be passed along: How do I specify\nother\n> parameters such as required encryption strength, the scope of resources\n> available through that secure link, etc? It should not be part of the\n> extension mechanism to describe such parameters directly but it must be\n> possible for the application to at least pass that information through.\n\nThe syntax just doesn't have a space for it (for some reason, even the\nusual extension mechanism of 'stuff after a semicolon' was omitted\nfrom the Upgrade syntax).  In any event, TLS provides for negotiation\nof these parameters directly; I suspect that leaving option\nnegotiation to the protocol to which we are switching (as opposed to\ntrying to shortcut it by doing it in HTTP) is the better approach.\n\n> * Chapter 9: There is no guarantee that \"http://w3.org\" and\n> \"https://w3.org\" point to the same resource just as well as there is no\n> guarantee that \"ftp://w3.org\" and \"http://w3.org\" point to the same\n> resource. In general, it is not possible to declare that the http\nnamespace\n> can take over the https namespace as you write in chapter 9.\n\nIt was not our intent to redefine the 'https' scheme, or to suggest\nthat implementations should somehow map it to this new mechanism.\nThat scheme exists and will forever mean HTTP over SSL on TCP port\n443.  From the reactions we got, we were evidently not successful at\nmaking this clear.\n\n> * I think there is a problem with the OPTIONS method and how you specify\n> that it should interact with the upgrade header field. The OPTIONS request\n> can be forwarded all the way to the origin server as described in HTTP/1.1\n> section 5.1.2. However, the Upgrade header field can't (which you also\n> point out) as it is a hop-by-hop header field. For example, the user agent\n> sends an OPTIONS request like this:\n\n> OPTIONS http://w3.org HTTP/1.1\n> Upgrade : HTTP+TLS/1.0\n> Connection: Upgrade\n\n> this will be forwarded through the first proxy as\n\n> OPTIONS http://w3.org HTTP/1.1\n> or\n> OPTIONS * HTTP/1.1\n> Host: w3.org\n\n> Note that the proxy hasn't agreed on tunneling yet - it has only forwarded\n> the request according to normal HTTP/1.1 rules. If the user agent instead\n> sends a request like this:\n\n> OPTIONS * HTTP/1.1\n> Host: w3.org\n> Upgrade : HTTP+TLS/1.0\n> Connection: Upgrade\n\n> then the request is intended for the proxy only (in the role as a server)\n> and won't get forwarded.\n\nI'm not sure that I agree with your interpretation of this latter\nformulation - I think that they are both the same, but I'm afraid that\nI do agree with your conclusion that the Upgrade header should not be\nforwarded, which causes some problems with what we suggest as an\nend-to-end solution.  It would require, in effect, that the user agent\nunderstand the entire proxy chain and do a series of OPTIONS/Upgrade\nrequests to create secure connections through each individually (not\nan optimal solution, IMHO).\n\nI wonder what existing proxies really do with Upgrade; in particular:\n - Do they in fact forward it (incorrectly?) in a response if it is\n   _not_ (incorrectly) listed in a Connection header?\n - Whether or not they react to it in a 101 response (our draft would have\n   them switch to tunnel mode - what do they do today)?\n\n> I believe the intended semantics (I call it \"repeated hop-by-hop\") can be\n> modeled by the HTTP extension framework like this (and is sort of the idea\n> of the CONNECT draft [1]):\n\n> M-OPTIONS http://w3.org HTTP/1.1\n> Host: w3.org\n> Man: \"http://www.iana.org/http/ext/tls\"\n\n> This request will be tunneled through the proxy because it doesn't know\nthe\n> extension (or if it does then it will know what it must tunnel), or it\nwill\n> fail and refuse to cooperate (which also is the case for the OPTIONS\n> method). As the extension is not hop-by-hop, it will be forwarded all the\n> way to the ultimate recipient.\n\nThat is still a change to an existing proxy, though - because it\nrequires that the proxy conform to the extensions draft.  I agree that\nwould be a good thing, but I don't think that it changes our situation\nat all now.\n\n> The response could of course still be a 101 Switching Protocols which\nwould\n> then be propagated all the way back to the user agent and all\n> intermediaries will already be in tunnel mode.\n\nBut will they be expecting to stay there for all subsequent requests,\nor will they just switch to tunnel mode for the current request and\nthen expect to be 1.1 again afterwards?\n\n> ps: Reference 7 ([7]) doesn't seem to be used anywhere\n\nWe didn't put the marker in - it is in the acknowledgements; will fix in the\nnext go 'round.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-10897652"}, {"subject": "errata for RFC 2616 &amp; 261", "content": "I think we need to establish errata for RFC 2616 & 2617,\nsince minor problems are popping up.\n\nAre there any volunteers willing to help take this on? \nThere've been a couple of candidates already.\n\nOb-erratum RFC 2616:\n\nSection 3.5 calls for IANA to establish a registry of\n\"content-coding value tokens\" that includes gzip, compress,\nand deflate.\n\n Section 3.6 calls for IANA to establish a registry of\n\"transfer-coding value tokens\" that includes\n\"chunked\" and \"identity\", but goes on to list\n\"gzip\", \"compress\" and \"deflate\". But \"gzip\", \"compress\"\nand \"deflate\" are not transfer-coding value tokens, they're\ncontent-coding value tokens.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-10914196"}, {"subject": "RE: errata for RFC 2616 &amp; 261", "content": "> From: Larry Masinter \n> Subject: errata for RFC 2616 & 2617\n> I think we need to establish errata for RFC 2616 & 2617,\n> since minor problems are popping up.\n> \n> Are there any volunteers willing to help take this on? \n\nSure - how is it done?\n\n  For 2617 we have one from Joe Orton:\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1999/0171.html\n\nappended below - an ABNF error.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\nFrom:Joe Orton [joe@orton.demon.co.uk]\nSent:Thursday, July 08, 1999 14:23\nTo:http-wg@hplb.hpl.hp.com\nSubject:rfc2617: BNF for 'domain'\n\n\nSection 3.2.1.: Does \n\n  domain = \"domain\" \"=\" <\"> URI ( 1*SP URI ) <\">\n\nreally describe \"A quoted, space-separated list of URIs\"? Should there\nbe a '*' before the '('?\n\njoe\n\n-- \nJoe Orton\njoe@orton.demon.co.uk ... jeo101@york.ac.uk\nhttp://www.orton.demon.co.uk/\n\n\n\n", "id": "lists-012-10922629"}, {"subject": "Re: errata for RFC 2616 &amp; 261", "content": "In a previous episode Larry Masinter said...\n:: \n\n:: Ob-erratum RFC 2616:\n:: \n:: Section 3.5 calls for IANA to establish a registry of\n:: \"content-coding value tokens\" that includes gzip, compress,\n:: and deflate.\n:: \n::  Section 3.6 calls for IANA to establish a registry of\n:: \"transfer-coding value tokens\" that includes\n:: \"chunked\" and \"identity\", but goes on to list\n:: \"gzip\", \"compress\" and \"deflate\". But \"gzip\", \"compress\"\n:: and \"deflate\" are not transfer-coding value tokens, they're\n:: content-coding value tokens.\n:: \n\nI don't get it.. but, I am having a bad day so it's most likely my\nfault at interpretation.. but I thought the various compression\ncodings made sense for both content-coding and transfer-coding..\n\nhere's my understanding: they are different things: the former is a\nproperty of the entity, and the latter is a property of the\nmessage.. and the various proxies and clients in the chain are going\nto handle them differently depending on whether they are content or\ntransfer based (i.e. application/postscript that is content-coded gzip\nshould probably be saved to disk as .ps.gz but if it is transfer-coded\ngzip then the UA would like decompress and display, and if anything\ngets saved it's the .ps only), but the algorithms and encodings\nthemselves make sense in both contexts.. whereas something like\nchunked or jeff's delta musings make sense only as transfer encodings.\n\nand I thought that's exactly what the document says... so where am I\nconfused?\n\n-Pat\n\n\n\n", "id": "lists-012-10932798"}, {"subject": "Looking for TIS/Gauntlet contac", "content": "Hi,\n  We're dealing with a compatibility issue with \ngauntlet proxy and IE.  We'd like to know if there\nis someone from TIS on this list.\n\nIf so, please contact me.\n\nJosh\n\n---\nJosh Cohen <josh@microsoft.com>\nProgram Manager IE - Networking Protocols \n\n\n\n", "id": "lists-012-1094081"}, {"subject": "Re: errata for RFC 2616 &amp; 261", "content": "Patrick McManus <mcmanus@appliedtheory.com> writes:\n    ::  Section 3.6 calls for IANA to establish a registry of\n    :: \"transfer-coding value tokens\" that includes\n    :: \"chunked\" and \"identity\", but goes on to list\n    :: \"gzip\", \"compress\" and \"deflate\". But \"gzip\", \"compress\"\n    :: and \"deflate\" are not transfer-coding value tokens, they're\n    :: content-coding value tokens.\n    \n    I don't get it.. but, I am having a bad day so it's most likely my\n    fault at interpretation.. but I thought the various compression\n    codings made sense for both content-coding and transfer-coding..\n    \nYou're right; RFC2616 is pretty clear that there are two distinct\nIANA registries, one for content-coding and one for transfer-coding.\nBoth registries currently contain the subset\ngzip|deflate|compress|identity\nbut only transfer-coding also includes\nchunked\nand only content-coding also includes (as deprecated choices)\nx-gzip|x-compress\n\n    here's my understanding: they are different things: the former is a\n    property of the entity, and the latter is a property of the\n    message.. and the various proxies and clients in the chain are\n    going to handle them differently depending on whether they are\n    content or transfer based (i.e. application/postscript that is\n    content-coded gzip should probably be saved to disk as .ps.gz but\n    if it is transfer-coded gzip then the UA would like decompress and\n    display, and if anything gets saved it's the .ps only), but the\n    algorithms and encodings themselves make sense in both contexts..\n    whereas something like chunked or jeff's delta musings make sense\n    only as transfer encodings.\n\nActually, our \"Delta encoding in HTTP\" draft\nhttp://www.ics.uci.edu/pub/ietf/http/draft-mogul-http-delta-01.txt\nhas a lengthy discussion of this issue, and shows that delta encoding\ncan be done either way - as a content-coding or as a transfer-coding.\n(See section 5.3 of that draft, conveniently titled \"Content-coding or\nTransfer-coding?\".)\n\nThere are some tricky interactions between content-codings,\ntransfer-codings, and range responses, which are explained in section 4\nof that draft (\"Relationship between content-coding, transfer-coding,\nand ranges\"), although these are not specific to delta encoding.\n\nOnce you take these interactions into account, in fact, delta encoding\nusually makes more sense as a content-coding than as a transfer-coding.\n\nIn your example of a response that is marked\nContent-type: application/postscript\nContent-Encoding: gzip\nit's not entirely clear to me whether it should be saved to\ndisk in compressed format, but if one wants to actually display\nthis response in a browser (or a helper application, in this case)\nit should certainly be decompressed first.\n\n-Jeff\n\n\n\n", "id": "lists-012-10942168"}, {"subject": "RE: errata for RFC 2616 &amp; 261", "content": "OK, I blew it. 'compress', 'gzip' and 'deflate' are both\ncontent-encodings and transfer-encodings. Thanks to my\nfriends for correcting me.\n\nSorry for being so quick on the trigger today.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-10952123"}, {"subject": "Netscape-Enterprise http server  proble", "content": "Will someone from Netscape who knows/is responsible for and\nknowledgeable about technical details of Netscape-Enterprise server (3.5.x +)\nget in touch with me soon?\n\nEmail to \"Mike Belshe\" (mbelshe@netscape.com), who had submitted \npublic report as part of HTTP/1.1 Feature List Report Summary \nhttp://www.w3.org/Protocols/HTTP/Forum/Reports/#Public\nbounced.\n\nThis is regarding a security problem that we have discovered with this server.\n\ncheers,\nbala\n\nBalachander Krishnamurthy\nwww.research.att.com/~bala/papers\n\n\n\n", "id": "lists-012-10959579"}, {"subject": "Re: generated hypertext (HTML) version of RFC261", "content": "At 12:06 31/07/1999 -0500, Dan Connolly wrote:\n>Henrik and anybody who's interested...\n>\n>I'm doing some writing about the HTTP 1.1, and I was frustrated\n>that I can't make hypertext links to specific sections, and\n>in general, it's sort of a pain to navigate. So I scratched\n>the itch with a little perl script.\n\nNeat - thank you!\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-10966826"}, {"subject": "RE: Comments on HTTP TLS Upgrade draf", "content": "At 09:45 26/07/1999 -0400, Scott Lawrence wrote:\n\n>> * Chapter 5.1: There is no link between the use of the Upgrade header\n>field\n>> and the cachability of a response as you indicate in the second last\n>> paragraph. Upgrade is optional and it is fully ok for a cache to serve an\n>> already cached HTTP response either using HTTP or HTTP/TLS (if the cache\n>> chooses to honor the Upgrade header field). Obviously, it is not OK to do\n>> it the other way.\n>\n>I believe that you are referring to:\n>\n>   Furthermore a caching proxy SHOULD not reply to a request with\n>   Upgrade tokens from its cache.\n>\n>The intent here is just to remind the proxy that Upgrade, as a\n>hop-by-hop header, is not appropriate to cache with a response and\n>forward as part of that response; it is not a property of the returned\n>entity, but of the immediate (in this case origin-proxy) connection.\n\nI was referring to this in 5.1:\n\n   Furthermore a caching proxy SHOULD not reply to a request with\n   Upgrade tokens from its cache. Clients are still advised to\n   explicitly include \"Cache-control: no-cache\" in this case.\n\nwhich seems to say something else (which in fact is not true) - see below.\n\n>> If the client starts with an OPTIONS request and the cache honors the\n>> upgrade request, subsequent requests on that secure connection must of\n>> course be served on top of the same secure connection.\n>\n>The rules that govern what happens after any upgrade are those of the\n>protocol to which the connection has changed, and are therefor beyond\n>the scope of this document.\n\nIn general yes, but you are not dealing with the general case - you are\ndescribing the very specific case of changing from HTTP on TCP to HTTP on\nSSL. I think you have to describe how responses that have been obtained via\nHTTP can be reused by caches after proper end-to-end validation and served\nvia HTTP/SSL.\n\n>> * Chapter 7: Saying that the protocol is HTTP doesn't say a whole lot more\n>> about the service than saying that it is TLS.\n>\n>> More importantly, the Upgrade header field is also intended to upgrade\n>HTTP\n>> itself, for example from HTTP/1.1 to some future HTTP/3.0. However, when\n>> you collapse HTTP and TLS into one token, you loose the capability (or run\n>> into a multiplicative explosion) expressing that the application would\n>like\n>> to upgrade to both HTTP/3.0 *and* TLS/1.0. Instead it is more useful to do\n>> it like this:\n>\n>> Upgrade: HTTP/3.0, TLS/1.0\n>\n>> In other words, there is no need for the \"clustering\" of Upgrade header\n>> field tokens.\n>\n>We considered this alternative at some length, and at one point had\n>written the draft that way.  We liked the idea of the Upgrade response\n>header being able to specify a series of protocol tokens so as to\n>specify a stack to be used after the change.  We were not sure that\n>this would be understood, however.  RFC 2616 is vague on just what\n>multiple tokens could mean in an Upgrade response.  We elected for the\n>simpler set of expectations.\n\nUnfortunately this leads easily to an explosion of tokens that have to be\nregistered. HTTP doesn't have to say anything about the relationship\nbetween the various tokens in the upgrade header field. It is sufficient\nfor the protocols (or sub protocols) independently to define how they can\ninteract with other protocols. For example, it can be stated in the\nregistration of the SSL upgrade token that it can work with any version of\nHTTP.\n\n>> A general problem with the Upgrade header field tokens is, however, that\n>> they do not allow for parameters to be passed along: How do I specify\n>other\n>> parameters such as required encryption strength, the scope of resources\n>> available through that secure link, etc? It should not be part of the\n>> extension mechanism to describe such parameters directly but it must be\n>> possible for the application to at least pass that information through.\n>\n>The syntax just doesn't have a space for it (for some reason, even the\n>usual extension mechanism of 'stuff after a semicolon' was omitted\n>from the Upgrade syntax).  In any event, TLS provides for negotiation\n>of these parameters directly; I suspect that leaving option\n>negotiation to the protocol to which we are switching (as opposed to\n>trying to shortcut it by doing it in HTTP) is the better approach.\n\nIt is not a short cut - it is an exchange of hints (metadata) which can put\nthe recipient in a better position to select the best choice. It would be a\nshort cut if it was binding for what happens after the upgrade is done - it\nis not.\n\n>> * Chapter 9: There is no guarantee that \"http://w3.org\" and\n>> \"https://w3.org\" point to the same resource just as well as there is no\n>> guarantee that \"ftp://w3.org\" and \"http://w3.org\" point to the same\n>> resource. In general, it is not possible to declare that the http\n>namespace\n>> can take over the https namespace as you write in chapter 9.\n>\n>It was not our intent to redefine the 'https' scheme, or to suggest\n>that implementations should somehow map it to this new mechanism.\n>That scheme exists and will forever mean HTTP over SSL on TCP port\n>443.  From the reactions we got, we were evidently not successful at\n>making this clear.\n\nok\n\n>> * I think there is a problem with the OPTIONS method and how you specify\n>> that it should interact with the upgrade header field. The OPTIONS request\n>> can be forwarded all the way to the origin server as described in HTTP/1.1\n>> section 5.1.2. However, the Upgrade header field can't (which you also\n>> point out) as it is a hop-by-hop header field. For example, the user agent\n>> sends an OPTIONS request like this:\n>\n>> OPTIONS http://w3.org HTTP/1.1\n>> Upgrade : HTTP+TLS/1.0\n>> Connection: Upgrade\n>\n>> this will be forwarded through the first proxy as\n>\n>> OPTIONS http://w3.org HTTP/1.1\n>> or\n>> OPTIONS * HTTP/1.1\n>> Host: w3.org\n>\n>> Note that the proxy hasn't agreed on tunneling yet - it has only forwarded\n>> the request according to normal HTTP/1.1 rules. If the user agent instead\n>> sends a request like this:\n>\n>> OPTIONS * HTTP/1.1\n>> Host: w3.org\n>> Upgrade : HTTP+TLS/1.0\n>> Connection: Upgrade\n>\n>> then the request is intended for the proxy only (in the role as a server)\n>> and won't get forwarded.\n>\n>I'm not sure that I agree with your interpretation of this latter\n>formulation - I think that they are both the same, but I'm afraid that\n>I do agree with your conclusion that the Upgrade header should not be\n>forwarded, which causes some problems with what we suggest as an\n>end-to-end solution.  It would require, in effect, that the user agent\n>understand the entire proxy chain and do a series of OPTIONS/Upgrade\n>requests to create secure connections through each individually (not\n>an optimal solution, IMHO).\n>\n>I wonder what existing proxies really do with Upgrade; in particular:\n> - Do they in fact forward it (incorrectly?) in a response if it is\n>   _not_ (incorrectly) listed in a Connection header?\n> - Whether or not they react to it in a 101 response (our draft would have\n>   them switch to tunnel mode - what do they do today)?\n>\n>> I believe the intended semantics (I call it \"repeated hop-by-hop\") can be\n>> modeled by the HTTP extension framework like this (and is sort of the idea\n>> of the CONNECT draft [1]):\n>\n>> M-OPTIONS http://w3.org HTTP/1.1\n>> Host: w3.org\n>> Man: \"http://www.iana.org/http/ext/tls\"\n>\n>> This request will be tunneled through the proxy because it doesn't know\n>the\n>> extension (or if it does then it will know what it must tunnel), or it\n>will\n>> fail and refuse to cooperate (which also is the case for the OPTIONS\n>> method). As the extension is not hop-by-hop, it will be forwarded all the\n>> way to the ultimate recipient.\n>\n>That is still a change to an existing proxy, though - because it\n>requires that the proxy conform to the extensions draft.  I agree that\n>would be a good thing, but I don't think that it changes our situation\n>at all now.\n\nNope, a proxy would regardless of whether it knows HTTP extension framework\nor not dive into tunnel mode or return a 501 (Not Implemented). Similarly,\nan optional extension would be passed through to the end:\n\nOPTIONS http://w3.org HTTP/1.1\nHost: w3.org\nOpt: \"http://www.iana.org/http/ext/tls\"\n\nIt would work even with proxies that don't support the extension framework.\n\n>> The response could of course still be a 101 Switching Protocols which\n>would\n>> then be propagated all the way back to the user agent and all\n>> intermediaries will already be in tunnel mode.\n>\n>But will they be expecting to stay there for all subsequent requests,\n>or will they just switch to tunnel mode for the current request and\n>then expect to be 1.1 again afterwards?\n\nOnce a proxy agrees to tunnel, it does not take part of the HTTP\ncommunication and stays in tunnel mode until both transport connections\nhave been closed, see 1.2:\n\ntunnel\n\nAn intermediary program which is acting as a blind relay between\ntwo connections. Once active, a tunnel is not considered a party\nto the HTTP communication, though the tunnel may have been initiated\nby an HTTP request. The tunnel ceases to exist when both ends of the\nrelayed connections are closed.\n\nThere is actually a bug here - the 101 (Switching Protocol) status code is\npassed though proxies (and-to-end) while the Upgrade header field it is\nresponding to is hop-by-hop. This means that a client behind a proxy will\nget the 101 response even though it hasn't asked for an upgrade but the\nproxy did.\n\nIt should be mentioned that 101 (Switching Protocols) shouldn't be\nforwarded by proxies if not tunnelling. In 5.1, you mention that a proxy\nreceiving a 101 should tunnel but this does not work if the proxy initiated\nthe Upgrade header field by itself.\n\nThe HTTP extension framework solves this problem by having the Ext: and\nC-Ext: header fields to indicate who accepted an extension.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-10974721"}, {"subject": "Re: Comments on HTTP TLS Upgrade draf", "content": ">>We considered this alternative at some length, and at one point had\n>>written the draft that way.  We liked the idea of the Upgrade response\n>>header being able to specify a series of protocol tokens so as to\n>>specify a stack to be used after the change.  We were not sure that\n>>this would be understood, however.  RFC 2616 is vague on just what\n>>multiple tokens could mean in an Upgrade response.  We elected for the\n>>simpler set of expectations.\n>\n>Unfortunately this leads easily to an explosion of tokens that have to be\n>registered. HTTP doesn't have to say anything about the relationship\n>between the various tokens in the upgrade header field. It is sufficient\n>for the protocols (or sub protocols) independently to define how they can\n>interact with other protocols. For example, it can be stated in the\n>registration of the SSL upgrade token that it can work with any version of\n>HTTP.\n\nI agree with Henrik.  And I don't see anything vague about the definition\nof Upgrade in RFC 2616 -- the answer to this question is in the very first\nsentence.\n\nAs for using the extension stuff, Upgrade is intended to be a simple\nswitching mechanism for the immediate connection, in contrast to the\nvarious general extension mechanisms.  The two may or may not be used\nin tandem, or separately, but must not be co-dependent in their definition.\nUpgrade uses only the protocol tokens --- all further negotiation is\npostponed until after the protocol is switched.  Anything more complex\nthan that can and should be accomplished via the extension mechanism,\nthough I have yet to see any real need for further complexity which\nwasn't better accomplished by an extra round-trip -- complex things\nshould not expect to be as efficient as simple things.\n\n>>> The response could of course still be a 101 Switching Protocols which\n>>would\n>>> then be propagated all the way back to the user agent and all\n>>> intermediaries will already be in tunnel mode.\n>>\n>>But will they be expecting to stay there for all subsequent requests,\n>>or will they just switch to tunnel mode for the current request and\n>>then expect to be 1.1 again afterwards?\n>\n>Once a proxy agrees to tunnel, it does not take part of the HTTP\n>communication and stays in tunnel mode until both transport connections\n>have been closed, see 1.2:\n>\n>tunnel\n>\n>An intermediary program which is acting as a blind relay between\n>two connections. Once active, a tunnel is not considered a party\n>to the HTTP communication, though the tunnel may have been initiated\n>\n>by an HTTP request. The tunnel ceases to exist when both ends of the\n>relayed connections are closed.\n>\n>There is actually a bug here - the 101 (Switching Protocol) status code is\n>passed though proxies (and-to-end) while the Upgrade header field it is\n>responding to is hop-by-hop. This means that a client behind a proxy will\n>get the 101 response even though it hasn't asked for an upgrade but the\n>proxy did.\n\n101 is not passed through proxies.  Section 10.1 excludes 1xx responses\nthat were requested by the proxy, as would be the case here.\n\n>It should be mentioned that 101 (Switching Protocols) shouldn't be\n>forwarded by proxies if not tunnelling. In 5.1, you mention that a proxy\n>receiving a 101 should tunnel but this does not work if the proxy initiated\n>the Upgrade header field by itself.\n\nActually, a tunnel is never a proxy, even if it was a proxy at some time\nin the past.  The spec needs to be consistent in defining requirements\nonly in terms of the role of the application at the time of its\ncommunication for that request/response.  Otherwise we would have to\nplace five exceptions after every requirement.\n\n....Roy\n\n\n\n", "id": "lists-012-10993687"}, {"subject": "Tokens for requesting/indicating upgrad", "content": "[I've split my responses into two - one to address the upgrade tokens\nquestion and another to respond to proxy issues -SDL]\n\n> From: Roy T. Fielding\n> Subject: Re: Comments on HTTP TLS Upgrade draft\n\nHenrik says:\n\n> >Unfortunately this leads easily to an explosion of tokens that have to be\n> >registered. HTTP doesn't have to say anything about the relationship\n> >between the various tokens in the upgrade header field. It is sufficient\n> >for the protocols (or sub protocols) independently to define how they can\n> >interact with other protocols.\n\nWhile I prefer having the Upgrade response indicate an ordered list\nspecifying the stack, I don't agree that HTTP doesn't have to say anything.\nIf I specify:\n\n  Upgrade: CompressLayer/1.0, MuxLayer/1.1, HTTP/1.1\n\nit certainly isn't the same as:\n\n  Upgrade: MuxLayer/1.1, CompressLayer/1.0, HTTP/1.1\n\nand the client needs to know which I mean.  This is a difference between the\nusage (as I understand it) of the Upgrade header in a request and a\nresponse - the request doesn't specify a stack, it specifies a set of\nchoices (without indicating which combinations are possible - a problem, I\nthink), but in a response it specifies an ordered stack.\n\n> I agree with Henrik.  And I don't see anything vague about the definition\n> of Upgrade in RFC 2616 -- the answer to this question is in the very first\n> sentence.\n\nI actually prefer the multiple-token solution, myself.  One point I thought\nwould cause confusion was the sentence (from 2616#14.42):\n\n   The Upgrade header field only applies to switching application-layer\n   protocols upon the existing transport-layer connection.\n\nWe are actually using it to insert a new layer into the stack, not change\nthe application layer protocol (indeed, in the IPP case we are treating both\nTLS and HTTP as session layer protocols, but...).\n\n> As for using the extension stuff, Upgrade is intended to be a simple\n> switching mechanism for the immediate connection, in contrast to the\n> various general extension mechanisms.  The two may or may not be used\n> in tandem, or separately, but must not be co-dependent in their\n> definition.\n> Upgrade uses only the protocol tokens --- all further negotiation is\n> postponed until after the protocol is switched.  Anything more complex\n> than that can and should be accomplished via the extension mechanism,\n> though I have yet to see any real need for further complexity which\n> wasn't better accomplished by an extra round-trip -- complex things\n> should not expect to be as efficient as simple things.\n\nI agree, which is why I didn't want to include even hints in the upgrade\nresponse about TLS/SSL options.\n\nI'm perfectly happy to change the text back to a multiple-token rather than\ncombined-token [Rohit?].\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-11005701"}, {"subject": "Re: Comments on HTTP TLS Upgrade draf", "content": "At 00:31 02/08/1999 -0700, Roy T. Fielding wrote:\n\n>As for using the extension stuff, Upgrade is intended to be a simple\n>switching mechanism for the immediate connection, in contrast to the\n>various general extension mechanisms.  The two may or may not be used\n>in tandem, or separately, but must not be co-dependent in their definition.\n>Upgrade uses only the protocol tokens --- all further negotiation is\n>postponed until after the protocol is switched.  Anything more complex\n>than that can and should be accomplished via the extension mechanism,\n>though I have yet to see any real need for further complexity which\n>wasn't better accomplished by an extra round-trip -- complex things\n>should not expect to be as efficient as simple things.\n\nI think the experience from the current use of User-Agent shows that people\nin practice tends to overload simple tokens with as much information as\npossible. Although it is slightly different from the Upgrade header field\ntokens, the exchange of metadata about the communication should be light\nweight and not necessarily cost an extra RTT. Solving this using first\nclass objects is much to be preferred - hence the push for HTTP extension\nframework. \n\n>>There is actually a bug here - the 101 (Switching Protocol) status code is\n>>passed though proxies (and-to-end) while the Upgrade header field it is\n>>responding to is hop-by-hop. This means that a client behind a proxy will\n>>get the 101 response even though it hasn't asked for an upgrade but the\n>>proxy did.\n>\n>101 is not passed through proxies.  Section 10.1 excludes 1xx responses\n>that were requested by the proxy, as would be the case here.\n\nDuh - I was looking at draft 07 where it wasn't mentioned. I am glad it is\nnow.\n\n>>It should be mentioned that 101 (Switching Protocols) shouldn't be\n>>forwarded by proxies if not tunnelling. In 5.1, you mention that a proxy\n>>receiving a 101 should tunnel but this does not work if the proxy initiated\n>>the Upgrade header field by itself.\n>\n>Actually, a tunnel is never a proxy, even if it was a proxy at some time\n>in the past.  The spec needs to be consistent in defining requirements\n>only in terms of the role of the application at the time of its\n>communication for that request/response.  Otherwise we would have to\n>place five exceptions after every requirement.\n\nI agree - it was the point in the draft about transition from proxy to\ntunnel that I was objecting against.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-11017107"}, {"subject": "Re: The last LAST CALL: HTTP/1.1, Authentication, Stat", "content": ">Just a comment from the sidelines: there have been significant changes\n>in the authentication draft, and yet a LAST CALL is issued only hours\n>after it is issued?\n\n\nThe proposed modifications have been extensively reviewed by the\nediting group, and the change within addresses the concerns raised\npreviously. A \"last call\" is appropriate to call for technical review for\na proprosal that has undergone substantial review by the editorial\ngroup.\n\n>Are any servers going to implement the new stuff by next week? Otherwise\n>I won't bother hurrying on my side.\n\nI've been told there are implementations already underway; your participation\n(yes, please hurry) will allow us to document independent implementations,\nand move forward quickly.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-1102106"}, {"subject": "procow : protocol compliance on the we", "content": "[abstract, observations, and url of a paper that might be of interest.\n apologies for those of you who get duplicate copies]\n\nabstract\n\nWith the recent (draft) standardization of the HTTP/1.1 protocol on\nthe Web, it is natural to ask what percentage of popular Web sites\nspeak HTTP/1.1 and how {\\it compliant} are these so-called HTTP/1.1\nservers.  We attempt to answer these questions through a series of\nexperiments based on the protocol standard. The tests are run on a\ncomprehensive list of popular Web sites to which a good fraction of\nthe Web traffic is directed. Our experiments were conducted on a\nglobal extensible testing infrastructure that we built to answer the\nabove questions.  The same infrastructure will be used to answer\nquestions like the percentage of the traffic flow that is end-to-end\nHTTP/1.1.  Our results show reasons for concern on the state of\nHTTP/1.1 protocol compliancy and the subset of features actually\navailable to end users.\n\nsome interesting observations:\n\nwe tested over 500 'popular' soi disant HTTP/1.1 sites and only about 60% \nwere unconditionally compliant on 3 basic tests (GET, HEAD, dealing with \nabsence of Host header). 7% failed all three tests. 70% of the sites handled\npersistent connections and nearly that many pipelining. Half of the sites\nhandled range requests. 20% of sites didn't support any of persistent \nconnections/pipelining/range. 30% of sites passed all 6 tests.\n\nif u want to look at the paper (jointly written with martin arlitt of hp-labs)\nplease see\n      http://www.research.att.com/~bala/papers/procow-1.ps.gz\n\ncheers,\nbala\n\nbalachander krishnamurthy\nwww.research.att.com/~bala/papers\n\n\n\n", "id": "lists-012-11027466"}, {"subject": "HTTP Errata Pag", "content": "I've begun collecting the errata for the two HTTP Draft Standard RFCs, and\nputting them on a single page you can get through:\n\n      http://purl.org/NET/http-errata\n\nPlease update the HTTP Working Group web pages to include this pointer.\n\nNew items for the page should be sent to the HTTP Working Group list page.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-11035819"}, {"subject": "more errata in RFC 261", "content": "Some of the editorial notes got left in the text version.  That's what\nall the \n\n   jg639 jg640 jg641 jg642 jg643 jg644 jg645 jg646 jg647\n\nnotes mean.\n\nAlso, this gem comes from Tony Finch:\n\n------- Forwarded Message\n\nMessage-ID: <14245.33299.84271.563615@chiark.greenend.org.uk>\nDate: Mon, 2 Aug 1999 12:33:39 +0100 (BST)\nFrom: Tony Finch <dot@dotat.at>\nSubject: RFC 2616\n\nIt looks like some characters are missing in section 4.4 paragraph 4:\n\n   4.If the message uses the media type \"multipart/byteranges\", and the\n     ransfer-length is not otherwise specified, then this self-\n     elimiting media type defines the transfer-length. This media type\n     UST NOT be used unless the sender knows that the recipient can arse\n     it; the presence in a request of a Range header with ultiple byte-\n     range specifiers from a 1.1 client implies that the lient can parse\n     multipart/byteranges responses.\n\nI particularly like \"...MUST NOT be used unless the recipient can arse\nit\".\n\nTony.\n\n------- End of Forwarded Message\n\n\n\n", "id": "lists-012-11044031"}, {"subject": "Re: Fragments in &quot;Location&quot; fiel", "content": ">Something's come up in the CGI RFC discussion, and I'm hoping\n>you can clarify.\n>\n>RFC 2396 contains the following bits:\n>\n>> absoluteURI   = scheme \":\" ( hier_part | opaque_part )\n>:\n>> URI-reference = [ absoluteURI | relativeURI ] [ \"#\" fragment ]\n>\n>but 2616 says:\n>\n>> Location       = \"Location\" \":\" absoluteURI\n>\n>Does this mean that \"Location\" values cannot include fragments?\n\nYes.  Oops.  draft-bos-http-redirect-00.txt seems to assume a fragment\nis allowed in redirects.  The HTTP syntax should be\n\n   Location       = \"Location\" \":\" absoluteURI [ \"#\" fragment ]\n\nThe CGI syntax also allows relative references, since those are interpreted\ninternally by the server (unlike HTTP).\n\n....Roy\n\n\n\n", "id": "lists-012-11051634"}, {"subject": "RE: Fragments in &quot;Location&quot; fiel", "content": "> The HTTP syntax should be\n> \n>    Location       = \"Location\" \":\" absoluteURI [ \"#\" fragment ]\n\nI suppose this belongs in the errata, although we might need to explain\nthe circumstances under which a fragment identifier is appropriate.\n\n\n\n", "id": "lists-012-11059808"}, {"subject": "RE: Fragments in &quot;Location&quot; fiel", "content": "> From: Larry Masinter\n>\n> > The HTTP syntax should be\n> >\n> >    Location       = \"Location\" \":\" absoluteURI [ \"#\" fragment ]\n>\n> I suppose this belongs in the errata, although we might need to explain\n> the circumstances under which a fragment identifier is appropriate.\n\nI'll update the errata page.\n\nWhen would it not be appropriate? I can't think of a case... but then it's\nMonday morning.\n\n\n\n", "id": "lists-012-11068585"}, {"subject": "RE: Fragments in &quot;Location&quot; fiel", "content": "> > > The HTTP syntax should be\n> > >\n> > >    Location       = \"Location\" \":\" absoluteURI [ \"#\" fragment ]\n> >\n> > I suppose this belongs in the errata, although we might need to explain\n> > the circumstances under which a fragment identifier is appropriate.\n> \n> I'll update the errata page.\n> \n> When would it not be appropriate? I can't think of a case... but then it's\n> Monday morning.\n\nWell, let's see. \n\n- I don't think it's appropriate to have a fragment\n  with a 201 Created response, e.g., to a POST.\n\n- I don't know what it would mean to return a fragment with a 300\nMultiple Choices, since the choice decision is intended to be made\non resource characteristics and not fragment characteristics (?).\n\n- I think we're mainly concerned with 301/302/303/307, but I don't think\nit makes sense to POST to a URI with a fragment.\n\n- I don't think it's appropriate to return a fragment with\n 305 Use Proxy.\n\nIn addition, we should specify the behavior in the case where\nthere was a fragment with the original URI, e.g.,\n  http://host1.com/resource1#fragment1\nwhere /resource1 redirects to\n  http://host2.com/resource2#fragment2\nis 'fragment1' discarded? Do you find fragment2 and then\nfind fragment1 within it? We don't have fragment combination rules.\n\nLarry\n\n\n\n", "id": "lists-012-11076448"}, {"subject": "Re: Fragments in &quot;Location&quot; fiel", "content": "Larry Masinter wrote:\n\n> - I don't know what it would mean to return a fragment with a 300\n> Multiple Choices, since the choice decision is intended to be made\n> on resource characteristics and not fragment characteristics (?).\n\nMight it mean \"make your choice based on resource characteristics, then use the\nfragment on your next request\"?\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-11085665"}, {"subject": "Re: Fragments in &quot;Location&quot; fiel", "content": "Roy T. Fielding wrote:\n> \n> >but 2616 says:\n> >\n> >> Location       = \"Location\" \":\" absoluteURI\n> >\n> >Does this mean that \"Location\" values cannot include fragments?\n> \n> Yes.  Oops.  draft-bos-http-redirect-00.txt seems to assume a fragment\n> is allowed in redirects.  The HTTP syntax should be\n> \n>    Location       = \"Location\" \":\" absoluteURI [ \"#\" fragment ]\n\nDoes this mean that we (in the CGI work) can assume that fragments\nare legal in our overload definition of Location?\n-- \n#ken    P-)}\n\nKen Coar                    <http://Web.Golux.Com/coar/>\nApache Software Foundation  <http://www.apache.org/>\n\"Apache Server for Dummies\" <http://ASFD.MeepZor.Com/>\n\n\n\n", "id": "lists-012-11093574"}, {"subject": "HTTP/1.1 redirect handlin", "content": "I am trying to update libwww-perl to deal with automatic redirect as\nspecified by HTTP/1.1.  I am reading <draft-ietf-http-v11-spec-rev-03>.\n\nFirst some comments:\n\n  1)  Perhaps there should be a note explaining why there is no\n      code for 306 (if there is a reason)?\n\n  2)  The \"Note:\" for 305 says:\n\n      > Note: RFC 2068 was not clear that 305 was intended to redirect a\n      > single request, or to be generated by origin servers only. Not\n      > observing these limitations has significant security consequences.\n\n      I can not parse this sentence within this context without\n      substituting \"or\" with \"and\".  Is this just a typo?\n\nAnd then some question I hope you can clarify?\n\n  A common phrase for all descriptions are:\n\n     > If the new URI is a location, its URL SHOULD be given by the Location\n     > field in the response.\n\n  but I don't understand what the case of \"the new URI\" not being a\n  location would imply?\n\nIf I summarize the behaviour prescribed I get this table (I only care\nabout the client behaviour):\n\n      PREREQ          METHOD  URI         PROXY\n      -------------   ------  ----------  -----------\n301   GET|HEAD|ask    same    Location    -\n302   GET|HEAD|ask    same    Location    -\n303   -               GET     Location    -\n305   -               same    same        Location\n307   GET|HEAD|ask    same    Location    -\n\n\"ask\" means to ask the user if resending the request to the new\nlocation is ok.\n\n302 and 307 are described to be identical, but the notes seems to\nindicate that 302 would better be implemented as 303 to conform to\nwhat most browsers do.  Should I do that?\n\nIf the original method was HEAD, then I assume that the sensible thing\nto do for 303 is to use HEAD (not GET) in the redirect requests too.\nAgree?\n\nWhy is there no demand for \"confirmation by the user\" for 305 request?\n\nRegards,\nGisle Aas\n\n\n\n", "id": "lists-012-1110014"}, {"subject": "FW: generated hypertext (HTML) version of RFC2616 [yet again", "content": "Attached is that message from Dan Connolly that mysteriously failed to\nreach the list.\n\n\nattached mail follows:\nHenrik and anybody who's interested...\n\nI'm doing some writing about the HTTP 1.1, and I was frustrated\nthat I can't make hypertext links to specific sections, and\nin general, it's sort of a pain to navigate. So I scratched\nthe itch with a little perl script.\n\nI just updated\nhttp://www.w3.org/Protocols/\n$Id: Overview.html,v 1.169 1999/07/31 16:24:46 connolly Exp $\n\nto point to a hypertext version of the HTTP 1.1 draft standard:\n\nHypertext Transfer Protocol -- HTTP/1.1\nderived from RFC2616 by way of \nrfc2html.pl $Revision: 1.5 $ $Date: 1999/07/31 16:45:29 $ by\nDan Connolly\nhttp://www.w3.org/Protocols/rfc2616/rfc2616.html\n\n\nThe generated HTML is split into sections:\nhttp://www.w3.org/Protocols/rfc2616/rfc2616-sec1.html\nhttp://www.w3.org/Protocols/rfc2616/rfc2616-sec2.html\n...\n\nAbout the perl script:\n\n==============\nhttp://www.w3.org/Protocols/rfc2616/rfc2html.pl\n\n#!/usr/bin/perl\n# $Id: rfc2html.pl,v 1.6 1999/07/31 17:03:21 connolly Exp $\n#\n# Usage:\n#  perl rfc2html.pl rfc2616.txt >rfc2616.html\n#   also creates rfc2616-secN.html for each section N\n#\n# Features\n#  splits by section\n#  marks up paragraphs and definition lists\n#  marks up section headings with anchors\n#  marks up TOC with hypertext links\n#  marks up indendet sections as <pre>\n#  creates well-formed XML output\n#\n#\n# TODO\n#  update XML namespace declaration per XHTML spec\n#  add link types (e.g. rel='bibref')\n#  markup ul, ol in body text as such rather than as <pre>\n#  markup references section with links to other RFCs and docs\n#  generalize &convert() params: title, short title, basename,\nbibsection\n#     for other RFCs\n#\n# BY\n#  Dan Connolly <connolly@w3.org>\n#  http://www.w3.org/People/Connolly/\n#\n# LICENSE\n#\n# Copyright (c) 1999 World Wide Web Consortium (W3C,\nhttp://www.w3.org/),\n# (Massachusetts Institute of Technology, Institut National de\n# Recherche en Informatique et en Automatique, Keio University). All\n# Rights Reserved. \n#\n# Permission to use, copy, modify, and distribute this software\n# and its documentation for any purpose and without fee or\n# royalty is hereby granted, per the terms and conditions in\n#\n# W3C Intellectual Property Notice and Legal Disclaimers\n# http://www.w3.org/COPYRIGHT\n# 1999/07/28 13:54:29\n\n==============\n\n\n\n\n-- \nDan Connolly, W3C\nhttp://www.w3.org/People/Connolly/\n\n\n\n", "id": "lists-012-11102049"}, {"subject": "RE: Fragments in &quot;Location&quot; fiel", "content": "> Does this mean that we (in the CGI work) can assume that fragments\n> are legal in our overload definition of Location?\n\nIf it is that a CGI script can set the location header field with a value\ncontaining a fragment then yes.\n\nBtw, I have seen several servers sending relative Location header field\nvalues. This does make sense but does it break any existing applications we\nknow of?\n\nRegarding allowing fragments, I believe the same is the case for\nContent-Location:\n\n       Content-Location = \"Content-Location\" \":\"\n                         ( absoluteURI | relativeURI )\n\nshould be\n\n       Content-Location = \"Content-Location\" \":\"\n                         ( absoluteURI | relativeURI ) [ \"#\" fragment ]\n\nI don't like introducing a lot of \"sanity rules\" for when they are allowed\nand when not. I think that proxy redirection (305 Use Proxy) is the only one\nthat shouldn't be allowed.\n\nHenrik Frystyk Nielsen\nfrystyk@microsoft.com\n\n\n\n", "id": "lists-012-11112907"}, {"subject": "RE: IPP&gt;MOD Should IPP notification use http as transport", "content": "> (connection opened)\n> >>> GET /printer/status/job#2343 HTTP/1.1\n> >>>\n> <<< Content-type: application/ipp-status-messages\n> <<<\n> (pause)\n> <<< status: page 1 printed\n> (pause)\n> <<< status: page 2 printed\n> (pause)\n> <<< status: page 3 printed\n> (pause)\n> <<< status: out of paper\n> (pause)\n> <<< status: resumed\n> (pause)\n> <<< status: page 4 printed\n> <<< status: job complete\n> (connection closed)\n\nI don't like this example (of using GET for printer status)\nwithout any cache-control headers, and with the presumption\nthat the entity body is streamed with indefinite pauses. I know\npeople do this, but I'd be reluctant to recommend it, because\nit interacts badly with caches and will the recommended semantics\nof HTTP as a request/response protocol.\n\nIf one of the (pause)s is, say, a minute, some intermediary\nmight just break off the connection as stalled. And then there's\nno particular guarantees of completeness. A proxy might\nlegitimately want to buffer the entire response, sending\nback '100 continue' messages, and then batch the entire message\nbody.\n\nAlso, the URI has a problem:\n\n/printer/status/job#2343\n\nSince '#' is a reserved character for fragment delimiter, and\nHTTP URIs don't de-encode the data, this is not a good example.\n\n\n\n", "id": "lists-012-11122320"}, {"subject": "Re: Fragments in &quot;Location&quot; fiel", "content": ">Regarding allowing fragments, I believe the same is the case for\n>Content-Location:\n>\n>       Content-Location = \"Content-Location\" \":\"\n>                         ( absoluteURI | relativeURI )\n>\n>should be\n>\n>       Content-Location = \"Content-Location\" \":\"\n>                         ( absoluteURI | relativeURI ) [ \"#\" fragment ]\n\nNo, that would be incorrect.  The reference in Content-Location must be\nto a resource, not a fragment of a resource representation.\n\n....Roy\n\n\n\n", "id": "lists-012-11132209"}, {"subject": "Re: IPP&gt;MOD Should IPP notification use http as transport", "content": "> I don't like this example (of using GET for printer status)\n> without any cache-control headers, and with the presumption\n> that the entity body is streamed with indefinite pauses. \n\nunderstood.  I realized that such headers would be needed when\nI wrote the example, but am not so intimately familiar with\nHTTP that I could cite them off the top of my head; and I didn't\nwant to take the time to look them up on a Friday night.  sorry.\n\nI suspect that few present-day proxies will want to buffer the entire \nresponse before relaying it to the client, because buffreing the\nentire response is now known to a significant and very annoying affect \non response time.  \n\nat any rate, to the extent that we believe that this technique won't \nwork, then to me that is a good reason to abandon the idea of using \nHTTP for printer notifications.\n\n> Also, the URI has a problem:\n> \n> /printer/status/job#2343\n> \n> Since '#' is a reserved character for fragment delimiter, and\n> HTTP URIs don't de-encode the data, this is not a good example.\n\nright you are.\n\nKeith\n\n\n\n", "id": "lists-012-11140158"}, {"subject": "RE: Fragments in &quot;Location&quot; fiel", "content": "> >Regarding allowing fragments, I believe the same is the case for\n> >Content-Location:\n> >\n> >       Content-Location = \"Content-Location\" \":\"\n> >                         ( absoluteURI | relativeURI )\n> >\n> >should be\n> >\n> >       Content-Location = \"Content-Location\" \":\"\n> >                         ( absoluteURI | relativeURI ) [ \"#\" \n> fragment ]\n> \n> No, that would be incorrect.  The reference in \n> Content-Location must be\n> to a resource, not a fragment of a resource representation.\n\nThe fragment (or view) is a function of the media type of the representation\nchosen and hence does not have to be defined the same way across the set of\nrepresentations available for content negotiation of any particular\nresource. For example, a fragment in HTML pointing to page 3 does not have\nto be defined the same way for PS.\n\nIt is therefore functionally correct that Content-Location can contain a\nfragment. The problem is of course that HTTP servers currently do not see\nthe fragment under normal circumstances because clients do not send it but\nthis is the same as for the Location header field. In fact, I consider this\na bug in HTTP :)\n\nHenrik Frystyk Nielsen,\nmailto:frystyk@microsoft.com\n\n\n\n", "id": "lists-012-11149460"}, {"subject": "Re: Fragments in &quot;Location&quot; fiel", "content": ">The fragment (or view) is a function of the media type of the representation\n>chosen and hence does not have to be defined the same way across the set of\n>representations available for content negotiation of any particular\n>resource. For example, a fragment in HTML pointing to page 3 does not have\n>to be defined the same way for PS.\n\nA fragment is a fragment of a media type.  Content-Location refers to\nthe resource, not the representation of the resource as a media type.\nA resource is not a media type, and therefore a fragment is not appropriate\nfor this field.\n\nThis is different from Location because Location is redirecting the\nuser agent to a different view, not identifying the resource included\nin the message.\n\n....Roy\n\n\n\n", "id": "lists-012-11158822"}, {"subject": "RE: Fragments in &quot;Location&quot; fiel", "content": "> From: Rodent of Unusual Size <Ken.Coar@Golux.Com>\n\n> > Yes.  Oops.  draft-bos-http-redirect-00.txt seems to assume a fragment\n> > is allowed in redirects.  The HTTP syntax should be\n> >\n> >    Location       = \"Location\" \":\" absoluteURI [ \"#\" fragment ]\n>\n> Does this mean that we (in the CGI work) can assume that fragments\n> are legal in our overload definition of Location?\n\nI don't understand the question; I looked at:\nhttp://web.golux.com/coar/cgi/draft-coar-cgi-v11-03.html#7.2.1.2\n\nand it looks ok to me.  In what way do you consider it to be overloaded?\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-11166914"}, {"subject": "Add On... #423", "content": "1087\n*Earn $2000 - $5000 weekly-starting within 1-4 weeks\n*78% Profit Paid Daily\n*No Selling\n*No Risk Guarantee\n*Work from home, No overhead, or employees.\n*High Tech Training & Support\n*Not MLM, 100x more profitable\n*Multibillion Dollar Travel Industry\n \nThe most incredible part of our business\nis that ALL MY CLIENTS CALL ME!\n \nDO YOU QUALIFY FOR OUR MENTOR PROGRAM?\nACCEPTING ONLY 12 NEW ASSOCIATES\n \nThis is not a hobby!  Serious Inquires Only!!\n \n24 Hour Toll Free Message 1-888-248-6850\n\nIf your an entrepreneur or have always wanted to be your own BOSS,\nread on.  We supply state-of-the-art training and a support system\nthat allows you to work your business from your home with just a\nphone-without cold calling.  DO NOT CALL ME IF YOU ARE LOOKING FOR A \n\"GET RICH QUICK\" SCHEME or some extra cash or if you're lazy.  We are\nonly looking for  FOCUSED serious entrepreneurs. (Pt/FT) with the\nDESIRE to  improve their lifestyle immediately. \n\n////////////////////////////////////////////////////////////////\nPlease remove at mailto:tohhs33@yahoo.com?subject=remove\n///////////////////////////////////////////////////////////////\n\n\n\n", "id": "lists-012-11175877"}, {"subject": "I-D ACTION:draft-ietf-http-state-man-mec11.txt,.p", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-11.txt,.ps\nPages: 24\nDate: 16-Aug-99\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal [Netscape], but it can interoperate with\nHTTP/1.0 user agents that use Netscape's method.  (See the HISTORICAL\nsection.)\nThis document reflects implementation experience with RFC 2109 [RFC2109]\nand obsoletes it.\n\nA URL for this Internet-Draft is:\nhttp://www.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-11.txt\n\nInternet-Drafts are also available by anonymous FTP. Login with the username\n\"anonymous\" and a password of your e-mail address. After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-11.txt\".\n\nA list of Internet-Drafts directories can be found in\nhttp://www.ietf.org/shadow.html \nor ftp://ftp.ietf.org/ietf/1shadow-sites.txt\n\n\nInternet-Drafts can also be obtained by e-mail.\n\nSend a message to:\nmailserv@ietf.org.\nIn the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-11.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-11183456"}, {"subject": "another possible ietf-httpext work item: proxy cookie", "content": "I've had an (unsubmitted) I-D for \"proxy cookies\" in my back pocket for\nseveral weeks.  Once the dust settles for regular cookies, I would like\nto submit it.  (That's why I delayed -- to avoid intertwined discussions\non htt-state.)  ietf-http-ext seems like it might be a good home for\nproxy cookies as a work item.  Comments?\n\nDave Kristol\n\n\n\n", "id": "lists-012-1118357"}, {"subject": "draft-ietf-tls-http-upgrade02 availabl", "content": "A revised version of our draft on 'Upgrading to TLS Within HTTP/1.1'\n(draft-ietf-tls-http-upgrade-02.txt) has been sent to IANA, and so\nshould shortly be available in the usual repositories.\nIt is available now from:\n\n  http://www.agranat.com/fs/public/lawrence/tls-http-upgrade\n\nThis revision:\n\n- Clarifies that the https scheme is not modified by this proposal;\nit continues to mean 'SSL on TCP port 443'.\n\n- Re-introduces the CONNECT method, to establish an end-to-end-tunnel\nacross proxies, if necessary before UPGRADE'ing. This is a standards-\ntrack definition of the existing installed base. It is based on Ari\nLuotonen's original drafts, but used here to tunnel at port 80 instead\nof 443.\n\n- Uses multiple Upgrade tokens to represent a \"stack\" of protocols which\nwork together. In this case, \"Upgrade: TLS/1.0, HTTP/1.1\"\n\n--\nRohit Khare <rohit@4k-associates.com>,   Scott Lawrence\n<lawrence@agranat.com>\n\n\n\n", "id": "lists-012-11192324"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "> Re: IPP> Chunked POST\n>\n> Roy T. Fielding (fielding@kiwi.ics.uci.edu)\n> Thu, 17 Dec 1998 22:03:24 -0800\n>   ------------------------------------------------------------------------\n>\n> >In my opinion, Ken Coar is correct in saying that for a server to\n> >be *both* HTTP/1.1 compliant and CGI/1.1 compliant it MUST buffer\n> >chunked POST data and provide a Content-Length for the CGI script.\n>\n> Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n> encoding (and puking) would be non-compliance, but requiring a\n> content-length for a given resource is necessary for many reasons\n> (DoS and legacy system protection).\n\nThis is a meaningless distinction.  Consider this thought experiment:  we have\ntwo HTTP servers, A and B.\n\nServer A can and does parse the chunked encoding.  But it sends a 411 \"Length\nRequired\" response with a \"Connection: close\" header in response to any\nrequest that does not include a \"Content-Length\" header.  This is a compliant\nserver.\n\nServer B understands no transfer-coding except \"identity\".  It cannot receive\nor decode the \"chunked\" transfer-coding.  It sends a 411 \"Length Required\"\nresponse with a \"Connection: close\" header in response to any request that\ndoes not include a \"Content-Length\" header.   This is a non-compliant server.\n\nIf we look at these servers as black boxes, observing their behavior only\nthrough their external interfaces, they are virtually indistinguishable\n(unless we look at the product tokens or something).  So it's meanless to say\nthat all HTTP/1.1 applications that receive entities must understand (be able\nto receive and decode) the ?chunked? transfer-coding.\n\n\n> ...\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q4/0210.html\n\n\n\n", "id": "lists-012-11200674"}, {"subject": "RE: IPP&gt; Chunked POS", "content": ">\n>Actually they are quite distinguishable. A HTTP/1.1 client, who expects all\n>1.1 servers to support chunked transfer, would probably become hopelessly\n>confused by server B's behavior and thus be unable to communicate with\n>server B.\n\nBut the behavior, as seen from the client, is the same in either case!\n\n>\n>The key here is that the \"length required\" error would not tip off the 1.1\n>client that it shouldn't use chunked transfer because in so far as the 1.1\n>client is concerned using chunked transfer does provide length information\n>so therefore the error's requirement has been met.\n\nNo, the 411 specifically says \"The server refuses to accept the request without\na defined Content-Length. The client MAY repeat the request if it adds a valid\nContent-Length header field containing the length of the message-body in the\nrequest message.\"  The 411 means that the client must provide a Content-Length\nheader, not just the length information embedded in the chunked transfer-coding.\nOf course, if the client does provide a Content-Length header, then it must use\nthe \"identity\" transfer-coding.\n\n>\n>Thus failing to support chunked transfer in a 1.1 server prevents\n>interoperability, which is the very definition of non-compliance.\n>\n>              Yaron\n>\n>> -----Original Message-----\n>> From: Carl Kugler [mailto:kugler@us.ibm.com]\n>> Sent: Tue, August 17, 1999 3:54 PM\n>> To: http-wg@hplb.hpl.hp.com\n>> Subject: Re: IPP> Chunked POST\n>>\n>>\n>> > Re: IPP> Chunked POST\n>> >\n>> > Roy T. Fielding (fielding@kiwi.ics.uci.edu)\n>> > Thu, 17 Dec 1998 22:03:24 -0800\n>> >\n>> --------------------------------------------------------------\n>> ----------\n>> >\n>> > >In my opinion, Ken Coar is correct in saying that for a server to\n>> > >be *both* HTTP/1.1 compliant and CGI/1.1 compliant it MUST buffer\n>> > >chunked POST data and provide a Content-Length for the CGI script.\n>> >\n>> > Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n>> > encoding (and puking) would be non-compliance, but requiring a\n>> > content-length for a given resource is necessary for many reasons\n>> > (DoS and legacy system protection).\n>>\n>> This is a meaningless distinction.  Consider this thought\n>> experiment:  we have\n>> two HTTP servers, A and B.\n>>\n>> Server A can and does parse the chunked encoding.  But it\n>> sends a 411 \"Length\n>> Required\" response with a \"Connection: close\" header in\n>> response to any\n>> request that does not include a \"Content-Length\" header.\n>> This is a compliant\n>> server.\n>>\n>> Server B understands no transfer-coding except \"identity\".\n>> It cannot receive\n>> or decode the \"chunked\" transfer-coding.  It sends a 411\n>> \"Length Required\"\n>> response with a \"Connection: close\" header in response to any\n>> request that\n>> does not include a \"Content-Length\" header.   This is a\n>> non-compliant server.\n>>\n>> If we look at these servers as black boxes, observing their\n>> behavior only\n>> through their external interfaces, they are virtually\n>> indistinguishable\n>> (unless we look at the product tokens or something).  So it's\n>> meanless to say\n>> that all HTTP/1.1 applications that receive entities must\n>> understand (be able\n>> to receive and decode) the \"chunked\" transfer-coding.\n>>\n>>\n>> > ...\n>>\n>> http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q4/0210.html\n>>\n\n\n\n", "id": "lists-012-11209996"}, {"subject": "(no subject", "content": "university\n\n\n\n", "id": "lists-012-11221920"}, {"subject": "Re: IPP&gt; Chunked POS", "content": ">> Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n>> encoding (and puking) would be non-compliance, but requiring a\n>> content-length for a given resource is necessary for many reasons\n>> (DoS and legacy system protection).\n>\n>This is a meaningless distinction.  Consider this thought experiment:  we have\n>two HTTP servers, A and B.\n>\n>Server A can and does parse the chunked encoding.  But it sends a 411 \"Length\n>Required\" response with a \"Connection: close\" header in response to any\n>request that does not include a \"Content-Length\" header.  This is a compliant\n>server.\n>\n>Server B understands no transfer-coding except \"identity\".  It cannot receive\n>or decode the \"chunked\" transfer-coding.  It sends a 411 \"Length Required\"\n>response with a \"Connection: close\" header in response to any request that\n>does not include a \"Content-Length\" header.   This is a non-compliant server.\n>\n>If we look at these servers as black boxes, observing their behavior only\n>through their external interfaces, they are virtually indistinguishable\n>(unless we look at the product tokens or something).  So it's meanless to say\n>that all HTTP/1.1 applications that receive entities must understand (be able\n>to receive and decode) the =93chunked=94 transfer-coding.\n\nIf the purpose of the text was to delineate one lame example from another,\nthen I'd agree with you.  The reason it is there is to prevent an HTTP/1.1\napplication from mistakenly thinking the chunked encoding is *no* encoding\nand saving the chunk-sizes as part of the data.  As far as the protocol\nis concerned, recognizing Transfer-Encoding chunked, and responding with\n411 and connection close, is equivalent to parsing the chunked encoding.\n\nNobody is going to prevent you from building a server that responds with\n411 to every request without implementing chunked.  It would be a dumb\nthing to do, but the standard doesn't prevent people from doing dumb things\n(only things that won't interoperate with others via HTTP).\n\n....Roy\n\n\n\n", "id": "lists-012-11227797"}, {"subject": "RE: IPP&gt; Chunked POS", "content": "My point is that on the server side, it's a meanless MUST.  An ostensibly\nHTTP/1.1 compliant server can always (legally) respond with 411 whenever it\nencounters a chunked request.  Then there is no way to tell (short of taking\napart the server) whether or not the server is actually capable of receiving and\ndecoding the chunked transfer-coding.  This seems to be a widely exploited\nloophole.\n\n     -Carl\n\n\n\"Yaron Goland (Exchange)\" <yarong@Exchange.Microsoft.com> on 08/17/99 05:51:56\nPM\n\nTo:   Carl Kugler/Boulder/IBM@IBMUS, \"Yaron Goland (Exchange)\"\n      <yarong@Exchange.Microsoft.com>\ncc:   http-wg@hplb.hpl.hp.com\nSubject:  RE: IPP> Chunked POST\n\n\n\n\n\nAll requirements must be taken within the context of the over all standard.\nGiven that the standard requires Chunked at a MUST level it is fair to\ninterpret that requirement as over riding an optional response code.\n\n          Yaron\n\n> -----Original Message-----\n> From: kugler@us.ibm.com [mailto:kugler@us.ibm.com]\n> Sent: Tue, August 17, 1999 4:23 PM\n> To: Yaron Goland (Exchange)\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: RE: IPP> Chunked POST\n>\n>\n>\n>\n> >\n> >Actually they are quite distinguishable. A HTTP/1.1 client,\n> who expects all\n> >1.1 servers to support chunked transfer, would probably\n> become hopelessly\n> >confused by server B's behavior and thus be unable to\n> communicate with\n> >server B.\n>\n> But the behavior, as seen from the client, is the same in either case!\n>\n> >\n> >The key here is that the \"length required\" error would not\n> tip off the 1.1\n> >client that it shouldn't use chunked transfer because in so\n> far as the 1.1\n> >client is concerned using chunked transfer does provide\n> length information\n> >so therefore the error's requirement has been met.\n>\n> No, the 411 specifically says \"The server refuses to accept\n> the request without\n> a defined Content-Length. The client MAY repeat the request\n> if it adds a valid\n> Content-Length header field containing the length of the\n> message-body in the\n> request message.\"  The 411 means that the client must provide\n> a Content-Length\n> header, not just the length information embedded in the\n> chunked transfer-coding.\n> Of course, if the client does provide a Content-Length\n> header, then it must use\n> the \"identity\" transfer-coding.\n>\n> >\n> >Thus failing to support chunked transfer in a 1.1 server prevents\n> >interoperability, which is the very definition of non-compliance.\n> >\n> >              Yaron\n> >\n> >> -----Original Message-----\n> >> From: Carl Kugler [mailto:kugler@us.ibm.com]\n> >> Sent: Tue, August 17, 1999 3:54 PM\n> >> To: http-wg@hplb.hpl.hp.com\n> >> Subject: Re: IPP> Chunked POST\n> >>\n> >>\n> >> > Re: IPP> Chunked POST\n> >> >\n> >> > Roy T. Fielding (fielding@kiwi.ics.uci.edu)\n> >> > Thu, 17 Dec 1998 22:03:24 -0800\n> >> >\n> >> --------------------------------------------------------------\n> >> ----------\n> >> >\n> >> > >In my opinion, Ken Coar is correct in saying that for a\n> server to\n> >> > >be *both* HTTP/1.1 compliant and CGI/1.1 compliant it\n> MUST buffer\n> >> > >chunked POST data and provide a Content-Length for the\n> CGI script.\n> >> >\n> >> > Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n> >> > encoding (and puking) would be non-compliance, but requiring a\n> >> > content-length for a given resource is necessary for many reasons\n> >> > (DoS and legacy system protection).\n> >>\n> >> This is a meaningless distinction.  Consider this thought\n> >> experiment:  we have\n> >> two HTTP servers, A and B.\n> >>\n> >> Server A can and does parse the chunked encoding.  But it\n> >> sends a 411 \"Length\n> >> Required\" response with a \"Connection: close\" header in\n> >> response to any\n> >> request that does not include a \"Content-Length\" header.\n> >> This is a compliant\n> >> server.\n> >>\n> >> Server B understands no transfer-coding except \"identity\".\n> >> It cannot receive\n> >> or decode the \"chunked\" transfer-coding.  It sends a 411\n> >> \"Length Required\"\n> >> response with a \"Connection: close\" header in response to any\n> >> request that\n> >> does not include a \"Content-Length\" header.   This is a\n> >> non-compliant server.\n> >>\n> >> If we look at these servers as black boxes, observing their\n> >> behavior only\n> >> through their external interfaces, they are virtually\n> >> indistinguishable\n> >> (unless we look at the product tokens or something).  So it's\n> >> meanless to say\n> >> that all HTTP/1.1 applications that receive entities must\n> >> understand (be able\n> >> to receive and decode) the \"chunked\" transfer-coding.\n> >>\n> >>\n> >> > ...\n> >>\n> >> http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q4/0210.html\n> >>\n>\n>\n\n\n\n", "id": "lists-012-11237069"}, {"subject": "Re: IPP&gt; Chunked POS", "content": ">\n>>> Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n>>> encoding (and puking) would be non-compliance, but requiring a\n>>> content-length for a given resource is necessary for many reasons\n>>> (DoS and legacy system protection).\n>>\n>>This is a meaningless distinction.  Consider this thought experiment:  we have\n>>two HTTP servers, A and B.\n>>\n>>Server A can and does parse the chunked encoding.  But it sends a 411 \"Length\n>>Required\" response with a \"Connection: close\" header in response to any\n>>request that does not include a \"Content-Length\" header.  This is a compliant\n>>server.\n>>\n>>Server B understands no transfer-coding except \"identity\".  It cannot receive\n>>or decode the \"chunked\" transfer-coding.  It sends a 411 \"Length Required\"\n>>response with a \"Connection: close\" header in response to any request that\n>>does not include a \"Content-Length\" header.   This is a non-compliant server.\n>>\n>>If we look at these servers as black boxes, observing their behavior only\n>>through their external interfaces, they are virtually indistinguishable\n>>(unless we look at the product tokens or something).  So it's meanless to say\n>>that all HTTP/1.1 applications that receive entities must understand (be able\n>>to receive and decode) the =93chunked=94 transfer-coding.\n>\n>If the purpose of the text was to delineate one lame example from another,\n>then I'd agree with you.  The reason it is there is to prevent an HTTP/1.1\n>application from mistakenly thinking the chunked encoding is *no* encoding\n>and saving the chunk-sizes as part of the data.  As far as the protocol\n>is concerned, recognizing Transfer-Encoding chunked, and responding with\n>411 and connection close, is equivalent to parsing the chunked encoding.\n>\n>Nobody is going to prevent you from building a server that responds with\n>411 to every request without implementing chunked.  It would be a dumb\n>thing to do, but the standard doesn't prevent people from doing dumb things\n>(only things that won't interoperate with others via HTTP).\n>\nThat's the clarification I was looking for.  [Not that it's good news.]\n\nSo, for interoperability, a client always needs to be prepared to fall back to\nContent-Length with \"identity\" transfer-coding, and it's just  plain out of luck\n(or at least forced to buffer) if it cannot determine the message length in\nadvance.  Or should it fall back to HTTP/0.9 behavior and signal the end of the\nmessage by closing the connection?\n\n     -Carl\n\n>....Roy\n>\n\n\n\n", "id": "lists-012-11252027"}, {"subject": "comments on draft-ietf-http-authentication01.tx", "content": "Here are some comments on draft-ietf-http-authentication-01.\n\nDave Kristol\n======================\nSubstantive stuff:\n\nGeneral\n    (Formatting)  On a number of pages (e.g., 5,7,9), the text runs\n    into the footer.\n\n    The spec. is incomplete in its description of whether auth-params\n    are case-sensitive.  For example, \"realm\" and \"stale\" are mentioned\n    explicitly.  I think it would be simpler to say the\n    attribute/parameter/directive (see below) names are\n    case-insensitive, but that the values may or may not be\n    case-sensitive.\n\n    What should a client do if it receives unrecognized attributes?\n    What should a server do if it receives unrecognized attributes?\n\n1.2 Access Authentication Framework\n    credentials = basic-credentials | auth-scheme #auth-param\n\n    I think this should now read\n    credentials = basic-credentials | digest-credentials\n| auth-scheme #auth-param\n\n    The term \"protection space\" gets used without a definition (here),\n    but the spec. describes how a client can reuse credentials for such\n    a protection space.  I think we should say that the description of\n    any auth-scheme must describe the rules for deciding when two\n    objects are in the same protection space.  In particular, a client\n    must be able to tell, so it knows whether or not to send credentials\n    unprompted.\n\nSect. 3.2.1, The WWW-Authenticate Response Header\n    [domain attribute]\n    If this keyword is omitted or empty, the client should assume that\n    the domain consists of all URIs on the responding server.\n\nThis behavior is different from Basic.  If we want Digest to be\na more or less drop-in replacement, shouldn't the default\nbehavior mimic Basic?\n\nFurthermore, this paragraph constitutes the equivalent of a\ndescription of the Digest protection space, but it never calls\nit such.  It would be helpful to be more explicit.  (There's\nadditional related information in section 3.3.)\n\nSect. 3.2.2, The Authorization Request Header\n    [cnonce attribute]\n    RFC 2069-compliant implementations might break upon receiving this\n    new, previously unknown attribute.\n\n    [nonce-count]\n    The grammar for nc-value says\nnc-value         = 8LHEX\n    but the examples all show four-character nc= values, as in this\n    section.  The examples ought to be compliant. :-)\n\n    Also, why have such a restrictive syntax?  Why not 1*LHEX?\n\n    [description of calculations]\n    If the \"qop\" value is \"auth\":\nShouldn't this read\n    If the \"qop\" value is \"auth\" or \"auth-int\":\n\nSect. 3.2.3, The Authentication-Info Header\n    What should a client do if the rspauth=response-digest information\n    is wrong?\n       \n    Isn't there the risk that an intervening proxy could change the\n    status code?\n... Authorization header for the request, A2 is\n   A2       = Status-Code \":\" digest-uri-value\nand if \"qop=auth-int\", then A2 is\n   A2       = Status-Code \":\" digest-uri-value \":\" H(entity-body)\n\n    \n\nTypographical nits, etc.:\n\nAbstract\n    \"This document also provides ...\"\n       ==== -> delete\n\n    \"... other new elements have been added - for compatibility, ...\"\n    change to\n    \"... other new elements have been added.  For compatibility, ...\"\n\nSect. 1.1, Reliance on...\n    \"It uses using the extended...\"\n         ===== -> delete\n\nSect. 2, Basic Authentication Scheme\n    The \"basic\" authentication ...\n        ======= -> Basic\n\n    There are no optional [Basic] authentication parameters.\n      ======= -> add\n\n       credentials = basic-credentials\n      ^-- delete extra space\n      challenge   = \"Basic\" realm\n\n    A proxy may respond with the same challenge using the\n         ======== -> change to: an analogous\n    Proxy-Authenticate header field.\n\nSect. 3.1.1 Purpose\n    This document provides the specification for such a scheme, which\nchange to: \"a different scheme that\" <-   ====================\n    does not send the password in cleartext. It is referred to as\n    \"Digest Access Authentication\".\n\nSect. 3.2.1, The WWW-Authenticate Response Header\n     digest-challenge  = 1#( realm | [ domain ] | nonce |\n                         [ opaque ] |[ stale ] | [ algorithm ] |\n                      [ qop-options ] )\n      === -> indent correctly\n\n    The meanings of the values of the parameters used above are as follows:\n\nThese thingies get referred to variously (and inconsistently) as\n\"parameters\", \"directives\", and \"attributes\".  It would be nice\nif one term were chosen and used consistently.\n  \n    [algorithm attribute]\n    ... for the difference in usage, see the description .\n    ^-- delete\nSect. 3.2.2, The Authorization Request Header\n    credentials      = \"Digest\" digest-response\nshould be\n    digest-credentials      = \"Digest\" digest-response\n\n\n    see below for the defintions for A1 and A2.\n    ^-- See      ========== -> definitions\n\n    This stuff should move after the description of A1:\nwhere\n   passwd   = < user's password >\n\n    This example needs to quote realm-value:\nusername=\"Mufasa\", realm=myhost@testrealm.com\n    should be\nusername=\"Mufasa\", realm=\"myhost@testrealm.com\"\n\n    The description of white space in the digest is repetitive and\n    excessive (IMO).  (And, I suspect, people will *still* get it\n    wrong, even with the belabored point.)\n\nSect. 3.2.3, The Authentication-Info Header\n    [message-qop attribute]\n    qop\n    === -> change to: message-qop\n       Indicates the \"quality of protection\" options applied to the\n      ^ -- delete\n      response by the server.  The value \"auth\" indicates authentication;\n\n\n    header, except that if \"qop=auth\" or is not specified in the\n    ^-- insert: qop\n    Authorization header for the request, A2 is\n\nSect. 3.3, Digest Operation\n    A client may remember the username, password and nonce values, so\n    that future requests within the specified <domain> may include\n        ==================\nchange to: domain specified by the domain attribute\n    the Authorization header preemptively.\n\n    As with the basic scheme, proxies must be completely transparent\n    ===== -> Basic\n\nSect. 3.6, Proxy-Authentication and Proxy-Authorization\n\n    ... sections 10.30 and 10.31 of the HTTP/1.1 specification [2] and\n     =====   =====\n    should be: 14.33   14.34\n\n    authentication, the proxy/server must issue the \"HTTP/1.1 401\n    Unauthorized \" response with a \"Proxy-Authenticate\" header.\n    ^-- delete\n\nSect. 3.5, Example\n    ... same as that for the WWW-Authenticate header as defined above in\n    section 2.1.\n        === -> should be: 3.2.1\n\n    The client/proxy must then re-issue the request with a Proxy-\n    Authorization header, with attributes as specified for the Authorization\n    header in section TBD above.\n          === -> should be: 3.2.2\n\nSect. 4.1, Authentication of Clients using Basic Authentication\n    The first three paragraphs are a bit repetitions.  Here's a\n    revision:\n\n    Basic authentication is not a secure method of user\n    authentication.  It essentially transmits the user's password as\n    cleartext across the physical network.  Therefore it SHOULD NOT be\n    used (without enhancements) to protect sensitive or valuable\n    information.  Digest authentication fixes this flaw.\n\n    Basic authentication does not protect the entity, which is\n    transmitted as cleartext across the physical network.  Digest\n    authentication can reduce this vulnerability.\n\n    HTTP does not preclude additional authentication schemes and\n    encryption mechanisms from being employed to increase security, nor\n    does it preclude the addition of enhancements (such as schemes to\n    use one-time passwords) to the authentication schemes described\n    here.\n\nSect. 4.2, Authentication of Clients using Digest Authentication\n    authentication is both useful and appropriate (any service in\n    change to:      ==== -> e.  (Any\n    present use that uses Basic should be switched to Digest as soon\n    as practical).\n    == -> .)\n\nSect. 4.5, Replay Attacks\n    use of  integrity protection most metadata in header fields is\n       ^-- delete\n\nSect. 4.6, Weakness Created by Multiple Authentication Schemes\n    agent should choose to use  the first challenge it understands and\n           ^-- delete\n    only as good as that of the of the weakest of the authentication\n    ====== -> delete\n\nSect. 4.8, Man in the Middle\n    The countermeasure against this attack is to for clients to be\n          === -> delete\n\nSect. 4.10, Precomputed dictionary attacks\n    The countermeasure against this attack is to for clients to be\n          === -> delete\n\nSect. 4.11, Batch brute force attacks\n    The countermeasure against this attack is to for clients to be\n          === -> delete\n\nSect. 4.14, Summary\n    nonces or digests to eliminate the possibility of replay. Others may\n          add: be ---^\nSect. 7, Authors' Addresses\n\n    Scott D. Lawrence\n    Agranat Systems, Inc.\n    1345 Main St.\n    Waltham, MA  02154, USA\n\n    EMail: stewart@OpenMarket.com\n   ====================== -> lawrence@agranat.com\n\n\n\n", "id": "lists-012-1125399"}, {"subject": "RE: IPP&gt; Chunked POS", "content": "Maybe we're just missing an appropriate usage of \"SHOULD\" in the HTTP spec.\n\nThis needs another editing pass, but roughly:\n\n\"An HTTP/1.1 server SHOULD not only receive and interpret data with a\nchunked encoding, it SHOULD treat chunked data as if it had received data\nwith Content-Length. Note, however, that some server implementations will\naccept data with Content-Length but will reply with \"411 Length Required\"\nto chunked data, at least for those URLs that are implemented via CGI 1.x[ref]\n because of CGI/1.x's requirement that the Content-length be supplied.\nEven in those cases, it would be preferable for implementations to buffer\nthe data in order to compute the length to pass on to the CGI program.\"\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n> -----Original Message-----\n> From: kugler@us.ibm.com [mailto:kugler@us.ibm.com]\n> Sent: Wednesday, August 18, 1999 8:00 AM\n> To: Roy T. Fielding\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: IPP> Chunked POST\n>\n>\n>\n>\n>\n> >\n> >>> Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n> >>> encoding (and puking) would be non-compliance, but requiring a\n> >>> content-length for a given resource is necessary for many reasons\n> >>> (DoS and legacy system protection).\n> >>\n> >>This is a meaningless distinction.  Consider this thought\n> experiment:  we have\n> >>two HTTP servers, A and B.\n> >>\n> >>Server A can and does parse the chunked encoding.  But it sends a\n> 411 \"Length\n> >>Required\" response with a \"Connection: close\" header in response to any\n> >>request that does not include a \"Content-Length\" header.  This is a\n> compliant\n> >>server.\n> >>\n> >>Server B understands no transfer-coding except \"identity\".  It\n> cannot receive\n> >>or decode the \"chunked\" transfer-coding.  It sends a 411 \"Length Required\"\n> >>response with a \"Connection: close\" header in response to any request that\n> >>does not include a \"Content-Length\" header.   This is a\n> non-compliant server.\n> >>\n> >>If we look at these servers as black boxes, observing their behavior only\n> >>through their external interfaces, they are virtually indistinguishable\n> >>(unless we look at the product tokens or something).  So it's\n> meanless to say\n> >>that all HTTP/1.1 applications that receive entities must\n> understand (be able\n> >>to receive and decode) the =93chunked=94 transfer-coding.\n> >\n> >If the purpose of the text was to delineate one lame example from another,\n> >then I'd agree with you.  The reason it is there is to prevent an HTTP/1.1\n> >application from mistakenly thinking the chunked encoding is *no* encoding\n> >and saving the chunk-sizes as part of the data.  As far as the protocol\n> >is concerned, recognizing Transfer-Encoding chunked, and responding with\n> >411 and connection close, is equivalent to parsing the chunked encoding.\n> >\n> >Nobody is going to prevent you from building a server that responds with\n> >411 to every request without implementing chunked.  It would be a dumb\n> >thing to do, but the standard doesn't prevent people from doing dumb things\n> >(only things that won't interoperate with others via HTTP).\n> >\n> That's the clarification I was looking for.  [Not that it's good news.]\n>\n> So, for interoperability, a client always needs to be prepared to fall back to\n> Content-Length with \"identity\" transfer-coding, and it's just  plain\n> out of luck\n> (or at least forced to buffer) if it cannot determine the message length in\n> advance.  Or should it fall back to HTTP/0.9 behavior and signal the\n> end of the\n> message by closing the connection?\n>\n>      -Carl\n>\n> >....Roy\n> >\n>\n>\n>\n\n\n\n", "id": "lists-012-11262131"}, {"subject": "Re: IPP&gt; Chunked POS", "content": ">To: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n>cc: http-wg@hplb.hpl.hp.com\n>Subject: Re: IPP> Chunked POST\n>\n>\n>\n>\n>\n>\n>\n>>\n>>>> Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n>>>> encoding (and puking) would be non-compliance, but requiring a\n>>>> content-length for a given resource is necessary for many reasons\n>>>> (DoS and legacy system protection).\n>>>\n>>>This is a meaningless distinction.  Consider this thought experiment:  we\nhave\n>>>two HTTP servers, A and B.\n>>>\n>>>Server A can and does parse the chunked encoding.  But it sends a 411 \"Length\n>>>Required\" response with a \"Connection: close\" header in response to any\n>>>request that does not include a \"Content-Length\" header.  This is a compliant\n>>>server.\n>>>\n>>>Server B understands no transfer-coding except \"identity\".  It cannot receive\n>>>or decode the \"chunked\" transfer-coding.  It sends a 411 \"Length Required\"\n>>>response with a \"Connection: close\" header in response to any request that\n>>>does not include a \"Content-Length\" header.   This is a non-compliant server.\n>>>\n>>>If we look at these servers as black boxes, observing their behavior only\n>>>through their external interfaces, they are virtually indistinguishable\n>>>(unless we look at the product tokens or something).  So it's meanless to say\n>>>that all HTTP/1.1 applications that receive entities must understand (be able\n>>>to receive and decode) the =93chunked=94 transfer-coding.\n>>\n>>If the purpose of the text was to delineate one lame example from another,\n>>then I'd agree with you.  The reason it is there is to prevent an HTTP/1.1\n>>application from mistakenly thinking the chunked encoding is *no* encoding\n>>and saving the chunk-sizes as part of the data.  As far as the protocol\n>>is concerned, recognizing Transfer-Encoding chunked, and responding with\n>>411 and connection close, is equivalent to parsing the chunked encoding.\n>>\n>>Nobody is going to prevent you from building a server that responds with\n>>411 to every request without implementing chunked.  It would be a dumb\n>>thing to do, but the standard doesn't prevent people from doing dumb things\n>>(only things that won't interoperate with others via HTTP).\n>>\n>That's the clarification I was looking for.  [Not that it's good news.]\n>\n>So, for interoperability, a client always needs to be prepared to fall back to\n>Content-Length with \"identity\" transfer-coding, and it's just  plain out of\nluck\n>(or at least forced to buffer) if it cannot determine the message length in\n>advance.  Or should it fall back to HTTP/0.9 behavior and signal the end of the\n>message by closing the connection?\n>\nOops, just realized after sending this that it won't do for the client to close\nthe connection, since it would never be able to get a response.  So the only\nreal interoperable option is Content-Length/identity.\n\n>     -Carl\n>\n>>....Roy\n>>\n>\n>\n\n\n\n", "id": "lists-012-11274587"}, {"subject": "RE: IPP&gt; Chunked POS", "content": "My point is that responding to Transfer-Encoding chunked by sending 411 and\nConnection close is HTTP/1.1 COMPLIANT behavior.  By which I mean that it does\nNOT violate HTTP/1.1.\n\nMy mileage does vary.  I have encountered product groups which refuse to take\nthe development and testing hit to make a server capable accepting chunked\nPOSTs, when it can legally reject them with 411.\n\n     -Carl\n\n\n\"Yaron Goland (Exchange)\" <yarong@Exchange.Microsoft.com> on 08/18/99 11:11:38\nAM\n\nTo:   Carl Kugler/Boulder/IBM@IBMUS, \"Roy T. Fielding\"\n      <fielding@kiwi.ics.uci.edu>\ncc:   http-wg@hplb.hpl.hp.com\nSubject:  RE: IPP> Chunked POST\n\n\n\n\n\nYou are free to implement whatever you want but I am certainly not going to\nask MS to take the development and testing hit to allow our clients to work\naround servers which choose to knowingly violate HTTP/1.1.\n\n     Your mileage may vary,\n\n               Yaron\n\n> -----Original Message-----\n> From: kugler@us.ibm.com [mailto:kugler@us.ibm.com]\n> Sent: Wednesday, August 18, 1999 8:00 AM\n> To: Roy T. Fielding\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: IPP> Chunked POST\n>\n>\n>\n>\n>\n> >\n> >>> Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n> >>> encoding (and puking) would be non-compliance, but requiring a\n> >>> content-length for a given resource is necessary for many reasons\n> >>> (DoS and legacy system protection).\n> >>\n> >>This is a meaningless distinction.  Consider this thought\n> experiment:  we have\n> >>two HTTP servers, A and B.\n> >>\n> >>Server A can and does parse the chunked encoding.  But it\n> sends a 411 \"Length\n> >>Required\" response with a \"Connection: close\" header in\n> response to any\n> >>request that does not include a \"Content-Length\" header.\n> This is a compliant\n> >>server.\n> >>\n> >>Server B understands no transfer-coding except \"identity\".\n> It cannot receive\n> >>or decode the \"chunked\" transfer-coding.  It sends a 411\n> \"Length Required\"\n> >>response with a \"Connection: close\" header in response to\n> any request that\n> >>does not include a \"Content-Length\" header.   This is a\n> non-compliant server.\n> >>\n> >>If we look at these servers as black boxes, observing their\n> behavior only\n> >>through their external interfaces, they are virtually\n> indistinguishable\n> >>(unless we look at the product tokens or something).  So\n> it's meanless to say\n> >>that all HTTP/1.1 applications that receive entities must\n> understand (be able\n> >>to receive and decode) the =93chunked=94 transfer-coding.\n> >\n> >If the purpose of the text was to delineate one lame example\n> from another,\n> >then I'd agree with you.  The reason it is there is to\n> prevent an HTTP/1.1\n> >application from mistakenly thinking the chunked encoding is\n> *no* encoding\n> >and saving the chunk-sizes as part of the data.  As far as\n> the protocol\n> >is concerned, recognizing Transfer-Encoding chunked, and\n> responding with\n> >411 and connection close, is equivalent to parsing the\n> chunked encoding.\n> >\n> >Nobody is going to prevent you from building a server that\n> responds with\n> >411 to every request without implementing chunked.  It would\n> be a dumb\n> >thing to do, but the standard doesn't prevent people from\n> doing dumb things\n> >(only things that won't interoperate with others via HTTP).\n> >\n> That's the clarification I was looking for.  [Not that it's\n> good news.]\n>\n> So, for interoperability, a client always needs to be\n> prepared to fall back to\n> Content-Length with \"identity\" transfer-coding, and it's just\n>  plain out of luck\n> (or at least forced to buffer) if it cannot determine the\n> message length in\n> advance.  Or should it fall back to HTTP/0.9 behavior and\n> signal the end of the\n> message by closing the connection?\n>\n>      -Carl\n>\n> >....Roy\n> >\n>\n\n\n\n", "id": "lists-012-11285520"}, {"subject": "RE: IPP&gt; Chunked POS", "content": "> My point is that responding to Transfer-Encoding chunked by sending 411 and\n> Connection close is HTTP/1.1 COMPLIANT behavior.  By which I mean that it does\n> NOT violate HTTP/1.1.\n\n...\n\n> My mileage does vary.  I have encountered product groups which refuse to take\n> the development and testing hit to make a server capable accepting chunked\n> POSTs, when it can legally reject them with 411.\n\nWe might make replying '411 and connection close' to a chunked encoding\n'conditionally compliant' rather than 'compliant' because otherwise,\nas has been pointed out, there are inteorperability problems.\n\nThis isn't an errata, but it belongs on the issues list.\n\nOn the other hand, a server can legally also reply '200 OK' to every POST,\nand just ignore the data. A server can legally reply '404 Not Found' to\nevery GET, too! So the fact that there are some stubborn implementors\ndoesn't mean we should change the standard.\n\nThere is still a 'call' for an HTTP implementation guide; it isn't in the\ncharter of the HTTP working group, but this kind of advice might belong\nthere.\n\n\n\n", "id": "lists-012-11298962"}, {"subject": "Re: IPP&gt; Chunked POS", "content": ">We might make replying '411 and connection close' to a chunked encoding\n>'conditionally compliant' rather than 'compliant' because otherwise,\n>as has been pointed out, there are inteorperability problems.\n\nThere is no interoperability problem here.  The server response is\nunderstandable by the client.  Any server has the right to deny\nservice rather than buffer an unknown amount of data.  No change is\nneeded to the spec.\n\n....Roy\n\n\n\n", "id": "lists-012-11308133"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "\"Roy T. Fielding\" wrote:\n> \n> >We might make replying '411 and connection close' to a chunked encoding\n> >'conditionally compliant' rather than 'compliant' because otherwise,\n> >as has been pointed out, there are inteorperability problems.\n> \n> There is no interoperability problem here.  The server response is\n> understandable by the client.  Any server has the right to deny\n> service rather than buffer an unknown amount of data.  No change is\n> needed to the spec.\n> \n> ....Roy\n\nQuite right. Also, the fact that an implementation or product is \ncompliant does not automatically mean that it is a good or \neven a fully featured implementation. \n\nKyle Dent\n<kdent@seaglass.com>\nhttp://www.seaglass.com\n\n\n\n", "id": "lists-012-11316478"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "with the availability of 413 (request too large), 411 was not made\nclear to me from the specification.. it's only from working with it\nfor the last year that I've come to some clarity on the topic, and who\nknows if that's shared with other folks.\n\nIn a previous episode Roy T. Fielding said...\n:: \n:: [..]  Any server has the right to deny\n:: service rather than buffer an unknown amount of data.  \n\n a server does not commit to an all or nothing buffering process.. it\ncan buffer and if the request seems to be getting too large, it may\nabort with 413...\n\nrequiring attempts at chunked processing never requires commitments to\nunknown amounts of material. Each chunk is labeled with a length and\nthe server is free to send back 413 if the length of the upcoming\nchunk pushes it over a threshold of that which it is willing to\nbuffer.\n\nHowever, I can see that under some circumstances like CGI a server\nmight be unwilling to continue to buffer a chunked request at some\nthreshold (and it must be buffered because CGI requires a\ncontent-length) but if it had come with a c-l it could be streamed to\nthe CGI, so 411 would seem to convey the feeling. \n\nHowever, making that threshold 0 is ridiculous. The vast majority if\nCGIs take very little input.. there are exceptions of course but\npresumably they can be designed in an environment that can either\nafford the buffering or done with something other than CGI 1.1 that\ndoesn't have the CL problem.\n\nthe 411 distinction of \"you must be capable of doing chunked, but you're\nnever actually required to do it\" is too fine of a line in my\nopinion.... but that's a retrospective, there really isn't anything\nthat can be done about it now even if we agreed the semantics for it\nneeded to be stronger. (and I suspect we don't agree.. que sera sera).\n\n-P\n\n\n\n", "id": "lists-012-11325182"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "Right, a client can interoperate with any server as long as it sticks to\nContent-Length.  But if it cannot determine the message length in advance it's\nprobably SOL.\n\n     -Carl\n\n\n\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> on 08/18/99 11:59:21 PM\n\nTo:   Larry Masinter <masinter@parc.xerox.com>\ncc:   Carl Kugler/Boulder/IBM@IBMUS, \"Yaron Goland (Exchange)\"\n      <yarong@exchange.microsoft.com>, http-wg@hplb.hpl.hp.com\nSubject:  Re: IPP> Chunked POST\n\n\n\n\n\n>We might make replying '411 and connection close' to a chunked encoding\n>'conditionally compliant' rather than 'compliant' because otherwise,\n>as has been pointed out, there are inteorperability problems.\n\nThere is no interoperability problem here.  The server response is\nunderstandable by the client.  Any server has the right to deny\nservice rather than buffer an unknown amount of data.  No change is\nneeded to the spec.\n\n....Roy\n\n\n\n", "id": "lists-012-11334606"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "kugle-@us.ibm.com wrote: \noriginal article:http://www.egroups.com/group/http-wg/?start=8712\n> >> Sending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\n> >> encoding (and puking) would be non-compliance, but requiring a\n> >> content-length for a given resource is necessary for many reasons\n> >> (DoS and legacy system protection).\n> >\n> >This is a meaningless distinction.  Consider this thought\nexperiment:  we have\n> >two HTTP servers, A and B.\n> >\n> >Server A can and does parse the chunked encoding.  But it sends a\n411 \"Length\n> >Required\" response with a \"Connection: close\" header in response to\nany\n> >request that does not include a \"Content-Length\" header.  This is a\ncompliant\n> >server.\n> >\n> >Server B understands no transfer-coding except \"identity\".  It\ncannot receive\n> >or decode the \"chunked\" transfer-coding.  It sends a 411 \"Length\nRequired\"\n> >response with a \"Connection: close\" header in response to any\nrequest that\n> >does not include a \"Content-Length\" header.   This is a\nnon-compliant server.\n> >\n> >If we look at these servers as black boxes, observing their behavior\nonly\n> >through their external interfaces, they are virtually\nindistinguishable\n> >(unless we look at the product tokens or something).  So it's\nmeanless to say\n> >that all HTTP/1.1 applications that receive entities must understand\n(be able\n> >to receive and decode) the =93chunked=94 transfer-coding.\n> \n> If the purpose of the text was to delineate one lame example from\nanother,\n> then I'd agree with you.  The reason it is there is to prevent an\nHTTP/1.1\n> application from mistakenly thinking the chunked encoding is *no*\nencoding\n> and saving the chunk-sizes as part of the data.  As far as the\nprotocol\n> is concerned, recognizing Transfer-Encoding chunked, and responding\nwith\n> 411 and connection close, is equivalent to parsing the chunked\nencoding.\n> \n> Nobody is going to prevent you from building a server that responds\nwith\n> 411 to every request without implementing chunked.  It would be a dumb\n> thing to do, but the standard doesn't prevent people from doing dumb\nthings\n> (only things that won't interoperate with others via HTTP).\n\nIsn't that what Apache does with chunked POSTs?\n\n> \n> ....Roy\n> \n\n\n\n", "id": "lists-012-11344259"}, {"subject": "Re: IPP&gt; Chunked POS", "content": ">> Nobody is going to prevent you from building a server that responds with\n>> 411 to every request without implementing chunked.  It would be a dumb\n>> thing to do, but the standard doesn't prevent people from doing dumb things\n>> (only things that won't interoperate with others via HTTP).\n>\n>Isn't that what Apache does with chunked POSTs?\n\nNo.  It is what mod_cgi requires.  Apache implements chunked in the core,\nwhere any module can make use of it if they so desire.\n\nAs I've said many times before, implementing a new protocol service\nusing CGI is a bad idea.  Making HTTP requirements based on the limitations\nof CGI is even worse.\n\n....Roy\n\n\n\n", "id": "lists-012-11353687"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "I don't want to implement a new protocol service using CGI.  But I'm worried\nabout things like proxies refusing to forward chunked requests.\n\nI haven't kept up with the Jakarta thing, but it will be interesting to see if\nthe Apache Java servlet implementation supports chunked POST.\n\n     -Carl\n\n\n\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> on 08/20/99 11:47:01 AM\n\nTo:   Carl Kugler/Boulder/IBM@IBMUS\ncc:   http-wg@cuckoo.hpl.hp.com\nSubject:  Re: IPP> Chunked POST\n\n\n\n\n\n>> Nobody is going to prevent you from building a server that responds with\n>> 411 to every request without implementing chunked.  It would be a dumb\n>> thing to do, but the standard doesn't prevent people from doing dumb things\n>> (only things that won't interoperate with others via HTTP).\n>\n>Isn't that what Apache does with chunked POSTs?\n\nNo.  It is what mod_cgi requires.  Apache implements chunked in the core,\nwhere any module can make use of it if they so desire.\n\nAs I've said many times before, implementing a new protocol service\nusing CGI is a bad idea.  Making HTTP requirements based on the limitations\nof CGI is even worse.\n\n....Roy\n\n\n\n", "id": "lists-012-11361341"}, {"subject": "I-D ACTION:draft-ietf-http-state-man-mec12.txt,.p", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-12.txt,.ps\nPages: 23\nDate: 30-Aug-99\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal [Netscape], but it can interoperate with\nHTTP/1.0 user agents that use Netscape's method.  (See the HISTORICAL\nsection.)\nThis document reflects implementation experience with RFC 2109 [RFC2109]\nand obsoletes it.\n\nA URL for this Internet-Draft is:\nhttp://www.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-12.txt\n\nInternet-Drafts are also available by anonymous FTP. Login with the username\n\"anonymous\" and a password of your e-mail address. After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-12.txt\".\n\nA list of Internet-Drafts directories can be found in\nhttp://www.ietf.org/shadow.html \nor ftp://ftp.ietf.org/ietf/1shadow-sites.txt\n\n\nInternet-Drafts can also be obtained by e-mail.\n\nSend a message to:\nmailserv@ietf.org.\nIn the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-12.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-11369586"}, {"subject": "Form for reporting on HTTP/1.1  implementation", "content": "In addition to a series of private requests to the listed implementers of HTTP/1.1,\nI would like to request that any others with partial or full HTTP/1.1 implementations\ntake the time to fill out the form\n        http://www.agranat.com:1998/\n\nas pointed to by\n\n       http://www.w3.org/Protocols/HTTP/Forum/Reports/\n\nFilling out the form may help with the review of the specification, as well.\n\nWe're in working group LAST CALL; soon after the close (Friday, March 27), a final\ndraft will be prepared (reflecting comments received) for sending on to the IESG.\n\nThanks,\n\nLarry\n\n\n\n", "id": "lists-012-1144399"}, {"subject": "Serverside roles in the HTT", "content": "As you may be aware, I have submitted an Internet-Draft,\ndraft-nottingham-http-roles-00.txt.\n\nI'd very much appreciate comments from the http-wg, as I see it as primarily\nin that domain (with some overlap to wrec and the cgi efforts).\n\nI've already gotten a few comments, and I should make some clarifications.\n\n* The draft is an applicability statement, not a protocol specification.\n* My goal is to clarify responsibility for protocol implementation between\nservers, publishers, and facilities like CGI, PHP, NSAPI, ISAPI, Cold\nFusion, etc.\n* Some of the measures it calls for will place some additional load on the\nserver; this is an intentional decision. I believe that server vendors have\nsacrificed protocol compliance for benchmark speed, which in the long run\ndoesn't do anyone any good.\n\nThis is probably a hard pill to swallow for server vendors; I understand\nthat some of the things it calls for will be difficult to implement. I'd\nlike to use the draft process as an opportunity to work them out.\n\nThe draft is available at:\nhttp://www.ietf.org/internet-drafts/draft-nottingham-http-roles-00.txt\nPlease have a look at the references as well, as they help motivate the\ndocument.\n\nAny comments or help would be greatly appreciated. I'm particularly\ninterested in help with defining WebDAV PROPPATCH extensions that will allow\nmanipulation of HTTP-related metadata (such as Cache-Control and Expires\nheaders, and content negotiation hints).\n\nCheers,\n\n--\nMark Nottingham, Melbourne Australia\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-11476251"}, {"subject": "Re: Serverside roles in the HTT", "content": "Mark,\n\nMark Nottingham wrote:\n\n> As you may be aware, I have submitted an Internet-Draft,\n> draft-nottingham-http-roles-00.txt.\n>\n> I'd very much appreciate comments from the http-wg, as I see it as primarily\n> in that domain (with some overlap to wrec and the cgi efforts).\n>\n> I've already gotten a few comments, and I should make some clarifications.\n>\n> * The draft is an applicability statement, not a protocol specification.\n> * My goal is to clarify responsibility for protocol implementation between\n> servers, publishers, and facilities like CGI, PHP, NSAPI, ISAPI, Cold\n> Fusion, etc.\n> * Some of the measures it calls for will place some additional load on the\n> server; this is an intentional decision. I believe that server vendors have\n> sacrificed protocol compliance for benchmark speed, which in the long run\n> doesn't do anyone any good.\n\nActually, in our case, it's less benchmark speed than backwards compatibility\nwith existing applications that has prevented  broader protocol compliance. You\nwill already see a lot more HTTP/1.1 features in NES 4.0, which should hit the\nmarket RSN. It will still not be HTTP/1.1 compliant by RFC2616, but we made it a\nmuch better-behaving server overall.\n\n> This is probably a hard pill to swallow for server vendors; I understand\n> that some of the things it calls for will be difficult to implement. I'd\n> like to use the draft process as an opportunity to work them out.\n\nOf the \"Content-generation\" mechanisms you cite above, some offer lower-level\naccess to the HTTP reply than others, and it's not necessarily feasible or\ndesirable to implement all your suggestions in all cases. For example, CGI is,\nif limited, a high-level interface, and it's easy for the daemon to intercept\nand process their responses. Then you have servlets (which you don't mention in\nyour paper), at a slightly lower-level. The servlet classes expose some HTTP\nfeatures so the complexity of intercepting and correcting content is greater.\nThen you have server plug-ins which expose many low-level server internal\nfeatures and are more difficult to intercept in the daemon, as there are so many\nserver helper functions that can be called in many orders. Plug-ins can also be\nmore than content-generators - they may handle tasks like redirection and\nauthentication, which are also more difficult to correct on the fly in the\ndaemon.\n\nFor those applications that are strictly generating content, I think your\nsuggestions make sense. We came largely to the same conclusions recently before\nI got a chance to see your draft, and we have even implemented some solutions to\nthese problems already; though you aren't likely to see the results until we\nhave a fully compliant HTTP/1.1 server.\n\nAs far as your draft, I think there should be fewer \"MUST\" requirements. For\ninstance, the partial content requirement won't necessarily fly with all type of\ncontent generators. According to RFC2616 section 14.5, a server is not required\nto serve ranges for all resources - it can send \"Accept-range: None\". That\ndoesn't preclude it from supporting ranges on other resources, such as static\nfiles. I like this flexibility.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-11485012"}, {"subject": "Re: Serverside roles in the HTT", "content": "Julien,\n\nThanks for taking the time to comment.\n\n\n> Of the \"Content-generation\" mechanisms you cite above, some offer\n> lower-level access to the HTTP reply than others, and it's not necessarily feasible or\n> desirable to implement all your suggestions in all cases. For example, CGI\n> is, if limited, a high-level interface, and it's easy for the daemon to\n> intercept and process their responses. Then you have servlets (which you don't\n> mention in your paper), at a slightly lower-level. The servlet classes expose some\n> HTTP features so the complexity of intercepting and correcting content is\n> greater.\n\nGood point; I'm fairly ignorant of the inner workings of servlets as well as their newer cousins like JSP. They very well may fall out of scope for this \nproposal. However, I think that would be a shame; even people with the knowledge to use these facilities rarely implement anything beyond the very \nbasics of the protocol.\n\nThere's nothing in the draft (although this isn't articulated well) that says mechanisms can't be established to override these server-level responsiblities. \n\nWhat I'm trying to do is give the people I called 'publishers' the assurance that, if they use a server and generator compliant with this draft, they won't \nneed to worry about details of protocol implementation -- not that they do (that's the root of the problem). \n\nWhether the server or the 'content generators' is the best place to do that is an interesting question; my feeling is that it's simpler and more reliable to do \nit in the server.\n\n\n> Then you have server plug-ins which expose many low-level server internal\n> features and are more difficult to intercept in the daemon, as there are so\n> many server helper functions that can be called in many orders. Plug-ins can\n> also be more than content-generators - they may handle tasks like redirection and\n> authentication, which are also more difficult to correct on the fly in the\n> daemon.\n> \n> For those applications that are strictly generating content, I think your\n> suggestions make sense. We came largely to the same conclusions recently\n\nIt does become a nightmare if it is applied to everything that can be tied into a server. It may be prudent to draw a line at APIs, or perhaps there needs to \nbe a further distinction in the draft - \n* content generators (CGI, PHP, ASP, etc) - have a 'shallow', high-level view into the server, server is required to handle all protocol features (as in current \ndraft)\n* APIs et al (Servlets, ISAPI, NSAPI, etc) - low-level 'deep' view, api user has library of protocol feature calls available\n\nWould this be more practical?\n\n\n> before I got a chance to see your draft, and we have even implemented some\n> solutions to\n> these problems already; though you aren't likely to see the results until\n> we have a fully compliant HTTP/1.1 server.\n\nCan't wait!\n\n \n> As far as your draft, I think there should be fewer \"MUST\" requirements.\n\nIt depends on how you place the draft. If you think of it as a peer document to 2616 (you must be compliant with this draft to be considered HTTP \ncompliant), you're right - it's far too strict. However, if you consider it as an additional, higher level of compliance, the MUSTS are important. \n\nI do think that some of the SHOULDs (especially to do with buffering and *perhaps* synthetic validation) might be better as MAYs.\n\n\n> For instance, the partial content requirement won't necessarily fly with all\n> type of content generators. According to RFC2616 section 14.5, a server is not\n> required to serve ranges for all resources - it can send \"Accept-range: None\". That\n> doesn't preclude it from supporting ranges on other resources, such as\n> static files. I like this flexibility.\n\nBut from a protocol/network/user perspective, does it make any sense that a feature is available for one resource but not another, based on the \nunderlying technology? If you have a PDF file that happens to be generated by a CGI script, it's a huge pain to get it to support ranges, validation and \nother facilities in the script itself. I've done it, and it was not pleasant. \n\nA lot of the feedback that I've gotten has been in this vein - that not every object is going to take advantage of all of these 'extra' services. But, if they \naren't available by default, chances are they'll never be thought of, much less handled.\n\nThe other way that I'm working on attacking this problem is creating a series of wrapper scripts / libraries that will take care of as much of this stuff as \npossible automatically. Unfortunately, it's very much an uphill battle (for instance, if I include a content-length in Netscape or Apache CGI, it will make the \nconnection persistent; no such luck with other servers), so this would only be a temporary measure. \n\n\nCheers,\n\n--\nMark Nottingham, Melbourne Australia\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-11495773"}, {"subject": "Re: Serverside roles in the HTT", "content": "> Servers MUST support a persistent connection if the content\n>   generator supplies a Content-Length header. If it is not available,\n>  the server SHOULD attempt to buffer the response in order to\n>   generate one, although this MAY be circumvented if:\n>   * the server does not have resources (i.e., memory) to do so, or\n>   * the object is very large, and overall latency becomes\n>   unacceptable, or\n>   * the time required to generate the object adds unacceptable latency\n...\n>  Servers MUST serve chunked encoding responses for all objects, if:\n>   * Content-Length is unavailable and impractical to generate\n>   and\n>   * the client advertises itself as HTTP/1.1 capable, or\n>   * the client includes 'chunked' in a TE header\n\nWhy wouldn't streaming the response with the \"chunked\" encoding be preferable to\nbuffering and generating a Content-Length?  I don't think the server should\nattempt to buffer the response unless it's talking to an HTTP/1.0 client.\n\n> 6.4 Transfer-Encoding\n\n>   Servers SHOULD reply to requests that allow transfer encoding of\n>  objects (i.e., TE header present) with appropriate encoding, in a\n>  fashion transparent to the content generator.\n\nThis paragraphs seems to suggest that transfer encoding is only allowed if the\nTE request-header is present.  However, if the TE field-value is empty or if no\nTE field is present, the transfer-coding \"chunked\" is allowed, and is always\nacceptable.\n\n>   There MUST be a method by which content generators can specify that\n>   content is not to be buffered; this MAY be performed by a pseudo-\n>   HTTP header that is consumed by the server.\n\nI think you're only addressing generated (response) content in this statement.\nI'd also like to have a method by which content generators can specify that the\nreceived (request) content is not to be buffered.   Sometimes the \"content\ngenerator\" can be doing something useful with the content of a large POST\nrequest while it's still being received (or even while it's still being\ngenerated by the client).  In some applications (e.g., IPP), POST content may be\nso large that you don't even want the server to attempt to buffer it.\n\n     -Carl Kugler\n\n\n\n", "id": "lists-012-11508040"}, {"subject": "Re: Serverside roles in the HTT", "content": "Hi,\n\nMark Nottingham wrote:\n\n> Good point; I'm fairly ignorant of the inner workings of servlets as well as their newer cousins like JSP. They very well may fall out of scope for this\n> proposal. However, I think that would be a shame; even people with the knowledge to use these facilities rarely implement anything beyond the very\n> basics of the protocol.\n>\n> There's nothing in the draft (although this isn't articulated well) that says mechanisms can't be established to override these server-level responsiblities.\n\nWhen you have MUST requirements it seems to somewhat preclude them.\n\n\n> What I'm trying to do is give the people I called 'publishers' the assurance that, if they use a server and generator compliant with this draft, they won't\n> need to worry about details of protocol implementation -- not that they do (that's the root of the problem).\n\n> Whether the server or the 'content generators' is the best place to do that is an interesting question; my feeling is that it's simpler and more reliable to do\n> it in the server.\n\nIt's true that most applications that generate content don't deal with those 1.1 features like range requests.\nIt makes sense to have a server mechanism for dealing with them transparently.\n\nBut it should still be possible to defer the responsability to handle these features to the application, if only to maintain compatibility with current\napplications. Sometimes it's actually the intended effect - you may want the application to actually respond differently to byteranges or other types of ranges, for\ninstance, than what the server implementor might expect.\n\n> It does become a nightmare if it is applied to everything that can be tied into a server. It may be prudent to draw a line at APIs, or perhaps there needs to\n> be a further distinction in the draft -\n> * content generators (CGI, PHP, ASP, etc) - have a 'shallow', high-level view into the server, server is required to handle all protocol features (as in current\n> draft)\n> * APIs et al (Servlets, ISAPI, NSAPI, etc) - low-level 'deep' view, api user has library of protocol feature calls available\n>\n> Would this be more practical?\n\nYes, I think it makes sense to make this distinction.\n\n> > As far as your draft, I think there should be fewer \"MUST\" requirements.\n>\n> It depends on how you place the draft. If you think of it as a peer document to 2616 (you must be compliant with this draft to be considered HTTP\n> compliant), you're right - it's far too strict. However, if you consider it as an additional, higher level of compliance, the MUSTS are important.\n\nOnly if they truly apply to all resources in a generic way and I don't think there was any in your draft that did.\n\n> I do think that some of the SHOULDs (especially to do with buffering and *perhaps* synthetic validation) might be better as MAYs.\n\nActually, the buffering is one of the main advantages of 1.1 and I believe it should be a SHOULD. But I agree about the synthetic validation being a MAY.\n\n> > For instance, the partial content requirement won't necessarily fly with all\n> > type of content generators. According to RFC2616 section 14.5, a server is not\n> > required to serve ranges for all resources - it can send \"Accept-range: None\". That\n> > doesn't preclude it from supporting ranges on other resources, such as\n> > static files. I like this flexibility.\n>\n> But from a protocol/network/user perspective, does it make any sense that a feature is available for one resource but not another, based on the\n> underlying technology?\n\nThe problem is that the underlying server technology is not monolithic. There isn't a clear line between the daemon and what you call content generators.\nIf you look at the way that large web sites are implemented today, it can be pretty complex. You always have an HTTP daemon on the front-end, then possibly a\nplug-in running in the daemon process, which in turn may communicate with an application server on a remote machine, itself accessing corporate data on other\nservers.\n\nIn such a case, I would argue that the content generator is the application server. But the HTTP daemon cannot handle all aspects of the protocol related to such a\ncomplex environment ; instead the plug-in would have to deal with some of them.\n\n>If you have a PDF file that happens to be generated by a CGI script, it's a huge pain to get it to support ranges, validation and\n>other facilities in the script itself. I've done it, and it was not pleasant.\n\nWhat you suggest when requesting the server implementor to do this job is that  the server has to regenerate the whole dynamic page just to get a few bytes. This\nmay involve rerunning the application. Even if your CGI sends exactly the same-content with the same last-modified date, so that the client still thinks it's the\nsame content between HTTP requests, the PDF plug-in might make 10 different HTTP requests in a row with various ranges. The server executes the script 10 times,\ngets the full PDF content from the script, and then only sends the requested bytes. Technically, it works, but it's extremely inefficient.\nOn the other hand, if you let the CGI itself handle the range request, then it's not so bad : the CGI will try to generate those requested bytes itself and won't\nwaste memory or CPU trying to generate and send the entire thing.\n\nRanges work best with direct access content (eg: static files) or with cacheable content ; with dynamic content, the server typically only has sequential access to\nthe content and does not cache it.\n\nThat's not to say the HTTP server shouldn't do anything. I think it makes sense for it to transparently do chunking on CGI and plug-ins output, and the way we are\ndoing it now works very nicely (though I wish we could use our own browser to test this feature :)).\n\n> A lot of the feedback that I've gotten has been in this vein - that not every object is going to take advantage of all of these 'extra' services. But, if they\n> aren't available by default, chances are they'll never be thought of, much less handled.\n\nSome features are the responsibility of the application, not all are the server's.\n\n> The other way that I'm working on attacking this problem is creating a series of wrapper scripts / libraries that will take care of as much of this stuff as\n> possible automatically. Unfortunately, it's very much an uphill battle (for instance, if I include a content-length in Netscape or Apache CGI, it will make the\n> connection persistent; no such luck with other servers), so this would only be a temporary measure.\n\nThis won't be an issue in the long term at least for NES servers; with our post-4.0 release, you won't need to do any of this to get the full benefits of persistent\nconnections and chunking.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-11517626"}, {"subject": "Comments on revision 03 of &quot; Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.1 &quot", "content": "As the primary author of a free http/1.0+ web server (for OS/2)\nwith a goal of keeping the product reasonably up to date,\nI have been sporadically following the http 1.1 drafts, related RFC's, \nand the HTTP-WG mailing list.  At the recommendation of several\nworking group members, I have read the rev. 03 of the draft; with an\neye toward what an implementor might have problems with. From\nthat viewpoint, consider the following comments:\n\n) The importance of chunked coding -- add a note to 19.6\n\n  The default of persistent connections has major  implications for the\n  delivery of dynamic  documents,especially when compared to  http \n   1.0. Although this is discussed in the draft, I  believe that it\n    should be strongly emphasiszd. In  particular, a paragraph should be\n    added to 19.6.1 (or  19.6.2). For example:\n\n   \"Given that persistent connections are the http/1.1  default, special\n   care must be taken when  dynamically generating output, especially   \n   when  earlier portions of content are sent to the client  as they are\n   generated (say, to prevent automatic or  human time-outs). In this\n   (and other) cases, there  may  be no way of knowing the final length\n    of   content, hence a content-length header can not be  added.\n    Hence, either the connection must be closed after transmittal of\n    content, or chunked coding  must be used. \n\nII) \"Chunked\" in the TE header -- clarify  description\n\n  It is somewhat odd that:\n    i) Given that (section 3.6.1, and reiterated in \n        step 3 of 14.39)\n        \"All HTTP/1.1 applications MUST be able to   recieve and\n         decode the \"chunked\" transfer    coding...\"\n   ii) Also in (section 3.6.1, and reiterated in 14.39 and\n       14.40)\n       \"A server using chunked transfer-coding in a    response MUST\n        NOT use the trailer for other   header fields than ... unless the\n       \"chunked\"    transfer-coding is present ..in the TE field).\n\n     Since 1.1 apps (such as http/1.1 servers) must  understand     \n     \"chunked\", then point ii seems to mean  (informally):\n      \"the use of \"chunked\" in the TE field tells an http/1.1 server that\n      header fields other then Content-MD5   and Authentication-Info\n     (and Content-Length)   may be included in the trailer\".\n\n    Assuming I'm not misreading, it might be useful to \n    include this comment (or an appropriately formal\n    version).\n\n   \nIII) Pipelining -- does order of execution matter?\n\n  Section 8.1.2.2 states:\n    \"A server MUST send it's responses to those  requests in the  same\n   order that the request were recieved.\"\n\n  Does this imply that there should be no parallelism when  processing\n   piplined requests: that request A should  be completely answered\n  before request B is   considered. Or, is parallel resolution of these\n  requests permitted, so long as order of return is   serial (and follows\n   the order of requests).\n  \n\nIV) Closing connections \n\n    Section 8.1.4 states:\n\n    i) \"When a client or server wishes to time-out, it   SHOULD issue a\n       graceful close on the transport   connection\".\n\n    Does this imply some sort of action at the http level? \n    That is, should a 4xx (or 5xx) response be sent? \n\n    And what if this time out occurs in the middle of a  response;\n     say,the early portion of a dynamic  resource is sent, and then an\n     unexpectedly long  delay occurs whilst resolving the remainder of     \n     this  dynamic resource (thus, a new response line &    headers can\n     not be sent).\n\n    \n    ii) \"Servers SHOULD NOT close a connection in the   middle of\n       transmitting a  response,  unless a network or client failure is \n       suspected\".\n\n    Does that disallow a \"total time per connection\" server setting? \n    Even if an otherwise legitimate request is taking hours to resolve? \n\n\nV) Minor points\n  \n  * It might be useful to add in section 10.3.7\n       \"The difference between 302 and 307 is that 307 insures  (for\n        http 1.1 clients) that a redirection occurs without a change in\n        method.\"\n\n  *In 13.1.1, point 2, it is written:\n       \"In the default case, this means it meets the least restrictive\n        freshness requirement...\"\n   Shouldn't that be \"most restrictive\".\n\n\nDaniel Hellerstein\ndanielH@econ.ag.gov\nhttp://rpbcam.econ.ag.gov/srehttp\n\n\n\n", "id": "lists-012-1151839"}, {"subject": "Re: Serverside roles in the HTT", "content": "Carl,\n\nThanks for that. I should clarify in the draft that it's aimed at 1.1 AND\n1.0 connections (hence, buffering and so forth).\n\n\n> Why wouldn't streaming the response with the \"chunked\" encoding be\npreferable to\n> buffering and generating a Content-Length?  I don't think the server\nshould\n> attempt to buffer the response unless it's talking to an HTTP/1.0 client.\n\nThat's what is intended; will clarify.\n\n\n> >   Servers SHOULD reply to requests that allow transfer encoding of\n> >  objects (i.e., TE header present) with appropriate encoding, in a\n> >  fashion transparent to the content generator.\n>\n> This paragraphs seems to suggest that transfer encoding is only allowed if\nthe\n> TE request-header is present.  However, if the TE field-value is empty or\nif no\n> TE field is present, the transfer-coding \"chunked\" is allowed, and is\nalways\n> acceptable.\n\nIntention here was more on the compress, deflate, gzip side than chunked.\nperhaps\n(e.g., compression-related TE request headers)\ninstead of\n(i.e., TE header present)\n?\n\n\n> >   There MUST be a method by which content generators can specify that\n> >   content is not to be buffered; this MAY be performed by a pseudo-\n> >   HTTP header that is consumed by the server.\n>\n> I think you're only addressing generated (response) content in this\nstatement.\n> I'd also like to have a method by which content generators can specify\nthat the\n> received (request) content is not to be buffered.   Sometimes the \"content\n> generator\" can be doing something useful with the content of a large POST\n> request while it's still being received (or even while it's still being\n> generated by the client).  In some applications (e.g., IPP), POST content\nmay be\n> so large that you don't even want the server to attempt to buffer it.\n\nAh, interesting. yes, the request side would be good to put in there - will\nreread the IPP> chunked post messages.\n\nI hestiated to put in actual suggestions for buffering thresholds.\nSuggestions?\n\nThanks,\n\n\n\n", "id": "lists-012-11532051"}, {"subject": "Re: Serverside roles in the HTT", "content": "Hi,\n\nkugler@us.ibm.com wrote:\n\n> > Servers MUST support a persistent connection if the content\n> >   generator supplies a Content-Length header. If it is not available,\n> >  the server SHOULD attempt to buffer the response in order to\n> >   generate one, although this MAY be circumvented if:\n> >   * the server does not have resources (i.e., memory) to do so, or\n> >   * the object is very large, and overall latency becomes\n> >   unacceptable, or\n> >   * the time required to generate the object adds unacceptable latency\n> ...\n> >  Servers MUST serve chunked encoding responses for all objects, if:\n> >   * Content-Length is unavailable and impractical to generate\n> >   and\n> >   * the client advertises itself as HTTP/1.1 capable, or\n> >   * the client includes 'chunked' in a TE header\n>\n> Why wouldn't streaming the response with the \"chunked\" encoding be preferable to\n> buffering and generating a Content-Length?\n\nIt depends on the size of the object and how long it takes to generate the content.\nIf the content is small and generated quickly, it makes sense for the server to\nbuffer the response and transparently add a content-length header, as opposed to\nusing chunking. Chunking for small objects will waste bandwidth unnecessarily and\nprovide no benefits if the full content is immediately available.\nIf the application generates too much content to buffer, or ends up taking too long\nto generate little content, then the server can choose to chunk.\nBut this may be a matter for webmasters do decide depending on their applications,\nhow much bandwidth they have, and how much latency they can accept, so we are making\nthose time and buffer size thresholds configurable. We may also add settings for the\nminimum size of the chunks themselves to conserve bandwidth.\n\n> > 6.4 Transfer-Encoding\n>\n> >   Servers SHOULD reply to requests that allow transfer encoding of\n> >  objects (i.e., TE header present) with appropriate encoding, in a\n> >  fashion transparent to the content generator.\n>\n> This paragraphs seems to suggest that transfer encoding is only allowed if the\n> TE request-header is present.  However, if the TE field-value is empty or if no\n> TE field is present, the transfer-coding \"chunked\" is allowed, and is always\n> acceptable.\n\nGood point - looks like Mark should add a provision for this case.\n\n> >   There MUST be a method by which content generators can specify that\n> >   content is not to be buffered; this MAY be performed by a pseudo-\n> >   HTTP header that is consumed by the server.\n>\n> I think you're only addressing generated (response) content in this statement.\n> I'd also like to have a method by which content generators can specify that the\n> received (request) content is not to be buffered.   Sometimes the \"content\n> generator\" can be doing something useful with the content of a large POST\n> request while it's still being received (or even while it's still being\n> generated by the client).  In some applications (e.g., IPP), POST content may be\n> so large that you don't even want the server to attempt to buffer it.\n\nThis won't work for some content generators.\nFor example, you can't start a CGI before you know the length of the POST data\nbecause you have to set it in the CGI process environment variables.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-11541623"}, {"subject": "httpwgrequest&#64;hplb.hp.co", "content": "subscribe http-wg Ray Slater\n\n__________________________________________________\nDo You Yahoo!?\nBid and sell for free at http://auctions.yahoo.com\n\n\n\n", "id": "lists-012-11552854"}, {"subject": "Re: Serverside roles in the HTT", "content": "jpierre wrote:\n>\n>\n>Hi,\n>\n>kugler@us.ibm.com wrote:\n>\n>> > Servers MUST support a persistent connection if the content\n>> >   generator supplies a Content-Length header. If it is not available,\n>> >  the server SHOULD attempt to buffer the response in order to\n>> >   generate one, although this MAY be circumvented if:\n>> >   * the server does not have resources (i.e., memory) to do so, or\n>> >   * the object is very large, and overall latency becomes\n>> >   unacceptable, or\n>> >   * the time required to generate the object adds unacceptable latency\n>> ...\n>> >  Servers MUST serve chunked encoding responses for all objects, if:\n>> >   * Content-Length is unavailable and impractical to generate\n>> >   and\n>> >   * the client advertises itself as HTTP/1.1 capable, or\n>> >   * the client includes 'chunked' in a TE header\n>>\n>> Why wouldn't streaming the response with the \"chunked\" encoding be preferable\n to\n>> buffering and generating a Content-Length?\n>\n>It depends on the size of the object and how long it takes to generate the\ncontent.\n>If the content is small and generated quickly, it makes sense for the server to\n>buffer the response and transparently add a content-length header, as opposed\nto\n>using chunking. Chunking for small objects will waste bandwidth unnecessarily\nand\n>provide no benefits if the full content is immediately available.\n\nIf the full content is immediately available, it would be sent as one chunk.\nHow does that waste bandwith?   Let's see:  \"Content-Length: xxxx\" header, about\n20 bytes; vs. \"Transfer-Encoding:  chunked\" header, about 27 bytes.  A\nchunk-length and CRLF, say 6 bytes, and a 0,CRLF,CRLF terminator, say 5 more\nbytes.  So we're looking at about 38 bytes of overhead vs. 20.   Is that\nsignificant?\n\nChunking does provide benefits if the full content is not immediately available,\nand adds (what I consider to be) an insignificant cost if it is not, so I think\nit would be better to default to chunking instead of using a bunch of heuristics\nto try to coalesce response content.\n\nEither that, or provide a way for \"content-producers\" to control the\ntransfer-coding.\n\n>If the application generates too much content to buffer, or ends up taking too\nlong\n>to generate little content, then the server can choose to chunk.\n>But this may be a matter for webmasters do decide depending on their\napplications,\n>how much bandwidth they have, and how much latency they can accept, so we are\nmaking\n>those time and buffer size thresholds configurable. We may also add settings\nfor the\n>minimum size of the chunks themselves to conserve bandwidth.\n>\n>> > 6.4 Transfer-Encoding\n>>\n>> >   Servers SHOULD reply to requests that allow transfer encoding of\n>> >  objects (i.e., TE header present) with appropriate encoding, in a\n>> >  fashion transparent to the content generator.\n>>\n>> This paragraphs seems to suggest that transfer encoding is only allowed if\nthe\n>> TE request-header is present.  However, if the TE field-value is empty or if\nno\n>> TE field is present, the transfer-coding \"chunked\" is allowed, and is always\n>> acceptable.\n>\n>Good point - looks like Mark should add a provision for this case.\n>\n>> >   There MUST be a method by which content generators can specify that\n>> >   content is not to be buffered; this MAY be performed by a pseudo-\n>> >   HTTP header that is consumed by the server.\n>>\n>> I think you're only addressing generated (response) content in this\nstatement.\n>> I'd also like to have a method by which content generators can specify that\nthe\n>> received (request) content is not to be buffered.   Sometimes the \"content\n>> generator\" can be doing something useful with the content of a large POST\n>> request while it's still being received (or even while it's still being\n>> generated by the client).  In some applications (e.g., IPP), POST content may\n be\n>> so large that you don't even want the server to attempt to buffer it.\n>\n>This won't work for some content generators.\n>For example, you can't start a CGI before you know the length of the POST data\n>because you have to set it in the CGI process environment variables.\n\nThe one pathological case is CGI/1.1 and chunked POST requests.  Supposedly,\nfuture versions of CGI will deal with chunking.  And if the request provides\nContent-Length, you don't have to buffer it, even for CGI/1.1.\n\n>\n>--\n>for a good time, try kill -9 -1\n>\n>\n> - jpierre.vcf\n> - smime.p7s\n\n\n\n", "id": "lists-012-11559433"}, {"subject": "Re: Serverside roles in the HTT", "content": "Mark wrote:\n>\n>Carl,\n>\n>Thanks for that. I should clarify in the draft that it's aimed at 1.1 AND\n>1.0 connections (hence, buffering and so forth).\n>\n>\n>> Why wouldn't streaming the response with the \"chunked\" encoding be\n>preferable to\n>> buffering and generating a Content-Length?  I don't think the server\n>should\n>> attempt to buffer the response unless it's talking to an HTTP/1.0 client.\n>\n>That's what is intended; will clarify.\n>\n>\n>> >   Servers SHOULD reply to requests that allow transfer encoding of\n>> >  objects (i.e., TE header present) with appropriate encoding, in a\n>> >  fashion transparent to the content generator.\n>>\n>> This paragraphs seems to suggest that transfer encoding is only allowed if\n>the\n>> TE request-header is present.  However, if the TE field-value is empty or\n>if no\n>> TE field is present, the transfer-coding \"chunked\" is allowed, and is\n>always\n>> acceptable.\n>\n>Intention here was more on the compress, deflate, gzip side than chunked.\n>perhaps\n>(e.g., compression-related TE request headers)\n>instead of\n>(i.e., TE header present)\n>?\n>\n\nIn that case, I think you want the \"Accept-Encoding\" header, not TE. (Make sure\nyou're looking at a fresh copy of the spec.)  The issue is of content-encoding\nvs. transfer-encoding.\n\n>\n>> >   There MUST be a method by which content generators can specify that\n>> >   content is not to be buffered; this MAY be performed by a pseudo-\n>> >   HTTP header that is consumed by the server.\n>>\n>> I think you're only addressing generated (response) content in this\n>statement.\n>> I'd also like to have a method by which content generators can specify\n>that the\n>> received (request) content is not to be buffered.   Sometimes the \"content\n>> generator\" can be doing something useful with the content of a large POST\n>> request while it's still being received (or even while it's still being\n>> generated by the client).  In some applications (e.g., IPP), POST content\n>may be\n>> so large that you don't even want the server to attempt to buffer it.\n>\n>Ah, interesting. yes, the request side would be good to put in there - will\n>reread the IPP> chunked post messages.\n>\n>I hestiated to put in actual suggestions for buffering thresholds.\n>Suggestions?\n>\n>Thanks,\n>\n>\n>\n\n\n\n", "id": "lists-012-11572296"}, {"subject": "Re: Serverside roles in the HTT", "content": "Hi,\n\nkugler@us.ibm.com wrote:\n\n> If the full content is immediately available, it would be sent as one chunk.\n\nNot necessarily.\nIt depends how the CGI is writing to stdout, on the OS, pipe buffer size, and on the\nload on the machine.\nIf a dumb CGI writes one byte at a time to stdout, even in a tight loop, it's\npossible that the kernel will make the daemon thread wake up and return it that\nsingle byte, which would then be sent as a chunk if no buffer thresholds are set.\nEvery other 1-byte write may also result in another chunk. Some thresholds are\nnecessary to avoid those cases.\n\n> >This won't work for some content generators.\n> >For example, you can't start a CGI before you know the length of the POST data\n> >because you have to set it in the CGI process environment variables.\n>\n> The one pathological case is CGI/1.1 and chunked POST requests.  Supposedly,\n> future versions of CGI will deal with chunking.  And if the request provides\n> Content-Length, you don't have to buffer it, even for CGI/1.1.\n\nTrue - and we don't buffer the POST data in our daemon.\n\nThis reminds me of a case I was debugging a few weeks ago :\nan echo CGI script writes the POST data to stdout as it comes, in a simple loop. The\nproblem is that most HTTP/1.0 clients send the full request first, then wait for the\nserver reply.\n\nBut if the POST data is large, at some point, the server's socket buffer gets full\nwhen trying to send the reply, since the client isn't reading the reply yet (it's\nstill not done POSTing the request). At that point, both the clients and the server\nare trying to send() and one of them times out (typically the server).\n\nI haven't seen anything in the CGI spec that requires a script to first read all its\ninput before its starts writing to stdout. The script should be the proper place to\ndo the buffering for this case.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-11582642"}, {"subject": "Re: Serverside roles in the HTT", "content": "kugle-@us.ibm.com wrote: \noriginal article:http://www.egroups.com/group/http-wg/?start=8737\n> Hi,\n> \n> kugler@us.ibm.com wrote:\n> \n> > If the full content is immediately available, it would be sent as\none chunk.\n> \n> Not necessarily.\n> It depends how the CGI is writing to stdout, on the OS, pipe buffer\nsize, and on the\n> load on the machine.\n> If a dumb CGI writes one byte at a time to stdout, even in a tight\nloop, it's\n> possible that the kernel will make the daemon thread wake up and\nreturn it that\n> single byte, which would then be sent as a chunk if no buffer\nthresholds are set.\n> Every other 1-byte write may also result in another chunk. Some\nthresholds are\n> necessary to avoid those cases.\n> \n\nWell, I just want to caution against optimizing for a pathological case\nat the expense (e.g., reduced responsiveness) of normal cases.\n\nAnyway, it's not that important anyway, since we're only talking about\nthe default behavior.  Mark's draft provides that:\n   There MUST be a method by which content generators can specify that\n   content is not to be buffered; this MAY be performed by a pseudo-\n   HTTP header that is consumed by the server.\nSo, if I don't want my responses buffered I can turn off buffering (in\ntheory).\n\n    -Carl\n\n\n\n", "id": "lists-012-11592526"}, {"subject": "Re: Serverside roles in the HTT", "content": "> In that case, I think you want the \"Accept-Encoding\" header, not TE. (Make\nsure\n> you're looking at a fresh copy of the spec.)  The issue is of\ncontent-encoding\n> vs. transfer-encoding.\n\nNo, I was thinking of transfer-encoding, which can be used for\ncompression-type services.\n\nContent-Encoding would be interesting, but potentially it really isn't as\nuser-transparent as most of the other facilities I'm talking about. Will\nthink about it...\n\n\n\n", "id": "lists-012-11600913"}, {"subject": "POST Header", "content": "First message to the list..\n\nI bumped into HTTP POST version issues about a month ago, and am looking\nfor a definative reference to some data that defines the standard HTTP POST\nformats for  HTTP/0.9, HTTP/1.0 and HTTP/1.1\n\nI managed to figured out by brute force that the HTTP version was required\non the POST /url line in order to get the server to support multple\nheaders.  But upgrading the version from 1.0 to 1.1 caused the servers\n(Apache and NES) to abort with \"invalid protocol\" messages.\n\nWeb searches, RFC reads, and consultation with Netscape and it's affiliates\nhas not produced an answer.\n\nAny help would be greatly appreciated...\n\nWilbur\n\n     --------------------------------------------\n        Putting A Human Face On Technology ;-)\n     --------------------------------------------\n  Literally!  http://www.monmouth.com/~wstreett/FaceIT/\n\n\n\n", "id": "lists-012-11608820"}, {"subject": "Host header issu", "content": "In going over some implementation stuff here, I ran into\nan issue that I apparently misread in the spec.\n\nAccording to the spec, the host: header is always required\nin HTTP/1.1.  In my mind, I always perceived that to mean\nthat it was required as long as a relativeURI was used\nin the request.   Looking closely at the spec, I see\nthat is not the case.\n\nIt seems to me that if you have an absolute URI, the host\nheader is redundant.  If I remember correctly, from the wg\nmeetings in the long past, the host header was added because\nabsolute URIs werent used to origin servers.\nNow that HTTP/1.1 allows the use of absolute URIs, doesnt\nit seem wise to remove the requirement of the host: header when\nthey are used?\n\nIt certainly makes things simpler if Im build a new client/server\nsystem to just use absolute URIs consistently.\n\nCan anyone explain what the justification was for requiring\nthe host: header even when their is a perfectly good\nhost indication in the absoluteURI?\n\nFrankly, I can't beleive I didn't raise this sooner, I always\nassumed this was the case.   I only ran into it in a debate\nhere with a colleague.\n\n\n\n", "id": "lists-012-11616065"}, {"subject": "Re: Host header issu", "content": "> In going over some implementation stuff here, I ran into\n> an issue that I apparently misread in the spec.\n> \n> According to the spec, the host: header is always required\n> in HTTP/1.1.  In my mind, I always perceived that to mean\n> that it was required as long as a relativeURI was used\n> in the request.   Looking closely at the spec, I see\n> that is not the case.\n[...]\n> Can anyone explain what the justification was for requiring\n> the host: header even when their is a perfectly good\n> host indication in the absoluteURI?\n\nHTTP/1.1 was deliberately specified the way that it is as\na transition measure. \n\nRFC 2616 says in section 5.1.2:\n\n\"To allow for transition to absoluteURIs in all requests in future\nversions of HTTP, all HTTP/1.1 servers MUST accept the absoluteURI\nform in requests, even though HTTP/1.1 clients will only generate\nthem in requests to proxies.\"\n\nNote that if you have a client send an absolute URL to anything\nbut a proxy, it's not HTTP/1.1.\n\nBut this could allow writing, say, an HTTP/2.0 spec that used\nabsolute URLs everywhere. (Though the complexity of writing\nHTTP/1.1 may push that further into the future, and recent\ntalk leans more towards a binary wire protocol.)\n\nThe Host: header was introduced because it allowed use of name\nbased virtual hosting without breaking old servers. It could be\nignored by servers that did not understand it, whereas shifting\nclients to using absolute URLs would have caused old HTTP/1.0\nservers to return \"Forbidden\" errors all over the place.\n\nSo you can look at this potentially as a two phase transition,\neach step of which is compatible with one version prior.\n\n--\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-012-11625212"}, {"subject": "Re: Host header issu", "content": ">Can anyone explain what the justification was for requiring\n>the host: header even when their is a perfectly good\n>host indication in the absoluteURI?\n\nSo that an old proxy will forward the Host field, and to satisfy an\nIESG request.\n\n....Roy\n\n\n\n", "id": "lists-012-11634050"}, {"subject": "HTTP/0.", "content": "Is there a document that describes HTTP/0.9?\n\nWhat I am particularly interested in is the format of the request line\n(there were no headers, right?).\nThanks,\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-1163905"}, {"subject": "RE: Host header issu", "content": "> -----Original Message-----\n> From: Albert-Lunde@nwu.edu [mailto:Albert-Lunde@nwu.edu]\n> Sent: Saturday, September 04, 1999 9:52 PM\n> To: Josh Cohen (Exchange)\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: Host header issue\n> \n> \n> But this could allow writing, say, an HTTP/2.0 spec that used\n> absolute URLs everywhere. (Though the complexity of writing\n> HTTP/1.1 may push that further into the future, and recent\n> talk leans more towards a binary wire protocol.)\n> \n> The Host: header was introduced because it allowed use of name\n> based virtual hosting without breaking old servers. It could be\nI agree with this..\n\n> ignored by servers that did not understand it, whereas shifting\n> clients to using absolute URLs would have caused old HTTP/1.0\n> servers to return \"Forbidden\" errors all over the place.\n>\nYes, obviously a 1.0 server will choke all over the place if you\ninclude an absoluteURI.  However, including both a host header\nand an absoluteURI will choke the server as well.\nIf your talking to a 1.0 server, there simply is no way you\ncan expect it to understand an absoluteURI.\n\nIn the 1.1 case, it can understand both an absolute URI as well\nas the Host: header case.\n\nSo, I dont see this as a compatible \"transistion\".\nIf we perceive the \"transistion\" to be toward absoluteURIs,\nthen we should not discourage their use.  This is\neffectively what we have done.  The http/1.1 spec teaches\nus that there is no good reason to use absoulteURIs\nsince we MUST include a host header as well.  (Even\nwhen we know that the server and proxy are 1.1 compliant\nand understand absoluteURIs).\n\nI guess what I'm complaining about is that the spec shouldnt\nrequire a server to bounce a request if there is an absoluteURI\nbut no Host: header, especially when it can understand the\nimplied host: header value.\n\nI think its especially frustrating since it violates one of\nthe basic tenents of protocol design which is be conservative\nin what you send, but liberal in what you accept.\n\nI think we should remove this MUST from server implementation.\n \n> So you can look at this potentially as a two phase transition,\n> each step of which is compatible with one version prior.\n> \n> --\n>     Albert Lunde                      Albert-Lunde@nwu.edu\n> \n\n\n\n", "id": "lists-012-11641574"}, {"subject": "RE: Host header issu", "content": "The reasoning for the MUST requirement for the Host header\nwas to leave absolutely no ambiguity for whether/when it\nwas required, and to ensure that there sufficient servers\nthat required it to force individuals to upgrade their clients\nif they had clients that didn't send it.\n\nAdding this requirement was imposed by the IESG; I don't\nthink it was originally the consensus of the HTTP working\ngroup to do so.\n\nThe arguments you've given against the requirement are\nprimarily ones of 'general design principles'. If you\nwere to develop or deploy a server that did not follow\nthis guideline, you'd have a non-compliant server that\nwas interoperable with all compliant clients, in addition\nto being interoperable with some non-compliant clients.\nThat's good implementation advice in general, but there\nare occasionally reasons to violate general design principles.\n\nYou might be able to convince the IESG to drop the requirement\nin the case where an absolute URI is supplied, but it's\nmy guess that the answer will depend on how successful the\noriginal attempt to change user behavior through requirements\non server compliance has been. Are there sufficient \ncompliant HTTP servers around that users have upgraded\ntheir clients? My impression is 'no'.\n\nLarry\n\n\n\n", "id": "lists-012-11652381"}, {"subject": "RE: Host header issu", "content": "> -----Original Message-----\n> From: Larry Masinter [mailto:masinter@parc.xerox.com]\n> Sent: Monday, September 06, 1999 1:15 PM\n> To: Josh Cohen (Exchange); Albert-Lunde@nwu.edu\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: RE: Host header issue\n> \n> You might be able to convince the IESG to drop the requirement\n> in the case where an absolute URI is supplied, but it's\n> my guess that the answer will depend on how successful the\n> original attempt to change user behavior through requirements\n> on server compliance has been. Are there sufficient \n> compliant HTTP servers around that users have upgraded\n> their clients? My impression is 'no'.\n>\nFor non-absoluteURI requests:\n\nI would think that you would have to search alot to find\na real browser that doesnt send the host: header today.\nSo, the \"user behavior\" is pretty much done.\n\nIm sure there are some scripts and stuff that dont, but\nI dont think that they are going to change their behavior no\nmatter what.  Since they probably do http/1.0 requests, no matter\nhow many 1.1 web servers get deployed, those servers are not\ngoing to require a host header from 1.0 requests.\n\nthis rule is not doing anything to cause these rare\nscript clients to be upgraded since servers will still support\n1.0 without host:.\n\nfor the absoluteURI request case:\n\nIf an absoluteURI is supplied to an origin server by the client,\nthen that client is obviously a 1.1 client. (excluding the pathological\ncase where a 1.0 client mistakes an origin for a proxy)\n\nThis rule is adding no value in the 1.1 client case.\n\nThe end result is that a 1.1 client request must send redundant\ninfo both in the host: header and absoluteURI if it wants to use \nabsoluteURIs.\nTo furthur complicate things, at \"some point in the future\", the\nclient will (perhaps in 1.2?) be able to stop sending both ?\n\nIn summary, IMHO, this MUST is a well intentioned addition to the spec\nthat in the end tries (but fails) to cover a legacy case and in the process,\ncomplicates the mainstream/future case.   I think this is bad.\n\n\n\n", "id": "lists-012-11661191"}, {"subject": "Re: Host header issu", "content": "\"Josh Cohen (Exchange)\" wrote:\n\n> If an absoluteURI is supplied to an origin server by the client,\n> then that client is obviously a 1.1 client. (excluding the pathological\n> case where a 1.0 client mistakes an origin for a proxy)\n\nThis isn't necessarily pathological.  My home network includes a proxy; it runs\nApache with mod_proxy, and that same Apache also serves up its own content.  It\nhas two CNAMEs, \"proxy\" and \"www\"; my browser is configured to use \"proxy\" as\nits proxy.  So, if I send my browser to http://www/foo.html, then Apache gets a\nrequest whose request-URI is http://www/foo.html.  It knows that www is itself,\nso it delivers its own content.\n\n--\n/============================================================\\\n|John Stracke    |http://www.ecal.com|My opinions are my own.|\n|Chief Scientist |===========================================|\n|eCal Corp.      |\"Who died and made you king?\" \"My father.\" |\n|francis@ecal.com|                                           |\n\\============================================================/\n\n\n\n", "id": "lists-012-11671630"}, {"subject": "RE: Host header issu", "content": "> -----Original Message-----\n> From: John Stracke [mailto:francis@ecal.com]\n> Sent: Tuesday, September 07, 1999 11:30 AM\n> To: http-wg@hplb.hpl.hp.com\n> Subject: Re: Host header issue\n> \n> This isn't necessarily pathological.  My home network \n> includes a proxy; it runs\n> Apache with mod_proxy, and that same Apache also serves up \n> its own content.  It\n> has two CNAMEs, \"proxy\" and \"www\"; my browser is configured \n> to use \"proxy\" as\n> its proxy.  So, if I send my browser to http://www/foo.html, \n> then Apache gets a\n> request whose request-URI is http://www/foo.html.  It knows \n> that www is itself,\n> so it delivers its own content.\n>\nIn which case you really dont need the Host: header...\nhence my point..\n\n> --\n> /============================================================\\\n> |John Stracke    |http://www.ecal.com|My opinions are my own.|\n> |Chief Scientist |===========================================|\n> |eCal Corp.      |\"Who died and made you king?\" \"My father.\" |\n> |francis@ecal.com|                                           |\n> \\============================================================/\n> \n> \n\n\n\n", "id": "lists-012-11679633"}, {"subject": "RE: Host header issu", "content": "You know, I'm still not sure under which situation you\nwould \n\na) not require a Host: header to be sent by a client;\n   (since there are HTTP/1.1 servers that would error\n   if the Host header were omitted)\n\nb) not require that a server respond with an error if the Host\n  header were missing (since HTTP/1.1 clients should only\n  send absolute URIs to proxies.)\n\nProcedurally, since HTTP/1.1 is Draft Standard, the only way\nI can think to institute a change such as this would be to write\na separate document, as Proposed Standard, which updates 2616;\nI'd suggest:\n\n   Omitting Host Header from HTTP/1.1 Requests with Full URI\n\nas a title.\n\nNote that:\n\n   1. If Request-URI is an absoluteURI, the host is part of the\n     Request-URI. Any Host header field value in the request MUST be\n     ignored.\n\nso a valid request would be\n\nGET http://host.dom/path HTTP/1.1\nHost: \n\nand you're saving 7 bytes for the \"CRLFHost:\" But then, if you're so\nconcerned about saving 6 bytes, why not also work on encoding the 20 bytes\nthat comprise \"GET http://\" and \"HTTP/1.1\" into something more efficient?\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-11688908"}, {"subject": "Re: Host header issu", "content": "Here's the story, folks (short history lesson):\n\nThe lack of host information meant that IP addresses has had to be assigned\nfor all virtual host web sites.\n\nNot only did this waste IP addresses (getting to be a scarce resource), \nit caused MAJOR headaches among major ISP's doing Web hosting (think about \nthe problems caused by adding the 257'th host to a web server on a class \nC net (or whatever you call the CIDR equivalent these days).\n\nSo certain (more than one) IESG/IAB members insisted this get fixed in\nsuch a way that it COULD NOT UNDER ANY CIRCUMSTANCES be gotten wrong in all\nsoftware going forward.  There was no confidence in people getting\nit right otherwise.\n\nAt least one of these people said 'I don't care if HTTP/1.1 has anything\nelse', as a way to show the vehemence of the problem.\n\nErgo, the requirement to report buggy clients with an error.\n\nAnd note Roy's remark about proxies...\n\nMight there be some further optimization possible?  Maybe.\nBut we sweat blood over them words, as I remember.  And they were written\nto enable transitioning to absolute URI's in a V1.2, as I also remember\n(we had a problem that would have made that hard at one point, that did\nget fixed).\n\nIf you insist on riding this horse, you should start by refreshing your \nmind with all the discussion on the topic, and of exactly how the spec \nwords got to where they are. The issues were more subtle than they\nfirst appeared, as I remember the discussion (and I don't remember all\nthe subtleties).\n- Jim\n\n\n\n", "id": "lists-012-11697452"}, {"subject": "RE: Host header issu", "content": "> -----Original Message-----\n> From: Larry Masinter [mailto:masinter@parc.xerox.com]\n> Sent: Tuesday, September 07, 1999 4:41 PM\n> To: Josh Cohen (Exchange)\n> Cc: HTTP Working Group\n> Subject: RE: Host header issue\n> \n> b) not require that a server respond with an error if the Host\n>   header were missing (since HTTP/1.1 clients should only\n>   send absolute URIs to proxies.)\n> \nI will double check, but my reading of the spec says that\norigin servers need to understand absoluteURIs.  If they \nreceive an absolute URI, what is the point of requiring \na host header?\n\n\n\n", "id": "lists-012-11706375"}, {"subject": "Comments on revision 03 of &quot; Hypertext Transfer Protocol &ndash;&ndash", "content": "IV.i - This really means, graceful socket close to avoid TCP reset.\nIV.ii - no, there is (somewhere) a phrase I asked for basically saying\n        that in any case the implementation is free to defend itself\n        from attacks. (and this to IV.i also).\n        The real problem that this item addresses is that the status\n        code is sent up-front, and if there is no content-length, there\n        is no good way for the client to know if the object was\n        truncated (or any indication of why).  This is especially a\n        problem for FTP proxies, who may not themselves know the size\n        of the object unless they parse a directory listing from the\n        server.  If one is chunking the output, you have a trailer, but\n        no currently defined way to indicate there that an error\n        occured.  Your only choice is to abort the connection.\n\nI for one, would really like to see the ability to add a trailer to\nthe end of any response indicating status.\n\n** Reply to note from Daniel Hellerstein <danielh@MAILBOX.ECON.AG.GOV>\nMon, 23 Mar 1998 11:58:27 -0500 >   \n<snip>\n>   \n> IV) Closing connections \n>   \n>     Section 8.1.4 states:\n>   \n>     i) \"When a client or server wishes to time-out, it   SHOULD issue a\n>        graceful close on the transport   connection\".\n>   \n>     Does this imply some sort of action at the http level? \n>     That is, should a 4xx (or 5xx) response be sent? \n>   \n>     And what if this time out occurs in the middle of a  response;\n>      say,the early portion of a dynamic  resource is sent, and then an\n>      unexpectedly long  delay occurs whilst resolving the remainder of     \n>      this  dynamic resource (thus, a new response line &    headers can\n>      not be sent).\n>   \n>     \n>     ii) \"Servers SHOULD NOT close a connection in the   middle of\n>        transmitting a  response,  unless a network or client failure is \n>        suspected\".\n>   \n>     Does that disallow a \"total time per connection\" server setting? \n>     Even if an otherwise legitimate request is taking hours to resolve? \n>   \n<snip>\n>   \n> Daniel Hellerstein\n> danielH@econ.ag.gov\n> http://rpbcam.econ.ag.gov/srehttp\n>   \n> \n \n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-1171050"}, {"subject": "RE: Host header issu", "content": "> > b) not require that a server respond with an error if the Host\n> >   header were missing (since HTTP/1.1 clients should only\n> >   send absolute URIs to proxies.)\n> > \n> I will double check, but my reading of the spec says that\n> origin servers need to understand absoluteURIs.\n\nYes.\n\n>  If they \n> receive an absolute URI, what is the point of requiring \n> a host header?\n\nTo insure that clients will send them, without a doubt,\nsince the client->server request is made without knowing\nthe compliance of the server its sending them to.\n\nAs was pointed out in a private communication, there might be\nolder proxies that would forward \n\n  GET http://example.com/path HTTP/1.1\n\nto 'example.com' merely as\n   GET /path HTTP/1.1\n\nwithout adding a Host header. However, if the client supplies\n\n   GET http://example.com/path HTTP/1.1\n   Host: example.com\n\nthen the proxy would forward\n\n   GET /path HTTP/1.1\n   Host: example.com\n\nSo clients should send Host headers even with absolute\nURLs when talking to proxies, which is the only time\nthey should send absolute URLs. So clients should send\nHost headers always. So servers should insure that clients\nare compliant by always requiring Host headers, even with\nabsolute URLs.\n\nLarry\n\n\n\n", "id": "lists-012-11714629"}, {"subject": "Re: Host header issu", "content": "At 08:13 AM 9/8/99 -0700, you wrote:\n>Not only did this waste IP addresses (getting to be a scarce resource), \n>it caused MAJOR headaches among major ISP's doing Web hosting (think about \n>the problems caused by adding the 257'th host to a web server on a class \n>C net (or whatever you call the CIDR equivalent these days).\n\nOh, so the problem with the IP address space is the reason that the design\ndoesn't make sense.  That makes perfect sense!\n\n\n>So certain (more than one) IESG/IAB members insisted this get fixed in\n>such a way that it COULD NOT UNDER ANY CIRCUMSTANCES be gotten wrong in all\n>software going forward.  There was no confidence in people getting\n>it right otherwise.\n\nPeople?  I thought that this was technology?\n\n>At least one of these people said 'I don't care if HTTP/1.1 has anything\n>else', as a way to show the vehemence of the problem.\n\nIn other words, fix our problems in other areas with a screwy design.\n\nTypical \"design by committee\" stuff.\n\nNow everything's clear.\n\nWilbur\n\n     --------------------------------------------\n        Putting A Human Face On Technology ;-)\n     --------------------------------------------\n  Literally!  http://www.monmouth.com/~wstreett/FaceIT/\n\n\n\n", "id": "lists-012-11723262"}, {"subject": "Re: Host header issu", "content": "Wilbur Streett wrote:\n\n> >So certain (more than one) IESG/IAB members insisted this get fixed in\n> >such a way that it COULD NOT UNDER ANY CIRCUMSTANCES be gotten wrong in all\n> >software going forward.  There was no confidence in people getting\n> >it right otherwise.\n>\n> People?  I thought that this was technology?\n\nSo how do you build technology? At eCal, we get people to do it.  :-)\n\nSeriously, though.  Engineering is more than coming up with the best\ntechnology; it's also coming up with something that can be built by real people\nwith real, human failings.  Protocols that are too complex for the people who\nneed to implement them will lose out to simpler protocols, if only because the\nimplementations of the complex protocols will be buggy and late.\n\n--\n/===============================================================\\\n|John Stracke    |http://www.ecal.com|My opinions are my own.   |\n|Chief Scientist |==============================================|\n|eCal Corp.      |The problem with any unwritten law is that you|\n|francis@ecal.com|don't know where to go to erase it.           |\n\\===============================================================/\n\n\n\n", "id": "lists-012-11731896"}, {"subject": "Proxies and broken HTT", "content": "I'd be grateful for some advice on how the slogan\n'applications should be liberal in what they receive and \nconservative in what they send' should be applied to\nproxy servers.\n\nThe particular case I have in mind is an origin server\n(which shall remain nameless) returning an entity body\nwith a 304. The client-side of our proxy is liberal \nenough to cope with this on receipt, but the server-\nside conservatively refuses to forward it.\n\nIn this scenario it seems pretty reasonable to discard \nthe entity: there's no way of reporting an error back\nto the origin server, and it seems a bit extreme to send\nthe client a 502. But trying to generalize to other\nkinds of upstream and downstream bad behaviour opens up \na can of worms. The options seem to be,\n\n1. Forward broken HTTP.\n\n2. Enforce strict conformance by responding to all bad\n   requests with 400s and all bad replies with 502s.\n\n3. Fixup up bad requests/replies on their way through \n   wherever possible.\n\n(1) is pretty much out of the question; (2) is over the\ntop; and (3) is completely unspecified, so liable to be\ndone differently be different proxy implementors.\n\nWhat to do?\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                          Cromwell Media\nInternet Systems Architect           5/6 Glenthorne Mews\n+44 (0)181 410 2230                  London, W6 0LJ\nmsabin@cromwellmedia.co.uk           England\n\n\n\n", "id": "lists-012-11739870"}, {"subject": "RE: Proxies and broken HTT", "content": "> The particular case I have in mind is an origin server\n> (which shall remain nameless) returning an entity body\n> with a 304. The client-side of our proxy is liberal\n> enough to cope with this on receipt, but the server-\n> side conservatively refuses to forward it.\n>\n> In this scenario it seems pretty reasonable to discard\n> the entity:\n\nSounds right to me.\n\n> [...] But trying to generalize to other\n> kinds of upstream and downstream bad behaviour opens up\n> a can of worms. The options seem to be,\n>\n> 1. Forward broken HTTP.\n>\n> 2. Enforce strict conformance by responding to all bad\n>    requests with 400s and all bad replies with 502s.\n>\n> 3. Fixup up bad requests/replies on their way through\n>    wherever possible.\n>\n> (1) is pretty much out of the question; (2) is over the\n> top; and (3) is completely unspecified, so liable to be\n> done differently be different proxy implementors.\n\nI think that the only sensible thing to do is to figure each case out as\nthey arise; you've arrived at a good solution for this one, and it\nsounds like you have it right - the server side is stricter than the\nclient side.\n\nI would make sure that your log files include the Server: header for any\noffending responses, so that proxy operators can complain effectivly to\nvendors.  I assume that you've not kept silent to the (nameless) vendor\nfor this obvious bug.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-11749048"}, {"subject": "Re: Host header issu", "content": "At 01:57 PM 9/9/99 -0400, you wrote:\n>> People?  I thought that this was technology?\n>\n>So how do you build technology? At eCal, we get people to do it.  :-)\n>\n>Seriously, though.  Engineering is more than coming up with the best\n>technology; it's also coming up with something that can be built by real\npeople\n>with real, human failings.  Protocols that are too complex for the people who\n>need to implement them will lose out to simpler protocols, if only because\nthe\n>implementations of the complex protocols will be buggy and late.\n\nAnd designing protocols in order to work around implementation failures\nmeans that the unnecessary complexity is added to the protocol, which then\nleads to more implementation issues, and then more complexity is added in\nthe protocol to resolve implementation issues, ...  and it spirals out of\ncontrol.\n\nThe reality is that elegance of design, (read, KISS), makes for a\ntechnology that can be implemented, supported, and useful.  HTTP wasn't\ndesigned by Commitee, and was simple enough that everyone could implement\nit.  Hence, even with it's design flaws, it became popular.\n\nThe reality of technology acceptance curves being what they are, I don't\nsee much more progress on HTTP with the sort of design process and\ndecisions that occured with the Host Header.\n\nWilbur\n\n\n\n     --------------------------------------------\n        Putting A Human Face On Technology ;-)\n     --------------------------------------------\n  Literally!  http://www.monmouth.com/~wstreett/FaceIT/\n\n\n\n", "id": "lists-012-11757831"}, {"subject": "RE: Proxies and broken HTT", "content": "Scott Lawrence wrote,\n> I think that the only sensible thing to do is to\n> figure each case out as they arise; you've arrived at \n> a good solution for this one, and it sounds like you \n> have it right - the server side is stricter than the\n> client side.\n\nAgreed, but the problem here is that _my_ decision on\nwhat the Right Thing to do isn't necessarily going to\nbe the same as everyone else's ... who knows, I might\neven get it wrong ;-) Maybe it's not a big deal, but \nthis does seem to open up a hole for interoperability \nproblems to sneak through.\n\nGranted this is probably moot (interoperability problems\nof this sort pale into insignificance when measured\nagainst all the broken UAs and servers out there), but\nit'd be nice if there was an annex to RFC2616 which \ntaped it down.\n\n> I assume that you've not kept silent to the (nameless) \n> vendor for this obvious bug.\n\nIndeed ... apparently fixed now.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                          Cromwell Media\nInternet Systems Architect           5/6 Glenthorne Mews\n+44 (0)181 410 2230                  London, W6 0LJ\nmsabin@cromwellmedia.co.uk           England\n\n\n\n", "id": "lists-012-11766787"}, {"subject": "Re: Host header issu", "content": "> From: Wilbur Streett <WStreett@mail.Monmouth.com>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Fri, 10 Sep 1999 08:58:00 -0400\n> To: John Stracke <francis@ecal.com>\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: Host header issue\n> -----\n> At 01:57 PM 9/9/99 -0400, you wrote:\n> >> People?  I thought that this was technology?\n> >\n> >So how do you build technology? At eCal, we get people to do it.  :-)\n> >\n> >Seriously, though.  Engineering is more than coming up with the best\n> >technology; it's also coming up with something that can be built by real\n> people\n> >with real, human failings.  Protocols that are too complex for the people who\n> >need to implement them will lose out to simpler protocols, if only because\n> the\n> >implementations of the complex protocols will be buggy and late.\n> \n> And designing protocols in order to work around implementation failures\n> means that the unnecessary complexity is added to the protocol, which then\n> leads to more implementation issues, and then more complexity is added in\n> the protocol to resolve implementation issues, ...  and it spirals out of\n> control.\n> \n> The reality is that elegance of design, (read, KISS), makes for a\n> technology that can be implemented, supported, and useful.  HTTP wasn't\n> designed by Commitee, and was simple enough that everyone could implement\n> it.  Hence, even with it's design flaws, it became popular.\n> \n> The reality of technology acceptance curves being what they are, I don't\n> see much more progress on HTTP with the sort of design process and\n> decisions that occured with the Host Header.\n> \n\nThe Host header is a band-aid to a badly botched design (the lack of\nhost information in the basic request is the botch): it is NOT a fix\nto botched implementations.  And we had to fix it without breaking\nexisting deployed software.\n\nThe consequences of this particular botch in HTTP/1.0 design have SERIOUS \noperational problems, only really visible at scale (and not particularly \nto those who worked on HTTP/1.1, but those people who deploy HTTP in volume \non the Internet).\n\nSomehow, one requirement laid on the HTTP working group from outside\nto make sure this botch would reliably be fixed does not seem\nexcessive to me.  It is, to my memory, about the only externally\nimposed requirement placed on the working group.  I don't remember\nmuch else externally required of us.  The complexity comes from\nhaving to retrofit a fix to a broken design.\n\nSo please go review the history of this, before making opinionated\npronoucement on the people wo worked on fixing this problem.\n- Jim\n\n\n\n", "id": "lists-012-11775432"}, {"subject": "Re: Host header issu", "content": "At 06:07 AM 9/10/99 -0700, you wrote:\n>\n>> From: Wilbur Streett <WStreett@mail.Monmouth.com>\n>> Resent-From: http-wg@hplb.hpl.hp.com\n>> Date: Fri, 10 Sep 1999 08:58:00 -0400\n>> To: John Stracke <francis@ecal.com>\n>> Cc: http-wg@hplb.hpl.hp.com\n>> Subject: Re: Host header issue\n>> -----\n>> At 01:57 PM 9/9/99 -0400, you wrote:\n>> >> People?  I thought that this was technology?\n>> >\n>> >So how do you build technology? At eCal, we get people to do it.  :-)\n>> >\n>> >Seriously, though.  Engineering is more than coming up with the best\n>> >technology; it's also coming up with something that can be built by real\n>> people\n>> >with real, human failings.  Protocols that are too complex for the\npeople who\n>> >need to implement them will lose out to simpler protocols, if only because\n>> the\n>> >implementations of the complex protocols will be buggy and late.\n>> \n>> And designing protocols in order to work around implementation failures\n>> means that the unnecessary complexity is added to the protocol, which then\n>> leads to more implementation issues, and then more complexity is added in\n>> the protocol to resolve implementation issues, ...  and it spirals out of\n>> control.\n>> \n>> The reality is that elegance of design, (read, KISS), makes for a\n>> technology that can be implemented, supported, and useful.  HTTP wasn't\n>> designed by Commitee, and was simple enough that everyone could implement\n>> it.  Hence, even with it's design flaws, it became popular.\n>> \n>> The reality of technology acceptance curves being what they are, I don't\n>> see much more progress on HTTP with the sort of design process and\n>> decisions that occured with the Host Header.\n>> \n>\n>The Host header is a band-aid to a badly botched design (the lack of\n>host information in the basic request is the botch): it is NOT a fix\n>to botched implementations.  And we had to fix it without breaking\n>existing deployed software.\n>\n>The consequences of this particular botch in HTTP/1.0 design have SERIOUS \n>operational problems, only really visible at scale (and not particularly \n>to those who worked on HTTP/1.1, but those people who deploy HTTP in volume \n>on the Internet).\n\nSo you admit that it was an operational implementation problem that caused\nthis design \"fix\".  Somehow I think that this operational problem could\nhave been resolved without adding a requirement of \"Host Header\" in\nconjunction with the already defined Host information.  Breaking down a\nrequest to map to a pool of servers isn't something that should have\naffected the entire design of HTTP.\n\n>Somehow, one requirement laid on the HTTP working group from outside\n>to make sure this botch would reliably be fixed does not seem\n>excessive to me.  It is, to my memory, about the only externally\n>imposed requirement placed on the working group.  I don't remember\n>much else externally required of us.  The complexity comes from\n>having to retrofit a fix to a broken design.\n\nSo the design change was imposed by corporate interests that thought that\nthe proper fix was to break the existing design concepts for their own\nreasons?\n\n>So please go review the history of this, before making opinionated\n>pronoucement on the people wo worked on fixing this problem.\n\nMy statements were not an \"opinionated pronouncement\" on the people who\nworked on fixing the problem.  I don't know the group of people you seek to\ndefend.  The protocol specification was not the appropriate place to solve\nthe server farm issues, particularly when it forces new issues into\neveryone else's work.  Just because someone wants to run a server farm\ndoesn't mean that the basic protocol was \"flawed\".  The routing of HTTP\nrequests to multiple servers is not something that should have forced a\ndesign paridign shift in the basic \"peer to peer\" design intrinsic to HTTP.\n\nExpressing the design realities of \"design by committee\" is not an\nopinionated pronouncement.  Perhaps if you have less political motivations,\nand more basic design motivations, you'd see that I'm only expressing a\nsimple truth.  One that is certainly not limited to any particular group.\n\nWilbur\n\n     --------------------------------------------\n        Putting A Human Face On Technology ;-)\n     --------------------------------------------\n  Literally!  http://www.monmouth.com/~wstreett/FaceIT/\n\n\n\n", "id": "lists-012-11786396"}, {"subject": "RE: Host header issu", "content": "Wilbut Streett wrote,\n> Just because someone wants to run a server farm\n> doesn't mean that the basic protocol was \"flawed\".  \n\nFlawed? Perhaps not ... maybe just \"inadequate as things\nturned out in practice\". That seems like a perfectly\ngood justification for a (comparitively minor) design\ntweak.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                          Cromwell Media\nInternet Systems Architect           5/6 Glenthorne Mews\n+44 (0)181 410 2230                  London, W6 0LJ\nmsabin@cromwellmedia.co.uk           England\n\n\n\n", "id": "lists-012-11799198"}, {"subject": "mistaken reference to &quot;unsafe&quot; in RFC 261", "content": "Actually, RFC 2616 should have said simply\n\n    Characters other than those in the \"reserved\" set (see RFC 2396 [42])\n    are equivalent to their \"\"%\" HEX HEX\" encoding.\n\nThe reason being that excluded characters will not appear in a valid URI,\nand thus it doesn't matter if encoded excluded characters might\nmistakenly be compared equal to their non-encoded character.\n\nThanks for the report,\n\n....Roy\n\nIn message <C0E81C20AD21D311BDB200805FCCD9411C1B39@aunt9.ausys.se>,\nAnders Edenbrandt writes:\n>\n>Hello,\n>\n>In the latest HTTP standard (RFC 2616), in section 3.2.3 \"URI Comparison\",\n>it says:\n>Characters other than those in the \"reserved\" and \"unsafe\" sets (see\n>   RFC 2396 [42]) are equivalent to their \"\"%\" HEX HEX\" encoding.\n>\n>However, RFC 2396 has no definition of a character set called \"unsafe\".\n>The former HTTP standard (RFC 2068) makes this definition:\n>\n>unsafe         = CTL | SP | <\"> | \"#\" | \"%\" | \"<\" | \">\"\n>\n>Is this the character set you mean? Or is it all the characters mentioned\n>in section 2.4.3 \"Excluded US-ASCII Characters\" in RFC 2396? That is,\n>in addition to the characters above also the following:\n>\n>unwise      = \"{\" | \"}\" | \"|\" | \"\\\" | \"^\" | \"[\" | \"]\" | \"`\"\n>\n>\n>I would be grateful for a clarification on this point.\n>\n>Yours sincerely,\n>\n>Anders Edenbrandt, Ph.D.\n>\n>***\n>Anders Edenbrandt\n>AU-System Radio AB\n>SE-223 70  Lund, SWEDEN\n>Phone: +46-46-286 32 36, Fax: +46-46-286 56 20\n>Mobile: +46- 705-37 32 36\n>mailto:Anders.Edenbrandt@radio.ausys.se , http://www.ausys.se\n\n\n\n", "id": "lists-012-11807486"}, {"subject": "Re: HTTP/0.", "content": "    Is there a document that describes HTTP/0.9?\n\n    What I am particularly interested in is the format of the request\n    line (there were no headers, right?).\n\nRFC1945 documents HTTP/0.9.  The format of the request is (was):\n\n       Simple-Request  = \"GET\" SP Request-URI CRLF\n\nand the response format was:\n\n       Simple-Response = [ Entity-Body ]\n\nwith no version numbers, no response status line, and no request or\nresponse headers.  \n\nBased on my reading of some server source code, I believe that instead\nof a Content-Type response header, the older browsers apparently\nunderstood a tag like <PLAINTEXT> to indicate non-HTML responses.\n(Someone will no doubt correct me if I'm wrong.)\n\n-Jeff\n\n\n\n", "id": "lists-012-1180977"}, {"subject": "RE: mistaken reference to &quot;unsafe&quot; in RFC 261", "content": "I think this belongs in the errata\n (http://purl.org/NET/http-errata).\n\n\n> -----Original Message-----\n> From: Roy T. Fielding [mailto:fielding@kiwi.ICS.UCI.EDU]\n> Sent: Friday, September 10, 1999 7:37 PM\n> To: Anders Edenbrandt\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: mistaken reference to \"unsafe\" in RFC 2616\n> \n> \n> Actually, RFC 2616 should have said simply\n> \n>     Characters other than those in the \"reserved\" set (see RFC 2396 [42])\n>     are equivalent to their \"\"%\" HEX HEX\" encoding.\n> \n> The reason being that excluded characters will not appear in a valid URI,\n> and thus it doesn't matter if encoded excluded characters might\n> mistakenly be compared equal to their non-encoded character.\n> \n> Thanks for the report,\n> \n> ....Roy\n> \n> In message <C0E81C20AD21D311BDB200805FCCD9411C1B39@aunt9.ausys.se>,\n> Anders Edenbrandt writes:\n> >\n> >Hello,\n> >\n> >In the latest HTTP standard (RFC 2616), in section 3.2.3 \"URI Comparison\",\n> >it says:\n> >Characters other than those in the \"reserved\" and \"unsafe\" sets (see\n> >   RFC 2396 [42]) are equivalent to their \"\"%\" HEX HEX\" encoding.\n> >\n> >However, RFC 2396 has no definition of a character set called \"unsafe\".\n> >The former HTTP standard (RFC 2068) makes this definition:\n> >\n> >unsafe         = CTL | SP | <\"> | \"#\" | \"%\" | \"<\" | \">\"\n> >\n> >Is this the character set you mean? Or is it all the characters mentioned\n> >in section 2.4.3 \"Excluded US-ASCII Characters\" in RFC 2396? That is,\n> >in addition to the characters above also the following:\n> >\n> >unwise      = \"{\" | \"}\" | \"|\" | \"\\\" | \"^\" | \"[\" | \"]\" | \"`\"\n> >\n> >\n> >I would be grateful for a clarification on this point.\n> >\n> >Yours sincerely,\n> >\n> >Anders Edenbrandt, Ph.D.\n> >\n> >***\n> >Anders Edenbrandt\n> >AU-System Radio AB\n> >SE-223 70  Lund, SWEDEN\n> >Phone: +46-46-286 32 36, Fax: +46-46-286 56 20\n> >Mobile: +46- 705-37 32 36\n> >mailto:Anders.Edenbrandt@radio.ausys.se , http://www.ausys.se\n> \n> \n\n\n\n", "id": "lists-012-11817266"}, {"subject": "CONNECT HEL", "content": "Hi all,\n\nI 'ave been writing a proxy server,this is working fine for both GET\n& POST method,I need to add functionality for HTTPS ,which is been used in\nhotmail mail.\nThe browser request contains CONNECT method,is it a http\nmethod,becoz i 'ave not seen the method in RFC.Is it a https request?\nAny ptrs to resources for https will be helpful.\n\nTIA,\n\nMani.\n\n\n\n", "id": "lists-012-11828188"}, {"subject": "Re: CONNECT HEL", "content": "draft-ietf-tls-http-upgrade-02 contains the first IETF \nstandards-track definition of the CONNECT method for generic TCP \ntunnel establishment. It should be appearing at IETF-wide last call \nany day now.\n\nDon't worry, the definition is functionally identical to Ari \nLuotenen's  original I-D proposals, as already widely implemented. \nThe trick with our new draft, though, is that you CONNECT to port \n*80*, then use Upgrade:, rather than 443...\n\nRohit Khare\n\n\nAt 7:07 AM -0400 8/19/99, Internet-Drafts@ietf.org wrote:\n>\n>A URL for this Internet-Draft is:\n>http://www.ietf.org/internet-drafts/draft-ietf-tls-http-upgrade-02.txt\n\n\n\n", "id": "lists-012-11835485"}, {"subject": "Re: Serverside roles in the HTT", "content": "I was just paging through previous e-mail to prepare draft-01, and came\nacross this, which I don't think I responded to.\n\n[me]\n> >If you have a PDF file that happens to be generated by a CGI script, it's\na huge pain to get it to support ranges, validation and\n> >other facilities in the script itself. I've done it, and it was not\npleasant.\n\n[JP]\n> What you suggest when requesting the server implementor to do this job is\nthat  the server has to regenerate the whole dynamic page just to get a few\nbytes. This\n> may involve rerunning the application. Even if your CGI sends exactly the\nsame-content with the same last-modified date, so that the client still\nthinks it's the\n> same content between HTTP requests, the PDF plug-in might make 10\ndifferent HTTP requests in a row with various ranges. The server executes\nthe script 10 times,\n> gets the full PDF content from the script, and then only sends the\nrequested bytes. Technically, it works, but it's extremely inefficient.\n> On the other hand, if you let the CGI itself handle the range request,\nthen it's not so bad : the CGI will try to generate those requested bytes\nitself and won't\n> waste memory or CPU trying to generate and send the entire thing.\n\nI agree to some extent. I need to make it more clear that the draft is about\ndefaults; in this case, if you have a more efficient way of handling range\nrequests in the CGI, have it generate a 'Accept-Ranges: bytes' header, and\nthe server should know that the application is capable of dealing with\npartial content. Best of both worlds.\n\nEven if this isn't taken advantage of, IMHO it's much easier to scale a\nserver (hardware) than the complete network between any possible user and\nthe server. Regenerating the entire object to send a few bytes is\ninefficient on the server, but it's better than leaving it in the\npublisher's hands; as in most cases it won't be done at all.\n\nI'd very much like a survey to be done of Webmasters, CGI scripters, etc.,\nto ask them where they think the responsibilty for handling these features\ncurrently lies.\n\n\n> Ranges work best with direct access content (eg: static files) or with\ncacheable content ; with dynamic content, the server typically only has\nsequential access to\n> the content and does not cache it.\n\nI know what you're getting at, but I really want to redefine what people\nthink of as dynamic content, as well as cacheability.\n\nDynamic content to me is defined by dependence on either the identity\n(however derived) of the current user, or some other external source of\ncontent entropy that causes two hits to the same request at the same time to\ngenerate different content. That's it; it has nothing to do with the\npresence of the string cgi-bin, a query or anything else.\n\nIn my mind cachability is two very separate properties; assigned TTL\n(through Expires, Cache-Control or assumed through LM) and ability to\nvalidate through conditional request. In the future, it may expand to\ninclude such methods as ability to delta encode.\n\n\n> That's not to say the HTTP server shouldn't do anything. I think it makes\nsense for it to transparently do chunking on CGI and plug-ins output, and\nthe way we are\n> doing it now works very nicely (though I wish we could use our own browser\nto test this feature :)).\n\n*grin* know that feeling...\n\n--\nMark Nottingham\n\n\n\n", "id": "lists-012-11843334"}, {"subject": "Re: Serverside roles in the HTT", "content": "Mark Nottingham:\n>\n>As you may be aware, I have submitted an Internet-Draft,\n>draft-nottingham-http-roles-00.txt.\n>\n>I'd very much appreciate comments from the http-wg, as I see it as primarily\n>in that domain (with some overlap to wrec and the cgi efforts).\n\nI read the draft; I won't give specific comments, just some high level\nobservations.\n\nWith my http-wg background, it looks like the draft mixes two things\nthat I would rather see covered separately.  On the one hand, there\nare a lot of efficiency requirements, which, if not followed, won't\naffect protocol-level correctness as such.  I'll stay silent on these\nas there has been a lot of discussion about them already.\n\nOn the other hand, there are a few things that do have an impact on\nthe ability of a CGI author to produce correct HTTP/1.1 responses.  An\nexample is the requirement in section 6.2 that validators must be\npassed through without modification.\n\nSome time ago I updated the Apache mod_negotiation to HTTP/1.1 and TCN\ncompliance.  While doing this I encountered several cases where the\ncurrent CGI specification does not define server behavior, and where\nhaving a defined behavior would greatly help CGI authors in being more\nHTTP/1.1 friendly/compliant.  I'm saying CGI specification here but\nthis observation really extends to all CGI-like interfaces where\napplication code supplies headers and and a response body to the\nserver.\n\nIt occured to me that the CGI specification would benefit from a few\nHTTP/1.1 targeted extensions and updates.  These extensions have\nlittle to do with performance directly, but more with protocol and\ncache correctness, which of course does affect performance on a higher\nlevel.  Updating the CGI specification for HTTP/1.1 is _way_ down on\nmy todo list however, so I doubt I'll ever get around to doing it.  Of\ncourse, with sufficient encouragement and offers to share the\nworkload, this item may move higher on my todo list.\n\nIn any case, as an example of the kind of things I would want a\n1.1-oriented CGI update to define, here is is the type of facility I\nwould prefer that a server provides with respect to HTTP/1.1 entity\ntags:\n\n  The server should *not* pass from the CGI interface into the\n  HTTP/1.1 response unchanged!  Rather, by default, the CGI script\n  should be able to return a `content validator' to the server, which\n  will be used by the server as _one_ of the components of the\n  HTTP/1.1 entity tag.  Another component is a server-generated\n  validator for the script file itself.  Thus, if the script is\n  updated, correct cache resynchronisation is guaranteed.  If the\n  response body output by the script is not used directly as a HTTP\n  response body, but in stead embedded in other content before being\n  sent to the client, then the entity tag should also act as a\n  validator for the other content.  Naturally, the server, not the\n  script, is responsible by default for correctly processing HTTP/1.1\n  if-none-match and if-match header fields.\n\nNote that Apache, for example, currently does not do much of the above\nat all.  Therefore, as a CGI script writer for Apache I would not\ncurrently create scripts that emit entity tags.  Not emitting these\ntags is the safest route to HTTP/1.1 compliance.\n\nWith respect to draft-nottingham-http-roles-00.txt, I would conclude\nthat it either needs to separate out the CGI extensions that help CGI\nauthors in producing correct HTTP/1.1, or drop such extensions\nalltogether.\n\nKoen.\n\n\n\n", "id": "lists-012-11854084"}, {"subject": "proble", "content": "Hello Friends,\n        As I am starting to code HTTP Server version1.1 .Please can any one help me out for starting implementation (Refernece site if possible).Any RFC dealing with implementation of server\n\nThanks\n\n\n\n", "id": "lists-012-11864704"}, {"subject": "Re: Serverside roles in the HTT", "content": "Hi, Mark!\n\nSorry for the delay in answering - things can be a bit busy at times.\n\nMark Nottingham wrote:\n\n> I agree to some extent. I need to make it more clear that the draft is about\n> defaults; in this case, if you have a more efficient way of handling range\n> requests in the CGI, have it generate a 'Accept-Ranges: bytes' header, and\n> the server should know that the application is capable of dealing with\n> partial content. Best of both worlds.\n\nNoted. This adds some extra complexity again, but it should work.\n\n> Even if this isn't taken advantage of, IMHO it's much easier to scale a\n> server (hardware) than the complete network between any possible user and\n> the server. Regenerating the entire object to send a few bytes is\n> inefficient on the server, but it's better than leaving it in the\n> publisher's hands; as in most cases it won't be done at all.\n\nI'm not sure that it's better to have it done this way as opposed to not having\nit done at all.\nPartial content was designed to provide faster access to the needed data, thus\nimproving performance.\nThese benefits are particularly evident with large objects.\n\nHowever, with a large dynamic object and the approach you suggest, these\nbenefits would be effectively negated since the application would have to run\nmultiple times instead of one. Instead of implementing some relatively complex\ncode to handle it transparently which would have questionable value and\nperformance, I would be much more inclined to have the server add an\n\"accept-ranges: none\" by default for CGIs and other applications so that this\nsituation would not happen.\nOf course, this would be done only if the application doesn't insert its own\nAccept-ranges to indicate that it can handle it.\n\n> I'd very much like a survey to be done of Webmasters, CGI scripters, etc.,\n> to ask them where they think the responsibilty for handling these features\n> currently lies.\n\nI don't think we need to do such a poll to know the results :-).\nEven though it sounds nice to have ranges for CGIs, my take as a server\nimplementor is that having it done transparently in the server is at best not\ngoing to help and in most cases will hurt performance both on the server and\nclient sides.\n\n> > Ranges work best with direct access content (eg: static files) or with\n> cacheable content ; with dynamic content, the server typically only has\n> sequential access to\n> > the content and does not cache it.\n>\n> I know what you're getting at, but I really want to redefine what people\n> think of as dynamic content, as well as cacheability.\n>\n> Dynamic content to me is defined by dependence on either the identity\n> (however derived) of the current user,\n\nIf it's purely the identity then it is only an authentication scheme.\nIn the case you described previously, it sounds like your CGI script was not\nreally generating content but only providing authentication.\n\nTo handle such cases, there are much better ways :\n- use LDAP ACLs if your users are in an LDAP server . That's the easiest way -\nno programming required\n- use a server plug-in to do your own authentication for a resource\n\nIf on the other hand the bits of the PDF were generated on the fly, let's say\nfrom the user input, then the CGI might make sense.\nBut I wonder how the PDF plug-in deals with POST when it makes its partial\ncontent requests .. It would have to repost the whole\nuser input for every range request. You can see how inefficient that is - no\nmatter if it's the server or the CGI script that handles the range.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-11870901"}, {"subject": "HTTP Serve", "content": "Hi,\n    Can any one please help me to listout thee minimal requrements for HTTP Server /1.1 .As I am working on unix system please inform about the implentation related site (which is simple)so that I can go forward.\n\nand\n    How the GET and POST methods are implemented.\nI tried to write one simple server(HTTP/1.0 withGET and HEAD methods),there when no filename is given i gave filename as some default html page.I am using mozilla/4.0 browser.Then the problem is instead of showing the html page .It is giving the window saying to download the file.If anyone knows the reason please write to me\n\nThanks\n\n\n\n", "id": "lists-012-11881901"}, {"subject": "Re: Comments on revision 03 of &quot; Hypertext Transfer Protocol &ndash;&ndash;HTTP/1.1 &quot", "content": "    ) The importance of chunked coding -- add a note to 19.6\n    \n      The default of persistent connections has major  implications for the\n      delivery of dynamic  documents,especially when compared to  http \n       1.0. Although this is discussed in the draft, I  believe that it\nshould be strongly emphasiszd. In  particular, a paragraph should be\nadded to 19.6.1 (or  19.6.2). For example:\n    \n       \"Given that persistent connections are the http/1.1  default, special\n       care must be taken when  dynamically generating output, especially   \n       when  earlier portions of content are sent to the client  as they are\n       generated (say, to prevent automatic or  human time-outs). In this\n       (and other) cases, there  may  be no way of knowing the final length\nof   content, hence a content-length header can not be  added.\nHence, either the connection must be closed after transmittal of\ncontent, or chunked coding  must be used. \n    \nI'm not sure it is necessary to state this in the spec.  Any implementor\nwho does the wrong thing will find out quickly enough.\n\n    II) \"Chunked\" in the TE header -- clarify  description\n    \n      It is somewhat odd that:\ni) Given that (section 3.6.1, and reiterated in \n    step 3 of 14.39)\n    \"All HTTP/1.1 applications MUST be able to   recieve and\n     decode the \"chunked\" transfer    coding...\"\n       ii) Also in (section 3.6.1, and reiterated in 14.39 and\n   14.40)\n   \"A server using chunked transfer-coding in a    response MUST\n    NOT use the trailer for other   header fields than ... unless the\n   \"chunked\"    transfer-coding is present ..in the TE field).\n    \n Since 1.1 apps (such as http/1.1 servers) must  understand     \n \"chunked\", then point ii seems to mean  (informally):\n  \"the use of \"chunked\" in the TE field tells an http/1.1 server that\n  header fields other then Content-MD5   and Authentication-Info\n (and Content-Length)   may be included in the trailer\".\n    \nAssuming I'm not misreading, it might be useful to \ninclude this comment (or an appropriately formal\nversion).\n    \nI think your confusion is the result of the history behind this\ndesign.  Originally (RFC2068) only a few fields could be placed\nin the footer:\n      applications MUST NOT send\n   header fields in the footer which are not explicitly defined as being\n   appropriate for the footer, such as Content-MD5 or future extensions\n   to HTTP for digital signatures or other facilities.\n\nThis means that an RFC2068-compliant client might not be expecting\narbitrary fields in the footer.  We had a lengthy discussion on the\nmailing list about the right way to resolve this, and this led to\nthe current design, in which the Trailer header field is used to\nindicate which (other) header fields are included in a trailer.\n       \n    III) Pipelining -- does order of execution matter?\n    \n      Section 8.1.2.2 states:\n\"A server MUST send it's responses to those  requests in the  same\n       order that the request were recieved.\"\n    \n      Does this imply that there should be no parallelism when  processing\n       piplined requests: that request A should  be completely answered\n      before request B is   considered. Or, is parallel resolution of these\n      requests permitted, so long as order of return is   serial (and follows\n       the order of requests).\n      \nYou're right, this paragraph could be ambiguous:\n\n  A client that supports persistent connections MAY \"pipeline\" its\n  requests (i.e., send multiple requests without waiting for each\n  response). A server MUST send its responses to those requests in the\n  same order that the requests were received.\n\nIt should say something like:\n\n  A client that supports persistent connections MAY \"pipeline\" its\n  requests (i.e., send multiple requests without waiting for each\n  response). A server MUST send its responses to a series of requests\n  on a single transport connection in the same order that the requests\n  were received.\n\nWe never intended to impose an ordering between requests on different\nconnections.\n    \n    IV) Closing connections \n    \nSection 8.1.4 states:\n    \ni) \"When a client or server wishes to time-out, it   SHOULD issue a\n   graceful close on the transport   connection\".\n    \nDoes this imply some sort of action at the http level? \nThat is, should a 4xx (or 5xx) response be sent? \n\nNo, this is about the transport connection.  Since some people believe\nthat other transport protocols besides TCP might be used, there was\nsome pressure to avoid specific discussions about TCP here.\n\nThere's another Internet-Draft (draft-ietf-http-connection-00.txt)\nthat covers this in more detail.  Perhaps this should be revived\nand moved forward as an IETF Informational document?\n\nAnyway, the implication of the statement you quoted is basically\n\"close the TCP connection, don't RESET it.\"\n    \nii) \"Servers SHOULD NOT close a connection in the   middle of\n   transmitting a  response,  unless a network or client failure is \n   suspected\".\n    \nDoes that disallow a \"total time per connection\" server setting? \nEven if an otherwise legitimate request is taking hours to resolve? \n    \nThe \"total time per connection\" setting is allowed, because this\nis normally used to detect \"network or client failure\".  It would\nbe foolish to implement such a timer that isn't restarted on the\nreception of each new request (on a persistent connection).\n    \n      *In 13.1.1, point 2, it is written:\n   \"In the default case, this means it meets the least restrictive\n    freshness requirement...\"\n       Shouldn't that be \"most restrictive\".\n    \nNope.  This was also the subject of intense debate, several years\nago.  The basic philosophy is that a client is allowed to loosen\nthe requirement imposed by the server, or vice versa.  Note that\nthe sentence you quoted from finishes by saying \"if the origin\nserver so specifies, it is the freshness requirement of the origin\nserver alone.\"  I.e., the origin server CAN prevent the client\nfrom loosening the freshness requirement.\n\n-Jeff\n\n   \n\n\n\n", "id": "lists-012-1188598"}, {"subject": "Re: Serverside roles in the HTT", "content": "Quoting Julien Pierre <jpierre@netscape.com>:\n\n> However, with a large dynamic object and the approach you suggest, these\n> benefits would be effectively negated since the application would have to\n> run multiple times instead of one. \n\nThis would only be true for PDFs and other non-sequential access methods, not file download resumption, etc. \n\n> Instead of implementing some relatively complex\n> code to handle it transparently which would have questionable value and\n> performance, I would be much more inclined to have the server add an\n> \"accept-ranges: none\" by default for CGIs and other applications so that\n> this situation would not happen.\n> Of course, this would be done only if the application doesn't insert its\n> own Accept-ranges to indicate that it can handle it.\n\nWell put. I'm not particularly interested in fighting for partial content; it has limited uses and is a pain to implement in caches (which is the prime area of \ninterest for me). How about something along the lines of servers MAY handle ranges transparently, but if they don't, they MUST send Accept-Ranges: \nnone, unless the content generator has set an Accept-Ranges?\n\nThe other way to approach it would be to default to no ranges, allow the generator to communicate that it can handle them, and allow the generator to \ndictate to the server that it should handle them (by regenerating content, if they server implementor has chosen to do this). Hmm, his seems a bit \nunwieldy...\n\n\n> If it's purely the identity then it is only an authentication scheme.\n> In the case you described previously, it sounds like your CGI script was\n> not really generating content but only providing authentication.\n> \n> To handle such cases, there are much better ways :\n> - use LDAP ACLs if your users are in an LDAP server . That's the easiest\n> way - no programming required\n> - use a server plug-in to do your own authentication for a resource\n\nIn this particular case, I was charged with making a CGI application's output (PDFs) cacheable, without changing it; there was a lot\nof logic in determining the authentication criteria, and the implementors did it in CGI, because it seemed natural for them to do so. There's a fair amount \nof this sort of thing going on; people don't really think about the protocol-level implications of what they do.\n\nI'm on holiday right now (in an internet cafe in London), so I won't be reading my mail too often. However, I do plan to submit a revised draft incorporating \na lot of what's been discussed when I get back, and hopefully this can be discussed in Washington.\n\n\n--\nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-11888537"}, {"subject": "Re: Last Call: Upgrading to TLS Within HTTP/1.1 to  Proposed Standar", "content": "> >The IESG has received a request from the Transport Layer Security\n> >Working Group to consider Upgrading to TLS Within HTTP/1.1\n> ><draft-ietf-tls-http-upgrade-02.txt> as a Proposed Standard.\n\n> >To: iesg@ietf.org, IETF-Announce:;\n> >From: Harald Tveit Alvestrand <Harald@Alvestrand.no>\n> >Subject: Re: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed\n> >   Standard\n[...]\n>IANA considerations section for upgrade tokens is not thought through.\n>At the least, the registrant should be allowed to change the contact\ndetails\n>for a registration, so the statement\n>\n>  > 1. The registration for a given token MUST NOT be changed once\nregistered.\n>\n>is obviously not what's desired.\n>\n>I'd suggest the following rules:\n>\n>1. A token, once registered, stays registered forever.\n>2. The registration MUST name a responsible party for the registration.\n>3. The registration MUST name a point of contact.\n>4. The registration MAY name the documentation required for the token.\n>5. The responsible party MAY change the registration at any time. The\n>     IANA will keep a record of all such changes, and make them\navailable\n>     upon request.\n>6. The responsible party for the first registration of a \"product\"\ntoken\n>     MUST approve later registrations of a \"version\" token together\nwith that\n>     \"product\" token before they can be registered.\n>7. If absolutely required, the IESG MAY reassign the responsibility for\n>     a token. This will normally only be used in the case when a\nresponsible\n>     party cannot be contacted.\n>\n>A lot more words, but I think it's more workable.\n\nAn excellent formulation.  The authors will gratefully accept this as a\nfriendly amendment if the IESG concurs.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-11898559"}, {"subject": "gif fil", "content": "Hello,\n        I am writing a http server.To send an image file to client,I'm giving content-type as image/gif.But the file is not viewed properly.It is showing only the button over there.Whether we need some encoding over there\n\nThanks\n  \n\n\n\n", "id": "lists-012-11910597"}, {"subject": "http", "content": "Hi all,\n  \n           I am trying to support https in our proxy server,\n\nIf i directly connect to the web server from  the browser (https://)\nthen, the page is getting displayed properly.\n\nBut when i root the call throu' the proxy server, the browser is\nreceing the following \n\n<html><head><title>Error</title></head><body>The network request is\nnot supported. </body></html>\n\nI coudl see the request sent to the server as\n\nCONNECT tstsrv1:443 HTTP/1.0\nUser-Agent: Mozilla/4.0 (compatible; MSIE 5.0; Windows NT; DigExt)\nHost: tstsrv1\nContent-Length: 0\nProxy-Connection: Keep-Alive\nPragma: no-cache\n\n\nCan Any one help me\n\n\nWith Regards,\nParamasivam Gowri Sankar\n***********************************************************\nP-Mail :Cognizant Tech. Sols.,Gr. Floor,Deepak Complex,\nOp. Pune Golf Course,Airport Road,Yerwada,\nPune-411006  V-Mail(020)6691960/61( ext - 2233)\nE-Mail :: pgowri@pun.cts-corp.com, gowri@email.com\n***********************************************************\n\n\n\n", "id": "lists-012-11916868"}, {"subject": "RE: http", "content": "The original draft describing the CONNECT method has expired.  A new\ndraft is available (and in IETF Last Call - we are attempting to get it\nto Proposed Standard status):\n\n  http://www.ietf.org/internet-drafts/draft-ietf-tls-http-upgrade-02.txt\n\nThis describes a new proposal on upgrading existing connections to use\nTLS, but also documents the existing CONNECT method.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-11925161"}, {"subject": "A proposed method for tracing broken link", "content": "Hi all,\n\nmy name is Matteo Pampolini, I'm an italian communications researcher\nengineer and I'm involved in Internet applications and services development.\n\nCurrently I'm working on a method for tracing broken links inside an HTML\npage: is there anyone working on the same topic? I searched the W3C site\nbut I found nothing about it.\n\nI have a quite simple proposal on how to perform such a task, should I write\na detailed document and submit it to this list?\n\nSorry for this last question but I'm new to this mailing list, so I would like\nto know exactly how to behave.\n\nThanks a lot,\n\n--\nMatteo Pampolini (matteo.pampolini@italtel.it)\n\nStrategic Planning and Innovation - Advanced Research\n\nItaltel S.p.A. 20019 Settimo Milanese (Mi) Italy\n\nPhone  +39.02.4388.9375  Fax  +39.02.4388.9120\n\n\n\n", "id": "lists-012-11932799"}, {"subject": "GI", "content": "Hello Friends,\n            Please inform me how to download an image file.\n            Since I'm writing HTTP server ,It is giving problems while downloading image file.I'm using Internet Explorer as my web browser.\n\nBye\n\n\n\n", "id": "lists-012-11940833"}, {"subject": "Re: Host header issu", "content": "In the recent discussion on this topic I haven't seen a query on the\ntext in section 14.23 \"Host\"  of RFC 2616 which says :\n\n  \"If the requested URI does not include an Internet host\n   name for the service being requested, then the Host header field MUST\n\n   be given with an empty value. \"\n\nIt is the \"with an empty value\" that confuses me - this seems to\ncontradict what is written in section 5.1.2:\n  \"The most common form of Request-URI is that used to identify a\n   resource on an origin server or gateway. In this case the absolute\n   path of the URI MUST be transmitted (see section 3.2.1, abs_path) as\n   the Request-URI, and the network location of the URI (authority) MUST\n\n   be transmitted in a Host header field. \"\n[...]\n       GET /pub/WWW/TheProject.html HTTP/1.1\n       Host: www.w3.org\n\nIf the text in 14.23 were followed you'd get\n\n       GET /pub/WWW/TheProject.html HTTP/1.1\n       Host:\n\nwhich would surely be wrong. My understanding from the spec and this\ndiscussion thread is that it should be possible to identify the host,\nwhether by a relative URI plus valid Host value or by an absolute URI\n(plus redundant Host header, which I suppose you could legitimately\nallow to have an empty value in this case?)\n\nIs this one for the errata?\n\nGeoff Macartney\n\n\n\n", "id": "lists-012-11947010"}, {"subject": "Re: Host header issu", "content": "In message <37F363CD.87C3D72@apion-tss.com>, Geoff Macartney writes:\n>In the recent discussion on this topic I haven't seen a query on the\n>text in section 14.23 \"Host\"  of RFC 2616 which says :\n>\n>  \"If the requested URI does not include an Internet host\n>   name for the service being requested, then the Host header field MUST\n>   be given with an empty value. \"\n>\n>It is the \"with an empty value\" that confuses me - this seems to\n>contradict what is written in section 5.1.2:\n>  \"The most common form of Request-URI is that used to identify a\n>   resource on an origin server or gateway. In this case the absolute\n>   path of the URI MUST be transmitted (see section 3.2.1, abs_path) as\n>   the Request-URI, and the network location of the URI (authority) MUST\n>   be transmitted in a Host header field. \"\n\nSome URI do not have an authority component, and therefore have no host,\nbut can still be requested from an HTTP proxy.  This is not a contradiction.\n\n....Roy\n\n\n\n", "id": "lists-012-11955250"}, {"subject": "RE: A proposed method for tracing broken link", "content": "--------------on Fri, 24 Sep 1999 15:53:53 +0200   Matteo Pampolini (matteo.pampolini@italtel.it) wrote:\n\nHi all,\n\nmy name is Matteo Pampolini, I'm an italian communications researcher\nengineer and I'm involved in Internet applications and services development.\n\nCurrently I'm working on a method for tracing broken links inside an HTML\npage: is there anyone working on the same topic? I searched the W3C site\nbut I found nothing about it.\n\nI have a quite simple proposal on how to perform such a task, should I write\na detailed document and submit it to this list?\n\nSorry for this last question but I'm new to this mailing list, so I would like\nto know exactly how to behave.\n\nThanks a lot,\n\n\n>Now\n>\n\nI chandra am a graduating student and working on the same type of project ( tracing broken links ). Infact i am new to the project. i would like to hear your proposal \non the aforementioned topic\nthank you\nchandra.\n\n------\nContent-Type: text/html;\ncharset\nContent-Transfer-Encoding: quoted-printable\n\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n<HTML><HEAD>\n<META content=\"text/html; charset=iso-8859-1\" http-equivntent-Type>\n<META content=\"MSHTML 5.00.2314.1000\" name=GENERATOR>\n<STYLE></STYLE>\n</HEAD>\n<BODY bgColor=#ffffff>\n<DIV><FONT face\nial>--------------on <I>Fri, 24 Sep 1999 15:53:53 \n+0200</I>&nbsp;</FONT><FONT face\nial> &nbsp;Matteo Pampolini \n(<I>matteo.pampolini@italtel.it</I>) wrote:<BR></FONT></DIV>\n<DIV><FONT face\nial>Hi all,<BR><BR>my name is Matteo Pampolini, I'm an italian \ncommunications researcher<BR>engineer and I'm involved in Internet applications \nand services development.<BR><BR>Currently I'm working on a method for tracing \nbroken links inside an HTML<BR>page: is there anyone working on the same topic? \nI searched the W3C site<BR>but I found nothing about it.<BR><BR>I have a quite \nsimple proposal on how to perform such a task, should I write<BR>a detailed \ndocument and submit it to this list?<BR><BR>Sorry for this last question but I'm \nnew to this mailing list, so I would like<BR>to know exactly how to \nbehave.<BR><BR>Thanks a lot,</FONT></DIV>\n<DIV><FONT face\nial></FONT>&nbsp;</DIV>\n<DIV><FONT face\nial></FONT>&nbsp;</DIV>\n<DIV><FONT face\nial>&gt;Now</FONT></DIV>\n<DIV><FONT face\nial>&gt;</FONT></DIV>\n<DIV>&nbsp;</DIV>\n<DIV><FONT face\nial>I chandra am a graduating student&nbsp;and working on the \nsame type of project ( tracing broken links ). Infact i am new to the project. i \nwould like to hear your proposal </FONT></DIV>\n<DIV><FONT face\nial>on the aforementioned topic</FONT></DIV>\n<DIV><FONT face\nial>thank you</FONT></DIV>\n<DIV><FONT face\nial>chandra.</FONT></DIV></BODY></HTML>\n\n------\n\n\n\n", "id": "lists-012-11963607"}, {"subject": "WG meeting in Washington", "content": "Are there plans for http-wg to meet at the next IETF? If so, I'd like to request that discussion about draft-nottingham-http-roles-00.txt be considered (I plan \nto have 01 available before the draft deadline). I'm of the opinion that it's most appropriate here, but I'm open to discussing alternate forums.\n\nThanks,\n\n--\nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-11973722"}, {"subject": "Re: WG meeting in Washington", "content": "In message <19991001164327.D51EFC4F0@mail.mnot.net>, Mark Nottingham writes:\n>Are there plans for http-wg to meet at the next IETF? If so, I'd like to\n>request that discussion about draft-nottingham-http-roles-00.txt be\n>considered (I plan \n>to have 01 available before the draft deadline). I'm of the opinion that it's \n>most appropriate here, but I'm open to discussing alternate forums.\n\nThe HTTP working group is expected to close as soon as the last of the\nexisting drafts are moved to RFC.\n\nWhile there is value in documenting some implementation concerns for HTTP,\nyour draft is not appropriate for the IETF standards track.  The reason is\nbecause IETF standards specify the protocol, not the means by which servers\nare implemented to conform to that protocol.  Phrasing a bunch of\nimplementation concerns as if they were protocol requirements is not\nappropriate, however well intentioned and useful the document may be.\n\n....Roy\n\n\n\n", "id": "lists-012-11980750"}, {"subject": "Re: WG meeting in Washington", "content": "> While there is value in documenting some implementation concerns for HTTP,\n> your draft is not appropriate for the IETF standards track.  The reason is\n> because IETF standards specify the protocol, not the means by which servers\n> are implemented to conform to that protocol.  Phrasing a bunch of\n> implementation concerns as if they were protocol requirements is not\n> appropriate, however well intentioned and useful the document may be.\n\nOK. I was thinking that it may have qualified as an Applicability Statement, as in section 3.2 of RFC 2026. Looking back, I see how that's probably taking \ntoo much license with what's there, at least for http-wg.\n\nIn any case, I'm happy if people just start thinking and discussing this stuff, irregardless of the status of the doc. If there is another more appropriate \nplace that this sort of thing could live, please tell me.\n\nThanks,\n\n--\nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-11989338"}, {"subject": "RE: WG meeting in Washington", "content": "I think that the HTTP working group mailing list\nis an appropriate forum for discussion of\nMark Nottingham's document \"Server-Side Roles in the HTTP\".\n\nEven after the working group officially closes, the mailing\nlist remains open for discussion of evolution of the\nprotocol, errata, and progression to 'Standard' status.\n\nPlease note that 'http-wg@hplb.hpl.hp.com' is preferable to\n'http-wg@cuckoo.hpl.hp.com'.\n\nI've seen, for other protocols, an \"Implementation Guide\"\nreleased as an Informational document. \n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-11997568"}, {"subject": "RE: Host header issu", "content": "> In the recent discussion on this topic I haven't seen a query on the\n> text in section 14.23 \"Host\"  of RFC 2616 which says :\n> \n>   \"If the requested URI does not include an Internet host\n>    name for the service being requested, then the Host header field MUST\n>    be given with an empty value. \"\n>\n> It is the \"with an empty value\" that confuses me - this seems to\n> contradict what is written in section 5.1.2:\n>   \"The most common form of Request-URI is that used to identify a\n>    resource on an origin server or gateway. In this case the absolute\n>    path of the URI MUST be transmitted (see section 3.2.1, abs_path) as\n>    the Request-URI, and the network location of the URI (authority) MUST\n> \n>    be transmitted in a Host header field. \"\n> [...]\n>        GET /pub/WWW/TheProject.html HTTP/1.1\n>        Host: www.w3.org\n> \n> If the text in 14.23 were followed you'd get\n> \n>        GET /pub/WWW/TheProject.html HTTP/1.1\n>        Host:\n> \n> which would surely be wrong. My understanding from the spec and this\n> discussion thread is that it should be possible to identify the host,\n> whether by a relative URI plus valid Host value or by an absolute URI\n> (plus redundant Host header, which I suppose you could legitimately\n> allow to have an empty value in this case?)\n> \n> Is this one for the errata?\n\nWell, it probably deserves a clarification. What I vaguely recall\nis that we were trying to leave room for non-HTTP based proxying,\ne.g., where you asked your proxy\n\n       GET news:comp.infosystems.www HTTP/1.1\n       Host:\n\nClearly not the interpretation you read into it. Does anyone else\nrecall what we really meant?\n\n\n\n", "id": "lists-012-12006590"}, {"subject": "Re: WG meeting in Washington", "content": ">I think that the HTTP working group mailing list\n>is an appropriate forum for discussion of\n>Mark Nottingham's document \"Server-Side Roles in the HTTP\".\n\nOnly if it isn't talking about issues of compliance and using the\nkey words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\",  \"MAY\", and \"OPTIONAL\" in\nas described in RFC-2119.  That is not what an implementation guide\ndoes, and I doubt that the IESG would want that in an Informational\ndocument.\n\nI have no objection to discussions on how to best implement HTTP,\nor further rationale to back up the HTTP requirements.  What I object\nto is implementation advice for general-purpose server technology\nthat is made-over to look like protocol requirements.  The W3C does\nthat sort of thing, but the IETF does not (at least not without clearly\ndistinguishing between informational content and protocol requirements).\n\n....Roy\n\n\n\n", "id": "lists-012-12015608"}, {"subject": "RE: Host header issu", "content": "I cant say if this is consensus, but my take was that\nyou only use the host header when you are proxying/requesting\nhttp urls.  This is because the Host: header and value\nonly make sense for HTTP URLS.\n\nThe main point of adding the host header was to address\nthe \"shortcoming\" of http urls that didnt indicate\na host portion.\n\n\n> -----Original Message-----\n> From: Larry Masinter [mailto:masinter@parc.xerox.com]\n> Sent: Monday, October 04, 1999 5:01 PM\n> To: Geoff Macartney\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: RE: Host header issue\n> \n> \n> > In the recent discussion on this topic I haven't seen a query on the\n> > text in section 14.23 \"Host\"  of RFC 2616 which says :\n> > \n> >   \"If the requested URI does not include an Internet host\n> >    name for the service being requested, then the Host \n> header field MUST\n> >    be given with an empty value. \"\n> >\n> > It is the \"with an empty value\" that confuses me - this seems to\n> > contradict what is written in section 5.1.2:\n> >   \"The most common form of Request-URI is that used to identify a\n> >    resource on an origin server or gateway. In this case \n> the absolute\n> >    path of the URI MUST be transmitted (see section 3.2.1, \n> abs_path) as\n> >    the Request-URI, and the network location of the URI \n> (authority) MUST\n> > \n> >    be transmitted in a Host header field. \"\n> > [...]\n> >        GET /pub/WWW/TheProject.html HTTP/1.1\n> >        Host: www.w3.org\n> > \n> > If the text in 14.23 were followed you'd get\n> > \n> >        GET /pub/WWW/TheProject.html HTTP/1.1\n> >        Host:\n> > \n> > which would surely be wrong. My understanding from the spec and this\n> > discussion thread is that it should be possible to identify \n> the host,\n> > whether by a relative URI plus valid Host value or by an \n> absolute URI\n> > (plus redundant Host header, which I suppose you could legitimately\n> > allow to have an empty value in this case?)\n> > \n> > Is this one for the errata?\n> \n> Well, it probably deserves a clarification. What I vaguely recall\n> is that we were trying to leave room for non-HTTP based proxying,\n> e.g., where you asked your proxy\n> \n>        GET news:comp.infosystems.www HTTP/1.1\n>        Host:\n> \n> Clearly not the interpretation you read into it. Does anyone else\n> recall what we really meant?\n> \n> \n\n\n\n", "id": "lists-012-12024328"}, {"subject": "Re: HTTP/0.", "content": ">\n>Is there a document that describes HTTP/0.9?\n\nThe 1.0 spec (rfc1945) has some text on HTTP/0.9 in section 4.1.\n\n>What I am particularly interested in is the format of the request line\n>(there were no headers, right?).\n\nRight.\n\n>Thanks,\n>\n>Richard L. Gray\n>will code for chocolate\n\nKoen.\n\n\n\n", "id": "lists-012-1202850"}, {"subject": "A proposal for Host heade", "content": "I would like to propose:\n\n1) If a client is issuing a 1.1 request and the client has obtained\npositive knowledge, possibly through an out of band mechanism, that all\nproxies and the origin server in the request path are 1.1 compliant or \nbetter, that it may omit the host header when absolute URIs are used.\n\n2) Clients are permitted to use absoluteURIs when talking to 1.1 servers.\n\nDoes this seem reasonable ?\n\n\n\n", "id": "lists-012-12034670"}, {"subject": "Proble", "content": "Hello,\n    As i am writing http server.I got some problems while sending animated images with extension gif.\nI am sending image/gif header in the response.\nFor some i can't see the image and for some images the image is not appeared ,just showing button over there.\n\nIf anyone know the reason,please inform me\n\nThanku\nBye\n\n\n\n", "id": "lists-012-12042039"}, {"subject": "Re: A proposal for Host heade", "content": "\"Josh Cohen (Exchange)\" wrote:\n\n> I would like to propose:\n>\n> 1) If a client is issuing a 1.1 request and the client has obtained\n> positive knowledge, possibly through an out of band mechanism, that all\n> proxies and the origin server in the request path are 1.1 compliant or\n> better, that it may omit the host header when absolute URIs are used.\n>\n> 2) Clients are permitted to use absoluteURIs when talking to 1.1 servers.\n>\n> Does this seem reasonable ?\n\nThe problem is that, even if you find out that foo.example.com is a 1.1\nserver, you can't be certain that that will remain true.  There's the usual\nproblem of people upgrading their software, of course, but there's also the\nissue of load balancing.  If not all the machines that handle requests to\nfoo.example.com are running 1.1 servers, then any assumption about whether\nthe one you're talking to is 1.1 or not will break eventually.  Similarly,\nthere might be a heterogeneous proxy farm somewhere along the path.\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |This sentance has threee errors.             |\n|francis@ecal.com|                                             |\n\\==============================================================/\n\n\n\n", "id": "lists-012-12048337"}, {"subject": "HTTP to &quot;Standard&quot;? When", "content": "If they won't let the working group close, maybe we should try to do\nsomething. Actually, I don't think the WG needs to remain officially\nopen for this, but...\n\nUnder RFC 2026:\n\n   A specification for which significant implementation and successful\n   operational experience has been obtained may be elevated to the\n   Internet Standard level.  An Internet Standard (which may simply be\n   referred to as a Standard) is characterized by a high degree of\n   technical maturity and by a generally held belief that the specified\n   protocol or service provides significant benefit to the Internet\n   community.\n\n   A specification that reaches the status of Standard is assigned a\n   number in the STD series while retaining its RFC number.\n\nand\n\n   A specification shall remain at the Draft Standard level for at least\n   four (4) months, or until at least one IETF meeting has occurred,\n   whichever comes later.\n\nRFC 2616 was published June 1999, so it's been 4 months. We only\nhave a few errata. RFC 2616 was pretty stable for over a year.\nThere is significant implementation and successful operational\nexperience for most of HTTP & HTTP-AUTH, although a few corners\nof the specs might need a little more baking.\n\nWe might want to do another round on the implementation\nreports, and try to get some finer granularity of reporting.\nLast time we enumerated every section, but I think we might want\nto look more carefully at every MUST, SHOULD, and MAY to see\nif there's both implementation and successful operational\nexperience.\n\nWhat's a reasonable schedule for advancement to \"Standard\"?\nI'm thinking on the 6-month timeframe. March 2000.\n\nin lunacy,\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-12056974"}, {"subject": "Re: HTTP to &quot;Standard&quot;? When", "content": ">What's a reasonable schedule for advancement to \"Standard\"?\n>I'm thinking on the 6-month timeframe. March 2000.\n\nGeez, Larry, get a hobby.  ;-)\n\nSeriously, the timeframe would be six months after all the dependent\nstandards (MIME, URI, etc.) make it to Standard.  I'm pretty sure\nI'll be finished with my dissertation by then and ready to retire.\n\n....Roy\n\n\n\n", "id": "lists-012-12065795"}, {"subject": "Re: A proposal for Host heade", "content": "In message <BFF90FB6CF66D111BF4F0000F840DB850BCBBB91@LASSIE>, \"Josh Cohen (Exch\nange)\" writes:\n>I would like to propose:\n>\n>1) If a client is issuing a 1.1 request and the client has obtained\n>positive knowledge, possibly through an out of band mechanism, that all\n>proxies and the origin server in the request path are 1.1 compliant or \n>better, that it may omit the host header when absolute URIs are used.\n>\n>2) Clients are permitted to use absoluteURIs when talking to 1.1 servers.\n>\n>Does this seem reasonable ?\n\nOf course not.  Try it with any HTTP/1.1 compliant server now and you\nwill get a 400 response.  Those servers won't disappear just because\nan RFC is modified.\n\nThis part of the standard will never change without a version bump.  Never.\n\n....Roy\n\n\n\n", "id": "lists-012-12073266"}, {"subject": "Re: WG meeting in Washington", "content": "Quoting \"Roy T. Fielding\" <fielding@kiwi.ICS.UCI.EDU>:\n> \n> I have no objection to discussions on how to best implement HTTP,\n> or further rationale to back up the HTTP requirements.  What I object\n> to is implementation advice for general-purpose server technology\n> that is made-over to look like protocol requirements.  The W3C does\n> that sort of thing, but the IETF does not (at least not without clearly\n> distinguishing between informational content and protocol requirements).\n\nRoy,\n\nI didn't intend to 'make over' the advice to look like protocol requirements, but I freely admit that they can be interpreted that way; I \nchose to do it that way because it seemed the most natural and clear way to do so. My logic was that the requirements set forth \nin it were only in the scope of the document, so that a product could be referred to as compliant with it, over and above protocol \ncompliance. This will hopefully be more clear in a future draft of the document (if there is indeed any point in further revision).\n\nI apologize if it's not politic to submit this to the IETF, but it seemed the best place to start (as I don't have the cash to join the \nW3C). I am (or at least I was during the writing of this draft) very much an outsider. If someone has a suggestion about where \nthis document would best reside (whether that place is in the http-wg or not, as there does seem to be some dissention), I'll be \nmore than happy to talk about it (as I've stated before).\n\n--\nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-12081065"}, {"subject": "Re: WG meeting in Washington", "content": ">I didn't intend to 'make over' the advice to look like protocol requirements, \n\nSorry, that wasn't intended as a criticism of you -- just the current\nformat of your draft.  If it were actually a protocol spec, I'd be\ncongratulating you on following the proper form and making it easier on\nthe reader.  What I am worried about is misinterpreting implementation\nguidance as protocol requirements.\n\n>but I freely admit that they can be interpreted that way; I \n>chose to do it that way because it seemed the most natural and clear way to do\n> so. My logic was that the requirements set forth \n>in it were only in the scope of the document, so that a product could be\n>referred to as compliant with it, over and above protocol \n>compliance. This will hopefully be more clear in a future draft of the\n>document (if there is indeed any point in further revision).\n\nI think it is a worthwhile document, provided that it emphasizes rationale\nand not requirements.  For example, there is very good rationale for having\nthe server constrain the i/o interface to conform to the protocol, and there\nare many examples you might use to explain the rationale, but the protocol\nrequirements do not change as a result.  However, phrasing that rationale\nas \"a server MUST do ...\" is stating a protocol requirement even if the\nscope of the document excludes protocol requirements.\n\n>I apologize if it's not politic to submit this to the IETF, but it seemed the \n>best place to start (as I don't have the cash to join the \n>W3C). I am (or at least I was during the writing of this draft) very much an\n>outsider. If someone has a suggestion about where \n>this document would best reside (whether that place is in the http-wg or not, \n>as there does seem to be some dissention), I'll be \n>more than happy to talk about it (as I've stated before).\n\nI'm not a W3C member, so I wouldn't encourage that path either.  What I\nmeant is that W3C does theoretically include implementation conformance \namong its members as part of its spec writing scope.\n\nLarry is the one who decides what is or is not appropriate for this forum,\nand it seems that he wants to discuss the document here regardless of its\neventual publication status.  I agree with his reasons, though again I would\nprefer that the format be changed so that it won't be misinterpreted.\n\n....Roy\n\n\n\n", "id": "lists-012-12090433"}, {"subject": "Re: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed Standar", "content": "I didn't realize you were waiting for me to approve. Please do make \nthe necessary changes and submit a new document.\n\n-Jeff\n\n>>>>>>>>>>>>>>>>>> Original Message <<<<<<<<<<<<<<<<<<\n\nOn 9/22/99, 5:23:55 PM, \"Scott Lawrence\" <lawrence@agranat.com> wrote \nregarding Re: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed \nStandard:\n\n\n> > >The IESG has received a request from the Transport Layer Security\n> > >Working Group to consider Upgrading to TLS Within HTTP/1.1\n> > ><draft-ietf-tls-http-upgrade-02.txt> as a Proposed Standard.\n\n> > >To: iesg@ietf.org, IETF-Announce:;\n> > >From: Harald Tveit Alvestrand <Harald@Alvestrand.no>\n> > >Subject: Re: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed\n> > >   Standard\n> [...]\n> >IANA considerations section for upgrade tokens is not thought \nthrough.\n> >At the least, the registrant should be allowed to change the contact\n> details\n> >for a registration, so the statement\n> >\n> >  > 1. The registration for a given token MUST NOT be changed once\n> registered.\n> >\n> >is obviously not what's desired.\n> >\n> >I'd suggest the following rules:\n> >\n> >1. A token, once registered, stays registered forever.\n> >2. The registration MUST name a responsible party for the \nregistration.\n> >3. The registration MUST name a point of contact.\n> >4. The registration MAY name the documentation required for the \ntoken.\n> >5. The responsible party MAY change the registration at any time. The\n> >     IANA will keep a record of all such changes, and make them\n> available\n> >     upon request.\n> >6. The responsible party for the first registration of a \"product\"\n> token\n> >     MUST approve later registrations of a \"version\" token together\n> with that\n> >     \"product\" token before they can be registered.\n> >7. If absolutely required, the IESG MAY reassign the responsibility \nfor\n> >     a token. This will normally only be used in the case when a\n> responsible\n> >     party cannot be contacted.\n> >\n> >A lot more words, but I think it's more workable.\n\n> An excellent formulation.  The authors will gratefully accept this as \na\n> friendly amendment if the IESG concurs.\n\n> --\n> Scott Lawrence           Director of R & D        \n<lawrence@agranat.com>\n> Agranat Systems, Inc.  Embedded Web Technology   \nhttp://www.agranat.com/\n\n\n> ---\n> You are currently subscribed to ietf-tls as: [jis@mit.edu]\n> To unsubscribe, forward this message to \nleave-ietf-tls-557Y@lists.consensus.com\n\n\n\n", "id": "lists-012-12099814"}, {"subject": "Re: Comments on revision 03 of &quot; Hypertext Transfer Protocol &ndash;&ndash;HTTP/1.1 &quot", "content": "Daniel Hellerstein <danielh@MAILBOX.ECON.AG.GOV> wrote:\n\n\n>As the primary author of a free http/1.0+ web server (for OS/2)\n>with a goal of keeping the product reasonably up to date,\n>I have been sporadically following the http 1.1 drafts, related RFC's, \n>and the HTTP-WG mailing list.  At the recommendation of several\n>working group members, I have read the rev. 03 of the draft; with an\n>eye toward what an implementor might have problems with. From\n>that viewpoint, consider the following comments:\n\n\nThis is  just the kind of careful review the draft needs at this point.\nThanks!\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-1210232"}, {"subject": "Re: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed Standar", "content": "Looks like we have an additional problem. The IANA comments:\n\n>draft-ietf-tls-http-upgrade-02.txt, and has the following comment \nwith\n>regards to the publication of this document.\n>\n>In the \"References\" section, there are three works in progress:\n>\n>[3] \"HTTP over TLS\"\n>[5] \"WebDAV Advanced Collections Protocol\"\n>[8] \"Tunneling TCP based protocols throught Web proxy \n>                     servers\"\n>\n>Current status?  Are any of them normative?\n>\n>\n>Joyce K. Reynolds\n>IANA Liaison to the IESG\n\n[3] definitely appears to be normative. Can we get Eric's Document \nadvanced (This question is to the working group)?\n\n-Jeff\n\n\n\n", "id": "lists-012-12112773"}, {"subject": "Re: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed Standar", "content": "Jeffrey Schiller <jis@mit.edu> writes:\n> [3] definitely appears to be normative. Can we get Eric's Document \n> advanced (This question is to the working group)?\nI've received last call comments from Bodo Moeller with new\nwording for one section. There have been no technical \nchanges.\n\nI've just responded to Bodo's message. Unless there are any\nobjections, I believe we can move the document forward\nwith the indicated trivial changes. (I.e. I'll submit a new\nI-D with them.)\n\n-Ekr\n-- \n[Eric Rescorla                                   ekr@rtfm.com]\n          PureTLS - free SSLv3/TLS software for Java\n                http://www.rtfm.com/puretls/\n\n\n\n", "id": "lists-012-12121985"}, {"subject": "RE: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed Standar", "content": "> From: Jeffrey Schiller [mailto:jis@mit.edu]\n> Sent: Tuesday, October 12, 1999 23:10\n> To: IETF Transport Layer Security WG\n> Cc: Rohit Khare; Http-Wg@Hplb. Hpl. Hp. Com\n> Subject: Re: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed\n> Standard\n>\n>\n> Looks like we have an additional problem. The IANA comments:\n>\n> >draft-ietf-tls-http-upgrade-02.txt, and has the following comment\n> with\n> >regards to the publication of this document.\n> >\n> >In the \"References\" section, there are three works in progress:\n> >\n> >[3] \"HTTP over TLS\"\n> >[5] \"WebDAV Advanced Collections Protocol\"\n> >[8] \"Tunneling TCP based protocols throught Web proxy\n> >                     servers\"\n> >\n> >Current status?  Are any of them normative?\n> >\n> >\n> >Joyce K. Reynolds\n> >IANA Liaison to the IESG\n>\n> [3] definitely appears to be normative. Can we get Eric's Document\n> advanced (This question is to the working group)?\n\n[3] documents existing practice for https\n[5] is noted because it defines an HTTP status code - we could remove it\nfrom this document and let them add an entry to the registry we are\ncreating.\n[8] is an expired draft, referenced only because it was the original\ndescription - this document replaces it as far as standards track is\nconcerned.\n\n\n\n", "id": "lists-012-12131292"}, {"subject": "Re: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed Standar", "content": "Scott Lawrence wrote:\n\n> [5] is noted because it defines an HTTP status code - we could remove it\n> from this document and let them add an entry to the registry we are\n> creating.\n\nI looked at draft-ietf-tls-http-upgrade-02, and spotted one glitch: you\nmention that WebDAV defines status codes 420-424, but you didn't mention\n207.  Not a big deal, maybe, but worth adding if you're updating the doc\nanyway.\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |\"I lost an 7-foot boa constrictor once in our|\n|francis@ecal.com|house.\" --Gary Larson on his youth           |\n\\==============================================================/\n\n\n\n", "id": "lists-012-12142206"}, {"subject": "RE: Last Call: Upgrading to TLS Within HTTP/1.1 to Proposed Standar", "content": "> > [3] definitely appears to be normative. Can we get Eric's Document\n> > advanced (This question is to the working group)?\n\n> [3] documents existing practice for https\n\nHowever this is still a normative reference, meaning that you need to \nhave this document to understand what is being discussed. That is why \nI suggested that Eric's document be reviewed by the WG and submitted \nas a Proposed Standard.\n\n> [5] is noted because it defines an HTTP status code - we could remove \nit\n> from this document and let them add an entry to the registry we are\n> creating.\n> [8] is an expired draft, referenced only because it was the original\n> description - this document replaces it as far as standards track is\n> concerned.\n\nThese are not a problem.\n\n-Jeff\n\n\n\n", "id": "lists-012-12150696"}, {"subject": "simple httpproxy ", "content": "Hi,\n\nI am developing a http proxy server , where I need to just filter the \nrequests based on the URL's accessed and forward the request to the origin \nserver.\n\nWith the kind of limited resources my proxy server would be executing, I \nwant to make it a very simple one.  Do I need to implement the whole http \nprotocol for proxy as per RFC2616 or can I just look at he URL and forward \nthe request as it is to the server?\n\nCould somebody point me to a resources , which might help me in writing a \nsimple http proxy server with URL filtering support??\n\nThanks in advance,\n\nVinit Kumar\n\n______________________________________________________\nGet Your Private, Free Email at http://www.hotmail.com\n\n\n\n", "id": "lists-012-12159855"}, {"subject": "http proxy &amp; tunnel differences ?", "content": "Hi,\n\nRFC 2616 defines a http proxy and a tunnel. But I'am confused on how a http \ntunnel works.\n\nA proxy works by taking a request from a client and connecting to the origin \nserver indicated in the request.  Here the client is configured to go \nthrough the proxy.\n\nHow does a http tunnel work. Is the initial connection similar ?\nDoes a client (browser) need to configured differently when it goes through \na tunnel or is it same as the configuration required when it goes through \nthe proxy ? Are there to separate tcp connections for each request even in a \ntunnel ?\n\nThanks in Advance,\n\nRegards,\n\nVinit Kumar\n\n\n______________________________________________________\nGet Your Private, Free Email at http://www.hotmail.com\n\n\n\n", "id": "lists-012-12166748"}, {"subject": "RE: http proxy &amp; tunnel differences ?", "content": "> From: Vinit Kumar\n> To: http-wg@hplb.hpl.hp.com\n> Subject: http proxy & tunnel differences ??\n\n> A proxy works by taking a request from a client and\n> connecting to the origin\n> server indicated in the request.  Here the client is configured to go\n> through the proxy.\n\nActually, the client may or may not know about the proxy.\n\n> How does a http tunnel work. Is the initial connection similar ?\n> Does a client (browser) need to configured differently when\n> it goes through\n> a tunnel or is it same as the configuration required when it\n> goes through\n> the proxy ? Are there to separate tcp connections for each\n> request even in a\n> tunnel ?\n\nThe difference is in the behavior of the intermediate system (proxy or\ntunnel).  A tunnel just forwards the request and the response\nunmodified.  A proxy at least adds its own identification to a Via\nheader, and may also respond from a cache, require proxy authentication,\nor any number of other proxy-specific functions.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12174617"}, {"subject": "Re: http proxy &amp; tunnel differences ?", "content": ">> A proxy works by taking a request from a client and\n>> connecting to the origin\n>> server indicated in the request.  Here the client is configured to go\n>> through the proxy.\n>\n>Actually, the client may or may not know about the proxy.\n\nThe client always knows about the proxy -- that is what distinguishes\na proxy from a gateway.  A \"reverse proxy\" is a gateway.\n\n>> How does a http tunnel work. Is the initial connection similar ?\n>> Does a client (browser) need to configured differently when\n>> it goes through\n>> a tunnel or is it same as the configuration required when it\n>> goes through\n>> the proxy ? Are there to separate tcp connections for each\n>> request even in a\n>> tunnel ?\n\nSome tunnels are activated by a proxy request, some are simply\nport forwarding TCP firewalls (either on the client side or the\nserver side, or both), and others are gateways to other servers.\nThe important thing from HTTP's perspective is that once an\nintermediary becomes a tunnel, it is no longer conscious of the\nHTTP communication -- only of bytes being relayed from one connection\nto another.\n\n....Roy\n\n\n\n", "id": "lists-012-12184097"}, {"subject": "RE: HTTP/0.", "content": "> -----Original Message-----\n> From: Jeffrey Mogul [mailto:mogul@pa.dec.com]\n> Sent: Tuesday, March 24, 1998 10:39 AM\n> To: rlgray@us.ibm.com\n> Cc: HTTP Working Group\n> Subject: Re: HTTP/0.9 \n> \n> [http/0.9]\n> Based on my reading of some server source code, I believe that instead\n> of a Content-Type response header, the older browsers apparently\n> understood a tag like <PLAINTEXT> to indicate non-HTML responses.\n> (Someone will no doubt correct me if I'm wrong.)\n> \nIs this <pre> </pre>\nor does anyone remember anything different?\n\n(This tweaked my curiosity)\n\n> -Jeff\n> \n\n\n\n", "id": "lists-012-1219010"}, {"subject": "RE: http proxy &amp; tunnel differences ?", "content": "> -----Original Message-----\n> From: Roy T. Fielding [mailto:fielding@kiwi.ICS.UCI.EDU]\n> Sent: Tuesday, October 26, 1999 8:40 AM\n> To: Scott Lawrence\n> Cc: Vinit Kumar; http-wg@hplb.hpl.hp.com\n> Subject: Re: http proxy & tunnel differences ?? \n> >\n> >Actually, the client may or may not know about the proxy.\n> \n> The client always knows about the proxy -- that is what distinguishes\n> a proxy from a gateway.  A \"reverse proxy\" is a gateway.\n>\nActually, for better or worse, the client does not always\nknow about the proxy.  Transparent proxies are quite common.\nAgain, for better or worse, mostly worse IMHO :)\n \n> >> How does a http tunnel work. Is the initial connection similar ?\n> >> Does a client (browser) need to configured differently when\n> >> it goes through\n> >> a tunnel or is it same as the configuration required when it\n> >> goes through\n> >> the proxy ? Are there to separate tcp connections for each\n> >> request even in a\n> >> tunnel ?\n> \n> Some tunnels are activated by a proxy request, some are simply\n> port forwarding TCP firewalls (either on the client side or the\n> server side, or both), and others are gateways to other servers.\n> The important thing from HTTP's perspective is that once an\n> intermediary becomes a tunnel, it is no longer conscious of the\n> HTTP communication -- only of bytes being relayed from one connection\n> to another.\n> \n> ....Roy\n> \n\n\n\n", "id": "lists-012-12193053"}, {"subject": "Re: http proxy &amp; tunnel differences ?", "content": ">Actually, for better or worse, the client does not always\n>know about the proxy.  Transparent proxies are quite common.\n\nThose are not transparent proxies -- they are firewall gateways,\neven when they occur near to the client.  My use of \"proxy\" and\n\"gateway\" are defined in the HTTP spec, which does not always\ncorrespond to what some marketing departments call their products.\n\n....Roy\n\n\n\n", "id": "lists-012-12203486"}, {"subject": "RE: Https proxies  was Re: http proxy &amp; tunnel differences ?", "content": "> From: douglas.e.reed@att.net\n\n> This looks like an opportunity to expand my understanding\n> on https proxies as well. When a client issues a CONNECT\n> request to a proxy, the proxy creates a connection to\n> the remote site, and the proxy returns an HTTP 200\n> response to the client. From that point, the client\n> and remote site exchange data through the proxy. My\n> question is what is that data? Does the client have to\n> issue a full SSL handshake over the connection, or can\n> the client just send data. In other words, what happens\n> after the CONNECT from a client's perspective.\n\nCONNECT really just asks a proxy to create the origin server connection\nand then switch to tunnel mode for the pair of connections\n(client-proxy, proxy-origin).  What the client does over it is up to the\nclient.  Usage of CONNECT is not limited to https; for https you must\nbegin with the handshake.\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12212053"}, {"subject": "Re: http proxy &amp; tunnel differences ?", "content": "> >> How does a http tunnel work. Is the initial connection similar ?\n> >> Does a client (browser) need to configured differently when\n> >> it goes through\n> >> a tunnel or is it same as the configuration required when it\n> >> goes through\n> >> the proxy ? Are there to separate tcp connections for each\n> >> request even in a\n> >> tunnel ?\n>\n>Some tunnels are activated by a proxy request, some are simply\n>port forwarding TCP firewalls (either on the client side or the\n>server side, or both), and others are gateways to other servers.\n>The important thing from HTTP's perspective is that once an\n>intermediary becomes a tunnel, it is no longer conscious of the\n>HTTP communication -- only of bytes being relayed from one connection\n>to another.\n>\n>....Roy\n\nFor tunnels that are activated by a proxy request, A browser has to be \nconfigured to go through the proxy.So a request from a browser to a HTTP \nProxy or a HTTP tunnel(activated by a request) is one and the same. What I \nmean is that, even if the intermediary is a HTTP tunnel, the client always \ntalks to the tunnel thru one part of the connection and a tunnel will \nforward that on another connection to the ORIGIN SERVER. Even the origin \nservers response will come to the intermediate tunnel which get forwarded to \nthe client on the other connection.\n\nThese requests or the responses MAY contain some hop-by-hop header \ninformation such as Keep-Alive, Connection Header (close), Max-Forwards \netc..  While a proxy can take care of them ,what will a should a HTTP tunnel \ndo? A http tunnel would'nt even look at the response or at the request(apart \nfrom looking at the URL to see where the forwarding connection should be \nestablished).\n\nHow does a HTTP tunnel take care of persistant connections and request \npipelining?\n\nRegards,\n\nVinit Kumar\n\n\n\n\n______________________________________________________\nGet Your Private, Free Email at http://www.hotmail.com\n\n\n\n", "id": "lists-012-12221350"}, {"subject": "Https proxies  was Re: http proxy &amp; tunnel differences ?", "content": "This looks like an opportunity to expand my understanding\non https proxies as well. When a client issues a CONNECT\nrequest to a proxy, the proxy creates a connection to\nthe remote site, and the proxy returns an HTTP 200 \nresponse to the client. From that point, the client\nand remote site exchange data through the proxy. My \nquestion is what is that data? Does the client have to\nissue a full SSL handshake over the connection, or can\nthe client just send data. In other words, what happens\nafter the CONNECT from a client's perspective. \n\nRegards,\nDoug\n--\n-----------------------------------------------------\nDoug Reed\nmailto:douglas.e.reed@worldnet.att.net\n-----------------------------------------------------\n> >> A proxy works by taking a request from a client and\n> >> connecting to the origin\n> >> server indicated in the request.  Here the client is configured to go\n> >> through the proxy.\n> >\n> >Actually, the client may or may not know about the proxy.\n> \n> The client always knows about the proxy -- that is what distinguishes\n> a proxy from a gateway.  A \"reverse proxy\" is a gateway.\n> \n> >> How does a http tunnel work. Is the initial connection similar ?\n> >> Does a client (browser) need to configured differently when\n> >> it goes through\n> >> a tunnel or is it same as the configuration required when it\n> >> goes through\n> >> the proxy ? Are there to separate tcp connections for each\n> >> request even in a\n> >> tunnel ?\n> \n> Some tunnels are activated by a proxy request, some are simply\n> port forwarding TCP firewalls (either on the client side or the\n> server side, or both), and others are gateways to other servers.\n> The important thing from HTTP's perspective is that once an\n> intermediary becomes a tunnel, it is no longer conscious of the\n> HTTP communication -- only of bytes being relayed from one connection\n> to another.\n> \n> ....Roy\n> \n\n\n\n", "id": "lists-012-12231296"}, {"subject": "Re: http proxy &amp; tunnel differences ?", "content": ">For tunnels that are activated by a proxy request, A browser has to be \n>configured to go through the proxy.So a request from a browser to a HTTP \n>Proxy or a HTTP tunnel(activated by a request) is one and the same. \n\nNo, they aren't.  The terms \"client\", \"server\", \"proxy\", \"tunnel\", etc.,\nare all regarding the ROLE of the software for a particular request/response.\nThe first request went to a proxy -- after the 200 response, it is no\nlonger a proxy.  It is not an HTTP tunnel -- it is just a tunnel, period.\n\n>What I \n>mean is that, even if the intermediary is a HTTP tunnel, the client always \n>talks to the tunnel thru one part of the connection and a tunnel will \n>forward that on another connection to the ORIGIN SERVER. Even the origin \n>servers response will come to the intermediate tunnel which get forwarded to \n>the client on the other connection.\n>\n>These requests or the responses MAY contain some hop-by-hop header \n>information such as Keep-Alive, Connection Header (close), Max-Forwards \n>etc..  While a proxy can take care of them ,what will a should a HTTP tunnel \n>do? A http tunnel would'nt even look at the response or at the request(apart \n>from looking at the URL to see where the forwarding connection should be \n>established).\n>\n>How does a HTTP tunnel take care of persistant connections and request \n>pipelining?\n\nRFC 2616:\n\n   tunnel\n      An intermediary program which is acting as a blind relay between\n      two connections. Once active, a tunnel is not considered a party\n      to the HTTP communication, though the tunnel may have been\n      initiated by an HTTP request. The tunnel ceases to exist when both\n      ends of the relayed connections are closed.\n\nBy definition, it does not grok HTTP, therefore it doesn't take care\nof persistant connections and request pipelining.\n\n....Roy\n\n\n\n", "id": "lists-012-12242026"}, {"subject": "Standardize Kerberos authentication and authorization in HTTP", "content": "It seems to me that it would be good to have an IETF standard on how to do\nKerberos-based authentication and authorization in HTTP.  By the\nauthorization part I mean the ability to pass proxy tickets, including\nforwarded and/or forwardable TGTs.  RFC 2712 (Kerberos in TLS) clearly\naddresses the authentication part, but, because TLS doesn't address\nauthorization, does not clearly address the authorization part.  However, a\npossible approach would be to use RFC 1964's technique of putting forwarded\ntickets in the Authenticator (RFC 2712 *does* pass an Authenticator).  A\npossible drawback of this approach is that some server-side products (e.g.,\nJava Servelet engines) may not pass as many TLS details as they should.\n\nKerberos is well known in UNIX-land, and is coming in Windows 2000.  In\nfact, Microsoft already has a way of doing both authentication and\nauthorization based on Kerberos.  An informed source tells me their\ntechnique, while not publicly documented, is based on IETF standards (e.g.,\nRFC 1964).  A plausible and happy scenario would be for them to submit\ntheir technique, and a consensus formed around it.\n\nDoes this make sense to you?\n\nWhere should such an effort be homed?  Larry assures me the HTTP-WG is\nshutting down and won't take any new work.  I don't have any strong opinion\non the matter.\n\nThanks,\nMike Spreitzer <spreitze@parc.xerox.com>\nhttp://parcweb.parc/spreitze/ (Xerox internal)\nhttp://www.parc.xerox.com/spreitze/ (external)\n+1-650-812-4833\n\n\n\n", "id": "lists-012-12251169"}, {"subject": "Re: Standardize Kerberos authentication and authorization in HTTP", "content": "At 04:52 PM 11/2/99 , Mike Spreitzer wrote:\n\n\n>Where should such an effort be homed?  Larry assures me the HTTP-WG is\n>shutting down and won't take any new work.  I don't have any strong opinion\n>on the matter.\n\nShouldn't WWW Security be the sponsor of such an effort?  HTTP is the\nbasic protocol but it is up to the relevant WGs to recommend track\nenhancements that will support the additional requirements (e.g.\nWEBDAV).\n===========================================================\nKevin J. Dyer                                Draper Laboratory  MS 35\nEmail: <kdyer@draper.com>                    555 Tech. Sq.\nPhone: 617-258-4962                          Cambridge, MA 02139\nFAX: 617-258-2061\n---------------------------------------------------------------------------- \n------------------------------------------\n     _/_/_/_/     _/         _/  _/  _/       _/   _/_/_/_/\n    _/      _/   _/_/     _/_/  _/  _/_/     _/  _/\n   _/       _/  _/ _/   _/ _/  _/  _/  _/   _/   _/_/_/\n _/      _/   _/  _/ _/  _/  _/  _/    _/ _/         _/\n_/_/_/_/     _/    _/   _/  _/  _/       _/  _/_/_/_/\n        Data Management & Information Navigation Systems\n=========================================================== \n\n\n\n", "id": "lists-012-12260467"}, {"subject": "Persistant Connection over prox", "content": "Hi,\n\nIs it possible for a proxy server to receive  multiple HTTP requests from a \nUser Agent over one connection , where the each of the requests are for a \nresource on different ORIGIN Servers?\n\n\nThanks in Advance,\n\nVinit Kumar\n\n\n______________________________________________________\nGet Your Private, Free Email at http://www.hotmail.com\n\n\n\n", "id": "lists-012-12269296"}, {"subject": "Issues with draft-ietf-http-v11-spec-rev03.tx", "content": "A few issues with draft-ietf-http-v11-spec-rev-03.txt.\n\n1)\n    3.7.2: Mutipart Types\n\n    ...\n    CRLF to represent line breaks between body-parts. Unlike in RFC 2046,\n    the epilogue of any multipart message MUST be empty; HTTP applications\n    MUST NOT transmit the epilogue (even if the original multipart contains\n    an epilogue). These restrictions exist in order to preserve the self-\n    delimiting nature of a multipart message-body, wherein the \"end\" of the\n    message-body is indicated by the ending multipart boundary.\n\nThis seems to indicate that all multipart message-bodies are self\ndelimiting. However, section 4.4 allows only the multipart/byteranges to\nbe self-delimiting (i.e. all others must be delimited by one of the other\nmethods).\n\nThis is probably too late in the game, but given the above quote why does\nSection 4.4 only use multipart/byteranges, and not allow all multiparts\nto be self delimiting? Alternatively, if section 4.4 is left as it is,\nthe above quoted paragraph should be restricted to multipart/byteranges.\n\n\n2)\n    5.1.2. Request-URI\n\n    ...\n    In requests that they forward, transparent proxies MUST NOT rewrite the\n    \"abs_path\" part of a Request-URI in any way except as noted above to\n    replace a null abs_path with \"*\", no matter what the proxy does in its\n    internal implementation.\n\nTwo things:\n- \"... as noted above ...\" - this seems to be left over from earlier\n  drafts, as this isn't noted anywhere anymore.\n\n- I've become somewhat confused: when a client is sending an \"OPTIONS *\"\n  (or a \"TRACE *\") request via a proxy, should it use\n  \"OPTIONS http://www.some.server HTTP/1.1\" or \"OPTIONS * HTTP/1.1\"?\n  The above quote (and the text further up) indicates the former\n  (rfc-2068 behaviour), but then why was the example removed from this\n  section? I went back and looked at draft-ietf-http-options-02.txt for\n  enlightenment, but it didn't help.\n    \n  So, while I found rfc-2068 nice and clear I can't say the same about\n  the current draft.\n\n\n3) minor typo:\n\nSection 14.11, line 6020:\n\n  If the content-coding of an entity is not \"identity\", then the response\n  MUST including a Content-Encoding entity-header (section 14.11) that\n             ^^^\n\n    should be\n\n  If the content-coding of an entity is not \"identity\", then the response\n  MUST include a Content-Encoding entity-header (section 14.11) that\n\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1227487"}, {"subject": "RE: Persistant Connection over prox", "content": "> From: Vinit Kumar\n> Subject: Persistant Connection over proxy\n>\n> Is it possible for a proxy server to receive  multiple HTTP\n> requests from a\n> User Agent over one connection , where the each of the\n> requests are for a\n> resource on different ORIGIN Servers?\n\nYes.  In doing it this way, the client would be accepting the fact that\na slow response from one origin server would delay any others pipelined\nbehind it, but other than that there is no reason not to do it that way.\n\nSimilarly, an origin server might receive multiple requests on the same\nconnection from a proxy that were from different User Agents.\n\n--\nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12276438"}, {"subject": "Failure to do persistent connectio", "content": "Hi,\n\nI'm implementing a program to connect a site and walk through\nthe first few pages. To do this I choose my.yahoo.com as my\ntarget, and start to implement as follows:\n\nI use a URL instance to connect to \"http: file://my.yahoo.com\" url.\nIt responses with a single cookie header as:\n\n  Set-Cookie: B=somevalue; expires=date; path=/ ; domain=.yahoo.com;\n\nMy program parses this cookie and prepare the following string\nto return back in the next POST request:\n\n  Cookie: $version=0; B=somevalue; $path=/; $domain=.yahoo.com\n  (The 'B=samoevalue' format is tested too)\n  \nThe program will attach a parameter list for logging in to\nMyYahoo in the following format:\n\n  param1=value1&param2=value2&...\n\n(The parameters list is complete and all parameters name are\nspelled correctly), but the server will response with a new\ncookie and send a page that contains a message which says \"An\nError Occurred Setting Your User Cookie\".\n\nI tried every thing I guessed, but no success. I don't know\nwhat is the problem. The last tested code is attached bellow.\n\nThank you in advance for your kind attentions.\n@liz@\n\n---------------------------------------\n\n System.getProperties().put( \"proxySet\",  \"true\" );\n System.getProperties().put( \"proxyHost\", \"128.1.1.1\" );\n System.getProperties().put( \"proxyPort\", \"80\" );\n \n URL url = new URL(\"http://my.yahoo.com\");\n HttpURLConnection connection = (HttpURLConnection)url.openConnection();\n   \n connection.setRequestMethod(\"GET\");\n connection.setRequestProperty(\"Connection\", \"Keep-Alive\");\n connection.setRequestProperty(\"Proxy-Connection\", \"Keep-Alive\");\n connection.setRequestProperty(\"Cashe-Control\", \"no-cashe\");\n connection.setDoOutput(true);\n connection.setUseCaches(false);\n Object content = connection.getContent();\n   \n Cookie cookie = null;\n int    index  = 0;\n String header = connection.getHeaderField(index);\n String key    = connection.getHeaderFieldKey(index);\n  \n StringBuffer cookieStr = new StringBuffer(\"\");\n   \n while (header != null) {\n  if (\"Set-Cookie\".equalsIgnoreCase(key)) {\n   // parses the cookie header\n   cookie = parseCookie(header);\n   if (cookieStr.length() > 0) {\n    cookieStr.append(\",\");\n   }\n   // converts the cookie to this format:\n   //    $version=1; name=value; $path=path; $domain=domain\n   cookieStr.append(toRFC_2109(cookie));\n  }\n    \n  index++;\n  header = connection.getHeaderField(index);\n  key    = connection.getHeaderFieldKey(index);\n    \n }\n\n BufferedInputStream in = (BufferedInputStream)content;\n OutputStream out = new FileOutputStream(args[0]);\n int c;\n while ((c=in.read())>= 0){\n  out.write((char)c);\n }\n out.close();\n   \n  \n url = new URL(\"http://login.yahoo.com/config/login\");\n connection = (HttpURLConnection)url.openConnection();\n   \n SimpleDateFormat formatter = new SimpleDateFormat (\"EEE, dd MMM yyyy HH:mm:ss 'GMT'\");\n Date now = new Date();\n String dateStr = formatter.format(now);\n System.out.println(\"Date=<\" + dateStr + \">\");\n   \n connection.setRequestMethod(\"POST\");\n connection.setRequestProperty(\"Cashe-Control\", \"no-cashe\");\n connection.setRequestProperty(\"Connection\", \"Keep-Alive\");\n connection.setRequestProperty(\"Proxy-Connection\", \"Keep-Alive\");\n connection.setRequestProperty(\"Date\", dateStr);\n connection.setDoOutput(true);\n connection.setUseCaches(false);\n   \n   \n if ( ! \"\".equals(cookieStr)) {\n  connection.setRequestProperty(\"Cookie\", cookieStr.toString());\n }\n\n String params = URLEncoder.encode(\".tries\")      + \"=\" + URLEncoder.encode(\"1\")  + \"&\" + \n   URLEncoder.encode(\".done\")       + \"=\" + URLEncoder.encode(\"http://my.yahoo.com\") + \"&\" +\n   URLEncoder.encode(\".src\")        + \"=\" + URLEncoder.encode(\"my\") + \"&\" +\n   //... other parames\n   \n\n connection.setRequestProperty(\"Content-Length\", Integer.toString(params.length()));\n DataOutputStream o = new DataOutputStream(connection.getOutputStream());\n\n o.writeBytes(params);\n o.flush();\n o.close();\n   \n content = connection.getContent();\n   \n in = (BufferedInputStream)content;\n out = new FileOutputStream(args[1]);\n while ((c=in.read())>= 0){\n  out.write((char)c);\n }\n out.close();\n   \n System.out.println();\n \n\n\n\n", "id": "lists-012-12284479"}, {"subject": "Proxy aut", "content": "Hey,\n  \n Since we're talking about proxies....\nIm curious to know what others think the right thing\naccording to the intent of the 1.1 spec to do is\nin this situation:\n\nIf you have two chained proxy servers:\n\nclient -> proxy1 -> proxy2 -> origin server\n\nIf proxy 2 challenges for proxy-authentication (in its realm),\nshould the challenge go back to the client if proxy1 doesnt intend\nto satisfy the challenge ?\n\nMy understanding was that the intent was that this situation was\nto be covered.  By this I mean a client can auth to a proxy up the chain.\nThe spec is somewhat ambiguous, it says the proxy-auth headers are \nhop-by-hop, but then mentions that chained proxy-auth can work.\n\n\n\n", "id": "lists-012-12295514"}, {"subject": "Re: Proxy aut", "content": "\"Josh Cohen (Exchange)\" <joshco@Exchange.Microsoft.com> wrote:\n  >  Since we're talking about proxies....\n  > Im curious to know what others think the right thing\n  > according to the intent of the 1.1 spec to do is\n  > in this situation:\n  > \n  > If you have two chained proxy servers:\n  > \n  > client -> proxy1 -> proxy2 -> origin server\n  > \n  > If proxy 2 challenges for proxy-authentication (in its realm),\n  > should the challenge go back to the client if proxy1 doesnt intend\n  > to satisfy the challenge ?\n  > \n  > My understanding was that the intent was that this situation was\n  > to be covered.  By this I mean a client can auth to a proxy up the chain.\n  > The spec is somewhat ambiguous, it says the proxy-auth headers are \n  > hop-by-hop, but then mentions that chained proxy-auth can work.\n\nMy understanding has always been that proxy authentication is strictly\nhop-by-hop.  So proxy1 should not bump the authentication request up to\nthe client.  After all, it's proxy1 that has a trust relationship with\nproxy2, whereas the client may have no such relationship.\n\nDave Kristol\n\n\n\n", "id": "lists-012-12303725"}, {"subject": "Re: Proxy aut", "content": "On Thu, 18 Nov 1999, Dave Kristol wrote:\n\n> \"Josh Cohen (Exchange)\" <joshco@Exchange.Microsoft.com> wrote:\n>   >  Since we're talking about proxies....\n>   > Im curious to know what others think the right thing\n>   > according to the intent of the 1.1 spec to do is\n>   > in this situation:\n>   > \n>   > If you have two chained proxy servers:\n>   > \n>   > client -> proxy1 -> proxy2 -> origin server\n>   > \n>   > If proxy 2 challenges for proxy-authentication (in its realm),\n>   > should the challenge go back to the client if proxy1 doesnt intend\n>   > to satisfy the challenge ?\n>   > \n>   > My understanding was that the intent was that this situation was\n>   > to be covered.  By this I mean a client can auth to a proxy up the chain.\n>   > The spec is somewhat ambiguous, it says the proxy-auth headers are \n>   > hop-by-hop, but then mentions that chained proxy-auth can work.\n> \n> My understanding has always been that proxy authentication is strictly\n> hop-by-hop.  So proxy1 should not bump the authentication request up to\n> the client.  After all, it's proxy1 that has a trust relationship with\n> proxy2, whereas the client may have no such relationship.\n\nMy recollection matchs DaveK's ... it was acknowledged at the time that\nthe proxy auth was hop-hop only and as I recall the WG rejected an attempt\nto extend the protocol to accurately allow for proxy authentication thru\nanother proxy. \n\nI believe there are cases where Josh's scenario would be valuable but it\nisn't what was defined.\n\nDave Morris\n\n\n\n", "id": "lists-012-12312831"}, {"subject": "Re: Proxy aut", "content": ">If you have two chained proxy servers:\n>\n>client -> proxy1 -> proxy2 -> origin server\n>\n>If proxy 2 challenges for proxy-authentication (in its realm),\n>should the challenge go back to the client if proxy1 doesnt intend\n>to satisfy the challenge ?\n>\n>My understanding was that the intent was that this situation was\n>to be covered.  By this I mean a client can auth to a proxy up the chain.\n>The spec is somewhat ambiguous, it says the proxy-auth headers are \n>hop-by-hop, but then mentions that chained proxy-auth can work.\n\nSpecifically, RFC 2616 says:\n\n   The HTTP access authentication process is described in \"HTTP\n   Authentication: Basic and Digest Access Authentication\" [43]. Unlike\n   WWW-Authenticate, the Proxy-Authenticate header field applies only to\n   the current connection and SHOULD NOT be passed on to downstream\n   clients. However, an intermediate proxy might need to obtain its own\n   credentials by requesting them from the downstream client, which in\n   some circumstances will appear as if the proxy is forwarding the\n   Proxy-Authenticate header field.\n\n   ...\n\n   The HTTP access authentication process is described in \"HTTP\n   Authentication: Basic and Digest Access Authentication\" [43] . Unlike\n   Authorization, the Proxy-Authorization header field applies only to\n   the next outbound proxy that demanded authentication using the Proxy-\n   Authenticate field. When multiple proxies are used in a chain, the\n   Proxy-Authorization header field is consumed by the first outbound\n   proxy that was expecting to receive credentials. A proxy MAY relay\n   the credentials from the client request to the next proxy if that is\n   the mechanism by which the proxies cooperatively authenticate a given\n   request.\n\nI don't see anything ambiguous about that.  Proxy authentication appears\nto be chained if the credentials are chained, but that is no different\nthan saying it is hop-by-hop.  Being hop-by-hop does not imply that\nthe proxy cannot clue one side of the hop based on the other side's info.\n\nThe reason it is specified this way has been discussed many times before.\n\n....Roy\n\n\n\n", "id": "lists-012-12322705"}, {"subject": "RE: SOA", "content": " http://msdn.microsoft.com/workshop/xml/general/SOAP_V09.asp\n\nmight lead you to believe that it is a work item of the HTTP working\ngroup, but it is not; the HTTP working group is scheduled to close as\nsoon as is possible, and is accepting no new work.\n\nAs far as I can remember, SOAP has never been discussed by the HTTP\nworking group or any other IETF working group list (not even applcore,\nwhere it might otherwise belong), although I think I did hear it\nmuttered in the hallways in DC.\n\n> -----Original Message-----\n> From: George W. Leach [mailto:gwleach@gte.net]\n> Sent: Thursday, December 02, 1999 8:57 AM\n> To: masinter@parc.xerox.com\n> Subject: SOAP\n> \n> \n> \n> Hi Larry,\n> \n> I spotted a news item that indicated that Microsoft had submitted\n> its Simple Object Access Protocol (SOAP) spec to the IETF.  It\n> is listed as an internet draft under the auspices of the HTTP Working\n> Group:\n> \n> http://msdn.microsoft.com/workshop/xml/general/SOAP_V09.asp\n> \n> I also found it in the Internet Drafts directory:\n> \n> http://search.ietf.org/internet-drafts/draft-box-http-soap-00.txt\n> \n> What I am trying to figure out is will this actually go anywhere?\n> \n> It appears on the surface to be competitive with the XML-RPC\n> mechanism that webMethods has developed.\n> \n> Thanks\n> \n> George\n> \n> \n> \n\n\n\n", "id": "lists-012-12331570"}, {"subject": "Please feedback: a proposed method for tracing broken link", "content": "Hi everybody,\n\nthis is Matteo Pampolini again from Italy. Few months ago I wrote\nto this mailing list with the intention of proposing a method for\ntracing broken links.\n\nThank you very much to everyone that replied to my e-mail, but\nunfortunately I was the involved in another project that kept me\nvery busy 'til some days ago.\n\nNow I prepared a very very draft document with the basis of my\nidea: I ask anyone interested on it to start a discusion, of course\nfeeling free to tell me any comments  and remarks.\n\nGood work to everyone,\n\n--\nMatteo Pampolini (matteo.pampolini@italtel.it)\n\nStrategic Planning and Innovation - Advanced Research\n\nItaltel S.p.A. 20019 Settimo Milanese (Mi) Italy\n\nPhone  +39.02.4388.9375  Fax  +39.02.4388.9120\n\n*****************************************************\n*                                                   *\n* ...but gravity always wins (Radiohead)            *\n*                                                   *\n*****************************************************\n\n\n\n\nM. Pampolini\nAdvanced Research Laboratories\nItaltel S.p.A.\nDecember 1999\n\n\nA proposed method for tracing broken links\n\nPreface\n\nThis is intentionally a draft and informal document, with the aim to \nfocus on a problem that, in my personal opinion, is growing faster and \nfaster with the expansion of the Internet, and whose effects can be \nexperimented every day when surfing the Net.\n\nThat is why I decided to think about it, to define a very first proposal \nto try resolve, at least partially, this problem, conscious that there \ncould be a great deal of improvement on it, especially on the efficiency \nside (and I really hope there will be) and that many issues are still \nopened.\n\nUnfortunately, though I try to keep, as far as I can, in touch with the \nWorld Wide Web evolution, in terms of architecture and new protocols, I?m \nnot actually involved in any IETF working group: that?s why I?d like to \nhave the largest feedback on this topic from the members of HTTP mailing \nlist interested on it.\n\nThe problem\n\nEveryday, when surfing the Net, we follow hyperlinks: however as new \nsites are daily added to the worldwide database that is the Internet, \nothers are deleted or moved, leading to a great dinamicity in information \nmanagement, but also to HTML documents that refer to a non-existent URL, \nthus obtaining the well-known ?404 - Not found? HTTP error.\n\nI have to say that this phenomenon was quite rare only few months ago, \nbut now this error appears more frequently on my (and I suppose \neveryone?s) browser.\n\nA possible solution\n\nLet?s first say that the proposed solution requires modifications either \non the browser side or the HTTP server one: this is probably the major \ndrawback, but it?s possible that with different contributions these \nmodifications could be avoided at least on one side.\n\nBy now, we will focus on static HTML pages, not created, for example, \nthrough CGI scripts or similar, but located in the usual tree structure \nof personal pages in the server space.\n\nSo, imagine you have just downloaded from Web server S1 an HTML page, say \nP1, that contains an hyperlink to page P2 on server S2: you click on it \nand error 404 is reported. In first analysis this could mean that page P2 \ndoesn?t exist anymore in the space of server S2. Currently at this point \nyou can?t do anything but follow another link, and another surfer on the \nother side of the world experience the same behaviour.\n\nThings would go better if your browser could inform the Web server S1 \nthat the URL referring to page P2 is invalid or ?broken?: this could be \ndone through a special HTTP action.\n\nServer S1 then keeps trace of this URL and, if possible, of the page that \nreferenced it. Actually you cannot discriminate if page P2 really doesn?t \nexist anymore or other problems occurred, so server S1 doesn?t take any \ndecision on action to perform.\n\nBut if, for example,a reference counter is associated to this broken URL \nand other browsers in different times reasonably distant each other, say \nin a week period, inform server S1 that URL relative to page P2 is \nunreachable, then there?s a good chance that page P2 has been cut off.\n\nNow server S1 could do many things:\n\n\n* Automatically send an e-mail to the Webmaster informing him of the \nbroken link: this one could eventually in turn ask the author of page P1 \nto eliminate the link.\n\n* Find a way to inform the next surfer that loads page P1 that the \nreferred page P2 is unavailable. This could be done by parsing page P1 \nbefore sending it and adding a new tag, say <BROKEN>, after tag <HREF> in \npage P1. The browser in turn parses the page and when it finds a broken \nlink it is displayed in a different fashion, meaning ?unavailable, don?t \nclick on it?, or it?s not displayed at all.\n\n\nThe process of parsing the page by the Web server could lead to \nefficiency problems, so it could simply send, maybe formatted as an HTML \ndocument, the list, if exists, of links referred by a particular page \nthat appear to be unavailable. The browser receives such a list and \nbehaves as before.\n\nConclusion\n\nIn this document I tried to give the basic idea of a mechanism useful to \navoid following links that aren?t available anymore. Of course many other \nproblems arise from this technique and not the totality of cases is \nfaced. However this just wants to be the starting point for a \nconstructive discussion that, in my optimistic prevision, could lead to a \nmore sofisticated solution of the problem.\n\n\n\n", "id": "lists-012-12340277"}, {"subject": "Apology for goof in posting SOAP V1 draf", "content": "Dear WG members,\n\nI would like to apologize on behalf of the authors of the SOAP 1.0\nspecification for incorrectly identifying the SOAP 1.0 draft as being\nassociated with the HTTP WG.  Many of us are new to the IETF, and not\nfamiliar with the nuances of the IETF process.  We did not in any way intend\nto create a misleading impression and we have corrected the error by\nsubmitting a new draft without the offending header.  If anyone notices any\nremaining procedural error in the draft please let one of the people on the\nCC line know and we will correct it as soon as possible.\n\nThanks,\n\nSatish Thatte\n\n\n\n", "id": "lists-012-12353007"}, {"subject": "comments on draft-ietf-http-trust-state-mgt02.tx", "content": "Here are my comments on draft-ietf-http-trust-state-mgt-02.txt.  I\nthink the default settings for the use of trust labels and cookies need\nto be discussed more.  See below, under 3.3.2.\n\nDave Kristol\n==============\n\nSubstantive:\n\n3.1  Syntax: General\n    the optional label attributes \"by\" \"gen\" \"for\" \"on\" and \"exp\" \n        are required. \n\n    -> except that the syntax shows that \"by\" is still optional.\n\n    privacypractice = \"purpose\" 1*purposerating \n--> 1*purposerating is inconsistent with the later examples,\nwhich show things like \"0:4\" (several places).\n\n    trust-label-data = labelattr-data privacy-practice [cookielist]\n    ...\n    \"cookielist\" refers to the list of cookie names in the cookieblock \n    extension.\n--> It would be better to create a \"cookielist\" non-terminal\nin the grammar (and use it for non-terminal \"cookieinfo\").\n\n    purpose. Services that disclose that they use data for \"other\" purposes \n    should provide human readable explanations of those purposes. \n--> The specification needs to say where/how to get them.\n\n    \"Contacting Visitors for Marketing of Services or Products\" \n     value = 4\n--> I'm unclear what this means.  Does it mean that the cookie\naccompanies (and is) the \"contact\", or does it imply there's\nsome kind of subsequent or out-of-band contact?\n\nFor example, a cookie from an (third-party) advertising site\nmight be construed as purpose 4.\n\n    \"non-identifiable\"                               value = 0\n    \"identifiable\"                                   value = 1\n--> This begs for a definition of what comprises \"information\ngathered ... in identifiable form\", or, at the very least, an\nexample.\n\n3.3.1 Interpreting the trust-label\n    A trust-label is ignored if the \"exp-date\" attribute of labelattr\n    is less than or equal to the current date.\n--> Wouldn't it also make sense to do likewise if the exp-date\nattribute is <= the Date: header in the server's response?\n\n    The labelattr-data, privacy-practice, and cookielist in the decrypted \n    trust-label-data from the sigblock must match the plaintext labelattr,\n    privacy-practice, and cookielist for the signature to be valid.\n\n--> I don't think this is quite right.  If I understand how\nthis stuff works,\n    1) The user agent calculates its own cleartext signature\n    for the trust-label-data; and\n    2) It compares the result against the decrypted sigblock\n    value.\nThat is, the attributes themselves don't get decrypted.  Also,\nit's important to emphasize how the user agent should\ncanonicalize the attributes when it does its calculation.\n\nTherefore, I think the wording should be something like this:\n\n    The user agent validates the signature as follows.  It calculates\n    its own signature for the trust-label from labelattr-data,\n    privacy-practice, and cookielist, using the canonicalization rules\n    of [DSIG].  (That is, it must mimic the trust authority's\n    calculation.)  It then uses the public key of the trust authority\n    to decrypt the sigblock value and compares the result against its\n    own calculation.  The signature is valid if the two values agree.\n    ------\n\n    If the digital signature is invalid, then the trust-label should be \n    ignored and the cookie should not be set.\n--> I think the user agent should do whatever it would do when\nit receives a cookie that has no trust label, rather than\nalways not set the cookie.  I think that's more consistent.\n\n3.3.2Accepting or rejecting Cookies\n    User agents should have the following default preferences:\n      Automatically accept:\nCookies from verifiable transactions with trust-labels with \n  purposerating 0 through 4 and dourating 0;\nCookies from unverifiable transactions with trust-labels with \n  purposerating 0 through 4 and idrating 0 and \n  dourating 1.\n      Automatically reject:\nCookies from unverifiable transactions without trust-labels.\n\n--> IMO the default purposerating should be 0-3.  If you want to\ncreate an installation option that asks users (\"opt in\") whether\n4 is okay, fine.\n\n--> There's no mention above of *signed* trust labels.  IMO\ncookies for unverifiable transactions should require a signed\nlabel (by default).\n\n--> As the specification is now written, the \"by\" attribute is\noptional, as are signatures.  So a site need not even name a\ntrust authority, can invent trust labels, and can bypass the\n\"protections\" the defaults should provide.  It also makes it\neven simpler to work around the admittedly weak \"protections\"\nin state-man-mec-08.  That's unsatisfactory.\n\nI think the spec. should require, at a minimum, either a \"by\"\nor a sigblock.  The \"by\" is a lame protection, but at least if\na site fraudulently sends an unsigned label by a bogus trust\nauthority there might be some recourse under consumer fraud\nrules.\n\nNits:\n\nIn a couple of places, the formatting is broken:  lines extend beyond\nthe margin:  Domainofuse; 3.4.3 Discovery ...; 5.2 False ...; 7.;\nACKNOWLEDGEMENTS.\n\nThere are a couple of references to RFC 2109.  Should they be changed\nto state-man-mec-08?\n\nThere are a couple of instances of expired \"exp\" attributes in the\nexamples:  exp \"1997.12.31T23:59-0000\".\n\n3. OUTLINE\n\n    The server sends a Set-Cookie and/or a Set-Cookie2 header to the user \n    Agent along with a PICS-Label header containing the trust-label.  The \n    ^-- change: a\n\n3.1  Syntax: General\n    The specific cookies are listed in the cookieinfo extension to the \n    PICS label or to all compatible cookies if no cookies listed in the \n    cookieinfo extension.\nREWORD AS\n    The PICS label applies to the cookies listed in the cookieinfo\n    extension, or to all compatible cookies if no cookies are\n    explicitly listed.\n\n    an extension to include a list of the specific cookies to \n        to which the trust-label applies;\n== -> delete\n\n    the optional label attributes \"by\" \"gen\" \"for\" \"on\" and \"exp\" \n        are required. \nREWORD AS (and see note, earlier, about \"by\")\n    the optional PICS label attributes \"by\", \"gen\", \"for\", \"on\", and\n\"exp\" are required by this specification.\n\n    serviceID       = \n  \"http://www.w3.org/PICS/systems/P3P-http-trust-state-01.rat\" |\n^-- insert: <\"> insert <\"> --^\n    --> it's supposed to be quoted\n\n    The Identifiable rating specifies whether the information gathered is in \n    identifiable form, is associated with identifiable information, or used \n           insert: \"is\" --^\n    to derive personal identity.\n\n3.3 User Agent Role\n\n    The user agent receives a cookie headers and trust-labels from\n        = -> delete\n\n3.3.3  User intervention\n    The user agent may prompt the user to verify that it wishes to reject a\n    cookie in conditions where the cookie is being rejected based on\n    a default preference or no preference applies.\n       ^-- insert: when\n\n3.4.3 Discovery of privacy-practice ratings\n    well-known rating service, http:\\www.w3.org\\PICS\\1.0\\P3P\\v1.0,\n    change to: http://www.w3.org/PICS/1.0/PRP/v1.0\n    is referenced in this document.\n\n4.1 Example 1\n\n1.  User Agent preferences:\n\n    In this example, the user agent has a preference for automatically\n    accepting cookies from domains that have valid rating of \n    \"domainofuse 0\".\n-> REWORD AS\n    In this example, the user agent has a preference for automatically\n    accepting cookies from domains that have a valid rating of \n    \"domainofuse 0\", and the user agent does not require signed labels.\n\n4.2 Example 2\n    \"domainofuse 0\" or \"domainofuse 1 by www.aaa.org.\n         ^-- insert: \"\n\n(Under 5)\n    www.aaa.org\" is acceptable to the user agent.\n    ^-- insert: \"\n\n5.2 False representation\n    site's adherence to its stated privacy practice.  However, if the\n    site digitally signs the label, then that may represents a legally\n      delete --^\n    binding contract on the site to follow the professed privacy\n    ...\n\n7. ACKNOWLEDGEMENTS\n\n    This document represents input from Dave Kristol, Yaron Goland,\n==== -> \"David M.\", please.\n    Joseph Reagle, and Jonathan Stark..\n\n\n\n", "id": "lists-012-1236268"}, {"subject": "working of http request and response!", "content": "hi!\n\nthis may be a simple question but i am unable to understand hence mailing  \nhere,so please do give me an answer even if the answer  is trivial:-\n\n1.) when User Agents(clients) send multiple requests on a persistent \nconnection  (for example say 5 requests from a user sent on the same \nconnection) then will the server send the response to all the requests on \nthat same  connection ? if the answer to this question is yes !!\n  then another question -if the response has data of around 200kbytes for \neach request will this not affect the response time from the client point of \nview ? and during the time the server is writing the response if that \nparticular process is context switched (since it's time share of cpu is \nover) then  will the server do all this writing from start once it gets back \nthe cpu time or will continue from where it was context switched.\n\nthanks in advance for the answers and i really appreciate your help in this \nregard.\nkiran cheedella\n\n______________________________________________________\nGet Your Private, Free Email at http://www.hotmail.com\n\n\n\n", "id": "lists-012-12363532"}, {"subject": "Working with COOKIES ...", "content": "Hi,\n \nI have an application wherein I am using a C/C++ browser control to download\nan HTML page from the internet. I want to know that the page/URL I am am\ninterested in, sets persistent cookies or not, as I am not interested in\nsession level cookies. For this I am using InternetGetCookie function call\nof Microsoft Internet Platform SDK. This gives me all the cookies relevant\nto that page as name value pairs, but it does not give me the expiry time of\nthat cookie.\n \nNow the question is .. how do I find out the expiry time/date for the cookie\n.. what is the API to find out the same ?\n \nAny pointers in this regard would be welcome.\n \nRegards,\nParag\n \n\n\n\n", "id": "lists-012-12370875"}, {"subject": "What's the mechanism for adding new error codes", "content": "When Mike Spreitzer <spreitze@parc.xerox.com> asked about\nstandardized authorization and authentication in HTTP something\nrang a bell.  I've been part of a group to implement authentication\nand authorization using either static or one-time passwords\ndepending on the remote location and or resource requested for\nan in-house system.  We've had to overload return codes (4XX and\n5XX), use convoluted HTML hacks, or create out of band\nconnections in order to achieve the simple challenge response\nmechanisms that most of us are use to everyday when a\npassword has expired or a token needs resynchronized.\n\nWith the world moving more toward a browser as their desktop\nfor large scale collaboration efforts.  How do we as developers\nand content providers let a user know that his or her password\nhas expired or that they are required to change their password\nbecause their temporary password was just that\n\"TEMPORARY\"?  You can make the providers / developers\nwrite workarounds that use HTML / Java, but it's not fixing the\nproblem.  The solution seems to be we add or enhance the\nerror returns that are in the protocol to allow for greater\nflexibility.\n\nWhat is the mechanism that will allow someone or group\nto add new error codes to the HTTP protocol?  Are the\nindividual W3O working groups like WWW security or\nWEBDAV tasked with adding each of their own codes\nand how do we referee the conflicts?\n\nSome examples:\n\n4XX   Password Change Required\n\nSelf explanatory.\n\n4XX   Next Passcode or Token\n\nFor use with one-time password smart\ncards like Axent or SecurID\n\n4XX   Reauthentication required\n\nArguably can use a 401, but this should allow\nfor more information to be displayed in the\npop-up / applet.\n\nand I know there have been others that have bounced through this\nlist as well.\n\nJust a thought before we close up shop!\n\nKevin Dyer\n\n===========================================================\nKevin J. Dyer     Draper Laboratory  MS 35\nEmail: <kdyer@draper.com>     555 Tech. Sq.\nPhone: 617-258-4962     Cambridge, MA 02139\nFAX: 617-258-2061\n---------------------------------------------------------------------------- \n------------------------------------------\n    _/_/_/_/    _/          _/  _/  _/        _/     _/_/_/_/\n   _/      _/   _/_/     _/_/  _/  _/_/     _/   _/\n  _/       _/  _/ _/   _/ _/  _/  _/  _/   _/    _/_/_/\n _/      _/   _/  _/ _/  _/  _/  _/    _/ _/            _/\n_/_/_/_/   _/    _/    _/  _/  _/        _/  _/_/_/_/\n        Data Management & Information Navigation Systems\n=========================================================== \n\n\n\n", "id": "lists-012-12377865"}, {"subject": "please remove me from this alia", "content": "please remove me from this alias\n\n\n\n", "id": "lists-012-12388098"}, {"subject": "Re: What's the mechanism for adding new error codes", "content": "http://www.ietf.org/internet-drafts/draft-ietf-tls-http-upgrade-04.txt\n\nis awaiting IESG  announcement of an Internet-wide last call -- it \nwas LC'd at TLS-WG and here.  It will establish a new IANA registry \nfor new reply codes.\n\nPolicies, forms, etc. can be found there...\n\nThanks,\nRohit\n\nPS. I'm conspicuously ducking the question of whether your intended \napplication is \"correct\" or not; this is just an answer to the \nquestion of \"how\"...\n\n\n\n", "id": "lists-012-12394597"}, {"subject": "Re: What's the mechanism for adding new error codes", "content": "At 06:45 PM 12/9/99 , Rohit Khare wrote:\n\n>http://www.ietf.org/internet-drafts/draft-ietf-tls-http-upgrade-04.txt\n>\n>is awaiting IESG  announcement of an Internet-wide last call -- it was \n>LC'd at TLS-WG and here.  It will establish a new IANA registry for new \n>reply codes.\n\nThanks,  I did notice the error registry component since the main topic of \nthe draft was TLS.\n\n\n>Policies, forms, etc. can be found there...\n>\n>Thanks,\n>Rohit\n>\n>PS. I'm conspicuously ducking the question of whether your intended \n>application is \"correct\" or not; this is just an answer to the question of \n>\"how\"...\n\n\nQuestion to the group:  Should we set aside a block of error codes in each \nrange for local requirements?  I compare this to\n                          the reserved blocks of IP addresses for us behind \nNATs or standalone systems.\n===========================================================\nKevin J. Dyer     Draper Laboratory  MS 35\nEmail: <kdyer@draper.com>     555 Tech. Sq.\nPhone: 617-258-4962     Cambridge, MA 02139\nFAX: 617-258-2061\n---------------------------------------------------------------------------- \n------------------------------------------\n    _/_/_/_/    _/          _/  _/  _/        _/     _/_/_/_/\n   _/      _/   _/_/     _/_/  _/  _/_/     _/   _/\n  _/       _/  _/ _/   _/ _/  _/  _/  _/   _/    _/_/_/\n _/      _/   _/  _/ _/  _/  _/  _/    _/ _/            _/\n_/_/_/_/   _/    _/    _/  _/  _/        _/  _/_/_/_/\n        Data Management & Information Navigation Systems\n=========================================================== \n\n\n\n", "id": "lists-012-12402889"}, {"subject": "RE: What's the mechanism for adding new error codes", "content": "why dont you just use the HTTP authentication scheme system and make use of\nthe auth-info header ?\n\n> -----Original Message-----\n> From: Kevin J. Dyer [mailto:kdyer@draper.com]\n> Sent: Monday, December 13, 1999 8:28 AM\n> To: IETF HTTP WG\n> Subject: Re: What's the mechanism for adding new error codes?\n> \n> \n> At 06:45 PM 12/9/99 , Rohit Khare wrote:\n> \n> >http://www.ietf.org/internet-drafts/draft-ietf-tls-http-upgra\n> de-04.txt\n> >\n> >is awaiting IESG  announcement of an Internet-wide last call \n> -- it was \n> >LC'd at TLS-WG and here.  It will establish a new IANA \n> registry for new \n> >reply codes.\n> \n> Thanks,  I did notice the error registry component since the \n> main topic of \n> the draft was TLS.\n> \n> \n> >Policies, forms, etc. can be found there...\n> >\n> >Thanks,\n> >Rohit\n> >\n> >PS. I'm conspicuously ducking the question of whether your intended \n> >application is \"correct\" or not; this is just an answer to \n> the question of \n> >\"how\"...\n> \n> \n> Question to the group:  Should we set aside a block of error \n> codes in each \n> range for local requirements?  I compare this to\n>                           the reserved blocks of IP addresses \n> for us behind \n> NATs or standalone systems.\n> ===========================================================\n> Kevin J. Dyer     Draper Laboratory  MS 35\n> Email: <kdyer@draper.com>     555 Tech. Sq.\n> Phone: 617-258-4962     Cambridge, MA 02139\n> FAX: 617-258-2061\n> --------------------------------------------------------------\n> -------------- \n> ------------------------------------------\n>     _/_/_/_/    _/          _/  _/  _/        _/     _/_/_/_/\n>    _/      _/   _/_/     _/_/  _/  _/_/     _/   _/\n>   _/       _/  _/ _/   _/ _/  _/  _/  _/   _/    _/_/_/\n>  _/      _/   _/  _/ _/  _/  _/  _/    _/ _/            _/\n> _/_/_/_/   _/    _/    _/  _/  _/        _/  _/_/_/_/\n>         Data Management & Information Navigation Systems\n> =========================================================== \n> \n\n\n\n", "id": "lists-012-12412066"}, {"subject": "Clarification  on cacheabilit", "content": "Heya,\n\nBackground:\nI'm working with the WAP folk on their end to end security proposal.\nOne of the pieces of this is the concept of a navigation document.\nThis navdoc aims to provide functionality like the 305/306 were\nintended to( but ended up not).  In short, the navdoc is to be\nreturned when accessing a resource that requires an alternate\nproxy or path which may include various, lets say, other infrastructure.\n\nSo, the question we have come to is what HTTP response code should\nbe returned by a server when the navdoc is returned (instead of\nthe resource itself)?\n\nWhen the client receives a navigation document, instead of the content\nresource it actually asked for, it is supposed to interpret the navdoc\nand perform the necessary steps such as connect to X proxy, hang up\nand dial a different PPP serve, etc..  Then it essentially retries the\nrequest through that new chain.\n\nThe current plan is to use 200 OK as the response code for simplicity\nand cacheability.\n\nI don't necessarily think this is the best way to go, so I have \nbeen advocating using either an existing 3xx code or a new 3xx code.\nThe only problem using a 3xx code is cacheability.   There are\ntwo sections of the HTTP draft which seem conflicting on this.\n\nSection 6.1 says that any unknown 3xx header must be treated\nas 300, but NOT cached.\nSection 13.4 seems to impy that if you have a cache-control: \nheader then you may cache it.\nThese sections are quoted at the bottom of this message.\n\nQuestion:\nSo, my question essentially comes down to this:\nIf I invent a new 3xx status code, or overload an existing 3xx\ncode but I include a cache-control: header to explicitly\nallow caching, can a cache cache the results?\n\nSection 6.1\nHTTP status codes are extensible. HTTP applications are not required to\nunderstand the meaning of all registered status codes, though such\nunderstanding is obviously desirable. However, applications MUST understand\nthe class of any status code, as indicated by the first digit, and treat any\nunrecognized response as being equivalent to the x00 status code of that\nclass, with the exception that an unrecognized response MUST NOT be cached.\nFor example, if an unrecognized status code of 431 is received by the\nclient, it can safely assume that there was something wrong with its request\nand treat the response as if it had received a 400 status code. In such\ncases, user agents SHOULD present to the user the entity returned with the\nresponse, since that entity is likely to include human- readable information\nwhich will explain the unusual status. \n\nsection 13.4\nA response received with any other status code (e.g. status codes 302 \n   and 307) MUST NOT be returned in a reply to a subsequent request \n   unless there are cache-control directives or another header(s) that \n   explicitly allow it. For example, these include the following: an \n   Expires header (section 14.21); a \"max-age\", \"s-maxage\",  \"must- \n   revalidate\", \"proxy-revalidate\", \"public\" or \"private\" cache-control \n   directive (section 14.9). \n\n---\nJosh Cohen\n\n\n\n", "id": "lists-012-12422202"}, {"subject": "Re: Clarification on cacheabilit", "content": ">So, my question essentially comes down to this:\n>If I invent a new 3xx status code, or overload an existing 3xx\n>code but I include a cache-control: header to explicitly\n>allow caching, can a cache cache the results?\n\nOnly if the cache recognizes (understands the semantics of) the\nnew status code.\n\nRationale: Consider the addition of the 206 status code (partial content).\nThe contents are cacheable, but only by a cache that understands that\nthe contents are a splice of the cacheable content and not the content\nitself.  Thus, unrecognized status code responses must not be cached,\nregardless of the presence of cache-control, since the contents of\ncache-control are intended for those recipient that do recognize the code.\n\nBTW, this is what you want for the WAP answer.  If they try to use anything\nother than a 3xx, they are on their own.\n\n....Roy\n\n\n\n", "id": "lists-012-12432519"}, {"subject": "RE: Clarification on cacheabilit", "content": "> -----Original Message-----\n> From: Roy T. Fielding [mailto:fielding@kiwi.ICS.UCI.EDU]\n> Sent: Wednesday, December 22, 1999 11:55 AM\n> To: Josh Cohen (Exchange)\n> Cc: HTTP-WG (E-mail)\n> Subject: Re: Clarification on cacheability \n> \n> \n> >So, my question essentially comes down to this:\n> >If I invent a new 3xx status code, or overload an existing 3xx\n> >code but I include a cache-control: header to explicitly\n> >allow caching, can a cache cache the results?\n> \n> Only if the cache recognizes (understands the semantics of) the\n> new status code.\n> \n> BTW, this is what you want for the WAP answer.  If they try \n> to use anything\n> other than a 3xx, they are on their own.\n>\nThanks for the response.\nSo, what do you mean by that?  SHould they re-use a 3xx code that allows\ncaching ? Which of those would be best ? \nThe idea of using 305 Use proxy came up, what do you think of that?\n\nIn regard to your 206 rationale..\nThe 206 is a clear case of why you dont want to cache, but by default\nit wouldnt be.  Actively adding cache-control would instruct the cache to\ncache it.  However, that would be broken, thus you shouldnt send cache\ncontrol\nwith 206.  In this (the wap case), caching is what you want, so sending a\ncache\ncontrol would be the desired result if the cache cached it.\nIf the default is to not cache it, why not allow cache control to \nallow caching if either: the cache understands the response code OR\nif the response is safe to cache \"in the normal way\"?\n\n> ....Roy\n> \n\n\n\n", "id": "lists-012-12440469"}, {"subject": "Re: Clarification on cacheabilit", "content": "Josh Cohen writes:\n    In regard to your 206 rationale..\n    The 206 is a clear case of why you dont want to cache, but by\n    default it wouldnt be.  Actively adding cache-control would\n    instruct the cache to cache it.  However, that would be\n    broken, thus you shouldnt send cache control with 206.  In\n    this (the wap case), caching is what you want, so sending a\n    cache control would be the desired result if the cache cached\n    it.\n\nI'm not sure I agree.  I don't see any reason why a 206 response\nshould not be cached by a proxy that \"recognizes\" the response.\n(The language that you quoted from section 6.1, \"an unrecognized\nresponse MUST NOT be cached\", is a little vague about what\n\"recognize\" means, but I think we probably agree on that.)\n\nFor example, an HTTP/1.1 server should be able to send\n\nHTTP/1.1 206 Partial Content\nDate: whatever\nContent-Range: whatever\nEtag: \"whatever\"\nCache-control: max-age=1138\n\nThis means that\n(1) a proxy that does not \"understand\" 206 responses\nMUST NOT cache it -- since otherwise the partial\ncontent might be returned to an equally naive HTTP/1.0\nbrowser.\n(2) a proxy that does understand 206 responses can\nonly cache it for 1138 seconds.\n\nIf we followed your interpretation, there would be no way\nfor a server to put restrictions on the caching of an\nHTTP/1.1-specific response (via Cache-Control or Expires)\nwithout allowing HTTP/1.0 caches to store it - yuck.\n\n    If the default is to not cache it, why not allow cache\n    control to allow caching if either: the cache understands the\n    response code OR if the response is safe to cache \"in the\n    normal way\"?\n\nI suppose we could have defined a cache-control directive that\nsays \"this response is safe to cache by a proxy that\ndoesn't actually understand the response status code\", but I think\nthat would not have been very easy to specify.\n\nI actually do not understand why you want to treat the \"navdoc\"\nas a 3xx \"Redirection\" response instead of treating it as\nopaque (to HTTP) content.  You haven't provided quite enough\ncontext to explain why this mechanism should be integrated\nwith HTTP, instead of layered above it.\n\n-Jeff\n\n\n\n", "id": "lists-012-12450823"}, {"subject": "RE: Clarification on cacheabilit", "content": "so do you think that a server should just\nrespond 200 ok with the navigation document instead of\nthe content-body ?\n\n\n> -----Original Message-----\n> From: Jeffrey Mogul [mailto:mogul@pa.dec.com]\n> Sent: Wednesday, December 22, 1999 1:16 PM\n> To: Josh Cohen (Exchange)\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: Clarification on cacheability \n> \n> \n> Josh Cohen writes:\n>     In regard to your 206 rationale..\n>     The 206 is a clear case of why you dont want to cache, but by\n>     default it wouldnt be.  Actively adding cache-control would\n>     instruct the cache to cache it.  However, that would be\n>     broken, thus you shouldnt send cache control with 206.  In\n>     this (the wap case), caching is what you want, so sending a\n>     cache control would be the desired result if the cache cached\n>     it.\n> \n> I'm not sure I agree.  I don't see any reason why a 206 response\n> should not be cached by a proxy that \"recognizes\" the response.\n> (The language that you quoted from section 6.1, \"an unrecognized\n> response MUST NOT be cached\", is a little vague about what\n> \"recognize\" means, but I think we probably agree on that.)\n> \n> For example, an HTTP/1.1 server should be able to send\n> \n> HTTP/1.1 206 Partial Content\n> Date: whatever\n> Content-Range: whatever\n> Etag: \"whatever\"\n> Cache-control: max-age=1138\n> \n> This means that\n> (1) a proxy that does not \"understand\" 206 responses\n> MUST NOT cache it -- since otherwise the partial\n> content might be returned to an equally naive HTTP/1.0\n> browser.\n> (2) a proxy that does understand 206 responses can\n> only cache it for 1138 seconds.\n> \n> If we followed your interpretation, there would be no way\n> for a server to put restrictions on the caching of an\n> HTTP/1.1-specific response (via Cache-Control or Expires)\n> without allowing HTTP/1.0 caches to store it - yuck.\n> \n>     If the default is to not cache it, why not allow cache\n>     control to allow caching if either: the cache understands the\n>     response code OR if the response is safe to cache \"in the\n>     normal way\"?\n> \n> I suppose we could have defined a cache-control directive that\n> says \"this response is safe to cache by a proxy that\n> doesn't actually understand the response status code\", but I think\n> that would not have been very easy to specify.\n> \n> I actually do not understand why you want to treat the \"navdoc\"\n> as a 3xx \"Redirection\" response instead of treating it as\n> opaque (to HTTP) content.  You haven't provided quite enough\n> context to explain why this mechanism should be integrated\n> with HTTP, instead of layered above it.\n> \n> -Jeff\n> \n\n\n\n", "id": "lists-012-12460261"}, {"subject": "Re: Clarification on cacheabilit", "content": "Josh Cohen writes:\n    so do you think that a server should just\n    respond 200 ok with the navigation document instead of\n    the content-body ?\n    \nSeems plausible to me, but I really don't know enough about\nthe details of the \"navigation document\".\n\nYou were the one who wanted to change from 200 to 3xx - why?\n\n-Jeff\n\n\n\n", "id": "lists-012-12471442"}, {"subject": "Re: Clarification on cacheabilit", "content": ">So, what do you mean by that?  SHould they re-use a 3xx code that allows\n>caching ? Which of those would be best ? \n>The idea of using 305 Use proxy came up, what do you think of that?\n\nI'd just define a new 3xx code and use that.\n\n>In regard to your 206 rationale..\n>The 206 is a clear case of why you dont want to cache, but by default\n>it wouldnt be.  Actively adding cache-control would instruct the cache to\n>cache it.  However, that would be broken, thus you shouldnt send cache\n>control\n\nNo, you are confusing the requirements.  A cache that doesn't understand 206\nwon't cache it because of the MUST NOT on unrecognized codes.  A cache\nthat does understand 206 will look at the cache-control field.\n\nThese are forward-looking requirements: we don't know what the semantics\nof the new code may be, and this is the only way to specify proper handling\nof future sementics in the presence of intermediary caches.\n\nI think the WAP folks are confused in any case -- it would be foolish to\nmark a client-specific redirection response as cacheable by anything\nless than a fully-compliant 1.1 proxy with Vary, which itself is so rare\nthat they should just assume that their new code will be understood by\nany cache that might gain from caching it.\n\n....Roy\n\n\n\n", "id": "lists-012-12478591"}, {"subject": "RE: Clarification on cacheabilit", "content": "> -----Original Message-----\n> From: Roy T. Fielding [mailto:fielding@kiwi.ICS.UCI.EDU]\n> Sent: Wednesday, December 22, 1999 2:35 PM\n> To: Josh Cohen (Exchange)\n> Cc: HTTP-WG (E-mail)\n> Subject: Re: Clarification on cacheability \n> \n> \n> >So, what do you mean by that?  SHould they re-use a 3xx code \n> that allows\n> >caching ? Which of those would be best ? \n> >The idea of using 305 Use proxy came up, what do you think of that?\n> \n> I'd just define a new 3xx code and use that.\n>\nthat sounds good, except that existing caches cant cache it.\n \n> >In regard to your 206 rationale..\n> >The 206 is a clear case of why you dont want to cache, but by default\n> >it wouldnt be.  Actively adding cache-control would instruct \n> the cache to\n> >cache it.  However, that would be broken, thus you shouldnt \n> send cache\n> >control\n> \n> No, you are confusing the requirements.  A cache that doesn't \n> understand 206\n> won't cache it because of the MUST NOT on unrecognized codes.  A cache\n> that does understand 206 will look at the cache-control field.\n> \n> These are forward-looking requirements: we don't know what \n> the semantics\n> of the new code may be, and this is the only way to specify \n> proper handling\n> of future sementics in the presence of intermediary caches.\n> \n> I think the WAP folks are confused in any case -- it would be \n> foolish to\n> mark a client-specific redirection response as cacheable by anything\n> less than a fully-compliant 1.1 proxy with Vary, which itself \n> is so rare\n> that they should just assume that their new code will be understood by\n> any cache that might gain from caching it.\n> \nBut any cache can cache it as long as it obeys the cache-control/expires\nheader.\nThe navdoc is safe to cache..\n> ....Roy\n> \n\n\n\n", "id": "lists-012-12487234"}, {"subject": "Re: Clarification on cacheabilit", "content": ">But any cache can cache it as long as it obeys the cache-control/expires\n>header.\n>The navdoc is safe to cache..\n\nIf one client is led to believe that they have received a representation\nof the requested resource, while another client is led to believe that\nthey have received instructions on how to navigate to such a representation,\nthen the navdoc is not safe for a shared cache.  If, however, the target\nof the request is supposed to be navigation instructions, then the\nresponse should be 200 (and thus be cacheable by default).\n\n....Roy\n\n\n\n", "id": "lists-012-12497213"}, {"subject": "RE: Clarification on cacheabilit", "content": "> -----Original Message-----\n> From: Roy T. Fielding [mailto:fielding@kiwi.ICS.UCI.EDU]\n> Sent: Wednesday, December 22, 1999 5:15 PM\n> To: Josh Cohen (Exchange)\n> Cc: HTTP-WG (E-mail)\n> Subject: Re: Clarification on cacheability \n> \n> \n> >But any cache can cache it as long as it obeys the \n> cache-control/expires\n> >header.\n> >The navdoc is safe to cache..\n> \n> If one client is led to believe that they have received a \n> representation\n> of the requested resource, while another client is led to believe that\n> they have received instructions on how to navigate to such a \n> representation,\n> then the navdoc is not safe for a shared cache.  \n\nthis is the case.  This is why I started the discussion since I thought\nsending back 200 OK (with the unexpected navdoc) was a mistake.\n\nAs usual, your comments are excellent.  \nif a GET to a URL ALWAYS results in a (unexpected navdoc), then\nis it really safe to cache?\nI guess if there is any chance that the response could be different,\nbased on client auth, client type, or whatever, then it is not safe to\ncache. (especially since caches dont filter based on accept before\nreturning responses)\nIts likely that one client who passes an accept: navdoc could\ncause the cache to cache a navdoc.  Later a client that does not\npass the accept: navdoc would incorrectly get a navdoc that it\ncant understand.  Thus the caching is broken.\n\n\n\n", "id": "lists-012-12505145"}, {"subject": "Re: Clarification on cacheabilit", "content": "Josh Cohen:\n>\n[...]\n>I guess if there is any chance that the response could be different,\n>based on client auth, client type, or whatever, then it is not safe to\n>cache. \n\nIt _is_ safe to make the response cacheable as long as you use Vary\ncorrectly, this is what Vary was invented for.\n\n>(especially since caches dont filter based on accept before\n>returning responses)\n\nAs Roy said, most (all?) 1.1 caches don't actually implement the\nrefined filtering made possible by Vary.  They implement the Vary\nrequirements in 1.1 by treating 'Vary: anything' as equivalent to\n'no-cache'.\n\nBut you can still use Vary if you want to help possible future caches:\nI would consider this to be good protocol design.  You can find some\nexamples of the use of Vary+Expires in RFC2295.\n\n\nKoen.\n\n\n\n", "id": "lists-012-12515019"}, {"subject": "server parsing of URI path parameter", "content": "I've been looking at use of parameters on path segments of URIs, as\ndiscussed in RFC2396 (section 3.3).\n\nAs I understand it, the original reason that parameters were used, and\ntherefore the semicolon was a reserved character, is pre-HTTP/1.1 range\nspecification. I'm not clear on their current use, and was a bit surprised\nto see 2396 say that each path segment could have its own parameters, so\nthat you could get:\n  http://www.foo.com/bar;a=1;b=2/baz/bat.gif;g=5\nand so forth. If anyone could clear up the history here, I'd be grateful.\n\nWith that in mind, a more HTTP-specific question:\nHow should an origin server treat a request that includes parameters? Some\ntrival testing suggests that current practice is to treat them as part of\nthe path, so that an error is returned. Would it be better to have them\nignore parameters that aren't understood? \n\nI suspect the answer depends on the purpose and uses of parameters, but I'm\ninterested to get some views on this.\n\nThanks,\n\n-- \nMark Nottingham, Melbourne Australia \nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-12523385"}, {"subject": "questions regarding draft-ietf-http-authentication0", "content": "Two questions regarding draft-ietf-http-authentication-01:\n\n1)  Section 3.2.2, request-digest description:\n\nIf the \"qop\" value is \"auth\":\n\n   request-digest  = <\"> < KD ( H(A1),     unq(nonce-value)\n       \":\" nc-value\n       \":\" unq(cnonce-value)\n       \":\" unq(qop-value)\n       \":\" H(A2)\n       ) <\">\n\n    Shouldn't that be\n\nIf the \"qop\" value is \"auth\" or \"auth-int\":\n\n    ? Otherwise the calculation of request-digest isn't defined for\n    qop auth-int.\n\n\n2)  Section 3.2.2, \"MD5-sess\" description:\n\nThis creates a 'session key' for the authentication of subsequent\nrequests and responses which is different for each session, thus\nlimiting the amount of material hashed with any one key. ...\n\n    How long does a session last? I.e. when should this session key be\n    discarded? When the server sends a new nonce or new algorithm?\n\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1252840"}, {"subject": "http-v11-spec-rev03; proxy 100 (Continue) issu", "content": "In section 8.2.4, Requirements for HTTP/1.1 proxies, it says:\n\n\"- If the proxy knows that the version of the next-hop server is\nHTTP/1.0 or lower, it MUST NOT forward the request, and it MUST respond\nwith a 417 (Expectation Failed) status.\"\n\nThe problem is with the normative MUSTs, which we think ought to be\nat most normative SHOULDs (and perhaps this bullet should just be\nremoved).  The reasons are:\n- There is no interoperability issue here, since if the version of the\n  next-hop server is unknown, the request, including the Expect header\n  field, MUST be forwarded, and the client needs to be able to (for\n  compatibility reasons) time out waiting for the 100 (Continue) and\n  send the body anyway.\n- The likely behaviour of a client, in the case of recieving a 417 with\n  out some reason (like a challenge for credentials), is to just retry\n  without the Expect header, so the only effect is to increase latency.\n\nOur proxy implementation currently will just generate a 100 (continue)\nin this case.\n\nI apologize for not catching this earlier (I checked and it goes at\nleast back to rev-01).\n\n\nAlso, in the same section, it says:\n\n\"Proxies SHOULD maintain a cache recording the HTTP version numbers\nfrom recently-referenced next-hop servers.\"\n\nThis is not an interoperability issue either, and so we do not think\nthe \"SHOULD\" ought to be normative.  Perhaps it ought to be phrased as\nan \"encouragement\", as advice to implementers is elsewhere.\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-1261444"}, {"subject": "RE: server parsing of URI path parameter", "content": "> From: Mark Nottingham [mnot@pobox.com]\n> To: http-wg@hplb.hpl.hp.com\n> Subject: server parsing of URI path parameters\n\n> I've been looking at use of parameters on path segments of URIs,\n> as discussed in RFC2396 (section 3.3).\n> [...]\n> With that in mind, a more HTTP-specific question: How should an\n> origin server treat a request that includes parameters?\n\nAny confusion here results from equating 'URL path' to 'file path';\nit need not be.  Something in the server is responsible for\ninterpreting a path - doing so as a file path is just one\npossibility.\n\n> I suspect the answer depends on the purpose and uses of\n> parameters, but I'm interested to get some views on this.\n\nExactly so.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12615763"}, {"subject": "Re: server parsing of URI path parameter", "content": "In message <19991230095914.A436@i.mnot.net>, Mark Nottingham writes:\n>\n>I've been looking at use of parameters on path segments of URIs, as\n>discussed in RFC2396 (section 3.3).\n>\n>As I understand it, the original reason that parameters were used, and\n>therefore the semicolon was a reserved character, is pre-HTTP/1.1 range\n>specification.\n\nActually, no, they were introduced in RFC 1808 long before the\nrange hack.  Before that, I'd say they were most influenced by\nthe deisre to support versioning a la VMS.\n\n>I'm not clear on their current use, and was a bit surprised\n>to see 2396 say that each path segment could have its own parameters, so\n>that you could get:\n>  http://www.foo.com/bar;a=1;b=2/baz/bat.gif;g=5\n>and so forth. If anyone could clear up the history here, I'd be grateful.\n\nThink of it as an issue of precedence.  In almost all URL parsers,\nthe slash characters have precedence over any other character during\nroutine parsing of components.  The result is that the path is\nseparated into path segments before anything looks at the semicolons.\n\n>With that in mind, a more HTTP-specific question:\n>How should an origin server treat a request that includes parameters? Some\n>trival testing suggests that current practice is to treat them as part of\n>the path, so that an error is returned. Would it be better to have them\n>ignore parameters that aren't understood? \n\nThat is entirely dependent on the nature of the resource.  A server\ncan interpret its own namespace however it wants.\n\nCheers,\n\n....Roy\n\n\n\n", "id": "lists-012-12624350"}, {"subject": "(no subject", "content": "JOIN ME AT THE TOP\n\n\nMy name is Robert Todd \n\nThe Golden Era Of Internet Marketing Has Arrived! \n\nWill You Be Part Of It? \n\nHow Can You? \n\nHaving the potential to reach millions with relatively little\ninvestment, the Internet provides opportunities for mass \nmarketing that\nhave never been seen before! You can now compete on the same \nlevel as\nthe Big Boys. \n\nYour presence on the World Wide Web can be as great as the \nbiggest of\nthe big corporations -- \nIF YOU HAVE THE TOOLS - and -\nIF YOU KNOW HOW TO USE THEM! \n\n  You now have before you the Opportunity to get in on the Ground\nFloor of a BRAND NEW Network Marketing Opportunity? Have  you \never\nwished you could have started out in the beginning with Nu-Skin, \nAmway\nor any of the other networking Giants? \n\n  Our plan is to bring massive signups into our program. \n\nThe support and tools we provide are unequaled . These tools have \nbeen\nused by networkers to build enormous downlines in short periods \nof time.\n(One Networker built a downline of 80,000 in 12 months while \nworking a\nfull time job, using our tools). \n\nAnd I promise you will not find a more qualified nor will you \nfind a\nmore supportive upline anywhere in the world. \n\nWhat we are offering is a business with the following     * FREE\nState of the art webpage \n    * FREE On hands Training.\n    * FREE Tremendous Upline Support! \n    * FREE Step-By-Step Business Building Plan!     *\nFREE Opportunity Business Conferences Daily!     * FREE take\nadvantage of our unlimited lead generating system. \n\n    Also Everyone Who Joins Us can have \n* Merchant account - 99% approval \n* Web Hosting \n* Flat Rate Long Distance Telephone Service for only $39.99 per \nmonth. \n\n* You can add unlimited calling card for additional $59.99 \nMonthly. This\nservice is 24 hours X 7 days a week only in the lower 48 states \n\nWhen you decide to take advantage of this Opportunity, you will:\n    * HAVE the Potential to Earn an unlimited Income!\n    * HAVE a Golden \nOpportunity to Secure Your Financial Future! \n\n    * AND finally earn the Money most \n\nNetworkers only Dream about \n\n    * Be Mentored by networking Leaders from Nu-Skin, Amway,\nExcel, Etc. \n\n    * Unlimited access to all our leaders.     * \n\n$5,500 Bonus commission on 6 sales! No limit on $5,500 Bonuses. \n\nBe a part of the fastest growing market in the world! \n\nAbout the Internet: \n\nThe Internet has grown from 2 million users in 1993 to over 70 \nmillion\nusers today \n\nExperts project that there will be 500 million users by the year \n2000\nThe number of new users on the \"Net\" doubles every 8 months Every \ncable,\nlong distance company and \"Baby Bell\" will insure that all \nAmericans\nhave easy access to the Internet including TV set access through \n\"Web\nTV\" providers We Provide a complete \"Marketplace on the Internet\" \nfor\nhome and business consumers \n\n-This is a precious commodity. Busy consumers are looking for \nfaster,\nmore hassle free ways to do their shopping- According to The \nKiplinger\nLetter, May 9, 1997, the number of firms currently using the \nInternet to\nbuy or sell is growing 10% per month... doubling every seven \nmonths! \n\nMake sure you DO NOT miss this new and exciting opportunity that \nis\ntaking the Internet \nby storm! \n\n    WE ALREADY HAVE MLM LEADERS JOINING OUR COMPANY DAILY.\nSECURE YOUR POSITION NOW FOR A MOST REASONABLE JOINING FEE! \n\nIf interested please fill in info below and I'll get back to you \nASAP. \n\nSend to kahuna-info2@online-venture.com \n\nName: \n\nEmail address: \n\nTelephone: \n\nBest time to call: \n\nSincerely yours, \n\n\nRobert Todd \n\n~~~~~~~~~~~~~~~~~~~~ \nRobert Todd ICQ#1769450\nPO Box 3077\nOneco, FL 34205 USA\ncyber-kahuna@webtv.net \n\nhttp://www.freeyellow.com/members/irongate/index.htm\n \nToll Free VoiceMail 877-817-6020\nFAX 310-495-0415 \n\n\n\n \n \n \n \n \n \n\n\n\n", "id": "lists-012-12633245"}, {"subject": "unsubscrib", "content": "unsubscribe\n\nN.SatyaVani \nA.M.T.S \nChiplogic India Pvt. Ltd. \nPlot No.144,2nd Floor,Akashganga Chambers \nSrinagar Colony ,Hyderabad :500073  (A.P.) \nTel: +91(40)3750251,3750252 (Off)\n                   3040203(Res)\nEmail:vani@chiplogic.com      (Official) \n\n\n\n", "id": "lists-012-12643261"}, {"subject": "draft-ietf-tls-httpupgrade reissue", "content": "The secretariat didn't forward this announcement to this list, so I\nwill.\n\nThe only change from version -04 is the addition of the standard\nboilerplate and references  regarding the keywords (MUST, SHOULD,\nMAY, etc).\n\nA New Internet-Draft is available from the on-line Internet-Drafts\ndirectories.\nThis draft is a work item of the Transport Layer Security Working\nGroup of the IETF.\n\nTitle: Upgrading to TLS Within HTTP/1.1\nAuthor(s): R. Khare, S. Lawrence\nFilename: draft-ietf-tls-http-upgrade-05.txt\nPages: 13\nDate: 05-Jan-00\n\nThis memo explains how to use the Upgrade mechanism in HTTP/1.1 to\ninitiate Transport Layer Security (TLS) over an existing TCP\nconnection. This allows unsecured and secured HTTP traffic to share\nthe same well known port (in this case, http: at 80 rather than\nhttps: at 443). It also enables 'virtual hosting,' so a single HTTP\n+ TLS server can disambiguate traffic intended for several hostnames\nat a single IP address.\nSince HTTP/1.1[1] defines Upgrade as a hop-by-hop mechanism, this\nmemo also documents the HTTP CONNECT method for establishing\nend-to-end tunnels across HTTP proxies. Finally, this memo\nestablishes new IANA registries for public HTTP status codes, as\nwell as public or private Upgrade product tokens.\n\nA URL for this Internet-Draft is:\nhttp://www.ietf.org/internet-drafts/draft-ietf-tls-http-upgrade-05.t\nxt\n\nInternet-Drafts are also available by anonymous FTP. Login with the\nusername\n\"anonymous\" and a password of your e-mail address. After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-tls-http-upgrade-05.txt\".\n\nA list of Internet-Drafts directories can be found in\nhttp://www.ietf.org/shadow.html\nor ftp://ftp.ietf.org/ietf/1shadow-sites.txt\n\n\nInternet-Drafts can also be obtained by e-mail.\n\nSend a message to:\nmailserv@ietf.org.\nIn the body type:\n\"FILE /internet-drafts/draft-ietf-tls-http-upgrade-05.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail\nreaders\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12649684"}, {"subject": "HTTP/proxy call", "content": "Hello, I just subscribed to this list.  It didn't look that\nactive(archives), I hope I am wrong.\n\nOf course I have a question.  GET http://www.webpage.com/download.zip\nHTTP/1.0 is a valid get request for file download.zip.  Now, how does the\nSERVER have to handle this?  Assuming the file exists.  What I mean is, when\ni click on the link in a browser(IE 5.01), I am prompted for the SaveAs/Run\nfrom current.  Well I wrote the web server, so I know that I simply send the\nfile back to the requestor.  If the file is small(2000 bytes) then it is\ncompletely downloaded before the user is able to click anything. I know my\nquestion is vague, but 1. I am wondering if there are other ways that a\nserver can distribute the file?  Also, 2. what are my security concerns when\nmaking a request like this on the BROWSER end, if any?\n\nFinally, 3.  How do I keep a Proxy from caching a file like this?  what if\nit is a 400 meg file?\n\nThanks much for any help at all.  Am I in the correct list?\n\n\n\n", "id": "lists-012-12659147"}, {"subject": "test mail dont read thi", "content": "test\n\n\n\n=================================================\nJeongKook, Seo (mmouse@web114.co.kr)\nArisoo Internet Inc.\nWebDeveloment Team\n\nTel : +82-2-525-8386\nFax : +82-2-3474-4587\n\nhttp://www.web14.co.kr\n=================================================\n\n\n\n", "id": "lists-012-12666240"}, {"subject": "webmail vulnerabilities: a new pragma token", "content": "Before making this suggestion to client app vendors, I would very much\nappreciate the comments of this working group.\n\nBackground:\n\nOn the Bugtraq security discussion mailing list[1], there has been much\nconversation of late about webmail vulnerabilities. Essentially, the\nwebmail sites offer HTTP/HTML frontends to read Internet mail. They\nnormally can display HTML-encoded email. Such systems usually try to\nremove all scripting code from email before displaying it. This is to\nprevent those scripts from being executed in a way that might exploit\ncurrent client scripting lnguage problems, or simply exploit the trust\nthat a user might normally place in the site running the webmail frontend.\n\nSuggestion:\n\nIt would be nice if there were on an HTTP header that, if sent to the\nclient, would cause the client to disable javascript, vbscript, etc. for\nthat document only. Sites who wished to display untrusted pages (webmail\nsites, web discussion forums, etc.) could then use a multi-frame layout.\nAny frame that contained untrusted code would have this header included in\nthe delivery of its content to ensure that the scripts would not be\nevaluated, regardless of the normal client settings; other frames, whose\n\"trusted\" documents would be sent without this header, would still be able\nto use scripting (if enabled on the client).\n\nMay I suggest\n\nPragma: disable-scripting\n\nwhich I suppose means a no-cache page would be sent with\n\nPragma: no-cache, disable-scripting\n\nPer RFC 2616, all Pragma headers must be passed to the client by all proxy\nserver or gateway applications. So this header would be passed to the\nclient application, as desired. But is it an acceptable use for \"Pragma\"?\n\nComments, suggestions?\n\n-Peter\n\nhttp://www.bastille-linux.org/ : working towards more secure Linux systems\n\n[1] http://www.securityfocus.com/\n\n\n\n", "id": "lists-012-12672978"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "Hello all,\n\nTo get straight to the point.  I think the stated is the reason Pragma exists. \n It is a perfectly apropos usage and addresses the problems discussed on \nBugTraq directly and efficiently.  However, this does not completely address \nthe issues in implementation of the clients and their treatment of these \nWebMail systems nor the treatment of proxies concerning those systems.  Perhaps \na poll of those providers would glean some information concerning the current \ntreatment or their client expectations as to the treatment of the 'un-trusted' \ncontent types.  In any event, this seems to be at least a relevant and \nappropriate use of Pragma as stipulated in RFC 2616.\n\nEric\n\nEric Williams, Pres.\nInformation Brokers, Inc.    Phone: +1 202.889.4395\nhttp://www.infobro.com/        Fax: +1 202.889.4396\nmailto:eric@infobro.com      Pager: +1 301.303.8998\n           For More Info: info@infobro.com\n\n\nOn Wednesday, January 19, 2000 8:45 AM, Peter W [SMTP:peterw@usa.net] wrote:\n>\n> Before making this suggestion to client app vendors, I would very much\n> appreciate the comments of this working group.\n>\n> Background:\n>\n> On the Bugtraq security discussion mailing list[1], there has been much\n> conversation of late about webmail vulnerabilities. Essentially, the\n> webmail sites offer HTTP/HTML frontends to read Internet mail. They\n> normally can display HTML-encoded email. Such systems usually try to\n> remove all scripting code from email before displaying it. This is to\n> prevent those scripts from being executed in a way that might exploit\n> current client scripting lnguage problems, or simply exploit the trust\n> that a user might normally place in the site running the webmail frontend.\n>\n> Suggestion:\n>\n> It would be nice if there were on an HTTP header that, if sent to the\n> client, would cause the client to disable javascript, vbscript, etc. for\n> that document only. Sites who wished to display untrusted pages (webmail\n> sites, web discussion forums, etc.) could then use a multi-frame layout.\n> Any frame that contained untrusted code would have this header included in\n> the delivery of its content to ensure that the scripts would not be\n> evaluated, regardless of the normal client settings; other frames, whose\n> \"trusted\" documents would be sent without this header, would still be able\n> to use scripting (if enabled on the client).\n>\n> May I suggest\n>\n> Pragma: disable-scripting\n>\n> which I suppose means a no-cache page would be sent with\n>\n> Pragma: no-cache, disable-scripting\n>\n> Per RFC 2616, all Pragma headers must be passed to the client by all proxy\n> server or gateway applications. So this header would be passed to the\n> client application, as desired. But is it an acceptable use for \"Pragma\"?\n>\n> Comments, suggestions?\n>\n> -Peter\n>\n> http://www.bastille-linux.org/ : working towards more secure Linux systems\n>\n> [1] http://www.securityfocus.com/\n\n\n\n", "id": "lists-012-12683453"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "> From: Peter W\n> Subject: RE: webmail vulnerabilities: a new pragma token?\n\n> It would be nice if there were on an HTTP header that, if sent to\n> the client, would cause the client to disable javascript,\nvbscript,\n> etc. for that document only. Sites who wished to display untrusted\n> pages (webmail sites, web discussion forums, etc.) could then use\na\n> multi-frame layout.  Any frame that contained untrusted code would\n> have this header included in the delivery of its content to ensure\n> that the scripts would not be evaluated, regardless of the normal\n> client settings; other frames, whose \"trusted\" documents would be\n> sent without this header, would still be able to use scripting (if\n> enabled on the client).\n\n  The problem with Pragma as an extension mechanism is that there is\n  no way for the server to know whether or not the client\nunderstands\n  any particular pragma token, so it becomes an unreliable\nmechanism.\n  In this case, the server can send 'disable-scripting', but it\ncan't\n  tell whether or not that will have any effect.  Worse - today it\n  can be assured that it will not, since no browsers implement it.\n\n  The degree of trust that a user should have in scripts, as this\n  example illustrates, is really a property of the script itself, or\n  perhaps of the containing document, not of the server from which\nit\n  is obtained.  There are already mechanisms available for signing\n  email, so if anything we should be looking for ways for browsers\nto\n  make the trust decisions appropriately - based on the document,\nnot\n  the web server.\n\n  As an interim solution for the webmail sites today, I'd suggest\nthat\n  you've already got the basis for a solution.  Serve the framework\n  that you want the user to trust from 'webmailbox.example.com', and\n  then serve the content of the mail frame from 'mail.example.com'.\n  Instruct users to trust 'webmailbox' and not to trust 'mail'.  A\n  solution like this can be implemented with many of todays browsers\n  with no protocol change.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12694669"}, {"subject": "comments on draft-ietf-http-v11-spec-rev0", "content": "Here are my comments on draft-ietf-http-v11-spec-rev-03.\n\nDave Kristol\n==============\n\nSubstantive:\n\n10.2.5 204 No Content\n\nThe server has fulfilled the request but does not need to return an\nentity-body, and may want to return updated metainformation. The\nresponse MAY include new or updated metainformation in the form of\nentity-headers, which if present SHOULD be associated with the requested\nvariant.\n\n    Is it appropriate to sent an Etag with 204?  (I think so.)  If so,\n    then, because Etag is no longer an entity header, the wording needs\n    to be amended.\n\n14.26 If-None-Match\n    I was disappointed that the pseudo-code didn't make it.  OTOH,\n    there are now disclaimers about mixing various conditional\n    headers, which accomplishes the same thing.\n    \n    I would like to see stronger advice in 13.3.4 about what\n    combinations of conditional headers a client can send reliably to a\n    server (or which combinations it must not send).\n\nNits:\n\n10.2.7 206 Partial Content\n    .  Either a Content-Range header field (section 14.16) indicating the\n                        ===== --> Courier  (In the .ps version, this is\nRoman.)\n       range included with this response, or a multipart/byteranges\n       Content-Type including Content-Range fields for each part. If a\n       Content-Length header field is present in the response MUST match\n        insert: its value --^\n       the actual number of OCTETs transmitted in the message-body.\n\n10.4.14 413 Request Entity Too Large\n\nThe server is refusing to process a request because the request entity\nis larger than the server is willing or able to process . The server may\n      delete --^\n\n11 Access Authentication\n\nHTTP provides several optional challenge-response authentication\nmechanisms which MAY be used by a server to challenge a client request\nand by a client to provide authentication information. The general\nframework for access authentication, and the specification of \"basic\"\n      insert: s --^\n\n14.26 If-None-Match\n\nThe result of a request having both an If-None-Match header field and\neither an If-Match or an If-Unmodified-Since header fields is undefined\n        delete --^\nby this specification.\n\n14.28 If-Unmodified-Since\n\nThe result of a request having both an If-Unmodified-Since header field\nand either an If-None-Match or an If-Modified-Since header fields is\n       delete --^\nundefined by this specification.\n\n14.40 Trailer\n\nThe Trailer general field value indicates that the given set of header\nfields are present in the trailer of a message encoded with chunked\n   change to: that has ====\ntransfer-coding.\n\n19.6 Compatibility with Previous Versions\n\nFor most implementations of HTTP/1.0, each connection is established by\nthe client prior to the request and closed by the server after sending\nthe response. Some implementations implement the Keep-Alive version of\npersistent connections described in section Error! Reference source not\n    ===========================\nfound..\n======\n\n    In the PostScript version this looks like \"19.6.2.0\".  It's obviously\n    a dangling reference.\n\n19.6.3.1 Significant Changes From the Proposed Standard Protocol\n\nA new error code (416))was needed to indicate an error for a byte range\n      ^-- change to space\nrequest that falls outside of the actual contents of a document.\n(Section 10.4.17)\n\n\n\n", "id": "lists-012-1270054"}, {"subject": "Re: webmail vulnerabilities: a new pragma token", "content": "Just a reminder- Pragma: no-cache is a request header, not a response header.\n\nAlso, RFC2616 says that 'No new Pragma directives will be defined in HTTP'.\nThat seems to preclude this use of it.\n\nCheers,\n\n\nOn Wed, Jan 19, 2000 at 08:45:00AM -0500, Peter W wrote:\n> \n> Before making this suggestion to client app vendors, I would very much\n> appreciate the comments of this working group.\n> \n> Background:\n> \n> On the Bugtraq security discussion mailing list[1], there has been much\n> conversation of late about webmail vulnerabilities. Essentially, the\n> webmail sites offer HTTP/HTML frontends to read Internet mail. They\n> normally can display HTML-encoded email. Such systems usually try to\n> remove all scripting code from email before displaying it. This is to\n> prevent those scripts from being executed in a way that might exploit\n> current client scripting lnguage problems, or simply exploit the trust\n> that a user might normally place in the site running the webmail frontend.\n> \n> Suggestion:\n> \n> It would be nice if there were on an HTTP header that, if sent to the\n> client, would cause the client to disable javascript, vbscript, etc. for\n> that document only. Sites who wished to display untrusted pages (webmail\n> sites, web discussion forums, etc.) could then use a multi-frame layout.\n> Any frame that contained untrusted code would have this header included in\n> the delivery of its content to ensure that the scripts would not be\n> evaluated, regardless of the normal client settings; other frames, whose\n> \"trusted\" documents would be sent without this header, would still be able\n> to use scripting (if enabled on the client).\n> \n> May I suggest\n> \n> Pragma: disable-scripting\n> \n> which I suppose means a no-cache page would be sent with\n> \n> Pragma: no-cache, disable-scripting\n> \n> Per RFC 2616, all Pragma headers must be passed to the client by all proxy\n> server or gateway applications. So this header would be passed to the\n> client application, as desired. But is it an acceptable use for \"Pragma\"?\n> \n> Comments, suggestions?\n> \n> -Peter\n> \n> http://www.bastille-linux.org/ : working towards more secure Linux systems\n> \n> [1] http://www.securityfocus.com/\n> \n\n-- \nMark Nottingham, Melbourne Australia \nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-12704935"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "Mark,\n\nOn Wednesday, January 19, 2000 7:46 PM, Mark Nottingham [SMTP:mnot@pobox.com] \nwrote:\n>\n> Just a reminder- Pragma: no-cache is a request header, not a response header.\n\nThis is as it should be.\n\n> Also, RFC2616 says that 'No new Pragma directives will be defined in HTTP'.\n> That seems to preclude this use of it.\n\nBut what Peter suggests is an extension-pragma and not a new Pragma directive \ne.g.\n\nPragma: no-cache, disable-scripting\n\nand not;\n\nPragma: disable-scipting\n\nThe question that seems more apropos is what is the provider expecting from the \nclient, not the converse.  Any implementor that decides to modify her client to \nextend Pragma: no-cache to Pragma: no-cache, disable-scripting should \nreasonably expect the server to have implemented a Reply with scripting \ndisabled. OR should we expect the Response chain to ONLY disable those scripted \nelements it 'understands' or the client 'understands' - to me that is the \ndilemma.  What would the clients and servers [sic] be expected to expect. NPI\n\nEric\n\n\n> Cheers,\n>\n>\n> On Wed, Jan 19, 2000 at 08:45:00AM -0500, Peter W wrote:\n> >\n> > Before making this suggestion to client app vendors, I would very much\n> > appreciate the comments of this working group.\n> >\n> > Background:\n> >\n> > On the Bugtraq security discussion mailing list[1], there has been much\n> > conversation of late about webmail vulnerabilities. Essentially, the\n> > webmail sites offer HTTP/HTML frontends to read Internet mail. They\n> > normally can display HTML-encoded email. Such systems usually try to\n> > remove all scripting code from email before displaying it. This is to\n> > prevent those scripts from being executed in a way that might exploit\n> > current client scripting lnguage problems, or simply exploit the trust\n> > that a user might normally place in the site running the webmail frontend.\n> >\n> > Suggestion:\n> >\n> > It would be nice if there were on an HTTP header that, if sent to the\n> > client, would cause the client to disable javascript, vbscript, etc. for\n> > that document only. Sites who wished to display untrusted pages (webmail\n> > sites, web discussion forums, etc.) could then use a multi-frame layout.\n> > Any frame that contained untrusted code would have this header included in\n> > the delivery of its content to ensure that the scripts would not be\n> > evaluated, regardless of the normal client settings; other frames, whose\n> > \"trusted\" documents would be sent without this header, would still be able\n> > to use scripting (if enabled on the client).\n> >\n> > May I suggest\n> >\n> > Pragma: disable-scripting\n> >\n> > which I suppose means a no-cache page would be sent with\n> >\n> > Pragma: no-cache, disable-scripting\n> >\n> > Per RFC 2616, all Pragma headers must be passed to the client by all proxy\n> > server or gateway applications. So this header would be passed to the\n> > client application, as desired. But is it an acceptable use for \"Pragma\"?\n> >\n> > Comments, suggestions?\n> >\n> > -Peter\n> >\n> > http://www.bastille-linux.org/ : working towards more secure Linux systems\n> >\n> > [1] http://www.securityfocus.com/\n> >\n>\n> --\n> Mark Nottingham, Melbourne Australia\n> http://www.mnot.net/\n\n\n\n", "id": "lists-012-12714814"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "> -----Original Message-----\n> From: Eric D. Williams [mailto:eric@infobro.com]\n> Sent: Wednesday, January 19, 2000 7:32 PM\n> To: 'Mark Nottingham'; Peter W\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: RE: webmail vulnerabilities: a new pragma token?\n> \n> \n> Mark,\n> \n> On Wednesday, January 19, 2000 7:46 PM, Mark Nottingham \n> [SMTP:mnot@pobox.com] \n> wrote:\n> >\n> > Just a reminder- Pragma: no-cache is a request header, not \n> a response header.\n> \n> This is as it should be.\n>\nBut what your talking about means that the server would\nsend the pragma: disable-scripting to the client.\nThat is a response header, not a request header as the\nspec defines it.\n \n> > Also, RFC2616 says that 'No new Pragma directives will be \n> defined in HTTP'.\n> > That seems to preclude this use of it.\n> \n> But what Peter suggests is an extension-pragma and not a new \n> Pragma directive \n> e.g.\n> \n> Pragma: no-cache, disable-scripting\n>\nNo, thats a new directive.   You're just listing\nmultiple directives in a single pragma statement.\n \nThe point of that part of the spec is to declare\npragma obsolete and not to be used in future work.\n\n> \n> The question that seems more apropos is what is the provider \n> expecting from the \n> client, not the converse.  Any implementor that decides to \n> modify her client to \n> extend Pragma: no-cache to Pragma: no-cache, disable-scripting should \n> reasonably expect the server to have implemented a Reply with \n> scripting \n\nI think the original poster meant that the CLIENT\nwould disable scripting code that was sent by the server.\nWhat you seem to be describing is that the client would\nsend a requedt with \"please dont send me any script\"\nwhich defeats the purpose since you now have to trust\nthe server....\n\nI see your problem, but I dont think pragma is \nthe right place for a solution.\nAs a matter of fact, I dont think HTTP is the place\nfor your solution.  Why not just stick a meta tag\nin the HTML itself ?\n\nFinally, as scott said, this problem can be solved\nby using existing browser functionality.  This\nis certainly easier than modifying the protocol and\nhoping that people will implement it.  \n(I for one would recommend against implementing\n your solution in our browser)\n\n> \n> \n> > Cheers,\n> >\n> >\n> > On Wed, Jan 19, 2000 at 08:45:00AM -0500, Peter W wrote:\n> > >\n> > > Before making this suggestion to client app vendors, I \n> would very much\n> > > appreciate the comments of this working group.\n> > >\n> > > Background:\n> > >\n> > > On the Bugtraq security discussion mailing list[1], there \n> has been much\n> > > conversation of late about webmail vulnerabilities. \n> Essentially, the\n> > > webmail sites offer HTTP/HTML frontends to read Internet \n> mail. They\n> > > normally can display HTML-encoded email. Such systems \n> usually try to\n> > > remove all scripting code from email before displaying \n> it. This is to\n> > > prevent those scripts from being executed in a way that \n> might exploit\n> > > current client scripting lnguage problems, or simply \n> exploit the trust\n> > > that a user might normally place in the site running the \n> webmail frontend.\n> > >\n> > > Suggestion:\n> > >\n> > > It would be nice if there were on an HTTP header that, if \n> sent to the\n> > > client, would cause the client to disable javascript, \n> vbscript, etc. for\n> > > that document only. Sites who wished to display untrusted \n> pages (webmail\n> > > sites, web discussion forums, etc.) could then use a \n> multi-frame layout.\n> > > Any frame that contained untrusted code would have this \n> header included in\n> > > the delivery of its content to ensure that the scripts \n> would not be\n> > > evaluated, regardless of the normal client settings; \n> other frames, whose\n> > > \"trusted\" documents would be sent without this header, \n> would still be able\n> > > to use scripting (if enabled on the client).\n> > >\n> > > May I suggest\n> > >\n> > > Pragma: disable-scripting\n> > >\n> > > which I suppose means a no-cache page would be sent with\n> > >\n> > > Pragma: no-cache, disable-scripting\n> > >\n> > > Per RFC 2616, all Pragma headers must be passed to the \n> client by all proxy\n> > > server or gateway applications. So this header would be \n> passed to the\n> > > client application, as desired. But is it an acceptable \n> use for \"Pragma\"?\n> > >\n> > > Comments, suggestions?\n> > >\n> > > -Peter\n> > >\n> > > http://www.bastille-linux.org/ : working towards more \n> secure Linux systems\n> > >\n> > > [1] http://www.securityfocus.com/\n> > >\n> >\n> > --\n> > Mark Nottingham, Melbourne Australia\n> > http://www.mnot.net/\n> \n\n\n\n", "id": "lists-012-12726620"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "At 2:08pm Jan 19, 2000, Scott Lawrence wrote:\n\n> > From: Peter W\n\n> > It would be nice if there were on an HTTP header that, if sent to\n> > the client, would cause the client to disable javascript,\n> > vbscript, etc. for that document only.\n\n>   The problem with Pragma as an extension mechanism is that there is\n>   no way for the server to know whether or not the client understands\n>   any particular pragma token, so it becomes an unreliable\n>   mechanism.\n\nTrue, but that's just an issue of getting client vendors on board.\n\nOthers have written that Pragma is a request header. Hmm. While RFC 2616\nonly defines its meaning as a request header, it defines Pragma as a\ngeneral header field, doesn't it? (sections 4.5 and 14.32). Anyhow, if\nPragma is a bad choice, then perhaps a new entity header should be used,\nin which case this is not an HTTP protocol question, as new entity\nheaders can be added without changing the protocol. ;-)\n\n> The degree of trust that a user should have in scripts, as this\n> example illustrates, is really a property of the script itself, or\n> perhaps of the containing document, not of the server from which it is\n> obtained.  There are already mechanisms available for signing email,\n> so if anything we should be looking for ways for browsers to make the\n> trust decisions appropriately - based on the document, not the web\n> server.\n\nAgreed. I think it would be nice if HTTP provided a way to indicate to the\nclient where they might find a signature or verification service to\nauthenticate individual pieces of content. That way a photographer could\npublish an image file and a detached .asc PGP signature, and the HTTP\nclient could obtain information from the httpd about where/how to get the\nsignature. Arguably this could be an attribute of an <IMG> tag, but IMHO\nthe signature is as much an attribute of the document as the last-modified\ndate and should be revealed by the httpd, not the referring document. So\nthis, too, is a good candidate for a new entity header.\n\nI also think there should be an HTML/XML DTD extension to allow a document\nto mark a portion of it (e.g., a certain DIV) as signed text, e.g. the\nnews paragraphs are signed, but not the doubleclick ad banner. ;-)\n\n> As an interim solution for the webmail sites today, I'd suggest that\n> you've already got the basis for a solution.  Serve the framework that\n> you want the user to trust from 'webmailbox.example.com', and then\n> serve the content of the mail frame from 'mail.example.com'. Instruct\n> users to trust 'webmailbox' and not to trust 'mail'.  A solution like\n> this can be implemented with many of todays browsers with no protocol\n> change.\n\nWith Internet Explorer at least, yes, using Zones. Very clever idea. I\nlike it, except it's a hassle for the end users. But it would be nice,\ndon't you think, if a site could explicitly repudiate a document without\nneeding a different base URL?\n\nThat's the essence of the problem: we have two different\nrun-this-script? security models with Netscape and IE:\n Netscape: based on how we got the document (HTTP vs POP3/IMAP/NNTP)\n   -this made more sense when it wasn't so easy to anonymously\n    publish content through other folks' Web sites, see below\n IE:       based on where we got the document (domains & zones)\n   -made more sense when sites generally contained only content\n    locally produced or approved\n\nI don't see signed scripts going anywhere soon; too much of a hassle for\nWeb publishers. What I'm basically suggesting is a way to improve the\nall-or-nothing security model: let the server repudiate an individual\ndocument. Why not a META tag? Because I see this as applying to different\ndocuments than HTML(/XML). Consider PostScript files, Java classes, and\nother content that you might get through HTTP.\n\nTwo growing trends make this repudiation a valuable notion: webmail\nsystems (ala Hotmail) and discussion-based Web sites (ala Slashdot). In\nboth these models, it's natural that externally-supplied content should\nnot be trusted to the same degree that content created by Microsoft\n(Hotmail) or Andover (Slashdot) should be. And especially for webmail, it\nseems very, very difficult for the server to filter out all the different\nways of embedding scripts. (Actually, sanitizing other objects like\nPostScript files or Java classes would be more difficult.)\n\nAnyhow, until/unless we have a usable document-based trust model, it would\nbe helpful if servers could indicate that a document should not be given\nthe same trust as official documents.\n\nThanks very much,\n\n-Peter\n\nhttp://www.bastille-linux.org/ : working towards more secure Linux systems\n\n\n\n", "id": "lists-012-12741489"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "On Thursday, January 20, 2000 2:32 AM, Josh Cohen \n[SMTP:joshco@Exchange.Microsoft.com] wrote:\n> > -----Original Message-----\n> > From: Eric D. Williams [mailto:eric@infobro.com]\n> > Sent: Wednesday, January 19, 2000 7:32 PM\n> > To: 'Mark Nottingham'; Peter W\n> > Cc: http-wg@cuckoo.hpl.hp.com\n> > Subject: RE: webmail vulnerabilities: a new pragma token?\n> >\n> >\n> > Mark,\n> >\n> > On Wednesday, January 19, 2000 7:46 PM, Mark Nottingham\n> > [SMTP:mnot@pobox.com]\n> > wrote:\n> > >\n> > > Just a reminder- Pragma: no-cache is a request header, not\n> > a response header.\n> >\n> > This is as it should be.\n> >\n> But what your talking about means that the server would\n> send the Pragma: disable-scripting to the client.\n> That is a response header, not a request header as the\n> spec defines it.\n>\n> > > Also, RFC2616 says that 'No new Pragma directives will be\n> > defined in HTTP'.\n> > > That seems to preclude this use of it.\n> >\n> > But what Peter suggests is an extension-pragma and not a new\n> > Pragma directive\n> > e.g.\n> >\n> > Pragma: no-cache, disable-scripting\n> >\n> No, thats a new directive.   You're just listing\n> multiple directives in a single pragma statement.\n>\n> The point of that part of the spec is to declare\n> pragma obsolete and not to be used in future work.\n>\n\nGot it.  That is a fine explanation of the intent as Pragma was included only \nfor backward compatibility.\n\n> >\n> > The question that seems more apropos is what is the provider\n> > expecting from the\n> > client, not the converse.  Any implementor that decides to\n> > modify her client to\n> > extend Pragma: no-cache to Pragma: no-cache, disable-scripting should\n> > reasonably expect the server to have implemented a Reply with\n> > scripting\n>\n> I think the original poster meant that the CLIENT\n> would disable scripting code that was sent by the server.\n\nI disagree, I think the intent was to activate some 'procedure' in the response \nchain such that no scirpting elements would be returned in the content.  I \ncertainly could have mis-interpreted this (and the Pragma directive is a \nrequest header) but it seems that is the intent.\n\n> What you seem to be describing is that the client would\n> send a request with \"please don't send me any script\"\n> which defeats the purpose since you now have to trust\n> the server....\n\nThat's the rub.\n\n> I see your problem, but I dot think pragma is\n> the right place for a solution.\n> As a matter of fact, I dot think HTTP is the place\n> for your solution.  Why not just stick a meta tag\n> in the HTML itself ?\n\nTherein lies the dilemma.  A question of whether the intended use of the \nbrowser as 'eMail UI' would be defeated in a substantial portion of WebMail \nbased sites.  Also if the mere presence of a meta-tag could be used to disable \n(or enable) scripting in a browser (although, IMHO, disabling would be a good \nthing under some circumstances) it could lead to other issues concerning the \n'authenticity' or 'integrity' of page elements intended to be 'calculated' by \nthe browser or passed to other modules for further processing.\n\nCertainly we are not (yet) talking about SHTTP  like functionality for WebMail \nsystems (maybe HTML or XML will?) but these issues of trusted and un-trusted \nscripting elements are currently being dealt with at the provider side through \nscript filtering mechanisms, which seem to be inadequate at times.  This \nfunctionality may be best implemented as a Meta-Tag SUPPORTED in client \nimplementations (who turns scripting back on?).  This would require a change in \neither usage expectations or browser HTML-parsing implementations (\"I disable \nscripting every time I visit webmail.com?\", \"my browser ``remembers'' to \ndisable scripting for this site element\".) .  And still imposes a requirement \non the provider concerning 'what to expect' the client will do as it relates to \nthis meta-info certainly not a discussion for this WG, but IMHO may be best \nhandled at the protocol level.\n\n> Finally, as scott said, this problem can be solved\n> by using existing browser functionality.  This\n> is certainly easier than modifying the protocol and\n> hoping that people will implement it.\n> (I for one would recommend against implementing\n>  your solution in our browser)\n\nJust a note, this is not my solution.  Merely a seconding of a proposed measure \nto address some difficult areas concerning user expectations of behavior in \nthese systems.  I would contend that the solution WILL require either:\n\n1.  A change in the way WebMail systems are implemented (which could include \nprotocol changes) or;\n2.  A change in the functionality of browsers at some level (read HTML \nparsing).\n\n\nEric\n\nEric Williams, Pres.\nInformation Brokers, Inc.    Phone: +1 202.889.4395\nhttp://www.infobro.com/        Fax: +1 202.889.4396\nmailto:eric@infobro.com      Pager: +1 301.303.8998\n           For More Info: info@infobro.com\n\n\n\n", "id": "lists-012-12754062"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "> From: Peter W\n\n> It would be nice if there were on an HTTP header that, if sent\n> to the client, would cause the client to disable javascript,\n> vbscript, etc. for that document only.\n\n> > The problem with Pragma as an extension mechanism is\n> > that there is\n> > no way for the server to know whether or not the\n> > client understands\n> > any particular pragma token, so it becomes an unreliable\n> > mechanism.\n\n> True, but that's just an issue of getting client vendors on\n> board.\n\nBelieve me, that is a big hurdle to get over.  I'm one of the\nauthors of RFC 2617 - Digest Authentication, whose goal is simply\nto stop publishing passwords on the Net in cleartext (as all web\nmail sites do now - do any use Digest yet?  IE5 supports it.)\nGetting new extensions into browsers is not an easy task\n(Netscape doesn't even support HTTP/1.1 yet), and you have to\nalways assume that some won't have it.\n\n> Agreed. I think it would be nice if HTTP provided a way to\n> indicate to the client where they might find a signature or\n> verification service to authenticate individual pieces of\n> content. [...]\n> So this, too, is a good candidate for a new entity header.\n\nI should think that multipart/signed would be a better way to go;\nthe server provides both the object and its signature together.\n\n> I also think there should be an HTML/XML DTD extension to allow\n> a document to mark a portion of it (e.g., a certain DIV) as\n> signed text, e.g. the news paragraphs are signed, but not the\n> doubleclick ad banner. ;-)\n\nThat sounds like grist for the XML Signatures WG.\n\n> But it would be nice, don't you think, if a site could\n> explicitly repudiate a document without needing a different\n> base URL?\n\nI think that you've got the problem backwards - rather than\ntrying to find a way to explicitly repudiate things, the model\nneeds to be that nothing is trusted unless vouched for\nexplicitly.  Otherwise, an attacker can just strip the\nrepudiation and the result is a trusted object.\n\n> I don't see signed scripts going anywhere soon; too much of a\n> hassle for Web publishers.\n\nI think that if Web publishers want to run their programs on my\nsystem with my privileges, then the burden belongs on them to\nestablish that I can trust them and their scripts.\n\n> Two growing trends make this repudiation a valuable notion:\n> webmail systems (ala Hotmail) and discussion-based Web sites\n> (ala Slashdot). In both these models, it's natural that\n> externally-supplied content should not be trusted to the same\n> degree that content created by Microsoft (Hotmail) or Andover\n> (Slashdot) should be.\n\nInteresting assumption - that the service provider should be\npresumed more trustworthy.  One I don't share, but that is not a\nprotocol issue.\n\n> And especially for webmail, it seems\n> very, very difficult for the server to filter out all the\n> different ways of embedding scripts. (Actually, sanitizing\n> other objects like PostScript files or Java classes would be\n> more difficult.)\n\nWhich is why they should put the burden on the originator of the\nmail and the recipient to implement a trust model that flows\nthrough their service, rather than taking on the (probably\nimpossible) job of making all the content safe.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12769006"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "On Wed, 19 Jan 2000, Josh Cohen wrote:\n\n> \n> I see your problem, but I dont think pragma is \n> the right place for a solution.\n> As a matter of fact, I dont think HTTP is the place\n> for your solution.  Why not just stick a meta tag\n> in the HTML itself ?\n\nBecause that means parsing and modification of the HTML.  Doesn't scale to\nany future content type ... I for one prefer security features out of band\nto the channel or in this case processing layer of the client.\n\nBut I'm not sure this is a complete solution as proposed in any case. At\nwhat point does not having scripting enabled prevent reasonable rendering\nof the content?  Should plugins be enabled? What about content within\nframes (or iframe) within the file which reference other servers? Meta\nrefresh tags which redirect to new content?\n\nDave Morris\n\n\n\n", "id": "lists-012-12780609"}, {"subject": "Browser support for request pipelinin", "content": "Does anybody have any stats on which or how many, if any, \nmainstream browsers will pipeline requests over a persistent \nconnection as per RFC2616 8.1.1.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-12788643"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "(Please note that the mailing list is really http-wg@hplb.hpl.hp.com;\nthe \"cuckoo\" machine apparently is willing to forward some mail,\nbut I'm not at all sure that it's running the same mailing list.)\n\nEven if sticking this kind of information in HTTP were appropriate\n(which I don't think it is), using \"pragma\" would be the wrong\nway to go about it. The whole notion of \"Pragma\" as a HTTP header\ncame from programming languages which used \"Pragma\" as a way of\nsticking in random additional compiler directives because there were\na fixed number of \"reserved words\" in the language. There's was no\nreason to use \"Pragma\" as an extension mechanism in the first place,\nand certainly it shouldn't be continued.\n\n\n> It would be nice if there were on an HTTP header that, if sent\n> to the client, would cause the client to disable javascript,\n> vbscript, etc. for that document only.\n\nIf you really wanted ot go this way, how about a new MIME type, e.g.,\n\"message/unsafe;type=http\" which would have the semantics of\nmessage/type (message/rfc822 or message/http) with the proviso that\nthe body is likely to be unsafe content.\n\nAt least it would have the right extension behavior, namely\nthat unaware recipients might save the content to disk but would\nbe less likely to open it.\n\n\n\n", "id": "lists-012-12796643"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "In my opinion, the whole idea of using HTTP to alert the client of\n\"unsafe content\" is wrong-headed.  ALL content is unsafe unless you\ntrust it.  Therefore, the client *must* provide facilities to let the\nuser identify trusted content and to protect him/her from untrusted\ncontent.  The content could come equally well from FTP or some other\nprotocol.\n\nWe have seen that bugs in design and/or implementation of scripting\nlanguages have left users vulnerable.  These bugs would still exist\neven if an HTTP header were added.  And an HTTP header would provide no\nprotection from scripts obtained via FTP.  The real solution is to fix\nthe implementations.\n\nThat may seem like a slow and painful solution, but it puts control in\nthe hands of the user, who has the most at stake.  And given that new\nbrowsers would be required to detect a new HTTP header anyway, fixing\nthe problem (faulty browser security), and not the symptom (scripts\nembedded in web-based email), is the right way to go.\n\nDave Kristol\n\n\n\n", "id": "lists-012-12805551"}, {"subject": "Re: comments on draft-ietf-http-v11-spec-rev0", "content": "> \n> 14.26 If-None-Match\n>     I was disappointed that the pseudo-code didn't make it.  OTOH,\n>     there are now disclaimers about mixing various conditional\n>     headers, which accomplishes the same thing.\n> \n>     I would like to see stronger advice in 13.3.4 about what\n>     combinations of conditional headers a client can send reliably to a\n>     server (or which combinations it must not send).\n> \n\nI was/am opposed to having two independent specifications present in the \nsame document; it leads to questions about which is correct. I felt it \nwas better to clarify the text to remove the ambiguity.\n\nI would not oppose publishing the pseudo-code as an informational RFC\ndocument, with a disclaimer that the base specification rules.\n- Jim\n\n\n\n", "id": "lists-012-1280775"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "Larry,\n\nI think you are coming real close here.  We are definitely looking at an issue \nof 'levels of trust' that must be understood by client implementations and \nend-users.  I suppose another MIME type would address the issue and allow some \nfreeform for a time as to how to handle unsafe content.  For me you have made \nit no longer as perplexing a conundrum as before:\n\n1.  A change in the way WebMail systems are implemented (which could include \nprotocol changes) or;\n2.  A change in the functionality of browsers at some level (read HTML \nparsing).\n\nand,\n\nOn Thursday, January 20, 2000 2:16 PM, Larry Masinter \n[SMTP:masinter@attlabs.att.com] wrote:\n--8<--snip-->8--\n>\n> Even if sticking this kind of information in HTTP were appropriate\n> (which I don't think it is), using \"pragma\" would be the wrong\n> way to go about it. The whole notion of \"Pragma\" as a HTTP header\n> came from programming languages which used \"Pragma\" as a way of\n> sticking in random additional compiler directives because there were\n> a fixed number of \"reserved words\" in the language. There's was no\n> reason to use \"Pragma\" as an extension mechanism in the first place,\n> and certainly it shouldn't be continued.\n>\n\nI think the intent of the HTTP/1.1 support was for backward compatibility, only \nyes?  The 1.0 header request Pragma was a way of addressing \"no-cache\".\n\n> > It would be nice if there were on an HTTP header that, if sent\n> > to the client, would cause the client to disable javascript,\n> > vbscript, etc. for that document only.\n>\n> If you really wanted to go this way, how about a new MIME type, e.g.,\n> \"message/unsafe;type=http\" which would have the semantics of\n> message/type (message/rfc822 or message/http) with the proviso that\n> the body is likely to be unsafe content.\n\nHere is where the 'levels of trust' (that's the bug-a-boo) would be important \n(to me).  It would be good not to limit the description here to merely \n\"unsafe\", per se I think I see your point clearly.\n\n> At least it would have the right extension behavior, namely\n> that unaware recipients might save the content to disk but would\n> be less likely to open it.\n\nI don't know about that; if its not safe to a later 'aware' recipient is \nprobable and good, but older clients would not be able to discriminate.  That \ncould set up an interesting situation where browsers are updated or \ntrust-levels are upgraded; Excellent though.\n\nEric\n\nEric Williams, Pres.\nInformation Brokers, Inc.    Phone: +1 202.889.4395\nhttp://www.infobro.com/        Fax: +1 202.889.4396\nmailto:eric@infobro.com\n           For More Info: info@infobro.com\n\n\n\n", "id": "lists-012-12813754"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "> -----Original Message-----\n> From: Eric D. Williams [mailto:eric@infobro.com]\n> Sent: Thursday, January 20, 2000 12:16 PM\n> To: 'Larry Masinter'\n> Cc: 'http-wg@hplb.hpl.hp.com'\n> Subject: RE: webmail vulnerabilities: a new pragma token?\n> > Larry said:\n> > \n> > At least it would have the right extension behavior, namely\n> > that unaware recipients might save the content to disk but would\n> > be less likely to open it.\n> \n> Eric said:\n> \n> I don't know about that; if its not safe to a later 'aware' \n> recipient is \n> probable and good, but older clients would not be able to \n> discriminate.  That \n> could set up an interesting situation where browsers are updated or \n> trust-levels are upgraded; Excellent though.\n> \nMaybe Im misreading your words, but I think you missed\npart of larry's point.  By using a new MIME type,\nolder browsers would implicitly discriminate.\nToday, a browser that gets an unknown mime time, which this\nnew one would be, it will prompt the user to save it to disk\ninstead of showing it.\n\nThis would effectively prevent it from being displayed\nor executed without user consent.\n\n> Eric\n> \n> Eric Williams, Pres.\n> Information Brokers, Inc.    Phone: +1 202.889.4395\n> http://www.infobro.com/        Fax: +1 202.889.4396\n> mailto:eric@infobro.com\n>            For More Info: info@infobro.com\n> \n\n\n\n", "id": "lists-012-12824946"}, {"subject": "RE: webmail vulnerabilities: a new pragma token", "content": "On Thursday, January 20, 2000 3:54 PM, Josh Cohen \n[SMTP:joshco@Exchange.Microsoft.com] wrote:\n> > -----Original Message-----\n> > From: Eric D. Williams [mailto:eric@infobro.com]\n> > Sent: Thursday, January 20, 2000 12:16 PM\n> > To: 'Larry Masinter'\n> > Cc: 'http-wg@hplb.hpl.hp.com'\n> > Subject: RE: webmail vulnerabilities: a new pragma token?\n> > > Larry said:\n> > >\n> > > At least it would have the right extension behavior, namely\n> > > that unaware recipients might save the content to disk but would\n> > > be less likely to open it.\n> >\n> > Eric said:\n> >\n> > I don't know about that; if its not safe to a later 'aware'\n> > recipient is probable and good, but older clients would not be able to\n> > discriminate.  That could set up an interesting situation where browsers \nare updated or\n> > trust-levels are upgraded; Excellent though.\n> >\n> Maybe Im misreading your words, but I think you missed\n> part of larry's point.  By using a new MIME type,\n> older browsers would implicitly discriminate.\n> Today, a browser that gets an unknown mime time, which this\n> new one would be, it will prompt the user to save it to disk\n> instead of showing it.\n\nYes, that is true, my concern was/is with just that point.  After the item is \nsaved many users attempt to open that 'troublesome' downloaded page.  The only \nthing I am saying is the content after saving to disk can still contain \n'malicious' scripting code and the browser could not then successfully \ndiscriminate unless the file name was some how also unlinked from the types \n'compatible' with the browser (read viewer) and parsed without scripting.\n\nI say yes, Josh, you are correct that part was not clear.  Thanks.\n\n> This would effectively prevent it from being displayed\n> or executed without user consent.\n>\n\nYes, user consent is a primary factor here (always), as well as _expected \nbehavior_.  One should consider what a user would do concerning unknown types \nthat are nominally displayed in their client, it could get confusing to say the \nleast. A user may just map an association back to the browser at worst \n(education of the user is also a key issue) perhaps as you mentioned this is \nnot an effective track (http-wg), but I am still not sure.\n\nEric\n\n\n> >\n> > Eric Williams, Pres.\n> > Information Brokers, Inc.    Phone: +1 202.889.4395\n> > http://www.infobro.com/        Fax: +1 202.889.4396\n> > mailto:eric@infobro.com\n> >            For More Info: info@infobro.com\n> > \n\n\n\n", "id": "lists-012-12836035"}, {"subject": "Re: Browser support for request pipelinin", "content": "I have performed some limited tests while developing our\nWeb Robustifier.\nI found that Netscape Navigator 4.61/Linux doesn't support\npipelining as per RFC2616. It does however pipeline if\nyou browse a Netscape Enterprise server. I assume it is using\nthe explicit Keep-Alive (RFC2616:19.6.2) method.\nInternet Explorer v4.72.3110 does support RFC2616 pipelining\nthough, I have tested browsing Apache/1.3.9/Linux.\nTrying to browse the same Netscape Enterprise server (with IE)\nthat supported 'Keep-Alive' pipelining when browsed with the\nNetscape Navigator did not result in a pipelined connection\nthough.\n  \nThese are just my limited findings, so far. It would be nice\nif anyone else had more extensive tests to share.\n\nCheers,\n\n/Patrik.\n \nMiles Sabin wrote:\n> \n> Does anybody have any stats on which or how many, if any,\n> mainstream browsers will pipeline requests over a persistent\n> connection as per RFC2616 8.1.1.\n> \n> Cheers,\n> \n> Miles\n> \n> --\n> Miles Sabin                       Cromwell Media\n> Internet Systems Architect        5/6 Glenthorne Mews\n> +44 (0)20 8817 4030               London, W6 0LJ, England\n> msabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-12847294"}, {"subject": "RE: Browser support for request pipelinin", "content": "Patrik Winroth,\n> <snip/>\n\nThanks, but as far as I can see you're talking about what\nRFC 2616 call persistent connections.\n\nRequest pipelining presupposes a persistent connection, but\nadds the following: subsequent requests are be sent before the\ncorresponding responses are received, ie. instead of,\n\n  Request1\n           Response1\n  Request2\n           Response2\n  Request3\n           Response3\n\nwe could have,\n\n  Request1\n  Request2\n  Request3\n           Response1\n           Response2\n           Response3\n\n(although, clearly, if there are many requests we might expect \nthe request stream to partially overlap the response stream).\n\nSo, does anyone have any info on browser support for pipelining?\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n \n\n\n\n", "id": "lists-012-12855820"}, {"subject": "RE: Browser support for request pipelinin", "content": "> So, does anyone have any info on browser support for pipelining?\n\nAccording to \"Persistent Connection Behavior of Popular Browsers\" \n[1]:\n\n  \"Both Navigator and Explorer support persistent connections \n   to Web servers and proxies. However, neither supports request\n   pipelining on a persistent connection.\"\n\nI found that by rummaging around in the Web Characterization \nActivity area of the W3C web site [2].\n\n[1] http://www.cs.wisc.edu/~cao/papers/persistent-connection.html\n\n    Zhe Wang and Pei Cao\n    Department of Computer Sciences\n    University of Wisconsin, Madison\n    1210 West Dayton Street\n    Madison, WI 53706 USA\n    {zhe,cao}@cs.wisc.edu\n\n[2] http://www.w3.org/WCA/\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12864222"}, {"subject": "RE: Browser support for request pipelinin", "content": "Scott Lawrence wrote,\n> I found that by rummaging around in the Web Characterization \n> Activity area of the W3C web site [2].\n>\n> [1] http://www.cs.wisc.edu/~cao/papers/persistent-connection.html\n\nI've seen that, but it's dated Dec 9 1998, so things might have\nchanged since then.\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-12872959"}, {"subject": "Re: Browser support for request pipelinin", "content": "At 19:08 20.01.00 -0000, Miles Sabin wrote:\n>Does anybody have any stats on which or how many, if any, \n>mainstream browsers will pipeline requests over a persistent \n>connection as per RFC2616 8.1.1.\n\nOpera v4.0 supports HTTP 1.1 persistent connections and pipelining of\nrequests.\n\nOpera v4 is presently being tested by our beta test group, but we expect a\npublic beta to be released soon.\n\n\nSincerely,\nYngve N. Pettersen\n\n********************************************************************\nWeb-sites:       http://www.opera.no\n                 http://www.opera.com\n--------------------------------------------------------------------\nYngve Nysaeter Pettersen             Phone:  +47 23 23 48 88\nOpera Software AS                    Fax:    +47 23 23 48 70\nWaldemar Thranes gt. 86 B, \n0175 OSLO, Norway                    Email:  yngve@opera.com\n********************************************************************\n*                     Opera v3.61 out now.                         *\n********************************************************************\n\n\n\n", "id": "lists-012-12880783"}, {"subject": "what is the scope of acceptrang", "content": "(this is a minor point)\n\nIn section 14.5:                                                                       \n    The Accept-Ranges response-header field allows the server to\n    indicate   its acceptance of range requests for a resource ....              \n     Origin servers that accept byte-range requests MAY send \n     Accept-Ranges: bytes                                                  \n\nIt's a bit unclear what the scope of the accept-range\nresponse header is. Is it just for the requested resources, or for a larger\nset of resources?\n\nThat is, if the origin server will accept byte-range requests\nfor some, but not all, resources (say, not for html documents with\nserver side includes),  then what is the proper usage of\nAccept-Ranges:?\n\n\n\n", "id": "lists-012-1288512"}, {"subject": "On pipelinin", "content": "I'm redoing the persistent-connection/pipelining support in my web\nserver, and\nthe question came up of how to resolve pipelined requests. By resolve,\nI mean\nthe act of the server using the request line (and headers, etc) to\ndetermine which file to read, which script to run, etc ... the end\nresult being the creation of  a response to  send to the client.\n\nBasically,  in the following scenario (from mike sabin)..\n\n>  Request1\n>  Request2\n > Request3\n>         Response1\n>           Response2\n >         Response3\n>(although, clearly, if there are many requests we might expect \n>the request stream to partially overlap the response stream).\n\nwould \"resolution of request2\" await the completion of transmission\nof response1? Or can these  3 requests be resolved simultaneously\n(say, using\nseperate threads); with first response1 sent, then 2, then 3.\nThat is, could a multi-threaded server create response2,  wait for\nsuccesful transmission of  response1 (over the persistent connection),\nand then transmit response2 (over this\nsame persistent connection)?\n\nThis is a problem if the resolution of request2 can depend on the\nresolution of\nrequest 1 (for example, when request1 causes some client-specific\nstate variable, that is  used in request2, to change)\n\nThe definition of idempotency (rfc2616  9.1.2)  suggests NO -- since\nidentical sequences should return the same result, which would\npreclude\nsituations where the oddities of server load may dictate the  speed\nwith which\nsimultaneously resolved requests are processed.\n\nAny strong opinions?\n\n\nDaniel Hellerstein\ndanielh@crosslink.net\nhttp://www.srehttp.org\n\n\n\n", "id": "lists-012-12888995"}, {"subject": "Re: On pipelinin", "content": "Fred Bohle@NEON\n01/21/2000 09:48 AM\n\nDaniel,\n\n     I would suggest that the multi-threading be done at the connection\nlevel,\nnot at the request level.  That way you can be sure the requests are\nhandled in their\nproper order.  Multi-threading at the request level would require some\nother mechanism\nto be sure that they are executed in the same order, defeating the purpose\nof multi-\nthreading.  At a minimum, you must return the responses in the same order\nthat you\nreceived them.  Again multi-threading the requests would require a\nmechanism to\nensure this order, minimizing the benefits of such multi-threading.\n\n     Consider three requests that:\n1. display a database item.\n2. update that database item.\n3. display the same database item.\n\nThe intent here is to show the before and after views of the database item.\nIn a\nmulti-threaded request server, if the requests execute in 1,3,2 order, or\n2,1,3 order,\nthe before/after sense of the display is lost.  Note that this is true even\nif you force\nthe order of responses to be reshuffled as 1,2,3.\n\n     So, I don't see how you can make it work if you multi-thread the\nrequests.\n\nFred Bohle\n\n\n\n\n\n\n\"Daniel Hellerstein\" <DANIELH@mailbox.econ.ag.gov> on 01/21/2000 09:35:53\nAM\n\nTo:   http-wg@cuckoo.hpl.hp.com\ncc:    (bcc: Fred Bohle/Dev/Neon)\n\nSubject:  On pipelining\n\n\n\n\nI'm redoing the persistent-connection/pipelining support in my web\nserver, and\nthe question came up of how to resolve pipelined requests. By resolve,\nI mean\nthe act of the server using the request line (and headers, etc) to\ndetermine which file to read, which script to run, etc ... the end\nresult being the creation of  a response to  send to the client.\n\nBasically,  in the following scenario (from mike sabin)..\n\n>  Request1\n>  Request2\n > Request3\n>         Response1\n>           Response2\n >         Response3\n>(although, clearly, if there are many requests we might expect\n>the request stream to partially overlap the response stream).\n\nwould \"resolution of request2\" await the completion of transmission\nof response1? Or can these  3 requests be resolved simultaneously\n(say, using\nseperate threads); with first response1 sent, then 2, then 3.\nThat is, could a multi-threaded server create response2,  wait for\nsuccesful transmission of  response1 (over the persistent connection),\nand then transmit response2 (over this\nsame persistent connection)?\n\nThis is a problem if the resolution of request2 can depend on the\nresolution of\nrequest 1 (for example, when request1 causes some client-specific\nstate variable, that is  used in request2, to change)\n\nThe definition of idempotency (rfc2616  9.1.2)  suggests NO -- since\nidentical sequences should return the same result, which would\npreclude\nsituations where the oddities of server load may dictate the  speed\nwith which\nsimultaneously resolved requests are processed.\n\nAny strong opinions?\n\n\nDaniel Hellerstein\ndanielh@crosslink.net\nhttp://www.srehttp.org\n\n\n\n", "id": "lists-012-12897552"}, {"subject": "RE: On pipelinin", "content": "Daniel Hellerstein wrote,\n> Basically,  in the following scenario (from mike sabin)..\n>\n> Request1\n> Request2\n> Request3\n>         Response1\n>         Response2\n>         Response3\n>\n> would \"resolution of request2\" await the completion of \n> transmission of response1? Or can these  3 requests be \n> resolved simultaneously (say, using seperate threads); with \n> first response1 sent, then 2, then  3. That is, could a \n> multi-threaded server create response2,  wait for succesful \n> transmission of  response1 (over the persistent connection),\n> and then transmit response2 (over this same persistent \n> connection)?\n>\n> This is a problem if the resolution of request2 can depend on \n> the resolution of request 1 (for example, when request1 \n> causes some client-specific state variable, that is  used in \n> request2, to change)\n\nAs I read it RFC 2616 is silent on this. I think that's as it\nshould be, because server-side client-specific state isn't\nany part of the HTTP protocol. Again as I read it, a server\nis allowed to process the requests it receives on a persistent \nconnection in any order it chooses, including concurrently, so \nlong as responses are returned in the same order as the \ncorresponding requests were sent.\n\nI don't think that strictly speaking the constaints in 9.1.2\napply, because they're supposed to be enforced by the user-\nagent before request are sent rather than when they arrive at \nthe server.\n\nCheers,\n\n\nMiles (not Mike ;-)\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-12908360"}, {"subject": "RE: On pipelinin", "content": "> Basically,  in the following scenario (from mike sabin)..\n>\n> >  Request1\n> >  Request2\n>  > Request3\n> >         Response1\n> >           Response2\n>  >         Response3\n> >(although, clearly, if there are many requests we might expect\n> >the request stream to partially overlap the response stream).\n>\n> would \"resolution of request2\" await the completion of\n> transmission\n> of response1? Or can these  3 requests be resolved simultaneously\n> (say, using\n> seperate threads); with first response1 sent, then 2, then 3.\n> That is, could a multi-threaded server create response2,  wait for\n> succesful transmission of  response1 (over the persistent\n> connection),\n> and then transmit response2 (over this\n> same persistent connection)?\n\nIf request1 is a GET, HEAD, TRACE, or OPTIONS, then I would say that\nthe server could legitimately assume that it had no side effects.\nOne could be more conservative and assume that any CGI (or CGI-like)\nresource had side effects.  Having assumed that there were no side\neffects it might be reasonable to overlap the processing, if you\nhave the buffer space to spare.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-12917214"}, {"subject": "RE: On pipelinin", "content": "Multi-threading can be done for requests that come in over the\nsame connection; this seems valuable, especially if each of the\nrequests require computation or communication with other service\nelements in the network.\n\nFor example, you could have a server with a request thread and\na response thread, where requests would spawn additional threads\nto handle them if necessary (e.g., if the request was for a\nCGI or non-local resource) and then queue the results for the\nresponse thread.\n\n#    I would suggest that the multi-threading be done at the connection\nlevel,\n# not at the request level.  That way you can be sure the requests are\nhandled in their\n# proper order.  Multi-threading at the request level would require some\nother mechanism\n# to be sure that they are executed in the same order, defeating the purpose\nof multi-\n# threading.  At a minimum, you must return the responses in the same order\nthat you\n# received them.  Again multi-threading the requests would require a\nmechanism to\n# ensure this order, minimizing the benefits of such multi-threading.\n\n #     Consider three requests that:\n# 1. display a database item.\n# 2. update that database item.\n# 3. display the same database item.\n\n# The intent here is to show the before and after views of the database\nitem. In a\n# multi-threaded request server, if the requests execute in 1,3,2 order, or\n2,1,3 order,\n# the before/after sense of the display is lost.  Note that this is true\neven if you force\n# the order of responses to be reshuffled as 1,2,3.\n\n#      So, I don't see how you can make it work if you multi-thread the\nrequests.\n\nbut\n\n   Clients SHOULD NOT pipeline requests using non-idempotent methods or\n   non-idempotent sequences of methods (see section 9.1.2). Otherwise, a\n   premature termination of the transport connection could lead to\n   indeterminate results. A client wishing to send a non-idempotent\n   request SHOULD wait to send that request until it has received the\n   response status for the previous request.\n\nIt is not the server's responsibility to insure consistency, and a server\nimplementation that multithreads the responses would seem to be valid.\n\n\n\n", "id": "lists-012-12925600"}, {"subject": "Idempotent sequences of method", "content": "The discussion in the 'On piplining' thread reminded me of\nsomething I found puzzling in 9.1.\n\nI have a strong suspicion that although 9.1.2 suggests that\nan idempotent sequence of methods might contain non-safe\nidempotent methods (PUT, DELETE), this can't actually be the\ncase.\n\nHere's the argument,\n\n* A sequence which contains a PUT/DELETE <uri1> after a \n  GET/HEAD/TRACE/OPTIONS <uri2> is idempotent only if\n  uri1 is independent of uri2.\n\n* There is nothing in the definition of an HTTP URI which\n  allows us to infer that two URIs are independent.\n\nSuppose, for the sake of argument, that a URI path maps\nin a natural way onto part of the servers filesystem tree.\nIn this case, it's clear that we can't infer that,\n\n  http://www.foo.com/a/b/c\n\nis independent of,\n\n  http://www.foo.com/a\n\non a purely textual basis because if we were to DELETE the \nlatter we would have also deleted the former.\n\nAgain, with the filesystem mapping assumption, we can't even\nassume that,\n\n  http://www.foo.com/a/b/c\n\nis independent of,\n\n  http://www.foo.com/d/e/f\n\nbecause they might both reference the same underlying file\nin the servers filesystem (eg. via a symlink, or via an\nHTTP server path mapping).\n\nSo, we can't use textual difference of URIs to infer \nindependence. Given that we haven't anything else to go on at \nthe user-agent end I conclude that an idempotent sequence of\nmethods can't contain any non-safe methods.\n\nQED ;-)\n\nComments?\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-12934942"}, {"subject": "RE: On pipelining Repl", "content": ">If request1 is a GET, HEAD, TRACE, or OPTIONS, then I would say that\n>the server could legitimately assume that it had no side effects.\n>One could be more conservative and assume that any CGI (or CGI-like)\n>resource had side effects.  Having assumed that there were no side\n>effects it might be reasonable to overlap the processing, if you\n>have the buffer space to spare.\n\nThat's a reasonable notion.  However, in practice it might be a bit\ncomplicated (i.e.; server side includes in GETs of html documents\nmight\ncause changes in  \"state\" variables.).\n\nIt seems that the tradeoff is between speed and\nbeing-nice-to-less-then-careful-clients.\nFor now, I'll stick with sequential resolution, and then \nconsider how to detect circumstances where \nsequentiality can be loosened.\n\n\n\n", "id": "lists-012-12943204"}, {"subject": "RE: On pipelining Repl", "content": "Daniel Hellerstein wrote,\n> server side includes in GETs of html documents might cause \n> changes in  \"state\" variables.).\n\nThat's not allowed. GET is idempotent which implies it must\nbe stateless.\n\nYou should be using POST for that sort of thing.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-12950948"}, {"subject": "Re: On pipelining Repl", "content": "Miles Sabin wrote:\n> \n> Daniel Hellerstein wrote,\n> > server side includes in GETs of html documents might cause\n> > changes in  \"state\" variables.).\n> \n> That's not allowed. GET is idempotent which implies it must\n> be stateless.\n> \n> You should be using POST for that sort of thing.\n> \n\n SHOULD (in the rfc2119 sense) is unfortunately the operative\nword here. GET's idempotence is a SHOULD, not a MUST. Reference\nrfc2616, section 9.1.1. \n\n\n-cks\n\n\n\n", "id": "lists-012-12958627"}, {"subject": "Re: http-v11-spec-rev03; proxy 100 (Continue) issu", "content": "Richard L. Gray writes:\n\n    In section 8.2.4, Requirements for HTTP/1.1 proxies, it says:\n    \n    \"- If the proxy knows that the version of the next-hop server is\n    HTTP/1.0 or lower, it MUST NOT forward the request, and it MUST respond\n    with a 417 (Expectation Failed) status.\"\n    \n    The problem is with the normative MUSTs, which we think ought to be\n    at most normative SHOULDs (and perhaps this bullet should just be\n    removed).  The reasons are:\n    - There is no interoperability issue here, since if the version of the\n      next-hop server is unknown, the request, including the Expect header\n      field, MUST be forwarded, and the client needs to be able to (for\n      compatibility reasons) time out waiting for the 100 (Continue) and\n      send the body anyway.\n    - The likely behaviour of a client, in the case of recieving a 417 with\n      out some reason (like a challenge for credentials), is to just retry\n      without the Expect header, so the only effect is to increase latency.\n    \n    Our proxy implementation currently will just generate a 100 (continue)\n    in this case.\n    \n    I apologize for not catching this earlier (I checked and it goes at\n    least back to rev-01).\n\nI suppose you're right that it's inappropriate to base a MUST NOT\nwith a fact that the proxy is not required to know or verify.  So\nsince we probably don't want add a mandatory round-trip here (i.e.,\nproxy sends OPTIONS method to next-hop server to see what its version\nis), this \"MUST NOT\" ought to be a \"SHOULD NOT\".    \n    \n    Also, in the same section, it says:\n    \n    \"Proxies SHOULD maintain a cache recording the HTTP version numbers\n    from recently-referenced next-hop servers.\"\n    \n    This is not an interoperability issue either, and so we do not think\n    the \"SHOULD\" ought to be normative.  Perhaps it ought to be phrased as\n    an \"encouragement\", as advice to implementers is elsewhere.\n    \nHere I think the rules of RFC2119:\n\n3. SHOULD   This word, or the adjective \"RECOMMENDED\", mean that there\n   may exist valid reasons in particular circumstances to ignore a\n   particular item, but the full implications must be understood and\n   carefully weighed before choosing a different course.\n\nand\n\n   Imperatives of the type defined in this memo must be used with care\n   and sparingly.  In particular, they MUST only be used where it is\n   actually required for interoperation or to limit behavior which has\n   potential for causing harm (e.g., limiting retransmisssions)  For\n   example, they must not be used to try to impose a particular method\n   on implementors where the method is not required for\n   interoperability.\n\nsupport this as a SHOULD, under the \"limiting retransmissions\" rule.\n\nThe notion that SHOULD/MUST are allowed \"only if required for\ninteroperability\" is a misreading of this \"For example\" in RFC2119.\nIt's specifically allowed to use SHOULD/MUST for \"limiting\nretransmisssions\" and that isn't really an \"interoperability issue.\"\n\n-Jeff\n\n\n\n", "id": "lists-012-1295988"}, {"subject": "RE: On pipelining Repl", "content": "Christopher K. St. John wrote,\n> SHOULD (in the rfc2119 sense) is unfortunately the operative\n> word here. GET's idempotence is a SHOULD, not a MUST. \n> Reference rfc2616, section 9.1.1. \n\nNot at all. I don't see anything there, or anywhere else, which\nstates that GETs idempotence is optional.\n\nAll it says is that GET \"SHOULD NOT have any significance other \nthan retreival\". That's a *very* long way from permitting side-\neffects. It allows but discourages, for example, a server\nmaking a rude noise when it handles a GET ... but that's not \nstate.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-12965720"}, {"subject": "Re: On pipelining -Reply Repl", "content": ">> server side includes in GETs of html documents might cause \n> changes in  \"state\" variables.).\n>>That's not allowed. GET is idempotent which implies it must\n>be stateless.\n>You should be using POST for that sort of thing.\n\nInteresting point.\n\nBut does that mean that  proper http/1.1 server-software should\nenforce this? That server software should somehow not allow\nGET to invoke scripts, etc. that might change \"state\" variables. \nThat's a tough requirement (from a server author's point of view).\n\nAlso, what, if a response has an immediate exipiration (say, it's\ngenerated by\na script that changes state variables, or it's generated from an SHTML\nresource that\ncontains hit counts, time of day, or other dynamic elements).\n\nMy reading of idempotency suggests that in such cases the\n\"statelessness\"\nof GET is no longer a constraint. \n\n\n\n", "id": "lists-012-12973848"}, {"subject": "Re: On pipelining -Reply Repl", "content": "On Fri, 21 Jan 2000, Daniel Hellerstein wrote:\n\n> >> server side includes in GETs of html documents might cause \n> > changes in  \"state\" variables.).\n> >>That's not allowed. GET is idempotent which implies it must\n> >be stateless.\n> >You should be using POST for that sort of thing.\n> \n> Interesting point.\n> \n> But does that mean that  proper http/1.1 server-software should\n> enforce this? That server software should somehow not allow\n> GET to invoke scripts, etc. that might change \"state\" variables. \n> That's a tough requirement (from a server author's point of view).\n> \n> Also, what, if a response has an immediate exipiration (say, it's\n> generated by\n> a script that changes state variables, or it's generated from an SHTML\n> resource that\n> contains hit counts, time of day, or other dynamic elements).\n> \n> My reading of idempotency suggests that in such cases the\n> \"statelessness\"\n> of GET is no longer a constraint. \n\nThere is no reason for the 'server' which happens to be extended by a CGI\nor other bit of active code to have to enforce compliance ... that is the\nresponsiblity of the author of the application. And it is the application\nwhich will break or otherwise offend the user.  The protocol specifies the\ncorrect and safe way to produce well behaved applications.  During the\nyears of producing the HTTP RFCs, it was always understood that a content\nprovider could take the risk of changing server side state ... and a\ncommon example is a hit counter on a page. But would you want to insist\nthat pipelining be disabled because a non-semantic state change occured\nwith a different result?\n\nI wouldn't.\n\nDave Morris\n\n\n\n", "id": "lists-012-12981375"}, {"subject": "Re: On pipelining Repl", "content": "Miles Sabin wrote:\n> \n> Christopher K. St. John wrote,\n>\n> > SHOULD (in the rfc2119 sense) is unfortunately the operative\n> > word here. GET's idempotence is a SHOULD, not a MUST.\n> > Reference rfc2616, section 9.1.1.\n> \n> Not at all. I don't see anything there, or anywhere else, which\n> states that GETs idempotence is optional.\n>\n\n From section 9.1.1:\n\n     \"Naturally, it is not possible to ensure that the server does not\n   generate side-effects as a result of performing a GET request; in\n   fact, some dynamic resources consider that a feature. The important\n   distinction here is that the user did not request the side-effects,\n   so therefore cannot be held accountable for them.\"\n\n However, from section 9.1.2:\n\n    \"Methods can also have the property of \"idempotence\" in that (aside\n   from error or expiration issues) the side-effects of N > 0 identical\n   requests is the same as for a single request. The methods GET, HEAD,\n   PUT and DELETE share this property.\"\n\n Which, as I read it, says you are free to have GET requests change\nserver side state as long as the state changes are idempotent. (Although\n9.1.1's language can easily be read as weakening that considerably)\n\n \n> All it says is that GET \"SHOULD NOT have any significance other\n> than retreival\". That's a *very* long way from permitting side-\n> effects. It allows but discourages, for example, a server\n> making a rude noise when it handles a GET ... but that's not\n> state.\n>\n\n So, a compliant implementation may have idempotent side effects,\nbut an unconditionally compliant implementation may not.\n\n Unfortunately, my experience with servlets and other application server\ntechnologies leads me to believe that the \"idempotent\" requirement for\nside effects is widely ignored (and not just for page counters) Not that\nthat's an excuse, just a data point :-)\n\n Thanks for the clarification,\n\n\n-cks\n\n\n\n", "id": "lists-012-12990774"}, {"subject": "RE: On pipelining -Reply Repl", "content": "(mailing list name 'cuckoo' -> 'hplb' again).\n\nThere's no point in restricting side effects in general; the\nidea was to restrict side effects that somehow mattered! And\nwhether or not it matters depends on the application. For the\ntraditional web browsing application, clicking on a link twice,\nrefetching a web page, or even prospectively fetching a page\nthat you think someone *might* want, using GET to mirror a site,\netc ... those shouldn't have significant side effects.\n\nFor interactions between subsequent requests and pipelining,\nsince it is the client that decides whether to pipeline,\nit is also the client that has the responsibility for\ndeciding whether pipelining matters.\n\nFor web browsing, there's no real problem: the only\nstate change where consistency matters is the interaction\nbetween the POST from a form being filled and the subsequent\nGETs that are triggered by embedded URLs in the content\nthat is returned from the post.\n\nFor other applications that are being built on top of\nHTTP, there must be some agreement between client and\nback-end application as to what the dependencies and\ntransaction semantics must be; the HTTP server needn't\nenforce these, since the clients have complete control\nover whether they ATTEMPT pipelining.\n\nI suppose you might want to note that intermediaries\nshouldn't introduce pipelining (e.g., by prospectively\nguessing what URLs might appear in subsequent content\nand prefetching them!) but otherwise, this is a client,\nnot a server, responsibility (IMO).\n\nServers need not serialize processing pipelined requests;\nclients that care shouldn't pipeline.\n\nLarry\n-- \nhttp://larry.masinter.net\n\n\n\n", "id": "lists-012-12999382"}, {"subject": "Two question", "content": "1.  I looked into several captured packets of a HTTP request,\n     there are lots of \"OD OA\" as deviders for headers;\n     sometimes a request ends with \"OD 0A 0D 0A\", sometimes ends\n      with \"0A 0A 0A\" or ....\n     In the RFC, the request can end with 0D or 0A or their combination,\n\n     I wonder how does a HTTP server or HTTP client determines\n     a full request is received?.  Shouldn't we have a better way to\ndetermine\n     it?\n\n2.   When a user is using a Web browser , the \"Reload\" in netscape\n       or the\"Refresh\" in IE is pressed, what's the policy the browser\n       to use about when to start with a new connection( SYN issued\nfirst).\n       or to start with just a HTTP request( ACK push)?\n\nMany thanks!\n\n\n\n", "id": "lists-012-13007870"}, {"subject": "Re: Two question", "content": "Fred Bohle@NEON\n01/24/2000 09:18 AM\n\nTim,\n\n     I look for either 0A or 0D.  If I find 0D, I look for 0A behind it.\nIf any\nof these conditions are true, it is end of line.  If I find a line with no\ncharacters,\nthis is the null line that ends the headers.  That way you can end any\nline,\nany way you want, without looking for strings like 0D0A0D0A,  or 0A0A.\nI can detect screwy end of line pairs like 0A0D0A or 0D0A0A.\n\n\nFred\n\n\n\n", "id": "lists-012-13015453"}, {"subject": "Re: On pipelining -Reply Repl", "content": "Hi,\n\n\"David W. Morris\" wrote:\n\n> There is no reason for the 'server' which happens to be extended by a CGI\n> or other bit of active code to have to enforce compliance ... that is the\n> responsiblity of the author of the application. And it is the application\n> which will break or otherwise offend the user.  The protocol specifies the\n> correct and safe way to produce well behaved applications.  During the\n> years of producing the HTTP RFCs, it was always understood that a content\n> provider could take the risk of changing server side state ... and a\n> common example is a hit counter on a page. But would you want to insist\n> that pipelining be disabled because a non-semantic state change occured\n> with a different result?\n>\n> I wouldn't.\n\nI don't think anyone was suggesting that pipelining should be disabled for\nsuch applications; simply that the server should process the requests in\nserial order so as to preserve the expected behavior.\n\nI tend to agree with that. I considered multithreaded execution of pipelined\nrequests on a single connection for the Netscape Enterprise Server. However,\nit turned out that most deployed applications are not idempotent (irrespective\nof the request method), so in practice this would create more trouble than it\nwas worth. Also, the extra load on the server, in terms of buffer space and\nextra threads, was significant.\n\nThe cases where processing requests out of order would benefit most are when\nhigh-latency content is involved. Let's say for example there is a CGI request\nsent by the browser to update the HTML content of a frame, followed by\nrequests for static images that were part of a document. The CGI will take\nsome time to execute, and obviously with the current version of HTTP/1.1 that\nwill hold up the images. What is really needed to benefit is a way for the\nserver to send replies out-of-order . So the static images could be delivered\nright away while the CGI application is running. I have heard that HTTP/NG may\nactually resolve that issue but I haven't checked into it yet.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n", "id": "lists-012-13022737"}, {"subject": "Re: Two question", "content": ">\n>2.   When a user is using a Web browser , the \"Reload\" in netscape\n>       or the\"Refresh\" in IE is pressed, what's the policy the browser\n>       to use about when to start with a new connection( SYN issued\n>first).\n>       or to start with just a HTTP request( ACK push)?\n\nThere are no TCP/IP level or connection level policies implied as far\nas I know.  The implied policy is to get the page again, and get a\ncopy fresh from the origin server, without any intermediate cached\ncopy being used instead.  This implies that a new HTTP request has to be\nsent, but not necessarily that an existing persistent connection has\nto be broken.\n\nHope this helps,\n\nKoen.\n\n\n\n", "id": "lists-012-13031854"}, {"subject": "HTTP entity lengt", "content": "Hi,\n\nI would like to know how to get the length of the entity body for an\nHTTP packet. I know that there is a header field called \"content-lenght\"\nbut that this one is not enough to get to know the actual length for any\npossible case. Can anybody explain me what actions should be done in\norder to derive the length of the entity body from the HTTP header\nfields and for any possible situation?\n\nThanks a lot!\n\nJordi\n\n\n\n--\nJordi Ros Giralt\nDepartment of Electrical and Computer Engineering\nUniversity of California, Irvine\nhttp://www.eng.uci.edu/~jros\nphone: (949) 824 8491 (office) / (949) 725 6487 (home)\n\n\n\n", "id": "lists-012-13039184"}, {"subject": "Re: HTTP entity lengt", "content": "In a previous episode jordi ros said...\n\n:: \n:: I would like to know how to get the length of the entity body for an\n:: HTTP packet.\n\nHow to determine the length of the message body is the (exhaustive)\nsubject of section 4.4 of rfc 2616.\n\n-P\n\n\n\n", "id": "lists-012-13046268"}, {"subject": "protocol verificatio", "content": "Hi,\nI need to verify that my http implementation is compliant with\nthe http rfc. Don't we all? ;)\n\nI could install a number of browsers and do some manual testing\nbut it wouldn't stress the finer parts of the protocol (it wouldn't \nbe that professional either, if that is an issue) and regression\ntesting would be a pain.\n\nDoes anyone know about a http-1.1 test suite (free or not) which\nwould do the job?\n\nI did take a look at http://jigsaw.w3.org/HTTP/ but it didn't fit \nthe bill for me...\n\nKind regards\n/Jocke\n\n\n\n", "id": "lists-012-13053680"}, {"subject": "Re: what is the scope of acceptrang", "content": "On Thu, 26 Mar 1998, Daniel Hellerstein wrote:\n\n> (this is a minor point)\n> \n> In section 14.5:                                                                       \n>     The Accept-Ranges response-header field allows the server to\n>     indicate   its acceptance of range requests for a resource ....              \n>      Origin servers that accept byte-range requests MAY send \n>      Accept-Ranges: bytes                                                  \n> \n> It's a bit unclear what the scope of the accept-range\n> response header is. Is it just for the requested resources, or for a larger\n> set of resources?\n> \n> That is, if the origin server will accept byte-range requests\n> for some, but not all, resources (say, not for html documents with\n> server side includes),  then what is the proper usage of\n> Accept-Ranges:?\n> \n\nSee the definition of what a resource is:\n\n   resource\n      A network data object or service that can be identified by a URI,\n      as defined in section 3.2. Resources may be available in multiple\n      representations (e.g. multiple languages, data formats, size,\n      resolutions) or vary in other ways.\n\nIt means it will accept them for that resource, and only that resource.\nThe server may also accept range requests for other resources, but that is\nnot implied by the accept-ranges header.\n\n\n\n", "id": "lists-012-1305802"}, {"subject": "Re: protocol verificatio", "content": "> I need to verify that my http implementation is compliant with\n> the http rfc. Don't we all? ;)\n> \n> I could install a number of browsers and do some manual testing\n> but it wouldn't stress the finer parts of the protocol (it wouldn't \n> be that professional either, if that is an issue) and regression\n> testing would be a pain.\n> \n> Does anyone know about a http-1.1 test suite (free or not) which\n> would do the job?\n\n\ntake a look at www.research.att.com/~bala/papers/{procow-1.ps.gz, ietf99.ps}\nand then send me mail.\n\ncheers,\nbala\n\n\n\n", "id": "lists-012-13060777"}, {"subject": "Re: HTTP entity lengt", "content": "Patrick McManus wrote:\n\n> How to determine the length of the message body is the (exhaustive)\n> subject of section 4.4 of rfc 2616.\n\nThanks !\n\nI have gone through that part of the RFC and now a question arises to me.\nSuppose the following case:\n\nA- Our message includes a message-body, which means that condition 1 in\nsection 4.4 is false, and\nB- There is no Transfer-Encoding, which means that condition 2 in 4.4 is\nfalse, and\nC- There is no Content-Length field, which means that condition 3 in 4.4 is\nfalse (note that this is possible since from section 14.13 we have that\napplications SHOULD (only SHOULD) use this field to indicate the\ntransfer-length when it is not prohibited by rules in section 4.4) and\nD- The content-type is not \"multipart/byteranges\", which means that\ncondition 4 in 4.4 is false, and\nE- The server is not closing the connection, which means that condition 5\nis false.\n\nI think that according to the RFC the previous situation may happen and, if\nso (somebody may help me here), we cannot determine the transfer length\nsince 4.4 does not specify this case. I think that the key point is in the\nSHOULD condition of section 14.13. If instead we use a MUST in this\nsection, then we can break condition C.\nI would appreciate any comments, thanks!!!\n\nJordi\n\n--\nJordi Ros Giralt\nDepartment of Electrical and Computer Engineering\nUniversity of California, Irvine\nhttp://www.eng.uci.edu/~jros\nphone: (949) 824 8491 (office) / (949) 725 6487 (home)\n\n\n\n", "id": "lists-012-13068084"}, {"subject": "RE: HTTP entity lengt", "content": "> From: jordi ros\n\n> Suppose the following case:\n>\n> A- Our message includes a message-body, which means that\n> condition 1 in\n> section 4.4 is false, and\n> B- There is no Transfer-Encoding, which means that\n> condition 2 in 4.4 is\n> false, and\n> C- There is no Content-Length field, which means that\n> condition 3 in 4.4 is\n> false (note that this is possible since from section\n> 14.13 we have that\n> applications SHOULD (only SHOULD) use this field to indicate the\n> transfer-length when it is not prohibited by rules in\n> section 4.4) and\n> D- The content-type is not \"multipart/byteranges\", which\n> means that condition 4 in 4.4 is false, and\n> E- The server is not closing the connection, which means\n> that condition 5 is false.\n\nYour point A is where the problem lies.\n\nIf you don't send a Content-Length, and you don't send a\n'Transfer-Encoding: chunked', and your Content-Type is not some kind\nof 'multipart', then your message does not include a message-body;\nit ends at the CRLF following the last header.  If you are a client,\nthen anything you send following that will be interpreted as the\nbeginning of a new request; if you are a server then I have no idea\nwhat clients might do.\n\nSHOULD does not mean 'you can ignore this rule' - it means that you\ndon't have to do it to be compliant, but you aught to unless there\nis an excellent reason.\n\n\n\n", "id": "lists-012-13076540"}, {"subject": "Re: HTTP entity lengt", "content": ">If you don't send a Content-Length, and you don't send a\n>'Transfer-Encoding: chunked', and your Content-Type is not some kind\n>of 'multipart', then your message does not include a message-body;\n>it ends at the CRLF following the last header.  If you are a client,\n>then anything you send following that will be interpreted as the\n>beginning of a new request; if you are a server then I have no idea\n>what clients might do.\n\nThat applies to request messages only.  For responses other than\nthose required not to have a body, the message body includes\nanything that appears after the header fields until the connection\nis closed.\n\n....Roy\n\n\n\n", "id": "lists-012-13085231"}, {"subject": "Re: HTTP entity lengt", "content": ">> How to determine the length of the message body is the (exhaustive)\n>> subject of section 4.4 of rfc 2616.\n>\n>Thanks !\n>\n>I have gone through that part of the RFC and now a question arises to me.\n>Suppose the following case:\n>\n>A- Our message includes a message-body, which means that condition 1 in\n>section 4.4 is false, and\n>B- There is no Transfer-Encoding, which means that condition 2 in 4.4 is\n>false, and\n>C- There is no Content-Length field, which means that condition 3 in 4.4 is\n>false (note that this is possible since from section 14.13 we have that\n>applications SHOULD (only SHOULD) use this field to indicate the\n>transfer-length when it is not prohibited by rules in section 4.4) and\n>D- The content-type is not \"multipart/byteranges\", which means that\n>condition 4 in 4.4 is false, and\n>E- The server is not closing the connection, which means that condition 5\n>is false.\n>\n>I think that according to the RFC the previous situation may happen and, if\n\nNo, it cannot.  The specification says \"one of the following\".  There is\nno option beyond that.  The server will eventually close the connection\nand thereby will end the body.  The RFC is making a factual statement.\n\n....Roy\n\n\n\n", "id": "lists-012-13093165"}, {"subject": "If-ModifiedSince implementation questio", "content": "Quoting from 14.25 :\n\n\"14.25 If-Modified-Since\n\n   The If-Modified-Since request-header field is used with a method to\n   make it conditional: if the requested variant has not been modified\n   since the time specified in this field, an entity will not be\n   returned from the server; instead, a 304 (not modified) response will\n\n   be returned without any message-body.\n\n<snip>\n\n      Note: When handling an If-Modified-Since header field, some\n      servers will use an exact date comparison function, rather than a\n      less-than function, for deciding whether to send a 304 (Not\n      Modified) response. To get best results when sending an If-\n      Modified-Since header field for cache validation, clients are\n      advised to use the exact date string received in a previous Last-\n      Modified header field whenever possible. \"\n\nConsider the following case :\na) user browses a web site, gets a document\nb) server returns the date of the document, and a Last-Modified : xxx\n\nThe next day, the document is updated with a different version, and an\nolder date, yyy.\n\nc) user browses the web site, gets a document again, with\nIf-Modified-Since : xxx\nd) server does a \"less than\" comparison and returns 304\n\nIn this case the user just does not see the latest document, unless he\ndisables the caching.\nBut the situation could be much worse if byte ranges are involved. The\nbrowser could end up with a mix of several documents!\n\nTherefore, the servers that are doing an exact date comparison, as\nsuggested in the note, are doing the only safe thing.\nDoing a \"less than\" type of comparison will cause problems when files\nget rolled back to older versions.\nIf-Unmodified-Since suffers from the same flaw and is equally dangerous\n.\n\nWas the possibility of files being rolled to older versions just\noverlooked in the HTTP spec ?\nIf not, how are these cases supposed to be handled ?\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n", "id": "lists-012-13101207"}, {"subject": "Re: If-ModifiedSince implementation questio", "content": "On Wed, 2 Feb 2000, Julien Pierre wrote:\n\n> \n> Was the possibility of files being rolled to older versions just\n> overlooked in the HTTP spec ?\n> If not, how are these cases supposed to be handled ?\n\nOverlooked ... no, when the file is changed is must have a newer last\nchanged date ... the content of the web server changed even if the file\ncontains an older version of the content.  ETags were invented as the full\nsolution.\n\nDave Morris\n\n\n\n", "id": "lists-012-13109944"}, {"subject": "Re: If-ModifiedSince implementation questio", "content": "David,\n\n\"David W. Morris\" wrote:\n\n> On Wed, 2 Feb 2000, Julien Pierre wrote:\n>\n> >\n> > Was the possibility of files being rolled to older versions just\n> > overlooked in the HTTP spec ?\n> > If not, how are these cases supposed to be handled ?\n>\n> Overlooked ... no, when the file is changed is must have a newer last\n> changed date ... the content of the web server changed even if the file\n> contains an older version of the content.\n\nThat's all very good in theory.\nIn practice, some file systems on some operating systems (for example FAT on\nWindows NT) do not preserve the last-written, access or creation date. They\nonly keep a file date. Therefore, it is not feasible for the web server to\ntell that the document was indeed modified with an older date.\n\nIf it is still running, the daemon file cache could conceivably monitor the\ndates on all the files and notice that some were rolled back to an older date.\nThis would be a very expensive operation, and the information would be lost if\nthe server is shut down, as server caches are normally not persisted. It would\nalso be subject to limitations of the cache size - you don't want to cache\nprior date information for all files that were served, because your cache\ncannot use an unlimited amount of RAM. The much easier solution is to do an\ndate equality test instead of less than.\n\n> ETags were invented as the full\n> solution.\n\nYes, they solve the problem in fact. But most of today's browsers don't use\nit, and they will be around for a while. A new HTTP server still has to be\nable to work with them.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n", "id": "lists-012-13118070"}, {"subject": "Chunked sample messag", "content": "Hi,\n\nI've just read the section 3.6 (Transfer  Codings/Chunked Transfer\nCodings) of RFC 2616 and I think I understand the way the chunked\nmessages are built. However, I would be truly confident on that if I\ncould see a real example of a message using this packet transfer\nencoding. Does anybody knows a URI where the server responds sending\nchunked packets if I make a GET when making a telnet on it? Or just...\nwhere can I find a parsed packet sample showing the chunked structure?\n\nThanks!\n\nJordi\n\n\n\n", "id": "lists-012-13126649"}, {"subject": "RE: Chunked sample messag", "content": "> Does anybody knows a URI where the server \n> responds sending\n> chunked packets if I make a GET when making a telnet on \n> it? \n\nThere is a list of servers at:\n\nhttp://www.w3.org/Protocols/HTTP/Forum/\n\nMany of them have chunked response pages you can hit - among them:\n\n  http://test11.agranat.com/\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-13133657"}, {"subject": "cookies paten", "content": "The state management specification needs to reference the patent\nthat Lou / Netscape has on the technology.\n\nhttp://patent.womplex.ibm.com/details?pn=US05774670__\n\n....Roy\n\n\n\n", "id": "lists-012-13141290"}, {"subject": "RE: comments on draft-ietf-http-trust-state-mgt02.tx", "content": "Thanks for the extensive review and feedback!\n\n\nI will incorporate your comments except as noted below:\n\n>-----Original Message-----\n>From:Dave Kristol [SMTP:dmk@research.bell-labs.com]\n>Sent:Wednesday, March 25, 1998 1:00 PM\n>To:Jaye, Dan\n>Cc:http-state@lists.research.bell-labs.com; http-wg@cuckoo.hpl.hp.com\n>Subject:comments on draft-ietf-http-trust-state-mgt-02.txt\n>\n>Here are my comments on draft-ietf-http-trust-state-mgt-02.txt.  I\n>think the default settings for the use of trust labels and cookies need\n>to be discussed more.  See below, under 3.3.2.\n>\n>    privacypractice = \"purpose\" 1*purposerating \n>--> 1*purposerating is inconsistent with the later examples,\n>which show things like \"0:4\" (several places).\n>[Include and indent original message text]  I will incorporate the full BNF\n>for PICS ratings which supports enumerated rating numbers and ranges.\n>\n>    trust-label-data = labelattr-data privacy-practice [cookielist]\n>    ...\n>    purpose. Services that disclose that they use data for \"other\" purposes \n>    should provide human readable explanations of those purposes. \n>--> The specification needs to say where/how to get them.\n>[Include and indent original message text]  This should be explained in the\n>document referenced by commenturl.\n>\n>    \"Contacting Visitors for Marketing of Services or Products\" \n>     value = 4\n>--> I'm unclear what this means.  Does it mean that the cookie\n>accompanies (and is) the \"contact\", or does it imply there's\n>some kind of subsequent or out-of-band contact?\n>\n>For example, a cookie from an (third-party) advertising site\n>might be construed as purpose 4.\n>[Include and indent original message text]  It is meant to imply subsequent\n>out-of-band contact.  Displaying of a banner ad for example does not imply\n>contact.\n>\n>    \"non-identifiable\"                               value = 0\n>    \"identifiable\"                                   value = 1\n>--> This begs for a definition of what comprises \"information\n>gathered ... in identifiable form\", or, at the very least, an\n>example.\n>\n>[Include and indent original message text]  \n>P3P will go into greater detail but I will expand this in the document.\n>\n>\n>3.3.2Accepting or rejecting Cookies\n>    User agents should have the following default preferences:\n>      Automatically accept:\n>Cookies from verifiable transactions with trust-labels with \n>  purposerating 0 through 4 and dourating 0;\n>Cookies from unverifiable transactions with trust-labels with \n>  purposerating 0 through 4 and idrating 0 and \n>  dourating 1.\n>      Automatically reject:\n>Cookies from unverifiable transactions without trust-labels.\n>\n>--> IMO the default purposerating should be 0-3.  If you want to\n>create an installation option that asks users (\"opt in\") whether\n>4 is okay, fine.\n>[Include and indent original message text]  You are absolutely correct.  This\n>is a holdover of a version where purposerating 4 was still safe.  Out of band\n>contact should definitely not be a default.\n>\n>--> There's no mention above of *signed* trust labels.  IMO\n>cookies for unverifiable transactions should require a signed\n>label (by default).\n>\n>[Include and indent original message text]  \n>Part of the issue is the lack of maturity of digital signature\n>infrastructure.\n>I am interested in feedback on simplifying the sigblock options to explicitly\n>state RSA encryption given that RSA labs has given permission for use of\n>their implementation for non-commercial software for PICS Labels.\n>\n>\n>--> As the specification is now written, the \"by\" attribute is\n>optional, as are signatures.  So a site need not even name a\n>trust authority, can invent trust labels, and can bypass the\n>\"protections\" the defaults should provide.  It also makes it\n>even simpler to work around the admittedly weak \"protections\"\n>in state-man-mec-08.  That's unsatisfactory.\n>\n>I think the spec. should require, at a minimum, either a \"by\"\n>or a sigblock.  The \"by\" is a lame protection, but at least if\n>a site fraudulently sends an unsigned label by a bogus trust\n>authority there might be some recourse under consumer fraud\n>rules.\n>[Include and indent original message text]   This sounds right to me.  I will\n>word this as:\n>MUST include \"by\" or sigblock.  SHOULD include sigblock.\n>\n\n\n\n", "id": "lists-012-1314399"}, {"subject": "Re: cookies paten", "content": ">Last I looked I could not find a complete set of mail archives from these\n>lists, or other important historical lists that formed the early web\n>software community.  It would be a shame to allow this patent, but it\n>would be a bigger shame to lose that history.\n\nAll of http-wg is archived at\n   <http://www.ics.uci.edu/pub/ietf/http/hypermail/mboxes/>\n\nMost of www-talk is on egroups.com (somewhere), but Kevin Hughes\nprobably has a complete archive stashed away somewhere.\n\n....Roy\n\n\n\n", "id": "lists-012-13148718"}, {"subject": "Re: cookies paten", "content": "??? I for one would be very pleased if the Cookies patent was rigorously\nenforced, provided nobody could get a license on any terms whatsoever.\n\nCookies are an abomination. As many predicted they are used to invade\nprivacy.\n\nNot that we can get rid of them, but cest la vie.\n\n        Phill\n\n----- Original Message -----\nFrom: Roy T. Fielding <fielding@kiwi.ics.uci.edu>\nTo: Brian Behlendorf <brian@apache.org>\nCc: <http-wg@cuckoo.hpl.hp.com>\nSent: Saturday, February 05, 2000 8:15 PM\nSubject: Re: cookies patent\n\n\n> >Last I looked I could not find a complete set of mail archives from these\n> >lists, or other important historical lists that formed the early web\n> >software community.  It would be a shame to allow this patent, but it\n> >would be a bigger shame to lose that history.\n>\n> All of http-wg is archived at\n>    <http://www.ics.uci.edu/pub/ietf/http/hypermail/mboxes/>\n>\n> Most of www-talk is on egroups.com (somewhere), but Kevin Hughes\n> probably has a complete archive stashed away somewhere.\n>\n> ....Roy\n>\n>\n\n\n\n", "id": "lists-012-13156459"}, {"subject": "Re: cookies paten", "content": "In a message dated 2/5/00 10:08:05 PM Eastern Standard Time, \nhallam@ai.mit.edu writes:\n\n> ??? I for one would be very pleased if the Cookies patent was rigorously\n>  enforced, provided nobody could get a license on any terms whatsoever.\n>  \n>  Cookies are an abomination. As many predicted they are used to invade\n>  privacy.\n>  \n>  Not that we can get rid of them, but cest la vie.\n\nIt would hardly be the first time a disgusting kludge has been patented.\n\nJoachim Martillo\n\n\n\n", "id": "lists-012-13165598"}, {"subject": "IETF Adelaide and interim meetings for APPS WG", "content": "It has come to the attention of the Applications Area Directors\nthat one or more Applications area working groups have elected\nto not meet in Adelaide, and instead to hold an \"interim meeting\"\nin the United States, presumably because of distance and/or cost issues.\n\nIETF is an international organization, and it is IETF's longstanding \npractice to hold its meetings in various locations around the planet.\nThis serves both to encourage wider participation in IETF and also\nto more fairly distribute travel costs and inconvenience (over time) \namong all participants.  The scheduleing of an interim WG meeting in \nthe US in lieu of a WG meeting in Adelaide undermines this policy.  \nThis is insulting to non-US participants of IETF (many of whom have \nattended meetings in the US for years), embarassing to IETF as \na whole, and a threat to IETF's international stature.\n\nEven if a working group has few participants outside the United\nStates, a working group does not work in isolation from other\nworking groups.  Attendance at IETF meetings is an invaluable \nmechanism for cross-group collaboration.  \n\nRFC 2418 states:\n\n   Interim meetings are subject to the\n   same rules for advance notification, reporting, open participation,\n   and process, which apply to other working group meetings.\n\nSince normal working group meetings require advance notification\nvia email to the entire IETF list, and the process for getting a meeting\nslot involves prior approval of the Area Directors, the same\nrequirements apply to interim working group meetings.  Part of the \nreason for prior approval being required is to ensure that the \nlocations of the meetings are not being chosen to favor certain \nparticipants over others.  \n\nThere have been several violations of this policy since publication\nof RFC 2418.\n\nTherefore,\n\n- All interim meetings within the Applications Area which were not\n  previously and explicitly approved by the Applications Area Directors, \n  are hereby cancelled.\n\n- No Applications Area group will hold any interim meeting prior\n  to April 15.\n\n- No Applications Area group which does not hold a meeting in \n  Adelaide, will hold any interim meeting prior to July 31.\n  (i.e. prior to the Pittsburg IETF meeting)\n\n- This applies to all face to face meetings held for the purpose \n  of conducting working group discussion and to which the working \n  group is invited, even if labelled \"informal\" or otherwise \n  labelled to distinguish them from official working group meetings.\n\n- Exceptions to this policy may be made for recently chartered groups,\n  but Area Director approval is still required for such groups to\n  schedule interim meetings.\n\n\nfor the Applications Area Directors,\n\nKeith Moore\n\n\n\n", "id": "lists-012-13173262"}, {"subject": "Requirements Summar", "content": "A while back (I think it was late 1998), one of the Authors posted an\naddress (http://www.geocities.com/SiliconValley/Garage/3246/http11rq.txt)\nfor a draft of a HTTP/1.1 Requirements Summary.\n\nWhat has become of this document, both in terms of where it is, and status?\nIt would be most useful...\n\nCheers,\n\n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-13182294"}, {"subject": "Testing web server spee", "content": "Hi, \n\nI want to test my web server speed and I'm looking for some tool that can simulate thousands of users sending requests to the web server. I tried VELOMETER but this one does not support HTTP1.1. Does anybody knows of any such software that is HTTP1.1 capable?\n\nThanks,\n\nJordi\n\n\n\n", "id": "lists-012-13188797"}, {"subject": "Re: Testing web server spee", "content": " > I want to test my web server speed and I'm looking for some tool that\n can simulate thousands of users sending requests to the web server. I\n tried VELOMETER but this one does not support HTTP1.1. Does anybody knows\n of any such software that is HTTP1.1 capable?\n\nTry 'ab' (Apache benchmark). A simple http request/benchmarking program\nthat comes with Apache.\n\nI'm not sure which features you need from http/1.1, but this client is\nfairly simple code, and sometimes very useful.\n\nOn the downside, it doesn't simulate real world requests very well\n(dropped connections, slow connections, network lag times, etc...) but\ndoes give some indication of relative server speed.\n\n-drew\n\n-------\nDrew Streib <streib@li.org> 408.542.5725\n\nSystem Administrator, Linux International   | <dtype@li.org>\nInformation Architect, VA Linux Systems     | <dtype@valinux.com>\nSenior Developer, SourceForge               | <dtype@sourceforge.net>\nAdmirer, Occasional Programmer, Linux.com   | <dtype@linux.com>\nFounder, Shok Media                         | <dtype@shok.com>\n\n\n\n", "id": "lists-012-13195665"}, {"subject": "HTT", "content": "Hi all,\n\nCan anybody give me the diagram state of HTTP?\nI want to make it in PROMELA. I really appriciate it.\nThank you.\n\n-h\n\n\n\n", "id": "lists-012-13205436"}, {"subject": "Two newish technical reports based on an HTTP trac", "content": "About a year ago, I gathered a long (90-day) proxy trace of\nHTTP requests, including MD5 digests of the response bodies.\n(No, you can't get access to this trace, so please don't even ask.)\nI've finished a pair of technical reports based on this trace:\n\n(1)\n    \"A trace-based analysis of duplicate suppression in HTTP\"\n    Compaq Western Research Lab Research Report 99/2, November 1999\n    \n    Many HTTP resources (pages, graphics, etc.) are exact\n    duplicates of other resources with different URLs. If an HTTP\n    cache contains a duplicate of a requested resource, and could\n    detect this, it could avoid substantial network costs by\n    returning the cached duplicate in place of the requested URL.\n    Previous studies have shown that there is substantial\n    duplication of content in both HTTP and FTP, and several\n    protocols have been proposed to support efficient and safe\n    duplicate suppression in HTTP. We use traces covering\n    millions of HTTP requests to quantify the potential benefit\n    of an HTTP duplicate-suppression extension. In particular, we\n    show that the benefits vary depending on content-type, and\n    that a small fraction of Web servers account for most of the\n    duplicated resources.\n\n    http://www.research.digital.com/wrl/techreports/abstracts/99.2.html\nfor Postscript & PDF format versions\n\n(2) \"Errors in timestamp-based HTTP header values\"\n    Compaq Western Research Lab Research Report 99/3, December 1999\n\n    Many of the caching mechanism in HTTP, especially in\n    HTTP/1.0, depend on header fields that carry absolute\n    timestamp values.  Errors in these values could lead to\n    undetected cache incoherence, or to excessive cache misses.\n    Using an extensive proxy trace, we looked for HTTP responses\n    exhibiting several different categories of timestamp-related\n    errors.  A significant fraction of these responses have\n    detectable errors in timestamp-based header fields.\n    \n    http://www.research.digital.com/wrl/techreports/abstracts/99.3.html\nfor Postscript & PDF format versions\n\n-Jeff\n\nP.S.: The astute observer will notice that there is also a\ntimestamp error in the date on one of these technical reports; I\nwas a bit optimistic about when I would finish it, and then it\nwas easier to issue it back-dated than to revise our report\nnumbering scheme.\n\n\n\n", "id": "lists-012-13211620"}, {"subject": "HTTP traffic statistic", "content": "Hi,\n\nI am doing some analysis on the performance of an HTTP server for my research. Does anybody has or know where to get some information concerning real traffic statistics? More specifically, I would like to know what's the traffic percentage of each HTTP method (GET, PUT, HEAD,...) in a real world network. Any other statistics regarding HTTP traffic (such as number of bytes or packets per method-request ...) would also be of great help.\n\nThanks in advance!\n\nJordi\n\n\n-----------------------------------------------------------------------------------------------------------------\nJordi Ros                                             email: jros@ece.uci.edu\nElectrical and Computer Engineering      voice (work): (949) 622 0991\nUniversity of California, Irvine                  www: http://www.ece.uci.edu/~jros\n-----------------------------------------------------------------------------------------------------------------\n\n\n\n", "id": "lists-012-13220383"}, {"subject": "HTTP Age Header Field (Section 14.6) - should overflow value be 2^31 or (2^31)1", "content": "Section 14.6 of RFC 2616 states that if a cache receives an age that is\nlarger than it can represent, or if overflow occurs, then it must transmit\nan Age header with a value of 2147483648 (2^31).  Was the intent to select\nthe largest unsigned 31-bit integer, which is really (2^31)-1, or is there\nsome other rationale behind this choice?\n\nMD\n\n\n\n", "id": "lists-012-13227821"}, {"subject": "Re: HTTP Age Header Field (Section 14.6) - should overflow value be 2^31  or (2^31)1", "content": "Matthew Delco wrote:\n\n> Section 14.6 of RFC 2616 states that if a cache receives an age that is\n> larger than it can represent, or if overflow occurs, then it must transmit\n> an Age header with a value of 2147483648 (2^31).  Was the intent to select\n> the largest unsigned 31-bit integer, which is really (2^31)-1, or is there\n> some other rationale behind this choice?\n\n2^31 is the smallest positive integer which cannot be represented in a signed\n32-bit value; was that it?\n\n--\n/=================================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.   |\n|Chief Scientist |================================================|\n|eCal Corp.      |I will not buy this .signature, it is scratched.|\n|francis@ecal.com|                                                |\n\\=================================================================/\n\n\n\n", "id": "lists-012-13235595"}, {"subject": "Re: HTTP Age Header Field (Section 14.6) - should overflow value be 2^31  or (2^31)1", "content": "On Mon, 13 Mar 2000, John Stracke wrote:\n\n> Matthew Delco wrote:\n> \n> > Section 14.6 of RFC 2616 states that if a cache receives an age that is\n> > larger than it can represent, or if overflow occurs, then it must transmit\n> > an Age header with a value of 2147483648 (2^31).  Was the intent to select\n> > the largest unsigned 31-bit integer, which is really (2^31)-1, or is there\n> > some other rationale behind this choice?\n> \n> 2^31 is the smallest positive integer which cannot be represented in a signed\n> 32-bit value; was that it?\n\nI guess my main question is whether 1) it was the intent of the HTTP\nspecification to pick a 'maximum' value which can be represented by a\nsigned 32-bit value (and thus 2^31 was accidentally specified instead of\n2^31-1), or 2) a conscious decision was made to specify a value outside of\nthis range.  The later case seems less likely, but if it's true then I'm\nhaving trouble seeing why such value was selected given that it's likely\nto add some (admittedly trivial) complexity to an implementation.  The\nspec. could be read as stating \"if a cache receives a value larger than\nthe largest positive integer it can represent ... it MUST transmit a Age\nheader with a value [larger than it can represent]\".  Anyway, I doubt\nthere will ever be a page that's almost 70 years old, but it struck me as\nodd nonetheless.\n\nMD\n\n\n\n", "id": "lists-012-13244678"}, {"subject": "Re: HTTP Age Header Field (Section 14.6) - should overflow value be 2^31  or (2^31)1", "content": "On Mon, Mar 13, 2000 at 11:46:45AM -0800, Matthew Delco wrote:\n> header with a value [larger than it can represent]\".  Anyway, I doubt\n> there will ever be a page that's almost 70 years old, but it struck me as\n> odd nonetheless.\n\nMore to the point, it's a big cache indeed that keeps an object for that\nlong without validating it...\n\n*grin*\n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-13254391"}, {"subject": "user agent header questio", "content": "hello,\ni have one simple question: why user-agent header isn't standardized to\nreturn much more useful information that it does today? it says completely\nnothing about what is supported at client side, and we see so many\njavascripts on the net, that perform sily detection of browser and\nguess what that browser can. it would be much more useable to have\nstandardized header (why not User-Agent?) in a form like:\n\nUser-Agent: <user_agent_description> <header_separator> <supported_features_list>\n\nwhere (mix of regex and C style declarations)\n\n<user_agent_description>  -- <any_char> *\n<header_separator>        -- \"::\"\n<supported_features_list> -- <feature> [<feature_separator> <feature> \\\n                             [<feature_separator> <feature> [...]]]\n\n<feature>                 -- <feature_name>[<info_separator> <additional_info>]\n<feature_name>            -- <char> +\n<info_separator>          -- '/'\n<additional_info>         -- <char> +\n<feature_separator>       -- ','\n\n<any_char>                -- ' ' .. '\\x7f'\n<char>                    -- '!' .. '\\x7f' # no space\n\nthat would return for example\n\nUser-Agent: Mozilla/4.61 [en] (X11; U; OpenBSD 2.6 i386; Nav) :: HTML/4.0,JavaScript/1.2,Java/1.2.2,Frames,DHTML,Stylesheets\n\n(well, don't really pay attention to the numbers and items :-) instead of:\n\nUser-Agent: Mozilla/4.61 [en] (X11; U; OpenBSD 2.6 i386; Nav)\n\nthe reason is: the first header says me everything about what the\nbrowser supports, and the second gives nothing. it also seems\nreasonable to me to add this information to User-Agent header,\nbecasue it lists features of the borwser. however, it would\nbe anything bad if it would be User-Agent-Supports or something\nlike that.\n\nThank you in advance!..\n-- \nDenis A. Doroshenko\nOmnitel Ltd., Sevcenkos 25, Vilnius 2600, Lithuania\nmailto:d.doroshenko@omnitel.net\n\n\n\n", "id": "lists-012-13262402"}, {"subject": "Proxies, Request-URI, and digestur", "content": "Is it optional, or required, for proxy servers to rewrite the Request-URI\nfrom absoluteURI to abs_path form? I can't find where it is specified that\nit is allowed, except this reference in 2617: (3.2.2)\n\n   digest-uri\n     The URI from Request-URI of the Request-Line; duplicated here\n     because proxies are allowed to change the Request-Line in transit.\n\nIf it *is* up to the proxy server whether it rewrites the Request-URI or\nnot, I think that makes 2617 sec 3.2.2.5 an impossible demand? Regarding\nthe value of the digest-uri field submitted in an Authorization: header.\n\n                                           In particular, it MUST\n   be an \"absoluteURL\" if the Request-URI is an \"absoluteURL\".\n\n... if the proxy may change Request-URI from absoluteURI to abs_path, or\nit may leave it as an absoluteURI, how will the client know which to\nsubmit in digest-uri?\n\n(a typo, too: that section uses \"absoluteURL\" twice... should be\nabsoluteURI?)\n\nRegards,\n\njoe\n\n\n\n", "id": "lists-012-13271448"}, {"subject": "RE: user agent header questio", "content": "Hello!\n\nI apologize for my rude behavior, in advance.  Mailing lists frequently have\ntags, at the bottom with list information (i.e.: subscribe/unsubscribe,\narchive info, etc.).  However, this list does not follow that convention.\n\nI don't have the original email, which probably contained the info about\nunsubscribing.  I attempted to unsubscribe with this:\nhttp-wg-unsubscribe@hplb.hpl.hp.com\n\nObviously that attempt failed, so I am posting to this list for help.\nPlease, provide information about unsubscribing to this list.\n\nThank you,\nLarry\n\n-----Original Message-----\nFrom: Denis A. Doroshenko [mailto:d.doroshenko@omnitel.net]\nSent: Tuesday, March 14, 2000 2:12 AM\nTo: http-wg@cuckoo.hpl.hp.com\nSubject: user agent header question\n\n\nhello,\ni have one simple question: why user-agent header isn't standardized to\nreturn much more useful information that it does today? it says completely\nnothing about what is supported at client side, and we see so many\njavascripts on the net, that perform sily detection of browser and\nguess what that browser can. it would be much more useable to have\nstandardized header (why not User-Agent?) in a form like:\n\nUser-Agent: <user_agent_description> <header_separator>\n<supported_features_list>\n\nwhere (mix of regex and C style declarations)\n\n<user_agent_description>  -- <any_char> *\n<header_separator>        -- \"::\"\n<supported_features_list> -- <feature> [<feature_separator> <feature> \\\n                             [<feature_separator> <feature> [...]]]\n\n<feature>                 -- <feature_name>[<info_separator>\n<additional_info>]\n<feature_name>            -- <char> +\n<info_separator>          -- '/'\n<additional_info>         -- <char> +\n<feature_separator>       -- ','\n\n<any_char>                -- ' ' .. '\\x7f'\n<char>                    -- '!' .. '\\x7f' # no space\n\nthat would return for example\n\nUser-Agent: Mozilla/4.61 [en] (X11; U; OpenBSD 2.6 i386; Nav) ::\nHTML/4.0,JavaScript/1.2,Java/1.2.2,Frames,DHTML,Stylesheets\n\n(well, don't really pay attention to the numbers and items :-) instead of:\n\nUser-Agent: Mozilla/4.61 [en] (X11; U; OpenBSD 2.6 i386; Nav)\n\nthe reason is: the first header says me everything about what the\nbrowser supports, and the second gives nothing. it also seems\nreasonable to me to add this information to User-Agent header,\nbecasue it lists features of the borwser. however, it would\nbe anything bad if it would be User-Agent-Supports or something\nlike that.\n\nThank you in advance!..\n--\nDenis A. Doroshenko\nOmnitel Ltd., Sevcenkos 25, Vilnius 2600, Lithuania\nmailto:d.doroshenko@omnitel.net\n\n\n\n", "id": "lists-012-13279185"}, {"subject": "Re: Proxies, Request-URI, and digestur", "content": "> The client puts in digest-uri exactly what it put on the Request-URI.\n\nAh... my problem was, that the request then gets to the origin server as: \n\nFOOBAR /random/uri HTTP/1.1\nHost: origin.server.com\nAuthorization: Digest blah blah \n      digest-uri=\"http://origin.server.com/random/uri\" blah blah\n\nAnd I read 3.2.2.5 to mean \"digest-uri and Request-URI must match\noctet-by-octet\", where here they don't. Reading it again doesn't give the\nsame impression, though... the above request should not give a 400\nresponse?\n\nMy mistake, sorry. \n\njoe\n\n\n\n", "id": "lists-012-13289510"}, {"subject": "RE: comments on draft-ietf-http-authentication01.tx", "content": "> ----------\n> From: Dave Kristol[SMTP:dmk@research.bell-labs.com]\n> Sent: Monday, March 23, 1998 1:45 PM\n> To: jg@w3.org; masinter@parc.xerox.com\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: comments on draft-ietf-http-authentication-01.txt\n> \n> Here are some comments on draft-ietf-http-authentication-01.\n> \n> Dave Kristol\n> ======================\n> Substantive stuff:\n> \n> General\n>     (Formatting)  On a number of pages (e.g., 5,7,9), the text runs\n>     into the footer.\n> \n>     The spec. is incomplete in its description of whether auth-params\n>     are case-sensitive.  For example, \"realm\" and \"stale\" are mentioned\n>     explicitly.  I think it would be simpler to say the\n>     attribute/parameter/directive (see below) names are\n>     case-insensitive, but that the values may or may not be\n>     case-sensitive.\n> \n>     What should a client do if it receives unrecognized attributes?\n> \nIgnore them. I thought that was the \"HTTP way\" and needn't be expliclty\nstated.\n\n>     What should a server do if it receives unrecognized attributes?\n> \nDitto.\n\n> 1.2 Access Authentication Framework\n>     credentials = basic-credentials | auth-scheme #auth-param\n> \n>     I think this should now read\n>     credentials = basic-credentials | digest-credentials\n> | auth-scheme #auth-param\n> \nFine with me.\n\n>     The term \"protection space\" gets used without a definition (here),\n>     but the spec. describes how a client can reuse credentials for such\n>     a protection space.  I think we should say that the description of\n>     any auth-scheme must describe the rules for deciding when two\n>     objects are in the same protection space.  In particular, a client\n>     must be able to tell, so it knows whether or not to send credentials\n>     unprompted.\n> \nWhy does it need to tell? If it's wrong, by either sending incorrect ones or\nnot sending any, it'll get a 401 to tell it what to do. As far as I can see,\nfor Digest it's only an optimization. (For Basic, you don't want to send\nyour credentials to the wrong place...)\n\n> Sect. 3.2.1, The WWW-Authenticate Response Header\n>     [domain attribute]\n>     If this keyword is omitted or empty, the client should assume that\n>     the domain consists of all URIs on the responding server.\n> \n> This behavior is different from Basic.  If we want Digest to be\n> a more or less drop-in replacement, shouldn't the default\n> behavior mimic Basic?\n> \nAs you point out below, there are implementations. As I point out above, it\nshouldn't matter. If I were writing a browser, I'd guess that I should reuse\nthe key obtained from a previous 401/WWW-Auth until I left the server --\nthat way, I minimize the extra roundtrips.\n\nIn any case, this behavior is backwards compatible with Basic.\n\nNet-net: no compelling reason to change.\n\n> Furthermore, this paragraph constitutes the equivalent of a\n> description of the Digest protection space, but it never calls\n> it such.  It would be helpful to be more explicit.  (There's\n> additional related information in section 3.3.)\n> \nI'm not sure it is. \n\n> Sect. 3.2.2, The Authorization Request Header\n>     [cnonce attribute]\n>     RFC 2069-compliant implementations might break upon receiving this\n>     new, previously unknown attribute.\n> \nThen they would be broken. Unknown directives are supposed to be ignored.\n\n>     [nonce-count]\n>     The grammar for nc-value says\n> nc-value         = 8LHEX\n>     but the examples all show four-character nc= values, as in this\n>     section.  The examples ought to be compliant. :-)\n> \nSure.\n\n>     Also, why have such a restrictive syntax?  Why not 1*LHEX?\n> \nIt was 4LHEX, and I didn't think that 16 bit counter was enough, so I bumped\nit to 32; 1*LHEX is OK by me.\n\n>     [description of calculations]\n>     If the \"qop\" value is \"auth\":\n> Shouldn't this read\n>     If the \"qop\" value is \"auth\" or \"auth-int\":\n> \nYes.\n\n> Sect. 3.2.3, The Authentication-Info Header\n>     What should a client do if the rspauth=response-digest information\n>     is wrong?\n> \nNot accept the response.\n>        \n>     Isn't there the risk that an intervening proxy could change the\n>     status code?\n> ... Authorization header for the request, A2 is\n>    A2       = Status-Code \":\" digest-uri-value\n> and if \"qop=auth-int\", then A2 is\n>    A2       = Status-Code \":\" digest-uri-value \":\" H(entity-body)\n> \nWell, the status code isn't a header, but there's a general proscription\nagainst needlessly changing headers in 13.5.2. Maybe the status line\ncontents should be explicitly added to that list.\n\n> Typographical nits, etc.:\n> \nAs soon as can be done.\n\nPaul\n\n\n\n", "id": "lists-012-1329111"}, {"subject": "Last Call on delta encoding in HTTP/1.", "content": "    \nFolks,\n\nAs some of you know, we have been pushing addition of a delta encoding\nmechanism for http/1.1. The early paper on this was in SIGCOMM'97\n      \nPotential benefits of delta encoding and data compression for HTTP \n    http://www.research.att.com/~bala/papers/sigcomm97.ps.gz\n    \n3 ID's have been written and recently revised:\n\n    Delta encoding in HTTP:\n    http://www.ietf.org/internet-drafts/draft-mogul-http-delta-03.txt\n    \n    Instance Digests in HTTP:\n    http://www.ietf.org/internet-drafts/draft-mogul-http-digest-02.txt\n    \n    The VCDIFF Generic Differencing and Compression Data Format    \n    http://www.ietf.org/internet-drafts/draft-korn-vcdiff-01.txt\n    \nThis message constitutes the last call before we ask the IESG to consider\nthis as a Proposed Standard. If you have any concerns/questions, please send \nme email.\n    \ncheers,\nbala\n    \nbalachander krishnamurthy, at&t labs--research\nwww.research.att.com/~bala/papers\n\n\n\n", "id": "lists-012-13297072"}, {"subject": "Proxies and incorrect ContentLengt", "content": "I'm looking for a brief rundown on best-practice for how non-\ncaching, limited-buffering, proxies should handle origin server\nresponses with incorrect Content-Length headers.\n\nAs far as I can make out there are are only two cases where\na proxy will be able to _reliably_ detect an incorrect Content-\nLength,\n\n  HTTP1.1 origin server with Connection: close\n  HTTP1.0 origin server with no Connection: keep-alive\n\nin both cases a proxy can infer a Content-Length overrun\nbecause it expects the connection to be closed at the end of \nthe response entity. Overruns with persistent connections can't\neasily be distinguished from a broken subsequent response, and\nunderruns can't easily be distinguished from a broken \nconnection.\n\nGiven that overruns are quite common (usually the result of \nbroken CGIs/SSIs not accounting for the length of non-static \ndata) I'd quite like to be able to forward such responses.\nHowever I don't want to have to buffer the whole response to\nrecalculate the CL. I can see a couple of possibilities,\n\n  HTTP1.1 downstream client\n    Strip off the response Content-Length and forward with\n    chunked transfer encoding.\n\n  HTTP1.0 downstream client\n    Strip off the response Content-Length and close the\n    connection after the response entity.\n\nUnfortunately the second of these effectively precludes the\nuse of Keep-Alive on all HTTP1.0 responses: because the proxy\nwon't be able to determine whether or not there's been an\noverrun until the origin-server has run over the end, so *all*\nresponses have to be presumed to be potential overrunners.\n\nOther options avoid that problem, but look troublesome,\n\n  Truncate the reponse entity\n    Dangerous for non text/* types; problematic even for those\n    (eg. stripped trailing copyright messages).\n\n  Forward any content overrun, then close the connection.\n    Problematic for HTTP1.0 Keep-Alive clients which might\n    attempt to interpret the overrun as the headers of a\n    subsequest response; technically illegal for an HTTP1.1\n    proxy. OTOH, the proxy would be forwarding stuff which is\n    no more broken than would have been received had the origin\n    server been contacted directly.\n\nOpinions?\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-13305025"}, {"subject": "(no subject", "content": "help\n\n\n\n\n", "id": "lists-012-13314439"}, {"subject": "A Typical HTTP Proble", "content": "Hi All,\n\nMy basic idea is to port an access db to a mysql db.\n\nI have written a VC++ client which sends an http request  to a java servelet\nwhich accepts the request and sends it to a mysql db.\n\nI am reading all the records from a table in my access db and pumping each\nrecord as a string to the servelet.\n\nNow if i have 112 records in the table, the servelet gets only 56 requests\nand only 56 records get into the mysql database.\n\nThe Java Web Server - jrun needs apache on linux.. so basically jrun sits on\napache which processes the http requests .\n(jrun is required to run servelets on linux).\n\nlooks like an http timeout prob or somethin , but not able to figure out the\nprob.\n\nStill wondering if the problem is with HTTP/Apache/ Java Servelet / JRun or my\nclient.\n\nCould i please have some help on this coz this is really urgent.\n\nThanks in advance,\n\n\nVivek.\nVersaware Inc.\nvivekb@versaware.com\n\n\n\n", "id": "lists-012-13320398"}, {"subject": "Re: Proxies and incorrect ContentLengt", "content": "    I'm looking for a brief rundown on best-practice for how non-\n    caching, limited-buffering, proxies should handle origin server\n    responses with incorrect Content-Length headers.\n\nWe decided, when editing the HTTP/1.1 spec, NOT to specify\nthe behavior of the protocol in every possible error condition,\nbecause there are so many possible error cases (since there\nare so many different headers).\n\nInstead, I think one should fall back on one or both of two\nvery basic principles, when looking for guidance about how\nto make an implementation decision (and when you are sure\nthat the spec itself provides no guidance!):\n\n(1) The Robustness Principle, as explained in RFC791 (the\nIPv4 specification), section 3.2:\n  The implementation of a protocol must be robust.  Each implementation\n  must expect to interoperate with others created by different\n  individuals.  While the goal of this specification is to be explicit\n  about the protocol there is the possibility of differing\n  interpretations.  In general, an implementation must be conservative\n  in its sending behavior, and liberal in its receiving behavior.  That\n  is, it must be careful to send well-formed datagrams, but must accept\n  any datagram that it can interpret (e.g., not object to technical\n  errors where the meaning is still clear).\n\nTo apply this to an HTTP proxy, I would focus on the \"be liberal\nin its receiving behavior\" - i.e., don't drop responses because\nthey appear to be illegal.  In fact, I don't think we have ever\ndefined a status code that a proxy could use to say \"I dropped\nthe response you were expecting because it seemed illegal to me\",\nand it seems like a bug to silently drop (or truncate) responses.\n\n(2) The (less formally stated) rule for HTTP proxies:\nIt's always allowed to act like a tunnel.\n\nRFC2616 says\ntunnels are used when the\n   communication needs to pass through an intermediary (such as a\n   firewall) even when the intermediary cannot understand the contents\n   of the messages.\n\nwhich arguably applies in this case.\n\nHowever, there is a catch: if the proxy has modified the client's\noriginal request (e.g., to convert a simple request into a Range\nrequest), then it has already blown its chance to act like a\ntrue tunnel.  I'm not sure what to do in this case.\n    \nYou suggest, as one option:\n\n  Forward any content overrun, then close the connection.\n    Problematic for HTTP1.0 Keep-Alive clients which might\n    attempt to interpret the overrun as the headers of a\n    subsequest response; technically illegal for an HTTP1.1\n    proxy. OTOH, the proxy would be forwarding stuff which is\n    no more broken than would have been received had the origin\n    server been contacted directly.\n\nThis seems like the most straightforward \"act like a tunnel\"\napproach.  As you point out, it's no worse than if the proxy\nweren't involved in the first place, which is really the\nimplication of the Robustness Principle (\"first, do no harm\").\n\n-Jeff\n\n\n\n", "id": "lists-012-13327614"}, {"subject": "1.1 proxy interaction with 1.0 client and bad EOF behavio", "content": "consider this:\n\nI've got a 1.1 origin server.. that has foo.html which is a static\ndocument of 25052 bytes.. I've got a 1.0 client.. traditional response\nfor that document would have a Content-Length header of 25052 and the\nresponse would be terminated by EOF.. if that connection is terminated\naccidentlaly during transfer at least the client knows that 25052\nbytes haven't been received (if the server was a good implementation\nand included the CL header..)\n\nnow consider a 1.1 proxy in the chain.. the client sends a 1.0 request\nto the 1.1 proxy.. the proxy upgrades the request to 1.1 and adds a\nTE: gzip header.. the 1.1 server responds with \"transfer-encoding:\ngzip, chunked\" and no content-length header (because its prohibited to\nmix content length with non identity transfer-encoding in 4.4 ).. \n\nthe 1.1 proxy has a dilemna with a couple of bad choices:\n\n  a] strip the gzip as it's recvd from upstream and send what output\n  you have downstream.. the response header will lack a content-length\n  because the origin server was prohibited from sending the identity\n  size.. when you're all done, send EOF as terminator.. if downstream\n  response gets interrupted the client has no way to detect\n  truncation.\n\n  b] do store and forward, removing the the encoding and calculating\n  the identity length after it has all been received. This introduces\n  big time queuing delays.\n\nboth situations seem inferior to the proxy not being there at all and\nit seems to be a 1.1 interaction.. think something is missing?\n\n-P\n\n\n\n", "id": "lists-012-13337490"}, {"subject": "Question of many METHOD", "content": "Hello,\n\nI have a question about HTTP Protocol.\nI wonder if all of METHODs have been actually used (or\nimplemented?) because there are so MANY METHODs\n\n[1] TRACE\n  This function is like UNIX TRCEROUTE COMMAND?\n  This is only for Debug?\n  Is it useful?\n\n[2] PUT\n  Is this the same as FTP PUT?\n\n[3] DELETE\n  This is to delete files on a server?\n  If so, isn't it a security issue?\n\n[4] OPTIONS\n  What the function is for?\n\nA client (User Agent) must support all of these METHODs?\nI do not like to implement METHODs which I rarely use.\nAs XTHML1.1-Basic will include a thin spec for a thin client,\nthere must be a thin HTTP spec which supports only necessary\nfunctions.\nI am thinking that above [1],[3] and [4] are not necessary.\n\nIs there anybody who knows such a thin spec?\nAny information or opinion is welcomed.\n\nSincerely,\n\n-- Kojiro Goto\n-- goto@isp21.co.jp\n\n\n\n", "id": "lists-012-13345985"}, {"subject": "Email name change for this user", "content": "E-mail name change for this user.\n\nPlease change your address for this user to joanna.mcdaniel@surgient.com.  E-mails will be forwarded to this new address.\n\nThank you.\n\n\n\n", "id": "lists-012-13353762"}, {"subject": "RE: Question of many METHOD", "content": "Only some are commonly used.\n\n=== COMMONLY USED for Browsing ===\nGET - Retreive documents, used most by your web browser\nPOST - Posting forms of HTML pages (commonly used)\n\n=== HTTP/1.1 WEB EDITING ===\nHEAD - Commonly used for testing hyperlinks (same as GET, but without\ndata/content)\nPUT - Sending a file to a web server\nDELETE - Remove files\n\n=== Admin ===\nOPTIONS - Setting settings on a web server or proxy server??? don't know\nexactly....\nTRACE - Tracing the path of a request through proxy servers and to a web\nserver\n\n\nIf you are building a simple HTTP/1.1 interface you should implement\nGET and POST\n\nHEAD can be done simply using the same code as GET, but you don't get the\ndata/content)\n\nPUT/DELETE/e.d. should be implemented if you plan web publishing or\nauthoring\n\n\nSince I was building a proxy server, these functions are mostly used (GET &\nPOST).\nIf you build a proxy server, it's recommended to implement the CONNECT\ncommand for\ntunneling, but this not in your case (i think).\n\nHTTP/1.1 also supports authentication and much more methods to secure data\non a web server. The PUT, DELETE, e.d. functions are not a security issue if\nthey require you to authenticate with digest or NTLM authentication (or some\nalike). Also it's possible to use HTTPS for web publishing, what even\nprotects the data being transmitted, useful for sensative documenents.\n\nSo don't worry about security, just ensure you are making the library\nHTTP/1.1-compliant and are a bit compatible with HTTP/1.0, what should not\nbe an issue if you follow the standards.\n\n\nJoris Dobbelsteen\nj.p.tdobbelsteen@freeler.nl\n\n\n\n\n-----Original Message-----\nFrom: Kojiro Goto [mailto:goto@isp21.co.jp]\nSent: woensdag 5 april 2000 11:24\nTo: http-wg@cuckoo.hpl.hp.com\nSubject: Question of many METHODs\n\n\n\nHello,\n\nI have a question about HTTP Protocol.\nI wonder if all of METHODs have been actually used (or\nimplemented?) because there are so MANY METHODs\n\n[1] TRACE\n  This function is like UNIX TRCEROUTE COMMAND?\n  This is only for Debug?\n  Is it useful?\n\n[2] PUT\n  Is this the same as FTP PUT?\n\n[3] DELETE\n  This is to delete files on a server?\n  If so, isn't it a security issue?\n\n[4] OPTIONS\n  What the function is for?\n\nA client (User Agent) must support all of these METHODs?\nI do not like to implement METHODs which I rarely use.\nAs XTHML1.1-Basic will include a thin spec for a thin client,\nthere must be a thin HTTP spec which supports only necessary\nfunctions.\nI am thinking that above [1],[3] and [4] are not necessary.\n\nIs there anybody who knows such a thin spec?\nAny information or opinion is welcomed.\n\nSincerely,\n\n-- Kojiro Goto\n-- goto@isp21.co.jp\n\n\n\n", "id": "lists-012-13360114"}, {"subject": "RE: Question of many METHOD", "content": "> From: Kojiro Goto <goto@isp21.co.jp>\n> Subject: Question of many METHODs\n\n> I wonder if all of METHODs have been actually used (or\n> implemented?) because there are so MANY METHODs\n>\n> [1] TRACE\n>   This function is like UNIX TRCEROUTE COMMAND?\n>   This is only for Debug?\n>   Is it useful?\n\nI've found it very usefull when trying to determine what various\nproxies do to requests; it let's you see how the request arrived at\nthe server (as opposed to what you sent).\n\n> [2] PUT\n>   Is this the same as FTP PUT?\n\nIt serves the same purpose.\n\n> [3] DELETE\n>   This is to delete files on a server?\n\nYes.\n\n>   If so, isn't it a security issue?\n\nNo more so than any other security issue - the server can enforce\nwhatever security it wants to for any method.\n\n> [4] OPTIONS\n>   What the function is for?\n\nIt was meant to be a means of probing what features the server\nsupported, but never really got finished in that no standard way of\nexpressing much usefull information was specified.\n\nIt is usefull as a no-op request for determining whether or not the\nserver supports 1.1, or for carrying a request to upgrade (see\n\"Upgrading to TLS Within HTTP/1.1\" -\ndraft-ietf-tls-http-upgrade-04.txt; approved as Proposed Standard\nbut not yet out of the RFC Editors queue).\n\n> A client (User Agent) must support all of these METHODs?\n\nSince HTTP is always initiated by the client, it cannot be forced to\nsend any method, so in some sense it needn't support any methods it\ndoesn't want to.\n\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-13370520"}, {"subject": "Microsoft HTTP 1.1 Conformance", "content": "I just started checking out IE browser 5.0 for the Mac. Whereas in 4.5\nthere was a switch to get it to use Http 1.1 in proxy requests, no such\nswitch remains in the current version. \n\nI'm wondering if the microsoft browsers are going backwards on http 1,1\nconformance, just as they lost digest authentication from IE 3 in later\nreleases.\n\nCan anybody shed some light precisely what the status of IE's conformance\nto HTTP 1.1 is?\n\nCan anybody explain why 100s of MS hackers are unable after years to\nachieve 100% conformance?\n\nDoes this reflect the lack of competition for MS in the browser arena?\nAn explicit management strategy?\n\nPerhaps this suggests a remedy as well: IE A, B, and C\nand suggest a remedy?\n\n\n\n", "id": "lists-012-13379266"}, {"subject": "HTTP Format and How HTTP work", "content": "Hi all,\nCould anybody please tell me what is the format of HTTP/1.1 format for both\nrequest and response commands ? Also what the HTTP server do after it\nreceive the GET HTTP command from web client ? How is it process ? and where\nI can get or find those information related .\n\nThanks you very much for your help \nRegards\nDien Nguyen\n\n\n\n", "id": "lists-012-13386992"}, {"subject": "RE: Microsoft HTTP 1.1 Conformance", "content": "> From: John C. Mallery \n\n> I'm wondering if the microsoft browsers are going \n> backwards on http 1,1\n> conformance, just as they lost digest authentication from \n> IE 3 in later\n> releases.\n\nNo, I believe that they just changed the configurability of it.\n\nIE5 supports 1.1, and supports Digest (rfc2617).\n\n> Can anybody shed some light precisely what the status of \n> IE's conformance\n> to HTTP 1.1 is?\n\nFar far better than Netscape, which doesn't do 1.1 at all.\n\n\n\n", "id": "lists-012-13394223"}, {"subject": "RE: Microsoft HTTP 1.1 Conformance", "content": "At 10:06 -0400 04-12-2000, Scott Lawrence wrote:\n> > From: John C. Mallery\n>\n>> I'm wondering if the microsoft browsers are going\n>> backwards on http 1,1\n>> conformance, just as they lost digest authentication from\n>> IE 3 in later\n>> releases.\n>\n>No, I believe that they just changed the configurability of it.\n>\n>IE5 supports 1.1, and supports Digest (rfc2617).\n\nAnd what about http 1.1 proxying?\n\n\n\n", "id": "lists-012-13402150"}, {"subject": "Re: Microsoft HTTP 1.1 Conformance", "content": "And what about http 1.1 proxying?\n That is supported too.  Look under the advanced options tab for Internet\nproperties\n\n----- Original Message -----\nFrom: \"John C. Mallery\" <jcma@ai.mit.edu>\nTo: \"Scott Lawrence\" <lawrence@agranat.com>\nCc: <http-wg@cuckoo.hpl.hp.com>\nSent: Wednesday, April 12, 2000 10:38 AM\nSubject: RE: Microsoft HTTP 1.1 Conformance?\n\n\n> At 10:06 -0400 04-12-2000, Scott Lawrence wrote:\n> > > From: John C. Mallery\n> >\n> >> I'm wondering if the microsoft browsers are going\n> >> backwards on http 1,1\n> >> conformance, just as they lost digest authentication from\n> >> IE 3 in later\n> >> releases.\n> >\n> >No, I believe that they just changed the configurability of it.\n> >\n> >IE5 supports 1.1, and supports Digest (rfc2617).\n>\n> And what about http 1.1 proxying?\n>\n>\n\n\n\n", "id": "lists-012-13410020"}, {"subject": "RE: Microsoft HTTP 1.1 Conformance", "content": "Our proxy server also has support for http/1.1.\n\n> -----Original Message-----\n> From: Keith Hoffman [mailto:hoffmankeith@hotmail.com]\n> Sent: Thursday, April 13, 2000 9:56 AM\n> To: Scott Lawrence; John C. Mallery\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Microsoft HTTP 1.1 Conformance?\n> \n> \n> And what about http 1.1 proxying?\n>  That is supported too.  Look under the advanced options tab \n> for Internet\n> properties\n> \n> ----- Original Message -----\n> From: \"John C. Mallery\" <jcma@ai.mit.edu>\n> To: \"Scott Lawrence\" <lawrence@agranat.com>\n> Cc: <http-wg@cuckoo.hpl.hp.com>\n> Sent: Wednesday, April 12, 2000 10:38 AM\n> Subject: RE: Microsoft HTTP 1.1 Conformance?\n> \n> \n> > At 10:06 -0400 04-12-2000, Scott Lawrence wrote:\n> > > > From: John C. Mallery\n> > >\n> > >> I'm wondering if the microsoft browsers are going\n> > >> backwards on http 1,1\n> > >> conformance, just as they lost digest authentication from\n> > >> IE 3 in later\n> > >> releases.\n> > >\n> > >No, I believe that they just changed the configurability of it.\n> > >\n> > >IE5 supports 1.1, and supports Digest (rfc2617).\n> >\n> > And what about http 1.1 proxying?\n> >\n> >\n> \n\n\n\n", "id": "lists-012-13419145"}, {"subject": "RE: Microsoft HTTP 1.1 Conformance", "content": "At 10:49 -0700 04-13-2000, Josh Cohen wrote:\n>Our proxy server also has support for http/1.1.\n>\n\nI can't find anywhere to turn it on for the Mac IE 5.0 Browser.\nThey seem to have lost the switch.  I know there is a switch for\nwindows.\n\nBTW, are there reasons why http 1.1 proxying would not be the default\nfor the IE client?\n\n\n\n", "id": "lists-012-13430629"}, {"subject": "RE: Microsoft HTTP 1.1 Conformance", "content": "On Fri, 14 Apr 2000, John C. Mallery wrote:\n\n> BTW, are there reasons why http 1.1 proxying would not be the default\n> for the IE client?\n\nHow about providing users who have no control over the protocols\nsuppported by their firewall proxy with a reliable experience? I'm fairly\ncertain that any IT organization which chooses can use the IEAK (Admin\nKit) to build a locally configured IE variant which would have a different\ndefault suitable for the proxy provided by that organization.\n\nDave Morris\n\n\n\n", "id": "lists-012-13438568"}, {"subject": "REMOVE ME FROM YOUR EMAIL ADDRES", "content": "REMOVE ME FROM YOUR EMAIL ADDRESS\nSOME HOW I GOT CC:\n\n\n\n", "id": "lists-012-1343952"}, {"subject": "Re: Microsoft HTTP 1.1 Conformance", "content": "IE on Mac doesn't have that option.  I'm not sure why that decision was\nmade. I believe it does HTTP 1.1 all of the time (even through proxies).\n\nIt's not the default for the IE client because there are very few proxies\nthat are truly and fully HTTP 1.1 compliant.  Most are actually a mix - http\n1.0.5 if you will. :)    Having HTTP 1.1 through the proxy turned off by\ndefault helps avoid port negotiation issues on some proxies.\n\n\n----- Original Message -----\nFrom: \"John C. Mallery\" <jcma@ai.mit.edu>\nTo: \"Josh Cohen\" <joshco@exchange.microsoft.com>\nCc: <http-wg@cuckoo.hpl.hp.com>\nSent: Friday, April 14, 2000 10:42 AM\nSubject: RE: Microsoft HTTP 1.1 Conformance?\n\n\n> At 10:49 -0700 04-13-2000, Josh Cohen wrote:\n> >Our proxy server also has support for http/1.1.\n> >\n>\n> I can't find anywhere to turn it on for the Mac IE 5.0 Browser.\n> They seem to have lost the switch.  I know there is a switch for\n> windows.\n>\n> BTW, are there reasons why http 1.1 proxying would not be the default\n> for the IE client?\n>\n>\n\n\n\n", "id": "lists-012-13446697"}, {"subject": "Re: Microsoft HTTP 1.1 Conformance", "content": "At 16:25 -0500 04-14-2000, Keith Hoffman wrote:\n>IE on Mac doesn't have that option.  I'm not sure why that decision was\n>made. I believe it does HTTP 1.1 all of the time (even through proxies).\n\nNo testing with my proxy should IE 5.0 for the Mac makes 1.0 proxy requests\neven though it makes 1.1 requests when no proxy is specified.\n\nOne of the major intentions of http 1.1 was to clean up http proxying,\nand yet......\n\n>\n>It's not the default for the IE client because there are very few proxies\n>that are truly and fully HTTP 1.1 compliant.  Most are actually a mix - http\n>1.0.5 if you will. :)    Having HTTP 1.1 through the proxy turned off by\n>default helps avoid port negotiation issues on some proxies.\n>\n>\n>----- Original Message -----\n>From: \"John C. Mallery\" <jcma@ai.mit.edu>\n>To: \"Josh Cohen\" <joshco@exchange.microsoft.com>\n>Cc: <http-wg@cuckoo.hpl.hp.com>\n>Sent: Friday, April 14, 2000 10:42 AM\n>Subject: RE: Microsoft HTTP 1.1 Conformance?\n>\n>\n>> At 10:49 -0700 04-13-2000, Josh Cohen wrote:\n>> >Our proxy server also has support for http/1.1.\n>> >\n>>\n>> I can't find anywhere to turn it on for the Mac IE 5.0 Browser.\n>> They seem to have lost the switch.  I know there is a switch for\n>> windows.\n>>\n>> BTW, are there reasons why http 1.1 proxying would not be the default\n>> for the IE client?\n>>\n>>\n\n\n\n", "id": "lists-012-13455958"}, {"subject": "IPV6 address in host header, needs clarificatio", "content": "Following RFC 2732 & integrating with HTTP 1.1:\n\n> > The problem I have is in the Host field in the HTTP 1.1 header\n> > \n> > It says:\n> > \n> > Host hostname\n> > \n> > or\n> > \n> > Host hostname:port\n> > \n> > If hostname is an IPv6 literal address, then there is an ambiguity due\n> > the colon\n> > \n> > example:\n> > \n> > Host 3ffe::1:8080\n> > \n> > Does this mean hostname is 3ffe::1 and port 8080 or hostname\n> > 3ffe::1:8080 and default port 80 ?\n> > \n> > The only way to remove the ambiguity is to make the use of [ ] around\n> > the IPv6\n> > literal address madatory. Such as:\n> > \n> > Host [3ffe::1]:8080\n> > \n> > or\n> > \n> > Host [3ffe::1:8080]\n> > \n> > \n> > We ran into this problem at Connectathon 2000.\n\n\n\n", "id": "lists-012-13465091"}, {"subject": "Re: IPV6 address in host header, needs clarificatio", "content": "What useful purpose does sending an IP (v anything) address in\na Host header field perform?\n\n....Roy\n\n\n\n", "id": "lists-012-13473250"}, {"subject": "Re: IPV6 address in host header, needs clarificatio", "content": "This was the solution recommended at IETF-47.  Of course, you also have the \nproblem with userland utilities that accept host:port designations as well.\n\n> Following RFC 2732 & integrating with HTTP 1.1:\n> \n> > > The problem I have is in the Host field in the HTTP 1.1 header\n> > > \n> > > It says:\n> > > \n> > > Host hostname\n> > > \n> > > or\n> > > \n> > > Host hostname:port\n> > > \n> > > If hostname is an IPv6 literal address, then there is an ambiguity due\n> > > the colon\n> > > \n> > > example:\n> > > \n> > > Host 3ffe::1:8080\n> > > \n> > > Does this mean hostname is 3ffe::1 and port 8080 or hostname\n> > > 3ffe::1:8080 and default port 80 ?\n> > > \n> > > The only way to remove the ambiguity is to make the use of [ ] around\n> > > the IPv6\n> > > literal address madatory. Such as:\n> > > \n> > > Host [3ffe::1]:8080\n> > > \n> > > or\n> > > \n> > > Host [3ffe::1:8080]\n> > > \n> > > \n> > > We ran into this problem at Connectathon 2000.\n> \n-- \nZachary Amsden  zamsden@engr.sgi.com  3-6919  31-2-510  Core Protocols\n\n\n\n", "id": "lists-012-13481115"}, {"subject": "Re: IPV6 address in host header, needs clarificatio", "content": "On Mon, 17 Apr 2000, Roy T. Fielding wrote:\n\n> What useful purpose does sending an IP (v anything) address in\n> a Host header field perform?\n\nBeing able to make a HTTP/1.1 request at all using an IP address... since\nyou need a Host: header of some sort.\n\nAnd it is definitely useful or even necessary to allow requests using an\nIP address.  But that is another debate that comes up over and over in\ngeneral, especially with IPv6...\n\n\n\n", "id": "lists-012-13489831"}, {"subject": "Re: IPV6 address in host header, needs clarificatio", "content": ">> What useful purpose does sending an IP (v anything) address in\n>> a Host header field perform?\n>\n>Being able to make a HTTP/1.1 request at all using an IP address... since\n>you need a Host: header of some sort.\n\nThat's not what I meant.  Is this just a question about what to put\nin the field value, or is there some functionality/semantics desired\nbehind the server interpreting this value (which we have discussed\nin the past as being a bad thing).\n\nThe syntax for host is defined by reference to RFC 2396, which in\nturn is updated by the IPv6-within-URI specification (RFC 2732),\nso the answer for HTTP syntax has already been defined.\n\n....Roy\n\n\n\n", "id": "lists-012-13498538"}, {"subject": "Re: questions regarding draft-ietf-http-authentication0", "content": "> 1)  Section 3.2.2, request-digest description:\n> \n>         If the \"qop\" value is \"auth\":\n[snip]\n>     Shouldn't that be\n> \n>         If the \"qop\" value is \"auth\" or \"auth-int\":\n[snip]\n\nOoops, sorry, my scan of David Kristol's mail missed that he had already\nmentioned this.\n\nHowever, on to more questions and comments regarding\ndraft-ietf-http-authentication-01.txt :\n\n3) Section 3.2.2, digest-response description:\n\n        digest-response  = 1#( username | realm | nonce | digest-uri |\n                           response | [ algorithm ] | [cnonce] |\n                           [opaque] | [server] | [message-qop] |\n                                     ^^^^^^^^^^^\n                           [ nonce-count ] )\n\n    \"server\" is not defined anywhere. What is the syntax supposed to\n    be, and what purpose does it serve?\n\n\n4) Section 3.2.2, digest-response description:\n\n        cnonce\n          An opaque quoted string value provided by the client and used by both\n          client and server to avoid chosen plaintext attacks, to provide\n          mutual authentication, and to provide some message integrity\n          protection.  See the descriptions below of the calculation of the\n          response-digest and request-digest values.\n\n        nonce-count\n          This MUST be specified if a qop attribute is sent (see above), and\n          MUST NOT be specified if the server did not send a qop attribute in\n          the WWW-Authenticate header field. ...\n\n    I presume the \"MUST NOT\" above is to avoid problems with rfc-2068\n    implementations which might not handle an unknown attribute correctly\n    (although they ought to just be ignoring it)? If so, why isn't the\n    same language used for the cnonce? I.e. something like\n\n        cnonce\n          An opaque quoted string value provided by the client and used by both\n          client and server to avoid chosen plaintext attacks, to provide\n          mutual authentication, and to provide some message integrity\n          protection.  See the descriptions below of the calculation of the\n          response-digest and request-digest values. This attribute MUST NOT\n          be specified if the server did not send a qop attribute in the\n          WWW-Authenticate header field.\n\n\n5) For backwards compatibility with rfc-2069 there should be words to\n   explictly prevent a server from sending \"algorithm=MD5-sess\" but no\n   qop attribute. Maybe something like the following (just reusing the\n   wording from above again):\n\n        algorithm\n          A string indicating a pair of algorithms used to produce the digest\n          and a checksum. If this is not present it is assumed to be \"MD5\".\n          ...\n          colon concatenated with the data. The \"MD5-sess\" algorithm is\n          intended to allow efficient 3rd party authentication servers;\n          for the difference in usage, see the description. The \"MD5-sess\"\n          algorithm MUST NOT be specified if the qop-options attribute\n          is not present.\n\n\n6) The \"algorithm\" attribute is defined as:\n\n        algorithm         = \"algorithm\" \"=\" ( \"MD5\" | \"MD5-sess\" )\n\n     whereas rfc-2069 used the more general form\n\n        algorithm           = \"algorithm\" \"=\" ( \"MD5\" | token )\n\n     In the interest of possible future enhancements, I suggest changing\n     the current definition to:\n\n        algorithm         = \"algorithm\" \"=\" ( \"MD5\" | \"MD5-sess\" | token )\n\n\n7) Section 3.2.2, a small nit:\n\n        response\n          A string of 32 hex digits computed as defined below, which proves\n                                                                     ^^^^^^\n          that the user knows a password\n\n    I'm not very familiar with usage of \"prove\" in cryptographic circles,\n    but a correct \"response\" attribute certainly does not prove anything\n    in a mathematical sense. How about\n\n        response\n          A string of 32 hex digits computed as defined below. The reception\n          of a correct response provides a strong indication that the user\n          knows a password.\n\n\n8) Section 3.2.3: no words prohibit the server from sending, say, a qop\n   attribute but not a rspauth attribute. Also, while the cnonce is\n   required to be the same as used in the request, the nonce-count isn't.\n   Hence I propose the following change in wording:\n\n   Replace\n\n        where \"Status-Code\" is the status code (e.g., \"200\") from the\n        \"Status-Line\" of the response, as defined in section 6.1 of [2],\n        and \"digest-uri-value\" is the value of the \"uri\" directive on the\n        Authorization header in the request. The \"cnonce-value\" MUST be\n        one for the client request to which this message is the response.\n\n   by\n\n        where \"Status-Code\" is the status code (e.g., \"200\") from the\n        \"Status-Line\" of the response, as defined in section 6.1 of [2],\n        and \"digest-uri-value\" is the value of the \"uri\" directive on the\n        Authorization header in the request. The \"cnonce-value\" and\n        \"nc-value\" MUST be the ones used in the client request to which\n        this message is the response.\n\n        The \"response-auth\", \"cnonce\", and \"nonce-count\" attributes MUST\n        BE present if \"qop=auth\" or \"qop=auth-int\" is specified.\n\n\n9) Section 3.2.3: Actually, why are the cnonce and nonce-count attributes\n   sent in the Authorization-Info header? Or put differently, what makes\n   these two attributes special, as opposed to the nonce and uri (which\n   aren't sent back)?\n\n\n10) Section 3.2.2:\n\n        Implementers should be aware of how authenticated transactions\n        interact with shared caches. The HTTP/1.1 protocol specifies that\n        when a shared cache (see section 13.10 of [2]) has received a\n                                         ^^^^^\n        request containing an Authorization header and a response from\n        relaying that request, it MUST NOT return that response as a\n        reply to any other request, unless one of two Cache-Control (see\n        section 14.9 of [2]) directives was present in the response. If\n                ^^^^\n\n    Shouldn't those be sections 13.7 and 14.8, respectively?\n\n\n11) What's the status of the AUTH-INFO-SYNTAX issue? The issues page\n    http://www.w3.org/Protocols/HTTP/Issues/ lists the status as \"Drafting\",\n    but it's not in the current draft.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1350230"}, {"subject": "Question for HTTP/1.1 cache implementors (both proxy &amp; client caches", "content": "In RFC2616, section 14.9.6 (Cache Control Extensions), we have:\n\n   For example, consider a hypothetical new response directive called\n   community which acts as a modifier to the private directive. We\n   define this new directive to mean that, in addition to any non-shared\n   cache, any cache which is shared only by members of the community\n   named within its value may cache the response. An origin server\n   wishing to allow the UCI community to use an otherwise private\n   response in their shared cache(s) could do so by including\n\n       Cache-Control: private, community=\"UCI\"\n\n   A cache seeing this header field will act correctly even if the cache\n   does not understand the community cache-extension, since it will also\n   see and understand the private directive and thus default to the safe\n   behavior.\n\nBy extension, one would expect this example to work:\n\n       Cache-Control: no-store, community=\"UCI\"\n\nand it might, in fact, be preferable, since it prevents browser\ncaches (as well as proxy caches) from doing something wrong.\nSection 14.9.6 allows the \"community\" directive to modify\nthe \"requirements associated with the standard directive,\" so\nthat (FOR EXAMPLE) caches that implement the \"community\" directive\ncould ignore the \"no-store\" and do something useful.\n\nBut suppose instead that the example was:\n\n       Cache-Control: no-store, community=\"UCI\", max-age=30\n\nRFC2616 is not specific on what this header means to a vanilla\nHTTP/1.1 cache.  14.9.6 does make it clear that if the cache\ndoesn't understand \"community\", then it MUST interpret this\nheader as equivalent to:\n\n       Cache-Control: no-store, max-age=30\n\nThe specification (14.9.3, first paragraph) for \"max-age\" says:\n\n   The max-age directive on a response implies that the\n   response is cacheable (i.e., \"public\") unless some other, more\n   restrictive cache directive is also present.\n\nHowwever, I'm concerned that some implementations might have\nerroneously followed the reasoning \"well, if the origin server\ndidn't want us to cache the response, why did it give us a\nnon-zero max-age\"?  (This particular requirement was not\nstated with the usual MUST keyword, so it might have been\noverlooked.  Also, section 13.4 ambiguously says that\nresponses with \"other status codes\" are cachable if they\ninclude directives such as \"max-age\", but without saying \nthat more restrictive directives take precedence.)\n\nIf so, that would greatly complicate the design of the\ndelta-encoding extension that we're now trying to debug.\nWe need to make sure that delta-encoded responses are never\ncachable by caches that don't understand them, even if the\nresponses contain Cache-control directives aimed at caches\nthat do understand delta-encoding.  And we're not entirely\nsure that the Vary header will work efficiently for this case.\n\nSimilarly, if the response instead were to include:\n\n       Cache-Control: no-cache, max-age=30\n\ndo any existing HTTP/1.1 cache implementations ignore the\n\"no-cache\" in this case?\n\nIt would also be important to know if any HTTP/1.1 implementations\nthat support \"Cache-control: max-age\" actually ignore the\n\"no-store\" directive - which would be contrary to the spec,\nof course.\n\nFinally, when RFC2616 is revised to move from Draft Standard\nto Full Standard, would anyone object to clarifying the language\nin 14.9.3?  For example, replacing:\n\n   The max-age directive on a response implies that the\n   response is cacheable (i.e., \"public\") unless some other, more\n   restrictive cache directive is also present.\n\nwith\n\n   The max-age directive on a response implies that the\n   response is cacheable (i.e., \"public\") unless some other, more\n   restrictive cache directive is also present.  If a more\n   restrictive cache directive (such as \"no-cache\" or \"no-store\")\n   is present, the cache MUST ignore the max-age directive;\n   this supports extensibility using the mechanism described\n   in section 14.9.6.\n\nand perhaps also, under \"s-maxage\":\n\n   If a more restrictive cache directive is present, the cache\n   MUST ignore the s-maxage directive.\n\nfor the same reason.\n\n-Jeff\n\n\n\n", "id": "lists-012-13507082"}, {"subject": "Re: Question for HTTP/1.1 cache implementors (both proxy &amp; client caches", "content": ">Finally, when RFC2616 is revised to move from Draft Standard\n>to Full Standard, would anyone object to clarifying the language\n>in 14.9.3?  For example, replacing:\n>\n>   The max-age directive on a response implies that the\n>   response is cacheable (i.e., \"public\") unless some other, more\n>   restrictive cache directive is also present.\n>\n>with\n>\n>   The max-age directive on a response implies that the\n>   response is cacheable (i.e., \"public\") unless some other, more\n>   restrictive cache directive is also present.  If a more\n>   restrictive cache directive (such as \"no-cache\" or \"no-store\")\n>   is present, the cache MUST ignore the max-age directive;\n>   this supports extensibility using the mechanism described\n>   in section 14.9.6.\n>\n>and perhaps also, under \"s-maxage\":\n>\n>   If a more restrictive cache directive is present, the cache\n>   MUST ignore the s-maxage directive.\n>\n>for the same reason.\n\nWouldn't that become a contradiction with the extension scheme?\nIn other words, that requirement along with your example of\n\n      Cache-Control: no-store, community=\"UCI\", max-age=30\n\nwould require that the recipient ignore max-age even if it did\nunderstand the community extension.  I think that is why we decided\nto use relative constraints rather than absolute constraints in the\nlanguage above.\n\n....Roy\n\n\n\n", "id": "lists-012-13518670"}, {"subject": "Re: Question for HTTP/1.1 cache implementors (both proxy &amp; client caches", "content": "\"Roy T. Fielding\" <fielding@kiwi.ICS.UCI.EDU> writes:\nMessage-Id:  <200004191602.aa19350@gremlin-relay.ics.uci.edu>\n\n    >   The max-age directive on a response implies that the\n    >   response is cacheable (i.e., \"public\") unless some other, more\n    >   restrictive cache directive is also present.  If a more\n    >   restrictive cache directive (such as \"no-cache\" or \"no-store\")\n    >   is present, the cache MUST ignore the max-age directive;\n    >   this supports extensibility using the mechanism described\n    >   in section 14.9.6.\n    \n    Wouldn't that become a contradiction with the extension scheme?\n    In other words, that requirement along with your example of\n    \nCache-Control: no-store, community=\"UCI\", max-age=30\n\n    would require that the recipient ignore max-age even if it did\n    understand the community extension.  I think that is why we decided\n    to use relative constraints rather than absolute constraints in the\n    language above.\n\nNo contradiction, because 14.9.6 says:\n\nBoth the new\n   directive and the standard directive are supplied, such that\n   applications which do not understand the new directive will default\n   to the behavior specified by the standard directive, and those that\n   understand the new directive will recognize it as modifying the\n   requirements associated with the standard directive.\n\nThe key phrase here is that the new directive (\"community\", in\nthis example) \"[modifies] the requirements associated with the\nstandard directive.\"  Presumably, the \"modification\" needs to\nbe part of the specification for each extension.\n\nI.e., the specification for \"community\" could include,\nhypothetically, \"an implementation that complies with the\nspecification for the community directive SHOULD ignore the\nno-store directive if it appears together with the community\ndirective.\"  Which means that the no-store directive, being\nignored, would not take precedence over the max-age directive\nfor \"community-aware\" implementations.\n\n-Jeff\n\n\n\n", "id": "lists-012-13528378"}, {"subject": "Re: Question for HTTP/1.1 cache implementors (both proxy &amp; client caches", "content": ">I.e., the specification for \"community\" could include,\n>hypothetically, \"an implementation that complies with the\n>specification for the community directive SHOULD ignore the\n>no-store directive if it appears together with the community\n>directive.\"  Which means that the no-store directive, being\n>ignored, would not take precedence over the max-age directive\n>for \"community-aware\" implementations.\n\nBut the existing text already handles that situation as a\ncondition on the meaning of max-age, and the extension rules\nalready allow new extensions to modify the interpretation of\nother cache-directives.\n\nIt seems to me that adding a further MUST requirement will just\nraise the issue of which section has precedence, particularly since\nimplementing the extension mechanism itself is not a MUST requirement.\nThat is what I meant by a contradiction.\n\nI think we both agree on what is desirable from the implementation,\nbut I think the existing text is less confusing than the proposed change.\nIs there another alternative?\n\n....Roy\n\n\n\n", "id": "lists-012-13538904"}, {"subject": "Nontransparent proxies and the Vary header fiel", "content": "In section 14.44 (Vary header fields), I'm not sure I understand the\nreasoning behind the rule that says: 'The \"*\" value MUST NOT be generated by\na proxy server,'\n\nLooking back through the discussions, I see that Koen Holtman asked the same\nquestion (a long, long time ago) but I couldn't find any responses that\naddressed the issue.\n\nCan anyone shed some light on the justification behind the requirement?\n\nThanks,\nEd Windes, Spyglass, Inc., (ewindes@spyglass.com)\n\n> -----Original Message-----\n> From: koen@win.tue.nl [mailto:koen@win.tue.nl]\n> Sent: Friday, March 27, 1998 1:54 PM\n> To: http-wg@cuckoo.hpl.hp.com\n> Cc: koen@win.tue.nl\n> Subject: Comments on draft-ietf-http-v11-spec-rev-03\n>\n[ snip, snip ... ]\n> - Section 14.44:\n> \n> This section introduces a new (as far as I can see) requirement:\n> \n> #  The \"*\" value MUST NOT be generated by a proxy server; it \n> #  may only be generated by an origin server.\n> \n> I don't see any reason for having this requirement.  The general rule\n> should be that transparent proxies may never change or add a Vary\n> header (this is already implied elsewhere in the spec I believe), and\n> that non-transparent proxies can do whatever they want.\n> \n> By the way, proxies which support the TCN protocol extension _will_\n> sometimes generate \"*\", this is explicitly allowed by TCN.\n> \n> In short, the requirement should be deleted.\n> \n> \n\n\n\n", "id": "lists-012-13547577"}, {"subject": "Question on proxy/client behaviou", "content": "During testing of various proxy servers I have found one that exhibits what I believe is bad behaviour.  I am writing this group to see if there is any kind of concensus on this.\n\nThe proxy server in question is http 1.0 compatible.  When a client sends a http 1.1 request it will modify it to use http 1.0 going to the web server.  The web server responds fine to the request, but when the proxy forwards the response to the client, the client doesn't show the web page. \n\nI have traced this and found that proxy is dropping the tcp PUSH flag from SOME of the response packets coming back from the server.  These \"bad\" packets only have the tcp ACK flag.  Response packets that are read ok by the client contain both the PUSH and ACK flags.\n\nTo me it seems that there is definitely a problem on the proxy as it should consistently send either just the ACK or both the ACK and PUSH flags with all responses.  However, should the client still try to read the payload of a packet as http data even if there isn't a PUSH flag associated with that packet?\n\nI've looked through various RFC's and haven't found anything that says specifically if the client should or shouldn't do this.\n\nThanks,\n\nKeith\n\n\n\n", "id": "lists-012-13557407"}, {"subject": "RE: Question on proxy/client behaviou", "content": "> From: Keith Hoffman <hoffmankeith@hotmail.com>\n> To: http-wg@hplb.hpl.hp.com\n> Subject: Question on proxy/client behaviour\n\n> The proxy server in question is http 1.0 compatible.  When a\n> client sends a http 1.1 request it will modify it to use http\n> 1.0 going to the web server.  The web server responds fine to\n> the request, but when the proxy forwards the response to the\n> client, the client doesn't show the web page.\n\n> I have traced this and found that proxy is dropping the tcp PUSH\n> flag from SOME of the response packets coming back from the\n> server.  These \"bad\" packets only have the tcp ACK flag.\n> Response packets that are read ok by the client contain both the\n> PUSH and ACK flags.\n\nThe TCP PUSH flag shouldn't have any effect on which parts of the\nresponse the client sees (for some implementations it's possible\nthat it would affect the size and timing of the reads, but in the\nend there should be no difference).  Look for some other\ndifference. \n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-13565463"}, {"subject": "[Fwd: Last Call: Use of HTTP State Management to BCP", "content": "Please note that this is a last call for *two* documents.\n\nDave Kristol\n\nattached mail follows:\n\nThe IESG will consider publication of the following Internet-Drafts:\n\no Use of HTTP State Management <draft-iesg-http-cookies-03.txt> as a\n  BCP.\no HTTP State Management Mechanism\n  <draft-ietf-http-state-man-mec-12.txt> as a Proposed Standard.\n\n\nThe IESG plans to make a decision in the next few weeks, and solicits\nfinal comments on this action.  Please send any comments to the \niesg@ietf.org or ietf@ietf.org mailing lists by May 12, 2000.\n\nFiles can be obtained via\nhttp://www.ietf.org/internet-drafts/draft-iesg-http-cookies-03.txt\nhttp://www.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-12.txt\n\n\n\n", "id": "lists-012-13574556"}, {"subject": "Re: [Fwd: Last Call: Use of HTTP State Management to BCP", "content": "Dave Kristol:\n>Please note that this is a last call for *two* documents.\n\nI have looked at the latest changes in the documents, and have no\nproblems with both of them going forward.\n\n>\n>Dave Kristol\n\nKoen,\n\n\n-- Start of included mail From:  The IESG <iesg-secretary@ietf.org>\n\n>To:  \n>cc:  iesg@ietf.org\n>Subject:  Last Call: Use of HTTP State Management to BCP\n>Reply-To:  iesg@ietf.org\n>Date:  Fri, 28 Apr 2000 12:10:37 -0400\n>Sender:  scoya@cnri.reston.va.us\n\n>\n>The IESG will consider publication of the following Internet-Drafts:\n>\n>o Use of HTTP State Management <draft-iesg-http-cookies-03.txt> as a\n>  BCP.\n>o HTTP State Management Mechanism\n>  <draft-ietf-http-state-man-mec-12.txt> as a Proposed Standard.\n>\n>\n>The IESG plans to make a decision in the next few weeks, and solicits\n>final comments on this action.  Please send any comments to the \n>iesg@ietf.org or ietf@ietf.org mailing lists by May 12, 2000.\n>\n>Files can be obtained via\n>http://www.ietf.org/internet-drafts/draft-iesg-http-cookies-03.txt\n>http://www.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-12.txt\n-- End of included mail.\n\n\n\n", "id": "lists-012-13583324"}, {"subject": "Re: comments on draft-ietf-http-authentication01.tx", "content": "> >     The term \"protection space\" gets used without a definition (here),\n> >     but the spec. describes how a client can reuse credentials for such\n> >     a protection space.  I think we should say that the description of\n> >     any auth-scheme must describe the rules for deciding when two\n> >     objects are in the same protection space.  In particular, a client\n> >     must be able to tell, so it knows whether or not to send credentials\n> >     unprompted.\n> > \n> Why does it need to tell? If it's wrong, by either sending incorrect ones or\n> not sending any, it'll get a 401 to tell it what to do. As far as I can see,\n> for Digest it's only an optimization. (For Basic, you don't want to send\n> your credentials to the wrong place...)\n> \n> > Sect. 3.2.1, The WWW-Authenticate Response Header\n> >     [domain attribute]\n> >     If this keyword is omitted or empty, the client should assume that\n> >     the domain consists of all URIs on the responding server.\n> > \n> >       This behavior is different from Basic.  If we want Digest to be\n> >       a more or less drop-in replacement, shouldn't the default\n> >       behavior mimic Basic?\n> > \n> As you point out below, there are implementations. As I point out above, it\n> shouldn't matter. If I were writing a browser, I'd guess that I should reuse\n> the key obtained from a previous 401/WWW-Auth until I left the server --\n> that way, I minimize the extra roundtrips.\n\nThere is a certain tradeoff here. Sending the Authorization header with\nDigest credentials unnecessarily is not to be shrugged off too lightly,\nIMHO - this header is large and can easily double the number of bytes in\na request! (The example in the draft is typical and is 261 bytes).\nCertainly, extra roundtrips are to be avoided, but not at all costs.\nAlso, sending the Authorization header unnecessarily is likely to reduce\nthe cachability of many pages, thereby further increasing the traffic\n(how many responses currently contain the cache-control directive s-maxage\nor public? How quickly will this change?).\n\nI'm not really that comfortable with preemptively sending the\nAuthorization header with *all* requests to a server for which at least\none document needed authorization. I do prefer the heuristic used for\nthe Basic scheme as I believe it reflects reality much better (i.e.\nusually it's just certain url prefixes which require authorization).\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1364254"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Hi,\n\nI know this is an old messsage, but I must ask : has anybody tried to\nimplement this TLS upgrade draft ? Or is anybody contemplating it ?\nI would certainly like to get the benefits of secure software virtual\nservers, but I have concerns about this draft.\n\nAt first, as a server implementer, the proposal in this draft looks good,\nand quite straightforward to implement . All I have to do is parse the\nclear-text TLS upgrade request from the client, find the right keys and\ncerts for the virtual host requested, and upgrade the existing connection to\nTLS. Also, I can force a connection upon the client if an ACL is set. This\nis nothing major and looks clean.\n\nHowever, upon further examination, there are logistical and security\nproblems with doing a TLS upgrade. Here are a few.\n\n1) It is a fact that many server products out there use confidential data,\nlike application session keys, or other confidential state information, as\npart of the URI.\n\nSection 8.1 states :\n\"servers may apply resource access rules such as 'the FORM on this page must\nbe served and submitted using TLS'\n\nEven if a server enforces such a restriction, the URI will already be\ncompromised as part of the TLS upgrade process, because it is transmitted in\nclear by the client before the server instructs it to upgrade to TLS.\n\n2) Nowadays, web users know if they have an http link that the connection is\nnot secure, and that if it's https it is secure (SSL). It's possible by\nlooking at the source for an HTML form to determine ahead of time if the\ndata you will submit will go out in clear or not. The new upgrade draft\nmakes it unpredictable, by its use of regular http URLs.\n\nWith the new draft, there may be http:// link on a web form, for example to\nsubmit (POST) confidential data, made with the assumption that the server\nhas an ACL requiring a TLS upgrade and therefore that the connection will be\nsecured.\n\nA current-generation HTTP client will create a request with the form data\nfrom the user, and submit it in clear text to the server, compromising the\ndata even before the server gets a chance to deny it by requesting a TLS\nupgrade with an HTTP 426 return code. This compromises security in order to\nretain compatibility with existing HTTP clients. I do not think it is an\nacceptable compromise.\n\n3) Section 8.1 states :\n\n\"8.1 Implications for the https: URI Scheme\n\n   While nothing in this memo affects the definition of the 'https' URI\n   scheme, widespread adoption of this mechanism for HyperText content\n   could use 'http' to identify both secure and non-secure resources.\n\n   The choice of what security characteristics are required on the\n   connection is left to the client and server.  This allows either\n   party to use any information available in making this determination.\n   For example, user agents may rely on user preference settings or\n   information about the security of the network such as 'TLS required\n   on all POST operations not on my local net', or servers may apply\n   resource access rules such as 'the FORM on this page must be served\n   and submitted using TLS'. \"\n\nWhen it comes to security, I don't think ambiguity is a good thing, and it\nis not good to leave it entirely\n\"up to the client and server\" to determine what security characteristics are\nneeded for the connection.\nI think the draft should define at least one standard way to tell clients\nthat an upgrade is in order before connection establishment.\n\nTake the HTML form example described in my second remark.\n\nWhat is a new HTTP client aware of the TLS upgrade draft supposed to do when\nsubmitting such an HTML form ? Should it always try first thing to upgrade\nthe connection to any HTTP server to TLS, before issuing any meaningful\nrequest (containing URI and maybe also POST data) ?\n\nThat would certainly work technically and ensure best possible security, but\nit would be a major waste of bandwidth in 99% of the cases since current\nservers will not understand the upgrade request and reject it. Many existing\nservers don't support keep-alive and will even close the connection,\nintroducing extra latency due to the need to reconnect.\n\nIt may also be a waste of CPU for both the server and client, since perhaps\nthe connection didn't really need to be secure, but ended up as TLS because\nof the way the client negotiated the upgrade for every server, and the\nserver just accepted it.\n\nI think a key problem even for a next-generation HTTP client that is aware\nof the upgrade mechanism in this draft, and would be able to perform the\nupgrade to TLS - is that the original HTML form fails to specify that the\nclient should not POST the data in clear, but must request upgrade to TLS.\n\nThere needs to be a way to inform the client to request security prior to\nthe establishment of the connection. I think something still needs to be\nspecified in the URL to let the client know ahead of time that security is\nrequired for the connection. It cannot be a regular https:// URL since that\nwill not allow the server to differentiate the virtual host for which the\ndata is intended. I believe a clean way to do it is to instead have a\ndistinct protocol name in the URL.\n\nPerhaps there should be other ways to specify it than a protocol name, and\nperhaps it doesn't belong in the HTML and not HTTP specification, but I\nthink that this is a major hole in the draft. Certainly, a new protocol name\nmeans that some clients will not be able to follow some links on new servers\nthat require a TLS step up, but I think it is acceptable in the name of\nuncompromised security.\n\n\nI believe there is another approach to solving this problem of tying secure\nvirtual servers to IP addresses. It is completely different than the\nproposal in this draft, and does not address the issues of using dual ports\nfor non-secure and TLS, but I believe is is still relevant.\n\nInstead of trying to define a mechanism to upgrade the connection from\nnon-secure to secure, the virtual host negotiation could be made a standard\npart of the SSL/TLS protocols. I think that makes sense since these\nprotocols exchange certificates which are tied to the virtual host names.\nThis is how I would envision it would work :\n\nLet's take two virtual hosts, say secure.sun.com and secure.netscape.com .\nThey both resolve to the same IP address.\n\nA client wants to connect to https://secure.sun.com . The server happens to\npresent its public key with the certificate for secure.netscape.com . At\nthat point, the client normally brings a pop-up dialog saying that the\ncertificate doesn't match the host requested - which was secure.sun.com ,\nthen asks the user if he wants to continue, and the user usually stops the\nconnection.\n\nInstead, the client would by default agree to communicate with the server on\nthe current SSL/TLS socket, specifically to issue a \"virtual host change\"\nrequest for secure.sun.com .\n\nThe server would then realize that it also knows the key and cert for\nsecure.sun.com, and would be given the opportunity to renegotiate the SSL\nconnection with the client, this time presenting secure.sun.com's public key\nand certificate. The client could then verify that certificate and accept\nit, and the whole process would be transparent for the user.\n\nAs you will note, this process requires two full SSL/TLS handshakes, so it\nis more expensive in CPU. But that can be optimized. The secure virtual\nserver could have a unique public key, with several certificates signing\nthat same key - one certificate for each virtual host. It would then only\nhave to transmit the certificate for secure.sun.com, without having to\ntransmit a different public key. This second certificate would satisfy the\nclient, and would be much faster since it would save a second full\nhandshake. As part of TLS, this process would be generically applicable to\nany other type of TCP client/server application with TLS, not specifically\ntied to HTTP.\n\nIf you have made it this far, I'm now waiting for all your objections, so go\nahead and shoot !\n\nScott Lawrence wrote:\n\n> The secretariat didn't forward this announcement to this list, so I\n> will.\n>\n> The only change from version -04 is the addition of the standard\n> boilerplate and references  regarding the keywords (MUST, SHOULD,\n> MAY, etc).\n>\n> A New Internet-Draft is available from the on-line Internet-Drafts\n> directories.\n> This draft is a work item of the Transport Layer Security Working\n> Group of the IETF.\n>\n>         Title           : Upgrading to TLS Within HTTP/1.1\n>         Author(s)       : R. Khare, S. Lawrence\n>         Filename        : draft-ietf-tls-http-upgrade-05.txt\n>         Pages           : 13\n>         Date            : 05-Jan-00\n>\n> This memo explains how to use the Upgrade mechanism in HTTP/1.1 to\n> initiate Transport Layer Security (TLS) over an existing TCP\n> connection. This allows unsecured and secured HTTP traffic to share\n> the same well known port (in this case, http: at 80 rather than\n> https: at 443). It also enables 'virtual hosting,' so a single HTTP\n> + TLS server can disambiguate traffic intended for several hostnames\n> at a single IP address.\n> Since HTTP/1.1[1] defines Upgrade as a hop-by-hop mechanism, this\n> memo also documents the HTTP CONNECT method for establishing\n> end-to-end tunnels across HTTP proxies. Finally, this memo\n> establishes new IANA registries for public HTTP status codes, as\n> well as public or private Upgrade product tokens.\n>\n> A URL for this Internet-Draft is:\n> http://www.ietf.org/internet-drafts/draft-ietf-tls-http-upgrade-05.t\n> xt\n>\n> Internet-Drafts are also available by anonymous FTP. Login with the\n> username\n> \"anonymous\" and a password of your e-mail address. After logging in,\n> type \"cd internet-drafts\" and then\n>         \"get draft-ietf-tls-http-upgrade-05.txt\".\n>\n> A list of Internet-Drafts directories can be found in\n> http://www.ietf.org/shadow.html\n> or ftp://ftp.ietf.org/ietf/1shadow-sites.txt\n>\n> Internet-Drafts can also be obtained by e-mail.\n>\n> Send a message to:\n>         mailserv@ietf.org.\n> In the body type:\n>         \"FILE /internet-drafts/draft-ietf-tls-http-upgrade-05.txt\".\n>\n> NOTE:   The mail server at ietf.org can return the document in\n>         MIME-encoded form by using the \"mpack\" utility.  To use this\n>         feature, insert the command \"ENCODING mime\" before the \"FILE\"\n>         command.  To decode the response(s), you will need \"munpack\" or\n>         a MIME-compliant mail reader.  Different MIME-compliant mail\n> readers\n>         exhibit different behavior, especially when dealing with\n>         \"multipart\" MIME messages (i.e. documents which have been split\n>         up into multiple messages), so check your local documentation on\n>         how to manipulate these messages.\n>\n> --\n> Scott Lawrence      Director of R & D        <lawrence@agranat.com>\n> Agranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-13676979"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "At 10:42 PM -0700 5/3/00, Julien Pierre wrote:\n>I know this is an old messsage, but I must ask : has anybody tried to\n>implement this TLS upgrade draft ?\n\nYes, Scott has, and some other Internet Printing Protocol folks.\n\n>At first, as a server implementer, the proposal in this draft looks good,\n>and quite straightforward to implement .\n\nThanks! Scott  & I will take all the credit for the fine editing job, \nand none of the blame for the ideas :-)\n\n>1) It is a fact that many server products out there use confidential data,\n>like application session keys, or other confidential state information, as\n>part of the URI.\n\nCan't do much about that.\n\nThis is a general theme of my following comments. Guns can be pointed \nat feet. Triggers can be pulled...\n\n>Even if a server enforces such a restriction, the URI will already be\n>compromised as part of the TLS upgrade process, because it is transmitted in\n>clear by the client before the server instructs it to upgrade to TLS.\n\nIn general, I suspect that a secure-real-time web application should \nbe negotiating TLS *before* handing out a FORM at all. But even if it \ndid, the following rule applies:\n\nYou can *never* prevent a client from publishing data in the New York Times.\n\nIf the user's filling in sensitive data, it's the user-agent's job to \ndemand secure upgrades before sending as much as the server's, if not \nmore.\n\n>2) Nowadays, web users know if they have an http link that the connection is\n>not secure, and that if it's https it is secure (SSL).\n\nUser behavior has little to do with TCP port numbers. On the other \nhand, nothing in the draft supposes that we will ever change the \nexisting practice for Web Hypertext surfing applications.\n\n>A current-generation HTTP client will create a request with the form data\n>from the user, and submit it in clear text to the server, compromising the\n>data even before the server gets a chance to deny it by requesting a TLS\n>upgrade with an HTTP 426 return code. This compromises security in order to\n>retain compatibility with existing HTTP clients. I do not think it is an\n>acceptable compromise.\n\nDon't send the FORM in the clear, either.\n\n>When it comes to security, I don't think ambiguity is a good thing, and it\n>is not good to leave it entirely\n>\"up to the client and server\" to determine what security characteristics are\n>needed for the connection.\n\nThere is no such thing as absolute security. It is always, in the \nend, in the eye of the beholders.\n\n>I think the draft should define at least one standard way to tell clients\n>that an upgrade is in order before connection establishment.\n\nYes, it does -- 426.\n\n>That would certainly work technically and ensure best possible security, but\n>it would be a major waste of bandwidth in 99% of the cases since current\n>servers will not understand the upgrade request and reject it. Many existing\n>servers don't support keep-alive and will even close the connection,\n>introducing extra latency due to the need to reconnect.\n\nBala Krishnamurthy's PRO-COW compliance survey can tell you about the \nsmaller fraction that don't support any form of keep-alive.\n\n>It may also be a waste of CPU for both the server and client, since perhaps\n>the connection didn't really need to be secure, but ended up as TLS because\n>of the way the client negotiated the upgrade for every server, and the\n>server just accepted it.\n\nPurely as a matter of opinion, I'm worried about keeping Andy Grove \nemployed, not the other way around :-) Someday, every packet will \nfinally be encrypted, and the cycle-time argument is moot.\n\n>It cannot be a regular https:// URL since that\n>will not allow the server to differentiate the virtual host for which the\n>data is intended.\n\nSure it can -- interpret https: in a next-generation \"client\" as an \ninvitation to first try Upgrade on port 80, then fall-back to 423.\n\n>Instead of trying to define a mechanism to upgrade the connection from\n>non-secure to secure, the virtual host negotiation could be made a standard\n>part of the SSL/TLS protocols. I think that makes sense since these\n>protocols exchange certificates which are tied to the virtual host names.\n\nThis has long been on the \"to-do\" list for TLS/SSL. Installed base \nstrikes again, though, in making this case fairly unlikely. But I \nagree that's the \"right\" solution for so many more TLS/SSL enabled \napps than HTTP alone.\n\nLate night thoughts,\nRohit Khare\n\n\n\n", "id": "lists-012-13696013"}, {"subject": "RE: draft-ietf-tls-httpupgrade reissue", "content": "Rohit covered the ground pretty well, and all the points you raise\nwere discussed the first time around in one way or another, but I'll\nreiterate a point or two.\n\nUsers have been trained to believe that an 'https:' scheme means\n'secure', but what does it really mean?  What it means is 'try this\nconnection first on port 443 and negotiate (via the TLS/SSL\nhandshake) a set of security services'.  Key to this is\n'negotiate' - the resulting connection could negotiate a set of\nservices using any of several cipher suites, including easily\nbreakable or null encryption.  The 'https:' is, in effect, a single\nbit of information about how secure the connection for the request\nshould be, and that isn't enough to be meaningful.  A proper\ncomprehensive mechanism might provide for (for example) additional\nattributes to accompany HTML href that would specify the minimum\nrequired security services, but those don't exist and are outside\nthe scope of a protocol spec.\n\nSeveral reviewers objected (as you have) to the fact that an http:\nURL is exposed when first requested, and suggested that this\nrepresents a 'leak' - it should only be used on a secure connection.\nThe implication is that content (such as the link) should never be\nused over a connection less secure than the one over which it was\nfirst obtained (I serve a document over a secure connection, the\nuser clicks on a link and exposes a piece of it on an unsecured\nchannel).  The problem with this argument is that web clients do not\nnow and never have had this rule - secured and unsecured content is\ntreated in the same way.  What is the security status (rules for\nlinking) of a document I get from a floppy given to me by someone\nelse who downloaded it?  If there is a need for labelling of content\nwith security attributes, then that need is best met in the content\nitself, and the 'single bit' of appending an 's' to the scheme name\nis grossly insufficient.\n\nThe spec also serves to complete the definition of how the HTTP\nUpgrade mechanism should be used for any protocol change on an\nexisting connection.  When we actually tried to work it out for this\none case, we found that a number of issues (such as the lack of 426)\nhad not been addressed in the original 1.1 RFCs.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-13708421"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": ">Bala Krishnamurthy's PRO-COW compliance survey can tell you about \n>the smaller fraction that don't support any form of keep-alive.\n\nTo be sure, the blame is also on Martin Arlitt's shoulders :-)\n\nhttp://fog.hpl.hp.com/techreports/1999/HPL-1999-99.html\n\n>PRO-COW: Protocol Compliance on the Web\n>\n>Krishnamurthy, Balachander; Arlitt, Martin\n>\n>HPL-1999-99\n>990921\n>External\n>\n>Keyword(s): HTTP/1.1; protocol; World Wide Web; compliancy\n>\n>Abstract: With the recent (draft) standardization of the HTTP/1.1 \n>protocol on the Web, it is natural to ask what percentage of popular \n>Web sites speak HTTP/1.1 and how compliant are these so-called \n>HTTP/1.1 servers. We attempt to answer these questions through a \n>series of experiments based on the protocol standard. The tests are \n>run on a comprehensive list of popular Web sites to which a good \n>fraction of the Web traffic is directed. Our experiments were \n>conducted on a global extensible testing infrastructures that we \n>built to answer the above questions. The same infrastructure will be \n>used to answer questions like the percentage of the traffic flow \n>that is end-to-end HTTP/1.1 protocol compliancy and the subset of \n>features actually available to end users.\n>\n>23 Pages\n\n\nThe bottom line is that two thirds or more of public Web servers \nalready support persistent connections.\n\nBest,\nRohit Khare\n\n\n\n", "id": "lists-012-13719038"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Hi,\n\nRohit Khare wrote:\n\n> Thanks! Scott  & I will take all the credit for the fine editing job,\n> and none of the blame for the ideas :-)\n\nI don't think you'll get away with that. See below.\n\n> >1) It is a fact that many server products out there use confidential data,\n> >like application session keys, or other confidential state information, as\n> >part of the URI.\n>\n> Can't do much about that.\n>\n> This is a general theme of my following comments. Guns can be pointed\n> at feet. Triggers can be pulled...\n>\n> >Even if a server enforces such a restriction, the URI will already be\n> >compromised as part of the TLS upgrade process, because it is transmitted in\n> >clear by the client before the server instructs it to upgrade to TLS.\n>\n> In general, I suspect that a secure-real-time web application should\n> be negotiating TLS *before* handing out a FORM at all.\n\nMy question is : what would cause it to perform that TLS negotiation before hand,\nsince the URL is regular http ?\n\n> But even if it\n> did, the following rule applies:\n>\n> You can *never* prevent a client from publishing data in the New York Times.\n>\n> If the user's filling in sensitive data, it's the user-agent's job to\n> demand secure upgrades before sending as much as the server's, if not\n> more.\n\nI don't think users will waste their time filling forms if they are not ahead of\ntime certain that it will be transmitted securely.\nThe user-agent can only warn after the negotiation fails, and the user only has\nthe option of discarding all the data he entered.\n\n> >2) Nowadays, web users know if they have an http link that the connection is\n> >not secure, and that if it's https it is secure (SSL).\n>\n> User behavior has little to do with TCP port numbers. On the other\n> hand, nothing in the draft supposes that we will ever change the\n> existing practice for Web Hypertext surfing applications.\n\nMaybe your draft is appropriate for your internet printing applications. But\nclearly, it will not fill the needs of web surfers and ISPs who want to provide\nsecure virtual hosting without exhausting the IPv4 address space.\n\nThe duplicate TCP port number issue is IMHO less of a problem because it is rare\nto exhaust all 2**16 possible TCP ports on a server. That would mean you are\nrunning that many different daemons ...\n\n> >A current-generation HTTP client will create a request with the form data\n> >from the user, and submit it in clear text to the server, compromising the\n> >data even before the server gets a chance to deny it by requesting a TLS\n> >upgrade with an HTTP 426 return code. This compromises security in order to\n> >retain compatibility with existing HTTP clients. I do not think it is an\n> >acceptable compromise.\n>\n> Don't send the FORM in the clear, either.\n\nAgain, how do you propose that the client make the decision to upgrade ?\nDoes it have to try to upgrade on every non-secure HTTP link ?\nWill the browser prompt users for every link whether they want security ?\n\n> >When it comes to security, I don't think ambiguity is a good thing, and it\n> >is not good to leave it entirely\n> >\"up to the client and server\" to determine what security characteristics are\n> >needed for the connection.\n>\n> There is no such thing as absolute security. It is always, in the\n> end, in the eye of the beholders.\n\nThat's no reason to make it easier for people to shoot themselves in the foot,\nwhich is what this draft does.\n\n> >I think the draft should define at least one standard way to tell clients\n> >that an upgrade is in order before connection establishment.\n>\n> Yes, it does -- 426.\n\nJust how do you get a 426 before you establish a connection with the server ?\nTake the case with my HTML form, residing on site a, a product catalog site,\nwhich has no security.\nIt's got a link to POST the order data to site b , which is a secure virtual\nserver supporting TLS upgrade.\nWith your proposal, this link must be a regular http link.\nWhat will cause the browser to negotiate security when the credit card\ninformation is submitted to site b ?\nI contend that there has to be something on the order page to tell it to do so. I\nthink it should be part of the URL, perhaps a protocol of \"httpt\" - to specify\nhttp over TLS with upgrade.\nThe trigger cannot be a prompting option configured globally in the user-agent,\nbecause it would not be practical for users to be prompted for security on every\nregular HTTP request, and they would just disable it.\n\n\n> >That would certainly work technically and ensure best possible security, but\n> >it would be a major waste of bandwidth in 99% of the cases since current\n> >servers will not understand the upgrade request and reject it. Many existing\n> >servers don't support keep-alive and will even close the connection,\n> >introducing extra latency due to the need to reconnect.\n>\n> Bala Krishnamurthy's PRO-COW compliance survey can tell you about the\n> smaller fraction that don't support any form of keep-alive.\n\nEven so, the original point is bandwidth waste, and it is still valid.\nTo implement this, the user-agent must always initiate an upgrade request to the\nserver as its first request, then wait for the server to accept or ignore that\nrequest. This also adds latency in the initial client request to the server,\nwhich is delayed by this negotiation.\n\n\n> >It may also be a waste of CPU for both the server and client, since perhaps\n> >the connection didn't really need to be secure, but ended up as TLS because\n> >of the way the client negotiated the upgrade for every server, and the\n> >server just accepted it.\n>\n> Purely as a matter of opinion, I'm worried about keeping Andy Grove\n> employed, not the other way around :-) Someday, every packet will\n> finally be encrypted, and the cycle-time argument is moot.\n\nI'm in very strong disagreement. Keeping Intel (or Sun, for the matter) in\nbusiness is not the issue.\nYou may not be aware of that fact, but in a typical secure web server today, the\noverhead of doing encryption represents about 90% of CPU cycles spent. This means\na web server is an order of magnitude slower if it has to server secure\nconnections vs non-secure.\n\nIn turn, your proposal would result in an average tenfold increase in the\nhardware requirement for servers, as well as waste of energy to power all those\nCPUs, all for no good reason at all. Or the alternative would be a tenfold\nincrease in average web server latency, given no hardware upgrade.\n\nI contend that there are web applications for which it's perfectly ok to not be\nsecured - for instant, accessing publicly available content. And there is a\nminority of web applications where security is absolutely required, like\nelectronic commerce.\n\nSo far, I have never heard of a single application where security \"would be nice,\nbut it's ok if it is not available\" . This is what this draft really enables. I\ncall it a misfeature and I'm not eager to see it appear in mainstream HTTP\nclients and servers.\n\nI think it is necessary for a web application to be able to enforce security or\nno security.\n\n\n> >It cannot be a regular https:// URL since that\n> >will not allow the server to differentiate the virtual host for which the\n> >data is intended.\n>\n> Sure it can -- interpret https: in a next-generation \"client\" as an\n> invitation to first try Upgrade on port 80, then fall-back to 423.\n\nNo, this would break the semantic of the https URL.\nWhen you specify an https URL nowadays, it implicitly tells the client to connect\nonto the default https protocol port, which is 443, as long as the port is not\nexplicitly specified.\nIt is not very common, but I certainly have seen URLs of the form\nhttps://mysite:443 , with the port explicitly.\nBut the biggest problem is , if a different port is specified.\nWhat would be the behavior that you would propose for, say, an https://mysite:444\nURL ?\nShould the client skip the connection to port 80 and try TLS directly on 444 ?\nThis proposal seems shaky to me, because it effectively modifies the protocol by\nvirtue of running on a different port. I think the protocol should be able to run\non any TCP port without major behavior changes like this.\n\n> >Instead of trying to define a mechanism to upgrade the connection from\n> >non-secure to secure, the virtual host negotiation could be made a standard\n> >part of the SSL/TLS protocols. I think that makes sense since these\n> >protocols exchange certificates which are tied to the virtual host names.\n>\n> This has long been on the \"to-do\" list for TLS/SSL. Installed base\n> strikes again, though, in making this case fairly unlikely. But I\n> agree that's the \"right\" solution for so many more TLS/SSL enabled\n> apps than HTTP alone.\n\nGood. I think I will join their mailing list. Any pointers ?\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-13727963"}, {"subject": "Re: comments on draft-ietf-http-authentication01.tx", "content": "Paul Leach wrote:\n> > [DMK]\n> > Substantive stuff:\n> > General\n> >     (Formatting)  On a number of pages (e.g., 5,7,9), the text runs\n> >     into the footer.\n> >\n> >     The spec. is incomplete in its description of whether auth-params\n> >     are case-sensitive.  For example, \"realm\" and \"stale\" are mentioned\n> >     explicitly.  I think it would be simpler to say the\n> >     attribute/parameter/directive (see below) names are\n> >     case-insensitive, but that the values may or may not be\n> >     case-sensitive.\n> >\n> >     What should a client do if it receives unrecognized attributes?\n> >\n> Ignore them. I thought that was the \"HTTP way\" and needn't be expliclty\n> stated.\n> \n> >     What should a server do if it receives unrecognized attributes?\n> >\n> Ditto.\n\nI don't think it hurts to be explicit here.  Furthermore, since I got\nbeat up by Yaron about stating explicitly what agents should do with\nunrecognized attributes (namely, ignore) in RFC 2109, I feel obliged to\nreturn the favor.\n \n> >     The term \"protection space\" gets used without a definition (here),\n> >     but the spec. describes how a client can reuse credentials for such\n> >     a protection space.  I think we should say that the description of\n> >     any auth-scheme must describe the rules for deciding when two\n> >     objects are in the same protection space.  In particular, a client\n> >     must be able to tell, so it knows whether or not to send credentials\n> >     unprompted.\n> >\n> Why does it need to tell? If it's wrong, by either sending incorrect ones or\n> not sending any, it'll get a 401 to tell it what to do. As far as I can see,\n> for Digest it's only an optimization. (For Basic, you don't want to send\n> your credentials to the wrong place...)\n\nTrue enough.  But if you're right, why bother to define the notion of\n\"protection space\" at all?\n\nSince the spec. goes to the trouble of defining a protection space\n(sect. 1.2), I think each type of authentication (including successors\nto Basic and Digest) ought to state clearly what its protection space\nis.\n\n> \n> > Sect. 3.2.1, The WWW-Authenticate Response Header\n> >     [domain attribute]\n> >     If this keyword is omitted or empty, the client should assume that\n> >     the domain consists of all URIs on the responding server.\n> >\n> >       This behavior is different from Basic.  If we want Digest to be\n> >       a more or less drop-in replacement, shouldn't the default\n> >       behavior mimic Basic?\n> >\n> As you point out below, there are implementations. As I point out above, it\n> shouldn't matter. If I were writing a browser, I'd guess that I should reuse\n> the key obtained from a previous 401/WWW-Auth until I left the server --\n> that way, I minimize the extra roundtrips.\n\nOkay.\n \n> >       Furthermore, this paragraph constitutes the equivalent of a\n> >       description of the Digest protection space, but it never calls\n> >       it such.  It would be helpful to be more explicit.  (There's\n> >       additional related information in section 3.3.)\n> >\n> I'm not sure it is.\n\nSure it is.  The protection space is the set of URIs for which a given\nset of credentials applies.  Certainly when there's an explicit \"domain\"\nattribute it defines the protection space.  A missing \"domain\" attribute\nimplicitly sets the protection space to be \"the server\".\n\n> \n> > Sect. 3.2.2, The Authorization Request Header\n> >     [cnonce attribute]\n> >     RFC 2069-compliant implementations might break upon receiving this\n> >     new, previously unknown attribute.\n> >\n> Then they would be broken. Unknown directives are supposed to be ignored.\n\nWhere does it say that? :-)  Unrecognized headers should be ignored, but\neven RFC 2068 is not explicit about unrecognized attributes (the word\nused there).  Seriously, in the interests of increasing the likelihood\nof interoperating implementations, I think it helps to say what you\nmean.\n\n> > Sect. 3.2.3, The Authentication-Info Header\n> >     What should a client do if the rspauth=response-digest information\n> >     is wrong?\n> >\n> Not accept the response.\n\nHow does a client, which has already read a response, \"not accept\n[it]\"?  I'm picking nits here, true.  Does it mean that a browser would\nshow the user an error saying that the received response was in error? \nOr does it just stop spinning its logo and leave on the screen what was\nalready there?\n\nSuppose the client is a proxy.  What should it do vis-a-vis its client?\n\n> >\n> >     Isn't there the risk that an intervening proxy could change the\n> >     status code?\n> >       ... Authorization header for the request, A2 is\n> >          A2       = Status-Code \":\" digest-uri-value\n> >       and if \"qop=auth-int\", then A2 is\n> >          A2       = Status-Code \":\" digest-uri-value \":\" H(entity-body)\n> >\n> Well, the status code isn't a header, but there's a general proscription\n> against needlessly changing headers in 13.5.2. Maybe the status line\n> contents should be explicitly added to that list.\n\nIs it possible to say a proxy can't change its status code?  Suppose you\nhave browser B, caching proxy P, origin server S.  (I'm sure you'll tell\nme if this example is way off base.)  B requests object X, which it does\nnot have in its local cache.  P has the object, but the object has\nCache-Control: must-revalidate.  P sends a *conditional* request to S. \nAfter S asks for credentials, which response P passes to B, B asks again\nfor the X  S responds with 302 and (is this right?  possible?) an\nAuthentication-Info header.  The A-I header would presumably contain a\ndigest of the \"302\", but the proxy would return a 200 and supply X to B,\nalong with A-I.  B would be unable to match the A-I header and the\nresponse and would assume the response is bogus.\n\nDave Kristol\n\n\n\n", "id": "lists-012-1373905"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Hi,\n\nScott Lawrence wrote:\n\n> Rohit covered the ground pretty well, and all the points you raise\n> were discussed the first time around in one way or another, but I'll\n> reiterate a point or two.\n>\n> Users have been trained to believe that an 'https:' scheme means\n> 'secure', but what does it really mean?  What it means is 'try this\n> connection first on port 443 and negotiate (via the TLS/SSL\n> handshake) a set of security services'.  Key to this is\n> 'negotiate' - the resulting connection could negotiate a set of\n> services using any of several cipher suites, including easily\n> breakable or null encryption.\n\nAgreed. However, one can disable the null & easily breakable cipher\nsuites in their client, and therefore be sure that when https URLs are\nsubmitted, the connection is secure. The NULL encryption is by default\ndisabled in mainstream browsers.\n\n\n>  If there is a need for labelling of content\n> with security attributes, then that need is best met in the content\n> itself, and the 'single bit' of appending an 's' to the scheme name\n> is grossly insufficient.\n\nI agree that it would be better to get more security information in the\ncontent links than just that one \"s\" bit.\n\nHowever, the complete lack of even that one bit to determine the\nsecurity attribute, which is what you propose by using regular http\nURLs, is not merely grossly insufficient, but completely unacceptable.\n\nI understand that you are trying to keep some level of compatibility\nwith existing clients, and at the same time trying to unify the ports\nfor secure/non-secure servers, and allowing secure virtual servers. I\nbelieve however that none of the problems are solved adequately :\n\n- existing HTTP clients will compromise security when connected to the\nnew servers, because they will not be able to negotiate the TLS ugrade\n- existing HTTPS clients will not even connect to the new servers,\nbecause the server will be expecting an initial non-secure connection\nfollowed by an upgrade\n\nThis shows that it's not a practical solution for saving TCP ports at\nthis time. It requires an entirely new generation of servers and\nclients, and even then there is still doubt about how the upgrade to TLS\nis enforced, as mentioned in previous e-mails.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-13745134"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Julien Pierre wrote:\n\n> You may not be aware of that fact, but in a typical secure web server today, the\n> overhead of doing encryption represents about 90% of CPU cycles spent. This means\n> a web server is an order of magnitude slower if it has to server secure\n> connections vs non-secure.\n>\n> In turn, your proposal would result in an average tenfold increase in the\n> hardware requirement for servers, as well as waste of energy to power all those\n> CPUs, all for no good reason at all. Or the alternative would be a tenfold\n> increase in average web server latency, given no hardware upgrade.\n\nJust because a client always asks to upgrade doesn't mean the server has to obey.\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |\"HTTP is what happens in the absence of good |\n|francis@ecal.com|design.\" -- Keith Moore                      |\n\\==============================================================/\n\n\n\n", "id": "lists-012-13755065"}, {"subject": "RE: draft-ietf-tls-httpupgrade reissue", "content": "> From: Julien Pierre\n\n> I don't think users will waste their time filling forms\n> if they are not ahead of\n> time certain that it will be transmitted securely.\n\nIf they are that concerned about it, then they should not fill out\nforms that were not delivered securely.  If the form was delivered\nover an unsecured connection, it may have been modified in any\nnumber of ways to subvert the apparent intent of the form.  Browsers\ndon't normally expose the ACTION attribute of a form - an attacker\nmay have changed that, or modified field names - the possibilities\nare endless.  Encrypting one exchange in a multiple exchange\ntransaction is no security at all.\n\n> The duplicate TCP port number issue is IMHO less of a\n> problem because it is rare\n> to exhaust all 2**16 possible TCP ports on a server.\n\nThe concern is with the well-known ports - a much much smaller\nspace.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-13763439"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Hi,\n\nJohn Stracke wrote:\n\n> > In turn, your proposal would result in an average tenfold increase in the\n> > hardware requirement for servers, as well as waste of energy to power all those\n> > CPUs, all for no good reason at all. Or the alternative would be a tenfold\n> > increase in average web server latency, given no hardware upgrade.\n>\n> Just because a client always asks to upgrade doesn't mean the server has to obey.\n\nSo how does that make the draft useful if the upgrade is never allowed ?\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-13772707"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Hi,\n\nScott Lawrence wrote:\n\n> > From: Julien Pierre\n>\n> > I don't think users will waste their time filling forms\n> > if they are not ahead of\n> > time certain that it will be transmitted securely.\n>\n> If they are that concerned about it, then they should not fill out\n> forms that were not delivered securely.  If the form was delivered\n> over an unsecured connection, it may have been modified in any\n> number of ways to subvert the apparent intent of the form.  Browsers\n> don't normally expose the ACTION attribute of a form - an attacker\n> may have changed that, or modified field names - the possibilities\n> are endless.  Encrypting one exchange in a multiple exchange\n> transaction is no security at all.\n\nOK, assume the form was delivered on a secure server. Then it gets\nsubmitted to somebody else - a virtual host on a payment processing\nserver - to actually process the transaction. This is very common. The\naction URL will use regular http .\nHow does the security upgrade get triggered ?\n\nOne more thing to consider :\nLet's say you are browsing a site and the connection got upgraded to TLS\n. Then you sit idle for a while, and your keep-alive connection times\nout. You are on a different part of the site now, filled with normal\nhttp:// links. You click on one. The client has to reconnect. How does\nthe security get restored ?\n\n\n>\n> > The duplicate TCP port number issue is IMHO less of a\n> > problem because it is rare\n> > to exhaust all 2**16 possible TCP ports on a server.\n>\n> The concern is with the well-known ports - a much much smaller\n> space.\n>\n> --\n> Scott Lawrence      Director of R & D        <lawrence@agranat.com>\n> Agranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-13780813"}, {"subject": "Using Chunked encoding to send control messages to a clien", "content": "I'm using HTTP in an application as a transport for XML documents\nthat are 'POSTed' to a servlet for processing.\n\nI am using persistent connections and I have a need to in some \nsituations send a 'control message' back to a client indicating\nthat they have temporarily blocked from sending any more requests\nto the server (say for example that the server has become very busy\nand would like clients to stop sending requests until he can get\ncaught up).\n\nI've thought about using chunked encoding to respond to a request\nwith a chunked message whose body indicates to the client he/she \nhas been blocked. At a later time I can then send back another \nchunk indicating the client is no longer blocked, then send back\na chunk that has the response to the original request, and then\nsend back the zero chunk to indicate the end of the response.\n\nI have some misgivings about this because I feel that it would\nbe using the chunked encoding for a purpose that it was not intended\nfor and the chunks should all collectively, once concatenated together,\nmake up the response to the original request.\n\nDoes anyone have any thoughts on this either way? Is it an OK thing\nto do or is it a violation of the spirit of the protocol?\n\nOne problem I can see is that chunked responses are probably assembled\ntogether on the client side at the session layer while the 'special'\ncontrol message needs to be understood at the application layer. It would\nbe bad for the session layer to have to know about application layer\ncontrol messages and somehow dispatch the event that one has been\nreceived to the application lauyer.\n\nthx,\n\n--jl\n\n\n\n", "id": "lists-012-13790554"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Julien Pierre wrote:\n\n> John Stracke wrote:\n>\n> > Just because a client always asks to upgrade doesn't mean the server has to obey.\n>\n> So how does that make the draft useful if the upgrade is never allowed ?\n\nWho said never?\n\nA client that wants to use the draft can ask for an upgrade on every HTTP request.\nServers that aren't willing to spend cycles on TLS for a particular request can just\nignore the Upgrade: header.\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |\"God does not play games with His loyal      |\n|francis@ecal.com|servants.\" \"Whoo-ee, where have you *been*?\" |\n|                |--_Good Omens_                               |\n\\==============================================================/\n\n\n\n", "id": "lists-012-13799601"}, {"subject": "Re: Using Chunked encoding to send control messages to a clien", "content": "Why can't the server just stop reading requests?  Then TCP will block the\nsender automatically.\n\nI don't think chunking alone will solve your problem.  You might be able to\nget away with a MIME multipart response sent with chunked encoding.  The\nissue that always gets raised when this kind of thing comes up is this:  an\nHTTP proxy is allowed to buffer the response and remove the chunked\nencoding, sending monolithic response to the client.  With the multipart\nencoding, you could still pick apart the response, but now the timing would\nbe much different (you'd get all parts of the response at once).\n\nThat said, why would  a proxy ever want to do this buffering and\ncoalescing?  Most server vendors seem to abhor buffering arbitrarily large\nquantities of data.  And, of course, there is a detrimental effect on\nresponse time.  But that's the spec.  The HTTP spec even includes\npseudo-code for removing the chunked encoding and resending the body with\nContent-Length.\n\n     -Carl\n\n\n\nJames Lacey <jlacey@ftw.paging.mot.com> on 05/05/2000 06:46:33 AM\n\nPlease respond to James Lacey <jlacey@ftw.paging.mot.com>\n\nTo:   http-wg@hplb.hpl.hp.com\ncc:\nSubject:  Using Chunked encoding to send control messages to a client\n\n\n\n\n\nI'm using HTTP in an application as a transport for XML documents\nthat are 'POSTed' to a servlet for processing.\n\nI am using persistent connections and I have a need to in some\nsituations send a 'control message' back to a client indicating\nthat they have temporarily blocked from sending any more requests\nto the server (say for example that the server has become very busy\nand would like clients to stop sending requests until he can get\ncaught up).\n\nI've thought about using chunked encoding to respond to a request\nwith a chunked message whose body indicates to the client he/she\nhas been blocked. At a later time I can then send back another\nchunk indicating the client is no longer blocked, then send back\na chunk that has the response to the original request, and then\nsend back the zero chunk to indicate the end of the response.\n\nI have some misgivings about this because I feel that it would\nbe using the chunked encoding for a purpose that it was not intended\nfor and the chunks should all collectively, once concatenated together,\nmake up the response to the original request.\n\nDoes anyone have any thoughts on this either way? Is it an OK thing\nto do or is it a violation of the spirit of the protocol?\n\nOne problem I can see is that chunked responses are probably assembled\ntogether on the client side at the session layer while the 'special'\ncontrol message needs to be understood at the application layer. It would\nbe bad for the session layer to have to know about application layer\ncontrol messages and somehow dispatch the event that one has been\nreceived to the application lauyer.\n\nthx,\n\n--jl\n\n\n\n", "id": "lists-012-13807869"}, {"subject": "RE: Using Chunked encoding to send control messages to a clien", "content": "kugler@us.ibm.com wrote,\n> The issue that always gets raised when this kind of thing \n> comes up is this:  an HTTP proxy is allowed to buffer the \n> response and remove the chunked encoding, sending monolithic \n> response to the client.  With the multipart encoding, you \n> could still pick apart the response, but now the timing would\n> be much different (you'd get all parts of the response at \n> once).\n\nThat's one problem.\n\nAnother, which affects both chunked transfer encoding and the\nmultipart technique, is that a proxy is at liberty to set quite\na short idle-timeout (say 30 secs to a couple of minutes) to \nprotect itself from hung origin servers and double-sided DOS \nattacks (malicious client on one side, malicious server on the\nother).\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-13819356"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Hi,\n\nJohn Stracke wrote:\n\n> Julien Pierre wrote:\n>\n> > John Stracke wrote:\n> >\n> > > Just because a client always asks to upgrade doesn't mean the server has to obey.\n> >\n> > So how does that make the draft useful if the upgrade is never allowed ?\n>\n> Who said never?\n>\n> A client that wants to use the draft can ask for an upgrade on every HTTP request.\n> Servers that aren't willing to spend cycles on TLS for a particular request can just\n> ignore the Upgrade: header.\n\nThat still does not solve the problem !!!\n\nIf the client tries to upgrade to TLS on every request, it will fail 99% of the time,\nbecause servers don't support it. Suppose in 1% of the cases it will actually work.\n\nNow, suppose that in 2% of the cases, the request data was intended to be confidential\nand the user really didn't want to submit the data unsecured, but the server didn't\nnegotiate TLS. The only way for a user to make sure he does not mistakenly submit data\nunencrypted is to setup his browser to prompt him on every single HTTP request that\ndidn't successfully negotiate TLS.\n\nIn other words, a waste of time 98% of requests. Do you really find that to be acceptable\n?\n\nThe whole point I'm trying to make is that there should be a way for a web application\nthat is intended to be secure to enforce that fact and reasonably function on a server\nrunning on a common port with HTTP and TLS upgrade support. The draft does not propose a\nway to do that.\n\nIt would be as simple as a new type of \"httpt\" URL - which would tell the user-agent to\nconnect insecurely with the server, and immediately negotiate a TLS connection; and\notherwise not to proceed if the TLS upgrade fail. This cannot be a global user-agent\nsetting for reasons explained before - security is not always required nor desirable.\n\nIf you really want to get ISPs to stop wasting IP addresses on multiple secure servers,\nas well as separate the ports, then you need to make it possible to create new secure web\napplications that will work with the new user-agents and servers proposed in the new\ndraft, without incurring a high risk of falling back to non-secure connections. The draft\nis currently too vague in the way that the security is enforced and makes it way too easy\nto shoot yourself in the foot and end up with a non-secure connection if the negotiation\nfails, rather than aborting.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-13827904"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Hi,\n\nRohit Khare wrote:\n\n> You know, we send the HTTP/1.1 and Host: tokens 100% of the time.\n> Isn't that wasteful, too?\n\nNot as much. I meant also that the 98% of the time security pop-up *\nmillions of users would be a huge waste of time. I'm sure most everyone\nwould disable the browser setting if it existed.\n\n> >Now, suppose that in 2% of the cases, the request data was intended\n> >to be confidential\n> >and the user really didn't want to submit the data unsecured, but\n> >the server didn't\n> >negotiate TLS.\n>\n> This client would have to force the upgrade via OPTIONS as outlined\n> in the spec.\n\nBut how would the end-user inform its client of the fact that he wanted\nthe data to be secure ? You would have to remember to \"force\" security\nsomehow before submitting on a form ?\n\nThe security requirement is usually dictated by application logic. I\nthink the web application should have a way to hint the client, in the\nway of something in the referring document, so that it would happen\ntransparently.\n\nI'm not criticizing the underlying network protocol. I'm criticizing the\nURL specification that clients will use.\nThe server will support both HTTP and HTTP with TLS upgrade. But there is\nno good way for the client to decide when to use which, since http://\nURLs are used.\n\n> >The whole point I'm trying to make is that there should be a way for\n> >a web application\n> >that is intended to be secure to enforce that fact and reasonably\n> >function on a server\n> >running on a common port with HTTP and TLS upgrade support. The\n> >draft does not propose a\n> >way to do that.\n>\n> We believe it does, the last-call in the working group and the ietf\n> announce list believes it does, and now it falls to all of us,\n> including yourself, to find out if any of that rough consenus can be\n> backed by running code.\n\nI am quite confident that I can, but I'm not convinced yet that this is\nthe right way to do it.\n\n> An \"application\" -- say,\n> http://www.foo.edu/randompath/ReallySecureApp.cgi should send back a\n> 426 for ANY request for a resource under /randompath/. A \"user\" who\n> requires the same can use OPTIONS. A user who does not care, but\n> wants to follow the lead of the server, should reuse the Upgraded TLS\n> connection as long as he would have an equivalent port 443.\n\nAgain, what mechanism tells the client to use OPTIONS ?\n\n> And recall that this method is *not* being proposed as the idea for\n> hypertext publishing. The common browser is too deployed to be\n> changed in its ways.\n\nI don't think that's true. The rate of adoption for new HTTP software may\nnot be what it was in 1994 - 1996, but HTTP is still evolving, and there\nis still an evolution in the mainstream browsers and servers. In fact,\nthe whole reason I'm here on this list is that I'm trying to make the\nNetscape server support this evolution. I know some Netscape client\nengineers are reading this as well, and they would like to make it happen\non their side too.\n\nThe fact is, routable static IP addresses have become expensive due to\nthe fact that we are quickly running out of them. ISPs want a way to do\nsecure software virtual servers, without buying expensive IP addresses\nfor each. There is a demand in the marketplace for this, maybe not\nnecessarily for the purpose of hypertext publishing, but certainly for\nelectronic commerce. Since the current protocols do not allow this, the\nsolution calls for a new generation of servers and clients. It will\ncertainly take time to upgrade a significant portion of HTTP clients and\nservers in use today, but I think it can happen gradually. My wish is to\nsee it happen using open IETF standards in the next generation of HTTP\nservers and clients, rather than have to wait for the one after that,\nonce the quirks in the current specs are acknowledged.\n\n--\nfor a good time, try kill -9 -1\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-13837937"}, {"subject": "Has Netscape 5 become 6", "content": "Hi,\n\nI am currently working on a project on persistent connection and\nnon-persistent connection.\nI found that\nhttp://www.w3.org/Protocols/HTTP/Forum/Reports/#Public\nGagan Saksena mentions a test on Netscape 5\nI cannot find Netscape 5 on their website\n\nIn addidtion, I will appreciate any related information about persistent\nconnection and non-persistent connection\n\nThx\nJacinle Young\n\n\n\n", "id": "lists-012-13849002"}, {"subject": "Re: draft-ietf-tls-httpupgrade reissue", "content": "Julien Pierre wrote:\n\n> If the client tries to upgrade to TLS on every request, it will fail 99% of the time,\n> because servers don't support it.\n\nServers that don't support it ignore it, because RFC-2616 doesn't provide an error code to\nmean \"I don't understand that Upgrade:\".  (I just tried it on Apache 1.3.9 and 1.3.12, NES\n3.6, and IIS 5.0; they all behaved exactly the same with and without \"Upgrade: foo\".) This\nmeans that a client that sends the Upgrade: all the time won't break anything; it will cost\na few extra bytes, but not the extra round trips you're talking about.\n\n--\n/================================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.  |\n|Chief Scientist |===============================================|\n|eCal Corp.      |The cheapest, fastest, most reliable components|\n|francis@ecal.com|of a computer system are those that aren't     |\n|                |there.--Gordon Bell                            |\n\\================================================================/\n\n\n\n", "id": "lists-012-13856114"}, {"subject": "Re: Has Netscape 5 become 6", "content": "[Kaming Young <kmyoung7@ie.cuhk.edu.hk>]\n> Hi,\n> \n> I am currently working on a project on persistent connection and\n> non-persistent connection.\n> I found that\n> http://www.w3.org/Protocols/HTTP/Forum/Reports/#Public\n> Gagan Saksena mentions a test on Netscape 5\n> I cannot find Netscape 5 on their website\n> \n Yes.  There is to be no Netscape 5.  The next version (now in pre-release\nalpha) is 6.\n\n-- \nArithere is no spoon\n-------------------------------------------------------------------------\nhttp://www.nebcorp.com/~regs/pgp for PGP public key\n\n\n\n", "id": "lists-012-13864191"}, {"subject": "Re: Has Netscape 5 become 6", "content": "Yep,  Netscape had to make sure they had a higher version number than\nInternet Explorer because the version number is the true test of how\nadvanced/well developed an app is.  <G>\n\n----- Original Message -----\nFrom: \"Ari Gordon-Schlosberg\" <regs@nebcorp.com>\nTo: <http-wg@cuckoo.hpl.hp.com>\nSent: Monday, May 08, 2000 4:39 PM\nSubject: Re: Has Netscape 5 become 6?\n\n\n> [Kaming Young <kmyoung7@ie.cuhk.edu.hk>]\n> > Hi,\n> >\n> > I am currently working on a project on persistent connection and\n> > non-persistent connection.\n> > I found that\n> > http://www.w3.org/Protocols/HTTP/Forum/Reports/#Public\n> > Gagan Saksena mentions a test on Netscape 5\n> > I cannot find Netscape 5 on their website\n> >\n>  Yes.  There is to be no Netscape 5.  The next version (now in pre-release\n> alpha) is 6.\n>\n> --\n> Ari there is no spoon\n> -------------------------------------------------------------------------\n> http://www.nebcorp.com/~regs/pgp for PGP public key\n>\n>\n\n\n\n", "id": "lists-012-13872059"}, {"subject": "Re: comments on draft-ietf-http-authentication01.tx", "content": "Dave Kristol writes, regarding Paul's comments:\n\n    > >     What should a client do if it receives unrecognized attributes?\n    > >\n    > Ignore them. I thought that was the \"HTTP way\" and needn't be expliclty\n    > stated.\n    > \n    > >     What should a server do if it receives unrecognized attributes?\n    > >\n    > Ditto.\n    \n    I don't think it hurts to be explicit here.  Furthermore, since I\n    got beat up by Yaron about stating explicitly what agents should do\n    with unrecognized attributes (namely, ignore) in RFC 2109, I feel\n    obliged to return the favor.\n     \nI agree with Dave.  It may be hard for those of us who have spent\n3-4 years on this mailing list to remember that not everyone has\nso fully absorbed the \"HTTP way.\"  And it would be inhumane to\nsuggest that any newcomer try to catch up.  (According to www.findmail.com,\nthere are currently 8039 messages in the HTTP-WG archive!)\n\nWhenever someone asks \"what should X do in situation Y\" and we\ncan't answer it by quoting directly (and only) from a written\nspecification (i.e., RFC or unexpired I-D), then we have a problem\nthat should be solved.  Answers of the form \"do it the way it's\nalways been done\", whether by appeal to folklore or source code,\ndon't replace proper specification.\n\n-Jeff\n\n\n\n", "id": "lists-012-1388004"}, {"subject": "persistent conncetion HOL blockin", "content": "Hi,\n\nI just wonder if we use persistent connection, if the HOL blocking would\naffect the performance and in this case non-persistent connection will\ndo better.\n\nJacinle\n\n\n\n", "id": "lists-012-13880774"}, {"subject": "Re: persistent conncetion HOL blockin", "content": "hi,\n\nHOL blocking means Head Of Line blocking\nWhat i mean is\nconsider\nAlice send out a request for a large latest hit MP3 file and then a request\nfor a small text file\nwhich both resides on the same server.\nso the response for MP3 file will block the second response and the response\nthat follow in the persistent connection case.\n\nKaming Young\n\nJohn Stracke wrote:\n\n> Kaming Young wrote:\n>\n> > I just wonder if we use persistent connection, if the HOL blocking would\n> > affect the performance\n>\n> What is HOL blocking?\n>\n> --\n> /=================================================================\\\n> |John Stracke    | http://www.ecal.com |My opinions are my own.   |\n> |Chief Scientist |================================================|\n> |eCal Corp.      |Vlad was not a vampire, but that's the only nice|\n> |francis@ecal.com|thing that could be said about him.             |\n> \\=================================================================/\n\n\n\n", "id": "lists-012-13887656"}, {"subject": "Review of Platform for Privacy Preferences (P3P) requeste", "content": "The W3C is finishing up their Platform for Privacy Preferences (P3P)\nspecification:\n\n    http://www.w3.org/TR/2000/WD-P3P-20000424/\n\nThis specification appears to cover things that should be of interest to both\nthe HTTP and WREC working groups. Accordingly, I would like to ask that\ninterested parties review the current specification. Comments can either be\nsent to the appropriate WG mailing lists or to the Application Area Directors\n(myself, ned.freed@innosoft.com, and paf@swip.net).\n\nThanks!\n\nNed\n\n\n\n", "id": "lists-012-13896283"}, {"subject": "Re: persistent conncetion HOL blockin", "content": "Kaming Young writes:\n\n    HOL blocking means Head Of Line blocking\n\n    What i mean is consider Alice send out a request for a large latest\n    hit MP3 file and then a request for a small text file which both\n    resides on the same server.  so the response for MP3 file will\n    block the second response and the response that follow in the\n    persistent connection case.\n    \nThe HTTP/1.1 Draft Standard (RFC2616) address the head-of-line\nblocking issue with respect to proxies, in section 8.1.4, where\nit says:\n   A proxy SHOULD use up to 2*N connections to\n   another server or proxy, where N is the number of simultaneously\n   active users.\n\nI can't remember why we didn't say \"to avoid head-of-line blocking\"\nin this sentence, since this is precisely the reason for saying\nthat proxies aren't expected to multiplex lots of clients on\none connection.\n\nThe same paragraph says:\n   A single-user client SHOULD NOT maintain more than 2 connections\n   with any server or proxy.\n\nThe reasoning behind that requirement is related to a kind of\nhead-of-line blocking.  Consider a client loading a long HTML\npage with lots of images.  The client should probably start\nthe process of loading the images (on connection #2) while\ncontinuing to load the HTML (on connection #1); we don't want\nthe images blocked until the entire HTML file is loaded.\n\nI'm not sure it makes sense for a single browser window to\nbe simultaneously loading a text file and an MP3 file, but\nI guess it would make sense for a single user to be loading\nboth in separate windows of a single browser application.\nEven so, the spec still allows these two simultaneous connections.\n\nAnyway, we never specifically defined \"single-user\", so I\nthink a client implementor should use good judgement in\ndeciding whether a browser with multiple active windows\ncounts as one \"user\" or several.\n\nThe point of these requirements was not to force people to\nsuffer from head-of-line blocking.  It was to allow the\nimplementation of clients and proxies that do not suffer\nfrom head-of-line blocking, while discouraging them from\nusing more TCP connections than necessary.\n\n-Jeff\n\n\n\n", "id": "lists-012-13904153"}, {"subject": "WAP gateway and HTT", "content": "Welcome\n\nI would like ask you about WAP GATEWAY and Web Server.\nMay by somebody know what is sent in HTTP GET request header during communication\nWAPGETWAY Web SERVER with HTTTP/1.1 protocol?\nWhich filds Accep, User-Agent ... and value I must use to recieve proper response (.hdml\nor .wml file)?\n\nThanks for help.\nMarek Wyrwicki\n\n-- \n..............................\n.. Marek Wyrwicki\n.. ERICPOL TELECOM\n.. e-mail: mawy@ericpol.pl\n.. Work phone: +48 42 6315532\n..\n\n\n\n", "id": "lists-012-13913821"}, {"subject": "Re: persistent conncetion HOL blockin", "content": "Hi,\n\nThank you so much for enlightening me.\n\nHow could a client know in advance that it will have got a hugh response\nso that it opens another connection?\nspecification by file type?\n\nKaming\n\n\nOn Thu, 11 May 2000, Jeffrey Mogul wrote:\n\n> Kaming Young writes:\n> \n>     HOL blocking means Head Of Line blocking\n> \n>     What i mean is consider Alice send out a request for a large latest\n>     hit MP3 file and then a request for a small text file which both\n>     resides on the same server.  so the response for MP3 file will\n>     block the second response and the response that follow in the\n>     persistent connection case.\n>     \n> The HTTP/1.1 Draft Standard (RFC2616) address the head-of-line\n> blocking issue with respect to proxies, in section 8.1.4, where\n> it says:\n>    A proxy SHOULD use up to 2*N connections to\n>    another server or proxy, where N is the number of simultaneously\n>    active users.\n> \n> I can't remember why we didn't say \"to avoid head-of-line blocking\"\n> in this sentence, since this is precisely the reason for saying\n> that proxies aren't expected to multiplex lots of clients on\n> one connection.\n> \n> The same paragraph says:\n>    A single-user client SHOULD NOT maintain more than 2 connections\n>    with any server or proxy.\n> \n> The reasoning behind that requirement is related to a kind of\n> head-of-line blocking.  Consider a client loading a long HTML\n> page with lots of images.  The client should probably start\n> the process of loading the images (on connection #2) while\n> continuing to load the HTML (on connection #1); we don't want\n> the images blocked until the entire HTML file is loaded.\n> \n> I'm not sure it makes sense for a single browser window to\n> be simultaneously loading a text file and an MP3 file, but\n> I guess it would make sense for a single user to be loading\n> both in separate windows of a single browser application.\n> Even so, the spec still allows these two simultaneous connections.\n> \n> Anyway, we never specifically defined \"single-user\", so I\n> think a client implementor should use good judgement in\n> deciding whether a browser with multiple active windows\n> counts as one \"user\" or several.\n> \n> The point of these requirements was not to force people to\n> suffer from head-of-line blocking.  It was to allow the\n> implementation of clients and proxies that do not suffer\n> from head-of-line blocking, while discouraging them from\n> using more TCP connections than necessary.\n> \n> -Jeff\n> \n> \n\n\n\n", "id": "lists-012-13920539"}, {"subject": "Re: persistent conncetion HOL blockin", "content": "> The same paragraph says:\n>    A single-user client SHOULD NOT maintain more than 2 connections\n>    with any server or proxy.\n> \n> The reasoning behind that requirement is related to a kind of\n> head-of-line blocking.\n\nAlthough, in the case of a proxy if a client is trying to get requests\nfrom a number of different servers, through the same persistent connection\nto a proxy, then it's not too difficult to get in a situation where a\nproxy can't immediately return a response to the client because it's not\nready or able to reply to a previous request (on that same connection) \nfrom the client.\n\nFor example, a client could pipeline requests to a.com and b.com through a\nproxy. b.com might be fast and reply right away to the proxy, but the\nproxy can't forward the data to the client because a.com hasn't replied\nyet (and thus the response from b.com must be queued).  Even though the\nclient can maintain two connections to the proxy, this doesn't eliminate\nthe problem (although it does make it less likely).  The problem can also\nbecause worse in the case where b.com is cached by the proxy (or a delta\n[i.e., \"IM:\" header ] can be computed for b.com), but a.com is not cached.\n\nAdjusting HTTP so that responses can be returned back in a different order\nfrom requests would be one way to address the issue.  It would also be\nuseful in the case where two requests are made to the same server, and the\nsecond request can be serviced much sooner than the first request (e.g.,\nthe first request might be some sort of CGI request).  In the case of a\nsignificant amount of pipelining, the feature would also give a server the\nability to choose the best order in which to produce responses (e.g., long\nMP3s last).  I'm personally not a fan of \"enhancing\" protocols, so I'm\ninterested in any suggestions that people might have for working within\nthe current confines of HTTP.\n\nMD\n\n\n\n", "id": "lists-012-13930561"}, {"subject": "Cookie problem with IE5 and Windows 200", "content": "I created CGI applications a few years ago and set up a state management\nsystem using Cookies.  The application platform worked with all of the\ndifferent browsers, so long as they supported cookes.  I created the cookie\nheaders specifically in agreement with the Netscape specification,\nincluding putting spaces between the variables since it makes them easier\nto read.\n\nFor some reason, with IE5 and Windows 2000, the cookies no longer work.  I\nwrote software into the CGI to verify that the cookies were there, since\nthe design of the web site requires having some sort of state information\nabout the user.  Digging around on the internet via the search engines, I\nsee that other people are finding the same cookie problem, but I haven't\nseen anyone specifically figure out what the cause of the problem is or a\ngeneric solution.\n\nChecking Microsoft's web pages, their own cookie tools do not put spaces\nafter the set-cookie: header, so I rewrote everything to remove all spaces\nbetween variables in the cookie header, and yet the problem still persists.\n\nI have statements that the IE5/W2K platform is working with Amazon.com's\ncookies on the same machine, but not with mine.  I could probably compare\nthe cookies and reverse engine this particular foible of IE5 with W2K, but\nI'd rather not.  So I guess that Microsoft tested a few web sites and made\ntheir cookie handler work with them, but didn't write a generic handler.\n\nAnyone else run into these problems?  Anyone have a solution?  I can\nprovide source if anyone is interested.\n\nWilbur\n\n     --------------------------------------------\n        Putting A Human Face On Technology ;-)\n     --------------------------------------------\n  Literally!  http://www.monmouth.com/~wstreett/FaceIT/\n\n\n\n", "id": "lists-012-13939752"}, {"subject": "HTTP_REFERE", "content": "Hi all\n\nI have created a filter which reads the HTTP_REFERER field and writes it to \ndatabase.\nAs far as I know HTTP_REFERER is the server name where the current web page \nis hosted.\nIf I am asking for a page on my web server from a remote client ,\n( i.e ,I don't have any web page in my browser and just typed the URL),it is \ngiving the server name as Referer .\nCan anyone help me in this regard?Any usesful links are there for this?\n\nThanks in advance.\n\nAnupama\n\n________________________________________________________________________\nGet Your Private, Free E-mail from MSN Hotmail at http://www.hotmail.com\n\n\n\n", "id": "lists-012-13948529"}, {"subject": "local httpproxy with http in and https ou", "content": "Looking for a http-proxy that would run on Linux, accepts local http\nrequests and tunnels those as https through an ssl connection to web\nservers.\nRemember that this set-up was popular a few years ago when strong\nencryption was not available for the browsers outside the US.\nLooking for an open source version that runs on Linux.\n\nAny pointers/suggestions are appreciated.\nThanks, Frank.\n\n\n\n\n\n\n\n", "id": "lists-012-13955163"}, {"subject": "Re: Cookie problem with IE5 and Windows 200", "content": "At 10:34 AM 5/18/00 -0400, you wrote:\n<snip>\n\nI found the problem.  IE5 requires LOWER CASE domain names in the\nSet-Cookie: header or the cookies do not get set at all.\n\nI don't think that I'll say anything more.\n\nWilbur\n\n\n\n\n\n\n     --------------------------------------------\n        Putting A Human Face On Technology ;-)\n     --------------------------------------------\n  Literally!  http://www.monmouth.com/~wstreett/FaceIT/\n\n\n\n", "id": "lists-012-13961778"}, {"subject": "RE: comments on draft-ietf-http-authentication01.tx", "content": "> ----------\n> From: Dave Kristol[SMTP:dmk@bell-labs.com]\n> Sent: Friday, March 27, 1998 6:45 AM\n> To: Paul Leach\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: comments on draft-ietf-http-authentication-01.txt\n> \n> > >     What should a client do if it receives unrecognized attributes?\n> > >\n> > Ignore them. I thought that was the \"HTTP way\" and needn't be expliclty\n> > stated.\n> > \n> > >     What should a server do if it receives unrecognized attributes?\n> > >\n> > Ditto.\n> \n> I don't think it hurts to be explicit here.\n> \nOf course not. From your comment later on in your post, I inferred that you\nthought this was more than just a clarification.\n\n>   Furthermore, since I got\n> beat up by Yaron about stating explicitly what agents should do with\n> unrecognized attributes (namely, ignore) in RFC 2109, I feel obliged to\n> return the favor.\n> \nUnlike the Borg, we don't have a collective mind, and hence no collective\nguilt... :-)\n>  \n> > >     The term \"protection space\" gets used without a definition (here),\n> > >     but the spec. describes how a client can reuse credentials for\n> such\n> > >     a protection space.  I think we should say that the description of\n> > >     any auth-scheme must describe the rules for deciding when two\n> > >     objects are in the same protection space.  In particular, a client\n> > >     must be able to tell, so it knows whether or not to send\n> credentials\n> > >     unprompted.\n> > >\n> > Why does it need to tell? If it's wrong, by either sending incorrect\n> ones or\n> > not sending any, it'll get a 401 to tell it what to do. As far as I can\n> see,\n> > for Digest it's only an optimization. (For Basic, you don't want to send\n> > your credentials to the wrong place...)\n> \n> True enough.  But if you're right, why bother to define the notion of\n> \"protection space\" at all?\n> \nFor Basic, it's important. For Digest, it's not. That, and independent\nauthorship, are why the concept never showed up in the Digest section.\n\n> Since the spec. goes to the trouble of defining a protection space\n> (sect. 1.2), I think each type of authentication (including successors\n> to Basic and Digest) ought to state clearly what its protection space\n> is.\n> \nIf we have a chance to make editorial changes, then OK. But I am wieghing\neverything against the standard of \"does this mean that it is good enough to\npass Last Call or not\"? If it can pass Last Call and editorial changes can\nstill be made, I'm happy to make the changes you suggest.\n\nEven then, I think I'd call it the \"assumed protection space\" -- i.e. is\nwhat the client believes is protected by that set of credentials, until it\ndiscovers otherwise by either gettin a 401 on a URL it thought was in that\nsapce, or being prompted for credentials in the same realm for a URL that it\nthought wasn't in that space. Naming suggestions welcomed.\n\n> > \n> > > Sect. 3.2.1, The WWW-Authenticate Response Header\n> > >     [domain attribute]\n> > >     If this keyword is omitted or empty, the client should assume that\n> > >     the domain consists of all URIs on the responding server.\n> > >\n> > >       This behavior is different from Basic.  If we want Digest to be\n> > >       a more or less drop-in replacement, shouldn't the default\n> > >       behavior mimic Basic?\n> > >\n> > As you point out below, there are implementations. As I point out above,\n> it\n> > shouldn't matter. If I were writing a browser, I'd guess that I should\n> reuse\n> > the key obtained from a previous 401/WWW-Auth until I left the server --\n> > that way, I minimize the extra roundtrips.\n> \n> Okay.\n>  \n> > >       Furthermore, this paragraph constitutes the equivalent of a\n> > >       description of the Digest protection space, but it never calls\n> > >       it such.  It would be helpful to be more explicit.  (There's\n> > >       additional related information in section 3.3.)\n> > >\n> > I'm not sure it is.\n> \n> Sure it is.  The protection space is the set of URIs for which a given\n> set of credentials applies.  Certainly when there's an explicit \"domain\"\n> attribute it defines the protection space.  A missing \"domain\" attribute\n> implicitly sets the protection space to be \"the server\".\n> \nI meant to expand on that sentence, actually, but sent the reply before\ndoing so. The problem I have with the term \"protection space\" is that it\nimplies that anything outside the protection space is unprotected -- and\nthat's not true; it could be part of the protection space, the client just\ndoesn't know yet that his/her password is good in those other places.\n\n> > \n> > > Sect. 3.2.2, The Authorization Request Header\n> > >     [cnonce attribute]\n> > >     RFC 2069-compliant implementations might break upon receiving this\n> > >     new, previously unknown attribute.\n> > >\n> > Then they would be broken. Unknown directives are supposed to be\n> ignored.\n> \n> Where does it say that? :-)  Unrecognized headers should be ignored, but\n> even RFC 2068 is not explicit about unrecognized attributes (the word\n> used there).  Seriously, in the interests of increasing the likelihood\n> of interoperating implementations, I think it helps to say what you\n> mean.\n> \n> > > Sect. 3.2.3, The Authentication-Info Header\n> > >     What should a client do if the rspauth=response-digest information\n> > >     is wrong?\n> > >\n> > Not accept the response.\n> \n> How does a client, which has already read a response, \"not accept\n> [it]\"?  I'm picking nits here, true.  Does it mean that a browser would\n> show the user an error saying that the received response was in error?\n> \nThat's what I'd do. But we aren't supposed to prescribe UI behavior...\n>  \n> Or does it just stop spinning its logo and leave on the screen what was\n> already there?\n> \n> Suppose the client is a proxy.  What should it do vis-a-vis its client?\n> \nProxies do not posses enough info to check reponses. By design -- if they\ncould know it, that would mean that the protocol is insecure.\n\n> > >\n> > >     Isn't there the risk that an intervening proxy could change the\n> > >     status code?\n> > >       ... Authorization header for the request, A2 is\n> > >          A2       = Status-Code \":\" digest-uri-value\n> > >       and if \"qop=auth-int\", then A2 is\n> > >          A2       = Status-Code \":\" digest-uri-value \":\"\n> H(entity-body)\n> > >\n> > Well, the status code isn't a header, but there's a general proscription\n> > against needlessly changing headers in 13.5.2. Maybe the status line\n> > contents should be explicitly added to that list.\n> \n> Is it possible to say a proxy can't change its status code?  Suppose you\n> have browser B, caching proxy P, origin server S.  (I'm sure you'll tell\n> me if this example is way off base.)  B requests object X, which it does\n> not have in its local cache.  P has the object, but the object has\n> Cache-Control: must-revalidate.  P sends a *conditional* request to S. \n> After S asks for credentials, which response P passes to B, B asks again\n> for the X  S responds with 302 and (is this right?  possible?) an\n> Authentication-Info header.  The A-I header would presumably contain a\n> digest of the \"302\", but the proxy would return a 200 and supply X to B,\n> along with A-I.  B would be unable to match the A-I header and the\n> response and would assume the response is bogus.\n> \nNo, I think this is right on target (except it's 304 Not Modified, not 302).\nI think this is an important case to make work, for efficiency reasons. If I\nwere implementing an origin server, what I'd do, regardless of what the spec\nsays, is to calculate the response-digest assuming the proxy will turn the\nstatus code into 200. It violates the letter of the law but not the spirit.\nThe question that I can't figure out off the top of my head is: how well\nwould that work?\n\nThanks for the close read. \n\nLarry -- do we edit and resubmit for another Last Call? Or can we fix and\nsubmit for RFC?\n\nPaul\n\n\n\n", "id": "lists-012-1396353"}, {"subject": "Another revision of the ID on &quot;Delta encoding in HTTP&quot", "content": "About 2 months ago, Balachander Krishnamurthy posted (on this list)\na Last Call on a proposed extension to HTTP/1.1 to support\n\"delta encoding.\"  (More details are available in that message;\nsee http://www.ics.uci.edu/pub/ietf/http/hypermail/2000/0074.html)\nOur aim was to ask the IESG to advance this to Proposed Standard\nstatus.\n\nAt the time, we thought that we had resolved all of the important\nissues, but shortly thereafter we discovered some subtle flaws\nin the design, which were the result of trying too hard to use\nexisting HTTP mechanisms (specifically, the Content-Encoding\nand Accept-Encoding headers).  This resulted in several ambiguous\nsituations when combining delta encoding with existing HTTP/1.1\nfeatures.\n\nBala's Last Call is therefore retracted.\n\nOver the past two months, we've hashed out a modified design that\nseems to work (although, of course, that's what we originally\nthought about the last one).  In many ways, the modified design is\na lot more straightforward, but it does require the definition\nof some new HTTP headers.  We also had to add some details to\nprevent improper caching of delta-encoded responses.\n\nIn the process, we simplified the \"Delta encoding in HTTP\"\ndraft by removing some optional features (\"clustering\" and\n\"templates\") to a separate document, which is not yet ready\nfor release.  These features were raising complicated issues,\nespecially including potential security problems, and it\nseemed expedient to isolate them into a separate proposal.\n\nHere's an excerpt from the IETF announcement of the latest draft:\n\nTitle: Delta encoding in HTTP\nAuthor(s): J. Mogul, B. Krishnamurthy, F. Douglis,\n                          A. Feldmann, Y. Goland, A. van Hoff,  \n                          D. Hellerstein \nFilename: draft-mogul-http-delta-04.txt\nPages: 45\nDate: 19-May-00\n\n    Many HTTP requests cause the retrieval of slightly modified\n    instances of resources for which the client already has a\n    cache entry.  Research has shown that such modifying\n    updates are frequent, and that the modifications are\n    typically much smaller than the actual entity.  In such\n    cases, HTTP would make more efficient use of network\n    bandwidth if it could transfer a minimal description of the\n    changes, rather than the entire new instance of the\n    resource.  This is called 'delta encoding.'  This\n    document describes how delta encoding can be supported as a\n    compatible extension to HTTP/1.1.\n    \n    A URL for this Internet-Draft is:\n    http://www.ietf.org/internet-drafts/draft-mogul-http-delta-04.txt\n\nDiscussion of the delta-encoding specification takes place\non a separate mailing list; you can join by sending a request\nto <http-delta-request@pa.dec.com>.  We welcome comments from\npeople who have read the document.\n\n-Jeff\n\n\n\n", "id": "lists-012-13969208"}, {"subject": "Re: subscrib", "content": "06/20/2000 11:55 AM\nBrad Taylor@NEON\nBrad Taylor@NEON\nBrad Taylor@NEON\n06/20/2000 11:55 AM\n06/20/2000 11:55 AM\n\nWe are having a debate on browser behaviour and are trying to find what the\nofficial spec says.  What we see is that browsers (ie5.0 and netscape) when\n\"POST\"ing to our server construct a content-length and then append a 0d0a\nto the end.  This 0d0a is not contained in the content length.  Is this\naccording to \"SPEC\"?  If so would someone be so kind as to point it out (I\nam somewhat new to reading these specs).  Is this only going to occur for\nPOSTed data or will it also occur for other requests (GET, PUT,\netc.)....your consideration in advance is appreciated.\n\n\n\n", "id": "lists-012-13978948"}, {"subject": "CRLF on POST requests, where/how specified (repost from prior bad subject line", "content": "06/20/2000 11:57 AM\nBrad Taylor@NEON\nBrad Taylor@NEON\nBrad Taylor@NEON\n06/20/2000 11:57 AM\n06/20/2000 11:57 AM\n\nWe are having a debate on browser behaviour and are trying to find what the\nofficial spec says.  What we see is that browsers (ie5.0 and netscape) when\n\"POST\"ing to our server construct a content-length and then append a 0d0a\nto the end.  This 0d0a is not contained in the content length.  Is this\naccording to \"SPEC\"?  If so would someone be so kind as to point it out (I\nam somewhat new to reading these specs).  Is this only going to occur for\nPOSTed data or will it also occur for other requests (GET, PUT,\netc.)....your consideration in advance is appreciated.\n\n\n\n", "id": "lists-012-13985910"}, {"subject": "RE: CRLF on POST requests, where/how specified (repost from prior bad subject line", "content": "See RFC 2616 ( http://www.innosoft.com/rfc/rfc2616.html#sec-4.1 )\n\nQuoting:\n\n----\nRequest (section 5) and Response (section 6) messages use the\ngeneric message format of RFC 822 [9] for transferring entities (the\npayload of the message). Both types of message consist of a\nstart-line, zero or more header fields (also known as \"headers\"), an\nempty line (i.e., a line with nothing preceding the CRLF) indicating\nthe end of the header fields, and possibly a message-body.\n\n\n        generic-message = start-line\n                          *(message-header CRLF)\n                          CRLF\n                          [ message-body ]\n----\nThe CRLF that separates the headers from the body is not part of\neither.  Since it is not part of the body, it is not included in the\ncontent length.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-13994426"}, {"subject": "RE: CRLF on POST requests, where/how specified (repost from prior bad subject line", "content": "Fred Bohle@NEON\n06/20/2000 12:51 PM\n\nScott,\n\n     No, not the CRLF between the headers and the body.  We are asking\nabout a\nCRLF that follows the body.  It does not seem to be in the 1.0 spec, and\nthe 1.1 spec\nseems to specifically prohibit it.  And yet, IE5 and Netscape both will\nsend a CRLF\nafter the body for Content-type: application/x-www-url-encoded.\n\n     What do other web servers do to handle this CRLF?  We find that if we\ndecide to\nclose the connection with the end of the response we generate, (Connection:\nclose)\nand THEN the CRLF arrives from the client, the TCP layer will generate a\nReset packet.\nThis causes (IE5 at least) the client to fail processing the response we\njust sent.  So the\napplication stops, dead in the water.\n\n\n\nFred Bohle\n\n\n\n\n\n\"Scott Lawrence\" <lawrence@agranat.com> on 06/20/2000 12:36:45 PM\n\nTo:   Brad Taylor/Neon, http-wg@cuckoo.hpl.hp.com\ncc:    (bcc: Fred Bohle/Dev/Neon)\n\nSubject:  RE: CRLF on POST requests, where/how specified (repost from prior\n      bad subject line)\n\n\n\n\n\nSee RFC 2616 ( http://www.innosoft.com/rfc/rfc2616.html#sec-4.1 )\n\nQuoting:\n\n----\nRequest (section 5) and Response (section 6) messages use the\ngeneric message format of RFC 822 [9] for transferring entities (the\npayload of the message). Both types of message consist of a\nstart-line, zero or more header fields (also known as \"headers\"), an\nempty line (i.e., a line with nothing preceding the CRLF) indicating\nthe end of the header fields, and possibly a message-body.\n\n\n        generic-message = start-line\n                          *(message-header CRLF)\n                          CRLF\n                          [ message-body ]\n----\nThe CRLF that separates the headers from the body is not part of\neither.  Since it is not part of the body, it is not included in the\ncontent length.\n\n--\nScott Lawrence      Director of R & D        <lawrence@agranat.com>\nAgranat Systems   Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-14003953"}, {"subject": "Re: CRLF on POST requests, where/how specified (repost from prior bad subject line", "content": "\"Brad Taylor\" <btaylor@neonsys.com> wrote:\n  > \n  > \n  > \n  > 06/20/2000 11:57 AM\n  > Brad Taylor@NEON\n  > Brad Taylor@NEON\n  > Brad Taylor@NEON\n  > 06/20/2000 11:57 AM\n  > 06/20/2000 11:57 AM\n  > \n  > We are having a debate on browser behaviour and are trying to find what the\n  > official spec says.  What we see is that browsers (ie5.0 and netscape) when\n  > \"POST\"ing to our server construct a content-length and then append a 0d0a\n  > to the end.  This 0d0a is not contained in the content length.  Is this\n  > according to \"SPEC\"?  If so would someone be so kind as to point it out (I\n  > am somewhat new to reading these specs).  Is this only going to occur for\n  > POSTed data or will it also occur for other requests (GET, PUT,\n  > etc.)....your consideration in advance is appreciated.\n\nIIRC, early in HTTP/1.1 development there were servers that balked if\nthey received the message body associated with POST but the body didn't\nend with a CRLF.  Or maybe it was that some clients erroneously sent\nCRLF after every POST body.  Either way, RFC 2616 added the following\nwords (Sect. 4.1):\n\n   In the interest of robustness, servers SHOULD ignore any empty\n   line(s) received where a Request-Line is expected. In other words, if\n   the server is reading the protocol stream at the beginning of a\n   message and receives a CRLF first, it should ignore the CRLF.\n\nThe idea was that if there were pipelined requests, or multiple requests\non a persistent connection, the server should eat the extra CRLFs.\n\nDave Kristol\n\n\n\n", "id": "lists-012-14015029"}, {"subject": "Re: CRLF on POST requests, where/how specified (repost from prior bad subject line", "content": "On Tue, Jun 20, 2000 at 12:51:11PM -0500, Fred Bohle wrote:\n> \n>      No, not the CRLF between the headers and the body.  We are asking\n>      about a CRLF that follows the body.  It does not seem to be in\n>      the 1.0 spec, and the 1.1 spec seems to specifically prohibit it.\n>      And yet, IE5 and Netscape both will send a CRLF after the body\n>      for Content-type: application/x-www-url-encoded.\n\nThe spec clearly disallows this extra CRLF, i.e. it's a bug in those\nbrowsers. However, there is an easy workaround which many people use:\nwhen reading the request line, ignore all whitespace (or empty lines)\nuntil you hit the actual request line.\n\n>      What do other web servers do to handle this CRLF?  We find that\n>      if we decide to close the connection with the end of the response\n>      we generate, (Connection: close) and THEN the CRLF arrives from\n>      the client, the TCP layer will generate a Reset packet.  This\n>      causes (IE5 at least) the client to fail processing the response\n>      we just sent.  So the application stops, dead in the water.\n\nThis is a little tricky, but not much. Basically you need to do a\nshutdown on the socket, i.e. only close for sends; then you keep\nreading from the socket (and just discard the data) until either you get\nan EOF (in which case the client did the close), or until some timeout\n(a few seconds) in case the client doesn't close the connection. Note\nthat this problem is a general problem (i.e. not limited to the extra\nCRLF), because you can get the same effect when a client pipelines\nrequests.\n\n\n  Cheers,\n\n  Ronald\n\n\nP.S. take a look at the Apache source: read_request_line() in http_protocol.c\n     for the first problem, and lingering_close() in http_main.c for\n     the second - the comments are enlightening.\n\n\n\n", "id": "lists-012-14024828"}, {"subject": "apache source included....Re: CRLF on POST requests, where/how specified (repost from prior bad subject line", "content": "06/20/2000 02:08 PM\nBrad Taylor@NEON\nBrad Taylor@NEON\nBrad Taylor@NEON\n06/20/2000 02:08 PM\n06/20/2000 02:08 PM\n\n\n\n/*\n * More machine-dependent networking gooo... on some systems,\n * you've got to be *really* sure that all the packets are acknowledged\n * before closing the connection, since the client will not be able\n * to see the last response if their TCP buffer is flushed by a RST\n * packet from us, which is what the server's TCP stack will send\n * if it receives any request data after closing the connection.\n *\n * In an ideal world, this function would be accomplished by simply\n * setting the socket option SO_LINGER and handling it within the\n * server's TCP stack while the process continues on to the next request.\n * Unfortunately, it seems that most (if not all) operating systems\n * block the server process on close() when SO_LINGER is used.\n * For those that don't, see USE_SO_LINGER below.  For the rest,\n * we have created a home-brew lingering_close.\n *\n * Many operating systems tend to block, puke, or otherwise mishandle\n * calls to shutdown only half of the connection.  You should define\n * NO_LINGCLOSE in ap_config.h if such is the case for your system.\n */\n#ifndef MAX_SECS_TO_LINGER\n#define MAX_SECS_TO_LINGER 30\n#ifndef NO_LINGCLOSE\n\n/* Special version of timeout for lingering_close */\n\nstatic void lingerout(int sig)\n{\n#ifdef NETWARE\n    get_tsd\n#endif\n    if (alarms_blocked) {\n     alarm_pending = 1;\n     return;\n    }\n\n    if (!current_conn) {\n     ap_longjmp(jmpbuffer, 1);\n    }\n    ap_bsetflag(current_conn->client, B_EOUT, 1);\n    current_conn->aborted = 1;\n}\n\nstatic void linger_timeout(void)\n{\n#ifdef NETWARE\n    get_tsd\n#endif\n    timeout_name = \"lingering close\";\n    ap_set_callback_and_alarm(lingerout, MAX_SECS_TO_LINGER);\n}\n\n/* Since many clients will abort a connection instead of closing it,\n * attempting to log an error message from this routine will only\n * confuse the webmaster.  There doesn't seem to be any portable way to\n * distinguish between a dropped connection and something that might be\n * worth logging.\n */\nstatic void lingering_close(request_rec *r)\n{\n    char dummybuf[512];\n    struct timeval tv;\n    fd_set lfds;\n    int select_rv;\n    int lsd;\n\n    /* Prevent a slow-drip client from holding us here indefinitely */\n\n    linger_timeout();\n\n    /* Send any leftover data to the client, but never try to again */\n\n    if (ap_bflush(r->connection->client) == -1) {\n     ap_kill_timeout(r);\n     ap_bclose(r->connection->client);\n     return;\n    }\n    ap_bsetflag(r->connection->client, B_EOUT, 1);\n\n    /* Close our half of the connection --- send the client a FIN */\n\n    lsd = r->connection->client->fd;\n\n    if ((shutdown(lsd, 1) != 0) || r->connection->aborted) {\n     ap_kill_timeout(r);\n     ap_bclose(r->connection->client);\n     return;\n    }\n\n    /* Set up to wait for readable data on socket... */\n\n    FD_ZERO(&lfds);\n\n    /* Wait for readable data or error condition on socket;\n     * slurp up any data that arrives...  We exit when we go for an\n     * interval of tv length without getting any more data, get an error\n     * from select(), get an error or EOF on a read, or the timer expires.\n     */\n\n    do {\n     /* We use a 2 second timeout because current (Feb 97) browsers\n      * fail to close a connection after the server closes it.  Thus,\n      * to avoid keeping the child busy, we are only lingering long enough\n      * for a client that is actively sending data on a connection.\n      * This should be sufficient unless the connection is massively\n      * losing packets, in which case we might have missed the RST anyway.\n      * These parameters are reset on each pass, since they might be\n      * changed by select.\n      */\n#ifdef NETWARE\n        ThreadSwitch();\n#endif\n\n     FD_SET(lsd, &lfds);\n     tv.tv_sec = 2;\n     tv.tv_usec = 0;\n\n     select_rv = ap_select(lsd + 1, &lfds, NULL, NULL, &tv);\n\n    } while ((select_rv > 0) &&\n             (read(lsd, dummybuf, sizeof dummybuf) > 0));\n\n    /* Should now have seen final ack.  Safe to finally kill socket */\n\n    ap_bclose(r->connection->client);\n\n    ap_kill_timeout(r);\n}\n#endif /* ndef NO_LINGCLOSE */\n\n\n\n\n\"Life is hard, and then you die\" <ronald@innovation.ch> on 06/20/2000\n01:21:07 PM\n\nTo:   Fred Bohle/Dev/Neon\ncc:   http-wg@cuckoo.hpl.hp.com (bcc: Brad Taylor/Neon)\n\nSubject:  Re: CRLF on POST requests, where/how specified (repost from prior\n      bad subject line)\n\n\n\n\nOn Tue, Jun 20, 2000 at 12:51:11PM -0500, Fred Bohle wrote:\n>\n>      No, not the CRLF between the headers and the body.  We are asking\n>      about a CRLF that follows the body.  It does not seem to be in\n>      the 1.0 spec, and the 1.1 spec seems to specifically prohibit it.\n>      And yet, IE5 and Netscape both will send a CRLF after the body\n>      for Content-type: application/x-www-url-encoded.\n\nThe spec clearly disallows this extra CRLF, i.e. it's a bug in those\nbrowsers. However, there is an easy workaround which many people use:\nwhen reading the request line, ignore all whitespace (or empty lines)\nuntil you hit the actual request line.\n\n>      What do other web servers do to handle this CRLF?  We find that\n>      if we decide to close the connection with the end of the response\n>      we generate, (Connection: close) and THEN the CRLF arrives from\n>      the client, the TCP layer will generate a Reset packet.  This\n>      causes (IE5 at least) the client to fail processing the response\n>      we just sent.  So the application stops, dead in the water.\n\nThis is a little tricky, but not much. Basically you need to do a\nshutdown on the socket, i.e. only close for sends; then you keep\nreading from the socket (and just discard the data) until either you get\nan EOF (in which case the client did the close), or until some timeout\n(a few seconds) in case the client doesn't close the connection. Note\nthat this problem is a general problem (i.e. not limited to the extra\nCRLF), because you can get the same effect when a client pipelines\nrequests.\n\n\n  Cheers,\n\n  Ronald\n\n\nP.S. take a look at the Apache source: read_request_line() in\nhttp_protocol.c\n     for the first problem, and lingering_close() in http_main.c for\n     the second - the comments are enlightening.\n\n\n\n", "id": "lists-012-14034799"}, {"subject": "TLS upgrade implementations", "content": "Has anybody got a server which implements any of RFC2817? I have a\nclient which will send Upgrade but nothing to test against...\n\nRegards,\n\njoe\n\n\n\n", "id": "lists-012-14049787"}, {"subject": "Combining multiple messageheader field", "content": "Hi,\n\nRFC2616 says:\n\n| 4.2 Message Headers\n| [...]\n|    Multiple message-header fields with the same field-name MAY be\n|    present in a message if and only if the entire field-value for that\n|    header field is defined as a comma-separated list [i.e., #(values)].\n|    It MUST be possible to combine the multiple header fields into one\n|    \"field-name: field-value\" pair, without changing the semantics of the\n|    message, by appending each subsequent field-value to the first, each\n|    separated by a comma. The order in which header fields with the same\n|    field-name are received is therefore significant to the\n|    interpretation of the combined field value, and thus a proxy MUST NOT\n|    change the order of these field values when a message is forwarded.\n\nI'm building a HTTP UserAgent with PHP and now i'm dealing with this multiple\nmessage-header fields. If my UserAgent receives such multiple fields, should\nit combine these fields into one or should it keep the multiple fields and\nsend it to the server (if present in a request)? The problem behind this is\nthe storage of this header fields. Currently i have an associative array that\ndoes something like\n\nAccept => array(text/html, image/*)\n\nif there was\n\nAccept: text/html\nAccept: image/*\n\nbut when someone wants the value of Accept i only want to return a string and\ntherefor i only return the first value i.e. text/html which does not meet\nsomeones expactation.\n\nI appreciate your help,\nregards,\n  Bj?rn H?hrmann\n--\n\"Jede  Stra?e,  der  man konsequent \\ und du wirst ihn in G?nze sehen.\nbis zu ihrem Ende folgt,  f?hrt un- ) Stehst Du auf seinem Gipfel wird\nweigerlich  ins  Nichts.   Erklimme ( er  f?r  Dich  unsichtbar\" -- P.\neinen  Berg nur ein kleines  St?ck, \\ Irulan <http://bjoernsworld.de/>\n\n\n\n", "id": "lists-012-14056274"}, {"subject": "Re: Combining multiple messageheader field", "content": "On Sat, Jul 15, 2000 at 05:44:40PM +0200, Bjoern Hoehrmann wrote:\n[snip]\n> I'm building a HTTP UserAgent with PHP and now i'm dealing with this multiple\n> message-header fields. If my UserAgent receives such multiple fields, should\n> it combine these fields into one or should it keep the multiple fields and\n> send it to the server (if present in a request)? The problem behind this is\n> the storage of this header fields. Currently i have an associative array that\n> does something like\n[snip]\n\nCombining multiple header fields works just fine, with one exception: the\nSet-Cookie header field. Some browsers don't handle multiple cookies in a\nSet-Cookie header field (i.e. they require a separate header field for each\ncookie to be set). Furthermore, because a number of sites use broken cookies\nwhich contain commas in the value, parsing a combined Set-Cookie header field\ncan be a little tricky.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-14064998"}, {"subject": "Re: Combining multiple messageheader field", "content": "* Life is hard, and then you die <ronald@innovation.ch> wrote:\n| Combining multiple header fields works just fine, with one exception: the\n| Set-Cookie header field. Some browsers don't handle multiple cookies in a\n| Set-Cookie header field (i.e. they require a separate header field for each\n| cookie to be set).\n\nOk, good to know, thanks.\n\nregards,\n  Bj?rn H?hrmann\n\n\n\n", "id": "lists-012-14073511"}, {"subject": "Questions (errata?) about caching authenticated response", "content": "I've been reading RFCs 2616 and 2617 about caching authenticated\nresponses, and have possibly found some inconsistencies.\n\n#1.     The very last sentence of Sec 14.9.4 (under proxy-revalidate)\nsays: ``...such authenticated responses also need the public\ncache control directive in order to allow them to be cached at\nall''\n\nYet, Sec 14.8 lists three cache-control directives that allow a\nshared cache to reuse an authenticatd response: s-maxage,\nmust-revalidate, and public.\n\n#2.If must-revalidate alone is enough to allow an authenticated\nresponse to be cached, and if proxy-revalidate is the same\nas must-revalidate for a shared cache, is proxy-revalidate\nalone enough to allow an authenticated response to be cached?\n\nIf so, should proxy-revalidate be listed in section 14.8?\n\n#3.RFC 2617, Sec 3.2.2.5 says:\n\n    when a shared cache ... has received a request containing\n    an Authorization header and a response from relaying that\n    request, it MUST NOT return that response as a reply to any\n    other request, unless one of two Cache-Control (see section\n    14.9 of [RFC2616]) directives was present in the response.\n\nI believe this is referring to section 14.8, rather than 14.9,\nand \"two\" is not the right number?\n\nFinally, Sec 14.8 doesn't mention if a non-shared cache needs to treat\nan authenticated response specially.  I assume that a non-shared\ncache can store and reuse an authenticated response by default.\nShould that be made explicit?\n\nDuane W.\n\n\n\n", "id": "lists-012-14081229"}, {"subject": "RE: Questions (errata?) about caching authenticated response", "content": "The best solution for maximum security whould be:\n\nAuthenticated request\n=====================\nShared-Cache\nDo NOT cache the response, because it requires uses to authenticate, and may\nnot be accessed by everyone.\n\nPrivate-Cache\nA private-cache is used by ONLY ONE PERSON. This cache may cache the\nresponse (depending on the cache-control header), because it can only be\naccessed by one person.\n\n\n\n- Joris Dobbelsteen\n\n\n> -----Original Message-----\n> From: Duane Wessels [mailto:wessels@ircache.net]\n> Sent: donderdag 20 juli 2000 7:48\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Questions (errata?) about caching authenticated responses\n>\n>\n> I've been reading RFCs 2616 and 2617 about caching authenticated\n> responses, and have possibly found some inconsistencies.\n>\n> #1.     The very last sentence of Sec 14.9.4 (under proxy-revalidate)\n> says: ``...such authenticated responses also need the public\n> cache control directive in order to allow them to be cached at\n> all''\n>\n> Yet, Sec 14.8 lists three cache-control directives that allow a\n> shared cache to reuse an authenticatd response: s-maxage,\n> must-revalidate, and public.\n>\n> #2.If must-revalidate alone is enough to allow an authenticated\n> response to be cached, and if proxy-revalidate is the same\n> as must-revalidate for a shared cache, is proxy-revalidate\n> alone enough to allow an authenticated response to be cached?\n>\n> If so, should proxy-revalidate be listed in section 14.8?\n>\n> #3.RFC 2617, Sec 3.2.2.5 says:\n>\n>     when a shared cache ... has received a request containing\n>     an Authorization header and a response from relaying that\n>     request, it MUST NOT return that response as a reply to any\n>     other request, unless one of two Cache-Control (see section\n>     14.9 of [RFC2616]) directives was present in the response.\n>\n> I believe this is referring to section 14.8, rather than 14.9,\n> and \"two\" is not the right number?\n>\n> Finally, Sec 14.8 doesn't mention if a non-shared cache needs to treat\n> an authenticated response specially.  I assume that a non-shared\n> cache can store and reuse an authenticated response by default.\n> Should that be made explicit?\n>\n> Duane W.\n>\n>\n>\n>\n\n\n\n", "id": "lists-012-14090244"}, {"subject": "Downline U.S.A.", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nDownline U.S.A.!\n\nJoin a new downline club.  \nStraight line forced matrix.  \nCheck out this new opportunity!\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14100739"}, {"subject": "Downline U.S.A.", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nDownline U.S.A.!\n\nJoin a new downline club.  \nStraight line forced matrix.  \nCheck out this new opportunity!\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14108753"}, {"subject": "Unlimited Free Long Distance", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nUnlimited Free Long Distance!\n\nThe second offer in this message is for for Free Unlimited Long Distance.  \n\nI want to introduce you to a FREE site, TargetShop.com, that lets you earn\nTHOUSANDS OF DOLLARS just for referring your friends! (Earn up to $15,000)\n\nGo to TargetShop.com\nhttp://www.targetshop.com/Users/Level1.asp?refId=1026354 and register for\nMembership.  Be sure to use my Referral Number 1026354.  \n\nAfter you register, you can also earn easy money by referring others.\nTargetShop will pay you $37.50 per referral. Each time they refer someone,\nyou will also make money!  You can make $15,000 in no time at all! $15,000\nis the limit. ( check the website for more details)\n\nAs TargetShop.com Members, you also get discounts at select e-commerce\nsites using the TargetPoints you get when you sign up and take advantage\nof site features like the Playground. So, go sign up, and enjoy your\nMembership http://www.targetshop.com/Users/Level1.asp?refId=1026354\n\nGet Free Unlimited Long Distance\n\nDoes UNLIMITED FREE LONG DISTANCE sound good to you?  Say hello to Pagoo\nInternet Phone Number, the first real Internet phone company.   Get a\nPagoo Internet Phone Number during their Preview Release and use your PC\nwhile online to: \n\n- Make calls -including unlimited FREE long distance in the U.S\n- Answer calls made from any phone in the world\n- Get FREE voicemail & Caller ID. \n\nNO credit cards required.\nNO changes to your phone service.\nClick Here:  http://www.onresponse.com/onR_Ads.asp?a=15308&d=766\n\nGet $20 and 3 months Free Bill Paying Service\nhttp://www.commission-junction.com/track/track.dll?AID=444713&PID=294821&URL=https%3A%2F%2Fads%2Ecybergold%2Ecom%2Fdo%2Fcap%2FaepCJ%2Fcapcj015%2Fe\n\nEarn $20 per referral.  Open an online account with compubank and earn $40\nper referrral.\nCompuBank offers an excellent online banking option for the 21st Century.\nhttp://www.compubank.com/index.cfm?affinity=19039 \n\nJobsOnline is your online resource for employment and careerinformation. \nPost a resume, review salary information for thousands of jobs,& take a\njob aptitude test - just to name a few of the services. All absolutely\nFREE. \nRegister today and receive 30 minutes of long distance FREE.\nClick Here: http://www.onResponse.com/onr_ads.asp?a=15308&d=201\n\nNeed a Credit Card? Two Options\nNO Upfront Cash Security Deposits and NO Turndowns! Divorce - Bad\nCredit/No Credit? No Problem! Get your FREE guaranteed credit card today!!!\nClick Here: http://www.onresponse.com/onR_Ads.asp?a=15308&d=525 \n\nDesign your own Visa ONLINE from NextCard Visa With Low 2.9% Intro Rate\nClick Here: http://www.onResponse.com/onr_ads.asp?a=15308&d=175 \n\nGet Paid up to $10 per Ad! That's $30 per hour\nhttp://www.adsenger.com/signup.asp?ref=bjgibbs\n\nBest Brand New Paid to Read-BrowserClub(pays up to 7 levels)\nhttp://www.browserclub.com/GetPaid.asp?Ref=bjgibbs \n\nGet $2 when you join SearchCactus (paying now)\nhttp://www.searchcactus.com/member/welcome.asp?6568 \n\nIf you need a Free place to advertise, join our club Money Making\nOpportunites at globeclubs.\nMembership is growing everyday.\nhttp://globeclubs.theglobe.com/subscribe.taf?list=24958 \n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14116713"}, {"subject": "Unlimited Free Long Distance", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nUnlimited Free Long Distance!\n\nThe second offer in this message is for for Free Unlimited Long Distance.  \n\nI want to introduce you to a FREE site, TargetShop.com, that lets you earn\nTHOUSANDS OF DOLLARS just for referring your friends! (Earn up to $15,000)\n\nGo to TargetShop.com\nhttp://www.targetshop.com/Users/Level1.asp?refId=1026354 and register for\nMembership.  Be sure to use my Referral Number 1026354.  \n\nAfter you register, you can also earn easy money by referring others.\nTargetShop will pay you $37.50 per referral. Each time they refer someone,\nyou will also make money!  You can make $15,000 in no time at all! $15,000\nis the limit. ( check the website for more details)\n\nAs TargetShop.com Members, you also get discounts at select e-commerce\nsites using the TargetPoints you get when you sign up and take advantage\nof site features like the Playground. So, go sign up, and enjoy your\nMembership http://www.targetshop.com/Users/Level1.asp?refId=1026354\n\nGet Free Unlimited Long Distance\n\nDoes UNLIMITED FREE LONG DISTANCE sound good to you?  Say hello to Pagoo\nInternet Phone Number, the first real Internet phone company.   Get a\nPagoo Internet Phone Number during their Preview Release and use your PC\nwhile online to: \n\n- Make calls -including unlimited FREE long distance in the U.S\n- Answer calls made from any phone in the world\n- Get FREE voicemail & Caller ID. \n\nNO credit cards required.\nNO changes to your phone service.\nClick Here:  http://www.onresponse.com/onR_Ads.asp?a=15308&d=766\n\nGet $20 and 3 months Free Bill Paying Service\nhttp://www.commission-junction.com/track/track.dll?AID=444713&PID=294821&URL=https%3A%2F%2Fads%2Ecybergold%2Ecom%2Fdo%2Fcap%2FaepCJ%2Fcapcj015%2Fe\n\nEarn $20 per referral.  Open an online account with compubank and earn $40\nper referrral.\nCompuBank offers an excellent online banking option for the 21st Century.\nhttp://www.compubank.com/index.cfm?affinity=19039 \n\nJobsOnline is your online resource for employment and careerinformation. \nPost a resume, review salary information for thousands of jobs,& take a\njob aptitude test - just to name a few of the services. All absolutely\nFREE. \nRegister today and receive 30 minutes of long distance FREE.\nClick Here: http://www.onResponse.com/onr_ads.asp?a=15308&d=201\n\nNeed a Credit Card? Two Options\nNO Upfront Cash Security Deposits and NO Turndowns! Divorce - Bad\nCredit/No Credit? No Problem! Get your FREE guaranteed credit card today!!!\nClick Here: http://www.onresponse.com/onR_Ads.asp?a=15308&d=525 \n\nDesign your own Visa ONLINE from NextCard Visa With Low 2.9% Intro Rate\nClick Here: http://www.onResponse.com/onr_ads.asp?a=15308&d=175 \n\nGet Paid up to $10 per Ad! That's $30 per hour\nhttp://www.adsenger.com/signup.asp?ref=bjgibbs\n\nBest Brand New Paid to Read-BrowserClub(pays up to 7 levels)\nhttp://www.browserclub.com/GetPaid.asp?Ref=bjgibbs \n\nGet $2 when you join SearchCactus (paying now)\nhttp://www.searchcactus.com/member/welcome.asp?6568 \n\nIf you need a Free place to advertise, join our club Money Making\nOpportunites at globeclubs.\nMembership is growing everyday.\nhttp://globeclubs.theglobe.com/subscribe.taf?list=24958 \n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14128813"}, {"subject": "RE: comments on draft-ietf-http-authentication01.tx", "content": "> Larry -- do we edit and resubmit for another Last Call? Or can we fix and\n> submit for RFC?\n>\n> Paul\n\nI'll answer for Larry here, though he's the ultimate boss on process\nquestions.\n\nEdits recieved during a last call should be folded back in (if significant)\nand a new draft issued.  I don't believe another working group last call\nneed be issued unless there are substantive changes.\n\nIf there are any errors introduced (or further arguments), they can\nbe handled during IETF last call.\n\nNote that interoperability reports might affect a draft; for draft standard,\nthe requirement is 2 interoperable implementations for each feature of\nthe protocol.\n\nFor the base HTTP spec, my current thoughts are to go over the\ninteroperability reports with Henrik sometime in a week or two (by mail,\nas Henrik is a world traveller for most of the next month), and see\nif there are going to be any features we have to either get implemented,\nor drop from the HTTP spec.  I think we're in pretty good shape, but\nwon't be fully confident until more are in and we've checked off each\nfeature.  Once I see if anything needs to be dropped, I'll issue a base\nHTTP spec with whatever editorial work needs to be done from comments.\n- Jim\n\n\n\n", "id": "lists-012-1413862"}, {"subject": "Re: Questions (errata?) about caching authenticated response", "content": "I think the point here is that maximum 'security' is not always the goal;\nsometimes, all that's needed is trivial authentication (which is all that\ncan really be expected in any case), and cacheability of the objects due to\n\nCheers,\n\n\nOn Sat, Jul 22, 2000 at 04:05:31PM +0200, Joris Dobbelsteen wrote:\n> The best solution for maximum security whould be:\n> \n> Authenticated request\n> =====================\n> Shared-Cache\n> Do NOT cache the response, because it requires uses to authenticate, and may\n> not be accessed by everyone.\n> \n> Private-Cache\n> A private-cache is used by ONLY ONE PERSON. This cache may cache the\n> response (depending on the cache-control header), because it can only be\n> accessed by one person.\n> \n> \n> \n> - Joris Dobbelsteen\n> \n> \n> > -----Original Message-----\n> > From: Duane Wessels [mailto:wessels@ircache.net]\n> > Sent: donderdag 20 juli 2000 7:48\n> > To: http-wg@cuckoo.hpl.hp.com\n> > Subject: Questions (errata?) about caching authenticated responses\n> >\n> >\n> > I've been reading RFCs 2616 and 2617 about caching authenticated\n> > responses, and have possibly found some inconsistencies.\n> >\n> > #1.     The very last sentence of Sec 14.9.4 (under proxy-revalidate)\n> > says: ``...such authenticated responses also need the public\n> > cache control directive in order to allow them to be cached at\n> > all''\n> >\n> > Yet, Sec 14.8 lists three cache-control directives that allow a\n> > shared cache to reuse an authenticatd response: s-maxage,\n> > must-revalidate, and public.\n> >\n> > #2.If must-revalidate alone is enough to allow an authenticated\n> > response to be cached, and if proxy-revalidate is the same\n> > as must-revalidate for a shared cache, is proxy-revalidate\n> > alone enough to allow an authenticated response to be cached?\n> >\n> > If so, should proxy-revalidate be listed in section 14.8?\n> >\n> > #3.RFC 2617, Sec 3.2.2.5 says:\n> >\n> >     when a shared cache ... has received a request containing\n> >     an Authorization header and a response from relaying that\n> >     request, it MUST NOT return that response as a reply to any\n> >     other request, unless one of two Cache-Control (see section\n> >     14.9 of [RFC2616]) directives was present in the response.\n> >\n> > I believe this is referring to section 14.8, rather than 14.9,\n> > and \"two\" is not the right number?\n> >\n> > Finally, Sec 14.8 doesn't mention if a non-shared cache needs to treat\n> > an authenticated response specially.  I assume that a non-shared\n> > cache can store and reuse an authenticated response by default.\n> > Should that be made explicit?\n> >\n> > Duane W.\n> >\n> >\n> >\n> >\n> \n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-14140982"}, {"subject": "I've Developed The Perfect Hi-Tech, HiTouch, Home Busines", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\n\"After $4.5 Million In Sales, I've Developed The Perfect\n  Hi-Tech, Hi-Touch, Home Business That Guarantees Success\"\n\n  World record holder Gary Shawkey has spent the past 18 years\n  developing a UNIQUE business opportunity for financial success\n  that opens up TRUE wealth creation with the perfect biz model.\n\n  By following Gary's PROVEN WINNING system you can:\n\n  * Learn how to earn one of the BIGGEST incomes this\n    industry has EVER seen, marketing products that\n    virtually sell themselves online, day and night.\n\n  * Create reoccurring pay checks that come in month\n    after month, AUTOMATICALLY, without the need for\n    current business skills or any experience at all.\n\n  * Learn how we can FUND a down payment on a new\n    home, get that new car you've always wanted, and\n    create a HAPPY and SECURE future for you and\n    your family, without having to work any harder.\n\n  * Learn how to receive bonus checks of up to TEN\n    thousand dollars and more, simply by referring\n    others to use our LIFE CHANGING products.\n\n  This is a REAL opportunity for people like YOU who\n  desire success. It doesn't matter whether you have\n  little or no experience, we GUARANTEE that our\n  unique business opportunity will allow you to PROFIT\n  simply by following our PROVEN business model.\n\n  Using our \"Real Kicker Autoship\" program, we're\n  enabling you to earn rewards from multiple levels\n  of income, providing truly profitable leverage.\n\n  The result is more CASH for YOU, plus a wealth of\n  UNBELIEVABLE bonuses, delivered through the power\n  of our unique business structure. It's truly amazing.\n\n  For full and exciting information, go now to:\n\n  >>> http://BodiesBest.com/go.php?id=783 <<<\n   \n  FREE info by e mail: mailto:bodiesbestposition783@getresponse.com\n\n  P.S Act TODAY and we'll waive the setup fees! But\n      hurry, this offer only remains valid TODAY.\n\n  P.P.S Microsoft, Coca-Cola, and major TV stations\n      are BEGGING to use our business strategies.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14152348"}, {"subject": "I've Developed The Perfect Hi-Tech, HiTouch, Home Busines", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\n\"After $4.5 Million In Sales, I've Developed The Perfect\n  Hi-Tech, Hi-Touch, Home Business That Guarantees Success\"\n\n  World record holder Gary Shawkey has spent the past 18 years\n  developing a UNIQUE business opportunity for financial success\n  that opens up TRUE wealth creation with the perfect biz model.\n\n  By following Gary's PROVEN WINNING system you can:\n\n  * Learn how to earn one of the BIGGEST incomes this\n    industry has EVER seen, marketing products that\n    virtually sell themselves online, day and night.\n\n  * Create reoccurring pay checks that come in month\n    after month, AUTOMATICALLY, without the need for\n    current business skills or any experience at all.\n\n  * Learn how we can FUND a down payment on a new\n    home, get that new car you've always wanted, and\n    create a HAPPY and SECURE future for you and\n    your family, without having to work any harder.\n\n  * Learn how to receive bonus checks of up to TEN\n    thousand dollars and more, simply by referring\n    others to use our LIFE CHANGING products.\n\n  This is a REAL opportunity for people like YOU who\n  desire success. It doesn't matter whether you have\n  little or no experience, we GUARANTEE that our\n  unique business opportunity will allow you to PROFIT\n  simply by following our PROVEN business model.\n\n  Using our \"Real Kicker Autoship\" program, we're\n  enabling you to earn rewards from multiple levels\n  of income, providing truly profitable leverage.\n\n  The result is more CASH for YOU, plus a wealth of\n  UNBELIEVABLE bonuses, delivered through the power\n  of our unique business structure. It's truly amazing.\n\n  For full and exciting information, go now to:\n\n  >>> http://BodiesBest.com/go.php?id=783 <<<\n   \n  FREE info by e mail: mailto:bodiesbestposition783@getresponse.com\n\n  P.S Act TODAY and we'll waive the setup fees! But\n      hurry, this offer only remains valid TODAY.\n\n  P.P.S Microsoft, Coca-Cola, and major TV stations\n      are BEGGING to use our business strategies.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14163169"}, {"subject": "FINANCIALLY FREE in LESS THAN 30 Minutes a Da", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nFINANCIALLY FREE in LESS THAN 30 Minutes a Day\n\nFinally, you can make all the money you want... without\n having to change your current lifestyle!!\n\n * NO Telephone Calls!\n * NO Sending Faxes!\n * NO Stamps to Lick!\n * NO Running to the Post Office!\n\n Our State-of-the-Art - Totally Internet Driven System will\n allow you to build a substantial \"residual\" income in less\n than 30 minutes a day...\n\n RIGHT FROM THE COMFORT OF YOUR OWN HOME!\n \n 1000's are already using our system - and we've only begun\n to scratch the surface. Just listen to what people using\n our system are saying:\n\n    Using your system we have added 100 new members to\n    our business in less than 3 weeks. We are turning\n    heads with this system. It is so easy, simple and\n    effortless I can't believe it. - Bob Alter\n\n    I've sponsored 12 people so far, and this is only\n    my first month! This system is GREAT! - Mike Artobello\n\n    This system is GREAT! I've sponsored 8 people in 2\n    weeks! I'm really having fun! - Danny Davis\n\n    Now here what Brain McMullen a 25 year old has to say:\n       click on this link...\n    http://healthbaby.com/realaudio/BMIntroduceLFI.ram\n\n Wouldn't you like to unleash the power of YOUR computer?\n\n Take a FREE TEST DRIVE of our system TODAY!\n\n It won't cost a cent, and you will be under no obligation\n what-so-ever. Just drop by the following URL:\n\n  http://healthbaby.com/id18.htm#__1\n\n You'll be amazed at just how easy it is!!\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14173975"}, {"subject": "How to turn your mailbox into a MoneyMagne", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nHow to turn your mailbox into a Money-Magnet\n\nThis easy to use program spits out $20 checks... \nIt's a 3-part automated system, consisting of a KILLER \nclassified ad, a powerful one page sales letter delivered \nby auto responder, and a QUALITY product, delivered to \nyour customer by the company. \n\nYou can set up today...and actually be getting checks \nmailed to you tomorrow. \n\nFor complete details, send a blank email to: \ntooeasytopassup@aweber.com\n\nOr visit   \n \nhttp://www.cc-pages.com/vips/LeeCanfield/index.htm\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14183897"}, {"subject": "100% SPAM FREE email marketing works", "content": "Adworks Solo Advertising - http://adworks.cc\n\n----------------------------------------------------------\nA SOLO AD FROM ADWORKS PUBLISHING INC.\n----------------------------------------------------------\n\nDon't get left behind! You need to advertise your \nbusiness if you want to increase your sales. We are \noffering an Adworks SOLO AD SPECIAL for just $49.95 \n\nVisit http://adworkspublishing.com\n\nPlease do not hit REPLY. Please use the contact data\nin the advertisement for more information about this\nsolo ad. Please visit our advertiser's web site!\n\nTO UNSUBSCRIBE from this mailing list, please remove\nyour email address by following the instructions at\nthe bottom of this message. Thank you.\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\n100% SPAM FREE email marketing works!\n\nWe have developed a system to send your advertisement to over 1 million\nemails per day to promote your product or service! If you do not have a\nproduct or service you can become a reseller and we will spit the profits\n50-50 with you. You get paid every 2 weeks and notified by email of every\nsale instantly!! Explode your sales and make some real money on the web!!\n100% SPAM FREE email marketing works! You must use every tool available to\nyou online to really succeed ... so JOIN US in making your dreams become a\nreality! \n\nvisit\nhttp://1millionoptin.com\n\nWe will also send your ad to over 450,000 opt in email subscribers for a\nincredible fee of $22.50! Visit us today!\n\nYou cant lose with this program. Learn the secrets of the most successful\nonline marketers today!\n\ndave\nsales@1millionoptin.com\nhttp://1millionoptin.com\n\n-----------------------------------------------------------\nSOLO AD SOLO AD SOLO AD SOLO AD SOLO AD\n-----------------------------------------------------------\n\nSend e-mail to sysop@adworkspublishing.com for more info \non solo advertising and other marketing programs. \nVisit http://adworkspublishing.com to order your ads.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14192973"}, {"subject": "Adworks Buy One Solo, Get one Free", "content": "Adworks Solo Advertising - http://adworks.cc\n\nIf you buy a SOLO AD before JULY 31, you'll get\na second solo ad FREE! Just visit our website at\nhttp://adworkspublishing.com and order YOUR Solo Ad!\nIn the COMMENT field write \"2 for 1 SPECIAL\" and \nwe'll send out the exact ad FREE one week later!\n\nRegular price is $99.95 for a SOLO AD\nNot to be combined with other Specials.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14202325"}, {"subject": "Adworks Buy One Solo, Get one Free", "content": "Adworks Solo Advertising - http://adworks.cc\n\nIf you buy a SOLO AD before JULY 31, you'll get\na second solo ad FREE! Just visit our website at\nhttp://adworkspublishing.com and order YOUR Solo Ad!\nIn the COMMENT field write \"2 for 1 SPECIAL\" and \nwe'll send out the exact ad FREE one week later!\n\nRegular price is $99.95 for a SOLO AD\nNot to be combined with other Specials.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14209416"}, {"subject": "RE: Questions (errata?) about caching authenticated response", "content": "Something that if you need to authenticate to the proxy, and the response is\nprivate (e.g. private cache-control setting and/or authentication) that the\ncached object is stored in the proxy cache, but only be accessable if you\nauthenticate with the same name???? (Private caching done on a proxy\nserver?)\n\nMaybe that I'm getting it wrong...\n\n\nI know that proxies are primarily intend to lower internet bandwidth and\nprovide security (kind of firewall)...\n\n\n\n- Joris Dobbelsteen\n\n\n> -----Original Message-----\n> From: Mark Nottingham [mailto:mnot@mnot.net]\n> Sent: saturday 22 july 2000 21:28\n> To: Joris Dobbelsteen\n> Cc: WWW WG (E-mail)\n> Subject: Re: Questions (errata?) about caching authenticated responses\n>\n>\n>\n> I think the point here is that maximum 'security' is not\n> always the goal;\n> sometimes, all that's needed is trivial authentication (which\n> is all that\n> can really be expected in any case), and cacheability of the\n> objects due to\n>\n> Cheers,\n>\n>\n> On Sat, Jul 22, 2000 at 04:05:31PM +0200, Joris Dobbelsteen wrote:\n> > The best solution for maximum security whould be:\n> >\n> > Authenticated request\n> > =====================\n> > Shared-Cache\n> > Do NOT cache the response, because it requires uses to\n> authenticate, and may\n> > not be accessed by everyone.\n> >\n> > Private-Cache\n> > A private-cache is used by ONLY ONE PERSON. This cache may cache the\n> > response (depending on the cache-control header), because\n> it can only be\n> > accessed by one person.\n> >\n> >\n> >\n> > - Joris Dobbelsteen\n> >\n> >\n> > > -----Original Message-----\n> > > From: Duane Wessels [mailto:wessels@ircache.net]\n> > > Sent: donderdag 20 juli 2000 7:48\n> > > To: http-wg@cuckoo.hpl.hp.com\n> > > Subject: Questions (errata?) about caching authenticated responses\n> > >\n> > >\n> > > I've been reading RFCs 2616 and 2617 about caching authenticated\n> > > responses, and have possibly found some inconsistencies.\n> > >\n> > > #1.     The very last sentence of Sec 14.9.4 (under\n> proxy-revalidate)\n> > > says: ``...such authenticated responses also need the public\n> > > cache control directive in order to allow them to be cached at\n> > > all''\n> > >\n> > > Yet, Sec 14.8 lists three cache-control directives that allow a\n> > > shared cache to reuse an authenticatd response: s-maxage,\n> > > must-revalidate, and public.\n> > >\n> > > #2.If must-revalidate alone is enough to allow an\n> authenticated\n> > > response to be cached, and if proxy-revalidate is the same\n> > > as must-revalidate for a shared cache, is proxy-revalidate\n> > > alone enough to allow an authenticated response to be cached?\n> > >\n> > > If so, should proxy-revalidate be listed in section 14.8?\n> > >\n> > > #3.RFC 2617, Sec 3.2.2.5 says:\n> > >\n> > >     when a shared cache ... has received a request containing\n> > >     an Authorization header and a response from relaying that\n> > >     request, it MUST NOT return that response as a reply to any\n> > >     other request, unless one of two Cache-Control (see section\n> > >     14.9 of [RFC2616]) directives was present in the response.\n> > >\n> > > I believe this is referring to section 14.8, rather than 14.9,\n> > > and \"two\" is not the right number?\n> > >\n> > > Finally, Sec 14.8 doesn't mention if a non-shared cache\n> needs to treat\n> > > an authenticated response specially.  I assume that a non-shared\n> > > cache can store and reuse an authenticated response by default.\n> > > Should that be made explicit?\n> > >\n> > > Duane W.\n> > >\n> > >\n> > >\n> > >\n> >\n>\n> --\n> Mark Nottingham\n> http://www.mnot.net/\n>\n\n\n\n", "id": "lists-012-14216536"}, {"subject": "Re: Questions (errata?) about caching authenticated response", "content": "One scenario where caching of autheniticated objects is useful would be\nwhere there is a common, large object (say, a PDF file) which must be\nauthenticated. Instead of returning the PDF from the server upon each\nrequest, it's more efficient to allow an intermediate proxy to cache the\nobject, but force it to validate it each time, with the response headers\n\n  Cache-Control: public, max-age=0, must-revalidate\n\nThe catch here is that all downstream proxies must correctly evaluate this\nheader, or there will be problems. Therefore, this is only really useful in\na private network (I've deployed that solution before, to excellent effect),\nor on a trusted content delivery network.\n\nIf you wanted to have user-specific copies cached, I suppose you could use\nthe Vary mechanism:\n\n  Vary: Authorization\n\nbut not many caches support this kind of content negotiation (yet). This\nwould tend to blow out the cache size, and unless the same user accesses\nthese objects a lot, it's not very useful.\n\nCheers,\n\n\n\nOn Mon, Jul 24, 2000 at 07:30:09PM +0200, Joris Dobbelsteen wrote:\n> Something that if you need to authenticate to the proxy, and the response is\n> private (e.g. private cache-control setting and/or authentication) that the\n> cached object is stored in the proxy cache, but only be accessable if you\n> authenticate with the same name???? (Private caching done on a proxy\n> server?)\n> \n> Maybe that I'm getting it wrong...\n> \n> \n> I know that proxies are primarily intend to lower internet bandwidth and\n> provide security (kind of firewall)...\n> \n> \n> \n> - Joris Dobbelsteen\n> \n> \n> > -----Original Message-----\n> > From: Mark Nottingham [mailto:mnot@mnot.net]\n> > Sent: saturday 22 july 2000 21:28\n> > To: Joris Dobbelsteen\n> > Cc: WWW WG (E-mail)\n> > Subject: Re: Questions (errata?) about caching authenticated responses\n> >\n> >\n> >\n> > I think the point here is that maximum 'security' is not\n> > always the goal;\n> > sometimes, all that's needed is trivial authentication (which\n> > is all that\n> > can really be expected in any case), and cacheability of the\n> > objects due to\n> >\n> > Cheers,\n> >\n> >\n> > On Sat, Jul 22, 2000 at 04:05:31PM +0200, Joris Dobbelsteen wrote:\n> > > The best solution for maximum security whould be:\n> > >\n> > > Authenticated request\n> > > =====================\n> > > Shared-Cache\n> > > Do NOT cache the response, because it requires uses to\n> > authenticate, and may\n> > > not be accessed by everyone.\n> > >\n> > > Private-Cache\n> > > A private-cache is used by ONLY ONE PERSON. This cache may cache the\n> > > response (depending on the cache-control header), because\n> > it can only be\n> > > accessed by one person.\n> > >\n> > >\n> > >\n> > > - Joris Dobbelsteen\n> > >\n> > >\n> > > > -----Original Message-----\n> > > > From: Duane Wessels [mailto:wessels@ircache.net]\n> > > > Sent: donderdag 20 juli 2000 7:48\n> > > > To: http-wg@cuckoo.hpl.hp.com\n> > > > Subject: Questions (errata?) about caching authenticated responses\n> > > >\n> > > >\n> > > > I've been reading RFCs 2616 and 2617 about caching authenticated\n> > > > responses, and have possibly found some inconsistencies.\n> > > >\n> > > > #1.     The very last sentence of Sec 14.9.4 (under\n> > proxy-revalidate)\n> > > > says: ``...such authenticated responses also need the public\n> > > > cache control directive in order to allow them to be cached at\n> > > > all''\n> > > >\n> > > > Yet, Sec 14.8 lists three cache-control directives that allow a\n> > > > shared cache to reuse an authenticatd response: s-maxage,\n> > > > must-revalidate, and public.\n> > > >\n> > > > #2.If must-revalidate alone is enough to allow an\n> > authenticated\n> > > > response to be cached, and if proxy-revalidate is the same\n> > > > as must-revalidate for a shared cache, is proxy-revalidate\n> > > > alone enough to allow an authenticated response to be cached?\n> > > >\n> > > > If so, should proxy-revalidate be listed in section 14.8?\n> > > >\n> > > > #3.RFC 2617, Sec 3.2.2.5 says:\n> > > >\n> > > >     when a shared cache ... has received a request containing\n> > > >     an Authorization header and a response from relaying that\n> > > >     request, it MUST NOT return that response as a reply to any\n> > > >     other request, unless one of two Cache-Control (see section\n> > > >     14.9 of [RFC2616]) directives was present in the response.\n> > > >\n> > > > I believe this is referring to section 14.8, rather than 14.9,\n> > > > and \"two\" is not the right number?\n> > > >\n> > > > Finally, Sec 14.8 doesn't mention if a non-shared cache\n> > needs to treat\n> > > > an authenticated response specially.  I assume that a non-shared\n> > > > cache can store and reuse an authenticated response by default.\n> > > > Should that be made explicit?\n> > > >\n> > > > Duane W.\n> > > >\n> > > >\n> > > >\n> > > >\n> > >\n> >\n> > --\n> > Mark Nottingham\n> > http://www.mnot.net/\n> >\n> \n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-14229557"}, {"subject": "Re: comments on draft-ietf-http-authentication01.tx", "content": "Paul Leach wrote:\n> > [DMK]\n> >   Furthermore, since I got\n> > beat up by Yaron about stating explicitly what agents should do with\n> > unrecognized attributes (namely, ignore) in RFC 2109, I feel obliged to\n> > return the favor.\n> >\n> Unlike the Borg, we don't have a collective mind, and hence no collective\n> guilt... :-)\n\nI was returning the favor to another spec. writer, not to another MS\nperson.\n\n> [...]\n\n> > Since the spec. goes to the trouble of defining a protection space\n> > (sect. 1.2), I think each type of authentication (including successors\n> > to Basic and Digest) ought to state clearly what its protection space\n> > is.\n> >\n> If we have a chance to make editorial changes, then OK. But I am wieghing\n> everything against the standard of \"does this mean that it is good enough to\n> pass Last Call or not\"? If it can pass Last Call and editorial changes can\n> still be made, I'm happy to make the changes you suggest.\n\nSounds like it's editorial -- a clarification -- to me.\n\n> \n> Even then, I think I'd call it the \"assumed protection space\" -- i.e. is\n> what the client believes is protected by that set of credentials, until it\n> discovers otherwise by either gettin a 401 on a URL it thought was in that\n> sapce, or being prompted for credentials in the same realm for a URL that it\n> thought wasn't in that space. Naming suggestions welcomed.\n\nYou could just take the Humpty Dumpty approach:  \"protection space\"\nmeans what it's defined to mean.  Changing the term now may be hard.\n\n> [...]\n\n> > > > Sect. 3.2.3, The Authentication-Info Header\n> > > >     What should a client do if the rspauth=response-digest information\n> > > >     is wrong?\n> > > >\n> > > Not accept the response.\n> >\n> > How does a client, which has already read a response, \"not accept\n> > [it]\"?  I'm picking nits here, true.  Does it mean that a browser would\n> > show the user an error saying that the received response was in error?\n> >\n> That's what I'd do. But we aren't supposed to prescribe UI behavior...\n\nMaybe not in detail, but I suspect you would like the browser to inform\nthe user.  That doesn't seem like a onerous prescription.\n\n> >\n> > Or does it just stop spinning its logo and leave on the screen what was\n> > already there?\n> >\n> > Suppose the client is a proxy.  What should it do vis-a-vis its client?\n> >\n> Proxies do not posses enough info to check reponses. By design -- if they\n> could know it, that would mean that the protocol is insecure.\n\nYeah, okay, I got confused.\n\n> \n> > > >\n> > > >     Isn't there the risk that an intervening proxy could change the\n> > > >     status code?\n> > > >       ... Authorization header for the request, A2 is\n> > > >          A2       = Status-Code \":\" digest-uri-value\n> > > >       and if \"qop=auth-int\", then A2 is\n> > > >          A2       = Status-Code \":\" digest-uri-value \":\"\n> > H(entity-body)\n> > > >\n> > > Well, the status code isn't a header, but there's a general proscription\n> > > against needlessly changing headers in 13.5.2. Maybe the status line\n> > > contents should be explicitly added to that list.\n> >\n> > Is it possible to say a proxy can't change its status code?  Suppose you\n> > have browser B, caching proxy P, origin server S.  (I'm sure you'll tell\n> > me if this example is way off base.)  B requests object X, which it does\n> > not have in its local cache.  P has the object, but the object has\n> > Cache-Control: must-revalidate.  P sends a *conditional* request to S.\n> > After S asks for credentials, which response P passes to B, B asks again\n> > for the X  S responds with 302 and (is this right?  possible?) an\n> > Authentication-Info header.  The A-I header would presumably contain a\n> > digest of the \"302\", but the proxy would return a 200 and supply X to B,\n> > along with A-I.  B would be unable to match the A-I header and the\n> > response and would assume the response is bogus.\n> >\n> No, I think this is right on target (except it's 304 Not Modified, not 302).\n\nOops.\n\n> I think this is an important case to make work, for efficiency reasons. If I\n> were implementing an origin server, what I'd do, regardless of what the spec\n> says, is to calculate the response-digest assuming the proxy will turn the\n> status code into 200. It violates the letter of the law but not the spirit.\n> The question that I can't figure out off the top of my head is: how well\n> would that work?\n\nCouldn't a client, or proxy, make a Range request?  In that case the\nresponse could be 206.  Does sending the status code in response-auth\nreally add that much value?\n\nDave Kristol\n\n\n\n", "id": "lists-012-1423002"}, {"subject": "Adworks 2 for 1 special promotion", "content": "Adworks Solo Advertising - http://adworks.cc\n\nIf you buy a SOLO AD before JULY 31, you'll get\na second solo ad FREE! Just visit our website at\nhttp://adworkspublishing.com and order YOUR Solo Ad!\nIn the COMMENT field write \"2 for 1 SPECIAL\" and \nwe'll send out the exact ad FREE one week later!\n\nRegular price is $99.95 for a SOLO AD\nNot to be combined with other Specials.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14243807"}, {"subject": "UNSUBSCRIB", "content": "UNSUBSCRIBE\n\n\nattached mail follows:\nAdworks Solo Advertising - http://adworks.cc\n\nIf you buy a SOLO AD before JULY 31, you'll get\na second solo ad FREE! Just visit our website at\nhttp://adworkspublishing.com and order YOUR Solo Ad!\nIn the COMMENT field write \"2 for 1 SPECIAL\" and \nwe'll send out the exact ad FREE one week later!\n\nRegular price is $99.95 for a SOLO AD\nNot to be combined with other Specials.\n\n\n______________________________________________________________________\nTo unsubscribe, write to soloads-unsubscribe@listbot.com\n\nStart Your Own FREE Email List at http://www.listbot.com/links/joinlb\n\n\n\n", "id": "lists-012-14250816"}, {"subject": "RE: Questions (errata?) about caching authenticated response", "content": "> -----Original Message-----\n> From: Mark Nottingham [mailto:mnot@mnot.net]\n> Sent: donderdag 27 juli 2000 1:02\n> To: Joris Dobbelsteen\n> Cc: 'Mark Nottingham'\n> Subject: RE: Questions (errata?) about caching authenticated responses\n>\n>\n> Quoting Joris Dobbelsteen <joris.dobbelsteen@mail.com>:\n>\n> > >   Cache-Control: public, max-age=0, must-revalidate\n> >\n> > can't you just send Cache-Control: public, s-maxage=0,\n> proxy-revalidate\n> > This in order to let the user agent benefit from it's own (private)\n> > cache?\n>\n> s-maxage isn't widely supported, as far as I can tell.\n> max-age is safer.\n\nLooked at RFC2616 only, don't have experience with that. So i assume that\nyou are right...\n\n>\n>\n> > Agree,\n> > however you need to cache the 'access denied' document if\n> AUTHORIZATION\n> > is\n> > not used, so there are 2 items with the same URL (if you\n> want optimal\n> > caching and it is allowed by the server).\n>\n> That isn't possible in the HTTP.\n>\n\nI think it can be possible, however it's not supported by any cache, nor is\nit described in the RFCs (the 401 status code is not cachable according to\nthe RFC I assume) how to do it. But we better conform to the real standards\nand forget this option, until maybe a new version of HTTP.\nOK, so (in HTTP/1.1) that is not supported...\n\n>\n> > > If you wanted to have user-specific copies cached, I suppose\n> > > you could use\n> > > the Vary mechanism:\n> > >\n> > >   Vary: Authorization\n> >\n> > Usually you store user-specific only in the private cache, e.g.\n> > cache-control: private, so you don't waste space on the\n> proxy, since all\n> > documents are regenerated (like CGI, ASP and PHP). Most\n> users work on\n> > one\n> > machine only (most of the time).\n> > All right, this means only unauthenticated responses are\n> returned from\n> > the\n> > cache, but don't server send for documents that require\n> authentication\n> > and\n> > are not authenticated (yet) normally the HTTP/1.1 401 Unauthorized\n> > reponse\n> > code? You should rater reserve cache space for the\n> unauthorized document\n> > and\n> > the authorized (authenticated) document.\n>\n> I'm not sure I understand the question.\n>\n\nCan always take a guess...\n\nUse the headers:\ncache-control: public\nVary: Authorization\n\nThis means if a user authenticates to the server, a document is still\ndownloaded from the server. If no authorization header is included you will\nmost likely get a 401 status code, what will be send to give the user agent\na chance to send authentication data. This means only the 401 status code\ncan be cached (if allowed by the RFC), and other responses are not cached by\na shared cache.\nIn this cache you:\n* Don't benefit from the shared cache, since the 401 status code is (most\nlikely) not cached.\n* You could use the header \"cache-control: private\", since it won't make any\ndifference\n\nIf you get a document (HTTP/1.1 200 reponse) then you whould benefit from\nthe shared cache, but it won't leave the possibility open for the client to\nauthenticate, since it's not required. Some User-agent (clients) won't\nauthenticate unless explicitly requested. At least I think this is what a\nsaw when developing a HTTP/1.1 proxy server and did some testing with\nIE/4.0+. I did some console dumps of the requests.\nIn this cace you would:\n* benefit from a shared cache, at least for unauthenticated requests\n* get no authorization from (some) clients. They connect anonymously.\n\nCorrect???\n\n>\n> Cheers,\n>\n\n\n\n", "id": "lists-012-14258261"}, {"subject": "ticket based authenticatio", "content": "I think I recall some mention that security related issues were \nnot being dealt with by this group, but then I saw the RFC for \nBasic and Digest Access Authentication among this groups RFCs...\n\nIf this has been answered already, then a gentle reminder of \nwhere I need to look will be sufficient :P\n\n+---\n|\n| I would like to propose an extension to the HTTP standard to \n  include yet another authentication scheme.  This would allow for \n  clients to use a third-party URL (third party being not the \n  client and not the site requiring authentication) to generate the \n  authentication credentials.\n\n  This would allow for one site to know who someone is without \n  having access to the information which would prove their \n  identity.  This requires that the site requiring authentication \n  trust the site issuing the credentials.\n\n  This allows for a central authority to issue credentials without \n  untrusted sites having sufficient information to reproduce those \n  credentials.  This can be important when the identity of the \n  untrusted sites may be unknown, or when the information used to \n  authenticate to the central authority may be legally protected.\n\n  This scheme also solves the problem with POST requests and other\n  requests with a body when trying to implement this scheme with\n  the current standards (cookies and redirects) and with finite\n  credential lifetimes.  With the client unaware of the overall \n  process, the user experience is severly affected.\n\n  I believe this can be done within the present framework outlined \n  in RFC 2617.  \n\n  The auth-scheme would be \"ticket\" or \"third-party\" or some other \n  sensical tag.  The auth-param would be `realm' as presently \n  defined.  Any parameters required by the site issuing the \n  credentials would be included in the URL for that site (see next\n  paragraph).\n\n  In addition to the challange, a location header would need to be \n  sent so the client knows where to go to obtain the credentials.  \n  The client would need to not retry the request requiring the \n  credentials until it has obtained those credentials from \n  following the actions at that location.\n\n  Any request for credentials with this scheme should preempt any \n  other request for credentials with this same scheme.  This allows \n  a client to only track one such request at a time, without \n  requiring an unbounded stack of nested requests, or even \n  unrelated requests.  This also allows for the third-party site to \n  use any other authentication scheme it might find necessary \n  before issuing the credentials.\n\n  The third-party may issue the credentials in the response header \n  with the `Authorization' header line.  The client should be able \n  to use the contents of this header line verbatim in retrying the \n| original request.\n|\n+--\n\nThis is a bit rough in the description, but if there are any questions,\nlet me know.  If this is something worthwhile, I'll put together a more\nformal description.\n--\nJames Smith <JGSmith@TAMU.Edu>, 409-862-3725\nTexas A&M CIS Operating Systems Group, Unix\n\n\n\n", "id": "lists-012-14269825"}, {"subject": "Re: ticket based authenticatio", "content": "On Wed, Aug 02, 2000 at 09:34:07AM -0500, James G Smith wrote:\n> I think I recall some mention that security related issues were \n> not being dealt with by this group, but then I saw the RFC for \n> Basic and Digest Access Authentication among this groups RFCs...\n> \n> If this has been answered already, then a gentle reminder of \n> where I need to look will be sufficient :P\n> \n> +---\n> |\n> | I would like to propose an extension to the HTTP standard to \n>   include yet another authentication scheme.  This would allow for \n>   clients to use a third-party URL (third party being not the \n>   client and not the site requiring authentication) to generate the \n>   authentication credentials.\n[snip]\n\nIsn't the algorithm=MD5-sess in Digest auth sufficient? The A1 is\nbasically your ticket. Or maybe I'm missing something.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-14280224"}, {"subject": "RE: ticket based authenticatio", "content": "> From: ronald@innovation.ch\n\n> Isn't the algorithm=MD5-sess in Digest auth sufficient? The A1 is\n> basically your ticket. Or maybe I'm missing something.\n\nNo, Digest as currently defined allows the http server to consult a\nthird party authentication server in order to obtain the secret (but\ndoes not specify how that should be done).  It does not, however, meet\nthe need described here - that the http server be able to instruct the\nclient to first obtain credentials through the third party server.\n\n--\nScott Lawrence\n\n\n\n", "id": "lists-012-14288442"}, {"subject": "RE: ticket based authenticatio", "content": "You might look at Kerberos. Maybe this already provides what you want to\nhave in the authentication. However there needs to come a standard\ndescribing how to use Kerberos authentication with HTTP.\n\nAt least I assumed Kerberos also used tickets e.d. Maybe that it needs to be\na bit modified (e.g. HTTP-Kerberos), because something with certificates (I\ndon't know how it all works).....\n\nOnly a suggestion......\n\n- Joris Dobbelsteen\n\n> -----Original Message-----\n> From: jgsmith@hex.tamu.edu [mailto:jgsmith@hex.tamu.edu]On Behalf Of\n> James G Smith\n> Sent: woensdag 2 augustus 2000 16:34\n> To: http-wg@cuckoo.hpl.hp.com\n> Cc: JGSmith@TAMU.Edu\n> Subject: ticket based authentication\n>\n>\n> I think I recall some mention that security related issues were\n> not being dealt with by this group, but then I saw the RFC for\n> Basic and Digest Access Authentication among this groups RFCs...\n>\n> If this has been answered already, then a gentle reminder of\n> where I need to look will be sufficient :P\n>\n> +---\n> |\n> | I would like to propose an extension to the HTTP standard to\n>   include yet another authentication scheme.  This would allow for\n>   clients to use a third-party URL (third party being not the\n>   client and not the site requiring authentication) to generate the\n>   authentication credentials.\n>\n>   This would allow for one site to know who someone is without\n>   having access to the information which would prove their\n>   identity.  This requires that the site requiring authentication\n>   trust the site issuing the credentials.\n>\n>   This allows for a central authority to issue credentials without\n>   untrusted sites having sufficient information to reproduce those\n>   credentials.  This can be important when the identity of the\n>   untrusted sites may be unknown, or when the information used to\n>   authenticate to the central authority may be legally protected.\n>\n>   This scheme also solves the problem with POST requests and other\n>   requests with a body when trying to implement this scheme with\n>   the current standards (cookies and redirects) and with finite\n>   credential lifetimes.  With the client unaware of the overall\n>   process, the user experience is severly affected.\n>\n>   I believe this can be done within the present framework outlined\n>   in RFC 2617.\n>\n>   The auth-scheme would be \"ticket\" or \"third-party\" or some other\n>   sensical tag.  The auth-param would be `realm' as presently\n>   defined.  Any parameters required by the site issuing the\n>   credentials would be included in the URL for that site (see next\n>   paragraph).\n>\n>   In addition to the challange, a location header would need to be\n>   sent so the client knows where to go to obtain the credentials.\n>   The client would need to not retry the request requiring the\n>   credentials until it has obtained those credentials from\n>   following the actions at that location.\n>\n>   Any request for credentials with this scheme should preempt any\n>   other request for credentials with this same scheme.  This allows\n>   a client to only track one such request at a time, without\n>   requiring an unbounded stack of nested requests, or even\n>   unrelated requests.  This also allows for the third-party site to\n>   use any other authentication scheme it might find necessary\n>   before issuing the credentials.\n>\n>   The third-party may issue the credentials in the response header\n>   with the `Authorization' header line.  The client should be able\n>   to use the contents of this header line verbatim in retrying the\n> | original request.\n> |\n> +--\n>\n> This is a bit rough in the description, but if there are any\n> questions,\n> let me know.  If this is something worthwhile, I'll put\n> together a more\n> formal description.\n> --\n> James Smith <JGSmith@TAMU.Edu>, 409-862-3725\n> Texas A&M CIS Operating Systems Group, Unix\n>\n>\n\n\n\n", "id": "lists-012-14297176"}, {"subject": "Re: ticket based authenticatio", "content": "\"Joris Dobbelsteen\" <joris.dobbelsteen@mail.com> wrote:\n>You might look at Kerberos. Maybe this already provides what you want to\n>have in the authentication. However there needs to come a standard\n>describing how to use Kerberos authentication with HTTP.\n>\n>At least I assumed Kerberos also used tickets e.d. Maybe that it needs to be\n>a bit modified (e.g. HTTP-Kerberos), because something with certificates (I\n>don't know how it all works).....\n>\n>Only a suggestion......\n\nThis is what I was thinking.  However, I was not wanting to put the actual \ncontents of the ticket, or credential, in the standard, becuase that depends \non what the credential server and the website (which is requiring the \ncredential) would need to agree on.  It could be kerberos, but it could also \nbe a signed certificate issued by the credential server.\n\nBasically, I'm looking for a standard that can give a fairly reliable user \nexperience while allowing for transport of credentials issued to a client by \none site to be used at another site, but without cookies and the other messy \nkludges required.\n\nFor example, if the site requiring the authentication requires \nreauthentication in response to a POST (e.g., the credentials have expired), \nthat site must buffer the data until the request is retried with possibly a \nGET of some kind and the new credentials.  This can lead to DOS attacks on a \nsite.  By requiring the client to buffer the POST and retry again, we avoid \nthat situation and make for a more reliable user experience.\n\nBy avoiding nested authentication attempts of this type, we avoid the same \nbuffer problem on the client side.\n-- \nJames Smith <JGSmith@TAMU.Edu>, 979-862-3725\nTexas A&M CIS Operating Systems Group, Unix\n\n\n\n", "id": "lists-012-14309875"}, {"subject": "RE: ticket based authenticatio", "content": "While not commenting directly on the proposal, I would note in my\napplication deployment role ... firewall and application service provider\nissues make the missing function Scott mentions an important capability.\n\nThanks,\n  Dave Morris\n\nOn Wed, 2 Aug 2000, Scott Lawrence wrote:\n\n> \n> > From: ronald@innovation.ch\n> \n> > Isn't the algorithm=MD5-sess in Digest auth sufficient? The A1 is\n> > basically your ticket. Or maybe I'm missing something.\n> \n> No, Digest as currently defined allows the http server to consult a\n> third party authentication server in order to obtain the secret (but\n> does not specify how that should be done).  It does not, however, meet\n> the need described here - that the http server be able to instruct the\n> client to first obtain credentials through the third party server.\n\n\n\n", "id": "lists-012-14319188"}, {"subject": "[Fwd: Protocol Action: Use of HTTP State Management to BCP", "content": " \n\nattached mail follows:\n\n\nThe IESG has approved publication of the following Internet-Drafts:\n\no Use of HTTP State Management <draft-iesg-http-cookies-03.txt> as a\n  BCP.\no HTTP State Management Mechanism\n  <draft-ietf-http-state-man-mec-12.txt> as a Proposed Standard.\n\nThe IESG contact person is Patrik Faltstrom.\n \n \nTechnical Summary\n \nThe draft-ietf-http-state-man-mec-12.txt specifies a way to create a\nstateful session with HTTP requests and responses. It describes two new\nheaders, Cookie and Set-Cookie2, which carry state information between\nparticipating origin servers and user agents.  The method described\nhere differs from Netscape's Cookie proposal [Netscape], but it can\ninteroperate with HTTP/1.0 user agents that use Netscape's method.\n(See the HISTORICAL section.) The document reflects implementation\nexperiences from RFC 2109 [RFC2109] and obsoletes it.\n\nEven though this protocol has been approved for the Internet standards\ntrack, some current and potential uses of the protocol are not within\nthe scope of the standard approved by IESG.\ndraft-iesg-http-cookies-03.txt identifies specific uses of HTTP State\nManagement protocol which are either (a) nonstandard and thus not\nrecommended by IETF, or (b) nonstandard, believed to be harmful, and\ndiscouraged. It also details additional privacy considerations which\nare not covered by the HTTP State Management protocol specification.\n\nWorking Group Summary\n\nIt was early obvious that RFC 2109 had to be replaced due to\nimplementation experiences. Various players on the Internet do though\nuse cookies and states for different uses, and because of this, the\ndiscussion sometimes was quite heated. The IESG reviewed the issues,\nand wrote an applicability statement which after a lot of discussions\nended up becoming draft-iesg-http-cookies-03.txt, which explains what\nparts of the state-man-mec which are in scope of IETF discussions.\n\nProtocol Quality\n\nThe protocol was reviewed by Patrik Faltstrom.\n\n\nNote to RFC Editor:\n\nPlease include the following text as an IESG Note:\n\n\n The IESG notes that this mechanism makes use of the .local\n top-level domain (TLD) internally when handling host names\n that don't contain any dots, and that this mechanism might\n not work in the expected way should an actual .local TLD\n ever be registered.\n\n\n\n", "id": "lists-012-14328320"}, {"subject": "Adminstrivia: spam fi", "content": "Folks,\n\nMany apologies for all the nasty spam from Adworks over the last couple of\nweeks that had managed to leak through my filters.\n\nThe spambot was automagically subscribing to the list in order to post.  I've\nfixed that particular loophole (for now).\n\nIf you see spam, can you e-mail me directly and I'll try to fix a.s.a.p.\n\nThanks!\n--\n-- ange -- <><\n\n(http-wg mailng list administrator)ange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-012-14337100"}, {"subject": "ticket based authenticatio", "content": "From the response, it would seem some form of third-party \nauthentication may be desired and useful, but no clear concensus \non how best to go about it.  I don't have an answer to that, but \nI have put my thoughts together in the form of a draft, which \nshould appear sometime as \n\n     draft-smith-http-third-party-authentication-00.txt \n\nUntil it is posted on the IETF site, it is available from my\nown workstation at \n\n  http://hex.tamu.edu/drafts/draft-smith-http-third-party-authentication-00.txt \n\nI already have a correction for it -- the expiration time should \nbe in GMT (section 2.2).\n\nAn issue that is not addressed is how to indicate that the client\nshould abandon the authentication process and discard the pending\nrequest awaiting credentials.  The authentication process MUST\nindicate one and only one of success or abandonment.  When in doubt,\nthe client may abandon the process?  This could be the case if\nthe client becomes confused as to what is going on.\n--\nJames Smith <JGSmith@TAMU.Edu>, 409-862-3725\nTexas A&M CIS Operating Systems Group, Unix\n\n\n\n", "id": "lists-012-14343848"}, {"subject": "Re: ticket based authenticatio", "content": "At 4:47pm Aug 9, 2000, James G Smith wrote:\n\n> I have put my thoughts together in the form of a draft\n\nJames,\n\nI'm glad to see this taking shape. I have a longer note I haven't yet\nsent, but here are some comments:\n\n1) Privacy concerns. This looks like a \"nice\" alternative to third-party\nbanner-ad and web-bug cookies. Even \"better\", because 3rd party cookies\nare only visible to the ad/web-bug server, where this could be used to\nshare the same identifier with the content provider and the\n\"authentication\" provider.\n\n2) Multiple credentials? E.G., if my content site needs to verufy both\nthat the user is an employee of Acme Products and a US citizen, would my\ncontent site simply request one credential, and then the next iff the\nfirst was acceptable?\n\n3) Section 3.2. The expiration time provided by the authentication server\ncould be ignored by a (deliberately noncompliant) client. If you want to\nbe safe, you want a signed credential that includes an expiration time.\n\n4) Nit pick. I'd suggest using example.(org|com|net) domains as examples.\n\n5) Privacy #2: identity discovery. Send an HTML page that references a 1x1\npixel image. The URL for that image sends the client an authentication\nchallenge:\n WWW-Authenticate: Third-Party realm=\"CIA\" url=\"https://login.cia.gov/\"\nand uses that to see if the user is able/willing to prove their\nrelationship with the US Central Intelligence Agency.\n\n6) Auth server needs more info. The client should give the authentication\nserver some reason to go to the trouble of authenticating the client.\nAuthentication can be expensive, so the authentication provider might not\nwant to go to the trouble for unknown sites. This would also allow the\nauth server to do nice tricks like encrypting the response for that\nspecific content site. It could even send garbage data in case of failure\nto protect against identity tests.\n\n7) Caching credentials/realm. If two different sites want me to\nauthenticate for WallyWorld on tehuti.nowhere.org, how does my client know\nthat it's OK to send the credential obtained for the first site to the\nsecond site? I expect (maybe this should be clarified, or maybe I'm\ndense) that the credential would be resent to any URL on the content site\nthat issues the same realm challenge. But should the client also only\nprovide the credential if the authentication \"url\" is the same? Should\nthere be a cookie-like path restriction?\n\n-Peter\n\n-- \nhttp://www.bastille-linux.org/ : working towards more secure Linux systems\n\n\n\n", "id": "lists-012-14351970"}, {"subject": "RE: comments on draft-ietf-http-authentication01.tx", "content": "> ----------\n> From: Ronald.Tschalaer@psi.ch[SMTP:Ronald.Tschalaer@psi.ch]\n> Sent: Friday, March 27, 1998 1:45 AM\n> To: HTTP-WG@cuckoo.hpl.hp.com\n> Subject: Re: comments on draft-ietf-http-authentication-01.txt\n> \n> \n> > >     The term \"protection space\" gets used without a definition (here),\n> > >     but the spec. describes how a client can reuse credentials for\n> such\n> > >     a protection space.  I think we should say that the description of\n> > >     any auth-scheme must describe the rules for deciding when two\n> > >     objects are in the same protection space.  In particular, a client\n> > >     must be able to tell, so it knows whether or not to send\n> credentials\n> > >     unprompted.\n> > > \n> > Why does it need to tell? If it's wrong, by either sending incorrect\n> ones or\n> > not sending any, it'll get a 401 to tell it what to do. As far as I can\n> see,\n> > for Digest it's only an optimization. (For Basic, you don't want to send\n> > your credentials to the wrong place...)\n> > \n> > > Sect. 3.2.1, The WWW-Authenticate Response Header\n> > >     [domain attribute]\n> > >     If this keyword is omitted or empty, the client should assume that\n> > >     the domain consists of all URIs on the responding server.\n> > > \n> > >       This behavior is different from Basic.  If we want Digest to be\n> > >       a more or less drop-in replacement, shouldn't the default\n> > >       behavior mimic Basic?\n> > > \n> > As you point out below, there are implementations. As I point out above,\n> it\n> > shouldn't matter. If I were writing a browser, I'd guess that I should\n> reuse\n> > the key obtained from a previous 401/WWW-Auth until I left the server --\n> > that way, I minimize the extra roundtrips.\n> \n> There is a certain tradeoff here.\n> \nI agree.\n\n>  Sending the Authorization header with\n> Digest credentials unnecessarily is not to be shrugged off too lightly,\n> IMHO - this header is large and can easily double the number of bytes in\n> a request! (The example in the draft is typical and is 261 bytes).\n> Certainly, extra roundtrips are to be avoided, but not at all costs.\n> \nFor Digest, the choice is just a heuristic. For Basic, if you guess wrong,\nyou've given your password away.\n\n> Also, sending the Authorization header unnecessarily is likely to reduce\n> the cachability of many pages, thereby further increasing the traffic\n> (how many responses currently contain the cache-control directive s-maxage\n> or public? How quickly will this change?).\n> \nIt is worth adding a note that origin servers that receive requests with\nAuthorization headers when authorization is not needed SHOULD send back\nexplicit cache-control directives to allow the page to be cached.\n\n> I'm not really that comfortable with preemptively sending the\n> Authorization header with *all* requests to a server for which at least\n> one document needed authorization. I do prefer the heuristic used for\n> the Basic scheme as I believe it reflects reality much better (i.e.\n> usually it's just certain url prefixes which require authorization).\n> \nI'll look at some wording to make it clear that for digest, the domain space\nis just advisory, but with the implications noted above.\n\nPaul\n\n\n\n", "id": "lists-012-1435471"}, {"subject": "Spaces allowed between chunk size and semicolon", "content": "Hi all,\nsection 3.6.1 of RFC2616 the definition of a chunk body is given. I wonder\nwhether or not it is valid to include space characters between the chunk\nsize and the chunk extensions. Thus, if the following chunk body is valid:\n23;ext1<CRLF>\nIs the following chunk body then also valid:\n23<SP>;ext1<CRLF>\nEven more, if both are valid, do they represent the exact same chunk body?\n\nThanks in advance.\n\nKind regards,\nAlexander.\n\n\n\n", "id": "lists-012-14362046"}, {"subject": "Re: ticket based authenticatio", "content": "Peter W <peterw@usa.net> wrote:\n>At 4:47pm Aug 9, 2000, James G Smith wrote:\n>\n>> I have put my thoughts together in the form of a draft\n>\n>James,\n>\n>I'm glad to see this taking shape. I have a longer note I haven't yet\n>sent, but here are some comments:\n>\n>1) Privacy concerns. This looks like a \"nice\" alternative to third-party\n>banner-ad and web-bug cookies. Even \"better\", because 3rd party cookies\n>are only visible to the ad/web-bug server, where this could be used to\n>share the same identifier with the content provider and the\n>\"authentication\" provider.\n\nGood point.  We need some way for the client to know that the \nuser should be aware of the authentication request so it isn't \nsilently taking place.  The user should have a way to refuse \nauthentication as with Basic and Digest.  Of course, refusal with \nthose two is by virtue of the user having to usually enter some \ninformation.\n\nOne important difference between cookies and the credentials in\nthis scheme are that cookies can persist across client invocations.\nWe are explicitly disallowing / discouraging such behavior in\nthe draft.\n\n>2) Multiple credentials? E.G., if my content site needs to verufy both\n>that the user is an employee of Acme Products and a US citizen, would my\n>content site simply request one credential, and then the next iff the\n>first was acceptable?\n\nI was not thinking so much of authorization but of authentication.  \nThe credential would say this person is who they say they are, \nand no more unless agreed to by the site requiring the credentials \nand the site providing the credentials prior to the request.\n\nPerhaps be able to form a request such as:\n\n  HTTP/1.1 401 Unauthorized\n  WWW-Authenticate: Third-Party realm=\"testrealm@host.com\", \n             url=\"https://tehuti.nowhere.org/authenticate/\",\n             ou=acme, c=us\n\nThe other end might then require that the ou and c fields match\nin the ldap entry for the person authenticating.  Of course,\nthis is not the best example for this situation, but it does\nillustrate what is possible in the draft.\n\n>3) Section 3.2. The expiration time provided by the authentication server\n>could be ignored by a (deliberately noncompliant) client. If you want to\n>be safe, you want a signed credential that includes an expiration time.\n\nHaving implemented something along these lines before with cookies \nand redirects, I would expect the site issuing the credentials to \nencode the expiration time in the credentials.  The expiration \ngiven to the client would be purely advisory on their part.\n\nThis could be placed in a section for recommended practices in\ncreating credentials.\n\n>4) Nit pick. I'd suggest using example.(org|com|net) domains as examples.\n\n-nod-\n\n>5) Privacy #2: identity discovery. Send an HTML page that references a 1x1\n>pixel image. The URL for that image sends the client an authentication\n>challenge:\n> WWW-Authenticate: Third-Party realm=\"CIA\" url=\"https://login.cia.gov/\"\n>and uses that to see if the user is able/willing to prove their\n>relationship with the US Central Intelligence Agency.\n\nGood point.  We need a good way for the two sites involved to authenticate\nthemselves to each other.  The credentials can authenticate the issuer\nto the requester, but there isn't a good method for the other way around\nyet that is scalable.\n\nOne way is to send a signed identifier as part of the request (the\nWWW-Authenticate header).  This requires communication between\nthe issuer and the requesting site `behind the scenes'.  If this\ncould be made sufficient, it could be placed in the `recommended\npractices' section.  This could also address the next issue (#6).\n\n>6) Auth server needs more info. The client should give the authentication\n>server some reason to go to the trouble of authenticating the client.\n>Authentication can be expensive, so the authentication provider might not\n>want to go to the trouble for unknown sites. This would also allow the\n>auth server to do nice tricks like encrypting the response for that\n>specific content site. It could even send garbage data in case of failure\n>to protect against identity tests.\n>\n>7) Caching credentials/realm. If two different sites want me to\n>authenticate for WallyWorld on tehuti.nowhere.org, how does my client know\n>that it's OK to send the credential obtained for the first site to the\n>second site? I expect (maybe this should be clarified, or maybe I'm\n>dense) that the credential would be resent to any URL on the content site\n>that issues the same realm challenge. But should the client also only\n>provide the credential if the authentication \"url\" is the same? Should\n>there be a cookie-like path restriction?\n\nWe do need to be able to specify the scope of the credentials.  With\nBasic and Digest, it's trivial.  They are only good on the site which\nrequested them and for the realm.  Perhaps a combination of `url' and\n`realm' could be used.  But then we have the issue of sites requesting\nthem when they shouldn't have them...\n\nAllowing some scoping rules that cookies have would help here.  We\ncan create a system that is no worse than cookies :P  That's not\nall that comforting, but we do have a worst case scenario.\n\nWhen I worked on something similar to this, but with cookies, the\ncredentials would have been passed to any machine in the .tamu.edu\ndomain since that allowed us to have a central server able to\nissue to any other machine in the TAMU network, it kept machines\nfrom outside TAMU from using it, and it would allow people to\nauthenticate once and hop from site to site.\n-- \nJames Smith <JGSmith@TAMU.Edu>, 979-862-3725\nTexas A&M CIS Operating Systems Group, Unix\n\n\n\n", "id": "lists-012-14368972"}, {"subject": "CacheContro", "content": "There was in 5-2000 a discussion about the cache-control header in HTTP/1.1.\n\nMy question is what should a proxy/user-agent do if it receives the request:\n\ncache-control: private, public, max-age=180, s-maxage=120\n\n\nAccording to my e-mail archive it was a discussion Koen Holtman participated\nin.......\nBut I had some trouble and I suppose I didn't receive a lot of e-mails at\nthat time....\n\n\n- Joris Dobbelsteen\n\n\n\n", "id": "lists-012-14382496"}, {"subject": "Re: CacheContro", "content": "   My question is what should a proxy/user-agent do if it receives\n   the request:\n\ncache-control: private, public, max-age=180, s-maxage=120\n\nI'm going to assume that the word \"request\" in your email is\na mistake, and that you meant \"response\".  Three of the four\ndirectives you listed are undefined for requests.  With\nthat in mind:\n\nThe answer is different for a (shared) proxy and for a (non-shared)\nuser-agent.\n\nA user-agent (presumably with a \"non-shared cache\") should\nignore the \"private\" and \"s-maxage\" directives, and so\nthis is equivalent *FOR A NON-SHARED CACHE* to\n\nCache-Control: max-age=180\n\nA shared proxy cache, on the other hand, has no sensible\nway to interpret this header field, because RFC2616 says:\n\n   public\n      Indicates that the response MAY be cached by any cache, even if it\n      would normally be non-cacheable or cacheable only within a non-\n      shared cache. (See also Authorization, section 14.8, for\n      additional details.)\n\n   private\n      Indicates that all or part of the response message is intended for\n      a single user and MUST NOT be cached by a shared cache. This\n      allows an origin server to state that the specified parts of the\n      response are intended for only one user and are not a valid\n      response for requests by other users. A private (non-shared) cache\n      MAY cache the response.\n\nGiven that the \"private\" directive in your example does NOT list\nany specific parts of the response, it therefore must refer to\nthe entire response.  But this means that the \"public\" and\n\"private\" directives are conflicting, and so the server that\nsent the response has a bug.\n\nAs a general rule, I would recommend that if a received\nresponse appears to be buggy, it is stupid to cache it.\n(Another way to look at this: if the server implementor\nappears to have misunderstood the HTTP caching specification,\nthen a cache implementor ought to protect the user from\nthat implementor's mistake.)  Don't cache responses if\nthey cannot unambiguously be cached with \"semantic transparency\"\n(see RFC2616).\n\n-Jeff\n\n\n\n", "id": "lists-012-14389505"}, {"subject": "RE: CacheContro", "content": "> -----Original Message-----\n> From: Jeffrey Mogul [mailto:mogul@pa.dec.com]\n> Sent: maandag 14 augustus 2000 20:24\n> To: Joris Dobbelsteen\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Cache-Control\n>\n>\n>    My question is what should a proxy/user-agent do if it receives\n>    the request:\n>\n> cache-control: private, public, max-age=180, s-maxage=120\n>\n> I'm going to assume that the word \"request\" in your email is\n> a mistake, and that you meant \"response\".  Three of the four\n> directives you listed are undefined for requests.  With\n> that in mind:\n\nI didn't see that at time I was writing the mail, but you are right,\nI meant response instead of request.\n\n>\n> The answer is different for a (shared) proxy and for a (non-shared)\n> user-agent.\n>\n> A user-agent (presumably with a \"non-shared cache\") should\n> ignore the \"private\" and \"s-maxage\" directives, and so\n> this is equivalent *FOR A NON-SHARED CACHE* to\n>\n> Cache-Control: max-age=180\n\nWhat about \"public\"???\n\n>\n> A shared proxy cache, on the other hand, has no sensible\n> way to interpret this header field, because RFC2616 says:\n>\n>    public\n>       Indicates that the response MAY be cached by any cache,\n> even if it\n>       would normally be non-cacheable or cacheable only within a non-\n>       shared cache. (See also Authorization, section 14.8, for\n>       additional details.)\n>\n>    private\n>       Indicates that all or part of the response message is\n> intended for\n>       a single user and MUST NOT be cached by a shared cache. This\n>       allows an origin server to state that the specified parts of the\n>       response are intended for only one user and are not a valid\n>       response for requests by other users. A private\n> (non-shared) cache\n>       MAY cache the response.\n>\n> Given that the \"private\" directive in your example does NOT list\n> any specific parts of the response, it therefore must refer to\n> the entire response.  But this means that the \"public\" and\n> \"private\" directives are conflicting, and so the server that\n> sent the response has a bug.\n\nOk, that's true. That is why I send this example, to see how you (HTTP WG)\nthink how to handle this...\n\n>\n> As a general rule, I would recommend that if a received\n> response appears to be buggy, it is stupid to cache it.\n> (Another way to look at this: if the server implementor\n> appears to have misunderstood the HTTP caching specification,\n> then a cache implementor ought to protect the user from\n> that implementor's mistake.)  Don't cache responses if\n> they cannot unambiguously be cached with \"semantic transparency\"\n> (see RFC2616).\n\nLet's if I understand it right:\nMeaning your recommendation whould be:\n\nPrivate Cache (NON-SHARED)\n    Cache it (max-age = 180)\n\nPublic Cache (SHARED)\n    Don't cache it (conflicting cache-control)\n    However on the other had, the server did send s-maxage=120,\n    so it's caring for public caches.....\n\n>\n> -Jeff\n>\n>\n\n- Joris\n\n\n\n", "id": "lists-012-14398645"}, {"subject": "Improving HTTP/DAV status reportin", "content": "A common concern raised by developers of WebDAV clients and servers is the\ndeficiencies of HTTP/DAV status reporting. For example, two common problems\nare overloading of HTTP status codes to mean several different conditions,\nand the inability to precisely report many kinds of status using the\nexisting status codes. Even when an existing status code does cover a\ncondition well, there is no mechanism for passing supplemental status\ninformation. WebDAV provides two operand methods such as COPY and MOVE where\nan error could occur at the source or destination, a capability outside the\nscope of HTTP error reporting. It is also possible that multiple error\nconditions can simultaneously occur, yet only one can be reported in a\nresponse.\n\nEspecially for 4xx series status codes, there is a limited number of status\ncodes, leading to conservative use of status codes within the protocol\ndevelopment community, and leads to overloading of existing status codes. A\nprincipled expansion of the HTTP status code space seems to be a cornerstone\nof any effort to improve HTTP/DAV status reporting.\n\nTo evaluate and address these concerns, a new mailing list has been created\nto discuss issues concerning improved HTTP/DAV status reporting.\n\nTo post to this list, send your email to:\n\n  report@webdav.org\n\nSubscription instructions and archives are available at:\n\n  http://mailman.webdav.org/mailman/listinfo/report\n\nThis mailing list is welcome to all, and maintains a public archive.\n\nWhile the impetus for this discussion is coming from the WebDAV community,\nthis problem affects HTTP broadly, and hence a new forum for discussion has\nbeen created. It is expected that list members will determine where\nspecification work, if any, will take place.\n\n- Jim Whitehead <ejw@ics.uci.edu>\n\n\n\n", "id": "lists-012-14409518"}, {"subject": "Re: CacheContro", "content": "Joris Dobbelsteen wrote:\n\n> > A user-agent (presumably with a \"non-shared cache\") should\n> > ignore the \"private\" and \"s-maxage\" directives, and so\n> > this is equivalent *FOR A NON-SHARED CACHE* to\n> >\n> >       Cache-Control: max-age=180\n>\n> What about \"public\"???\n\nNobody else has access to that cache, so \"public\" can't mean anything.\n\n> Public Cache (SHARED)\n>     Don't cache it (conflicting cache-control)\n>     However on the other had, the server did send s-maxage=120,\n>     so it's caring for public caches.....\n\nMaybe.  Or maybe s-maxage crept in because of some memory trashing on the\nserver side.  Once you see that the server is buggy, you can't be sure what\nit wants.\n\n(Reminds me of a line from a Pratchett book: \"Why is he *doing* this?\" \"That\nis only a valid question if he is sane.  He may be doing it because the\nlittle green pixies tell him to.\"  :-)\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |Help stamp out vi in our lifetime!           |\n|francis@ecal.com|                                             |\n\\==============================================================/\n\n\n\n", "id": "lists-012-14418933"}, {"subject": "RE: Microsoft HTTP 1.1 Conformance", "content": "You can let the MS Proxy Server only work with HTTP/1.1 if you install the\nMicrosoft Internet Information Service (IIS) version 3.0 or later.\nAt least, this is what MS says....\n\n- Joris\n\n> -----Original Message-----\n> From: Craig Schoeberle [mailto:cschoeberle@mediaone.net]\n> Sent: vrijdag 18 augustus 2000 1:30\n> To: j.p.tdobbelsteen@freeler.nl\n> Subject: RE: Microsoft HTTP 1.1 Conformance?\n>\n>\n> Any resolution to the http 1.1 issue?  We are using MS three\n> version 2.0\n> proxy servers in an array configured for reverse proxy to our\n> web servers.\n> The proxy servers are responding with http version 1.0 header to our\n> application servers. I have searched the entier internet and\n> the registry\n> for this setting and no luck. Any ideas?\n>\n> Thanks,\n>\n> Craig Schoeberle\n>\n>\n\n\n\n", "id": "lists-012-14426833"}, {"subject": "Re: Microsoft HTTP 1.1 Conformance", "content": "Actually you need to have IIS 4.0 installed.\n\n\n----- Original Message -----\nFrom: \"Joris Dobbelsteen\" <joris.dobbelsteen@mail.com>\nTo: \"WWW WG (E-mail)\" <http-wg@cuckoo.hpl.hp.com>\nSent: Saturday, August 19, 2000 3:28 PM\nSubject: RE: Microsoft HTTP 1.1 Conformance?\n\n\n> You can let the MS Proxy Server only work with HTTP/1.1 if you install the\n> Microsoft Internet Information Service (IIS) version 3.0 or later.\n> At least, this is what MS says....\n>\n> - Joris\n>\n> > -----Original Message-----\n> > From: Craig Schoeberle [mailto:cschoeberle@mediaone.net]\n> > Sent: vrijdag 18 augustus 2000 1:30\n> > To: j.p.tdobbelsteen@freeler.nl\n> > Subject: RE: Microsoft HTTP 1.1 Conformance?\n> >\n> >\n> > Any resolution to the http 1.1 issue?  We are using MS three\n> > version 2.0\n> > proxy servers in an array configured for reverse proxy to our\n> > web servers.\n> > The proxy servers are responding with http version 1.0 header to our\n> > application servers. I have searched the entier internet and\n> > the registry\n> > for this setting and no luck. Any ideas?\n> >\n> > Thanks,\n> >\n> > Craig Schoeberle\n> >\n> >\n>\n>\n\n\n\n", "id": "lists-012-14435088"}, {"subject": "problem with responses to postrequest", "content": "i've got a problem while responding to POST requests.\n\ni'm currently developing a small http-server to configure some files via a\nwebbrowser.\n\nwhile sending responses to GET requests everything works fine and the\nbrowser prints the output. but if i try to handle a post request i encounter\ndifferent reactions by netscape navigator and internet explorer.\n\nafter processing the content of the post-request i'm sending the\nhtml-output to the client and close the connection. this works fine with NN. but IE\ndoesn't show the page and gives an error-msg telling me that the connection\nhas been reset.\n\ni just don't get it why NN is doing fine and IE not.\n\nwhat's the difference between responses to GET and POST requests ?\n\nwhy is IE telling me errors and not showing the transmitted page ?\n\n\ngreetings,\nolli\n\n\nhere's some of the code:\n\nwhile ((conn = accept(sock,(struct sockaddr *) &addr,&addr_length)) >= 0)\n{\n /*\n  reading/parsing header etc.\n */\n\nif (strcmp(request->method,\"POST\") == 0)\n{\n putsock(conn,\"HTTP/1.0 200 OK\\n\");\n putsock(conn,\"Content-Type: text/plain\\n\\n\");\n putsock(conn,\"Netscape Navigator shows this.\\\n              Internet Explorer gives error\");\n}\n\n// here NN says \"Document Done\" and shows the page\n// but IE gives error msg \"Connection Reset\"\nclose(conn); \n}\n\n\n-- \nSent through GMX FreeMail - http://www.gmx.net\n\n\n\n", "id": "lists-012-14444332"}, {"subject": "RE: problem with responses to postrequest", "content": "> after processing the content of the post-request i'm sending\n> the html-output to the client and close the connection. this\n> works fine with NN. but IE doesn't show the page and gives an\n> error-msg telling me that the connection has been reset.\n\n> what's the difference between responses to GET and POST\n> requests ?\n\nThere isn't any.\n \n> why is IE telling me errors and not showing the transmitted page ?\n \n> if (strcmp(request->method,\"POST\") == 0)\n> {\n>  putsock(conn,\"HTTP/1.0 200 OK\\n\");\n>  putsock(conn,\"Content-Type: text/plain\\n\\n\");\n>  putsock(conn,\"Netscape Navigator shows this.\\\n>               Internet Explorer gives error\");\n> }\n> \n> // here NN says \"Document Done\" and shows the page\n> // but IE gives error msg \"Connection Reset\"\n> close(conn); \n> }\n\nThose \"\\n\" should really be \"\\r\\n\", but that isn't your problem.\n\nYour TCP is probably (incorrectly) sending an RST when you do the\nclose.  If the browser hasn't read all of the response, then any\nunread part will be discarded and produce the error you are\nseeing.  The difference between the browsers could be timing or\ndifferences in the way they do i/o.\n\nIf your platform supports it, use shutdown() rather than close().\n\n--\nScott Lawrence         Architect             <lawrence@agranat.com>\nVirata             http://www.virata.com/         http://emweb.com/\n\n\n\n", "id": "lists-012-14453183"}, {"subject": "Re: problem with responses to postrequest", "content": "Fred Bohle@NEON\n08/25/2000 08:26 AM\n\nOlli,\n     You didn't mention http version in the browsers.  If Netscape\nis using 1.0 and IE is using 1.1, you may have a problem with omitting\nthe Connection: header.  I suggest adding a header line with\n\nConnection: close\n\nto tell the browser that you are closing the connection.  The addition of\na shutdown() call would help too.\n\n     When all else fails, read the RFCs 1945 and 2616, available\nfrom www.ietf.org.  Reading the Apache source code is useful too.\n\nFred\n\n\n\n\n\nOSeemann@gmx.net on 08/25/2000 05:32:15 AM\n\nTo:   http-wg@cuckoo.hpl.hp.com\ncc:    (bcc: Fred Bohle/Dev/Neon)\n\nSubject:  problem with responses to post-requests\n\n\n\n\ni've got a problem while responding to POST requests.\n\ni'm currently developing a small http-server to configure some files via a\nwebbrowser.\n\nwhile sending responses to GET requests everything works fine and the\nbrowser prints the output. but if i try to handle a post request i\nencounter\ndifferent reactions by netscape navigator and internet explorer.\n\nafter processing the content of the post-request i'm sending the\nhtml-output to the client and close the connection. this works fine with\nNN. but IE\ndoesn't show the page and gives an error-msg telling me that the connection\nhas been reset.\n\ni just don't get it why NN is doing fine and IE not.\n\nwhat's the difference between responses to GET and POST requests ?\n\nwhy is IE telling me errors and not showing the transmitted page ?\n\n\ngreetings,\nolli\n\n\nhere's some of the code:\n\nwhile ((conn = accept(sock,(struct sockaddr *) &addr,&addr_length)) >= 0)\n{\n /*\n  reading/parsing header etc.\n */\n\nif (strcmp(request->method,\"POST\") == 0)\n{\n putsock(conn,\"HTTP/1.0 200 OK\\n\");\n putsock(conn,\"Content-Type: text/plain\\n\\n\");\n putsock(conn,\"Netscape Navigator shows this.\\\n              Internet Explorer gives error\");\n}\n\n// here NN says \"Document Done\" and shows the page\n// but IE gives error msg \"Connection Reset\"\nclose(conn);\n}\n\n\n--\nSent through GMX FreeMail - http://www.gmx.net\n\n\n\n", "id": "lists-012-14462386"}, {"subject": "Re: problem with responses to postrequest", "content": "OSeemann@gmx.net wrote:\n\n> i've got a problem while responding to POST requests.\n>\n> i'm currently developing a small http-server to configure some files via a\n> webbrowser.\n\nYou might find that the easiest way to be compliant is to build on top of an existing\nWeb server.  If you use Apache, you can strip out the parts you don't need.\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |The plural of mongoose is polygoose.         |\n|francis@ecal.com|                                             |\n\\==============================================================/\n\n\n\n", "id": "lists-012-14472339"}, {"subject": "Use Proxy 50", "content": "RFC 2616 does not state whether a status code of 305 should include\nan entity or not.\n\nI am assuming that it SHOULD include an entity just like the other\n3xx status codes except 304.\n\nIs this a correct assumption?\n\n\n\n", "id": "lists-012-14480602"}, {"subject": "RE: comments on draft-ietf-http-authentication01.tx", "content": "> ----------\n> From: Dave Kristol[SMTP:dmk@bell-labs.com]\n> Sent: Friday, March 27, 1998 11:15 AM\n> To: Paul Leach\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: comments on draft-ietf-http-authentication-01.txt\n> \n> > \n> > Even then, I think I'd call it the \"assumed protection space\" -- i.e. is\n> > what the client believes is protected by that set of credentials, until\n> it\n> > discovers otherwise by either gettin a 401 on a URL it thought was in\n> that\n> > sapce, or being prompted for credentials in the same realm for a URL\n> that it\n> > thought wasn't in that space. Naming suggestions welcomed.\n> \n> You could just take the Humpty Dumpty approach:  \"protection space\"\n> means what it's defined to mean.  Changing the term now may be hard.\n> \nOK. I'll just clarify what \"proection space\" means -- the space that the\nclient may assume the same credentials are valid, until the server indicates\notherwise.\n\n> > [...]\n> \n> > > > > Sect. 3.2.3, The Authentication-Info Header\n> > > > >     What should a client do if the rspauth=response-digest\n> information\n> > > > >     is wrong?\n> > > > >\n> > > > Not accept the response.\n> > >\n> > > How does a client, which has already read a response, \"not accept\n> > > [it]\"?  I'm picking nits here, true.  Does it mean that a browser\n> would\n> > > show the user an error saying that the received response was in error?\n> > >\n> > That's what I'd do. But we aren't supposed to prescribe UI behavior...\n> \n> Maybe not in detail, but I suspect you would like the browser to inform\n> the user.  That doesn't seem like a onerous prescription.\n> \nYup. I'll put in something like: If the response-digest does not check, then\nthe user agent MUST NOT process it in the same way as if it were correct,\nand MUST indicate a security failure in an appropriate way.\n\n> > \n> > > > >\n> > > > >     Isn't there the risk that an intervening proxy could change\n> the\n> > > > >     status code?\n> > > > >       ... Authorization header for the request, A2 is\n> > > > >          A2       = Status-Code \":\" digest-uri-value\n> > > > >       and if \"qop=auth-int\", then A2 is\n> > > > >          A2       = Status-Code \":\" digest-uri-value \":\"\n> > > H(entity-body)\n> > > > >\n> > > > Well, the status code isn't a header, but there's a general\n> proscription\n> > > > against needlessly changing headers in 13.5.2. Maybe the status line\n> > > > contents should be explicitly added to that list.\n> > >\n> > > Is it possible to say a proxy can't change its status code?  Suppose\n> you\n> > > have browser B, caching proxy P, origin server S.  (I'm sure you'll\n> tell\n> > > me if this example is way off base.)  B requests object X, which it\n> does\n> > > not have in its local cache.  P has the object, but the object has\n> > > Cache-Control: must-revalidate.  P sends a *conditional* request to S.\n> > > After S asks for credentials, which response P passes to B, B asks\n> again\n> > > for the X  S responds with 302 and (is this right?  possible?) an\n> > > Authentication-Info header.  The A-I header would presumably contain a\n> > > digest of the \"302\", but the proxy would return a 200 and supply X to\n> B,\n> > > along with A-I.  B would be unable to match the A-I header and the\n> > > response and would assume the response is bogus.\n> > >\n> > No, I think this is right on target (except it's 304 Not Modified, not\n> 302).\n> \n> Oops.\n> \n> > I think this is an important case to make work, for efficiency reasons.\n> If I\n> > were implementing an origin server, what I'd do, regardless of what the\n> spec\n> > says, is to calculate the response-digest assuming the proxy will turn\n> the\n> > status code into 200. It violates the letter of the law but not the\n> spirit.\n> > The question that I can't figure out off the top of my head is: how well\n> > would that work?\n> \n> Couldn't a client, or proxy, make a Range request?  In that case the\n> response could be 206.\n> \nIf the proxy passes the range request toward the origin server, just adding\nthe conditional, then the origin server would know to use 206 in the\nresponse digest. If the proxy converts an ordinary request in a range\nrequest, then that would be a problem. Given that the proxy can't return\nanything from cache for a request with Authorization headers unless it has\nre-validated, I don't know that it would have occaision to do range\nrequests... I don't know off the top of my head -- Jeff?\n\n>   Does sending the status code in response-auth\n> really add that much value?\n> \nIt's hard to say. How much damage/confusion could the attacker cause by\nchanging the status code? For example, it could change a 200 OK into a 304\nNot Modified, causing the client to think it's local copy was up to date --\nthat could cause large financial losses.\n\nThe idea in communications seciurity is just to make sure everything is\nauthenticate, so you don't have to be very clever and think of the worst an\ninfinitely smart, infinitely malicious attacker can do.\n\nPaul\n\n\n\n", "id": "lists-012-1448574"}, {"subject": "Use Proxy 305 (corrected", "content": "RFC 2616 does not state whether a status code of 305 should include\nan entity or not.\n\nI am assuming that it SHOULD include an entity just like the other\n3xx status codes except 304.\n\nIs this a correct assumption?\n\n\n\n", "id": "lists-012-14486883"}, {"subject": "Re: problem with responses to postrequest", "content": "do you have a test site where I could see this and also do you have any \nnetwork traces of the problem?  I'd like to see if there is a problem with \nthe way IE handles this.\n\n\n>From: OSeemann@gmx.net\n>To: http-wg@cuckoo.hpl.hp.com\n>Subject: problem with responses to post-requests\n>Date: Fri, 25 Aug 2000 12:32:15 +0200 (MEST)\n>\n>i've got a problem while responding to POST requests.\n>\n>i'm currently developing a small http-server to configure some files via a\n>webbrowser.\n>\n>while sending responses to GET requests everything works fine and the\n>browser prints the output. but if i try to handle a post request i \n>encounter\n>different reactions by netscape navigator and internet explorer.\n>\n>after processing the content of the post-request i'm sending the\n>html-output to the client and close the connection. this works fine with \n>NN. but IE\n>doesn't show the page and gives an error-msg telling me that the connection\n>has been reset.\n>\n>i just don't get it why NN is doing fine and IE not.\n>\n>what's the difference between responses to GET and POST requests ?\n>\n>why is IE telling me errors and not showing the transmitted page ?\n>\n>\n>greetings,\n>olli\n>\n>\n>here's some of the code:\n>\n>while ((conn = accept(sock,(struct sockaddr *) &addr,&addr_length)) >= 0)\n>{\n>  /*\n>   reading/parsing header etc.\n>  */\n>\n>if (strcmp(request->method,\"POST\") == 0)\n>{\n>  putsock(conn,\"HTTP/1.0 200 OK\\n\");\n>  putsock(conn,\"Content-Type: text/plain\\n\\n\");\n>  putsock(conn,\"Netscape Navigator shows this.\\\n>               Internet Explorer gives error\");\n>}\n>\n>// here NN says \"Document Done\" and shows the page\n>// but IE gives error msg \"Connection Reset\"\n>close(conn);\n>}\n>\n>\n>--\n>Sent through GMX FreeMail - http://www.gmx.net\n>\n\n________________________________________________________________________\nGet Your Private, Free E-mail from MSN Hotmail at http://www.hotmail.com\n\n\n\n", "id": "lists-012-14493998"}, {"subject": "nit on IETF HTTPWG charter pag", "content": "The following (erroneous) milestones appear on the HTTP-WG's charter page,\n<http://www.ietf.org/html.charters/http-charter.html>:\n\nJun 96Review additional features for HTTP/1.2\n\nOct 96Submit HTTP/1.2 to IESG for consideration as a Proposed Standard.\n\nI realize the milestones are wildly incorrect, but there is no HTTP/1.2 (yet),\nand these references to it may lead someone to go digging around to try to find\nit.\n\nDave Kristol\n\n\n\n", "id": "lists-012-14503811"}, {"subject": "Re: Use Proxy 305 (corrected", "content": "In general all that is expected with a '305 Use Proxy'  response is a\nproxy  name in the 'Location' field. Why would you want to return an\nentity with the response?\n\nSumanth\n\n\"John C. Mallery\" wrote:\n\n> RFC 2616 does not state whether a status code of 305 should include\n> an entity or not.\n>\n> I am assuming that it SHOULD include an entity just like the other\n> 3xx status codes except 304.\n>\n> Is this a correct assumption?\n\n\n\n", "id": "lists-012-14510509"}, {"subject": "RE: problem with responses to postrequest", "content": "Are you sure \"\\n\" (C++) represents CRLF or only a CR???\n\nI thought it represents only the CR. Maybe this is the problem, because\nRFC2616 requires you to send only CRLF in the headers and\nstatus/'command'-line.\n\n- Joris\n\n> -----Original Message-----\n> From: OSeemann@gmx.net [mailto:OSeemann@gmx.net]\n> Sent: vrijdag 25 augustus 2000 12:32\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: problem with responses to post-requests\n>\n>\n> i've got a problem while responding to POST requests.\n>\n> i'm currently developing a small http-server to configure\n> some files via a\n> webbrowser.\n>\n> while sending responses to GET requests everything works fine and the\n> browser prints the output. but if i try to handle a post\n> request i encounter\n> different reactions by netscape navigator and internet explorer.\n>\n> after processing the content of the post-request i'm sending the\n> html-output to the client and close the connection. this\n> works fine with NN. but IE\n> doesn't show the page and gives an error-msg telling me that\n> the connection\n> has been reset.\n>\n> i just don't get it why NN is doing fine and IE not.\n>\n> what's the difference between responses to GET and POST requests ?\n>\n> why is IE telling me errors and not showing the transmitted page ?\n>\n>\n> greetings,\n> olli\n>\n>\n> here's some of the code:\n>\n> while ((conn = accept(sock,(struct sockaddr *)\n> &addr,&addr_length)) >= 0)\n> {\n>  /*\n>   reading/parsing header etc.\n>  */\n>\n> if (strcmp(request->method,\"POST\") == 0)\n> {\n>  putsock(conn,\"HTTP/1.0 200 OK\\n\");\n>  putsock(conn,\"Content-Type: text/plain\\n\\n\");\n>  putsock(conn,\"Netscape Navigator shows this.\\\n>               Internet Explorer gives error\");\n> }\n>\n> // here NN says \"Document Done\" and shows the page\n> // but IE gives error msg \"Connection Reset\"\n> close(conn);\n> }\n>\n>\n> --\n> Sent through GMX FreeMail - http://www.gmx.net\n>\n>\n\n\n\n", "id": "lists-012-14517990"}, {"subject": "RE: Use Proxy 305 (corrected", "content": "> -----Original Message-----\n> From: John C. Mallery [mailto:jcma@ai.mit.edu]\n> Sent: vrijdag 25 augustus 2000 21:22\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Use Proxy 305 (corrected)\n>\n>\n> RFC 2616 does not state whether a status code of 305 should include\n> an entity or not.\n\nYou missed a part. 305 (Use Proxy) MUST include a location header pointing\nto the proxy to be used using a URI.\n\n>\n> I am assuming that it SHOULD include an entity just like the other\n> 3xx status codes except 304.\n\n304 MUST include the date header.\nOr, if clock-less, it MUST include:\nETag and/or Content-Location\nExpires, Cache-Control, and/or Vary\n\n>\n> Is this a correct assumption?\n>\n>\n\nYou'd better read RFC2616 once again, and you see that you make the\nincorrect assumptions.\n\nAn other possibility is that you mixed the body and entity. Entity\nrepresents the headers and the body the actual data (e.g. HTML document).\n\nThe body is not needed, but may be included. Here is no information about.\nMS-IIS send a body with a e.g. 301 response. Other servers may not do\nthis...\n\n- Joris\n\n\n\n", "id": "lists-012-14527652"}, {"subject": "Re: Use Proxy 305 (corrected", "content": ">RFC 2616 does not state whether a status code of 305 should include\n>an entity or not.\n>\n>I am assuming that it SHOULD include an entity just like the other\n>3xx status codes except 304.\n>\n>Is this a correct assumption?\n\nThe spec is silent on the issue of sending entity bodies with a 305.\nI don't recall whether it is silent on purpose -- probably not.\nWhether your assumption above is correct depends a bit on whether you\nare writing client or server software.\n\n1) if you are writing a client that gets 305 responses, you cannot\nassume anything about an entity body being present, so you must be\nprepared to handle either case.\n\n2) if you are writing a server that sends 305 responses, the most\nsensible thing to do, in my opinion, is to include a short HTML entity\nbody with human-readable instructions about using a proxy.\n\nKoen.\n\n\n\n", "id": "lists-012-14536473"}, {"subject": "Proxyconnectio", "content": "Can anybody give me a url to some documentation on this header,\nand especially, how to get http 1.0 clients to do some rudimentary\npersistence on requests through a 1.1 proxy?\n\n\n\n", "id": "lists-012-14619304"}, {"subject": "RE: Proxyconnectio", "content": "> -----Original Message-----\n> From: John C. Mallery [mailto:jcma@ai.mit.edu]\n> Sent: zaterdag 2 september 2000 4:20\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Proxy-connection\n>\n>\n> Can anybody give me a url to some documentation on this header,\n> and especially, how to get http 1.0 clients to do some rudimentary\n> persistence on requests through a 1.1 proxy?\n\nRFC2616 also explains this header. It is actually the same as the connection\nheader, but specially for proxies.\nHowever I don't know why they needed Proxy-Connection, maybe because some\nproxies did not know/understand this header and have the orgin server not to\nrespond to it.\n\n* HTTP/1.0 default closes the connection, unless explicitly requested by the\nclient to keep the line open (alive).\n* HTTP/1.1 keeps the connection alive, unless explicitly mentioned by the\nclient/server to close the connection. Maybe that some responces require the\nconnecton to be closed.\n\nI thought there is another RFC handling persistent connections for HTTP/1.0\n(extension to the standard). Maybe it doesn't exist, but I think it\ndoes.....\n\n- Joris\n\n\n\n", "id": "lists-012-14625906"}, {"subject": "Note on interoperability reports..", "content": "For those of you who are sending in interoperability reports, thanks.\n\nFor those of you who have not yet sent one in who have HTTP/1.1 \nimplementations, we encourage you to do so as soon as possible.  See: \nhttp://www.w3.org/Protocols/HTTP/Forum/Reports/\n\nPlease NOTE: for the purposes of the IETF, a feature is tested if you've \nimplemented and tested it.  It does NOT have to be in shipping or otherwise \navailabile software. (so much the better, of course).  This is a common \nconfusion.\n\nThe point of the interoperability report is to show that the protocol\nhas been tested in all regards, not a status report on what is in current\nshipping product.\n\n- Jim\n\nTo quote from BCP 9, The Internet Standards Process:\n\n4.1.2  Draft Standard\n\n   A specification from which at least two independent and interoperable\n   implementations from different code bases have been developed, and\n   for which sufficient successful operational experience has been\n   obtained, may be elevated to the \"Draft Standard\" level.  For the\n   purposes of this section, \"interoperable\" means to be functionally\n   equivalent or interchangeable components of the system or process in\n   which they are used.  If patented or otherwise controlled technology\n   is required for implementation, the separate implementations must\n   also have resulted from separate exercise of the licensing process.\n   Elevation to Draft Standard is a major advance in status, indicating\n   a strong belief that the specification is mature and will be useful.\n\n   The requirement for at least two independent and interoperable\n   implementations applies to all of the options and features of the\n   specification.  In cases in which one or more options or features\n   have not been demonstrated in at least two interoperable\n   implementations, the specification may advance to the Draft Standard\n   level only if those options or features are removed.\n\n   The Working Group chair is responsible for documenting the specific\n   implementations which qualify the specification for Draft or Internet\n   Standard status along with documentation about testing of the\n   interoperation of these implementations.  The documentation must\n   include information about the support of each of the individual\n   options and features.  This documentation should be submitted to the\n   Area Director with the protocol action request. (see Section 6)\n\n   A Draft Standard must be well-understood and known to be quite\n   stable, both in its semantics and as a basis for developing an\n   implementation.  A Draft Standard may still require additional or\n   more widespread field experience, since it is possible for\n   implementations based on Draft Standard specifications to demonstrate\n   unforeseen behavior when subjected to large-scale use in production\n   environments.\n\n   A Draft Standard is normally considered to be a final specification,\n   and changes are likely to be made only to solve specific problems\n   encountered.  In most circumstances, it is reasonable for vendors to\n   deploy implementations of Draft Standards into a disruption sensitive\n   environment.\n\n \n\n\n\n", "id": "lists-012-1462941"}, {"subject": "Re: Proxyconnectio", "content": "On Fri, Sep 01, 2000 at 10:19:32PM -0400, John C. Mallery wrote:\n> Can anybody give me a url to some documentation on this header,\n> and especially, how to get http 1.0 clients to do some rudimentary\n> persistence on requests through a 1.1 proxy?\n\nI don't know of any specs. But basically it works just like the ol'\n'Connection: keep-alive' header, except that it only applies to\nproxies. Of course it has the same basic problem as the original\nConnection header (see section 19.6.2 in rfc 2616) in that if you\nhave two proxies in series and the first one doesn't understand\nthe Proxy-Connection header and therefore forwards it to the second\nproxy which does understand it, you'll have a mess.\n\nAnyway, if you're writing a 1.1 proxy then you can certainly use the\nProxy-Connection header on the client connection side (but only if the\nclient is 1.0, of course), with the caveat that if that client is\nactually a 1.0 proxy which doesn't understand the Proxy-Connection then\nyou're in trouble. I'm not sure if I understand your \"how to get 1.0\nclients to ...\" - other than sending back Proxy-Connection headers if\nthe client sent one, and making sure a Content-Length header is present\nin the response as often as possible (this may mean generating it if\npossible), there's nothing you can do.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-14634442"}, {"subject": "Legal token", "content": "Can anyone tell me if the following is a legal header ofr a response\nfrom a web server?\n\n\n   *** [tid=10e 108] Receiving response ( 30/8/2000 14:59:10 )\n\n   HTTP/1.1 200 OK\n   Server: Netscape-Enterprise/4.0\n   Date: Wed, 30 Aug 2000 19:02:26 GMT\n   Content-length: 148\n   Content-type: image/gif\n   Connection: Close\n   Connection: keep-alive\n\n\nSpecifically, it is the 'Connection: Close' followed by 'Connection:\nkeep-alive' that is in question.\n\nThanks in advance\n\n\nJim Witt\non-site at EC Cubed\n\n\n\n", "id": "lists-012-14642922"}, {"subject": "Re: The future of HTT", "content": "Message-ID: <3998006B.2201725B@canada.sun.com>\nDate: Mon, 14 Aug 2000 10:21:31 -0400\nFrom: Mark Baker <mark.baker@canada.sun.com>\nTo: xml-dist-app@w3.org\nCC: http-futures@bluescreen.org\nSubject: The future of HTTP\n\nHi all,\n\nAt the last IETF in Pittsburgh, a group of us got together and held a\n\"pre-BOF\", after our request for an Apps area BOF was denied due to lack\nof bandwidth.  At the end of this message is the description of the\npre-BOF that we posted.\n\nAll but one person who attended was an active WAPforum participant,\nreflecting the need that the wireless community feels for some more work\nto be done on HTTP, but at the same time identifying the need for\nparticipation from other communities.\n\nWe've just set up a mailing list, http-futures@bluescreen.org\n\nTo subscribe, send mail to majordomo@bluescreen.org with \"subscribe\nhttp-futures\" in the BODY\n\nArchives are at http://www.avogadro.com/ietf/http-futures\n\n==snip==\n\nHTTP futures discussion/pre-BOF !!!!!!!!!!!!!!!!!!\n\nThis discussion will focus on issues related to HTTP at the edge of the\nnetwork. In particular, the Internet at large, and HTTP in particular\nare\nbeing stretched in new or at least more intense ways by the advent of\nmyriad\ndevices or appliances, and by the related shift in Internet connectivity\nfrom wired connections to a higher number of wireless links (personal,\nlocal\nand wide-area).\n\nThis brings renewed and perhaps urgent interest to enhance HTTP.\n\nThis discussion will draw upon lessons learned from the HTTP working\ngroup\nas well as from the past activites on HTTP-NG.\n\nWe plan to address:\n   1. Creation of a list of requirements.\n   2. Creation of a list of issues which different communities\n      have with respect to HTTP1.1.\n   3. Discussion of whether HTTP1.1 is:\n        a. Good enough as is, so leave it as is.\n        b. Almost good enough, with a few fixes, so reopen HTTP\n   1.1 to address the list of current shortcomings.\n        c. Launch a new effort for a major revision/redesign of\n   HTTP. In this case, there are several options:\n- Reviving HTTP-NG and pursue the standardization of something heavily\nbased\non it.\n- Basing this new protocol on something else (BLOCKS comes to mind).\n[snip]\n\nMB\n\n\n\n----------------------------------------------------------------------------\n----\n\n\n\n", "id": "lists-012-14650080"}, {"subject": "Re: Legal token", "content": ">Can anyone tell me if the following is a legal header ofr a response\n>from a web server?\n>\n>\n>   *** [tid=10e 108] Receiving response ( 30/8/2000 14:59:10 )\n>\n>   HTTP/1.1 200 OK\n>   Server: Netscape-Enterprise/4.0\n>   Date: Wed, 30 Aug 2000 19:02:26 GMT\n>   Content-length: 148\n>   Content-type: image/gif\n>   Connection: Close\n>   Connection: keep-alive\n>\n>\n>Specifically, it is the 'Connection: Close' followed by 'Connection:\n>keep-alive' that is in question.\n\nInteresting combination of headers.  Digging through the specs, section\n19.7.1 of RFC2068 seems to allow the sending of `Connection:\nkeep-alive' to initiate a persistent connection with some legacy\nHTTP/1.0 clients.  The `Connection: Close' at the same time forces a\nnon-persistent connection in the case that the client is a 1.1 client.\nNothing seems to forbid the use of both at the same time.\n\nSo I think it is legal, even unambiguous.  But the semantics is\nstrange, so it probably reflects a bug in the server.\n\n>\n>Thanks in advance\n>\n>\n>Jim Witt\n>on-site at EC Cubed\n\nKoen.\n\n\n\n", "id": "lists-012-14660669"}, {"subject": "Re: Proxyconnectio", "content": "By experimentation, I found that returning the usual\nconnection and keep-alive headers works for Netscape and IE.\n\n\nAt 14:22 -0700 09-02-2000, Life is hard, and then you die wrote:\n>On Fri, Sep 01, 2000 at 10:19:32PM -0400, John C. Mallery wrote:\n>> Can anybody give me a url to some documentation on this header,\n>> and especially, how to get http 1.0 clients to do some rudimentary\n>> persistence on requests through a 1.1 proxy?\n>\n>I don't know of any specs. But basically it works just like the ol'\n>'Connection: keep-alive' header, except that it only applies to\n>proxies. Of course it has the same basic problem as the original\n>Connection header (see section 19.6.2 in rfc 2616) in that if you\n>have two proxies in series and the first one doesn't understand\n>the Proxy-Connection header and therefore forwards it to the second\n>proxy which does understand it, you'll have a mess.\n>\n>Anyway, if you're writing a 1.1 proxy then you can certainly use the\n>Proxy-Connection header on the client connection side (but only if the\n>client is 1.0, of course), with the caveat that if that client is\n>actually a 1.0 proxy which doesn't understand the Proxy-Connection then\n>you're in trouble. I'm not sure if I understand your \"how to get 1.0\n>clients to ...\" - other than sending back Proxy-Connection headers if\n>the client sent one, and making sure a Content-Length header is present\n>in the response as often as possible (this may mean generating it if\n>possible), there's nothing you can do.\n>\n>\n>  Cheers,\n>\n>  Ronald\n\n\n\n", "id": "lists-012-14668874"}, {"subject": "RE: Legal token", "content": "No, it isn't...\n\nYou can't close a connection and at the same time leave it open.....\nYou also can't close a door and leave it open at the same time......\n\n- Joris\n  -----Original Message-----\n  From: Jim Witt [mailto:JWitt@ECCubed.com]\n  Sent: dinsdag 5 september 2000 20:20\n  To: http-wg@cuckoo.hpl.hp.com\n  Subject: Legal tokens\n\n\n  Can anyone tell me if the following is a legal header ofr a response from\na web server?\n\n     *** [tid=10e 108] Receiving response ( 30/8/2000 14:59:10 )\n\n     HTTP/1.1 200 OK\n     Server: Netscape-Enterprise/4.0\n     Date: Wed, 30 Aug 2000 19:02:26 GMT\n     Content-length: 148\n     Content-type: image/gif\n     Connection: Close\n     Connection: keep-alive\n\n\n  Specifically, it is the 'Connection: Close' followed by 'Connection:\nkeep-alive' that is in question.\n\n  Thanks in advance\n\n\n  Jim Witt\n  on-site at EC Cubed\n\n\n\n", "id": "lists-012-14677200"}, {"subject": "RE: Legal token", "content": "If\n\nConnection: Close, keep-alive\n\nis legal, how about\n\nCache-Control: Public, Private, max-age=30, s-maxage=30\n\nThey are both buggy, and from my opinion not to be considered legal.\n\n\n- Joris\n\n> -----Original Message-----\n> From: Koen Holtman [mailto:koen@win.tue.nl]\n> Sent: woensdag 6 september 2000 7:25\n> To: Jim Witt\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Legal tokens\n> \n> \n> >Can anyone tell me if the following is a legal header ofr a response\n> >from a web server?\n> >\n> >\n> >   *** [tid=10e 108] Receiving response ( 30/8/2000 14:59:10 )\n> >\n> >   HTTP/1.1 200 OK\n> >   Server: Netscape-Enterprise/4.0\n> >   Date: Wed, 30 Aug 2000 19:02:26 GMT\n> >   Content-length: 148\n> >   Content-type: image/gif\n> >   Connection: Close\n> >   Connection: keep-alive\n> >\n> >\n> >Specifically, it is the 'Connection: Close' followed by 'Connection:\n> >keep-alive' that is in question.\n> \n> Interesting combination of headers.  Digging through the \n> specs, section\n> 19.7.1 of RFC2068 seems to allow the sending of `Connection:\n> keep-alive' to initiate a persistent connection with some legacy\n> HTTP/1.0 clients.  The `Connection: Close' at the same time forces a\n> non-persistent connection in the case that the client is a 1.1 client.\n> Nothing seems to forbid the use of both at the same time.\n> \n> So I think it is legal, even unambiguous.  But the semantics is\n> strange, so it probably reflects a bug in the server.\n> \n> >\n> >Thanks in advance\n> >\n> >\n> >Jim Witt\n> >on-site at EC Cubed\n> \n> Koen.\n> \n> \n\n\n\n", "id": "lists-012-14685168"}, {"subject": "Re: Legal token", "content": "Jim,\n\nJim Witt wrote:\n\n> Can anyone tell me if the following is a legal header ofr a response\n> from a web server?\n>\n>\n>    *** [tid=10e 108] Receiving response ( 30/8/2000 14:59:10 )\n>\n>    HTTP/1.1 200 OK\n>    Server: Netscape-Enterprise/4.0\n>    Date: Wed, 30 Aug 2000 19:02:26 GMT\n>    Content-length: 148\n>    Content-type: image/gif\n>    Connection: Close\n>    Connection: keep-alive\n>\n>\n> Specifically, it is the 'Connection: Close' followed by 'Connection:\n> keep-alive' that is in question.\n>\n> Thanks in advance\n>\n>\n> Jim Witt\n> on-site at EC Cubed\n\nI'd be interested in learning how you get our web server to generate\nsuch a reply, which certainly appears bogus. Are you using some\nthird-party plug-in or other server-side application by any chance ; or\nis this a response from one of the server built-in functions ?\n\nThanks.\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-14694218"}, {"subject": "Content-Encoding: gzip AND Content-Type: application/xjavascrip", "content": "Hello all...\n\nI have run into a problem with Netscape 4.x and some earlier versions of IE...\n\nThe following is the server info being generated by me\n\ntelnet www1 8050\nTrying 208............\nConnected to www1.......\nEscape character is '^]'.\nGET /...... HTTP/1.0\nAccept-Encoding: gzip\n\nHTTP/1.1 200 OK\nDate: Thu, 07 Sep 2000 03:09:40 GMT\nServer: Apache/1.3.12 (Unix) PHP/4.0.2 mod_ssl/2.6.4 OpenSSL/0.9.5a\nX-Powered-By: PHP/4.0.2\nExpires: Fri, 08 Sep 2000 03:09:43 GMT\nContent-Encoding: gzip\nConnection: close\nContent-Type: application/x-javascript\n\n\nWhen I use Netscape to retrieve this same file, I get errors about \nunrecognized characters in javascript... it isnt decoding the gzip and the \njs interpreter is decoding binary.\n\nHowever, if I remove the Content-Type: application/x-javascript header, \nnetscape does decode the gzip and display the file.  In order for me to use \nthis as a SRCed js file I NEED the Content-Type: application/x-javascript \nheader.\n\nIs there a header that is missing?? Does anyone know a workaround?? \nEssentially, i need to be able to send a gzip encoded javascript file and \nhave it interpretted.. This does work fine with IE 5. Is it just another \nexample of NS 4 blatant disregard for standards?? Or am I reading the \nstandard wrong and encodings of types other than html are not allowed?\n\nThank you\nJoe\n\n*******************************************************************\n\nFYI.... Please begin using my new email address\njoelink@joelink.net\n\njoelink@niu.edu will be phased out by Jan 1st 2001\n\nThank you\n\n*******************************************************************\n\n\n\n", "id": "lists-012-14701896"}, {"subject": "Re: Content-Encoding: gzip AND Content-Type: application/xjavascrip", "content": "Sounds like a bug in NS.\n\nI wouldn't say that IE is any better or worse; IIRC IE advertises that it\ncan do deflate, but doesn't support it correctly (if anyone can demonstrate\nthat it can, pls tell me - I'd love to get it to work).\n\nCheers,\n\n\n\nOn Wed, Sep 06, 2000 at 11:39:54PM -0500, Joseph Link wrote:\n> Hello all...\n> \n> I have run into a problem with Netscape 4.x and some earlier versions of IE...\n> \n> The following is the server info being generated by me\n> \n> telnet www1 8050\n> Trying 208............\n> Connected to www1.......\n> Escape character is '^]'.\n> GET /...... HTTP/1.0\n> Accept-Encoding: gzip\n> \n> HTTP/1.1 200 OK\n> Date: Thu, 07 Sep 2000 03:09:40 GMT\n> Server: Apache/1.3.12 (Unix) PHP/4.0.2 mod_ssl/2.6.4 OpenSSL/0.9.5a\n> X-Powered-By: PHP/4.0.2\n> Expires: Fri, 08 Sep 2000 03:09:43 GMT\n> Content-Encoding: gzip\n> Connection: close\n> Content-Type: application/x-javascript\n> \n> \n> When I use Netscape to retrieve this same file, I get errors about \n> unrecognized characters in javascript... it isnt decoding the gzip and the \n> js interpreter is decoding binary.\n> \n> However, if I remove the Content-Type: application/x-javascript header, \n> netscape does decode the gzip and display the file.  In order for me to use \n> this as a SRCed js file I NEED the Content-Type: application/x-javascript \n> header.\n> \n> Is there a header that is missing?? Does anyone know a workaround?? \n> Essentially, i need to be able to send a gzip encoded javascript file and \n> have it interpretted.. This does work fine with IE 5. Is it just another \n> example of NS 4 blatant disregard for standards?? Or am I reading the \n> standard wrong and encodings of types other than html are not allowed?\n> \n> Thank you\n> Joe\n> \n> *******************************************************************\n> \n> FYI.... Please begin using my new email address\n> joelink@joelink.net\n> \n> joelink@niu.edu will be phased out by Jan 1st 2001\n> \n> Thank you\n> \n> *******************************************************************\n> \n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-14711492"}, {"subject": "Re: Content-Encoding: gzip AND Content-Type:application/xjavascrip", "content": "Joseph Link wrote:\n\n> Is there a header that is missing?? Does anyone know a workaround??\n> Essentially, i need to be able to send a gzip encoded javascript file and\n> have it interpretted..\n\nI can think of a workaround on the client side, but it needs Java: use some\nuncompressed JavaScript that calls into Java to fetch the compressed file, then\nuses eval() to execute it.\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |Never mind the GUIs--Unix won't be for the   |\n|francis@ecal.com|masses until we fix backspace & delete.      |\n\\==============================================================/\n\n\n\n", "id": "lists-012-14721894"}, {"subject": "http-authentication01 comment", "content": "It would be _really_ nice to have a format other than plain text,\nhint, hint.  I doubt you compose this in notepad, and change bars are\nextremly helpful for last-call reviews, and the previous version had\nalternate formats.\n\nSection 3.6, Proxy-Authentication and Proxy-Authorization, references\nthese headers as 10.30 and 10.31.  In RFC 2068 and the current\ndraft (-03), this is 14.33 and 14.34.  Also, it makes reference to \"as\ndefined above in section 2.1\", which does not exist.\n\nInconsistency:\nHTTP-03 Section 14.33 says:\n\"Proxy-Authenticate SHOULD NOT be passed on\"\nAUTH-01 1.2 says:\n\"Both the Proxy-Authenticate and the Proxy-Authorization header fields\nare hop-by-hop headers\" (but HTTP-03 section 13.5.1 has no normative\nrequirements on existing hop-by-hop headers)\nAUTH-01 3.6 says:\n\"...Proxy-Authenticate... must not be passed on by proxies\"\n\nThe note that comprises the last paragraph of 3.6 applies to basic\nalso.\n\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-1472351"}, {"subject": "Re: HTTP/1.1: RFC2068 versus RFC2616, RFC1590 versus RFC204", "content": "* \"Alan J. Flavell\" <flavell@mail.cern.ch> wrote:\n| Greetings,\n|\n| In discussion on a German-language usenet group, Bjoern Hoehrmann\n| has pointed out a surprising discrepancy between RFC2068 and RFC2616.\n|\n| It was not entirely clear to me where this should be reported, but\n| your email address is on the issues list page at W3C, so maybe this is\n| the place to try first.\n\nAmazing coincidence :-) I was going to ask at the HTTP WG Mailing List when\nyour mail arrived. So let's ask there:\n\n| In section 3.7 \"Media Types\", the earlier RFC2068 refers correctly to\n| RFC2048 in relation to IANA registrations, but the later RFC still\n| refers to the obsoleted RFC1590.\n|\n| Looking at the history of these documents as noted at\n| http://www.w3.org/Protocols/History.html#HTTP11\n| one finds that _all_ of the drafts (even the one which is described as\n| having become RFC2068) are citing the obsolete RFC1590.  However,\n| RFC2068 itself had been corrected to cite RFC2048.\n|\n| As far as I could determine, the documents which set out the\n| differences between RFC2068 and the later drafts make no mention of\n| this difference - not even the ones which claim to show change-bars\n| >from RFC2068.  So it appears that the correction must have been\n| slipped-in to RFC2068 at the last moment - and somehow left no tracks\n| on the main drafting sequence.\n|\n| I don't see any mention of this on the HTTP/1.1 Specification Errata\n| page at\n| http://www.agranat.com/fs/public/lawrence/http_errata.html\n| either.\n|\n| Maybe an item should be added to the errata, calling for the\n| reference to RFC1590 to be corrected to RFC2048?\n\nbest regards,\n--\nBj?rn H?hrmann ^ mailto:bjoern@hoehrmann.de ^ http://www.bjoernsworld.de\nam Badedeich 7 ? Telefon: +49(0)4667/981ASK ? http://www.websitedev.de/\n25899 Dageb?ll # PGP Pub. KeyID: 0xA4357E78 # http://learn.to/quote +{i}\n    --- All I want for Christmas is well-formedness -- Evan Lenz ---\n\n\n\n", "id": "lists-012-14729880"}, {"subject": "Re: HTTP/1.1: RFC2068 versus RFC2616, RFC1590 versus RFC204", "content": "Generally, when an RFC gets prepared, one tries to catch references\nthat have been obsoleted, and the RFC editor also tries to do so as well.\n\nWe'll try to get it fixed for the full standard draft.\n- Jim\n\n--\nJim Gettys\nTechnology and Corporate Development\nCompaq Computer Corporation\njg@pa.dec.com\n\n\n\n", "id": "lists-012-14740351"}, {"subject": "RE: HTTP/1.1: RFC2068 versus RFC2616, RFC1590 versus RFC204", "content": "> In section 3.7 \"Media Types\", the earlier RFC2068 refers\n> correctly to RFC2048 in relation to IANA registrations, but the\n> later RFC still refers to the obsoleted RFC1590.\n\n> I don't see any mention of this on the HTTP/1.1 Specification\n> Errata page at\n> http://www.agranat.com/fs/public/lawrence/http_errata.html\n> either.\n\n> Maybe an item should be added to the errata, calling for the\n> reference to RFC1590 to be corrected to RFC2048?\n\nDone.\n \n http://purl.org/NET/http-errata#media-reg\n \n--\nScott Lawrence         Architect             <slawrence@virata.com>\nVirata             http://www.virata.com/         http://emweb.com/\n\n\n\n", "id": "lists-012-14749106"}, {"subject": "Re: Proxyconnectio", "content": "\"Joris Dobbelsteen\" <joris.dobbelsteen@mail.com> writes,\nre: the Proxy-connection header:\n    RFC2616 also explains this header. It is actually the same as the\n    connection header, but specially for proxies.  However I don't know\n    why they needed Proxy-Connection, maybe because some proxies did\n    not know/understand this header and have the orgin server not to\n    respond to it.\n    \nThis header is NOT discussed in RFC2616!\n\nAccording to the last discussion about this header in the HTTP-WG\narchive\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1999/0030.html\n\nthis is only a Netscape-specific header.\n\nRemember that an HTTP/1.1 system MUST NOT accept \"Connection\"\nheaders from HTTP/1.0 systems, because of the likelihood that\nthis was simply forwarded by a proxy that doesn't know what it\nmeans.  So HTTP/1.0 clients can't use \"Connection\", period.\n\nSome HTTP/1.0 clients used the \"Keep-Alive\" header to do\none-hop persistent connections, but for reasons explained in\nsection 19.6.2 of RFC2616, this can't be safely sent to\nproxies.  So Netscape (as far as I recall) introduced\n\"Proxy-Connection\" as a special-case workaround.  It is not\npart of HTTP/1.1.\n\n-Jeff\n\n\n\n", "id": "lists-012-14758317"}, {"subject": "Re: Legal token", "content": ">If\n>\n>Connection: Close, keep-alive\n>\n>is legal, how about\n>\n>Cache-Control: Public, Private, max-age=30, s-maxage=30\n>\n>They are both buggy, and from my opinion not to be considered legal.\n\nWell, when I say legal I mean legal according to what is written in\nthe specifications.  Your examples are clearly strange, but the\nspecifications do not disallow these strange examples, so they are\nlegal.\n\nNot that I would like to see these things on the wire, mind you.\n\n\n>\n>\n>- Joris\n\nKoen.\n\n\n\n", "id": "lists-012-14767004"}, {"subject": "RE: Proxyconnectio", "content": "> -----Original Message-----\n> From: Jeffrey Mogul [mailto:mogul@pa.dec.com]\n> Sent: dinsdag 12 september 2000 3:24\n> To: Joris Dobbelsteen\n> Cc: WWW WG (E-mail)\n> Subject: Re: Proxy-connection \n> \n> \n> \"Joris Dobbelsteen\" <joris.dobbelsteen@mail.com> writes,\n> re: the Proxy-connection header:\n>     RFC2616 also explains this header. It is actually the same as the\n>     connection header, but specially for proxies.  However I \n> don't know\n>     why they needed Proxy-Connection, maybe because some proxies did\n>     not know/understand this header and have the orgin server not to\n>     respond to it.\n>     \n> This header is NOT discussed in RFC2616!\n\nReviewed it, and you are right... I did mix it up with the existance\nof the proxy-authentication and proxy-authorization headers.\nAlso when developing for my own a proxy server, MSIE did also send the\nproxy-connection header.\n\n> \n> According to the last discussion about this header in the HTTP-WG\n> archive\n> http://www.ics.uci.edu/pub/ietf/http/hypermail/1999/0030.html\n> \n> this is only a Netscape-specific header.\n> \n> Remember that an HTTP/1.1 system MUST NOT accept \"Connection\"\n> headers from HTTP/1.0 systems, because of the likelihood that\n> this was simply forwarded by a proxy that doesn't know what it\n> means.  So HTTP/1.0 clients can't use \"Connection\", period.\n> \n> Some HTTP/1.0 clients used the \"Keep-Alive\" header to do\n> one-hop persistent connections, but for reasons explained in\n> section 19.6.2 of RFC2616, this can't be safely sent to\n> proxies.  So Netscape (as far as I recall) introduced\n> \"Proxy-Connection\" as a special-case workaround.  It is not\n> part of HTTP/1.1.\n\nSaid it all.....\nSo Proxy-Connection can be retired in HTTP/1.1 and be replaced with\nthe connection header.\n> \n> -Jeff\n> \n> \n\nWon't it be good for proxies if getting HTTP/1.0. For HTTP/1.1 look for\na connection header, and either discard the proxy-connection header, or\nuse it if the connection header is not present?\n\n- Joris\n\n\n\n", "id": "lists-012-14774816"}, {"subject": "RE: Legal token", "content": "> -----Original Message-----\n> From: Koen Holtman [mailto:koen@win.tue.nl]\n> Sent: dinsdag 12 september 2000 7:11\n> To: Joris Dobbelsteen\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Legal tokens\n>\n>\n> >If\n> >\n> >Connection: Close, keep-alive\n> >\n> >is legal, how about\n> >\n> >Cache-Control: Public, Private, max-age=30, s-maxage=30\n> >\n> >They are both buggy, and from my opinion not to be considered legal.\n>\n> Well, when I say legal I mean legal according to what is written in\n> the specifications.  Your examples are clearly strange, but the\n> specifications do not disallow these strange examples, so they are\n> legal.\n>\n> Not that I would like to see these things on the wire, mind you.\n>\n\nAgree,\n\nBut although it's not explicitly forbidden by the RFC, logic sense (you know\nwhat I mean) would forbid such constructions. The cache-control example was\nfrom another discussion, and Jeffrey Mogul (& John Strake) agreed such\nresponses should not be cached by a public cache, since this points to a\nbuggy server.\nBut not how do you explain illegal and legal responses?\nIf you have software that receives in a response the connection header(s)\nabove, you would suppose the header is buggy and cannot be used. And since\nit's not used by the software, can't we just say it's illegal?\nAnd since it's not in a request...\n\n>\n> >\n> >\n> >- Joris\n>\n> Koen.\n>\n>\n\n- Joris\n\n\n\n", "id": "lists-012-14784747"}, {"subject": "Re: Proxyconnectio", "content": "\"Joris Dobbelsteen\" <joris.dobbelsteen@mail.com> wrote:\n\n    Won't it be good for proxies if getting HTTP/1.0. For HTTP/1.1 look\n    for a connection header, and either discard the proxy-connection\n    header, or use it if the connection header is not present?\n    \nI don't have time to think through all of the possibilities, but\nI suspect that if you receive a message that contains BOTH\n\"Connection\" and \"Proxy-Connection\", then somewhere along the\nline, a mistake has been made.  (Unfortunately, without access\nto a formal spec for \"Proxy-Connection\", I'm not sure if we\ncan ever be definitive about it.)  In that case, the only\nsafe thing to do is to not use a persistent connection, since\nthere's a chance that one of the systems along the path isn't\nable to do the right thing.\n\n-Jeff\n\n\n\n", "id": "lists-012-14793703"}, {"subject": "Comments on draft-ietf-http-v11-spec-rev0", "content": "In reading draft-ietf-http-v11-spec-rev-03 I found some revisions\nwhich are potentially damaging.  I'll discuss them below.  I would\nlike to see these issues resolved before proceeding to draft standard.\n\n- Section 8.2.3:\n\nThis section now says:\n\n#  If a user agent sees the transport connection close before it\n#  receives a final response to its request, if the request method is\n#  idempotent (see section 9.1.2), the user agent SHOULD retry the\n#  request without user interaction.\n\nThe `befire it receives a final response' is a bit ambiguous, but I\nthink the most obvious reading is `before it has received the *entire*\nfinal response'.  By contrast, the text in 2068 has the SHOULD in a\nmuch more limited case:\n\n   ...and it sees the connection close\n   before receiving any status from the server, the client SHOULD retry\n   the request without user interaction....\n\nI don't know if the change from `any status' to `the final response'\nwas intentional or whether it was an editing mistake.  In any case I\nconsider the change to be quite dangerous: it requires a fully\ncompliant user agent to automatically retry, for example, a GET\nrequest yielding a 1 MB response if the connection closes halfway due\nto some unusual error condition.  The correct action for the user\nagent would be to alert the user of the unusual error.  An automatic\nretry could lead to very nasty problem scenarios.\n\nIn summary, I want the SHOULD retry condition to be restored to the\none in 2068.\n\n- Section 13.10:\n\nThis section introduces a new (as far as I can see) requirement:\n\n#  A cache that passes through requests for methods it does not understand\n#  should invalidate any entities referred to by the Request-URI.\n\nThis may seem like a good safety measure on the surface but I think\nthat it is in fact quite damaging.  First, designers of new methods\ncannot benefit much from the above rule because 1.0 and 2068 caches\nwill not adhere to it.  On the other hand, the new rule introduces a\nperformance penalty for new methods which do not in fact cause any\ninvalidation.  One such method would be M-GET, a GET extended with a\nmandatory extension, for example.  The performance penalty blocks\nimplied by the new rule makes certain ways of extending the protocol\ntoo expensive and thus shortens the lifetime of the 1.x suite.  I want\nthe requirement to be removed.\n\n\n- Section 14.2:\n\nrfc2068 had the sentence\n\n   The ISO-8859-1 character set can be assumed to be acceptable to all user\n   agents.\n\nbut this sentence has been deleted in the new draft!  I can't remember\nthat there was any rationale or discussion on the list for deleting\nit.  In any case, I think the deletion damages the protocol.  It is\nimportant to specify a charset which is always supported by all\nclients, else servers have no reliable way of sending fatal error\ndiagnostics in text/html entities.  The deletion of the above sentence\nalso makes the third paragraph in 14.2 slightly nonsensical.\n\nI know that the choice of ISO-8859-1 as the `always supported' charset\nhas been a subject of contention in the past, but that is no reason to\nomit specifying a fallback charset.  If implementation experience has\nshown that universally supporting ISO-8859-1 is too difficult, we can\ndiscuss changing the fallback charset to US-ASCII.\n\nIn short, the above sentence should be un-deleted.\n\n- Section 14.44:\n\nThis section introduces a new (as far as I can see) requirement:\n\n#  The \"*\" value MUST NOT be generated by a proxy server; it may only be\n#  generated by an origin server.\n\nI don't see any reason for having this requirement.  The general rule\nshould be that transparent proxies may never change or add a Vary\nheader (this is already implied elsewhere in the spec I believe), and\nthat non-transparent proxies can do whatever they want.\n\nBy the way, proxies which support the TCN protocol extension _will_\nsometimes generate \"*\", this is explicitly allowed by TCN.\n\nIn short, the requirement should be deleted.\n\n - - -\n\nIn addition to the above points, I found a potential problem which was\nalready present in 2068. It looks to me like there is a subtle\nrequirement on proxies buried in the following text from section 4.4:\n\n# 4.    If the message uses the media type \"multipart/byteranges\", and\n#       the transfer-length is not otherwise specified, then this self-\n#       delimiting media type defines the transfer-length. This media type\n#       MUST NOT be used unless the sender knows that the recipient can\n#       parse it; the presence in a request of a Range header with multiple\n#       byte-range specifiers implies that the client can parse\n#       multipart/byteranges responses.\n\nAs far as I can see, a 1.1 proxy will not delete an existing Range\nheader field when forwarding a request.  According to the last\nsentence above, any 1.1 proxy which forwards a request with a Range\nheader will be able to parse multipart/byteranges responses.  This\nseems to imply that any 1.1 proxy intended for use with\nbyterange-capable clients will always include code which can parse\nmultipart/byteranges responses to determine their length.\n\nWas this the original intention?  The start of 14.35.1 seems to imply\notherwise.  Has anybody tried sending a multipart/byteranges response\nwithout chunking and content-length through a 1.1 proxy which does not\nsupport byte range operations?\n\n\nKoen.\n\n\n\n", "id": "lists-012-1479645"}, {"subject": "Re: Legal token", "content": "koen@win.tue.nl (Koen Holtman) writes:\n\n    Well, when I say legal I mean legal according to what is written in\n    the specifications.  Your examples are clearly strange, but the\n    specifications do not disallow these strange examples, so they are\n    legal.\n\nJust to clarify: when we were writing the HTTP/1.1 spec, we made\na decision not to specifically list all of the possible non-compliant\ncombinations of headers and directives, because the combinatorics\nwould result in a huge list.\n\n(Please try to use words like \"compliant\" instead of \"legal.\")\n\nSo there are definitely cases which the spec does not explicitly\nsay \"you MUST NOT do this\", yet which any reasonable person who\nunderstood the spec would realize that these cases don't comply\nwith the intentions behind the spec.\n\nIn general, if you aren't sure that the spec allows some combination,\nthen it's a bad idea to send it, since you have no idea whether\nthe person who wrote the implementation at the receiving end\nunderstood things the same way that you did.\n\n-Jeff\n\n\n\n", "id": "lists-012-14801627"}, {"subject": "Last Call for comments on &quot;Delta encoding in HTTP&quot", "content": "After too many months and too many drafts, those of us who\nhave been working on a proposed specification for Delta\nEncoding in HTTP seem to have reached a consensus on the\nbasic design:\n\nTitle: Delta encoding in HTTP\nAuthor(s): J. Mogul, B. Krishnamurthy, F. Douglis, \n                          A. Feldmann, Y. Goland\nFilename: draft-mogul-http-delta-06.txt\nPages: 46\nDate: 25-Aug-00\n\n    Many HTTP requests cause the retrieval of slightly modified\n    instances of resources for which the client already has a\n    cache entry.  Research has shown that such modifying\n    updates are frequent, and that the modifications are\n    typically much smaller than the actual entity.  In such\n    cases, HTTP would make more efficient use of network\n    bandwidth if it could transfer a minimal description of the\n    changes, rather than the entire new instance of the\n    resource.  This is called 'delta encoding.'  This\n    document describes how delta encoding can be supported as a\n    compatible extension to HTTP/1.1.\n    \n    A URL for this Internet-Draft is:\n    http://www.ietf.org/internet-drafts/draft-mogul-http-delta-06.txt\n\nAlthough this draft is NOT a product of the HTTP WG, we would\nlike to give the WG the traditional two-week Last Call period\nfor comments before we ask the IESG to approve this draft as\na Proposed Standard.\n\nNOTE: we have removed from this document any support for\n\"Clustering\" and \"Templates\", which are now optional\nfeatures described in a separate draft:\n   http://www.ietf.org/internet-drafts/draft-mogul-http-dcluster-00.txt\nThis draft is NOT ready for submission to the IESG, and these\nfeatures are NOT up for discussion as part of this \"Last Call.\"\n\n-Jeff\n\n\n\n", "id": "lists-012-14809722"}, {"subject": "HTTP image transfe", "content": "I'm trying to implement an HTTP server but  I can not find in the RFCs\nany mention of how the images in an HTML document are transmitted.\nAnyone can help me ?\nHow are embedded images transmitted?\n\nThanks for your help.\n\nPascal\n\n\n\n", "id": "lists-012-14818003"}, {"subject": "Re: HTTP image transfe", "content": "\"Pascal FOURGUET\" <pascal.fourguet@leroy-autom.com> wrote:\n  > I'm trying to implement an HTTP server but  I can not find in the RFCs\n  > any mention of how the images in an HTML document are transmitted.\n  > Anyone can help me ?\n  > How are embedded images transmitted?\n\nThe client requests the top-level page (HTML document), which is a\n\"container document\" that includes links to embedded images.  After it\nreceives the response, the client then makes new requests for each of\nthe embedded images.\n\nThe server doesn't have to know that the top-level page has embedded\nimages.  It just receives requests, one by one, for the embedded\nimages, which it then has to satisfy.\n\nDave Kristol\n\n\n\n", "id": "lists-012-14825022"}, {"subject": "&quot;Via&quot; header fiel", "content": "Hi all.\n\nIf a proxy-cache delivers an entity without revalidation, does it has to send \nthe header field \"Via\" ?\n\nThanks.\n\nFr?d?ric LEDAIN (fledain@boostworks.com)\n--\nResearch & Development Engineer\nBoostWorks\n\n\n\n", "id": "lists-012-14832861"}, {"subject": "RE: &quot;Via&quot; header fiel", "content": "> If a proxy-cache delivers an entity without revalidation,\n> does it has to send\n> the header field \"Via\" ?\n\nYes.  The purpose of 'Via' is to allow the end systems to detect the\nidentity and versions of any proxies - it has nothing to do with\nvalidation.\n\n--\nScott Lawrence         Architect             <lawrence@agranat.com>\nVirata             http://www.virata.com/         http://emweb.com/\n\n\n\n\n", "id": "lists-012-14840287"}, {"subject": "Re: &quot;Via&quot; header fiel", "content": "    If a proxy-cache delivers an entity without revalidation, does it\n    has to send the header field \"Via\" ?\n    \nFirst of all, if the message arrived at the proxy with a \"Via\"\nheader, it wouldn't make sense (under any interpretation of\nthe spec) for the proxy to delete the Via header.  So presumably\nwhat you really meant to ask was \ndoes a proxy add its identity+protocol information to\nthe Via header (or add a new Via header if one is not\nalready present)?\n\nThe spec (section 14.45) says:\n   The Via general-header field MUST be used by gateways and proxies to\n   indicate the intermediate protocols and recipients between the user\n   agent and the server on requests, and between the origin server and\n   the client on responses.\n\nIf your proxy has the response in its cache, then obviously\nyour proxy is an \"intermediate ... recipient\" of the message,\nand therefore MUST use the Via header.  The spec makes no\nexception for cached responses.\n\nIf you think about it, the Via mechanism would be useless if\na proxy did not add itself when returning a cached response.\nThe paragraph I quoted above goes on to say:\n      It is analogous to the \"Received\" field of\n   RFC 822 [9] and is intended to be used for tracking message forwards,\n   avoiding request loops, and identifying the protocol capabilities of\n   all senders along the request/response chain.\n\nIf it only appeared on non-cached messages, it would not be\nvery useful in tracking down problems with caching proxies,\nwould it?\n\n-Jeff\n\n\n\n", "id": "lists-012-14848333"}, {"subject": "Conformance Test for HTTP 1.", "content": "Hi All,\n\nI have been looking around for a HTTP 1.1 Conformance test, or sombody\nthat\ncan perform such a test. I have been unable to find anything.\n\nIt would be gretaly appreciated if anybody can provide me with any\ninformation\nreagrding HTTP 1.1 conformance testing. I have looked at the WC3 site,\nthey do not have HTTP 1.1 Conformance test, instead they have\nPerformance tests.\n\nRegards,\n\nKristan\n\n\n\n", "id": "lists-012-14856668"}, {"subject": "monitored???", "content": "I am having to resend this coz no one replied....\n\ndoes this mean the connection is monitored????\n\n----Header Recv----\nHTTP/1.0 206 Partial Content\nDate: Mon, 02 Oct 2000 18:32:23 GMT\nServer: Apache/1.3.9 (Unix) Debian/GNU PHP/4.0b3\nLast-Modified: Sun, 01 Aug 1999 17:48:06 GMT\nETag: \"c6406-38af05-37a48856\"\nAccept-Ranges: bytes\nContent-Length: 1416946\nContent-Range: bytes 2297875-3714820/3714821\nContent-Type: application/octet-stream\nX-Cache: MISS from xxxxx\nX-Cache-Lookup: MISS from xxxxx:3128\nConnection: keep-alive\n\nRamana Tadepalli\nSanskrut Software Systems Pvt Ltd\nramana@rediffmail.com\nramanatadepalli@hotmail.com\nrtadepalli@yahoo.com\nYou don't have to know how the computer works, just how to work the\ncomputer.\n\n\n\n", "id": "lists-012-14864699"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "There's been a lot of discussion about HTTP compliance tests over the past\nfew months, but it hasn't gotten anywhere as of yet. Several vendors have\ntentatively said they might consider releasing their internally-developed\ntools if other vendors were also, but nothing has happened there yet.\n\nI also tried to get the WREC working group interested in this in Pittsburgh,\nbut the informal consensus there was that it wasn't appropriate for IETF to\nproduce such a tool.\n\nI've lately been considering starting discussion of development of something\nwithin the W3C, as it was involved in the development of the HTTP, and has\nan established history of developing similar tools (although I'm not sure if\nW3C can formally commit resources).\n\nIf anyone has any thoughts about this, please share them, because I'd like\nto get this moving.\n\nCheers,\n\n\n\n\nOn Thu, Oct 05, 2000 at 11:08:13AM +1100, Kristan Vingrys wrote:\n> Hi All,\n> \n> I have been looking around for a HTTP 1.1 Conformance test, or sombody\n> that\n> can perform such a test. I have been unable to find anything.\n> \n> It would be gretaly appreciated if anybody can provide me with any\n> information\n> reagrding HTTP 1.1 conformance testing. I have looked at the WC3 site,\n> they do not have HTTP 1.1 Conformance test, instead they have\n> Performance tests.\n> \n> Regards,\n> \n> Kristan\n> \n> \n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-14872772"}, {"subject": "RE: Conformance Test for HTTP 1.", "content": "> I've lately been considering starting discussion of \n> development of something\n> within the W3C,\n\nWe would certainly support such an effort, and participate.\n\n--\nScott Lawrence         Architect             <lawrence@agranat.com>\nVirata             http://www.virata.com/         http://emweb.com/\n\n> \n\n\n\n", "id": "lists-012-14881484"}, {"subject": "RE: monitored???", "content": "Ramana,\n\nNo, most likely they use a caching and reverse HTTP Proxy for the server you connect to. This way work-load is devided over two servers. For static pages most of it is done by the proxy, while generation of the active pages by a real Apache server (only forwarded by the reverse proxy).\n\nMS-Proxy, so far I know, supports this function (to act as a reverse proxy), but also several WebCache (from Network Appl.) that support this as well, or are designed to do only that. Information about this should not be asked to me, because these are thing I don't know.\n\nThe X-Cache is probably added by the reverse proxy. If your connection is monitored, they will not add information of this kind. It looks like the proxy didn't have the document cached (the MISS).\n\nHere I assume you don't connect through a for you known proxy server, e.g. the one in your company or ISP. It's also possible such proxy server adds this information.\n\n\n- Joris\n\n\n> -----Original Message-----\n> From: Ramana Tadepalli [mailto:ramana@sanskrut.com]\n> Sent: Thursday, 05 October 2000 8:29\n> To: HTTP WG\n> Subject: monitored????\n> \n> \n> I am having to resend this coz no one replied....\n> \n> does this mean the connection is monitored????\n> \n> ----Header Recv----\n> HTTP/1.0 206 Partial Content\n> Date: Mon, 02 Oct 2000 18:32:23 GMT\n> Server: Apache/1.3.9 (Unix) Debian/GNU PHP/4.0b3\n> Last-Modified: Sun, 01 Aug 1999 17:48:06 GMT\n> ETag: \"c6406-38af05-37a48856\"\n> Accept-Ranges: bytes\n> Content-Length: 1416946\n> Content-Range: bytes 2297875-3714820/3714821\n> Content-Type: application/octet-stream\n> X-Cache: MISS from xxxxx\n> X-Cache-Lookup: MISS from xxxxx:3128\n> Connection: keep-alive\n> \n> Ramana Tadepalli\n> Sanskrut Software Systems Pvt Ltd\n> ramana@rediffmail.com\n> ramanatadepalli@hotmail.com\n> rtadepalli@yahoo.com\n\n\n> You don't have to know how the computer works, just how to work the\n> computer.\n> \nDepends on what you are doing....\n> \n> \n\n- Joris Dobbelsteen\n\n\n\n", "id": "lists-012-14889747"}, {"subject": "Re: monitored???", "content": "YES\n\n\n>From: \"Ramana Tadepalli\" <ramana@sanskrut.com>\n>Reply-To: \"Ramana Tadepalli\" <ramana@sanskrut.com>\n>To: \"HTTP WG\" <http-wg@cuckoo.hpl.hp.com>\n>Subject: monitored????\n>Date: Thu, 5 Oct 2000 11:59:09 +0530\n>\n>I am having to resend this coz no one replied....\n>\n>does this mean the connection is monitored????\n>\n>----Header Recv----\n>HTTP/1.0 206 Partial Content\n>Date: Mon, 02 Oct 2000 18:32:23 GMT\n>Server: Apache/1.3.9 (Unix) Debian/GNU PHP/4.0b3\n>Last-Modified: Sun, 01 Aug 1999 17:48:06 GMT\n>ETag: \"c6406-38af05-37a48856\"\n>Accept-Ranges: bytes\n>Content-Length: 1416946\n>Content-Range: bytes 2297875-3714820/3714821\n>Content-Type: application/octet-stream\n>X-Cache: MISS from xxxxx\n>X-Cache-Lookup: MISS from xxxxx:3128\n>Connection: keep-alive\n>\n>Ramana Tadepalli\n>Sanskrut Software Systems Pvt Ltd\n>ramana@rediffmail.com\n>ramanatadepalli@hotmail.com\n>rtadepalli@yahoo.com\n>You don't have to know how the computer works, just how to work the\n>computer.\n>\n>\n\n_________________________________________________________________________\nGet Your Private, Free E-mail from MSN Hotmail at http://www.hotmail.com.\n\nShare information about yourself, create your own public profile at \nhttp://profiles.msn.com.\n\n\n\n", "id": "lists-012-14899733"}, {"subject": "RE: Conformance Test for HTTP 1.", "content": "Mark Nottingham wrote,\n> I've lately been considering starting discussion of \n> development of something within the W3C, as it was involved \n> in the development of the HTTP, and has an established \n> history of developing similar tools (although I'm not sure if\n> W3C can formally commit resources).\n>\n> If anyone has any thoughts about this, please share them, \n> because I'd like to get this moving.\n\nThis sounds like a fine idea (tho', as you say, it's an open\nquestion whether or not the W3C would be able to commit\nresources).\n\nDo you have any particular emphasis in mind: server, clients,\nor proxies, or all equal weight on all?\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                       Cromwell Media\nInternet Systems Architect        5/6 Glenthorne Mews\n+44 (0)20 8817 4030               London, W6 0LJ, England\nmsabin@cromwellmedia.com          http://www.cromwellmedia.com/\n\n\n\n", "id": "lists-012-14908943"}, {"subject": "RE: Conformance Test for HTTP 1.", "content": "One approach might be to build a test framework around Jigsaw objects,\nperhaps driven by JPython scripts for easy testcase development and\nplatform independence.\n\n     -Carl\n\n\n\n", "id": "lists-012-14917253"}, {"subject": "Re: Comments on draft-ietf-http-v11-spec-rev0", "content": "Koen Holtman wrote:\n\n> - Section 13.10:\n> \n> This section introduces a new (as far as I can see) requirement:\n> \n> #  A cache that passes through requests for methods it does not understand\n> #  should invalidate any entities referred to by the Request-URI.\n> \n> This may seem like a good safety measure on the surface but I think\n> that it is in fact quite damaging.  First, designers of new methods\n> cannot benefit much from the above rule because 1.0 and 2068 caches\n> will not adhere to it.  On the other hand, the new rule introduces a\n> performance penalty for new methods which do not in fact cause any\n> invalidation.  One such method would be M-GET, a GET extended with a\n> mandatory extension, for example.  The performance penalty blocks\n> implied by the new rule makes certain ways of extending the protocol\n> too expensive and thus shortens the lifetime of the 1.x suite.  I want\n> the requirement to be removed.\n\nI think I'm the instigator of this change.  While your example seems\nbenign enough, the danger is from methods that change the underlying\nobject, e.g., M-PUT.  The object in the cache would no longer look like\nthe one at the origin server and must be invalidated.  In the absence of\na way to tell intervening caches to invalidate their view of the object\nthe proxy cache has to do so by default.\n\nI suppose a compromise would be for a cache to mark a cached object as\n\"must-revalidate\" when it sees an unknown method that it passes along. \nCache experts:  would that work?\n\nDave Kristol\n\n\n\n", "id": "lists-012-1492434"}, {"subject": "RE: Conformance Test for HTTP 1.", "content": "Once you have all the pieces for a server, building a client is easy, IMO.\nFor something driven by scripts, with no GUI, etc, no need to actually\nrender HTML or graphics, pretty straightforward.  I have done some of this\nfor test IPP implementations (IPP uses HTTP as its transfer protocol, so\nsome of the tests I wrote were HTTP specific).  But I used my own HTTP\nclasses at the time.  The main thing you need is the HTTP parser, which is\npretty much the same on server and client sides.\n\n     -Carl\n\n\nYves Lafon <ylafon@w3.org> on 10/06/2000 10:29:54 AM\n\nTo:   Carl Kugler/Boulder/IBM@IBMUS\ncc:   Miles Sabin <msabin@cromwellmedia.co.uk>, http-wg@cuckoo.hpl.hp.com\nSubject:  RE: Conformance Test for HTTP 1.1\n\n\n\nOn Fri, 6 Oct 2000, Carl Kugler/Boulder/IBM wrote:\n\n>\n> One approach might be to build a test framework around Jigsaw objects,\n> perhaps driven by JPython scripts for easy testcase development and\n> platform independence.\n\nThat's what I did (except that some scripts are just plain sh ;) ). But\nbasically the small test case available at http://jigsaw.w3.org/HTTP/ is\nmade with some specific Jigsaw filters or configuration (for 406).\n\nBut testing means also testing the server, so a client version of the\ntest is also needed (my first guess was a servlet doing client request\non a server).\n\n--\nYves Lafon - W3C / Jigsaw - XML Protocol - HTTP\n\"Baroula que barouleras, au ti?u toujou t'entourneras.\"\n\n\n\n", "id": "lists-012-14924834"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "I think proxies are the biggest target, because they're so hard to implement\ncorrectly, and so much more complex. In my experience, there's a fairly wide\nvariance in how implementors choose to interpret the spec.\n\nOf course, once you do one for proxies, it's relatively easy to get client\nand server test suites out of it.\n\n\n\nOn Fri, Oct 06, 2000 at 10:24:14AM +0100, Miles Sabin wrote:\n> Mark Nottingham wrote,\n> > I've lately been considering starting discussion of \n> > development of something within the W3C, as it was involved \n> > in the development of the HTTP, and has an established \n> > history of developing similar tools (although I'm not sure if\n> > W3C can formally commit resources).\n> >\n> > If anyone has any thoughts about this, please share them, \n> > because I'd like to get this moving.\n> \n> This sounds like a fine idea (tho', as you say, it's an open\n> question whether or not the W3C would be able to commit\n> resources).\n> \n> Do you have any particular emphasis in mind: server, clients,\n> or proxies, or all equal weight on all?\n> \n> Cheers,\n> \n> \n> Miles\n> \n> -- \n> Miles Sabin                       Cromwell Media\n> Internet Systems Architect        5/6 Glenthorne Mews\n> +44 (0)20 8817 4030               London, W6 0LJ, England\n> msabin@cromwellmedia.com          http://www.cromwellmedia.com/\n> \n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-14934802"}, {"subject": "[Fwd: RFC 2965 on HTTP State Management Mechanism", "content": " \n\nattached mail follows:\n\nA new Request for Comments is now available in online RFC libraries.\n\n\n        RFC 2965\n\n        Title:    HTTP State Management Mechanism\n        Author(s):  D. Kristol, L. Montulli\n        Status:     Standards Track\nDate:       October 2000\n        Mailbox:    dmk@bell-labs.com, lou@montulli.org\n        Pages:      26\n        Characters: 56176\n        Obsoletes:  2109\n\n        I-D Tag:    draft-ietf-http-state-man-mec-12.txt\n\n        URL:        ftp://ftp.isi.edu/in-notes/rfc2965.txt\n\n\nThis document specifies a way to create a stateful session with\nHypertext Transfer Protocol (HTTP) requests and responses.  It\ndescribes three new headers, Cookie, Cookie2, and Set-Cookie2, which\ncarry state information between participating origin servers and user\nagents.  The method described here differs from Netscape's Cookie\nproposal, but it can interoperate with HTTP/1.0 user agents\nthat use Netscape's method.  \n\nThis document reflects implementation experience with RFC 2109 and\nobsoletes it.\n\nThis is now a Proposed Standard Protocol.\n\nThis document specifies an Internet standards track protocol for\nthe Internet community, and requests discussion and suggestions\nfor improvements.  Please refer to the current edition of the\n\"Internet Official Protocol Standards\" (STD 1) for the\nstandardization state and status of this protocol.  Distribution\nof this memo is unlimited.\n\nThis announcement is sent to the IETF list and the RFC-DIST list.\nRequests to be added to or deleted from the IETF distribution list\nshould be sent to IETF-REQUEST@IETF.ORG.  Requests to be\nadded to or deleted from the RFC-DIST distribution list should\nbe sent to RFC-DIST-REQUEST@RFC-EDITOR.ORG.\n\nDetails on obtaining RFCs via FTP or EMAIL may be obtained by sending\nan EMAIL message to rfc-info@RFC-EDITOR.ORG with the message body \nhelp: ways_to_get_rfcs.  For example:\n\n        To: rfc-info@RFC-EDITOR.ORG\n        Subject: getting rfcs\n\n        help: ways_to_get_rfcs\n\nRequests for special distribution should be addressed to either the\nauthor of the RFC in question, or to RFC-Manager@RFC-EDITOR.ORG.  Unless\nspecifically noted otherwise on the RFC itself, all RFCs are for\nunlimited distribution.echo \nSubmissions for Requests for Comments should be sent to\nRFC-EDITOR@RFC-EDITOR.ORG.  Please consult RFC 2223, Instructions to RFC\nAuthors, for further information.\n\n\nJoyce K. Reynolds and Sandy Ginoza\nUSC/Information Sciences Institute\n\n...\n\nBelow is the data which will enable a MIME compliant Mail Reader \nimplementation to automatically retrieve the ASCII version\nof the RFCs.\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-14943687"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "I just want to put my two cents into this conversation:\n\nI think the idea of doing compliancy testing is great.  And the idea of\nhaving one \"check everything test\" is also a good thought.  However, how do\nwe guarantee that the test scenarios created are actually following the\n\"specs\"?\n\nI think this is something better left to outside agencies to address.  The\ntesting game tends to get to be too industry biased.  Whether intentionally\nor not you will see tests similar to this proposed one done and get totally\ndifferent results depending on who does it.\n\nI know this actually sounds like a good argument to create a \"standard\ntest\", but in my opinion this leads the doorway too wide open to start\nskewing the tests in favor of one manufacturer/developer vs. another one.  I\nrealize that there are currently many industry leaders involved in this\norganization and they provide valuable insights.  However, they are just\ninvolved in the CREATION of standards, not in judging the conformance to\nthem.\n\nIn short, while this is a good idea with the best interests of everyone in\nmind, I think this is probably stepping outside of the charter of the\norganization.\n\n-kh\n\n----- Original Message -----\nFrom: \"Mark Nottingham\" <mnot@mnot.net>\nTo: \"Miles Sabin\" <msabin@cromwellmedia.co.uk>\nCc: <http-wg@cuckoo.hpl.hp.com>\nSent: Friday, October 06, 2000 11:30 AM\nSubject: Re: Conformance Test for HTTP 1.1\n\n\n>\n>\n> I think proxies are the biggest target, because they're so hard to\nimplement\n> correctly, and so much more complex. In my experience, there's a fairly\nwide\n> variance in how implementors choose to interpret the spec.\n>\n> Of course, once you do one for proxies, it's relatively easy to get client\n> and server test suites out of it.\n>\n>\n>\n> On Fri, Oct 06, 2000 at 10:24:14AM +0100, Miles Sabin wrote:\n> > Mark Nottingham wrote,\n> > > I've lately been considering starting discussion of\n> > > development of something within the W3C, as it was involved\n> > > in the development of the HTTP, and has an established\n> > > history of developing similar tools (although I'm not sure if\n> > > W3C can formally commit resources).\n> > >\n> > > If anyone has any thoughts about this, please share them,\n> > > because I'd like to get this moving.\n> >\n> > This sounds like a fine idea (tho', as you say, it's an open\n> > question whether or not the W3C would be able to commit\n> > resources).\n> >\n> > Do you have any particular emphasis in mind: server, clients,\n> > or proxies, or all equal weight on all?\n> >\n> > Cheers,\n> >\n> >\n> > Miles\n> >\n> > --\n> > Miles Sabin                       Cromwell Media\n> > Internet Systems Architect        5/6 Glenthorne Mews\n> > +44 (0)20 8817 4030               London, W6 0LJ, England\n> > msabin@cromwellmedia.com          http://www.cromwellmedia.com/\n> >\n>\n> --\n> Mark Nottingham\n> http://www.mnot.net/\n>\n>\n\n\n\n", "id": "lists-012-14954196"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "On Sat, Oct 07, 2000 at 03:54:58PM -0500, Caveman wrote:\n> I just want to put my two cents into this conversation:\n> \n> I think the idea of doing compliancy testing is great.  And the idea of\n> having one \"check everything test\" is also a good thought.  However, how do\n> we guarantee that the test scenarios created are actually following the\n> \"specs\"?\n> \n> I think this is something better left to outside agencies to address.  The\n> testing game tends to get to be too industry biased.  Whether intentionally\n> or not you will see tests similar to this proposed one done and get totally\n> different results depending on who does it.\n> \n> I know this actually sounds like a good argument to create a \"standard\n> test\", but in my opinion this leads the doorway too wide open to start\n> skewing the tests in favor of one manufacturer/developer vs. another one.  I\n> realize that there are currently many industry leaders involved in this\n> organization and they provide valuable insights.  However, they are just\n> involved in the CREATION of standards, not in judging the conformance to\n> them.\n> \n> In short, while this is a good idea with the best interests of everyone in\n> mind, I think this is probably stepping outside of the charter of the\n> organization.\n\nThese are pretty much the arguments that I remember people making at WREC in\nPittsburgh, and I think it's a good point. This is why I'm holding out hope\nfor the W3C...\n\n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-14965892"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": ">\n>I just want to put my two cents into this conversation:\n>\n>I think the idea of doing compliancy testing is great.  And the idea of\n>having one \"check everything test\" is also a good thought.  However, how\ndo\n>we guarantee that the test scenarios created are actually following the\n>\"specs\"?\n>\n\nI was thinking along the lines of a script (or script fragment) for each\nMUST in the spec.  MUSTs are supposed to be verifiable, right?  All\ncompliant implementations, regardless of manufacturer/developer, must do\nthe MUSTs, right?   Using scripts makes it easy for people to inspect a\nscript and correct it if it isn't according to spec.\n\n>I think this is something better left to outside agencies to address.  The\n>testing game tends to get to be too industry biased.  Whether intentionally\n>or not you will see tests similar to this proposed one done and get totally\n>different results depending on who does it.\n>\n>I know this actually sounds like a good argument to create a \"standard\n>test\", but in my opinion this leads the doorway too wide open to start\n>skewing the tests in favor of one manufacturer/developer vs. another one.  I\n>realize that there are currently many industry leaders involved in this\n>organization and they provide valuable insights.  However, they are just\n>involved in the CREATION of standards, not in judging the conformance to\n>them.\n>\n>In short, while this is a good idea with the best interests of everyone in\n>mind, I think this is probably stepping outside of the charter of the\n>organization.\n>\n>-kh\n>\n>----- Original Message -----\n>From: \"Mark Nottingham\" <mnot@mnot.net>\n>To: \"Miles Sabin\" <msabin@cromwellmedia.co.uk>\n>Cc: <http-wg@cuckoo.hpl.hp.com>\n>Sent: Friday, October 06, 2000 11:30 AM\n>Subject: Re: Conformance Test for HTTP 1.1\n>\n>\n>>\n>>\n>> I think proxies are the biggest target, because they're so hard to\n>implement\n>> correctly, and so much more complex. In my experience, there's a fairly\n>wide\n>> variance in how implementors choose to interpret the spec.\n>>\n>> Of course, once you do one for proxies, it's relatively easy to get client\n>> and server test suites out of it.\n>>\n>>\n>>\n>> On Fri, Oct 06, 2000 at 10:24:14AM +0100, Miles Sabin wrote:\n>> > Mark Nottingham wrote,\n>> > > I've lately been considering starting discussion of\n>> > > development of something within the W3C, as it was involved\n>> > > in the development of the HTTP, and has an established\n>> > > history of developing similar tools (although I'm not sure if\n>> > > W3C can formally commit resources).\n>> > >\n>> > > If anyone has any thoughts about this, please share them,\n>> > > because I'd like to get this moving.\n>> >\n>> > This sounds like a fine idea (tho', as you say, it's an open\n>> > question whether or not the W3C would be able to commit\n>> > resources).\n>> >\n>> > Do you have any particular emphasis in mind: server, clients,\n>> > or proxies, or all equal weight on all?\n>> >\n>> > Cheers,\n>> >\n>> >\n>> > Miles\n>> >\n>> > --\n>> > Miles Sabin                       Cromwell Media\n>> > Internet Systems Architect        5/6 Glenthorne Mews\n>> > +44 (0)20 8817 4030               London, W6 0LJ, England\n>> > msabin@cromwellmedia.com          http://www.cromwellmedia.com/\n>> >\n>>\n>> --\n>> Mark Nottingham\n>> http://www.mnot.net/\n>>\n>>\n>\n\n\n\n", "id": "lists-012-14974829"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "Carl,\n\nOnce we start doing any kind of compliancy checking we face the proverbial\n\"slippery slope\".  What comes next?  Seperate tests for things that MAY be\ndone according to the specs?  Things that SHOULD be?\n\nI think the best thing to do is stay out of the compliancy checking business\nall together.\n\nThanks,\n\nKeith\n\n\n\n----- Original Message -----\nFrom: \"Carl Kugler/Boulder/IBM\" <kugler@us.ibm.com>\nTo: \"Caveman\" <hoffmankeith@hotmail.com>\nCc: \"Mark Nottingham\" <mnot@mnot.net>; \"Miles Sabin\"\n<msabin@cromwellmedia.co.uk>; <http-wg@cuckoo.hpl.hp.com>\nSent: Monday, October 09, 2000 2:39 PM\nSubject: Re: Conformance Test for HTTP 1.1\n\n\n>\n> >\n> >I just want to put my two cents into this conversation:\n> >\n> >I think the idea of doing compliancy testing is great.  And the idea of\n> >having one \"check everything test\" is also a good thought.  However, how\n> do\n> >we guarantee that the test scenarios created are actually following the\n> >\"specs\"?\n> >\n>\n> I was thinking along the lines of a script (or script fragment) for each\n> MUST in the spec.  MUSTs are supposed to be verifiable, right?  All\n> compliant implementations, regardless of manufacturer/developer, must do\n> the MUSTs, right?   Using scripts makes it easy for people to inspect a\n> script and correct it if it isn't according to spec.\n>\n> >I think this is something better left to outside agencies to address.\nThe\n> >testing game tends to get to be too industry biased.  Whether\nintentionally\n> >or not you will see tests similar to this proposed one done and get\ntotally\n> >different results depending on who does it.\n> >\n> >I know this actually sounds like a good argument to create a \"standard\n> >test\", but in my opinion this leads the doorway too wide open to start\n> >skewing the tests in favor of one manufacturer/developer vs. another one.\nI\n> >realize that there are currently many industry leaders involved in this\n> >organization and they provide valuable insights.  However, they are just\n> >involved in the CREATION of standards, not in judging the conformance to\n> >them.\n> >\n> >In short, while this is a good idea with the best interests of everyone\nin\n> >mind, I think this is probably stepping outside of the charter of the\n> >organization.\n> >\n> >-kh\n> >\n> >----- Original Message -----\n> >From: \"Mark Nottingham\" <mnot@mnot.net>\n> >To: \"Miles Sabin\" <msabin@cromwellmedia.co.uk>\n> >Cc: <http-wg@cuckoo.hpl.hp.com>\n> >Sent: Friday, October 06, 2000 11:30 AM\n> >Subject: Re: Conformance Test for HTTP 1.1\n> >\n> >\n> >>\n> >>\n> >> I think proxies are the biggest target, because they're so hard to\n> >implement\n> >> correctly, and so much more complex. In my experience, there's a fairly\n> >wide\n> >> variance in how implementors choose to interpret the spec.\n> >>\n> >> Of course, once you do one for proxies, it's relatively easy to get\nclient\n> >> and server test suites out of it.\n> >>\n> >>\n> >>\n> >> On Fri, Oct 06, 2000 at 10:24:14AM +0100, Miles Sabin wrote:\n> >> > Mark Nottingham wrote,\n> >> > > I've lately been considering starting discussion of\n> >> > > development of something within the W3C, as it was involved\n> >> > > in the development of the HTTP, and has an established\n> >> > > history of developing similar tools (although I'm not sure if\n> >> > > W3C can formally commit resources).\n> >> > >\n> >> > > If anyone has any thoughts about this, please share them,\n> >> > > because I'd like to get this moving.\n> >> >\n> >> > This sounds like a fine idea (tho', as you say, it's an open\n> >> > question whether or not the W3C would be able to commit\n> >> > resources).\n> >> >\n> >> > Do you have any particular emphasis in mind: server, clients,\n> >> > or proxies, or all equal weight on all?\n> >> >\n> >> > Cheers,\n> >> >\n> >> >\n> >> > Miles\n> >> >\n> >> > --\n> >> > Miles Sabin                       Cromwell Media\n> >> > Internet Systems Architect        5/6 Glenthorne Mews\n> >> > +44 (0)20 8817 4030               London, W6 0LJ, England\n> >> > msabin@cromwellmedia.com          http://www.cromwellmedia.com/\n> >> >\n> >>\n> >> --\n> >> Mark Nottingham\n> >> http://www.mnot.net/\n> >>\n> >>\n> >\n>\n>\n\n\n\n", "id": "lists-012-14987721"}, {"subject": "RE: Conformance Test for HTTP 1.", "content": "> Once we start doing any kind of compliancy checking we face \n> the proverbial\n> \"slippery slope\".  What comes next?  Seperate tests for \n> things that MAY be\n> done according to the specs?  Things that SHOULD be?\n\nI don't understand the problem you seem to see.  \n\nSlippery slope to what?  Interoperability?\n\n--\nScott Lawrence         Architect             <lawrence@agranat.com>\nVirata             http://www.virata.com/         http://emweb.com/\n\n\n\n", "id": "lists-012-15002921"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "Keith wrote:\n\n>Carl,\n>\n>Once we start doing any kind of compliancy checking we face the proverbial\n>\"slippery slope\".  What comes next?  Seperate tests for things that MAY be\n>done according to the specs?  Things that SHOULD be?\n>\n>I think the best thing to do is stay out of the compliancy checking\nbusiness\n>all together.\n>\n>Thanks,\n>\n>Keith\n>\n>\n\nMAYs are untestable, by definition  Since MAY is semantically equivalent to\nMAY NOT, there is no way to test the hypothesis that an implementation\nconforms to the statement.\n\nSHOULDs are also untestable by definition, since an application is allowed\nto violate a SHOULD if it has a good reason to.  Since the test writer\ncan't know whether or not the implementer had a good reason to violate a\nSHOULD, SHOULDs can't be used for compliance testing.\n\nAny implementation that meets all the MUSTs is compliant.  That's all that\nreally matters for compliancy, IMO.  Obviously, this kind of compliancy\ntest tells you little about the quality of an implementation, but it might\nhelp shake out bugs and misinterpretations.\n\n     -Carl\n\n\n\n", "id": "lists-012-15011073"}, {"subject": "Re: Comments on draft-ietf-http-v11-spec-rev0", "content": "Koen Holtman wrote:\n\n> - Section 13.10:\n> \n> This section introduces a new (as far as I can see) requirement:\n> \n> #  A cache that passes through requests for methods it does not understand\n> #  should invalidate any entities referred to by the Request-URI.\n> \n> This may seem like a good safety measure on the surface but I think\n> that it is in fact quite damaging.  First, designers of new methods\n> cannot benefit much from the above rule because 1.0 and 2068 caches\n> will not adhere to it.  On the other hand, the new rule introduces a\n> performance penalty for new methods which do not in fact cause any\n> invalidation.  One such method would be M-GET, a GET extended with a\n> mandatory extension, for example.  The performance penalty blocks\n> implied by the new rule makes certain ways of extending the protocol\n> too expensive and thus shortens the lifetime of the 1.x suite.  I want\n> the requirement to be removed.\n\nDave Kristol wrote:\n    I think I'm the instigator of this change.  While your example\n    seems benign enough, the danger is from methods that change the\n    underlying object, e.g., M-PUT.  The object in the cache would no\n    longer look like the one at the origin server and must be\n    invalidated.  In the absence of a way to tell intervening caches to\n    invalidate their view of the object the proxy cache has to do so by\n    default.\n\n    I suppose a compromise would be for a cache to mark a cached object\n    as \"must-revalidate\" when it sees an unknown method that it passes\n    along.  Cache experts:  would that work?\n    \nHow does\nmark the cached object as \"must-revalidate\"\ndiffer from\ninvalidate the cached object\n\nexcept that the former propagates the change to outbound caches?\n\nI'm not sure that Koen would view this as a compromise :-)\n\nWould it work?  Well, the concept of invalidation-based protocols\nis in general not supported by HTTP.  My preference is to err on\nthe side of transparency rather than performance, although I agree with\nKoen that the transparency in this case might be somewhat illusory.\n\nBut I'm not sure what the problem is; my understanding is that\nthe whole point of creating the M-GET method is to prevent\n\"proxies that do not understand the method\" from forwarding it.\nI.e., they are supposed to return 501 (Not Implemented) or act\nas a tunnel (i.e., not cache anything).\n\nSo any caching proxy that does forward M-GET does \"understand\" it, and\nisn't covered by the requirement that Koen objects to.\n\n-Jeff\n\n\n\n", "id": "lists-012-1501455"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "Carl Kugler/Boulder/IBM wrote:\n\n> MAYs are untestable, by definition  Since MAY is semantically equivalent to\n> MAY NOT, there is no way to test the hypothesis that an implementation\n> conforms to the statement.\n>\n> SHOULDs are also untestable by definition, since an application is allowed\n\nBut it can be useful to know which choices your server makes.  For example, if\nI\nhave an HTTP client application that requires Digest-Authentication, then, as\nfar as I'm concerned, that MAY becomes a MUST.\n\n(I make no comment on whether the IETF should get involved.  :-)\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |\"You're nothing but a pack of ringleaders!\"  |\n|francis@ecal.com|--_Wyrd Sisters_, Terry Pratchett            |\n\\==============================================================/\n\n\n\n", "id": "lists-012-15019437"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "Err, could you give a more solid demonstration as to why this is not good?\n\n\nOn Tue, Oct 10, 2000 at 09:23:10AM -0500, Caveman wrote:\n> Carl,\n> \n> Once we start doing any kind of compliancy checking we face the proverbial\n> \"slippery slope\".  What comes next?  Seperate tests for things that MAY be\n> done according to the specs?  Things that SHOULD be?\n> \n> I think the best thing to do is stay out of the compliancy checking business\n> all together.\n> \n> Thanks,\n> \n> Keith\n> \n> \n> \n> ----- Original Message -----\n> From: \"Carl Kugler/Boulder/IBM\" <kugler@us.ibm.com>\n> To: \"Caveman\" <hoffmankeith@hotmail.com>\n> Cc: \"Mark Nottingham\" <mnot@mnot.net>; \"Miles Sabin\"\n> <msabin@cromwellmedia.co.uk>; <http-wg@cuckoo.hpl.hp.com>\n> Sent: Monday, October 09, 2000 2:39 PM\n> Subject: Re: Conformance Test for HTTP 1.1\n> \n> \n> >\n> > >\n> > >I just want to put my two cents into this conversation:\n> > >\n> > >I think the idea of doing compliancy testing is great.  And the idea of\n> > >having one \"check everything test\" is also a good thought.  However, how\n> > do\n> > >we guarantee that the test scenarios created are actually following the\n> > >\"specs\"?\n> > >\n> >\n> > I was thinking along the lines of a script (or script fragment) for each\n> > MUST in the spec.  MUSTs are supposed to be verifiable, right?  All\n> > compliant implementations, regardless of manufacturer/developer, must do\n> > the MUSTs, right?   Using scripts makes it easy for people to inspect a\n> > script and correct it if it isn't according to spec.\n> >\n> > >I think this is something better left to outside agencies to address.\n> The\n> > >testing game tends to get to be too industry biased.  Whether\n> intentionally\n> > >or not you will see tests similar to this proposed one done and get\n> totally\n> > >different results depending on who does it.\n> > >\n> > >I know this actually sounds like a good argument to create a \"standard\n> > >test\", but in my opinion this leads the doorway too wide open to start\n> > >skewing the tests in favor of one manufacturer/developer vs. another one.\n> I\n> > >realize that there are currently many industry leaders involved in this\n> > >organization and they provide valuable insights.  However, they are just\n> > >involved in the CREATION of standards, not in judging the conformance to\n> > >them.\n> > >\n> > >In short, while this is a good idea with the best interests of everyone\n> in\n> > >mind, I think this is probably stepping outside of the charter of the\n> > >organization.\n> > >\n> > >-kh\n> > >\n> > >----- Original Message -----\n> > >From: \"Mark Nottingham\" <mnot@mnot.net>\n> > >To: \"Miles Sabin\" <msabin@cromwellmedia.co.uk>\n> > >Cc: <http-wg@cuckoo.hpl.hp.com>\n> > >Sent: Friday, October 06, 2000 11:30 AM\n> > >Subject: Re: Conformance Test for HTTP 1.1\n> > >\n> > >\n> > >>\n> > >>\n> > >> I think proxies are the biggest target, because they're so hard to\n> > >implement\n> > >> correctly, and so much more complex. In my experience, there's a fairly\n> > >wide\n> > >> variance in how implementors choose to interpret the spec.\n> > >>\n> > >> Of course, once you do one for proxies, it's relatively easy to get\n> client\n> > >> and server test suites out of it.\n> > >>\n> > >>\n> > >>\n> > >> On Fri, Oct 06, 2000 at 10:24:14AM +0100, Miles Sabin wrote:\n> > >> > Mark Nottingham wrote,\n> > >> > > I've lately been considering starting discussion of\n> > >> > > development of something within the W3C, as it was involved\n> > >> > > in the development of the HTTP, and has an established\n> > >> > > history of developing similar tools (although I'm not sure if\n> > >> > > W3C can formally commit resources).\n> > >> > >\n> > >> > > If anyone has any thoughts about this, please share them,\n> > >> > > because I'd like to get this moving.\n> > >> >\n> > >> > This sounds like a fine idea (tho', as you say, it's an open\n> > >> > question whether or not the W3C would be able to commit\n> > >> > resources).\n> > >> >\n> > >> > Do you have any particular emphasis in mind: server, clients,\n> > >> > or proxies, or all equal weight on all?\n> > >> >\n> > >> > Cheers,\n> > >> >\n> > >> >\n> > >> > Miles\n> > >> >\n> > >> > --\n> > >> > Miles Sabin                       Cromwell Media\n> > >> > Internet Systems Architect        5/6 Glenthorne Mews\n> > >> > +44 (0)20 8817 4030               London, W6 0LJ, England\n> > >> > msabin@cromwellmedia.com          http://www.cromwellmedia.com/\n> > >> >\n> > >>\n> > >> --\n> > >> Mark Nottingham\n> > >> http://www.mnot.net/\n> > >>\n> > >>\n> > >\n> >\n> >\n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-15027460"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "Sure, an open-source testbed could be easily extended to perform lots of\nadditional testing.\n\n     -Carl\n\n\nJohn Stracke <francis@ecal.com>@localhost.localdomain on 10/10/2000\n09:35:32 AM\n\nSent by:  francis@localhost.localdomain\n\n\nTo:   http-wg@cuckoo.hpl.hp.com\ncc:\nSubject:  Re: Conformance Test for HTTP 1.1\n\n\n\nCarl Kugler/Boulder/IBM wrote:\n\n> MAYs are untestable, by definition  Since MAY is semantically equivalent\nto\n> MAY NOT, there is no way to test the hypothesis that an implementation\n> conforms to the statement.\n>\n> SHOULDs are also untestable by definition, since an application is\nallowed\n\nBut it can be useful to know which choices your server makes.  For example,\nif\nI\nhave an HTTP client application that requires Digest-Authentication, then,\nas\nfar as I'm concerned, that MAY becomes a MUST.\n\n(I make no comment on whether the IETF should get involved.  :-)\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |\"You're nothing but a pack of ringleaders!\"  |\n|francis@ecal.com|--_Wyrd Sisters_, Terry Pratchett            |\n\\==============================================================/\n\n\n\n", "id": "lists-012-15042929"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "The simplest answer as to why this isn't good is that it's outside the\ncharter of the IETF.  This organization is here to create standards.  Not to\nvalidate/judge whether someone is compliant with them.\n\n\n----- Original Message -----\nFrom: \"Mark Nottingham\" <mnot@mnot.net>\nTo: \"Caveman\" <hoffmankeith@hotmail.com>\nCc: \"Carl Kugler/Boulder/IBM\" <kugler@us.ibm.com>; \"Miles Sabin\"\n<msabin@cromwellmedia.co.uk>; <http-wg@cuckoo.hpl.hp.com>\nSent: Tuesday, October 10, 2000 1:55 PM\nSubject: Re: Conformance Test for HTTP 1.1\n\n\n>\n>\n> Err, could you give a more solid demonstration as to why this is not good?\n>\n>\n> On Tue, Oct 10, 2000 at 09:23:10AM -0500, Caveman wrote:\n> > Carl,\n> >\n> > Once we start doing any kind of compliancy checking we face the\nproverbial\n> > \"slippery slope\".  What comes next?  Seperate tests for things that MAY\nbe\n> > done according to the specs?  Things that SHOULD be?\n> >\n> > I think the best thing to do is stay out of the compliancy checking\nbusiness\n> > all together.\n> >\n> > Thanks,\n> >\n> > Keith\n> >\n> >\n> >\n> > ----- Original Message -----\n> > From: \"Carl Kugler/Boulder/IBM\" <kugler@us.ibm.com>\n> > To: \"Caveman\" <hoffmankeith@hotmail.com>\n> > Cc: \"Mark Nottingham\" <mnot@mnot.net>; \"Miles Sabin\"\n> > <msabin@cromwellmedia.co.uk>; <http-wg@cuckoo.hpl.hp.com>\n> > Sent: Monday, October 09, 2000 2:39 PM\n> > Subject: Re: Conformance Test for HTTP 1.1\n> >\n> >\n> > >\n> > > >\n> > > >I just want to put my two cents into this conversation:\n> > > >\n> > > >I think the idea of doing compliancy testing is great.  And the idea\nof\n> > > >having one \"check everything test\" is also a good thought.  However,\nhow\n> > > do\n> > > >we guarantee that the test scenarios created are actually following\nthe\n> > > >\"specs\"?\n> > > >\n> > >\n> > > I was thinking along the lines of a script (or script fragment) for\neach\n> > > MUST in the spec.  MUSTs are supposed to be verifiable, right?  All\n> > > compliant implementations, regardless of manufacturer/developer, must\ndo\n> > > the MUSTs, right?   Using scripts makes it easy for people to inspect\na\n> > > script and correct it if it isn't according to spec.\n> > >\n> > > >I think this is something better left to outside agencies to address.\n> > The\n> > > >testing game tends to get to be too industry biased.  Whether\n> > intentionally\n> > > >or not you will see tests similar to this proposed one done and get\n> > totally\n> > > >different results depending on who does it.\n> > > >\n> > > >I know this actually sounds like a good argument to create a\n\"standard\n> > > >test\", but in my opinion this leads the doorway too wide open to\nstart\n> > > >skewing the tests in favor of one manufacturer/developer vs. another\none.\n> > I\n> > > >realize that there are currently many industry leaders involved in\nthis\n> > > >organization and they provide valuable insights.  However, they are\njust\n> > > >involved in the CREATION of standards, not in judging the conformance\nto\n> > > >them.\n> > > >\n> > > >In short, while this is a good idea with the best interests of\neveryone\n> > in\n> > > >mind, I think this is probably stepping outside of the charter of the\n> > > >organization.\n> > > >\n> > > >-kh\n> > > >\n> > > >----- Original Message -----\n> > > >From: \"Mark Nottingham\" <mnot@mnot.net>\n> > > >To: \"Miles Sabin\" <msabin@cromwellmedia.co.uk>\n> > > >Cc: <http-wg@cuckoo.hpl.hp.com>\n> > > >Sent: Friday, October 06, 2000 11:30 AM\n> > > >Subject: Re: Conformance Test for HTTP 1.1\n> > > >\n> > > >\n> > > >>\n> > > >>\n> > > >> I think proxies are the biggest target, because they're so hard to\n> > > >implement\n> > > >> correctly, and so much more complex. In my experience, there's a\nfairly\n> > > >wide\n> > > >> variance in how implementors choose to interpret the spec.\n> > > >>\n> > > >> Of course, once you do one for proxies, it's relatively easy to get\n> > client\n> > > >> and server test suites out of it.\n> > > >>\n> > > >>\n> > > >>\n> > > >> On Fri, Oct 06, 2000 at 10:24:14AM +0100, Miles Sabin wrote:\n> > > >> > Mark Nottingham wrote,\n> > > >> > > I've lately been considering starting discussion of\n> > > >> > > development of something within the W3C, as it was involved\n> > > >> > > in the development of the HTTP, and has an established\n> > > >> > > history of developing similar tools (although I'm not sure if\n> > > >> > > W3C can formally commit resources).\n> > > >> > >\n> > > >> > > If anyone has any thoughts about this, please share them,\n> > > >> > > because I'd like to get this moving.\n> > > >> >\n> > > >> > This sounds like a fine idea (tho', as you say, it's an open\n> > > >> > question whether or not the W3C would be able to commit\n> > > >> > resources).\n> > > >> >\n> > > >> > Do you have any particular emphasis in mind: server, clients,\n> > > >> > or proxies, or all equal weight on all?\n> > > >> >\n> > > >> > Cheers,\n> > > >> >\n> > > >> >\n> > > >> > Miles\n> > > >> >\n> > > >> > --\n> > > >> > Miles Sabin                       Cromwell Media\n> > > >> > Internet Systems Architect        5/6 Glenthorne Mews\n> > > >> > +44 (0)20 8817 4030               London, W6 0LJ, England\n> > > >> > msabin@cromwellmedia.com          http://www.cromwellmedia.com/\n> > > >> >\n> > > >>\n> > > >> --\n> > > >> Mark Nottingham\n> > > >> http://www.mnot.net/\n> > > >>\n> > > >>\n> > > >\n> > >\n> > >\n>\n> --\n> Mark Nottingham\n> http://www.mnot.net/\n>\n>\n\n\n\n", "id": "lists-012-15052125"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": ">The simplest answer as to why this isn't good is that it's outside the\n>charter of the IETF.  This organization is here to create standards. \n>Not to validate/judge whether someone is compliant with them.\n\ncompliancy to a protocol is one of the key expectation of a specification.\nthe folks who worked on the spec have a clear interest in seeing whether\nall the effort they went to in terms of M/S/M were met. it is not an issue\nof \"judging\" and saying \"oh sorry, ee.software is broken\".\n\nclaiming that ietf should not be interested in compliancy appears a bit \nodd to me. we all stand to gain from making sure that implementations\nare compliant to a protocol spec. ietf should be interested. at least \nthey were interested last year when they invited me to give a talk at \na plenary sesion last year during their d.c. meeting - the talk was\non http/1.1 protocol compliancy.\n\nwhat is wrong with ietf-sponsored group coming up with a test suite \nfor an open protocol specification? anyone who wants have their\nsoftware tested against it can do so. ietf is unlikely to be partial\nin the construction of a compliancy test suite. \n\ncheers,\nbala\n\nbalachander krishnamurthy\nhttp://www.research.att.com/~bala/papers\n\n\n\n", "id": "lists-012-15070786"}, {"subject": "RE: Conformance Test for HTTP 1.", "content": "> The simplest answer as to why this isn't good is that it's\n> outside the\n> charter of the IETF.  This organization is here to create\n> standards.  Not to\n> validate/judge whether someone is compliant with them.\n\nCorrect.  But that doesn't mean that it wouldn't be valuable both to\nimplementors and to the IETF process to have a test suite exist\nsomewhere.  Even if the development of that suite wouldn't be an\nappropriate IETF WG (which I agree it would not), one of the\nweaknesses of the current IETF process is that there is not enough\nfeedback regarding which parts of the standards really get done.  I\nhelped run the effort to document what features had been done for the\ntransition from PS to DS for HTTP/1.1, and that data was pretty poor\nquality.\n\nOn the subject of SHOULDs - many of them in the HTTP/1.1 specs would\nprobably have been MUSTs but for some backward compatibility problems\nthat doing so would have introduced.  Over time, it will be of benefit\nto be able to actually measure whether those have been implemented as\nthe authors of the specs intended.\n\n--\nScott Lawrence         Architect             <lawrence@agranat.com>\nVirata             http://www.virata.com/         http://emweb.com/\n\n\n\n\n", "id": "lists-012-15079867"}, {"subject": "Re: Conformance Test for HTTP 1.", "content": "Right. That was pointed out earlier; discussion was as to where it would be\nappropriate. It wasn't clear if your mail was focused on just the HTTP-wg\n(which is dormant, and about to close anyway), or on the larger picture.\n\n\n\nOn Tue, Oct 10, 2000 at 12:04:09PM -0500, Keith Hoffman wrote:\n> The simplest answer as to why this isn't good is that it's outside the\n> charter of the IETF.  This organization is here to create standards.  Not to\n> validate/judge whether someone is compliant with them.\n> \n> \n> ----- Original Message -----\n> From: \"Mark Nottingham\" <mnot@mnot.net>\n> To: \"Caveman\" <hoffmankeith@hotmail.com>\n> Cc: \"Carl Kugler/Boulder/IBM\" <kugler@us.ibm.com>; \"Miles Sabin\"\n> <msabin@cromwellmedia.co.uk>; <http-wg@cuckoo.hpl.hp.com>\n> Sent: Tuesday, October 10, 2000 1:55 PM\n> Subject: Re: Conformance Test for HTTP 1.1\n> \n> \n> >\n> >\n> > Err, could you give a more solid demonstration as to why this is not good?\n> >\n> >\n> > On Tue, Oct 10, 2000 at 09:23:10AM -0500, Caveman wrote:\n> > > Carl,\n> > >\n> > > Once we start doing any kind of compliancy checking we face the\n> proverbial\n> > > \"slippery slope\".  What comes next?  Seperate tests for things that MAY\n> be\n> > > done according to the specs?  Things that SHOULD be?\n> > >\n> > > I think the best thing to do is stay out of the compliancy checking\n> business\n> > > all together.\n> > >\n> > > Thanks,\n> > >\n> > > Keith\n> > >\n> > >\n> > >\n> > > ----- Original Message -----\n> > > From: \"Carl Kugler/Boulder/IBM\" <kugler@us.ibm.com>\n> > > To: \"Caveman\" <hoffmankeith@hotmail.com>\n> > > Cc: \"Mark Nottingham\" <mnot@mnot.net>; \"Miles Sabin\"\n> > > <msabin@cromwellmedia.co.uk>; <http-wg@cuckoo.hpl.hp.com>\n> > > Sent: Monday, October 09, 2000 2:39 PM\n> > > Subject: Re: Conformance Test for HTTP 1.1\n> > >\n> > >\n> > > >\n> > > > >\n> > > > >I just want to put my two cents into this conversation:\n> > > > >\n> > > > >I think the idea of doing compliancy testing is great.  And the idea\n> of\n> > > > >having one \"check everything test\" is also a good thought.  However,\n> how\n> > > > do\n> > > > >we guarantee that the test scenarios created are actually following\n> the\n> > > > >\"specs\"?\n> > > > >\n> > > >\n> > > > I was thinking along the lines of a script (or script fragment) for\n> each\n> > > > MUST in the spec.  MUSTs are supposed to be verifiable, right?  All\n> > > > compliant implementations, regardless of manufacturer/developer, must\n> do\n> > > > the MUSTs, right?   Using scripts makes it easy for people to inspect\n> a\n> > > > script and correct it if it isn't according to spec.\n> > > >\n> > > > >I think this is something better left to outside agencies to address.\n> > > The\n> > > > >testing game tends to get to be too industry biased.  Whether\n> > > intentionally\n> > > > >or not you will see tests similar to this proposed one done and get\n> > > totally\n> > > > >different results depending on who does it.\n> > > > >\n> > > > >I know this actually sounds like a good argument to create a\n> \"standard\n> > > > >test\", but in my opinion this leads the doorway too wide open to\n> start\n> > > > >skewing the tests in favor of one manufacturer/developer vs. another\n> one.\n> > > I\n> > > > >realize that there are currently many industry leaders involved in\n> this\n> > > > >organization and they provide valuable insights.  However, they are\n> just\n> > > > >involved in the CREATION of standards, not in judging the conformance\n> to\n> > > > >them.\n> > > > >\n> > > > >In short, while this is a good idea with the best interests of\n> everyone\n> > > in\n> > > > >mind, I think this is probably stepping outside of the charter of the\n> > > > >organization.\n> > > > >\n> > > > >-kh\n> > > > >\n> > > > >----- Original Message -----\n> > > > >From: \"Mark Nottingham\" <mnot@mnot.net>\n> > > > >To: \"Miles Sabin\" <msabin@cromwellmedia.co.uk>\n> > > > >Cc: <http-wg@cuckoo.hpl.hp.com>\n> > > > >Sent: Friday, October 06, 2000 11:30 AM\n> > > > >Subject: Re: Conformance Test for HTTP 1.1\n> > > > >\n> > > > >\n> > > > >>\n> > > > >>\n> > > > >> I think proxies are the biggest target, because they're so hard to\n> > > > >implement\n> > > > >> correctly, and so much more complex. In my experience, there's a\n> fairly\n> > > > >wide\n> > > > >> variance in how implementors choose to interpret the spec.\n> > > > >>\n> > > > >> Of course, once you do one for proxies, it's relatively easy to get\n> > > client\n> > > > >> and server test suites out of it.\n> > > > >>\n> > > > >>\n> > > > >>\n> > > > >> On Fri, Oct 06, 2000 at 10:24:14AM +0100, Miles Sabin wrote:\n> > > > >> > Mark Nottingham wrote,\n> > > > >> > > I've lately been considering starting discussion of\n> > > > >> > > development of something within the W3C, as it was involved\n> > > > >> > > in the development of the HTTP, and has an established\n> > > > >> > > history of developing similar tools (although I'm not sure if\n> > > > >> > > W3C can formally commit resources).\n> > > > >> > >\n> > > > >> > > If anyone has any thoughts about this, please share them,\n> > > > >> > > because I'd like to get this moving.\n> > > > >> >\n> > > > >> > This sounds like a fine idea (tho', as you say, it's an open\n> > > > >> > question whether or not the W3C would be able to commit\n> > > > >> > resources).\n> > > > >> >\n> > > > >> > Do you have any particular emphasis in mind: server, clients,\n> > > > >> > or proxies, or all equal weight on all?\n> > > > >> >\n> > > > >> > Cheers,\n> > > > >> >\n> > > > >> >\n> > > > >> > Miles\n> > > > >> >\n> > > > >> > --\n> > > > >> > Miles Sabin                       Cromwell Media\n> > > > >> > Internet Systems Architect        5/6 Glenthorne Mews\n> > > > >> > +44 (0)20 8817 4030               London, W6 0LJ, England\n> > > > >> > msabin@cromwellmedia.com          http://www.cromwellmedia.com/\n> > > > >> >\n> > > > >>\n> > > > >> --\n> > > > >> Mark Nottingham\n> > > > >> http://www.mnot.net/\n> > > > >>\n> > > > >>\n> > > > >\n> > > >\n> > > >\n> >\n> > --\n> > Mark Nottingham\n> > http://www.mnot.net/\n> >\n> >\n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-15088452"}, {"subject": "request to close HTTPW", "content": "With the publication of RFC 2965, the HTTP working group has now completed\nall of the items in its charter, and has no open Internet Drafts.\n\nThis message requests that the HTTP working group be closed.\n\nLarry (as HTTP WG chair)\n-- \nhttp://larry.masinter.net\n\n\n\n", "id": "lists-012-15107289"}, {"subject": "Re: Comments on draft-ietf-http-v11-spec-rev0", "content": "Jeffrey Mogul wrote:\n> \n> Koen Holtman wrote:\n> \n> > - Section 13.10:\n> >\n> > This section introduces a new (as far as I can see) requirement:\n> >\n> > #  A cache that passes through requests for methods it does not understand\n> > #  should invalidate any entities referred to by the Request-URI.\n\n> But I'm not sure what the problem is; my understanding is that\n> the whole point of creating the M-GET method is to prevent\n> \"proxies that do not understand the method\" from forwarding it.\n> I.e., they are supposed to return 501 (Not Implemented) or act\n> as a tunnel (i.e., not cache anything).\n> \n> So any caching proxy that does forward M-GET does \"understand\" it, and\n> isn't covered by the requirement that Koen objects to.\n\nWell I'm confused (but you knew that already)!\n\nSection 13.10 says a cache *can* pass through a method it doesn't\nunderstand.  But Jeff says M-GET is meant to prevent the forwarding. \nSeems like a contradiction to me.\n\nDave Kristol\n\n\n\n", "id": "lists-012-1511321"}, {"subject": "FW: WG Action: HyperText Transfer Protocol (http) to conclud", "content": "As far as I know, this mailing list will remain open indefinitely\nfor discussions and clarifications of the HTTP specifications.\n\nRemember that we are collecting errata at: \n              http://purl.org/NET/http-errata\n\nIn addition, there have been several BOFs for HTTP related\nactivities.  \n\n-----Original Message-----\nFrom: The IESG [mailto:iesg-secretary@ietf.org]\nSent: Wednesday, October 11, 2000 11:58 AM\nTo: IETF-Announce:@loki.ietf.org\nSubject: WG Action: HyperText Transfer Protocol (http) to conclude\n\n\nThe HyperText Transfer Protocol (http) Working Group in the Applications\nArea of the IETF has concluded.\n\nThe IESG Contact Persons are Ned Freed and Patrik Faltstrom.\n\n\n\n", "id": "lists-012-15115002"}, {"subject": "FW: BCP 44, RFC 2964 on Use of HTTP State Managemen", "content": "-----Original Message-----\nFrom: RFC Editor [mailto:rfc-ed@ISI.EDU]\nSent: Thursday, October 12, 2000 4:49 PM\nTo: IETF-Announce; @loki.ietf.org\nCc: rfc-ed@ISI.EDU\nSubject: BCP 44, RFC 2964 on Use of HTTP State Management\n\n\n\nA new Request for Comments is now available in online RFC libraries.\n\n\n        BCP 44 \n        RFC 2964\n\n        Title:    Use of HTTP State Management\n        Author(s):  K. Moore, N. Freed\n        Status:     Best Current Practice\nDate:       October 2000\n        Mailbox:    moore@cs.utk.edu, ned.freed@innosoft.com\n        Pages:      8\n        Characters: 18899\n        Updates/Obsoletes/SeeAlso:    None\n\n        I-D Tag:    draft-iesg-http-cookies-03.txt\n\n        URL:        ftp://ftp.isi.edu/in-notes/rfc2964.txt\n\n\nThe mechanisms described in \"HTTP State Management Mechanism\"\n(RFC-2965), and its predecessor (RFC-2109), can be used for many\ndifferent purposes.  However, some current and potential uses of the\nprotocol are controversial because they have significant user privacy\nand security implications.  This memo identifies specific uses of\nHypertext Transfer Protocol (HTTP) State Management protocol which are\neither (a) not recommended by the IETF, or (b) believed to be harmful,\nand discouraged.  This memo also details additional privacy\nconsiderations which are not covered by the HTTP State Management\nprotocol specification.\n\nThis document specifies an Internet Best Current Practices for the\nInternet Community, and requests discussion and suggestions for\nimprovements.  Distribution of this memo is unlimited.\n\n\n\n", "id": "lists-012-15123451"}, {"subject": "ANN: HTTP compliance mailing lis", "content": "Since there seems to be some level of interest in HTTP compliance testing, \nbut it's not clear that such discussion belongs in the IETF, I've created a\nmailing list, http-compliance@egroups.com\n\nI'd like to discuss the nature and scoping of possible testing, and gather\nspecific items for testing. I'm also interesting in getting input from\nserver, client and intermediate vendors.\n\nHopefully, the list can be used to gauge the amount of interest in\ncompliance testing, define the issues (like scoping, nature of tests, etc)\nin order to be able to ease the work into a standards body like the W3C.\n\nYou can subscribe by sending mail to:\n  http-compliance-subscribe@egroups.com\nalternatively, visit\n  http://www.egroups.com/subscribe/http-compliance\n\nCheers,\n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-15133029"}, {"subject": "whither HTTP", "content": "I recall that at one time there were various thoughts on the table about where \nto take HTTP next (e.g. \"HTTP next generation\"). What ever happened to those \nthoughts/efforts? Are there any nascent HTTP 1.x or 2.0 efforts lurking about? \nI'm not necessarily advocating that there should be, I'm simply trying to see \nif the HTTP cognoscenti will share what's on their minds, protocol-wise.\n\nthanks,\n\nJeffH\n\n\n\n", "id": "lists-012-15140847"}, {"subject": "Re: FW: WG Action: HyperText Transfer Protocol (http) to conclud", "content": "\"Larry Masinter\" <masinter@attlabs.att.com> writes:\n\n> As far as I know, this mailing list will remain open indefinitely\n> for discussions and clarifications of the HTTP specifications.\n\nAs the mailing list owner, I'm happy to keep the list running for as long as\nyou need it.\n-- \n-- ange -- <><\n\nange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-012-15147560"}, {"subject": "caching problem", "content": "Dont know if I am doign this the right way?!?\n\nI am wondering what happens when HTTP 1.1 rules are specified in an HTTP 1.0\nprotocol - particulariy in regard to Cache-Control and proxies.\n\nHere is the environment.... IIS server, MSIE 5.0 browser and Squid Proxy.\n\nI have a web page that should not be cached by a proxy/browser etc.\nAppropriate Cache-Control headers are sent also. In the GET/POST and\nresponses, the HTTP version is specified as 1.0. (Yet HTTP 1.1 Cache-Control\nrules are also used.)\n\nI have two versions of the same web software. At the browser I am able to\nretrieve a web page from cache (one time only!) whereas with the latest\nversion, the page does not appear to be cached as the web page is retrieved\nfrom the server. (This is proven using a packet sniffer.) In both versions\nof software, packets sent by the server contain no-store, no-cache,\nmust-revalidate, max-age=1 cache control directives. The only difference\nbetween the two version of the software is that in the latest version, the\nheaders are sent separately to the web page. In the earlier version, part of\nthe document is sent with the headers.\n\nI am aware of the reliability of caches with 1.0 protocols, but if the only\ndifference (after using a packet sniffer) is that the headers are sent\nseparately, then I am at a loss to determine why in one case a page is be\nretrieved from cache (previous version) whereas (in the latest version) the\nuser is redirected to a separate page.\n\n\n\n", "id": "lists-012-15155568"}, {"subject": "Host header and UR", "content": "Something I was wondering about, does HTTP/1.1 allow this:\n\nYou send to a HTTP/1.1 proxy/gateway:\n\nGET http://www.you.com/ HTTP/1.1\nhost: gateway.me.com\n\nwere the intension is to get www.you.com through gateway.me.com\n\ngateway.me.com can be e.g. a caching proxy server or gateway...\n\nIs this actually possible HTTP/1.1-compliant???\n(however I don't expect proxies to support this)\n\n\n\n\n- Joris\n\n\n\n", "id": "lists-012-15163777"}, {"subject": "RE: caching problem", "content": "> -----Original Message-----\n> From: Tim Coates [mailto:tcoates@dynamics.net]\n> Sent: Thursday, 26 October 2000 3:51\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: caching problems\n> \n> \n> Dont know if I am doign this the right way?!?\n> \n> I am wondering what happens when HTTP 1.1 rules are specified \n> in an HTTP 1.0\n> protocol - particulariy in regard to Cache-Control and proxies.\n> \n> Here is the environment.... IIS server, MSIE 5.0 browser and \n> Squid Proxy.\n> \n> I have a web page that should not be cached by a proxy/browser etc.\n> Appropriate Cache-Control headers are sent also. In the GET/POST and\n> responses, the HTTP version is specified as 1.0. (Yet HTTP \n> 1.1 Cache-Control\n> rules are also used.)\n> \n> I have two versions of the same web software. At the browser \n> I am able to\n> retrieve a web page from cache (one time only!) whereas with \n> the latest\n> version, the page does not appear to be cached as the web \n> page is retrieved\n> from the server. (This is proven using a packet sniffer.) In \n> both versions\n> of software, packets sent by the server contain no-store, no-cache,\n> must-revalidate, max-age=1 cache control directives. The only \n> difference\n> between the two version of the software is that in the latest \n> version, the\n> headers are sent separately to the web page. In the earlier \n> version, part of\n> the document is sent with the headers.\n\nYou are using too many cache-control directives. no-cache is sufficent..\nNo-store is also enough, causing it not to be stores anywhere.\n\nno-store I would only recommend for sensative data, no-cache for non-sensative data, because it can save some network bandwidth.\n\nusing also the must-revalidate and max-age may confuse the cache, as it is *probably* non-compliant.\n\n> \n> I am aware of the reliability of caches with 1.0 protocols, \n> but if the only\n> difference (after using a packet sniffer) is that the headers are sent\n> separately, then I am at a loss to determine why in one case \n> a page is be\n> retrieved from cache (previous version) whereas (in the \n> latest version) the\n> user is redirected to a separate page.\n> \n> \n\nMaybe that you should also include\n\nPragma: no-cache\n\njust in case. Using HTTP/1.1 is better....\n\nI expect a HTTP/1.1 cache to interpet the cache-control header included in a HTTP/1.0 response. MSIE does this, I think, but including \"pragma: no-cache\" is much more reliable...\n\n\n- Joris\n\n\n\n", "id": "lists-012-15170897"}, {"subject": "RE: Host header and UR", "content": "> From: Joris Dobbelsteen\n\n> Something I was wondering about, does HTTP/1.1 allow this:\n>\n> You send to a HTTP/1.1 proxy/gateway:\n>\n> GET http://www.you.com/ HTTP/1.1\n> host: gateway.me.com\n>\n> were the intension is to get www.you.com through gateway.me.com\n>\n> gateway.me.com can be e.g. a caching proxy server or gateway...\n>\n> Is this actually possible HTTP/1.1-compliant???\n\nNo - the Host header value designates the origin server, not a\ngateway; if you send a full URL in the request line, then the two\nhosts should match.\n\n--\nScott Lawrence         Architect             <lawrence@agranat.com>\nVirata             http://www.virata.com/         http://emweb.com/\n\n\n\n\n", "id": "lists-012-15180748"}, {"subject": "Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "I'm curious about how HTTP/1.1 [RFC2616] persistent connections typically work \nwith respect to the typical browsers out in the wild today (Netscape & \nMicrosoft being the two I'm particularly interested in). If I cause a browser \nto send a GET request for a given URL (using HTTP/1.1) to a server, and the \nserver doesn't encounter any errors in processing it and responding, and then \nI (say) don't touch the browser for hours, what *typically* happens to the \nestablished HTTP/1.1 (-over-TCP) connection?\n\nI note that RFC2616 says (in part)..\n\n\n                             :\n8 Connections\n\n8.1 Persistent Connections\n                             :\n   HTTP implementations SHOULD implement persistent connections.\n                             :\n   A significant difference between HTTP/1.1 and earlier versions of\n   HTTP is that persistent connections are the default behavior of any\n   HTTP connection. That is, unless otherwise indicated, the client\n   SHOULD assume that the server will maintain a persistent connection,\n   even after error responses from the server.\n                             :\n\n\nAs it is written, this effectively puts the responsibility for closing the \nHTTP/1.1-cum-TCP connection on the client.\n\nIn nosing around on this subject, I note that in [W.R.Stevens, TCP/IP \nIllustrated Vol 1, http://www.dqc.org/~chris/tcpip_ill/], in chapter 23 \nStevens says that..\n\n1. \"Keepalives are not part of the TCP specification. ... Nevertheless, many \nimplementations provide the keep-alive timer.\"\n\n2. \"If there is no activity on a given connection for 2 hours, the server \nsends a probe segment to the client. ... A perpetual question by people \ndiscovering the keepalive option is whether the 2-hour idle time value can be \nchanged. They normally want it much lower, on the order of minutes. As we show \nin Appendix E, the value can usually be changed, but in all the systems \ndescribed in this appendix, the keepalive interval is a system-wide value, so \nchanging it affects all users of the option. \"\n\n..and in appendix E he shows kernel configuration parameters for several \nUnix-based TCP implementations, most all of which have a default 2-hour \ntimeout *before* a keepalive packet will be sent.\n\nI also note that Microsoft shows a default value of 2 hour idletime for the \nkeepalive timer in this doc:\n\n  http://www.microsoft.com/technet/winnt/reskit/sur_tcp2.asp\n\n\nSome questions (again, in the case of HTTP/1.1 persistent connections):\n\nQ1. Do the popular browsers typically take the platform's OS's TCP defaults \nfor\nthe keepalive (if such capability is provided by the TCP/IP stack, and if it \nis actually used by the browser), or do they typically set this value to \nsomething in particular?\n\n\nQ2. What typical assumptions are made on the browsers' parts about an \nestablished connection to a web site in the absence of user actions? If a\nbrowser opened a HTTP/1.1 connection and the server is behaving as-specified \nby RFC2616, then it is up to the browser to close the connection. What do \nbrowsers typically do? I looked through the documented configuration \nparameters for Netscape Communicator..\n\n  http://docs.iplanet.com/docs/manuals/communicator/newprefs/newprefn.html\n\n..and could not find a timeout setting that's applicable for this particular \ncase. How long will browsers, that are speaking HTTP/1.1, let this connection \nsit in the ESTABLISHED state?\n\n\nQ3. Are the popular browsers typically using HTTP/1.1, or HTTP/1.0? I didn't \nnotice any config parameters that might have something to do with setting the \ndefault.\n\nthanks,\n\nJeffH\n\n\n\n", "id": "lists-012-15188831"}, {"subject": "Re: questions regarding draft-ietf-http-authentication0", "content": "> > 5) For backwards compatibility with rfc-2069 there should be words to\n> >    explictly prevent a server from sending \"algorithm=MD5-sess\" but no\n> >    qop attribute. Maybe something like the following (just reusing the\n> >    wording from above again):\n[snip]\n> \n> I appreciate the intent, but I don't think this helps backward compatibility\n> at all. If the client doesn't understand MD5-Sess, then adding \"qop\" (which\n> isn't in 2069 either) won't help.\n\nHmm, yes, I think I got the logic backwards. Ok, scratch it.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1519646"}, {"subject": "RE: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "> -----Original Message-----\n> From: hodges@breakaway.Stanford.EDU\n> [mailto:hodges@breakaway.Stanford.EDU]On Behalf Of\n> Jeff.Hodges@KingsMountain.com\n> Sent: Thursday, 02 November 2000 8:44\n> To: http-wg@hplb.hpl.hp.com\n> Cc: Jeff.Hodges@KingsMountain.com\n> Subject: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n> \n> \n> I'm curious about how HTTP/1.1 [RFC2616] persistent \n> connections typically work \n> with respect to the typical browsers out in the wild today \n> (Netscape & \n> Microsoft being the two I'm particularly interested in). If I \n> cause a browser \n> to send a GET request for a given URL (using HTTP/1.1) to a \n> server, and the \n> server doesn't encounter any errors in processing it and \n> responding, and then \n> I (say) don't touch the browser for hours, what *typically* \n> happens to the \n> established HTTP/1.1 (-over-TCP) connection?\n> \n\n<<<SNIP>>>\n\n> \n> Some questions (again, in the case of HTTP/1.1 persistent \n> connections):\n> \n> Q1. Do the popular browsers typically take the platform's \n> OS's TCP defaults \n> for\n> the keepalive (if such capability is provided by the TCP/IP \n> stack, and if it \n> is actually used by the browser), or do they typically set \n> this value to \n> something in particular?\n\nHTTP/1.1 does not rely on the TCP timeout/keepalive. A HTTP/1.1 connected is alive only if the lower-layer connection is in the CONNECTED state.\n> \n> \n> Q2. What typical assumptions are made on the browsers' parts about an \n> established connection to a web site in the absence of user \n> actions? If a\n> browser opened a HTTP/1.1 connection and the server is \n> behaving as-specified \n> by RFC2616, then it is up to the browser to close the \n> connection. What do \n> browsers typically do? I looked through the documented configuration \n> parameters for Netscape Communicator..\n> \n>   \n> http://docs.iplanet.com/docs/manuals/communicator/newprefs/new\n> prefn.html\n> \n> ..and could not find a timeout setting that's applicable for this particular \n> case. How long will browsers, that are speaking HTTP/1.1, let this connection \n> sit in the ESTABLISHED state?\n\nThe browser can leave the connection open for an infinite ammount of time (if not restricted by lower-level protocols, like TCP).\n\nHowever, idle time-outs when the connection is CONNECTED are, in the real world, handled by the server instead of the client. This is because the server is not interrested in HTTP connections that are idle for an infinite ammount of time and just consuming bandwidth (the lower-level connection must be maintained)...\n\nIn the real world, the server does close the connection (when it's idle) without an explicit message that the connection is going to be closed. The server also cannot do this on HTTP level.\n\n\n\n> Q3. Are the popular browsers typically using HTTP/1.1, or HTTP/1.0? I didn't \n> notice any config parameters that might have something to do with setting the \n> default.\n\nHTTP/1.1, this version indicates the browser is capable of handling features that are not supported in HTTP/1.0 (such as HTTP keepalive).\n\nMSIE (for Windows) supports HTTP/1.1, but also HTTP/1.0 (default for proxy connections). Here the settings can be changed...\n\nThe version is also always send with a HTTP request or response (to indicate the browser/server capacties).\n\n> \n> thanks,\n> \n> JeffH\n> \n> \n\n\nHope to inform you. Maybe comments from the WG?\n\n\n- Joris\n\n\n\n", "id": "lists-012-15202565"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Jeff.Hodges@kingsmountain.com wrote:\n\n> I'm curious about how HTTP/1.1 [RFC2616] persistent connections typically work\n> with respect to the typical browsers out in the wild today (Netscape &\n> Microsoft being the two I'm particularly interested in). If I cause a browser\n> to send a GET request for a given URL (using HTTP/1.1) to a server, and the\n> server doesn't encounter any errors in processing it and responding, and then\n> I (say) don't touch the browser for hours, what *typically* happens to the\n> established HTTP/1.1 (-over-TCP) connection?\n\nThe web server will close the connection due to inactivity.\n\nYou have to realize that web servers are trying to service literally\nthousands of clients with only a relatively few TCP/IP connections.\nIf you don't use it you lose it.\n\nAlso, most web servers actively look for reasons to close a connection\nto a client. For example, if the web server generates any dynamic content\nfor the client, then it will usually close the connection after the response it\nsent back. Regardless of whether or not the client supplied a Connection:\nKeep-Alive header or not. The reasoning behind this is that if the sever\nhad to generate dynamic content on your behalf, then you've had your share\nand its time to give some other poor slob a turn.\n\n>\n>\n> I note that RFC2616 says (in part)..\n>\n>                              :\n> 8 Connections\n>\n> 8.1 Persistent Connections\n>                              :\n>    HTTP implementations SHOULD implement persistent connections.\n>                              :\n>    A significant difference between HTTP/1.1 and earlier versions of\n>    HTTP is that persistent connections are the default behavior of any\n>    HTTP connection. That is, unless otherwise indicated, the client\n>    SHOULD assume that the server will maintain a persistent connection,\n>    even after error responses from the server.\n>                              :\n>\n> As it is written, this effectively puts the responsibility for closing the\n> HTTP/1.1-cum-TCP connection on the client.\n\nNope. See my comments above.\n\nAlso, you have to realize that just cuz  the client sends a\nConnection: Keep-Alive header it is in no way a guarantee\nthat the server will not close the connection after the\nresponse is sent back.\n\nThere is a HUGE difference between the way the HTTP spec\nis written and the way that web server's are actually designed\nto work on the internet.\n\nI'm not saying that the web server designers violated the\nHTTP protocol. Rather they have simply done what they\nhave to do in order to protect their web server from\nattacks, deadlocks, and starving clients.\n\nRead further in the spec and you will see that the HTTP\nspec says that unsafe methods should not be pipelined.\n\nAn unsafe method is a method that in some sense changes\nthe state of the server and will not necessarily generate the\nsame response every time it is executed.\n\nOn the internet unsafe methods are typically used to\nrepresent a clients actions on the internet (i.e. I've just\nsent a request to buy product X with my credit card\nnumber aaaa-bbbb-cccc-dddd). Before submitting\nanother unsafe method I should be allowed to get\nfeedback about the current unsafe method and determine\nif I wish to proceed with the next unsafe method or not.\nSo, when receiving an unsafe method (POST) most\nweb servers will close the connection after the response\nis generated. Even if more unsafe methods have been sent\ninto the pipe. They are simply discarded.\n\n>\n>\n> In nosing around on this subject, I note that in [W.R.Stevens, TCP/IP\n> Illustrated Vol 1, http://www.dqc.org/~chris/tcpip_ill/], in chapter 23\n> Stevens says that..\n>\n> 1. \"Keepalives are not part of the TCP specification. ... Nevertheless, many\n> implementations provide the keep-alive timer.\"\n>\n> 2. \"If there is no activity on a given connection for 2 hours, the server\n> sends a probe segment to the client. ... A perpetual question by people\n> discovering the keepalive option is whether the 2-hour idle time value can be\n> changed. They normally want it much lower, on the order of minutes. As we show\n> in Appendix E, the value can usually be changed, but in all the systems\n> described in this appendix, the keepalive interval is a system-wide value, so\n> changing it affects all users of the option. \"\n>\n> ..and in appendix E he shows kernel configuration parameters for several\n> Unix-based TCP implementations, most all of which have a default 2-hour\n> timeout *before* a keepalive packet will be sent.\n>\n> I also note that Microsoft shows a default value of 2 hour idletime for the\n> keepalive timer in this doc:\n>\n>   http://www.microsoft.com/technet/winnt/reskit/sur_tcp2.asp\n\nHere I think you are confusing TCP/IP keep alive, with HTTP's\nConnection header and its Keep-Alive value.\n\nTrust me. These are two very different unrelatred things.\n\n>\n>\n> Some questions (again, in the case of HTTP/1.1 persistent connections):\n>\n> Q1. Do the popular browsers typically take the platform's OS's TCP defaults\n> for\n> the keepalive (if such capability is provided by the TCP/IP stack, and if it\n> is actually used by the browser), or do they typically set this value to\n> something in particular?\n\nBrowsers do not use this at all I'm quite sure.\n\n>\n>\n> Q2. What typical assumptions are made on the browsers' parts about an\n> established connection to a web site in the absence of user actions? If a\n> browser opened a HTTP/1.1 connection and the server is behaving as-specified\n> by RFC2616, then it is up to the browser to close the connection. What do\n> browsers typically do? I looked through the documented configuration\n> parameters for Netscape Communicator..\n>\n>   http://docs.iplanet.com/docs/manuals/communicator/newprefs/newprefn.html\n>\n> ..and could not find a timeout setting that's applicable for this particular\n> case. How long will browsers, that are speaking HTTP/1.1, let this connection\n> sit in the ESTABLISHED state?\n\nUntil the server closes it IMHO. The server will close the connection due\nto inactivity; 30 seconds by default for iPlanet web server.\n\n>\n>\n> Q3. Are the popular browsers typically using HTTP/1.1, or HTTP/1.0? I didn't\n> notice any config parameters that might have something to do with setting the\n> default.\n>\n> thanks,\n>\n> JeffH\n\n\n\n", "id": "lists-012-15215562"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Jeff.Hodges@KingsMountain.com wrote:\n\n> Q1. Do the popular browsers typically take the platform's OS's TCP defaults\n> for\n> the keepalive (if such capability is provided by the TCP/IP stack, and if it\n> is actually used by the browser), or do they typically set this value to\n> something in particular?\n\nSane applications typically do not use TCP keepalives.  TCP keepalives are a way\nof consuming bandwidth in order to make your application less reliable.\n\nSee, keepalives are misnamed; idle TCP connections will be kept alive\nindefinitely by default.  Using keepalives means you send an empty ACK every so\noften to see if the remote host is still up.  The problem is that sending that\nkeepalive will also mean you drop the connection if some intermediate router is\ndown.  But, if your link is idle, you don't care whether an intermediate router\nis down; you only care when you want to send data.  So keepalives just add\nanother way for your application to fail.\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |Two words that do not go together: \"Memorial |\n|francis@ecal.com|Cookbook\".                                   |\n\\==============================================================/\n\n\n\n", "id": "lists-012-15230155"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Thanks for all the info about how web servers are typically implemented. My \napologies tho, I didn't really make my questions clear. What I'm really  \ninterested in is how *browsers* are typically implemented, because, say, I'm \ngoing to write my own HTTP/1.1-speaking gizmo for a very application-specific \npurpose, and I have reasons to want it (the server side) to treat \nclient-initiated connections as persistent.\n\nSo this causes me to be curious about how BROWSERS will typically behave in \nthis context.\n\nAssuming one writes one's own HTTP/1.1-speaking gizmo (according to RFC2616), \nthen I have these questions...\n\n\nQ1. Do the popular BROWSERS typically take the platform's OS's TCP defaults \nfor the keepalive (IF such capability is provided by the TCP/IP stack, and IF \nit is actually used by the browser), or do they typically set this value to\nsomething in particular?\n\n\nQ2. What typical assumptions are made on the BROWSERS' parts about an \nestablished connection to a HTTP/1.1-speaking server in the absence of user \nactions? If a browser opened a HTTP/1.1 connection and such a server is \nbehaving as-specified by RFC2616, then it is up to the browser to close the \nconnection. What do browsers typically do?\n\nI looked through the documented configuration parameters for Netscape \nCommunicator..\n\n  http://docs.iplanet.com/docs/manuals/communicator/newprefs/newprefn.html\n\n..and could not find a timeout setting that's applicable for this particular \ncase. How long will BROWSERS, that are speaking HTTP/1.1, let this connection \nsit in the ESTABLISHED state?\n\n\nQ3. Are the popular BROWSERS typically speaking HTTP/1.1, or HTTP/1.0? I \ndidn't\nnotice any config parameters that might have something to do with setting the \ndefault.\n\n\nthanks again,\n\nJeffH\n\n\n\n", "id": "lists-012-15239707"}, {"subject": "RE: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "--\nScott Lawrence         Architect             <lawrence@agranat.com>\nVirata             http://www.virata.com/         http://emweb.com/\n\n> Q1. Do the popular BROWSERS typically take the platform's\n> OS's TCP defaults\n> for the keepalive (IF such capability is provided by the\n> TCP/IP stack, and IF\n> it is actually used by the browser), or do they typically\n> set this value to\n> something in particular?\n\nThey don't leave connections open and idle long enough for this to be\nrelevant.  Browsers typically close idle connections in one or two\nminutes, even if the server does not.\n\n> I looked through the documented configuration parameters\n> for Netscape\n> Communicator..\n\nCommunicator doesn't do 1.1 yet.\n\nQ3. Are the popular BROWSERS typically speaking HTTP/1.1, or HTTP/1.0?\nI\ndidn't\nnotice any config parameters that might have something to do with\nsetting the\ndefault.\n\nThe default for direct connections in IE is 1.1; I believe that Opera\nalso uses 1.1 by default now.\n\n\n\n\n", "id": "lists-012-15249564"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Jeff.Hodges@kingsmountain.com wrote:\n\n> Thanks for all the info about how web servers are typically implemented. My\n> apologies tho, I didn't really make my questions clear. What I'm really\n> interested in is how *browsers* are typically implemented, because, say, I'm\n> going to write my own HTTP/1.1-speaking gizmo for a very application-specific\n> purpose, and I have reasons to want it (the server side) to treat\n> client-initiated connections as persistent.\n>\n\nThe server-side will do whatever it wants no matter what you do on\nthe client side.\n\nBelieve me. I have been down this road.\n\nIf you are assuming that just because the HTTP protocol talks about\npersistent connections that you will be able to connect to a typical\nweb server and have the connection stay up as long as you do things\non the client side 'just right' you are mistaken.\n\nAs I said before web servers actively look for reasons to close connections.\nThere is _nothing_ that you can do on the client side to prevent this.\nSend Connection: Keep-Alive all you want. It will not keep the web\nserver from closing the connection whenever it darn well feels like it.\n\n>\n> So this causes me to be curious about how BROWSERS will typically behave in\n> this context.\n>\n> Assuming one writes one's own HTTP/1.1-speaking gizmo (according to RFC2616),\n> then I have these questions...\n>\n> Q1. Do the popular BROWSERS typically take the platform's OS's TCP defaults\n> for the keepalive (IF such capability is provided by the TCP/IP stack, and IF\n> it is actually used by the browser), or do they typically set this value to\n> something in particular?\n\nBrowsers do not use TCP/IP keep alive!\n\nAlmost no one uses TCP/IP keep alive mechanisms.\n\nAs John Stracke pointed out the use of TCP/IP keep\nalive actually makes your application less reliable\nand more likely to close an idle connection for no\nreason!\n\n>\n>\n> Q2. What typical assumptions are made on the BROWSERS' parts about an\n> established connection to a HTTP/1.1-speaking server in the absence of user\n> actions? If a browser opened a HTTP/1.1 connection and such a server is\n> behaving as-specified by RFC2616, then it is up to the browser to close the\n> connection. What do browsers typically do?\n>\n\nTypically, browsers do not close the connection.\n\nFor example, suppose the  server sends back an HTTP response to the client\nthat does not have a Content-Length: header field and that it is not\nchunked.\n\nThen the only way the client knows that it has read the\nentire response off of the pipe is when the server closes the connection.\nWhen the server closes the connection the client will receive a\nzero-byte read which is socket layer's indication that the pipe\nis broken.\n\n>\n> I looked through the documented configuration parameters for Netscape\n> Communicator..\n>\n>   http://docs.iplanet.com/docs/manuals/communicator/newprefs/newprefn.html\n>\n> ..and could not find a timeout setting that's applicable for this particular\n> case. How long will BROWSERS, that are speaking HTTP/1.1, let this connection\n> sit in the ESTABLISHED state?\n\nMy guess is that until the server closes the connection.\n\nWhich by the way, will be sooner than later.\n\n>\n>\n> Q3. Are the popular BROWSERS typically speaking HTTP/1.1, or HTTP/1.0? I\n> didn't\n> notice any config parameters that might have something to do with setting the\n> default.\n>\n\nI've done some snoops on browsers that are GETting HTML pages with\nlots of embedded links that are in the same realm as the original HTML\npage.\n\nDespite this obvious opportunity to take advantage of persistent connections\nthe browser opens  a connection for each subsequent GET.\n\nThis is true even though the browser is advertising that it supports HTTP/1.1\nMost browsers seem to be supporting receiveing of chunked dynamic content\nrather than persistent connections ...\n\n>\n> thanks again,\n>\n> JeffH\n\n\n\n", "id": "lists-012-15258542"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Jeff.Hodges@kingsmountain.com wrote:\n  > Thanks for all the info about how web servers are typically implemented. My \n  > apologies tho, I didn't really make my questions clear. What I'm really  \n  > interested in is how *browsers* are typically implemented, because, say, I'm \n  > going to write my own HTTP/1.1-speaking gizmo for a very application-specific \n  > purpose, and I have reasons to want it (the server side) to treat \n  > client-initiated connections as persistent.\n\nActually, I think most responses recognized that you were interested in\nbrowsers.  The problem is that the browser can only express its\npreference.  But the server is in command, and it can close a\nconnection whenever it feels it must.  That's why there were all the\ncomments on server behavior.  (In truth, the browser may *also* close\nthe connection at will, but that's a less common case.)\n\n  > \n  > So this causes me to be curious about how BROWSERS will typically behave in \n  > this context.\n  > \n  > Assuming one writes one's own HTTP/1.1-speaking gizmo (according to RFC2616), \n  > then I have these questions...\n  > \n  > \n\nI am not a browser implementer, so my answers are speculative, but....\n\n  > Q1. Do the popular BROWSERS typically take the platform's OS's TCP defaults \n  > for the keepalive (IF such capability is provided by the TCP/IP stack, and IF \n  > it is actually used by the browser), or do they typically set this value to\n  > something in particular?\n\nThey take the OS's defaults.\n\n  > \n  > \n  > Q2. What typical assumptions are made on the BROWSERS' parts about an \n  > established connection to a HTTP/1.1-speaking server in the absence of user \n  > actions? If a browser opened a HTTP/1.1 connection and such a server is \n  > behaving as-specified by RFC2616, then it is up to the browser to close the \n  > connection. What do browsers typically do?\n\nBrowsers probably open connections with the hope of keeping them open as\nlong as possible.  But as I said earlier, that doesn't mean they will stay\nopen.  I believe the only way the client discovers the connection has been\nclosed is by getting an error response when it tries to write to a (now-closed)\nconnection.\n\n[Rest deleted -- I believe the questions were answered earlier.]\n\nDave Kristol\n\n\n\n", "id": "lists-012-15270220"}, {"subject": "Re: Comments on draft-ietf-http-v11-spec-rev0", "content": "    > How does\n    > mark the cached object as \"must-revalidate\"\n    > differ from\n    > invalidate the cached object\n    > \n    In my mind, at least, invalidate implies \"never return it in\n    response to any later request\", and usually, that means to delete\n    it from the cache. Whereas \"must-revalidate\" implies \"keep the bits\n    in the cache, but do a conditional GET (or whatever) before\n    returning them in any later request\".\n\n    Thus, even if they are technically identical, the implication I\n    would form upon reading the two alternatives are quite different.\n\nI would agree that someone reading the phrase \"should invalidate any\nentities\" who has no understanding of the \"HTTP way\", and who has\nan understanding of CPU cache design, might understand it the way you did.\n\nHowever, section 13.10 (the one in question) says specifically:\n\n  In this section, the phrase \"invalidate an entity\" means that the cache\n  should either remove all instances of that entity from its storage, or\n  should mark these as \"invalid\" and in need of a mandatory revalidation\n  before they can be returned in response to a subsequent request.\n\nWe added this paragraph precisely because the term \"invalidate\"\nwas being used in various different ways by different people.\n\n-Jeff\n\n\n\n", "id": "lists-012-1527621"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "On Thu, 2 Nov 2000, James Lacey wrote:\n\n> Jeff.Hodges@kingsmountain.com wrote:\n> \n> > I'm curious about how HTTP/1.1 [RFC2616] persistent connections typically work\n> > with respect to the typical browsers out in the wild today (Netscape &\n> > Microsoft being the two I'm particularly interested in). If I cause a browser\n> > to send a GET request for a given URL (using HTTP/1.1) to a server, and the\n> > server doesn't encounter any errors in processing it and responding, and then\n> > I (say) don't touch the browser for hours, what *typically* happens to the\n> > established HTTP/1.1 (-over-TCP) connection?\n> \n> The web server will close the connection due to inactivity.\n> \n> You have to realize that web servers are trying to service literally\n> thousands of clients with only a relatively few TCP/IP connections.\n> If you don't use it you lose it.\n\nIt is fair to say that the client has no idea of what sort of resource \nlimits the server may have, so it should leave the connection open \nunless it has some particular reason (eg. wanting to limit the total\nnumber of connections it has open to all servers) to close it.  In \nreality, common browsers probably put some fixed time limit on how long\nthey keep a connection open; if anyone wants to know exactly what \nwhatever browsers they are concerned with do, try it.\n\nOn the same token, the server has no great desire to arbitrarily close \nconnections, so it will leave it open unless it has some \"reason\" to \nclose it.  Currently, many servers use a very simple metric for this,\nwhich is a fixed timeout of x seconds after the last response was \nsent.  This is arguably a very poor metric to use, there has been some \nsmall amount of research on adaptive keepalive timeouts based on load.\nThe server, however, has a lot more information on which to base a decision\nto close a connection.\n\nIt isn't that this sort of behaviour doesn't follow the spec, but simply\nthat there is no need to embed this sort of runtime information in a\nprotocol spec.  If you read the spec carefully, you will note it goes to\ngreat lengths to ensure that the server may close the connection whenever\nit is idle, which requires things like a half-duplex close, etc.\n\nThe bottom line, however, is that if you open a connection and make \none request every few hours, I really don't see why you should care\nif it is persistent or not, since persistent connections are just an\noptimization.\n\n> Also, most web servers actively look for reasons to close a connection\n> to a client. For example, if the web server generates any dynamic content\n> for the client, then it will usually close the connection after the response it\n> sent back. Regardless of whether or not the client supplied a Connection:\n> Keep-Alive header or not. The reasoning behind this is that if the sever\n> had to generate dynamic content on your behalf, then you've had your share\n> and its time to give some other poor slob a turn.\n\nHuh?  That would be a webserver with some very... odd ideas.\n\nIf you are talking about \"Connection: Keep-Alive\" then you appear to be\ntalking about HTTP/1.0, in which there is no chunked encoding so\nunless the server puts a content-length on its dynamic content\n(which is perfectly possible for it to do, but many don't for\nreasons that are also perfectly legitimate) then there is no way\nto use a persistent connection since in that case the only\nend-of-reponse marker you have is the close of connection.\n\n> \n> >\n> >\n> > I note that RFC2616 says (in part)..\n> >\n> >                              :\n> > 8 Connections\n> >\n> > 8.1 Persistent Connections\n> >                              :\n> >    HTTP implementations SHOULD implement persistent connections.\n> >                              :\n> >    A significant difference between HTTP/1.1 and earlier versions of\n> >    HTTP is that persistent connections are the default behavior of any\n> >    HTTP connection. That is, unless otherwise indicated, the client\n> >    SHOULD assume that the server will maintain a persistent connection,\n> >    even after error responses from the server.\n> >                              :\n> >\n> > As it is written, this effectively puts the responsibility for closing the\n> > HTTP/1.1-cum-TCP connection on the client.\n> \n> Nope. See my comments above.\n> \n> Also, you have to realize that just cuz  the client sends a\n> Connection: Keep-Alive header it is in no way a guarantee\n> that the server will not close the connection after the\n> response is sent back.\n> \n> There is a HUGE difference between the way the HTTP spec\n> is written and the way that web server's are actually designed\n> to work on the internet.\n\nThe HTTP spec also doesn't describe what sort of config files you\nshould use for your server, since that isn't part of the protocol either.\n\n> \n> I'm not saying that the web server designers violated the\n> HTTP protocol. Rather they have simply done what they\n> have to do in order to protect their web server from\n> attacks, deadlocks, and starving clients.\n> \n> Read further in the spec and you will see that the HTTP\n> spec says that unsafe methods should not be pipelined.\n> \n> An unsafe method is a method that in some sense changes\n> the state of the server and will not necessarily generate the\n> same response every time it is executed.\n> \n> On the internet unsafe methods are typically used to\n> represent a clients actions on the internet (i.e. I've just\n> sent a request to buy product X with my credit card\n> number aaaa-bbbb-cccc-dddd). Before submitting\n> another unsafe method I should be allowed to get\n> feedback about the current unsafe method and determine\n> if I wish to proceed with the next unsafe method or not.\n> So, when receiving an unsafe method (POST) most\n> web servers will close the connection after the response\n> is generated. Even if more unsafe methods have been sent\n> into the pipe. They are simply discarded.\n\nUmh... again, your reasoning here is a little confused.  Closing the \nconnection after sending the response to a POST by no means ensures that\nthis problem is avoided and, at that state of the game, is pointless.\n\nIt is legitimate to send a non-idempotent request with several idempotent\nrequests pipelined after it.\n\nI think you are again getting confused by the requirement for a\ncontent-length or chunking or the lack of a response body in order to\ndo persistent connections.\n\n\n\n", "id": "lists-012-15280764"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "> They [browsers] don't leave connections open and idle long enough for this\n> to be relevant.  Browsers typically close idle connections in one or two\n> minutes, even if the server does not.\n\nThanks, this is the sort of info I am interested in.\n\nWhen I said \"say I'm going to write my own HTTP/1.1-speaking gizmo\", I should \nprobably have made it clear that I am postulating a server-side gizmo, hence \nbeing interested in how the browsers out there in the wild will behave if this \nserver-side gizmo doesn't behave just like typical existing web servers.\n\n> Communicator doesn't do 1.1 yet.\n>\n> [...]\n> \n> The default for direct connections in IE is 1.1; I believe that Opera\n> also uses 1.1 by default now.\n\nInnaresting. Does anyone have a pointer handy to any documentation for any of \nthe popular browsers that indicates what all versions of what all protocols \nthey do speak? An admittedly cursory search for such wrt Communicator didn't \nturn up anything.\n\nthanks,\n\nJeffH\n\n\n\n", "id": "lists-012-15295952"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "...\n>\n>For example, suppose the  server sends back an HTTP response to the client\n>that does not have a Content-Length: header field and that it is not\n>chunked.\n>\n>Then the only way the client knows that it has read the\n>entire response off of the pipe is when the server closes the connection.\n>When the server closes the connection the client will receive a\n>zero-byte read which is socket layer's indication that the pipe\n>is broken.\n>\nThis is not good.  If this is the server's normal behavior, the client has\nno way to distinguish a dropped connection from end of file.  So the client\ncan never be sure it received an entire message.\n\nIf this is the server's behavior for error conditions, this is still not\ngood unless the server either waits for the entire request (could be a\nhumongous POST and/or a very slow connection), or only closes one half of\nthe connection (which, BTW, is impossible in Java, except maybe in the\nlatest releases).  Otherwise, the client might get a RST while transmitting\nthe request, and will then never see the error response.\n\n...\n>\n>I've done some snoops on browsers that are GETting HTML pages with\n>lots of embedded links that are in the same realm as the original HTML\n>page.\n>\n>Despite this obvious opportunity to take advantage of persistent connections\n>the browser opens  a connection for each subsequent GET.\n>\nI was looking at IE's (version 5 somthing) traffic yesterday, and it seems to send two requests per connection.\n\n\n     -Carl\n\n\n\n", "id": "lists-012-15304553"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Marc Slemko wrote:\n\n> On Thu, 2 Nov 2000, James Lacey wrote:\n>\n> > Jeff.Hodges@kingsmountain.com wrote:\n> >\n> > > I'm curious about how HTTP/1.1 [RFC2616] persistent connections typically work\n> > > with respect to the typical browsers out in the wild today (Netscape &\n> > > Microsoft being the two I'm particularly interested in). If I cause a browser\n> > > to send a GET request for a given URL (using HTTP/1.1) to a server, and the\n> > > server doesn't encounter any errors in processing it and responding, and then\n> > > I (say) don't touch the browser for hours, what *typically* happens to the\n> > > established HTTP/1.1 (-over-TCP) connection?\n> >\n> > The web server will close the connection due to inactivity.\n> >\n> > You have to realize that web servers are trying to service literally\n> > thousands of clients with only a relatively few TCP/IP connections.\n> > If you don't use it you lose it.\n>\n> It is fair to say that the client has no idea of what sort of resource\n> limits the server may have, so it should leave the connection open\n> unless it has some particular reason (eg. wanting to limit the total\n> number of connections it has open to all servers) to close it.  In\n> reality, common browsers probably put some fixed time limit on how long\n> they keep a connection open; if anyone wants to know exactly what\n> whatever browsers they are concerned with do, try it.\n>\n> On the same token, the server has no great desire to arbitrarily close\n> connections, so it will leave it open unless it has some \"reason\" to\n> close it.  Currently, many servers use a very simple metric for this,\n> which is a fixed timeout of x seconds after the last response was\n> sent.  This is arguably a very poor metric to use, there has been some\n> small amount of research on adaptive keepalive timeouts based on load.\n> The server, however, has a lot more information on which to base a decision\n> to close a connection.\n>\n> It isn't that this sort of behaviour doesn't follow the spec, but simply\n> that there is no need to embed this sort of runtime information in a\n> protocol spec.  If you read the spec carefully, you will note it goes to\n> great lengths to ensure that the server may close the connection whenever\n> it is idle, which requires things like a half-duplex close, etc.\n>\n> The bottom line, however, is that if you open a connection and make\n> one request every few hours, I really don't see why you should care\n> if it is persistent or not, since persistent connections are just an\n> optimization.\n>\n> > Also, most web servers actively look for reasons to close a connection\n> > to a client. For example, if the web server generates any dynamic content\n> > for the client, then it will usually close the connection after the response it\n> > sent back. Regardless of whether or not the client supplied a Connection:\n> > Keep-Alive header or not. The reasoning behind this is that if the sever\n> > had to generate dynamic content on your behalf, then you've had your share\n> > and its time to give some other poor slob a turn.\n>\n> Huh?  That would be a webserver with some very... odd ideas.\n>\n> If you are talking about \"Connection: Keep-Alive\" then you appear to be\n> talking about HTTP/1.0, in which there is no chunked encoding so\n> unless the server puts a content-length on its dynamic content\n> (which is perfectly possible for it to do, but many don't for\n> reasons that are also perfectly legitimate) then there is no way\n> to use a persistent connection since in that case the only\n> end-of-reponse marker you have is the close of connection.\n\nAs a concrete example the iPlanet Enterprise v4.1 server always\ncloses the connection after responding to a POST request or\nanytime that dynamic content is generated (possibly because\nthe response does not have a Content-Length: header field\nand the response is not chunked).\n\nI have verified this and it is clearly documented in their\nliterature.\n\n\n>\n>\n> >\n> > >\n> > >\n> > > I note that RFC2616 says (in part)..\n> > >\n> > >                              :\n> > > 8 Connections\n> > >\n> > > 8.1 Persistent Connections\n> > >                              :\n> > >    HTTP implementations SHOULD implement persistent connections.\n> > >                              :\n> > >    A significant difference between HTTP/1.1 and earlier versions of\n> > >    HTTP is that persistent connections are the default behavior of any\n> > >    HTTP connection. That is, unless otherwise indicated, the client\n> > >    SHOULD assume that the server will maintain a persistent connection,\n> > >    even after error responses from the server.\n> > >                              :\n> > >\n> > > As it is written, this effectively puts the responsibility for closing the\n> > > HTTP/1.1-cum-TCP connection on the client.\n> >\n> > Nope. See my comments above.\n> >\n> > Also, you have to realize that just cuz  the client sends a\n> > Connection: Keep-Alive header it is in no way a guarantee\n> > that the server will not close the connection after the\n> > response is sent back.\n> >\n> > There is a HUGE difference between the way the HTTP spec\n> > is written and the way that web server's are actually designed\n> > to work on the internet.\n>\n> The HTTP spec also doesn't describe what sort of config files you\n> should use for your server, since that isn't part of the protocol either.\n>\n> >\n> > I'm not saying that the web server designers violated the\n> > HTTP protocol. Rather they have simply done what they\n> > have to do in order to protect their web server from\n> > attacks, deadlocks, and starving clients.\n> >\n> > Read further in the spec and you will see that the HTTP\n> > spec says that unsafe methods should not be pipelined.\n> >\n> > An unsafe method is a method that in some sense changes\n> > the state of the server and will not necessarily generate the\n> > same response every time it is executed.\n> >\n> > On the internet unsafe methods are typically used to\n> > represent a clients actions on the internet (i.e. I've just\n> > sent a request to buy product X with my credit card\n> > number aaaa-bbbb-cccc-dddd). Before submitting\n> > another unsafe method I should be allowed to get\n> > feedback about the current unsafe method and determine\n> > if I wish to proceed with the next unsafe method or not.\n> > So, when receiving an unsafe method (POST) most\n> > web servers will close the connection after the response\n> > is generated. Even if more unsafe methods have been sent\n> > into the pipe. They are simply discarded.\n>\n> Umh... again, your reasoning here is a little confused.  Closing the\n> connection after sending the response to a POST by no means ensures that\n> this problem is avoided and, at that state of the game, is pointless.\n>\n> It is legitimate to send a non-idempotent request with several idempotent\n> requests pipelined after it.\n>\n> I think you are again getting confused by the requirement for a\n> content-length or chunking or the lack of a response body in order to\n> do persistent connections.\n\nNope. I'm clear on that. All I was trying to say was that some\nweb servers always close the connection after they have processed\nwhat they believe to be a non-idempotent request; iPlanet is\na case in point.\n\n\n\n", "id": "lists-012-15314099"}, {"subject": "RE: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "The decision on when to close is left to either side.  A server will\nclose the connection based on its resource-consumption requirements\nwhich may vary substantially based on the type of server and the\nnumber of clients it is intended to serve.  A client will close the\nconnection if it is connection-limited and needs to open many other\nconnections, or if it just believes in being network friendly.\n\nUnfortunately, none of the major browsers are network friendly,\nso they typically ignore the connection (not even recognizing FIN\nas an event) until they later attempt to use it again.  Most\ngeneral-purpose servers have a short activity time-out on\nconnections and will close the connection after that time-out\n(typically under 10 seconds, though a high-activity server will\nset this to one second or turn off persistent connections altogether).\n\nCheers,\n\nRoy T. Fielding, Chief Scientist, eBuilt, Inc.            (www.ebuilt.com)\n                 Chairman, The Apache Software Foundation (www.apache.org)  \n\n\n\n", "id": "lists-012-15329876"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "On Thu, 2 Nov 2000, James Lacey wrote:\n\n> > > to a client. For example, if the web server generates any dynamic content\n> > > for the client, then it will usually close the connection after the response it\n> > > sent back. Regardless of whether or not the client supplied a Connection:\n> > > Keep-Alive header or not. The reasoning behind this is that if the sever\n> > > had to generate dynamic content on your behalf, then you've had your share\n> > > and its time to give some other poor slob a turn.\n> >\n> > Huh?  That would be a webserver with some very... odd ideas.\n> >\n> > If you are talking about \"Connection: Keep-Alive\" then you appear to be\n> > talking about HTTP/1.0, in which there is no chunked encoding so\n> > unless the server puts a content-length on its dynamic content\n> > (which is perfectly possible for it to do, but many don't for\n> > reasons that are also perfectly legitimate) then there is no way\n> > to use a persistent connection since in that case the only\n> > end-of-reponse marker you have is the close of connection.\n> \n> As a concrete example the iPlanet Enterprise v4.1 server always\n> closes the connection after responding to a POST request or\n> anytime that dynamic content is generated (possibly because\n> the response does not have a Content-Length: header field\n> and the response is not chunked).\n> \n> I have verified this and it is clearly documented in their\n> literature.\n\nYou are missing the point; it doesn't do it out of some odd desire\nto \"share\" since a client has \"had it's fill of dynamic content\nfor now\", like you suggested it does.\n\n\n\n", "id": "lists-012-15339040"}, {"subject": "RE: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "What experimentation, if any, has been done with adaptive server timeouts? A\nproject I'm working on reduces timeouts based on current load. When a\ncertain point is reached connections are closed immediately until the load\ndrops down.\n\nI haven't got as far as testing this on a live site so I'm curious whether\nanyone has any real world experience.\n\n> -----Original Message-----\n> From: Fielding, Roy [mailto:fielding@eBuilt.com]\n> Sent: Thursday, November 02, 2000 1:08 PM\n> To: 'Jeff.Hodges@kingsmountain.com'; http-wg@hplb.hpl.hp.com\n> Subject: RE: Of HTTP/1.1 persistent connections and TCP \n> Keepalive timers\n> \n> \n> The decision on when to close is left to either side.  A server will\n> close the connection based on its resource-consumption requirements\n> which may vary substantially based on the type of server and the\n> number of clients it is intended to serve.  A client will close the\n> connection if it is connection-limited and needs to open many other\n> connections, or if it just believes in being network friendly.\n> \n> Unfortunately, none of the major browsers are network friendly,\n> so they typically ignore the connection (not even recognizing FIN\n> as an event) until they later attempt to use it again.  Most\n> general-purpose servers have a short activity time-out on\n> connections and will close the connection after that time-out\n> (typically under 10 seconds, though a high-activity server will\n> set this to one second or turn off persistent connections altogether).\n> \n> Cheers,\n> \n> Roy T. Fielding, Chief Scientist, eBuilt, Inc.            \n> (www.ebuilt.com)\n>                  Chairman, The Apache Software Foundation \n(www.apache.org)  \n\n\n\n", "id": "lists-012-15348556"}, {"subject": "HTTP features w/ low  'implemented' and 'tested", "content": "There've been nine implementation reports so far.\n\nThis message summarizes some of the troublesome issues:\n\nThe following features are not marked as implemented by two or more\nimplementations:\n\nH 10.1.2 101 Switching Protocols\nH 10.2.4 203 Non-Authoritative Information\nH 10.2.6 205 Reset Content\nH 14.15 Content-MD5\nH 14.42 Upgrade\nA 3.2.1, A 3.2.2, A3.2.3, A4.2 (all)\n\nIn addition, the following features are claimed to be implemented\nby two or more, but have not been tested by two or more independent\nimplementations:\n\n H 10.2.2 201 Created\n*H 10.2.3 202 Accepted\n H 10.3.6 305 Use Proxy\n*H 10.4.3 402 Payment Required\n H 10.4.9 408 Request Timeout\n H 10.4.10 409 Conflict\n H 10.4.11 410 Gone\n H 10.4.12 411 Length Required\n H 10.4.14 413 Request Entity Too Large\n H 10.4.15 414 Request-URI Too Long\n*H 13.3.3 Weak entity tags\n*H 14.37 Retry-After\n H 14.39 TE\n\nThe features except those marked with \"*\" were\ntested by one implementation against another,\nbut the testing wasn't mutual.\n\nIt would be good to focus on broadening the testing\nfor each of these features, though.\n\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-1536001"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Fred Bohle@NEON\n11/02/2000 12:27 PM\n\n\n     We seem to be diverging into TCP coding.  A read will return zero\nlength\nwhen the other end has issued a normal close (and all the data has been\nread).\nA read will return -1 when the connection is ReSeT, or there is a\nconnection time-out\nof any sort.  So the server can too tell the difference between end of data\nand\na connection failure.\n\nFred\n\n\n\n\nFrom: Carl Kugler/Boulder/IBM <kugler@us.ibm.com> on 11/02/2000 11:59 AM\n\nTo:   James Lacey <James.Lacey@Motorola.com>\ncc:   http-wg <http-wg@hplb.hpl.hp.com>\n\nSubject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n\n\n...\n>\n>For example, suppose the  server sends back an HTTP response to the client\n>that does not have a Content-Length: header field and that it is not\n>chunked.\n>\n>Then the only way the client knows that it has read the\n>entire response off of the pipe is when the server closes the connection.\n>When the server closes the connection the client will receive a\n>zero-byte read which is socket layer's indication that the pipe\n>is broken.\n>\nThis is not good.  If this is the server's normal behavior, the client has\nno way to distinguish a dropped connection from end of file.  So the client\ncan never be sure it received an entire message.\n\nIf this is the server's behavior for error conditions, this is still not\ngood unless the server either waits for the entire request (could be a\nhumongous POST and/or a very slow connection), or only closes one half of\nthe connection (which, BTW, is impossible in Java, except maybe in the\nlatest releases).  Otherwise, the client might get a RST while transmitting\nthe request, and will then never see the error response.\n\n...\n>\n>I've done some snoops on browsers that are GETting HTML pages with\n>lots of embedded links that are in the same realm as the original HTML\n>page.\n>\n>Despite this obvious opportunity to take advantage of persistent\nconnections\n>the browser opens  a connection for each subsequent GET.\n>\nI was looking at IE's (version 5 somthing) traffic yesterday, and it seems\nto send two requests per connection.\n\n\n     -Carl\n\n\n\n", "id": "lists-012-15360157"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive time", "content": "\"Fielding, Roy\" wrote:\n> \n> The decision on when to close is left to either side.  A server will\n> close the connection based on its resource-consumption requirements\n> which may vary substantially based on the type of server and the\n> number of clients it is intended to serve.  A client will close the\n> connection if it is connection-limited and needs to open many other\n> connections, or if it just believes in being network friendly.\n> \n> Unfortunately, none of the major browsers are network friendly,\n> so they typically ignore the connection (not even recognizing FIN\n> as an event) until they later attempt to use it again.  Most\n> general-purpose servers have a short activity time-out on\n> connections and will close the connection after that time-out\n> (typically under 10 seconds, though a high-activity server will\n> set this to one second or turn off persistent connections altogether).\n> \n\n\nApache server uses 15 seconds as a default timeout and 100 connections\nas the number of idle persistent connections. Over 60 percent of Web\nservers use Apache.\n\n\n\n", "id": "lists-012-15371360"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "I stand corrected.  But then, why was Content-Length added in HTTP/1.0 and\nTranfer-Encoding: chunked in HTTP/1.1?\n\n     -Carl\n\n\n\n\"Fred Bohle\" <fbohle@neonsys.com> on 11/02/2000 11:27:24 AM\n\nTo:   Carl Kugler/Boulder/IBM@IBMUS\ncc:   James Lacey <James.Lacey@Motorola.com>, http-wg\n      <http-wg@hplb.hpl.hp.com>\nSubject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n\n\n\n\n\n\n\nFred Bohle@NEON\n11/02/2000 12:27 PM\n\n\n     We seem to be diverging into TCP coding.  A read will return zero\nlength\nwhen the other end has issued a normal close (and all the data has been\nread).\nA read will return -1 when the connection is ReSeT, or there is a\nconnection time-out\nof any sort.  So the server can too tell the difference between end of data\nand\na connection failure.\n\nFred\n\n\n\n\nFrom: Carl Kugler/Boulder/IBM <kugler@us.ibm.com> on 11/02/2000 11:59 AM\n\nTo:   James Lacey <James.Lacey@Motorola.com>\ncc:   http-wg <http-wg@hplb.hpl.hp.com>\n\nSubject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n\n\n...\n>\n>For example, suppose the  server sends back an HTTP response to the client\n>that does not have a Content-Length: header field and that it is not\n>chunked.\n>\n>Then the only way the client knows that it has read the\n>entire response off of the pipe is when the server closes the connection.\n>When the server closes the connection the client will receive a\n>zero-byte read which is socket layer's indication that the pipe\n>is broken.\n>\nThis is not good.  If this is the server's normal behavior, the client has\nno way to distinguish a dropped connection from end of file.  So the client\ncan never be sure it received an entire message.\n\nIf this is the server's behavior for error conditions, this is still not\ngood unless the server either waits for the entire request (could be a\nhumongous POST and/or a very slow connection), or only closes one half of\nthe connection (which, BTW, is impossible in Java, except maybe in the\nlatest releases).  Otherwise, the client might get a RST while transmitting\nthe request, and will then never see the error response.\n\n...\n>\n>I've done some snoops on browsers that are GETting HTML pages with\n>lots of embedded links that are in the same realm as the original HTML\n>page.\n>\n>Despite this obvious opportunity to take advantage of persistent\nconnections\n>the browser opens  a connection for each subsequent GET.\n>\nI was looking at IE's (version 5 somthing) traffic yesterday, and it seems\nto send two requests per connection.\n\n\n     -Carl\n\n\n\n", "id": "lists-012-15379778"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive time", "content": ">Sent by: hkchoi@cc.gatech.edu\n>\n>\"Fielding, Roy\" wrote:\n>>\n>> The decision on when to close is left to either side.  A server will\n>> close the connection based on its resource-consumption requirements\n>> which may vary substantially based on the type of server and the\n>> number of clients it is intended to serve.  A client will close the\n>> connection if it is connection-limited and needs to open many other\n>> connections, or if it just believes in being network friendly.\n>>\n>> Unfortunately, none of the major browsers are network friendly,\n>> so they typically ignore the connection (not even recognizing FIN\n>> as an event) until they later attempt to use it again.  Most\n>> general-purpose servers have a short activity time-out on\n>> connections and will close the connection after that time-out\n>> (typically under 10 seconds, though a high-activity server will\n>> set this to one second or turn off persistent connections altogether).\n>>\n>\n>\nIdeally, you'd want the client to close the connection.  If the server\ncloses it, it has to keep the connection around in TIME_WAIT state for four\nminutes, right?  So a server that agressively closes idle connections might\nmake matters worse for itself if the client makes requests more often than\nonce every 4 minutes (for example, the Windows 2000 IPP client polling\njob-status evey minute or so).\n\n     -Carl\n\n\n\n", "id": "lists-012-15392920"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Carl Kugler/Boulder/IBM wrote:\n\n> I stand corrected.  But then, why was Content-Length added in HTTP/1.0 and\n> Tranfer-Encoding: chunked in HTTP/1.1?\n\nContent-Length was added so that persistent connections would be possible.\n\nWithout a content length, neither the client or the server would know\nwhere the message ended. You have to realize that TCP is a stream and\nit is up to the users of the stream to impose some kind of framing protocol\nonto the stream.\n\nTransfer-Encoding: chunked was added so that servers could return\npossibly large dynamic content in chunks, as it was generated, so that\nthe server did not have to buffer up all of the dynamic content before\nsending it back. With chunking you can also have persistent connections\nw/o the need of a Content-Length header field.\n\nOne curious think about chunking that I do not understand is that\nHTTP proxies are required to move the chunked (any?) encoding before\nforwarding the response.\n\n>\n>\n>      -Carl\n>\n> \"Fred Bohle\" <fbohle@neonsys.com> on 11/02/2000 11:27:24 AM\n>\n> To:   Carl Kugler/Boulder/IBM@IBMUS\n> cc:   James Lacey <James.Lacey@Motorola.com>, http-wg\n>       <http-wg@hplb.hpl.hp.com>\n> Subject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n>\n> Fred Bohle@NEON\n> 11/02/2000 12:27 PM\n>\n>      We seem to be diverging into TCP coding.  A read will return zero\n> length\n> when the other end has issued a normal close (and all the data has been\n> read).\n> A read will return -1 when the connection is ReSeT, or there is a\n> connection time-out\n> of any sort.  So the server can too tell the difference between end of data\n> and\n> a connection failure.\n>\n> Fred\n>\n> From: Carl Kugler/Boulder/IBM <kugler@us.ibm.com> on 11/02/2000 11:59 AM\n>\n> To:   James Lacey <James.Lacey@Motorola.com>\n> cc:   http-wg <http-wg@hplb.hpl.hp.com>\n>\n> Subject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n>\n> ...\n> >\n> >For example, suppose the  server sends back an HTTP response to the client\n> >that does not have a Content-Length: header field and that it is not\n> >chunked.\n> >\n> >Then the only way the client knows that it has read the\n> >entire response off of the pipe is when the server closes the connection.\n> >When the server closes the connection the client will receive a\n> >zero-byte read which is socket layer's indication that the pipe\n> >is broken.\n> >\n> This is not good.  If this is the server's normal behavior, the client has\n> no way to distinguish a dropped connection from end of file.  So the client\n> can never be sure it received an entire message.\n>\n> If this is the server's behavior for error conditions, this is still not\n> good unless the server either waits for the entire request (could be a\n> humongous POST and/or a very slow connection), or only closes one half of\n> the connection (which, BTW, is impossible in Java, except maybe in the\n> latest releases).  Otherwise, the client might get a RST while transmitting\n> the request, and will then never see the error response.\n>\n> ...\n> >\n> >I've done some snoops on browsers that are GETting HTML pages with\n> >lots of embedded links that are in the same realm as the original HTML\n> >page.\n> >\n> >Despite this obvious opportunity to take advantage of persistent\n> connections\n> >the browser opens  a connection for each subsequent GET.\n> >\n> I was looking at IE's (version 5 somthing) traffic yesterday, and it seems\n> to send two requests per connection.\n>\n>      -Carl\n\n\n\n", "id": "lists-012-15402318"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Marc Slemko wrote:\n\n> On Thu, 2 Nov 2000, James Lacey wrote:\n>\n> > > > to a client. For example, if the web server generates any dynamic content\n> > > > for the client, then it will usually close the connection after the response it\n> > > > sent back. Regardless of whether or not the client supplied a Connection:\n> > > > Keep-Alive header or not. The reasoning behind this is that if the sever\n> > > > had to generate dynamic content on your behalf, then you've had your share\n> > > > and its time to give some other poor slob a turn.\n> > >\n> > > Huh?  That would be a webserver with some very... odd ideas.\n> > >\n> > > If you are talking about \"Connection: Keep-Alive\" then you appear to be\n> > > talking about HTTP/1.0, in which there is no chunked encoding so\n> > > unless the server puts a content-length on its dynamic content\n> > > (which is perfectly possible for it to do, but many don't for\n> > > reasons that are also perfectly legitimate) then there is no way\n> > > to use a persistent connection since in that case the only\n> > > end-of-reponse marker you have is the close of connection.\n> >\n> > As a concrete example the iPlanet Enterprise v4.1 server always\n> > closes the connection after responding to a POST request or\n> > anytime that dynamic content is generated (possibly because\n> > the response does not have a Content-Length: header field\n> > and the response is not chunked).\n> >\n> > I have verified this and it is clearly documented in their\n> > literature.\n>\n> You are missing the point; it doesn't do it out of some odd desire\n> to \"share\" since a client has \"had it's fill of dynamic content\n> for now\", like you suggested it does.\n\nOK. OK. I get your point.\n\nIt may not be out of some odd desire to \"share\".\n\nBut it is certainly based on the idea that many\nclients need to be serviced with relatively few\nconnections.\n\n\n\n", "id": "lists-012-15416346"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": ">Sent by: jlacey@mothost.mot.com\n>Carl Kugler/Boulder/IBM wrote:\n>\n>> I stand corrected.  But then, why was Content-Length added in HTTP/1.0\nand\n>> Tranfer-Encoding: chunked in HTTP/1.1?\n>\n>Content-Length was added so that persistent connections would be possible.\n>\n>Without a content length, neither the client or the server would know\n>where the message ended. You have to realize that TCP is a stream and\n>it is up to the users of the stream to impose some kind of framing\nprotocol\n>onto the stream.\n>\n>Transfer-Encoding: chunked was added so that servers could return\n>possibly large dynamic content in chunks, as it was generated, so that\n>the server did not have to buffer up all of the dynamic content before\n>sending it back. With chunking you can also have persistent connections\n>w/o the need of a Content-Length header field.\n>\nAh yes, make perfect sense now.\n\n>One curious think about chunking that I do not understand is that\n>HTTP proxies are required to move the chunked (any?) encoding before\n>forwarding the response.\n>\nThat's news to me.  Doesn't that apply only to MIME gateways or something?\nWouldn't that require a potentially infinite buffer in the proxy?  Wouldn't\nit have a horrible impact on response time?\n\n     -Carl\n\n>>\n>>\n>>      -Carl\n>>\n>> \"Fred Bohle\" <fbohle@neonsys.com> on 11/02/2000 11:27:24 AM\n>>\n>> To:   Carl Kugler/Boulder/IBM@IBMUS\n>> cc:   James Lacey <James.Lacey@Motorola.com>, http-wg\n>>       <http-wg@hplb.hpl.hp.com>\n>> Subject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n>>\n>> Fred Bohle@NEON\n>> 11/02/2000 12:27 PM\n>>\n>>      We seem to be diverging into TCP coding.  A read will return zero\n>> length\n>> when the other end has issued a normal close (and all the data has been\n>> read).\n>> A read will return -1 when the connection is ReSeT, or there is a\n>> connection time-out\n>> of any sort.  So the server can too tell the difference between end of data\n>> and\n>> a connection failure.\n>>\n>> Fred\n>>\n>> From: Carl Kugler/Boulder/IBM <kugler@us.ibm.com> on 11/02/2000 11:59 AM\n>>\n>> To:   James Lacey <James.Lacey@Motorola.com>\n>> cc:   http-wg <http-wg@hplb.hpl.hp.com>\n>>\n>> Subject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n>>\n>> ...\n>> >\n>> >For example, suppose the  server sends back an HTTP response to the client\n>> >that does not have a Content-Length: header field and that it is not\n>> >chunked.\n>> >\n>> >Then the only way the client knows that it has read the\n>> >entire response off of the pipe is when the server closes the connection.\n>> >When the server closes the connection the client will receive a\n>> >zero-byte read which is socket layer's indication that the pipe\n>> >is broken.\n>> >\n>> This is not good.  If this is the server's normal behavior, the client has\n>> no way to distinguish a dropped connection from end of file.  So the client\n>> can never be sure it received an entire message.\n>>\n>> If this is the server's behavior for error conditions, this is still not\n>> good unless the server either waits for the entire request (could be a\n>> humongous POST and/or a very slow connection), or only closes one half of\n>> the connection (which, BTW, is impossible in Java, except maybe in the\n>> latest releases).  Otherwise, the client might get a RST while transmitting\n>> the request, and will then never see the error response.\n>>\n>> ...\n>> >\n>> >I've done some snoops on browsers that are GETting HTML pages with\n>> >lots of embedded links that are in the same realm as the original HTML\n>> >page.\n>> >\n>> >Despite this obvious opportunity to take advantage of persistent\n>> connections\n>> >the browser opens  a connection for each subsequent GET.\n>> >\n>> I was looking at IE's (version 5 somthing) traffic yesterday, and it seems\n>> to send two requests per connection.\n>>\n>>      -Carl\n>\n\n\n\n", "id": "lists-012-15426395"}, {"subject": "More On Persistent Connections..", "content": "     In the satellite industry we are highly motivated\nfrom a response time standpoint to having persistent connections\nand pipelining work.\n\n     To what extent will the use of a proxy server enable\npersistent connections between the browser and the proxy server?\nI'm assuming that the proxy would be more configurable to keep\nthe connections alive.\n\n     I bet there are issues regarding transparent and\nnon-transparent connections.\n\n     Also, how about HTTPS:? There's an even bigger\nreason to maintain persistent connections here in both CPU\nexpense of openning a connection and response time (extra\nhandshakes).\n\n     Do proxies help with HTTPS and persistent connections?\n\n     Are web servers more or less likely to support persistent\nconnections with HTTPS?\n\n     I could give my thoughts on this in more detail, but\nwould be interested to see if anyone is interested in kicking these\nitems around.\n\n     Doug..............\n\n\n\n", "id": "lists-012-15441224"}, {"subject": "RE: Comments on draft-ietf-http-v11-spec-rev0", "content": "> ----------\n> From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n> Sent: Friday, March 27, 1998 2:51 PM\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Comments on draft-ietf-http-v11-spec-rev-03 \n> \n>     \n> How does\n> mark the cached object as \"must-revalidate\"\n> differ from\n> invalidate the cached object\n> \nIn my mind, at least, invalidate implies \"never return it in response to any\nlater request\", and usually, that means to delete it from the cache. Whereas\n\"must-revalidate\" implies \"keep the bits in the cache, but do a conditional\nGET (or whatever) before returning them in any later request\".\n\nThus, even if they are technically identical, the implication I would form\nupon reading the two alternatives are quite different.\n\n\n\n", "id": "lists-012-1544598"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Carl Kugler/Boulder/IBM wrote:\n\n> >Sent by: jlacey@mothost.mot.com\n> >Carl Kugler/Boulder/IBM wrote:\n> >\n> >> I stand corrected.  But then, why was Content-Length added in HTTP/1.0\n> and\n> >> Tranfer-Encoding: chunked in HTTP/1.1?\n> >\n> >Content-Length was added so that persistent connections would be possible.\n> >\n> >Without a content length, neither the client or the server would know\n> >where the message ended. You have to realize that TCP is a stream and\n> >it is up to the users of the stream to impose some kind of framing\n> protocol\n> >onto the stream.\n> >\n> >Transfer-Encoding: chunked was added so that servers could return\n> >possibly large dynamic content in chunks, as it was generated, so that\n> >the server did not have to buffer up all of the dynamic content before\n> >sending it back. With chunking you can also have persistent connections\n> >w/o the need of a Content-Length header field.\n> >\n> Ah yes, make perfect sense now.\n>\n> >One curious think about chunking that I do not understand is that\n> >HTTP proxies are required to move the chunked (any?) encoding before\n> >forwarding the response.\n> >\n> That's news to me.  Doesn't that apply only to MIME gateways or something?\n> Wouldn't that require a potentially infinite buffer in the proxy?  Wouldn't\n> it have a horrible impact on response time?\n>\n>      -Carl\n\nI didn't say that it wasn't a controversial requirement ...\n\n>\n>\n> >>\n> >>\n> >>      -Carl\n> >>\n> >> \"Fred Bohle\" <fbohle@neonsys.com> on 11/02/2000 11:27:24 AM\n> >>\n> >> To:   Carl Kugler/Boulder/IBM@IBMUS\n> >> cc:   James Lacey <James.Lacey@Motorola.com>, http-wg\n> >>       <http-wg@hplb.hpl.hp.com>\n> >> Subject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n> >>\n> >> Fred Bohle@NEON\n> >> 11/02/2000 12:27 PM\n> >>\n> >>      We seem to be diverging into TCP coding.  A read will return zero\n> >> length\n> >> when the other end has issued a normal close (and all the data has been\n> >> read).\n> >> A read will return -1 when the connection is ReSeT, or there is a\n> >> connection time-out\n> >> of any sort.  So the server can too tell the difference between end of data\n> >> and\n> >> a connection failure.\n> >>\n> >> Fred\n> >>\n> >> From: Carl Kugler/Boulder/IBM <kugler@us.ibm.com> on 11/02/2000 11:59 AM\n> >>\n> >> To:   James Lacey <James.Lacey@Motorola.com>\n> >> cc:   http-wg <http-wg@hplb.hpl.hp.com>\n> >>\n> >> Subject:  Re: Of HTTP/1.1 persistent connections and TCP Keepalive timers\n> >>\n> >> ...\n> >> >\n> >> >For example, suppose the  server sends back an HTTP response to the client\n> >> >that does not have a Content-Length: header field and that it is not\n> >> >chunked.\n> >> >\n> >> >Then the only way the client knows that it has read the\n> >> >entire response off of the pipe is when the server closes the connection.\n> >> >When the server closes the connection the client will receive a\n> >> >zero-byte read which is socket layer's indication that the pipe\n> >> >is broken.\n> >> >\n> >> This is not good.  If this is the server's normal behavior, the client has\n> >> no way to distinguish a dropped connection from end of file.  So the client\n> >> can never be sure it received an entire message.\n> >>\n> >> If this is the server's behavior for error conditions, this is still not\n> >> good unless the server either waits for the entire request (could be a\n> >> humongous POST and/or a very slow connection), or only closes one half of\n> >> the connection (which, BTW, is impossible in Java, except maybe in the\n> >> latest releases).  Otherwise, the client might get a RST while transmitting\n> >> the request, and will then never see the error response.\n> >>\n> >> ...\n> >> >\n> >> >I've done some snoops on browsers that are GETting HTML pages with\n> >> >lots of embedded links that are in the same realm as the original HTML\n> >> >page.\n> >> >\n> >> >Despite this obvious opportunity to take advantage of persistent\n> >> connections\n> >> >the browser opens  a connection for each subsequent GET.\n> >> >\n> >> I was looking at IE's (version 5 somthing) traffic yesterday, and it seems\n> >> to send two requests per connection.\n> >>\n> >>      -Carl\n> >\n\n\n\n", "id": "lists-012-15448505"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "> One curious think about chunking that I do not understand is that\n> HTTP proxies are required to move the chunked (any?) encoding before\n> forwarding the response.\n\ntransfer encodings (of which chunk is one) are hop to hop.. so they\nhave to be theoretically removed.. but if the outgoing encoding is the\nsame as the incoming encoding that phsyical operation can certainly be\noptimized away. The requirement is that the outgoing encoding meets\nthe requirements of the request received by the proxy.. so if the\nproxy gets a 1.0 request it will need to remove 'chunked' and\nterminate the message in some other fashion. \n\nThat other way is probably to buffer it and add a CL, or more likely\nterminate it by closing the connection.\n\nwhich leads me to a pet peeve of mine.. 2616 prohibits the sending of\nCL when a non identity transfer encoding is used.. consider the case I\njust used of a 1.1 proxy with a 1.1 origin-server and a 1.0\nuser-agent.. the proxy receives a 1.1 chunked (and something else non\nidentity) response but it's only choice to send it downstream is to\nbuffer it in a store and fowrard fashion to figure out the content\nlength or close the connection w/ EOF which would rob of it of the\nchance to use 1.0 style keepalives... \n\nadmittedly some folks think 1.0 keepalives ought to be burned.. but by\nthat argument all responses to 1.0 reqs should lack CL's and be TCP\nterminated, right?\n\nif the content length is known by the original sender it seems a\nlittle odd to lose this information if for no reason other than use in\nresource allocation or UI thermometer widgets. Certainly with dynamic\ncontent the server probably doesn't know the CL anyhow at header time,\nbut what if it's just sending say trasnfer-encoding: deflate, chunked\n??\n\n-P\n\n\n\n", "id": "lists-012-15463270"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "> -----Original Message-----\n> From: Joris Dobbelsteen [mailto:joris.dobbelsteen@m...]\n...\n>\n> The browser can leave the connection open for an infinite ammount of time\n> (if not restricted by lower-level protocols, like TCP).\n>\n> However, idle time-outs when the connection is CONNECTED are, in the real\n> world, handled by the server instead of the client. This is because the\n> server is not interrested in HTTP connections that are idle for an\ninfinite\n> ammount of time and just consuming bandwidth (the lower-level connection\n> must be maintained)...\n>\n...\n\nHow does an idle connection consume bandwidth?\n\n     -Carl\n\n\n\n", "id": "lists-012-15473779"}, {"subject": "Of HTTP-overSSL/TLS connection persistenc", "content": "Ok, so now I'm postulating a server-side gizmo speaking \"HTTP 1.0 | \n1.1\"-over-SSL/TLS that tries its best to have such connections be persistent.\n\nWhat do browsers typically do in ~this~ case -- how long before they'll \ntimeout an idle connection if the user causes it to connect and then walks \naway for whatever reason?\n\nthanks,\n\nJeffH\n\n\n\n", "id": "lists-012-15481698"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Hi,\n\nJames Lacey wrote:\n\n> Carl Kugler/Boulder/IBM wrote:\n>\n> > I stand corrected.  But then, why was Content-Length added in HTTP/1.0 and\n> > Tranfer-Encoding: chunked in HTTP/1.1?\n>\n> Content-Length was added so that persistent connections would be possible.\n>\n> Without a content length, neither the client or the server would know\n> where the message ended. You have to realize that TCP is a stream and\n> it is up to the users of the stream to impose some kind of framing protocol\n> onto the stream.\n>\n> Transfer-Encoding: chunked was added so that servers could return\n> possibly large dynamic content in chunks, as it was generated, so that\n> the server did not have to buffer up all of the dynamic content before\n> sending it back. With chunking you can also have persistent connections\n> w/o the need of a Content-Length header field.\n\n\"Transfer-Encoding chunked\" also works on the client side, and would allow a\nclient to send dynamic content in its request. If both the client and the server\nare doing chunking, it makes streaming applications possible over a standard HTTP\nconnection (eg: VPN, multimedia, etc).\n\nI'd like to point out that with iPlanet Web Server 4.1, you can easily write such\napplications in NSAPI and all the chunking issues are transparently taken care of\nfor the application developer. The only requirement is of course that the client\nmust be HTTP/1.1 compliant and support chunking. The client counterpart of such a\nserver-side streaming application could be a Java applet or browser plugin.\n\n> One curious think about chunking that I do not understand is that\n> HTTP proxies are required to move the chunked (any?) encoding before\n> forwarding the response.\n\nIt seems that such a proxy would break the type of application mentioned above\n... Full duplex would be required for such an application. This is something that\nis not very well addressed in current HTTP specs (hint: this is just a trick to\nget immediate attention from the RFC2616 owners :)).\n\nTraditionally, with the content-length indicated in requests and responses, many\nbrowsers and servers have used a synchronous model, where clients try to submit\nthe whole request, then only after it's complete, begin read the server response.\nMany servers operate similarly and read the entire request before starting the\nreponse. This is generally true for small GET requests, which the server can\nafford to buffer. However, for POST, the server will not buffer since that would\nmake the memory footprint of the server very high when multiplied by the number\nof connections. So the HTTP server will just act as a sort of intermediate and\nuse a relatively small buffer for the POST data, then pass it gradually to the\nconsumer of the request - whether it's a CGI, servlet, or NSAPI application ...\n\nI have found that HTTP clients typically break if the server tries to respond too\nearly to a request, before it is complete. For example, this can happen with a\nlarge POST request to a non-existing object. The server detects early on that the\nURI doesn't point to a valid object on the server, and will try to respond\nimmediately with a 404 response. This is done before it reads the entire amount\nof POST data ; since that can be very large and slow to read.\n\nAt that point however, the client is still submitting the large POST data ; and\nthe connection ends up being reset since both sides are sending and the client\nisn't trying to read until its write loop is over. In iWS, we have some\nworkarounds that I consider ugly, including trying to read more POST data on such\nerroneous large requests and just discarding it, as well as doing shutdown before\nclose, just to make sure the client doesn't see a \"connection reset\" error. This\ndoesn't work 100% of the time since we set some limits, but obviously we aren't\ngoing to read megabytes of data if the request is too large, or wait a long time\non that socket, since that could be a denial of service attack - eating up socket\ndescriptors and in some cases also holding up a thread in the server. It would be\nreally good if the HTTP spec would address such cases and require both client and\nservers to try both sides of the socket before closing/shutting the connection\ndown ... Such workarounds would not be necessary and the servers could just\npeacefully shut down the connection after reporting an error, and assume that the\nclient in fact was really able to receive that error - not causing a \"connection\nreset\".\n\n--\nThe network is the computer, and it's down\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-15489566"}, {"subject": "Re: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Julien Pierre wrote:\n\n> Hi,\n>\n> James Lacey wrote:\n>\n> > Carl Kugler/Boulder/IBM wrote:\n> >\n> > > I stand corrected.  But then, why was Content-Length added in HTTP/1.0 and\n> > > Tranfer-Encoding: chunked in HTTP/1.1?\n> >\n> I have found that HTTP clients typically break if the server tries to respond too\n> early to a request, before it is complete. For example, this can happen with a\n\nthat's cuz the right thing to do would be to send expect continue along with POST\nthen wait for continue then send the data. But many servers are broken 2 with respect\nto this,\n\nRuslan\n\n\n\n\n", "id": "lists-012-15503124"}, {"subject": "Re: Of HTTP-overSSL/TLS connection persistenc", "content": "As far as timeouts, my experience (as a server developer) is that clients will\ntypically treat the HTTP over SSL connections the same way they do insecure HTTP\nconnections, and use the same persistent connection timeout.\n\nIf you are talking about persistent connections of clients with HTTP over TLS\nupgrade, then you just opened a can of worms :-) After either side times out that\npersistent connection, the client would have to reconnect to the server and\n\"reupgrade\" the connection to TLS ... Or maybe not, depending on what factors\ntrigger the TLS upgrade, - especially if it's triggered by the server on a part\nof the URI space - so you might end up with a nonsecure connection after the\nreconnect due to the timeout if the client reconnects with a different URI (there\ncould be other factors and other corresponding possible error cases ...). This is\none of the reasons I don't consider the current TLS upgrade draft very useful in\nthe real world; it seems to be only used for custom applications like IPP and I\ndon't think any major client supports it today. But my response probably goes\nbeyond what you were asking.\n\nJeff.Hodges@kingsmountain.com wrote:\n\n> Ok, so now I'm postulating a server-side gizmo speaking \"HTTP 1.0 |\n> 1.1\"-over-SSL/TLS that tries its best to have such connections be persistent.\n>\n> What do browsers typically do in ~this~ case -- how long before they'll\n> timeout an idle connection if the user causes it to connect and then walks\n> away for whatever reason?\n>\n> thanks,\n>\n> JeffH\n\n--\nThe network is the computer, and it's down\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n", "id": "lists-012-15512555"}, {"subject": "RE: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "As for removing chunked encoding. Why is it actually needed???\n\nThe only reason I see is when you make a HTTP/1.0 request (no chunked support) and the browser a subsequent HTTP/1.1 request and the server returns a chunked response back.\nI would think that it might be to ensure the client can always understand the request. Also it takes less time to make a subsequent request to a proxy than to the web server (most of the time).\nAlso as Carl indicated this requires the buffer of the proxy to be of infinite length if the connection must be kept open and this would even dramatically downgrade response time.\n\nI think you shouldn't do this over HTTP/1.1 connections, and over HTTP/1.0 connections just by sending NO content-length...\n\n\n\nAbout MSIE sending only two requests at a time: Microsoft has documented this and it is possibility to have the lowest response times for the web page. If one requests takes a lot of time, to the requests after this, the server will respond later. As a browser you cannot know the size of a request.\nThere was once a discussion about HOL (Head-of-Line) blocking that went into this problem. Microsoft avoided the problem this way...\n\n- Joris\n\n\n> -----Original Message-----\n> From: Carl Kugler/Boulder/IBM [mailto:kugler@us.ibm.com]\n> Sent: Thursday, 02 November 2000 20:46\n> To: James Lacey\n> Cc: http-wg\n> Subject: Re: Of HTTP/1.1 persistent connections and TCP \n> Keepalive timers\n> \n> \n> \n> >Sent by: jlacey@mothost.mot.com\n> >Carl Kugler/Boulder/IBM wrote:\n> >\n> >> I stand corrected.  But then, why was Content-Length added \n> in HTTP/1.0\n> and\n> >> Tranfer-Encoding: chunked in HTTP/1.1?\n> >\n> >Content-Length was added so that persistent connections \n> would be possible.\n> >\n> >Without a content length, neither the client or the server would know\n> >where the message ended. You have to realize that TCP is a stream and\n> >it is up to the users of the stream to impose some kind of framing\n> protocol\n> >onto the stream.\n> >\n> >Transfer-Encoding: chunked was added so that servers could return\n> >possibly large dynamic content in chunks, as it was \n> generated, so that\n> >the server did not have to buffer up all of the dynamic \n> content before\n> >sending it back. With chunking you can also have persistent \n> connections\n> >w/o the need of a Content-Length header field.\n> >\n> Ah yes, make perfect sense now.\n> \n> >One curious think about chunking that I do not understand is that\n> >HTTP proxies are required to move the chunked (any?) encoding before\n> >forwarding the response.\n> >\n> That's news to me.  Doesn't that apply only to MIME gateways \n> or something?\n> Wouldn't that require a potentially infinite buffer in the \n> proxy?  Wouldn't\n> it have a horrible impact on response time?\n> \n>      -Carl\n> \n> >>\n> >>\n> >>      -Carl\n> >>\n> >> \"Fred Bohle\" <fbohle@neonsys.com> on 11/02/2000 11:27:24 AM\n> >>\n> >> To:   Carl Kugler/Boulder/IBM@IBMUS\n> >> cc:   James Lacey <James.Lacey@Motorola.com>, http-wg\n> >>       <http-wg@hplb.hpl.hp.com>\n> >> Subject:  Re: Of HTTP/1.1 persistent connections and TCP \n> Keepalive timers\n> >>\n> >> Fred Bohle@NEON\n> >> 11/02/2000 12:27 PM\n> >>\n> >>      We seem to be diverging into TCP coding.  A read will \n> return zero\n> >> length\n> >> when the other end has issued a normal close (and all the \n> data has been\n> >> read).\n> >> A read will return -1 when the connection is ReSeT, or there is a\n> >> connection time-out\n> >> of any sort.  So the server can too tell the difference \n> between end of data\n> >> and\n> >> a connection failure.\n> >>\n> >> Fred\n> >>\n> >> From: Carl Kugler/Boulder/IBM <kugler@us.ibm.com> on \n> 11/02/2000 11:59 AM\n> >>\n> >> To:   James Lacey <James.Lacey@Motorola.com>\n> >> cc:   http-wg <http-wg@hplb.hpl.hp.com>\n> >>\n> >> Subject:  Re: Of HTTP/1.1 persistent connections and TCP \n> Keepalive timers\n> >>\n> >> ...\n> >> >\n> >> >For example, suppose the  server sends back an HTTP \n> response to the client\n> >> >that does not have a Content-Length: header field and \n> that it is not\n> >> >chunked.\n> >> >\n> >> >Then the only way the client knows that it has read the\n> >> >entire response off of the pipe is when the server closes \n> the connection.\n> >> >When the server closes the connection the client will receive a\n> >> >zero-byte read which is socket layer's indication that the pipe\n> >> >is broken.\n> >> >\n> >> This is not good.  If this is the server's normal \n> behavior, the client has\n> >> no way to distinguish a dropped connection from end of \n> file.  So the client\n> >> can never be sure it received an entire message.\n> >>\n> >> If this is the server's behavior for error conditions, \n> this is still not\n> >> good unless the server either waits for the entire request \n> (could be a\n> >> humongous POST and/or a very slow connection), or only \n> closes one half of\n> >> the connection (which, BTW, is impossible in Java, except \n> maybe in the\n> >> latest releases).  Otherwise, the client might get a RST \n> while transmitting\n> >> the request, and will then never see the error response.\n> >>\n> >> ...\n> >> >\n> >> >I've done some snoops on browsers that are GETting HTML pages with\n> >> >lots of embedded links that are in the same realm as the \n> original HTML\n> >> >page.\n> >> >\n> >> >Despite this obvious opportunity to take advantage of persistent\n> >> connections\n> >> >the browser opens  a connection for each subsequent GET.\n> >> >\n> >> I was looking at IE's (version 5 somthing) traffic \n> yesterday, and it seems\n> >> to send two requests per connection.\n> >>\n> >>      -Carl\n> >\n> \n> \n\n\n\n", "id": "lists-012-15521706"}, {"subject": "RE: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "> -----Original Message-----\n> From: jlacey@mothost.mot.com \n> [mailto:jlacey@mothost.mot.com]On Behalf Of\n> James Lacey\n> Sent: Thursday, 02 November 2000 20:22\n> To: Marc Slemko\n> Cc: http-wg\n> Subject: Re: Of HTTP/1.1 persistent connections and TCP \n> Keepalive timers\n> \n> \n> Marc Slemko wrote:\n> \n> > On Thu, 2 Nov 2000, James Lacey wrote:\n> >\n> > > > > to a client. For example, if the web server generates \n> any dynamic content\n> > > > > for the client, then it will usually close the \n> connection after the response it\n> > > > > sent back. Regardless of whether or not the client \n> supplied a Connection:\n> > > > > Keep-Alive header or not. The reasoning behind this \n> is that if the sever\n> > > > > had to generate dynamic content on your behalf, then \n> you've had your share\n> > > > > and its time to give some other poor slob a turn.\n> > > >\n> > > > Huh?  That would be a webserver with some very... odd ideas.\n> > > >\n> > > > If you are talking about \"Connection: Keep-Alive\" then \n> you appear to be\n> > > > talking about HTTP/1.0, in which there is no chunked encoding so\n> > > > unless the server puts a content-length on its dynamic content\n> > > > (which is perfectly possible for it to do, but many don't for\n> > > > reasons that are also perfectly legitimate) then there is no way\n> > > > to use a persistent connection since in that case the only\n> > > > end-of-reponse marker you have is the close of connection.\n> > >\n> > > As a concrete example the iPlanet Enterprise v4.1 server always\n> > > closes the connection after responding to a POST request or\n> > > anytime that dynamic content is generated (possibly because\n> > > the response does not have a Content-Length: header field\n> > > and the response is not chunked).\n> > >\n> > > I have verified this and it is clearly documented in their\n> > > literature.\n> >\n> > You are missing the point; it doesn't do it out of some odd desire\n> > to \"share\" since a client has \"had it's fill of dynamic content\n> > for now\", like you suggested it does.\n> \n> OK. OK. I get your point.\n> \n> It may not be out of some odd desire to \"share\".\n> \n> But it is certainly based on the idea that many\n> clients need to be serviced with relatively few\n> connections.\n> \nSome servers can close a connection after sending dynamic content because of security resons. (or because they are bad implementations.)\n\nAs for POST, if a server responds (except the 1xx status codes), the request must have ended. At least, this is the most logic thing for me. Also I don't know how to send a POST request without using content-length or chunked encoding and having the server knowning absolutely sure that request has ended.\n\n- Joris\n\n\n\n", "id": "lists-012-15538372"}, {"subject": "RE: questions regarding draft-ietf-http-authentication0", "content": "> ----------\n> From: Ronald.Tschalaer@psi.ch[SMTP:Ronald.Tschalaer@psi.ch]\n> Sent: Friday, March 27, 1998 1:36 AM\n> To: HTTP-WG@cuckoo.hpl.hp.com\n> Subject: Re: questions regarding draft-ietf-http-authentication-01\n> \n> \n> > 1)  Section 3.2.2, request-digest description:\n> > \n> >         If the \"qop\" value is \"auth\":\n> [snip]\n> >     Shouldn't that be\n> > \n> >         If the \"qop\" value is \"auth\" or \"auth-int\":\n> [snip]\n> \n> Ooops, sorry, my scan of David Kristol's mail missed that he had already\n> mentioned this.\n> \n> However, on to more questions and comments regarding\n> draft-ietf-http-authentication-01.txt :\n> \n> 3) Section 3.2.2, digest-response description:\n> \n>         digest-response  = 1#( username | realm | nonce | digest-uri |\n>                            response | [ algorithm ] | [cnonce] |\n>                            [opaque] | [server] | [message-qop] |\n>                                      ^^^^^^^^^^^\n>                            [ nonce-count ] )\n> \n>     \"server\" is not defined anywhere. What is the syntax supposed to\n>     be, and what purpose does it serve?\n> \nLeft from a previous edit. It shouldn't be there.\n\n\n> 4) Section 3.2.2, digest-response description:\n> \n>         cnonce\n>           An opaque quoted string value provided by the client and used by\n> both\n>           client and server to avoid chosen plaintext attacks, to provide\n>           mutual authentication, and to provide some message integrity\n>           protection.  See the descriptions below of the calculation of\n> the\n>           response-digest and request-digest values.\n> \n>         nonce-count\n>           This MUST be specified if a qop attribute is sent (see above),\n> and\n>           MUST NOT be specified if the server did not send a qop attribute\n> in\n>           the WWW-Authenticate header field. ...\n> \n>     I presume the \"MUST NOT\" above is to avoid problems with rfc-2068\n>     implementations which might not handle an unknown attribute correctly\n>     (although they ought to just be ignoring it)? If so, why isn't the\n>     same language used for the cnonce? I.e. something like\n> \n>         cnonce\n>           An opaque quoted string value provided by the client and used by\n> both\n>           client and server to avoid chosen plaintext attacks, to provide\n>           mutual authentication, and to provide some message integrity\n>           protection.  See the descriptions below of the calculation of\n> the\n>           response-digest and request-digest values. This attribute MUST\n> NOT\n>           be specified if the server did not send a qop attribute in the\n>           WWW-Authenticate header field.\n> \nActually, I don't know why the MUST NOT is there. But it does serve the\npurpose you suggest, so why not put it in both places. Sure.\n\n\n> 5) For backwards compatibility with rfc-2069 there should be words to\n>    explictly prevent a server from sending \"algorithm=MD5-sess\" but no\n>    qop attribute. Maybe something like the following (just reusing the\n>    wording from above again):\n> \n>         algorithm\n>           A string indicating a pair of algorithms used to produce the\n> digest\n>           and a checksum. If this is not present it is assumed to be\n> \"MD5\".\n>           ...\n>           colon concatenated with the data. The \"MD5-sess\" algorithm is\n>           intended to allow efficient 3rd party authentication servers;\n>           for the difference in usage, see the description. The \"MD5-sess\"\n>           algorithm MUST NOT be specified if the qop-options attribute\n>           is not present.\n> \nI appreciate the intent, but I don't think this helps backward compatibility\nat all. If the client doesn't understand MD5-Sess, then adding \"qop\" (which\nisn't in 2069 either) won't help.\n\n\n> 6) The \"algorithm\" attribute is defined as:\n> \n>         algorithm         = \"algorithm\" \"=\" ( \"MD5\" | \"MD5-sess\" )\n> \n>      whereas rfc-2069 used the more general form\n> \n>         algorithm           = \"algorithm\" \"=\" ( \"MD5\" | token )\n> \n>      In the interest of possible future enhancements, I suggest changing\n>      the current definition to:\n> \n>         algorithm         = \"algorithm\" \"=\" ( \"MD5\" | \"MD5-sess\" | token )\n> \nSure.\n\n\n> 7) Section 3.2.2, a small nit:\n> \n>         response\n>           A string of 32 hex digits computed as defined below, which\n> proves\n>  \n> ^^^^^^\n>           that the user knows a password\n> \n>     I'm not very familiar with usage of \"prove\" in cryptographic circles,\n>     but a correct \"response\" attribute certainly does not prove anything\n>     in a mathematical sense. How about\n> \n>         response\n>           A string of 32 hex digits computed as defined below. The\n> reception\n>           of a correct response provides a strong indication that the user\n>           knows a password.\n> \nThe use is consistent with crypto use.\n\n> 8) Section 3.2.3: no words prohibit the server from sending, say, a qop\n>    attribute but not a rspauth attribute. Also, while the cnonce is\n>    required to be the same as used in the request, the nonce-count isn't.\n>    Hence I propose the following change in wording:\n> \n>    Replace\n> \n>         where \"Status-Code\" is the status code (e.g., \"200\") from the\n>         \"Status-Line\" of the response, as defined in section 6.1 of [2],\n>         and \"digest-uri-value\" is the value of the \"uri\" directive on the\n>         Authorization header in the request. The \"cnonce-value\" MUST be\n>         one for the client request to which this message is the response.\n> \n>    by\n> \n>         where \"Status-Code\" is the status code (e.g., \"200\") from the\n>         \"Status-Line\" of the response, as defined in section 6.1 of [2],\n>         and \"digest-uri-value\" is the value of the \"uri\" directive on the\n>         Authorization header in the request. The \"cnonce-value\" and\n>         \"nc-value\" MUST be the ones used in the client request to which\n>         this message is the response.\n> \n>         The \"response-auth\", \"cnonce\", and \"nonce-count\" attributes MUST\n>         BE present if \"qop=auth\" or \"qop=auth-int\" is specified.\n> \nGood. Thanks for the proposed wording.\n\n\n> 9) Section 3.2.3: Actually, why are the cnonce and nonce-count attributes\n>    sent in the Authorization-Info header? Or put differently, what makes\n>    these two attributes special, as opposed to the nonce and uri (which\n>    aren't sent back)?\n> \nGood question.\n\n> 10) Section 3.2.2:\n> \n>         Implementers should be aware of how authenticated transactions\n>         interact with shared caches. The HTTP/1.1 protocol specifies that\n>         when a shared cache (see section 13.10 of [2]) has received a\n>                                          ^^^^^\n>         request containing an Authorization header and a response from\n>         relaying that request, it MUST NOT return that response as a\n>         reply to any other request, unless one of two Cache-Control (see\n>         section 14.9 of [2]) directives was present in the response. If\n>                 ^^^^\n> \n>     Shouldn't those be sections 13.7 and 14.8, respectively?\n> \nQuite possibly. They may have renumbered the HTTP/1.1 spec while we weren't\nnoticing.\n\n> 11) What's the status of the AUTH-INFO-SYNTAX issue? The issues page\n>     http://www.w3.org/Protocols/HTTP/Issues/ lists the status as\n> \"Drafting\",\n>     but it's not in the current draft.\n> \nI never saw it. I don't know why it's closed. I'll make the change it\nrequested.\n\nPaul\n\n\n\n", "id": "lists-012-1554271"}, {"subject": "RE: Of HTTP/1.1 persistent connections and TCP Keepalive timer", "content": "Content-length in HTTP/1.0 is not directly needed, except when using the addional RFC purposing persistent connections. It gave the client a indication of the length of the file. This is nice when downloading a large file, you know that you have all the data received (in case of line failures)...\n\nHTTP/1.1 added chunked encoding in order to let HTTP implementations benefit when sending content where the length is not known (like dynamic content or an real-time video/measurement) and benefit from the persistent connections. The next request (e.g. a picture) can be requested without waisting time to open a new connection.\n\n\nThat looks like the most logic explaination for me....\n\n- Joris\n\n> -----Original Message-----\n> From: Carl Kugler/Boulder/IBM [mailto:kugler@us.ibm.com]\n> Sent: Thursday, 02 November 2000 20:07\n> To: Fred Bohle\n> Cc: James Lacey; http-wg\n> Subject: Re: Of HTTP/1.1 persistent connections and TCP \n> Keepalive timers\n> \n> \n> \n> I stand corrected.  But then, why was Content-Length added in \n> HTTP/1.0 and\n> Tranfer-Encoding: chunked in HTTP/1.1?\n> \n>      -Carl\n> \n> \n> \n> \"Fred Bohle\" <fbohle@neonsys.com> on 11/02/2000 11:27:24 AM\n> \n> To:   Carl Kugler/Boulder/IBM@IBMUS\n> cc:   James Lacey <James.Lacey@Motorola.com>, http-wg\n>       <http-wg@hplb.hpl.hp.com>\n> Subject:  Re: Of HTTP/1.1 persistent connections and TCP \n> Keepalive timers\n> \n> \n> \n> \n> \n> \n> \n> Fred Bohle@NEON\n> 11/02/2000 12:27 PM\n> \n> \n>      We seem to be diverging into TCP coding.  A read will return zero\n> length\n> when the other end has issued a normal close (and all the \n> data has been\n> read).\n> A read will return -1 when the connection is ReSeT, or there is a\n> connection time-out\n> of any sort.  So the server can too tell the difference \n> between end of data\n> and\n> a connection failure.\n> \n> Fred\n> \n> \n> \n> \n> From: Carl Kugler/Boulder/IBM <kugler@us.ibm.com> on \n> 11/02/2000 11:59 AM\n> \n> To:   James Lacey <James.Lacey@Motorola.com>\n> cc:   http-wg <http-wg@hplb.hpl.hp.com>\n> \n> Subject:  Re: Of HTTP/1.1 persistent connections and TCP \n> Keepalive timers\n> \n> \n> ...\n> >\n> >For example, suppose the  server sends back an HTTP response \n> to the client\n> >that does not have a Content-Length: header field and that it is not\n> >chunked.\n> >\n> >Then the only way the client knows that it has read the\n> >entire response off of the pipe is when the server closes \n> the connection.\n> >When the server closes the connection the client will receive a\n> >zero-byte read which is socket layer's indication that the pipe\n> >is broken.\n> >\n> This is not good.  If this is the server's normal behavior, \n> the client has\n> no way to distinguish a dropped connection from end of file.  \n> So the client\n> can never be sure it received an entire message.\n> \n> If this is the server's behavior for error conditions, this \n> is still not\n> good unless the server either waits for the entire request (could be a\n> humongous POST and/or a very slow connection), or only closes \n> one half of\n> the connection (which, BTW, is impossible in Java, except maybe in the\n> latest releases).  Otherwise, the client might get a RST \n> while transmitting\n> the request, and will then never see the error response.\n> \n> ...\n> >\n> >I've done some snoops on browsers that are GETting HTML pages with\n> >lots of embedded links that are in the same realm as the \n> original HTML\n> >page.\n> >\n> >Despite this obvious opportunity to take advantage of persistent\n> connections\n> >the browser opens  a connection for each subsequent GET.\n> >\n> I was looking at IE's (version 5 somthing) traffic yesterday, \n> and it seems\n> to send two requests per connection.\n> \n> \n>      -Carl\n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n\n\n\n", "id": "lists-012-15549548"}, {"subject": "RE: Of HTTP-overSSL/TLS connection persistenc", "content": "Better head for SSL, since I don't know any browsers that actually support TLS (MSIE doesn't, I think)...\n\n> -----Original Message-----\n> From: hodges@breakaway.Stanford.EDU\n> [mailto:hodges@breakaway.Stanford.EDU]On Behalf Of\n> Jeff.Hodges@KingsMountain.com\n> Sent: Friday, 03 November 2000 23:39\n> To: http-wg\n> Subject: Of HTTP-over-SSL/TLS connection persistence\n> \n> \n> Ok, so now I'm postulating a server-side gizmo speaking \"HTTP 1.0 | \n> 1.1\"-over-SSL/TLS that tries its best to have such \n> connections be persistent.\n> \n> What do browsers typically do in ~this~ case -- how long \n> before they'll \n> timeout an idle connection if the user causes it to connect \n> and then walks \n> away for whatever reason?\n> \n> thanks,\n> \n> JeffH\n> \n> \n> \n\n\n\n", "id": "lists-012-15563169"}, {"subject": "Reference HTTP 1.1 Client And Server Implementation", "content": "     Are there reference HTTP 1.1 client and server implementations\navailable which could be used as the basis for moving Mozilla and Apache\nup to include 1.1 implementations with support for persistent connections and\nrequest pipelining?\n\n     Doug...................\n\n\n\n", "id": "lists-012-15572073"}, {"subject": "Re: Reference HTTP 1.1 Client And Server Implementation", "content": "dillon@hns.com wrote:\n\n>      Are there reference HTTP 1.1 client and server implementations\n\nThe W3C has some stuff; I don't know how complete it is.\n\n http://www.w3.org/Library\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |You! Out of the gene pool!                   |\n|francis@ecal.com|                                             |\n\\==============================================================/\n\n\n\n", "id": "lists-012-15579347"}, {"subject": "RE: Of HTTP-overSSL/TLS connection persistenc", "content": "My copy of MSIE 5.50.4134.0600 has a checkbox for \"Use TLS 1.0\".\n\n     -Carl\n\n\n\n\"Joris Dobbelsteen\" <joris.dobbelsteen@mail.com> on 11/05/2000 10:21:53 AM\n\nTo:   \"'http-wg'\" <http-wg@hplb.hpl.hp.com>\ncc:\nSubject:  RE: Of HTTP-over-SSL/TLS connection persistence\n\n\n\nBetter head for SSL, since I don't know any browsers that actually support\nTLS (MSIE doesn't, I think)...\n\n> -----Original Message-----\n> From: hodges@breakaway.Stanford.EDU\n> [mailto:hodges@breakaway.Stanford.EDU]On Behalf Of\n> Jeff.Hodges@KingsMountain.com\n> Sent: Friday, 03 November 2000 23:39\n> To: http-wg\n> Subject: Of HTTP-over-SSL/TLS connection persistence\n>\n>\n> Ok, so now I'm postulating a server-side gizmo speaking \"HTTP 1.0 |\n> 1.1\"-over-SSL/TLS that tries its best to have such\n> connections be persistent.\n>\n> What do browsers typically do in ~this~ case -- how long\n> before they'll\n> timeout an idle connection if the user causes it to connect\n> and then walks\n> away for whatever reason?\n>\n> thanks,\n>\n> JeffH\n>\n>\n>\n\n\n\n", "id": "lists-012-15587231"}, {"subject": "Fw: Last Call: On the use of HTTP as a Substrate for Other Protocols to BC", "content": "In case you hadn't seen this...\n\n----- Original Message -----\nFrom: \"The IESG\" <iesg-secretary@ietf.org>\nTo: <IETF-Announce:>\nSent: Sunday, November 05, 2000 8:35 AM\nSubject: Last Call: On the use of HTTP as a Substrate for Other Protocols to\nBCP\n\n\n>\n> The IESG has received a request to consider On the use of HTTP as a\n> Substrate for Other Protocols <draft-moore-using-http-01.txt> as a\n> BCP.  This has been reviewed in the IETF but is not the product of an\n> IETF Working Group.\n>\n> The IESG plans to make a decision in the next few weeks, and solicits\n> final comments on this action.  Please send any comments to the\n> iesg@ietf.org or ietf@ietf.org mailing lists by December 5, 2000.\n>\n> Files can be obtained via\n> http://www.ietf.org/internet-drafts/draft-moore-using-http-01.txt\n>\n\n\n\n", "id": "lists-012-15597746"}, {"subject": "RE: Of HTTP-overSSL/TLS connection persistenc", "content": "As Julien Pierre said, MSIE probably doesn't support TLS upgrade. Maybe it supports TLS with https://???\n\nAlso TLS is disabled as default. SSL/2.0 and SSL/3.0 are enabled by default (MSIE).\n\n- Joris\n\n> -----Original Message-----\n> From: Carl Kugler/Boulder/IBM [mailto:kugler@us.ibm.com]\n> Sent: Monday, 06 November 2000 17:45\n> To: Joris Dobbelsteen\n> Cc: 'http-wg'\n> Subject: RE: Of HTTP-over-SSL/TLS connection persistence\n> \n> \n> \n> My copy of MSIE 5.50.4134.0600 has a checkbox for \"Use TLS 1.0\".\n> \n>      -Carl\n> \n> \n> \n> \"Joris Dobbelsteen\" <joris.dobbelsteen@mail.com> on \n> 11/05/2000 10:21:53 AM\n> \n> To:   \"'http-wg'\" <http-wg@hplb.hpl.hp.com>\n> cc:\n> Subject:  RE: Of HTTP-over-SSL/TLS connection persistence\n> \n> \n> \n> Better head for SSL, since I don't know any browsers that \n> actually support\n> TLS (MSIE doesn't, I think)...\n> \n> > -----Original Message-----\n> > From: hodges@breakaway.Stanford.EDU\n> > [mailto:hodges@breakaway.Stanford.EDU]On Behalf Of\n> > Jeff.Hodges@KingsMountain.com\n> > Sent: Friday, 03 November 2000 23:39\n> > To: http-wg\n> > Subject: Of HTTP-over-SSL/TLS connection persistence\n> >\n> >\n> > Ok, so now I'm postulating a server-side gizmo speaking \"HTTP 1.0 |\n> > 1.1\"-over-SSL/TLS that tries its best to have such\n> > connections be persistent.\n> >\n> > What do browsers typically do in ~this~ case -- how long\n> > before they'll\n> > timeout an idle connection if the user causes it to connect\n> > and then walks\n> > away for whatever reason?\n> >\n> > thanks,\n> >\n> > JeffH\n> >\n> >\n> >\n> \n> \n> \n> \n> \n\n\n\n", "id": "lists-012-15607056"}, {"subject": "Re: Fw: Last Call: On the use of HTTP as a Substrate for Other Protocols to BC", "content": "I've got a comment.  The draft refers to \"Hypertext Transport Protocol\n(HTTP)\" and talks about using HTTP as \"the transport\" for RPCs, etc.   I\nthought HTTP was supposed to be a transfer protocol, not a transport.\nIsn't \"HTTP\" usually expanded as \"Hypertext Transfer Protocol\"?  Or is the\ndistiction between transfer and transport unimportant in this context?\n\n     -Carl\n\n\n\"Larry Masinter\" <LMM@acm.org> on 11/07/2000 08:02:08 AM\n\nTo:   \"HTTP Working Group\" <http-wg@hplb.hpl.hp.com>\ncc:\nSubject:  Fw: Last Call: On the use of HTTP as a Substrate for Other\n      Protocols to BCP\n\n\n\nIn case you hadn't seen this...\n\n----- Original Message -----\nFrom: \"The IESG\" <iesg-secretary@ietf.org>\nTo: <IETF-Announce:>\nSent: Sunday, November 05, 2000 8:35 AM\nSubject: Last Call: On the use of HTTP as a Substrate for Other Protocols\nto\nBCP\n\n\n>\n> The IESG has received a request to consider On the use of HTTP as a\n> Substrate for Other Protocols <draft-moore-using-http-01.txt> as a\n> BCP.  This has been reviewed in the IETF but is not the product of an\n> IETF Working Group.\n>\n> The IESG plans to make a decision in the next few weeks, and solicits\n> final comments on this action.  Please send any comments to the\n> iesg@ietf.org or ietf@ietf.org mailing lists by December 5, 2000.\n>\n> Files can be obtained via\n> http://www.ietf.org/internet-drafts/draft-moore-using-http-01.txt\n>\n\n\n\n", "id": "lists-012-15617516"}, {"subject": "Caching Problem with 1.0 and a Proxy Serve", "content": "A small problem to overcome...\n\nWe already know the caching problems with 1.0 but....\n\nFirstly, assume that no proxy server is used. A browser send a a request\nusing HTTP 1.1 protocol. The response it receives from the web server is\nalso HTTP 1.1. This can be proven viewing the logs provided by a packet\nsniffer. The document received by the browser contains 1.1 header for cache\ncontrol. The cache control instructions basically force the brower to do a\nget each time the user requests the page.\n\nNow... add a proxy server. I had to modify the Windows registry so that the\nbrowser would issue a 1.1 request to the web server (via the proxy). The\nbrowser sends a 1.1 request, but receives a 1.0 response - this is from the\nsame web server, and I am using the same browser.\n\nWhat appears to happen is that the proxy server downgrades the protocol\nidentifier (at least in the response back to the web browser) and forwards\nthe response containing the 1.1 cache control headers to the browser. The\nbrowser now interprets the response using the 1.0 instructions and the end\nresult is that the page is caches. If I ask for the same page again (and not\nusing a forced GET) the page is retrieved from the cache as no GET request\nis captured by the packet sniffer.\n\nI have different browser that upon receipt of a response that indicates a\n1.0 response, but contains 1.1 headers seems to disregard the protocol\nidentifier in the response and processes each header instruction received.\nThe end result is that even though the protocol identifier says it is 1.0,\nwhen a GET is issued, the page is requested from the web server.\n\nNote that in the example given above the same web page was requested, from\nthe same web server. We know that a proxy server downgrades the response,\nbut that affects the way the response is also handled by the browser.\n\nQuestions:\n\n1. What meaning (other than the obvious) is attached to the protocol\nidentifier (e.g. HTTP/1.0) that is contained in a browser request or\nresponse?\n\n2. How strictly should a web browser use the protocol identifier in\nprocessing a response from a web server?\n\nThanks,\nTim C.\n\n\n\n", "id": "lists-012-15628889"}, {"subject": "Re: Caching Problem with 1.0 and a Proxy Serve", "content": "    1. What meaning (other than the obvious) is attached to the protocol\n    identifier (e.g. HTTP/1.0) that is contained in a browser request or\n    response?\n    \nSee RFC 2145. \"Use and Interpretation of HTTP Version Numbers\",\n     J. C. Mogul, R. Fielding, J. Gettys, H. Frystyk, May 1997. \n\n     HTTP request and response messages include an HTTP protocol\n     version number. Some confusion exists concerning the proper use\n     and interpretation of HTTP version numbers, and concerning\n     interoperability of HTTP implementations of different protocol\n     versions. This document is an attempt to clarify the situation.\n\nRead it carefully, and your questions should be answered.  A\nproxy that converts an HTTP/1.1 request message to an HTTP/1.0\nrequest is almost certainly legal.  I would be more worried\n(well, \"pissed off\" is more accurate) about an HTTP/1.0 proxy\nthat does NOT label its messages as HTTP/1.0.\n\n    2. How strictly should a web browser use the protocol identifier in\n    processing a response from a web server?\n    \nWhenever an HTTP protocol header could be misinterpreted if\nyou are using the wrong version of the protocol spec, then\nthe browser should be very cautious about using it if the\nversion number seems to be inconsistent.  The HTTP/1.1 spec\n(RFC 2616) documents a number of specific cases.\n\n-Jeff\n\n\n\n", "id": "lists-012-15638360"}, {"subject": "Re: Caching Problem with 1.0 and a Proxy Serve", "content": "\"Tim Coates\" <tcoates@dynamics.net> wrote:\n  > A small problem to overcome...\n  > \n  > We already know the caching problems with 1.0 but....\n  > \n  > Firstly, assume that no proxy server is used. A browser send a a request\n  > using HTTP 1.1 protocol. The response it receives from the web server is\n  > also HTTP 1.1. This can be proven viewing the logs provided by a packet\n  > sniffer. The document received by the browser contains 1.1 header for cache\n  > control. The cache control instructions basically force the brower to do a\n  > get each time the user requests the page.\n  > \n  > Now... add a proxy server. I had to modify the Windows registry so that the\n  > browser would issue a 1.1 request to the web server (via the proxy). The\n  > browser sends a 1.1 request, but receives a 1.0 response - this is from the\n  > same web server, and I am using the same browser.\n  > \n  > What appears to happen is that the proxy server downgrades the protocol\n  > identifier (at least in the response back to the web browser) and forwards\n  > the response containing the 1.1 cache control headers to the browser. The\n  > browser now interprets the response using the 1.0 instructions and the end\n  > result is that the page is caches. If I ask for the same page again (and not\n  > using a forced GET) the page is retrieved from the cache as no GET request\n  > is captured by the packet sniffer.\n\nDo you know whether the proxy server is a *1.1* proxy?  More than\nlikely it is not.  Therefore the proxy won't know about HTTP/1.1, will\nsend requests as HTTP/1.0 requests because it doesn't know better, and\ntreat responses accordingly.  And that is why, as Jeff Mogul has already\nremarked, the browser should be cautious about how it interprets\n\nDave Kristol\nHTTP/1.1 headers.\n\n\n\n", "id": "lists-012-15647046"}, {"subject": "Thanks Re Caching Proble", "content": "Thanks to those who provided me with information about the caching problem\nwith the proxy server...\n\nWe were running a number of tests using Netscape, MSIE V5, Opera, Neoplanet\nbrowsers. We knew that the proxy server was dowgrading the response. Based\non the references provided by Jeff and other document Q222064 (\"Pragma:\nno-cache\" Tag May Not Prevent Pages from Being Cached) and Q234067 (Prevent\nCaching in Internet Explorer) (from MS Support) explain why the web page was\nstill being cached by the browser. The strange thing was the pages were not\nbeing cached by Netscape - even with HTTP/1.0. A problem still exists at the\nbrowser end however.\n\nFrom a security end we know that HTTP/1.0 has flaws (especially when you\nintroduce a web browser), but it raises the question of how many proxy\nserver are there which only implement HTTP/1.0. All it seems to takes is a\nsingle proxy server for a response to be downgraded, and for the browser to\nreceive that downgraded response and (correctly?) ignore any settings that\nare not associated with the protocol identifier in the response - such as\nCache-Control headers.\n\nRegards,\nTim C.\n\n\n\n", "id": "lists-012-15656383"}, {"subject": "RE: Thanks Re Caching Proble", "content": "> >From a security end we know that HTTP/1.0 has flaws (especially when you\n> introduce a web browser), but it raises the question of how many proxy\n> server are there which only implement HTTP/1.0. All it seems to takes is a\n> single proxy server for a response to be downgraded, and for the browser\nto\n> receive that downgraded response and (correctly?) ignore any settings that\n> are not associated with the protocol identifier in the response - such as\n> Cache-Control headers.\n\nIncorrectly.  If a browser supports the Cache-Control header field for\nany HTTP/1.x response, then it should support it for every HTTP/1.x\nresponse.\nThe definition of an HTTP header field is defined by the major number,\nnot the minor number.\n\n....Roy\n\n\n\n", "id": "lists-012-15664361"}, {"subject": "RE: Thanks Re Caching Proble", "content": "For MSIE there is reason to believe that the browser will cache documents with: \"Cache-Control: no-cache\", since MSIE's feature \"Work Offline\". Maybe that \"must-revalidate\" is never cached by the browser, since commercial banners usually asked to be retreived from the server when working offline.\nIt is possible that objects are not retreived from cache when working offline is disabled.\n\nTo validate this, you should search of ask Microsoft.\n\nAnd indeed Roy seems to be right. A HTTP/1.1 proxy/server should not serve requests for HTTP/2.0 or later clients, but may do it for a HTTP/1.9 or HTTP/1.10 request according to the HTTP/1.1 RFC. This is outlined in the RFC. This is one failure of the MS-IIS, that serves HTTP/2.0 or later requests, even though it is only a HTTP/1.1 server. Netscape doesn't do this...\nProtocol changes that are not compatible with HTTP/1.x should be HTTP/2.0 and not HTTP/1.2 (or any)...\n\n- Joris\n\n> -----Original Message-----\n> From: Fielding, Roy [mailto:fielding@eBuilt.com]\n> Sent: Wednesday, 08 November 2000 23:50\n> To: 'Tim Coates'; http-wg@cuckoo.hpl.hp.com\n> Subject: RE: Thanks Re Caching Problem\n> \n> \n> > >From a security end we know that HTTP/1.0 has flaws \n> (especially when you\n> > introduce a web browser), but it raises the question of how \n> many proxy\n> > server are there which only implement HTTP/1.0. All it \n> seems to takes is a\n> > single proxy server for a response to be downgraded, and \n> for the browser\n> to\n> > receive that downgraded response and (correctly?) ignore \n> any settings that\n> > are not associated with the protocol identifier in the \n> response - such as\n> > Cache-Control headers.\n> \n> Incorrectly.  If a browser supports the Cache-Control header field for\n> any HTTP/1.x response, then it should support it for every HTTP/1.x\n> response.\n> The definition of an HTTP header field is defined by the major number,\n> not the minor number.\n> \n> ....Roy\n> \n> \n\n\n\n", "id": "lists-012-15672490"}, {"subject": "HTTP", "content": "Dear Friends,\n\nHow can is use con.setRequestProperty() to send the url name from where the\ncurrent page has been loaded.\n\nThanks in advance\n\nDeepak\n\n\n\n", "id": "lists-012-15681680"}, {"subject": "Push technolog", "content": "Can anyone tell me where I can find information on this?  I read that HTTP\nincludes \"push\" technology to send information, but I can't find any solid\ninformation on the W3 website.  Any help is appreciated.\n\nThanks,\nPeter Foti\n\n\n\n", "id": "lists-012-15687880"}, {"subject": "IPv6 and HTTP (1.2?", "content": "Can anyone specify the requirements that IPv6 will have for changes to HTTP \n1.1, apart from URL structure?\n_____________________________________________________________________________________\nGet more from the Web.  FREE MSN Explorer download : http://explorer.msn.com\n\n\n\n", "id": "lists-012-15694672"}, {"subject": "Re: IPv6 and HTTP (1.2?", "content": "Thomas McLaren wrote:\n\n> Can anyone specify the requirements that IPv6 will have for changes to \n> HTTP 1.1, apart from URL structure?\n\nThere aren't any - there is already a spec (sorry, don't have the \nreference handy) on how to write an IPv6 address in a URL.\n\n\n-- \nScott Lawrence      Architect            lawrence@agranat.com\nVirata       Embedded Web Technology    http://www.emweb.com/\n\n\n\n", "id": "lists-012-15701501"}, {"subject": "RE: IPv6 and HTTP (1.2?", "content": "That is quite simple and already discussed in the WG.\n\nthe standard IPv6 address specifies an IPv6 address. Because there are\nalways more than one \":\", you know it's IPv6.\ne.g. \"fe80::01\" or \"fe80::3a3c:2bc:96f5:1060\"\n\nIf you specify a port, you must send the IPv6 address as:\n\"[\" IPv6-address \"]:\" port\n\ne.g. \"[fe80::01]:8080\" or \"[fe80::3a3c:2bc:96f5:1060]:81\"\n\nSo the URL is:\n\ne.g. \"http://fe80::01/\" or \"http://[fe80::3a3c:2bc:96f5:1060]:81/\"\n\nfor the rest nothing changes. The server (or gateway) just has to\nreqognise these addresses...\n\nPlease correct me if I'm wrong.....\nSend to the WG directly (no CC please), can take a while...\n\n\n- Joris\n\n> -----Original Message-----\n> From: Scott Lawrence [mailto:lawrence@agranat.com]\n> Sent: Tuesday 28 November, 2000 14:56\n> To: Thomas McLaren\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: IPv6 and HTTP (1.2?)\n> \n> \n> Thomas McLaren wrote:\n> \n> > Can anyone specify the requirements that IPv6 will have for \n> changes to \n> > HTTP 1.1, apart from URL structure?\n> \n> There aren't any - there is already a spec (sorry, don't have the \n> reference handy) on how to write an IPv6 address in a URL.\n> \n> \n> -- \n> Scott Lawrence      Architect            lawrence@agranat.com\n> Virata       Embedded Web Technology    http://www.emweb.com/\n> \n> \n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-15709275"}, {"subject": "RE: IPv6 and HTTP (1.2?", "content": "In Proposed Standard RFC 2732, IPv6 URLs must always have the\naddress enclosed in square brackets, whether or not a port\nis supplied.\n\nJoris Dobbelsteen wrote:\n\n> That is quite simple and already discussed in the WG.\n> \n> the standard IPv6 address specifies an IPv6 address. Because there are\n> always more than one \":\", you know it's IPv6.\n> e.g. \"fe80::01\" or \"fe80::3a3c:2bc:96f5:1060\"\n> \n> If you specify a port, you must send the IPv6 address as:\n> \"[\" IPv6-address \"]:\" port\n> \n> e.g. \"[fe80::01]:8080\" or \"[fe80::3a3c:2bc:96f5:1060]:81\"\n> \n> So the URL is:\n> \n> e.g. \"http://fe80::01/\" or \"http://[fe80::3a3c:2bc:96f5:1060]:81/\"\n> \n> for the rest nothing changes. The server (or gateway) just has to\n> reqognise these addresses...\n> \n> Please correct me if I'm wrong.....\n> Send to the WG directly (no CC please), can take a while...\n> \n> \n> - Joris\n> \n> > -----Original Message-----\n> > From: Scott Lawrence [mailto:lawrence@agranat.com]\n> > Sent: Tuesday 28 November, 2000 14:56\n> > To: Thomas McLaren\n> > Cc: http-wg@cuckoo.hpl.hp.com\n> > Subject: Re: IPv6 and HTTP (1.2?)\n> > \n> > \n> > Thomas McLaren wrote:\n> > \n> > > Can anyone specify the requirements that IPv6 will have for \n> > changes to \n> > > HTTP 1.1, apart from URL structure?\n> > \n> > There aren't any - there is already a spec (sorry, don't have the \n> > reference handy) on how to write an IPv6 address in a URL.\n> > \n> > \n> > -- \n> > Scott Lawrence      Architect            lawrence@agranat.com\n> > Virata       Embedded Web Technology    http://www.emweb.com/\n> > \n> > \n> \n\n\n\n", "id": "lists-012-15718686"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "> There've been nine implementation reports so far.\n\nAre the reports accessible anywhere?\nhttp://www.w3.org/Protocols/HTTP/Forum/Reports/ is empty.\n\n[snip]\n> It would be good to focus on broadening the testing\n> for each of these features, though.\n\nIt would help to know which servers (and clients) implemented which\nfeatures.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1572638"}, {"subject": "RE: IPv6 and HTTP (1.2?", "content": "How about priority information though? Am I correct in assuming that it will\nbe specified by the individual application, and is therefore outside of the\nscope of HTTP?\n\n- Tom\n\n<previous message follows>\n\nIn Proposed Standard RFC 2732, IPv6 URLs must always have the\naddress enclosed in square brackets, whether or not a port\nis supplied.\n\nJoris Dobbelsteen wrote:\n\n> That is quite simple and already discussed in the WG.\n>\n> the standard IPv6 address specifies an IPv6 address. Because there are\n> always more than one \":\", you know it's IPv6.\n> e.g. \"fe80::01\" or \"fe80::3a3c:2bc:96f5:1060\"\n>\n> If you specify a port, you must send the IPv6 address as:\n> \"[\" IPv6-address \"]:\" port\n>\n> e.g. \"[fe80::01]:8080\" or \"[fe80::3a3c:2bc:96f5:1060]:81\"\n>\n> So the URL is:\n>\n> e.g. \"http://fe80::01/\" or \"http://[fe80::3a3c:2bc:96f5:1060]:81/\"\n>\n> for the rest nothing changes. The server (or gateway) just has to\n> reqognise these addresses...\n>\n> Please correct me if I'm wrong.....\n> Send to the WG directly (no CC please), can take a while...\n>\n>\n> - Joris\n>\n> > -----Original Message-----\n> > From: Scott Lawrence [mailto:lawrence@agranat.com]\n> > Sent: Tuesday 28 November, 2000 14:56\n> > To: Thomas McLaren\n> > Cc: http-wg@cuckoo.hpl.hp.com\n> > Subject: Re: IPv6 and HTTP (1.2?)\n> >\n> >\n> > Thomas McLaren wrote:\n> >\n> > > Can anyone specify the requirements that IPv6 will have for\n> > changes to\n> > > HTTP 1.1, apart from URL structure?\n> >\n> > There aren't any - there is already a spec (sorry, don't have the\n> > reference handy) on how to write an IPv6 address in a URL.\n> >\n> >\n> > --\n> > Scott Lawrence      Architect            lawrence@agranat.com\n> > Virata       Embedded Web Technology    http://www.emweb.com/\n> >\n> >\n>\n\n\n\n", "id": "lists-012-15728253"}, {"subject": "RE: IPv6 and HTTP (1.2?", "content": "As far as I know is there no specification that handles this for HTTP.\n\n- Joris\n\n> -----Original Message-----\n> From: Tom McLaren [mailto:Tom@Mclaren.tc]\n> Sent: Tuesday 28 November, 2000 18:37\n> To: HTTP Working Group\n> Subject: RE: IPv6 and HTTP (1.2?)\n> \n> \n> How about priority information though? Am I correct in \n> assuming that it will\n> be specified by the individual application, and is therefore \n> outside of the\n> scope of HTTP?\n> \n> - Tom\n> \n> <previous message follows>\n> \n> In Proposed Standard RFC 2732, IPv6 URLs must always have the\n> address enclosed in square brackets, whether or not a port\n> is supplied.\n> \n> Joris Dobbelsteen wrote:\n> \n> > That is quite simple and already discussed in the WG.\n> >\n> > the standard IPv6 address specifies an IPv6 address. \n> Because there are\n> > always more than one \":\", you know it's IPv6.\n> > e.g. \"fe80::01\" or \"fe80::3a3c:2bc:96f5:1060\"\n> >\n> > If you specify a port, you must send the IPv6 address as:\n> > \"[\" IPv6-address \"]:\" port\n> >\n> > e.g. \"[fe80::01]:8080\" or \"[fe80::3a3c:2bc:96f5:1060]:81\"\n> >\n> > So the URL is:\n> >\n> > e.g. \"http://fe80::01/\" or \"http://[fe80::3a3c:2bc:96f5:1060]:81/\"\n> >\n> > for the rest nothing changes. The server (or gateway) just has to\n> > reqognise these addresses...\n> >\n> > Please correct me if I'm wrong.....\n> > Send to the WG directly (no CC please), can take a while...\n> >\n> >\n> > - Joris\n> >\n> > > -----Original Message-----\n> > > From: Scott Lawrence [mailto:lawrence@agranat.com]\n> > > Sent: Tuesday 28 November, 2000 14:56\n> > > To: Thomas McLaren\n> > > Cc: http-wg@cuckoo.hpl.hp.com\n> > > Subject: Re: IPv6 and HTTP (1.2?)\n> > >\n> > >\n> > > Thomas McLaren wrote:\n> > >\n> > > > Can anyone specify the requirements that IPv6 will have for\n> > > changes to\n> > > > HTTP 1.1, apart from URL structure?\n> > >\n> > > There aren't any - there is already a spec (sorry, don't have the\n> > > reference handy) on how to write an IPv6 address in a URL.\n> > >\n> > >\n> > > --\n> > > Scott Lawrence      Architect            lawrence@agranat.com\n> > > Virata       Embedded Web Technology    http://www.emweb.com/\n> > >\n> > >\n> >\n> \n> \n> \n> \n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-15738077"}, {"subject": "Http overhea", "content": "Hi!\n\nI am currently working with a traffic model for an IP network. I have a\nquestion about the amount of overhead involved when transfering binary\nfiles over http. As fas as I understood, http handles binary files so no\nencoding has to be performed (as for example files over SMTP uses MIME).\nSo, how many header-bytes does http add in each packet when sending a\nbinary file from a server to a client?\n\nI hope someone can help me with this.\n\nThanks,\nPatrik\n\n\n\n", "id": "lists-012-15748459"}, {"subject": "Re: Http overhea", "content": "On Thu, 30 Nov 2000, Patrik Carlsson wrote:\n\n> Hi!\n> \n> I am currently working with a traffic model for an IP network. I have a\n> question about the amount of overhead involved when transfering binary\n> files over http. As fas as I understood, http handles binary files so no\n> encoding has to be performed (as for example files over SMTP uses MIME).\n> So, how many header-bytes does http add in each packet when sending a\n> binary file from a server to a client?\n\nDepends on the server, but typically around 256 bytes.\nE.g. this instance of Apache response uses 279 bytes:\n\nab -v 4 http://www.foo.com:81/poweredby.png\n\nHTTP/1.1 200 OK\nDate: Thu, 30 Nov 2000 10:21:44 GMT\nServer: Apache/1.3.12 (Unix)  (Red Hat/Linux) PHP/3.0.15 mod_perl/1.21\nLast-Modified: Wed, 01 Mar 2000 18:37:44 GMT\nETag: \"33303-482-38bd6378\"\nAccept-Ranges: bytes\nContent-Length: 1154\nConnection: close\nContent-Type: image/png\n\n<1154 bytes of binary data>\n\nCheers,\n\n/Patrik.\n\n--\nPatrik Winroth                          <pwinroth@alteon.com>\n\n\n\n", "id": "lists-012-15755720"}, {"subject": "Re: Http overhea", "content": "     You have an HTTP response header (around 250 bytes) which goes\nin its own segment and then all succeeding data is sent in TCP segments as a\nstream of bytes.\nNo overhead other than the TCP/IP overhead.\n\n     The latest standard (HTTP 1.1) has provisions for compression and \"chunked\"\ntransfers\nwhich change this, but I haven't seen these used in any real-world situations\nyet.\n\n     Doug...................\n\n\n\n", "id": "lists-012-15764249"}, {"subject": "Re: Http overhea", "content": "dillon@hns.com wrote:\n\n> \n> \n>      You have an HTTP response header (around 250 bytes) which goes\n> in its own segment and then all succeeding data is sent in TCP segments as a\n> stream of bytes.\n> No overhead other than the TCP/IP overhead.\n> \n>      The latest standard (HTTP 1.1) has provisions for compression and \"chunked\"\n> transfers\n> which change this, but I haven't seen these used in any real-world situations\n> yet.\n\nChunked encoding is frequently used in situations where the server \nmight not know the size of the response body at the time it is \ngenerating headers.  This is often the case for CGI and other \ndynamic interfaces.\n\nIf chunked encoding is used, there is a short chunk header that \npreceeds each chunk - no more than 10 bytes; the chunk size is up to \nthe server, but is typically either the record size of the buffer it \nis using to read the dynamic content, or the buffer size of its \nnetwork buffers (depending on which memory it is optimizing for).\n\n\n-- \nScott Lawrence      Architect            lawrence@agranat.com\nVirata       Embedded Web Technology    http://www.emweb.com/\n\n\n\n", "id": "lists-012-15772088"}, {"subject": "Re: Http overhea", "content": "dillon@hns.com wrote:\n\n>      The latest standard (HTTP 1.1) has provisions for compression and \"chunked\"\n> transfers which change this, but I haven't seen these used in any real-world\n> situations\n> yet.\n\nApache will recognize a file with a \".gz\" extension as gzipped, and send the\nContent-Encoding: x-gzip line.  Netscape will recognize Content-Encoding: x-gzip,\nand uncompress the file.  Unfortunately, at least in my installation (Apache\n1.3.14, Red Hat 7), Apache doesn't look at the extension before the \".gz\" to get\nthe content-type; \"foo.txt.gz\" gets marked as Content-Type: application/x-gzip.\n\n--\n/=================================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.   |\n|Chief Scientist |================================================|\n|eCal Corp.      |But how do we know destroying the Van Allen belt|\n|francis@ecal.com|will kill all life on Earth if we don't try it? |\n\\=================================================================/\n\n\n\n", "id": "lists-012-15780644"}, {"subject": "RE: Http overhea", "content": "Many apache servers send data chunked. Takes a couple bytes (average of\n4) for every block transfered. Maybe a total overhead of an additional\n20-40 bytes per transfer (maybe less). This is just a guess...\n\n\n- Joris\n\n> -----Original Message-----\n> From: dillon@hns.com [mailto:dillon@hns.com]\n> Sent: Thursday 30 November, 2000 14:47\n> To: Patrik Carlsson\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Http overhead\n> \n> \n> \n> \n> \n>      You have an HTTP response header (around 250 bytes) which goes\n> in its own segment and then all succeeding data is sent in \n> TCP segments as a\n> stream of bytes.\n> No overhead other than the TCP/IP overhead.\n> \n>      The latest standard (HTTP 1.1) has provisions for \n> compression and \"chunked\"\n> transfers\n> which change this, but I haven't seen these used in any \n> real-world situations\n> yet.\n> \n>      Doug...................\n> \n> \n> \n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-15788780"}, {"subject": "Re: Http overhea", "content": "Almost all modern browsers support content-encoding (gzip, for example), and\nmost servers can be pummelled into serving compressed content; actually,\nI've seen a lot of activity around this recently, with vendors releasing\nproducts to faclilitate compression on the server, or to move it to an\nintermediary. \n\nWhile not many Web sites are using compression, the number is growing, and\nI've heard rumors of some Big sites playing with it on home pages, etc.\n(haven't checked recently).\n\nI've also heard rumors that browsers are starting to support\ntransfer-encoding.\n\n\n<plug>see http://www.mnot.net/cgi_buffer/</plug>\n\n\n\nOn Thu, Nov 30, 2000 at 08:47:08AM -0500, dillon@hns.com wrote:\n> \n> \n> \n>      You have an HTTP response header (around 250 bytes) which goes in its\n> own segment and then all succeeding data is sent in TCP segments as a\n> stream of bytes. No overhead other than the TCP/IP overhead.\n> \n>      The latest standard (HTTP 1.1) has provisions for compression and\n> \"chunked\" transfers which change this, but I haven't seen these used in\n> any real-world situations yet.\n> \n>      Doug...................\n> \n> \n\n-- \nMark Nottingham\nhttp://www.mnot.net/\n\n\n\n", "id": "lists-012-15797265"}, {"subject": "RE: Http overhea", "content": "Many apache servers send data chunked. Takes a couple bytes (average of\n4) for every block transfered. Maybe a total overhead of an additional\n20-40 bytes per transfer (maybe less). This is just a guess...\n\n\n- Joris\n\n\n> -----Original Message-----\n> From: francis@localhost.localdomain\n> [mailto:francis@localhost.localdomain]On Behalf Of John Stracke\n> Sent: Thursday 30 November, 2000 16:45\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Http overhead\n> \n> \n> dillon@hns.com wrote:\n> \n> >      The latest standard (HTTP 1.1) has provisions for \n> compression and \"chunked\"\n> > transfers which change this, but I haven't seen these used \n> in any real-world\n> > situations\n> > yet.\n> \n> Apache will recognize a file with a \".gz\" extension as \n> gzipped, and send the\n> Content-Encoding: x-gzip line.  Netscape will recognize \n> Content-Encoding: x-gzip,\n> and uncompress the file.  Unfortunately, at least in my \n> installation (Apache\n> 1.3.14, Red Hat 7), Apache doesn't look at the extension \n> before the \".gz\" to get\n> the content-type; \"foo.txt.gz\" gets marked as Content-Type: \n> application/x-gzip.\n> \n> --\n> /=================================================================\\\n> |John Stracke    | http://www.ecal.com |My opinions are my own.   |\n> |Chief Scientist |================================================|\n> |eCal Corp.      |But how do we know destroying the Van Allen belt|\n> |francis@ecal.com|will kill all life on Earth if we don't try it? |\n> \\=================================================================/\n> \n> \n> \n> \n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-15805802"}, {"subject": "RE: HTTP features w/ low 'implemented' and 'tested", "content": "> Are the reports accessible anywhere?\n> http://www.w3.org/Protocols/HTTP/Forum/Reports/ is empty.\n\n> [snip]\n> > It would be good to focus on broadening the testing\n> > for each of these features, though.\n\n> It would help to know which servers (and clients) implemented\n> which features.\n\nWe'll put out the reports some time in the next week or so;\nI want to verify the 'public' status of the implementation\nreports before doing so. There are many more implementations\nlisted than have reported at http://www.w3.org/Protocols/HTTP/Forum/#Test.\n\nIf you have a list of features you'd like to test against,\ncould you send it out?\n\nThanks,\n\nLarry\n\n\n\n", "id": "lists-012-1581185"}, {"subject": "Re: Http overhea", "content": "Joris Dobbelsteen wrote:\n\n> Many apache servers send data chunked. Takes a couple bytes (average of\n> 4) for every block transfered. Maybe a total overhead of an additional\n> 20-40 bytes per transfer (maybe less). This is just a guess...\n\nYes, but it's actually better than that: AFAIK, Apache uses chunked\ntransfer-encoding only for dynamic resources, where it can't predict the\ncontent-length.  The alternative would be (a) buffer the output before\nsending it down, or (b) defeat persistent connections.  Either (a) or (b)\nwould increase; (b) would actually cost extra bandwidth, and (a) would\ncause bandwidth consumption to come in spikes.  So, most likely, the cost\nof chunking is lower than the cost of not chunking; it's certainly lower\nthan the nominal overhead of the encoding.\n\n(Sorry to those to whom this is obvious--probably including Joris--but I\ndidn't want to leave anybody thinking they could save bandwidth by turning\noff chunking.  :-)\n\n--\n/=================================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.   |\n|Chief Scientist |================================================|\n|eCal Corp.      |In the country of the blind, the one-eyed man is|\n|francis@ecal.com|in therapy.                                     |\n\\=================================================================/\n\n\n\n", "id": "lists-012-15815243"}, {"subject": "RE: Http overhea", "content": "You are right, I did know this.\n\nIndeed that's why I favor chunked encoding for dynamic resources. MSN's\n(search) server (MS-IIS) transfers the data by just closing the\nconnection at the end (not sending any indication about the size of the\ndocument) what also decreases the transfer 'reliability' - or better,\nyou don't know wether you have all or not... HTML or JPEG/GIF you can\nguess it, but with many other sources you don't (like TXT)....\n\nAlso MSN's server has thus more overhead for the server and waisted time\nbetween the requests for the client...\n\n- Joris\n\n> -----Original Message-----\n> From: francis@localhost.localdomain\n> [mailto:francis@localhost.localdomain]On Behalf Of John Stracke\n> Sent: Friday 01 December, 2000 16:04\n> To: WWW WG (E-mail)\n> Subject: Re: Http overhead\n> \n> \n> Joris Dobbelsteen wrote:\n> \n> > Many apache servers send data chunked. Takes a couple bytes \n> (average of\n> > 4) for every block transfered. Maybe a total overhead of an \n> additional\n> > 20-40 bytes per transfer (maybe less). This is just a guess...\n> \n> Yes, but it's actually better than that: AFAIK, Apache uses chunked\n> transfer-encoding only for dynamic resources, where it can't \n> predict the\n> content-length.  The alternative would be (a) buffer the output before\n> sending it down, or (b) defeat persistent connections.  \n> Either (a) or (b)\n> would increase; (b) would actually cost extra bandwidth, and (a) would\n> cause bandwidth consumption to come in spikes.  So, most \n> likely, the cost\n> of chunking is lower than the cost of not chunking; it's \n> certainly lower\n> than the nominal overhead of the encoding.\n> \n> (Sorry to those to whom this is obvious--probably including \n> Joris--but I\n> didn't want to leave anybody thinking they could save \n> bandwidth by turning\n> off chunking.  :-)\n> \n> --\n> /=================================================================\\\n> |John Stracke    | http://www.ecal.com |My opinions are my own.   |\n> |Chief Scientist |================================================|\n> |eCal Corp.      |In the country of the blind, the one-eyed man is|\n> |francis@ecal.com|in therapy.                                     |\n> \\=================================================================/\n> \n> \n> \n> \n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-15823563"}, {"subject": "RE: Http overhea", "content": "It did was a while ago, when I did some experimenting (personal purpose)\nwith HTTP. Your server (MSN Search) didn't return a content-length field\nor send the data chunked (this is a good while ago)... Also another\nserver from CNET did the same thing. At least this was about a year ago,\nor maybe even a longer time a ago, this wasn't recently....\n\nI don't know how your search servers work, but maybe the server just\nforwarded a response from another server. The URL I gave to the server\n(using telnet) I had from the search area in IE 4. So maybe actual data\ncame from someone like Infoseek or Aslavista....\nThe server however did turn, at that time, \"Server: Microsoft-IIS[...]\".\n\nAt that time I was developing a simple proxy server (for myself), that\nran into trouble with sites not giving a content-length or using chunked\ntranfer-encoding. The first trouble I then developed support was chunked\ntranfer encoding, what did turn out to work correct. But at this time I\nstill couldn't download from MSN search of CNET. Possiblity that it was\nthis error.\n\nAlso the proxy didn't support caching (yet) and did turn out to be quite\nunreliable, especially with subsequent requests on a connection.\n\n\nBut don't forget I is a long while ago, what I actually should have put\nin the previous mail (my fault). Software and systems evolve, and you\nprobably don't have the same stuff you had a couple years ago...\n\n\n- Joris\n\n\n> -----Original Message-----\n> From: Chris Wendt [mailto:christw@microsoft.com]\n> Sent: Saturday 02 December, 2000 5:12\n> To: 'joris.dobbelsteen@mail.com'\n> Subject: RE: Http overhead\n> \n> \n> Joris, \n> \n> We in MSN Search do populate the content length field in the \n> header, just\n> verified. Could you point out what you consider wrong in what you are\n> seeing?\n>  \n> In general, with IIS, if the client is HTTP 1.0 (old), and \n> the server ASP\n> dumps the buffer before the response ends, then there may be a chunked\n> response.  I am not aware of our service ever invoking that \n> behavior.  This\n> should be the same behavior for any web server. \n> \n> Chris.. \n>  \n> -----Original Message----- \n> From: Joris Dobbelsteen [mailto:joris.dobbelsteen@mail.com] \n> Sent: Friday, December 01, 2000 1:24 PM \n> To: WWW WG (E-mail) \n> Subject: RE: Http overhead \n>  \n> You are right, I did know this. \n> Indeed that's why I favor chunked encoding for dynamic \n> resources. MSN's \n> (search) server (MS-IIS) transfers the data by just closing the \n> connection at the end (not sending any indication about the \n> size of the \n> document) what also decreases the transfer 'reliability' - or better, \n> you don't know wether you have all or not... HTML or JPEG/GIF you can \n> guess it, but with many other sources you don't (like TXT).... \n> Also MSN's server has thus more overhead for the server and \n> waisted time \n> between the requests for the client... \n> - Joris \n> > -----Original Message----- \n> > From: francis@localhost.localdomain \n> > [mailto:francis@localhost.localdomain]On Behalf Of John Stracke \n> > Sent: Friday 01 December, 2000 16:04 \n> > To: WWW WG (E-mail) \n> > Subject: Re: Http overhead \n> > \n> > \n> > Joris Dobbelsteen wrote: \n> > \n> > > Many apache servers send data chunked. Takes a couple bytes \n> > (average of \n> > > 4) for every block transfered. Maybe a total overhead of an \n> > additional \n> > > 20-40 bytes per transfer (maybe less). This is just a guess... \n> > \n> > Yes, but it's actually better than that: AFAIK, Apache uses chunked \n> > transfer-encoding only for dynamic resources, where it can't \n> > predict the \n> > content-length.  The alternative would be (a) buffer the \n> output before \n> > sending it down, or (b) defeat persistent connections.  \n> > Either (a) or (b) \n> > would increase; (b) would actually cost extra bandwidth, \n> and (a) would \n> > cause bandwidth consumption to come in spikes.  So, most \n> > likely, the cost \n> > of chunking is lower than the cost of not chunking; it's \n> > certainly lower \n> > than the nominal overhead of the encoding. \n> > \n> > (Sorry to those to whom this is obvious--probably including \n> > Joris--but I \n> > didn't want to leave anybody thinking they could save \n> > bandwidth by turning \n> > off chunking.  :-) \n> > \n> > -- \n> > /=================================================================\\ \n> > |John Stracke    | http://www.ecal.com |My opinions are my own.   | \n> > |Chief Scientist |================================================| \n> > |eCal Corp.      |In the country of the blind, the one-eyed man is| \n> > |francis@ecal.com|in therapy.                                     | \n> > \\=================================================================/ \n> > \n> > \n> > \n> > \n> \n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-15833064"}, {"subject": "Another erratum for RFC261", "content": "Minor editorial glitch:\n\nIn Section 13.5.1, the list of hop-by-hop headers includes\nTrailers\nbut the name of the header is \"Trailer\" (no 's').\n\n-Jeff\n\n\n\n", "id": "lists-012-15846119"}, {"subject": "Re: Push technolog", "content": "See http://home.netscape.com/assist/net_sites/pushpull.html\n\n     -Carl\n\nP.S.  It doesn't work on IE, despite its \"User-Agent:  Mozilla/4.0\n(compatible...\" claim.\n\n\npdf@bizfon.com on 11/15/2000 10:43:11 AM\n\nTo:   http-wg@cuckoo.hpl.hp.com\ncc:\nSubject:  Push technology\n\n\n\n\n\nCan anyone tell me where I can find information on this?  I read that HTTP\nincludes \"push\" technology to send information, but I can't find any solid\ninformation on the W3 website.  Any help is appreciated.\n\nThanks,\nPeter Foti\n\n\n\n", "id": "lists-012-15852329"}, {"subject": "SetCookie Heade", "content": "Hi,\n\nIs it normal for a server to reject the following header because of the\nfinal ';' ?\n\nSet-Cookie: MyName=MyValue; path=/;\n\n...looking in the spec it says the semi-colon acts as a 'separator', does\nthis mean that if it ends the Set-Cookie header it should reject it or is it\nat the discrepancy of the server?\n\nThe following works fine:\n\nSet-Cookie: MyName=MyValue; path=/\n\nThanks,\n\nJim.\n\n\nDisclaimer\nInformation in this E-mail and in any attachments is confidential\nand intended solely for the attention and use of the named addressee(s)\nThis information may be subject to legal, professional or other privilege.\n\nIf you are not the intended recipient, or a person responsible for \ndelivering it to the intended recipient, you are not authorised to\nand must not disclose, copy distribute, or retain this message or any part\nof it.\nIf you have received this message in error please contact us at once,\nso that we may take the appropriate action and avoid troubling you further.\n\n\n\n", "id": "lists-012-15860404"}, {"subject": "Re: Push technolog", "content": "also see http://www.pushcache.com\nand http://workshop.ircache.net/Papers/chen-html/\n\nRegards,\nIlker G.\n\n> Kimden: Carl Kugler [mailto:kugler@us.ibm.com]\n> Tarih: Thursday, December 07, 2000 11:39 PM\n> Kime: pdf@bizfon.com\n> Bilgi: http-wg@cuckoo.hpl.hp.com\n> Konu: Re: Push technology\n> \n> \n> \n> See http://home.netscape.com/assist/net_sites/pushpull.html\n> \n>      -Carl\n> \n> P.S.  It doesn't work on IE, despite its \"User-Agent:  Mozilla/4.0\n> (compatible...\" claim.\n> \n> \n> pdf@bizfon.com on 11/15/2000 10:43:11 AM\n> \n> To:   http-wg@cuckoo.hpl.hp.com\n> cc:\n> Subject:  Push technology\n> \n> \n> \n> \n> \n> Can anyone tell me where I can find information on this?  I \n> read that HTTP\n> includes \"push\" technology to send information, but I can't \n> find any solid\n> information on the W3 website.  Any help is appreciated.\n> \n> Thanks,\n> Peter Foti\n> \n> \n> \n> \n> \n\n\nBu e-postada bulunan t?m fikir ve g?r??ler ve ekindeki dosyalar sadece adres\nsahib(ler)ine ait olup, S?merbank A.? hi? bir ?ekilde sorumlu tutulamaz. \nThe information contained in this E-Mail and any files transmitted with it\nare intended solely for the use of the individual or entity to whom they are\naddressed and do not reflect those of Sumerbank A.S.\n\n\n\n", "id": "lists-012-15868089"}, {"subject": "Re: SetCookie Heade", "content": "Thomson James <James.Thomson@jeyes.co.uk> wrote:\n  > Hi,\n  > \n  > Is it normal for a server to reject the following header because of the\n  > final ';' ?\n  > \n  > Set-Cookie: MyName=MyValue; path=/;\n  > \n  > ...looking in the spec it says the semi-colon acts as a 'separator', does\n  > this mean that if it ends the Set-Cookie header it should reject it or is it\n  > at the discrepancy of the server?\n  > \n  > The following works fine:\n  > \n  > Set-Cookie: MyName=MyValue; path=/\n\nThe spec. does not allow the extra ';', but a tolerant client would\naccept it.\n\nDave Kristol\n\n\n\n", "id": "lists-012-15878169"}, {"subject": "this is a Test Repl", "content": "?\n\n>>> \"M.Taha Masood\" <taha.masood@streaming-networks.com> Dec 18, 2000  11:05 am >>>\n\n\n\n", "id": "lists-012-15892095"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "> If you have a list of features you'd like to test against,\n> could you send it out?\n\nSure. I'm looking for servers (better even: URLs) which implement any of\nthe following:\n\n    H 10.3.6 305 Use Proxy\n    H 10.3.7 307 Temporary Redirect\n    H 10.4.9 408 Request Timeout\n\n    H 14.37  Retry-After\n    H 14.39  TE\n    H 14.40  Trailer\n    H 14.41  Transfer-Encoding  (specifically: gzip, deflate, and compress)\n\n    A 3.2.1/3.2.2/3.2.3 Digest Authentication with \"qop=auth\" and \"qop=auth-int\"\n\nAlso looking for a proxy which does:\n\n    A 4.2  Digest Authentication with \"qop=auth\" and \"qop=auth-int\"\n    A 4.2  Proxy-Authentication-Info\n\nConcerning the low implemented list:\n\n    H 10.1.2 101 Switching Protocols\n    H 14.42  Upgrade\n\nI'm not sure what there is to implement here... If any server does\nimplement these, then what do they accept in the Upgrade header?\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1589483"}, {"subject": "remov", "content": "--\nManickam R.Sridhar\nCTO\nSitara Networks Inc.\n52, Second Avenue,\nSuite 200,\nWaltham, MA 02451.\n\n(781)487-5926 (office)\n\n\n\n", "id": "lists-012-15898479"}, {"subject": "RE: HTTP features w/ low 'implemented' and 'tested", "content": "For the features you've listed as wanting a server to test\nagainst, I've asked the 'contact' for the private reports\ndirectly. The implementation reports were:\n\nh 10.3.6 Use Proxy\n         1 other, 2 clients, 1 proxy\nh 10.3.7 Temporary Redirect\n         1 other, 3 clients, 1 proxy, 1 combined proxy/server (tested)\nh 10.4.9 Request Timeout\n         1 other, 2 clients\nh 14.37 Retry-After\n         1 other, 1 origin\nH 14.39 TE\n         1 other, 2 clients\nH 14.40 Trailer\n         1 other, 2 clients\nH 14.41 Transfer-Encoding\n         1 other, 1 client, 2 origin, 1 combined\n\n\nI thought I'd add some notes from the test reports:\n\nH 14.36Referert\n\n(client) isn't sure what to test. Implemented only in as much as that\nif the application set the header and a redirection occurs then the\nheader is modified accordingly.\n\nH 8.2.3 Automatic retrying of requests\n(client) sometimes requests can't be retried because it doesn't buffer\nthe entity\n(proxy) does not retry\n\nH 8.2.4 Use of 100 (Continue)\n(client) doesn't wait for 100 before sending content\n\nH 10.1.1 100 Continue\n(client) ignores all 100s\n\nH 10.2.3, 4, 6, 10.3.1,2,4,6,7; 10.4.3, 10: \"plugin or cgi could\"\n\n10.4.9 408 Request Timeout\n(server) would like to have general trailer for this\n\nH 14.17 Content-Type\n(server) common browsers have poor support for charset attribute\n\nH 14.20 Expect\n(client) 100-Continue only\n\nH 14.22 From\n(proxy) can generate\n\nH 14.45Via\nA missing Via is used to try and detect buggy HTTP/1.0 proxies.\n\nH 14.46 Warning\nNo user agents do anything with it\n\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-1597938"}, {"subject": "Happy new yea", "content": "To all WGs I'm subscribed to, nobody does it, so I'll just send it:\n\n\n\nHappy new year and a good start of the new millennium\n\n\n\n\n\nHope I can say this for all members of the WGs...\n\n\n- Joris\n\n\n\n", "id": "lists-012-16003423"}, {"subject": "remov", "content": "remove\n\n\n\n", "id": "lists-012-16012900"}, {"subject": "Logou", "content": "Dear Sirs,\n\nIs it required that user agents have a mechanism for expiring or forgetting\nthe passwords that are used to access HTTP servers?  IE: a \"logout\" button\nfor HTTP built-in authentication.\n\nI imagine that this is the sort of requirement that HTTP people think that\nthis should be in the HTML group - and vice-versa.\n\nHowever it is an embarrassing oversight in modern browsers.\n\n\n                                - Erik\n\n\n\n", "id": "lists-012-16018862"}, {"subject": "Re: Logou", "content": "\"Erik Aronesty\" <erik@primedata.org> wrote:\n  > \n  > Dear Sirs,\n  > \n  > Is it required that user agents have a mechanism for expiring or forgetting\n  > the passwords that are used to access HTTP servers?  IE: a \"logout\" button\n  > for HTTP built-in authentication.\n  > \n  > I imagine that this is the sort of requirement that HTTP people think that\n  > this should be in the HTML group - and vice-versa.\n  > \n  > However it is an embarrassing oversight in modern browsers.\n\n<sigh>\n\nYou have touched on one of *my* hot buttons.  I have argued for such a\nthing for, oh, about six years.  Obviously without success.  As you\nguess, it's not an HTTP issue, having nothing really to do with the\n*protocol*.  But it's also not an HTML issue, having nothing to do with\nthe content of pages.  Rather it's a user interface issue, and thus at\nthe discretion of the browser vendors.  And, for whatever reason, they\nhave never been interested in providing a way to discard passwords,\nexcept to exit the browser.\n\nI can think of two situations where such a feature would be *really*\nhandy:\n\n1) When I'm trying to debug server-side authentication code, and I want\nto force the browser I'm using to forget its passwords.\n\n2) In an environment where machines are shared (college computer lab,\npublic library, Internet cafe), and I want to discard the passwords\nI've entered before I leave the machine.\n\nSimilar reasoning would recommend a feature to discard all cookies, as\nwell, but that's another topic entirely. :-)\n\nDave Kristol\n\n\n\n", "id": "lists-012-16026242"}, {"subject": "Re: Logou", "content": "I decided several years ago to stop use http authentication and instead use a\nsimilar system with cookies, because http authentication transmits everything in\nunencoded form.  I realize that cookies don't provide much better security as the\ninitial password is going to\nbe unencoded, but somehow I got some (no doubt misplaced) peace of mind out of\nthat.\n\nAs to your question though, with cookies it's easy enough to just set a cookie with\nthe same name but a different value.  The new cookie will wipe out the old cookie.\n\n-Doug Sims\n\n\nDave Kristol wrote:\n\n> \"Erik Aronesty\" <erik@primedata.org> wrote:\n>   >\n>   > Dear Sirs,\n>   >\n>   > Is it required that user agents have a mechanism for expiring or forgetting\n>   > the passwords that are used to access HTTP servers?  IE: a \"logout\" button\n>   > for HTTP built-in authentication.\n>   >\n>   > I imagine that this is the sort of requirement that HTTP people think that\n>   > this should be in the HTML group - and vice-versa.\n>   >\n>   > However it is an embarrassing oversight in modern browsers.\n>\n> <sigh>\n>\n> You have touched on one of *my* hot buttons.  I have argued for such a\n> thing for, oh, about six years.  Obviously without success.  As you\n> guess, it's not an HTTP issue, having nothing really to do with the\n> *protocol*.  But it's also not an HTML issue, having nothing to do with\n> the content of pages.  Rather it's a user interface issue, and thus at\n> the discretion of the browser vendors.  And, for whatever reason, they\n> have never been interested in providing a way to discard passwords,\n> except to exit the browser.\n>\n> I can think of two situations where such a feature would be *really*\n> handy:\n>\n> 1) When I'm trying to debug server-side authentication code, and I want\n> to force the browser I'm using to forget its passwords.\n>\n> 2) In an environment where machines are shared (college computer lab,\n> public library, Internet cafe), and I want to discard the passwords\n> I've entered before I leave the machine.\n>\n> Similar reasoning would recommend a feature to discard all cookies, as\n> well, but that's another topic entirely. :-)\n>\n> Dave Kristol\n\n\n\n", "id": "lists-012-16035022"}, {"subject": "Re: Logou", "content": "Douglas Sims wrote:\n> [...]\n> As to your question though, with cookies it's easy enough to just set a cookie with\n> the same name but a different value.  The new cookie will wipe out the old cookie.\n\nYes, that solves the problem from the server's perspective, assuming an\napplication provides that capability.\n\nBut if I'm using a public access machine and I want to ensure that that machine\nhas removed all the cookies I might have received (including those that might\nhave been used for personalized services and/or authentication), *I* have no\ndirect way to do so.\n\nDave Kristol\n\n\n\n", "id": "lists-012-16044252"}, {"subject": "RE: Logou", "content": "<apologies for resend>\n\nExcuse me if I'm missing the point, but with IE (my browser of choice),\nisn't this addressed by the autocomplete option, esp. the ability to clear\npasswords/details? Also, on shared PCs, I find deleting temporary files when\nI leave a necessity.\n\nAn interesting point raised is whether browser vendors should expose what is\nprimarily internal functionality to aid the user (e.g. password remembrance)\nto external (primarily display only) data. Similar to Java access\npermissions? Hmmm ...\n\n-----Original Message-----\nFrom: Dave Kristol [mailto:dmk@research.bell-labs.com]\nSent: 02 January 2001 19:16\nTo: erik@primedata.org\nCc: http-wg@cuckoo.hpl.hp.com\nSubject: Re: Logout\n\n\n\"Erik Aronesty\" <erik@primedata.org> wrote:\n  >\n  > Dear Sirs,\n  >\n  > Is it required that user agents have a mechanism for expiring or\nforgetting\n  > the passwords that are used to access HTTP servers?  IE: a \"logout\"\nbutton\n  > for HTTP built-in authentication.\n  >\n  > I imagine that this is the sort of requirement that HTTP people think\nthat\n  > this should be in the HTML group - and vice-versa.\n  >\n  > However it is an embarrassing oversight in modern browsers.\n\n<sigh>\n\nYou have touched on one of *my* hot buttons.  I have argued for such a\nthing for, oh, about six years.  Obviously without success.  As you\nguess, it's not an HTTP issue, having nothing really to do with the\n*protocol*.  But it's also not an HTML issue, having nothing to do with\nthe content of pages.  Rather it's a user interface issue, and thus at\nthe discretion of the browser vendors.  And, for whatever reason, they\nhave never been interested in providing a way to discard passwords,\nexcept to exit the browser.\n\nI can think of two situations where such a feature would be *really*\nhandy:\n\n1) When I'm trying to debug server-side authentication code, and I want\nto force the browser I'm using to forget its passwords.\n\n2) In an environment where machines are shared (college computer lab,\npublic library, Internet cafe), and I want to discard the passwords\nI've entered before I leave the machine.\n\nSimilar reasoning would recommend a feature to discard all cookies, as\nwell, but that's another topic entirely. :-)\n\nDave Kristol\n\n\n\n", "id": "lists-012-16051431"}, {"subject": "Re: Logou", "content": "> > the passwords that are used to access HTTP servers?  IE: a \"logout\"\nbutton\n> > for HTTP built-in authentication.\n> >\n> > I imagine that this is the sort of requirement that HTTP people think\nthat\n> > this should be in the HTML group - and vice-versa.\n> >\n> > However it is an embarrassing oversight in modern browsers.\n>\n> One that some of us have tried hard to overcome, to no avail.  The\n> basic problem is that the browser vendors have listened carefully to\n> what thier customers want, and have heard loud and clear that they\n> don't want to have to remember passwords.\n\nOver 600 users have asked us within the last year how to \"log out\" of sites\nsuch as etrade and daytek which use HTTP based authentication.\n\nBrowser customers don't want to remember passwords - however they want\na \"logout button\" as well.  This is not a paradox and there is no\ninextricable reason why\nbrowsers can't cache usr information but have a button for \"clearing the\ncache\"\n\nI think the real reason that this has not been done is because both major\nbrowsers today have other agendas regarding network access and security.\n\nCurrently there is no way to clear the cache by having an HTTP server\nrequest\nit to be cleared - or by a user initiating the clearing of this information.\nThis\nis a basic security leak - and should be plugged.\n\n> Paul Leach of Microsoft and I attempted to provide a framework for a\n> solution to this and some related problems in a submission to the\n> W3C (User Agent Authentication Forms) in February of 1999:\n>\n>     http://www.w3.org/TR/1999/NOTE-authentform-19990203\n\n\nHowever, this is a \"forms based\" solution which undermines digest\nauthentication\nand other more \"standard\" forms of authentication - that have proved very\nhelpful\nto developers of web applications.\n\nSimply, there should be one line added to section 4.13\n\n    ftp://ftp.isi.edu/in-notes/rfc2617.txt\n\n\"It is reccomended that the authenticating agent provide a set mechanisms\nfor\nremoving entries from the \"password file\" associated with a given realm, for\nthe purposes of logging out of a system.\"\n\nAnd that's about all that's necessary.\n\nI don't think it needs a whole RFC ... just an addendum to existing ones.\n\n            - Erik\n\n\n\n", "id": "lists-012-16061045"}, {"subject": "Re: Logou", "content": "Sorry I found it... there is a recommendation,\n\n    Microsoft and Netscape just blindly ignore it:\n\nSection 15.6 \"Authentication Credentials and Idle Clients\":\n\n \"In particular, user agents which cache credentials are\n   encouraged to provide a readily accessible mechanism for discarding\n   cached credentials under user control.\"\n\nWhich neither do - even though it's a security hole.\n\n                - Erik\n\n----- Original Message -----\nFrom: \"Erik Aronesty\" <erik@primedata.org>\nTo: \"Scott Lawrence\" <slawrence@virata.com>\nCc: <http-wg@cuckoo.hpl.hp.com>\nSent: Tuesday, January 02, 2001 4:12 PM\nSubject: Re: Logout\n\n\n> > > the passwords that are used to access HTTP servers?  IE: a \"logout\"\n> button\n> > > for HTTP built-in authentication.\n> > >\n> > > I imagine that this is the sort of requirement that HTTP people think\n> that\n> > > this should be in the HTML group - and vice-versa.\n> > >\n> > > However it is an embarrassing oversight in modern browsers.\n> >\n> > One that some of us have tried hard to overcome, to no avail.  The\n> > basic problem is that the browser vendors have listened carefully to\n> > what thier customers want, and have heard loud and clear that they\n> > don't want to have to remember passwords.\n>\n> Over 600 users have asked us within the last year how to \"log out\" of\nsites\n> such as etrade and daytek which use HTTP based authentication.\n>\n> Browser customers don't want to remember passwords - however they want\n> a \"logout button\" as well.  This is not a paradox and there is no\n> inextricable reason why\n> browsers can't cache usr information but have a button for \"clearing the\n> cache\"\n>\n> I think the real reason that this has not been done is because both major\n> browsers today have other agendas regarding network access and security.\n>\n> Currently there is no way to clear the cache by having an HTTP server\n> request\n> it to be cleared - or by a user initiating the clearing of this\ninformation.\n> This\n> is a basic security leak - and should be plugged.\n>\n> > Paul Leach of Microsoft and I attempted to provide a framework for a\n> > solution to this and some related problems in a submission to the\n> > W3C (User Agent Authentication Forms) in February of 1999:\n> >\n> >     http://www.w3.org/TR/1999/NOTE-authentform-19990203\n>\n>\n> However, this is a \"forms based\" solution which undermines digest\n> authentication\n> and other more \"standard\" forms of authentication - that have proved very\n> helpful\n> to developers of web applications.\n>\n> Simply, there should be one line added to section 4.13\n>\n>     ftp://ftp.isi.edu/in-notes/rfc2617.txt\n>\n> \"It is reccomended that the authenticating agent provide a set mechanisms\n> for\n> removing entries from the \"password file\" associated with a given realm,\nfor\n> the purposes of logging out of a system.\"\n>\n> And that's about all that's necessary.\n>\n> I don't think it needs a whole RFC ... just an addendum to existing ones.\n>\n>             - Erik\n>\n\n\n\n", "id": "lists-012-16070347"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": ">Concerning the low implemented list:\n>\n>    H 10.1.2 101 Switching Protocols\n>    H 14.42  Upgrade\n>\n>I'm not sure what there is to implement here... If any server does\n>implement these, then what do they accept in the Upgrade header?\n\nPersonally, I find it incredibly shortsighted for the IETF not to\ninclude a special exception for optional features intended to support\nfuture protocol extensions and/or replacement.  I'd hate to have to\nexclude good engineering practice just to call it a draft standard.\n\n....Roy\n\n\n\n", "id": "lists-012-1607339"}, {"subject": "RE: Logou", "content": "I agree that a \"logout\" type button should certainly be implemented. I'm\ninterested in your choice of words however, naming the non-provision of an\nHTTP server cache clearance request as a security hole. In my opinion it is\nthe responsibility of the site to provide some form of timeout security. To\nprovide an HTTP type clearance of the cache is exposing the agent to what\namounts to control by a third party. Surely this would constitute a greater\nthreat to security and not be a road to wander down without serious\nconsideration of the potential future implications?\n\nTom\n\n> -----Original Message-----\n> From: Erik Aronesty [mailto:erik@primedata.org]\n> Sent: 02 January 2001 21:15\n> To: Erik Aronesty; Scott Lawrence\n> Cc: http-wg@cuckoo.hpl.hp.com; support@microsoft.com\n> Subject: Re: Logout\n>\n>\n>\n> Sorry I found it... there is a recommendation,\n>\n>     Microsoft and Netscape just blindly ignore it:\n>\n> Section 15.6 \"Authentication Credentials and Idle Clients\":\n>\n>  \"In particular, user agents which cache credentials are\n>    encouraged to provide a readily accessible mechanism for discarding\n>    cached credentials under user control.\"\n>\n> Which neither do - even though it's a security hole.\n>\n>                 - Erik\n>\n> ----- Original Message -----\n> From: \"Erik Aronesty\" <erik@primedata.org>\n> To: \"Scott Lawrence\" <slawrence@virata.com>\n> Cc: <http-wg@cuckoo.hpl.hp.com>\n> Sent: Tuesday, January 02, 2001 4:12 PM\n> Subject: Re: Logout\n>\n>\n> > > > the passwords that are used to access HTTP servers?  IE: a \"logout\"\n> > button\n> > > > for HTTP built-in authentication.\n> > > >\n> > > > I imagine that this is the sort of requirement that HTTP\n> people think\n> > that\n> > > > this should be in the HTML group - and vice-versa.\n> > > >\n> > > > However it is an embarrassing oversight in modern browsers.\n> > >\n> > > One that some of us have tried hard to overcome, to no avail.  The\n> > > basic problem is that the browser vendors have listened carefully to\n> > > what thier customers want, and have heard loud and clear that they\n> > > don't want to have to remember passwords.\n> >\n> > Over 600 users have asked us within the last year how to \"log out\" of\n> sites\n> > such as etrade and daytek which use HTTP based authentication.\n> >\n> > Browser customers don't want to remember passwords - however they want\n> > a \"logout button\" as well.  This is not a paradox and there is no\n> > inextricable reason why\n> > browsers can't cache usr information but have a button for \"clearing the\n> > cache\"\n> >\n> > I think the real reason that this has not been done is because\n> both major\n> > browsers today have other agendas regarding network access and security.\n> >\n> > Currently there is no way to clear the cache by having an HTTP server\n> > request\n> > it to be cleared - or by a user initiating the clearing of this\n> information.\n> > This\n> > is a basic security leak - and should be plugged.\n> >\n> > > Paul Leach of Microsoft and I attempted to provide a framework for a\n> > > solution to this and some related problems in a submission to the\n> > > W3C (User Agent Authentication Forms) in February of 1999:\n> > >\n> > >     http://www.w3.org/TR/1999/NOTE-authentform-19990203\n> >\n> >\n> > However, this is a \"forms based\" solution which undermines digest\n> > authentication\n> > and other more \"standard\" forms of authentication - that have\n> proved very\n> > helpful\n> > to developers of web applications.\n> >\n> > Simply, there should be one line added to section 4.13\n> >\n> >     ftp://ftp.isi.edu/in-notes/rfc2617.txt\n> >\n> > \"It is reccomended that the authenticating agent provide a set\n> mechanisms\n> > for\n> > removing entries from the \"password file\" associated with a given realm,\n> for\n> > the purposes of logging out of a system.\"\n> >\n> > And that's about all that's necessary.\n> >\n> > I don't think it needs a whole RFC ... just an addendum to\n> existing ones.\n> >\n> >             - Erik\n> >\n>\n>\n>\n\n\n\n", "id": "lists-012-16082063"}, {"subject": "Re: Logou", "content": "Dear Tom,\n\nThe site cannot easily know whether or not the request was coming from the\ncache or the client... unless the cache tells it.\n\nThus the server always relys on a \"third party\" (the browser or the\ncache)... to manage or respect authentication \"state\".\n\nIt's just an oversight that cookies are \"expirable\" (they have timeouts and\nthey can be forced by the server to expire) and usernames/passwords aren't.\n\nIn a way, \"cookies\" are \"more secure\" than the security mechanisms built\ninto http.\n\n                - Erik\n\n----- Original Message -----\nFrom: \"Tom McLaren\" <tom@mclaren.tc>\nTo: \"Erik Aronesty\" <erik@primedata.org>\nCc: <http-wg@cuckoo.hpl.hp.com>\nSent: Wednesday, January 03, 2001 4:47 AM\nSubject: RE: Logout\n\n\n> I agree that a \"logout\" type button should certainly be implemented. I'm\n> interested in your choice of words however, naming the non-provision of an\n> HTTP server cache clearance request as a security hole. In my opinion it\nis\n> the responsibility of the site to provide some form of timeout security.\nTo\n> provide an HTTP type clearance of the cache is exposing the agent to what\n> amounts to control by a third party. Surely this would constitute a\ngreater\n> threat to security and not be a road to wander down without serious\n> consideration of the potential future implications?\n>\n> Tom\n>\n> > -----Original Message-----\n> > From: Erik Aronesty [mailto:erik@primedata.org]\n> > Sent: 02 January 2001 21:15\n> > To: Erik Aronesty; Scott Lawrence\n> > Cc: http-wg@cuckoo.hpl.hp.com; support@microsoft.com\n> > Subject: Re: Logout\n> >\n> >\n> >\n> > Sorry I found it... there is a recommendation,\n> >\n> >     Microsoft and Netscape just blindly ignore it:\n> >\n> > Section 15.6 \"Authentication Credentials and Idle Clients\":\n> >\n> >  \"In particular, user agents which cache credentials are\n> >    encouraged to provide a readily accessible mechanism for discarding\n> >    cached credentials under user control.\"\n> >\n> > Which neither do - even though it's a security hole.\n> >\n> >                 - Erik\n> >\n> > ----- Original Message -----\n> > From: \"Erik Aronesty\" <erik@primedata.org>\n> > To: \"Scott Lawrence\" <slawrence@virata.com>\n> > Cc: <http-wg@cuckoo.hpl.hp.com>\n> > Sent: Tuesday, January 02, 2001 4:12 PM\n> > Subject: Re: Logout\n> >\n> >\n> > > > > the passwords that are used to access HTTP servers?  IE: a\n\"logout\"\n> > > button\n> > > > > for HTTP built-in authentication.\n> > > > >\n> > > > > I imagine that this is the sort of requirement that HTTP\n> > people think\n> > > that\n> > > > > this should be in the HTML group - and vice-versa.\n> > > > >\n> > > > > However it is an embarrassing oversight in modern browsers.\n> > > >\n> > > > One that some of us have tried hard to overcome, to no avail.  The\n> > > > basic problem is that the browser vendors have listened carefully to\n> > > > what thier customers want, and have heard loud and clear that they\n> > > > don't want to have to remember passwords.\n> > >\n> > > Over 600 users have asked us within the last year how to \"log out\" of\n> > sites\n> > > such as etrade and daytek which use HTTP based authentication.\n> > >\n> > > Browser customers don't want to remember passwords - however they want\n> > > a \"logout button\" as well.  This is not a paradox and there is no\n> > > inextricable reason why\n> > > browsers can't cache usr information but have a button for \"clearing\nthe\n> > > cache\"\n> > >\n> > > I think the real reason that this has not been done is because\n> > both major\n> > > browsers today have other agendas regarding network access and\nsecurity.\n> > >\n> > > Currently there is no way to clear the cache by having an HTTP server\n> > > request\n> > > it to be cleared - or by a user initiating the clearing of this\n> > information.\n> > > This\n> > > is a basic security leak - and should be plugged.\n> > >\n> > > > Paul Leach of Microsoft and I attempted to provide a framework for a\n> > > > solution to this and some related problems in a submission to the\n> > > > W3C (User Agent Authentication Forms) in February of 1999:\n> > > >\n> > > >     http://www.w3.org/TR/1999/NOTE-authentform-19990203\n> > >\n> > >\n> > > However, this is a \"forms based\" solution which undermines digest\n> > > authentication\n> > > and other more \"standard\" forms of authentication - that have\n> > proved very\n> > > helpful\n> > > to developers of web applications.\n> > >\n> > > Simply, there should be one line added to section 4.13\n> > >\n> > >     ftp://ftp.isi.edu/in-notes/rfc2617.txt\n> > >\n> > > \"It is reccomended that the authenticating agent provide a set\n> > mechanisms\n> > > for\n> > > removing entries from the \"password file\" associated with a given\nrealm,\n> > for\n> > > the purposes of logging out of a system.\"\n> > >\n> > > And that's about all that's necessary.\n> > >\n> > > I don't think it needs a whole RFC ... just an addendum to\n> > existing ones.\n> > >\n> > >             - Erik\n> > >\n> >\n> >\n> >\n>\n>\n>\n\n\n\n", "id": "lists-012-16095208"}, {"subject": "RE: Logou", "content": "On Wed, 3 Jan 2001, Tom McLaren wrote:\n\n> I agree that a \"logout\" type button should certainly be implemented. I'm\n> interested in your choice of words however, naming the non-provision of an\n> HTTP server cache clearance request as a security hole. In my opinion it is\n> the responsibility of the site to provide some form of timeout security. To\n> provide an HTTP type clearance of the cache is exposing the agent to what\n> amounts to control by a third party. Surely this would constitute a greater\n> threat to security and not be a road to wander down without serious\n> consideration of the potential future implications?\n\nAny control provided to the server should of course be scoped to the data\n'owned' by that server, hence no security exposure. Likewise, it should be\npossible for a user to expunge ALL data cached from their session, login\ncredentials, cookies, pages etc. It should be possible for the 'owner' of\nthe user agent installation to configure the UA to peform this function\nautomatically when closed, etc.  Again not a security issue if the action\nis performed by or directly on behalf of the human user.\n\nAnd of course, any well designed web application will implement a timeout\nstragegy because they can't trust the other end.  Unfortunately short\ntimeouts which improve security have the strong potential for very\nfrustrated users who happen to be interrupted by a phone call or other\ntask while in the middle of an interaction.\n\n\nDave Morris\n\n\n\n", "id": "lists-012-16110300"}, {"subject": "Re: Logou", "content": "On Wed, 3 Jan 2001, Erik Aronesty wrote:\n\n> The site cannot easily know whether or not the request was coming from the\n> cache or the client... unless the cache tells it.\n>\n> Thus the server always relys on a \"third party\" (the browser or the\n> cache)... to manage or respect authentication \"state\".\n>\n> It's just an oversight that cookies are \"expirable\" (they have timeouts and\n> they can be forced by the server to expire) and usernames/passwords aren't.\n>\n> In a way, \"cookies\" are \"more secure\" than the security mechanisms built\n> into http.\n\nExcept that as V1 cookies are implemented, once you set an expiration,\neven short on a cookie, it is written to the user's hard drive.  An\nimmediate exposure which to me negates any value in a timeout.\n\nWhat makes cookies more secure is that they can be used via a random\nopaque token as the value to create a server side notion of a session\nwhich can have expirations, sliding timeouts, re-authentication, etc. And\nif an application logout button is clicked, the cookie can be totally\nreset by client side javascript OR a serverside update.\n\nI've led a number of teams building web based applications and have never\nfound http level authentication worth using. The user experience is\nconfusing at best, the server side authentication processing excessively\ncomplex, error handling impossible to control, etc. It takes writing your\nown web server or complex ISAPI/NSAPI exits to control the interaction\nwith application authentication mechanisms.\n\nSo again cookies win.\n\nDave Morris\n\n\n\n", "id": "lists-012-16119002"}, {"subject": "Re: Logou", "content": "So, \"cookies win\" because :\n\n    - iis and netscape's support for http authentication are generally lousy\n\n    - http-authentication standard has somehow \"slipped\" and has been\noverlooked in each successive version\n\n    - the \"cookie standard\" has progressed quickly and supports all of the\nthings that regular authentication should have supported a long time ago:\n\n        - passing a \"server key\" to be added to the digest, which can then\nbe expired on the server\n        - setting expiration times\n\n                                            - Erik\n\n\n\n\n\n\n\n----- Original Message -----\nFrom: \"David W. Morris\" <dwm@xpasc.com>\nTo: \"Erik Aronesty\" <erik@primedata.org>\nCc: \"Tom McLaren\" <tom@mclaren.tc>; <http-wg@cuckoo.hpl.hp.com>\nSent: Wednesday, January 03, 2001 1:19 PM\nSubject: Re: Logout\n\n\n>\n> On Wed, 3 Jan 2001, Erik Aronesty wrote:\n>\n> > The site cannot easily know whether or not the request was coming from\nthe\n> > cache or the client... unless the cache tells it.\n> >\n> > Thus the server always relys on a \"third party\" (the browser or the\n> > cache)... to manage or respect authentication \"state\".\n> >\n> > It's just an oversight that cookies are \"expirable\" (they have timeouts\nand\n> > they can be forced by the server to expire) and usernames/passwords\naren't.\n> >\n> > In a way, \"cookies\" are \"more secure\" than the security mechanisms built\n> > into http.\n>\n> Except that as V1 cookies are implemented, once you set an expiration,\n> even short on a cookie, it is written to the user's hard drive.  An\n> immediate exposure which to me negates any value in a timeout.\n>\n> What makes cookies more secure is that they can be used via a random\n> opaque token as the value to create a server side notion of a session\n> which can have expirations, sliding timeouts, re-authentication, etc. And\n> if an application logout button is clicked, the cookie can be totally\n> reset by client side javascript OR a serverside update.\n>\n> I've led a number of teams building web based applications and have never\n> found http level authentication worth using. The user experience is\n> confusing at best, the server side authentication processing excessively\n> complex, error handling impossible to control, etc. It takes writing your\n> own web server or complex ISAPI/NSAPI exits to control the interaction\n> with application authentication mechanisms.\n>\n> So again cookies win.\n>\n> Dave Morris\n>\n>\n>\n\n\n\n", "id": "lists-012-16128013"}, {"subject": "RE: Logou", "content": ">-----Original Message-----\n>From: Erik Aronesty [mailto:erik@primedata.org]\n>Sent: Tuesday 02 January, 2001 22:15\n>To: Erik Aronesty; Scott Lawrence\n>Cc: http-wg@cuckoo.hpl.hp.com; support@microsoft.com\n>Subject: Re: Logout\n>\n>\n>\n>Sorry I found it... there is a recommendation,\n>\n>    Microsoft and Netscape just blindly ignore it:\n>\n>Section 15.6 \"Authentication Credentials and Idle Clients\":\n>\n> \"In particular, user agents which cache credentials are\n>   encouraged to provide a readily accessible mechanism for discarding\n>   cached credentials under user control.\"\n>\n>Which neither do - even though it's a security hole.\n>\nIE5+ does have (somewhat hidden) a function to remove the passwords\nremembered with the AutoComplete function\n(at least on Win2000). You can find it in:  Internet Options -> Content\n(Tab) -> Autocomplete (Button) -> Clear Passwords (Button).\n\nUsed for forms only. This function also asks you whether you want to store\n(instead of cached, it is stored until explicitly removed) the password.\n\n\nOther passwords you enter for use with HTTP authentication are stored in\nthe Windows PassWordList. Under Windows 95/98 and probably ME you can read\nit with just a simple tool (e.g. PWLTool, Back Orifice, ...). Windows\nNT/2000 provide better security on this one, since the tools don't work\nhere.\nOn the Windows 95/98 CD (called PWL-something) you can remove the\npasswords under Win95/98/ME, but you don't have it available on a public\ncomputer.\nLet's not forget, on Windows 95/98/ME security is virtually inexistent.\n\nBut here is always asked whether to store the password (or not).\n\n\nI don't use Netscape......\n\n\nAs for Lynx, this browser seems to 'forget' the passwords ONLY when it is\nclosed. I didn't test it long enough to check for a timeout, nor did I\nread the source of it to see it.\n\n\n\n>\n>   *** SNIP ***\n>\n\nAS for a NEWER mail:\n\n>-----Original Message-----\n>From: Erik Aronesty [mailto:erik@primedata.org]\n>Sent: Thursday 04 January, 2001 3:23\n>To: David W. Morris\n>Cc: Tom McLaren; http-wg@cuckoo.hpl.hp.com\n>Subject: Re: Logout\n>\n>\n>\n>So, \"cookies win\" because :\n>\n>    - iis and netscape's support for http authentication are\n>generally lousy\n\nAgreed, HTTP authentication is, even as standard FTP authentication (as\nthe only supported by IIS), also insecure. Only Digest authentication is a\nlittle bit secure by securely hashing the password.\nHowever the servers also have lousy authentication features, it is\npossible with IIS to do a little thing with it. I expect it is possible\nwith IIS to use the internal security in combination with the web\napplication. You can ask the username, as long as IIS handles the\nauthentication, but not in every environment.\nAlso I don't know if it is possible with ISAPI to 'capture' the password,\nbefore it is processed by the IIS server itself.\nBut still IIS lacks some of the simpler HTTP rules, as IE does for\nauthentication. IIS responds to e.g. HTTP/2.0 requests, as it shouldn't do\nthat.\n>\n>    - http-authentication standard has somehow \"slipped\" and has been\n>overlooked in each successive version\n>\nProbably HTTP was never designed with reasonable security in mind, jet\nother extensions that where needed at that point in time (don't forget\nHTTP/1.1 is a standard for 4 years, and designed/developed more than 4\nyears ago). The authentication protocols that are documented in RFC2617\ncan be considered insecure (even Digest Authentication has some\nweaknesses, not to talk about Basic authentication which has no security\nat all).\nAlso HTTP authentication (methods) do(es)n't depend on sessions, as with\ncookies it is possible.\n\nIn the past HTTP was used for the www.company.com pages, and such, that\ndon't require much security. For traffic that needs to be secure, HTTPS\nwas designed. But encrypting over 100 MB/s or even Gigabytes/s, with\nsufficient secure standards, is a hard job for CPUs, if it needs to be\ndone beside the job of being a (high-volume) HTTP server. Dedicated\nhardware (or plenty of processing power by the CPUs) is desired here.\n\n>    - the \"cookie standard\" has progressed quickly and\n>supports all of the\n>things that regular authentication should have supported a\n>long time ago:\n>\n>        - passing a \"server key\" to be added to the digest,\n>which can then\n>be expired on the server\n>        - setting expiration times\n\nAn issue with IE is the fact that passwords with forms are still stored.\nAn issue with all browsers that support cookies, it that they store the\ncookie on the system. Here is required that the user activates the logout\nfunction to erase the cookie, or at least make it useless. If the user\nrefuses this, the next user (or an adversary) can use the functionality\nprovided on victim's behalf.\n\nNext, what if an adversary finds a way to use his cookie (obtaining it\nusing spoofing or a man-in-the-middle attack (proxies are suitable for\nthis)), the adversary can use that cookie on the victim's behalf. However\nthis is also true for Basic and (maybe for a limited time, due to\n'key'-rotation) Digest authentication.\nWith scripting, it is possible to counter this treat, but not with cookies\nalone, as far as I know.\n\n\nIf you want security for HTTP, you should use Secure-HTTP only.\nAnd I see many companies using HTTPS in combination with cookies (e.g.\nMicrosoft or hotmail(?)).\nThey switch to HTTPS for authentication and use HTTP when you are logged\nin. Doesn't give the full satisfaction of security, but consider the\nincreased costs that have to be made when you use HTTPS only, in contrast\nto HTTP only (see above why).\n\nThe next version of HTTP should have better security functions than\ncurrently provides. The current standard doesn't satisfy the requirements\nof many servers, today.\n\n\n\n- Joris\n\n\n\n\n\n\nNote: There once was someone who wrote a draft about ticket-based\nauthentication about 5 months ago (8-2000). I didn't hear about the draft\nany more, so it might as be well vanished. The mail was called\n\"Ticket-based authentication\", that had 2 replies to the HTTP WG. This one\nmight incorporate what we were looking/asking for...\n\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-16138648"}, {"subject": "RE: Logou", "content": "> >    - the \"cookie standard\" has progressed quickly and\n> >supports all of the\n> >things that regular authentication should have supported a\n> >long time ago:\n> >\n> >        - passing a \"server key\" to be added to the digest,\n> >which can then\n> >be expired on the server\n> >        - setting expiration times\n>\n> An issue with IE is the fact that passwords with forms are still stored.\n\nCan be overcome/forbidden with HTML code.\n\nThe big problem with http + Basic auth is the static authentication\ninformation is sent cleartext. Easy replay. I've outlined a\napplication-based solution in Bugtraq. Basically:\n\n- login to https://server/login/path\n- get at least two cookies:\n  insecure \"ticket\" contains server-generated unique value that\n    can be used to lookup user info when requesting protected pages\n  secure ticket \"check\", only to login server, only to login path\n  [if you serve https content, an \"httpsticket\" as well]\n- central repository matches \"ticket\" to user, various stateful info\n- at logout, the server marks the ticket as invalid; no replay\n- periodically, content server can redirect user back to\n  https://server/login/path/verify?sendbackto=http://content/foo.html\n  - /login/path/verify checks for presence of both \"ticket\" and \"check\";\n    a net-sniffing bandit will lack \"check\"; at that point the \"ticket\"\n    is marked in the repository as 'tainted'; when the good user requests\n    another document, that user is warned about the taint, required to\n    log in again\n\nThe login system can be any system you prefer. user/pass, hardware token,\nclient certificate, whatever. The key is\n - attackers can't see the authentication data\n - they can't replay the insecure cookie, not for long, anyway\n\n> Next, what if an adversary finds a way to use his cookie (obtaining it\n> using spoofing or a man-in-the-middle attack (proxies are suitable for\n> this)), the adversary can use that cookie on the victim's behalf.\n\nSee above.\n\nAs for webmail, I've also suggested ways to protect authentication tickets\nfrom hostile email scripting by using one-time tickets for documents\ncontaining untrusted code, e.g. mail reading frames.\n\n> The next version of HTTP should have better security functions than\n> currently provides. The current standard doesn't satisfy the requirements\n> of many servers, today.\n\nThis is the place to make suggestions...\n\n...IMO, with a little creativity the current infrastructure suffices; or,\nat least it can't be significantly improved upon.\n\n-Peter\n\n\n\n", "id": "lists-012-16152937"}, {"subject": "RE: HTTP features w/ low 'implemented' and 'tested", "content": ">Concerning the low implemented list:\n>\n>    H 10.1.2 101 Switching Protocols\n>    H 14.42  Upgrade\n>\n>I'm not sure what there is to implement here... If any server does\n>implement these, then what do they accept in the Upgrade header?\n\n> Personally, I find it incredibly shortsighted for the IETF not to\n> include a special exception for optional features intended to support\n> future protocol extensions and/or replacement.  I'd hate to have to\n> exclude good engineering practice just to call it a draft standard.\n\nSurely we can find two different people to try to implement\nUpgrade and Switching Protocols and test their implementations\nagainst each other.\n\nIf I write a client that attempts: \n\n Upgrade: HTCPCP/0.0104\n\ncan we get someone to create a server that returns 101 Switching Protocols\ninstead of 419?\n\nLarry\n\n\n\n", "id": "lists-012-1615592"}, {"subject": "RE: Logou", "content": ">-----Original Message-----\n>From: Peter W [mailto:peterw@usa.net]\n>Sent: Saturday, 6 January 2001 23:33\n>To: Joris Dobbelsteen\n>Cc: WWW WG (E-mail)\n>Subject: RE: Logout\n>\n>\n>> >    - the \"cookie standard\" has progressed quickly and\n>> >supports all of the\n>> >things that regular authentication should have supported a\n>> >long time ago:\n>> >\n>> >        - passing a \"server key\" to be added to the digest,\n>> >which can then\n>> >be expired on the server\n>> >        - setting expiration times\n>>\n>> An issue with IE is the fact that passwords with forms are \n>still stored.\n>\n>Can be overcome/forbidden with HTML code.\n>\n>The big problem with http + Basic auth is the static authentication\n>information is sent cleartext. Easy replay. I've outlined a\n>application-based solution in Bugtraq. Basically:\n>\nBasic is completely insecure. Digest has some security hazards:\nServer sends a 'key' to use with hashing. When the same 'key' is used,\nthe hashed password captured can be reused.\nAlso doesn't digest authentication (nor basic authentication) provide\ndata integrity.\n\n>- login to https://server/login/path\n>- get at least two cookies:\n>  insecure \"ticket\" contains server-generated unique value that\n>    can be used to lookup user info when requesting protected pages\n>  secure ticket \"check\", only to login server, only to login path\n>  [if you serve https content, an \"httpsticket\" as well]\n>- central repository matches \"ticket\" to user, various stateful info\n>- at logout, the server marks the ticket as invalid; no replay\n>- periodically, content server can redirect user back to\n>  https://server/login/path/verify?sendbackto=http://content/foo.html\n>  - /login/path/verify checks for presence of both \"ticket\" \n>and \"check\";\n>    a net-sniffing bandit will lack \"check\"; at that point the \"ticket\"\n>    is marked in the repository as 'tainted'; when the good \n>user requests\n>    another document, that user is warned about the taint, required to\n>    log in again\n>\nYou get one 'public' ticket for use over HTTP, and one 'secret' that is\nused only over the HTTPS for validation of the originator of the\nrequest. That is what I understand....\nHTTPS can be used to rotate 'public' ticket (not really public, but\nanyone can read it anyway once transmitted over HTTP).\n\nSeems securely enough to me for general purposes, however there are\nstill some real vulnerabilities...\nThe 'public' ticket CAN be used, in some occasions, for a (limited)\ntime.\n\nNext there is no garantee of data integrity. This part I'm missing. I\ncan hijack a connection and alter it's (unsecured) data, thus sending\ndamaging messages on your behalf (or someone else).\n\nAs for webmail, when this security is implemented on this pad, later on\nI can still alter the data, when send over the SMTP 'network'. So in\nthis case, it's not a real requirement, but when data is send using\nSMTP, probably too many messages are send for the adversery to care\nabout them all.\n\nI would prefer a scenario of an employee downloading and uploading data\nfrom his company, that doesn't require data confidence (there's no data\nconfidence when using HTTP).\n\n>The login system can be any system you prefer. user/pass, \n>hardware token,\n>client certificate, whatever. The key is\n> - attackers can't see the authentication data\n> - they can't replay the insecure cookie, not for long, anyway\n>\n>> Next, what if an adversary finds a way to use his cookie \n>(obtaining it\n>> using spoofing or a man-in-the-middle attack (proxies are \n>suitable for\n>> this)), the adversary can use that cookie on the victim's behalf.\n>\n>See above.\n\nThink I got it. But still vulnerable, see above...\n>\n>As for webmail, I've also suggested ways to protect \n>authentication tickets\n>from hostile email scripting by using one-time tickets for documents\n>containing untrusted code, e.g. mail reading frames.\n>\n>> The next version of HTTP should have better security functions than\n>> currently provides. The current standard doesn't satisfy the \n>requirements\n>> of many servers, today.\n>\n>This is the place to make suggestions...\n>\n>...IMO, with a little creativity the current infrastructure \n>suffices; or,\n>at least it can't be significantly improved upon.\n>\n>-Peter\n>\n>\n\nThe obvious thing that is still bad is:\n* lack of data integrity validation\n* possibility to use a 'ticket' for a (limited) time\n\nThe all-in-one solution for this problem is this (maybe can be proposed\nas a security extension to HTTP):\n[Have a good time reading - 20 kB mail message]\n\n\n===\nHope I didn't make too much typo errors. and the goal/purpose is\nclear...\nIf you find the word \"of\" and you think it should not appear here, try\nwhether \"or\" fits better. The Dutch \"of\" is the English \"or\". I tend to\nmix these up sometimes (why I don't know).\n===\n\n\nIt relies fully on public-key encryption and looks simuliar to S/MIME.\nNote that also proxies should be aware of the use of this feature. Maybe\nthey can be told using cache-control directive not to manipulate the\nmessage?\nI suppose \"Cache-control: no-transform\" can do the job, but fearing many\nproxies will not obey to the directive.\n\nIn the case that data confidence is required, using HTTPS is required.\n\n*** REQUIREMENTS\nSERVER\n* Valid certificate to ensure data integrity and orgin authentication.\n* Optional: Hardware hashing and hash signing support, to improve\nperformance dramatically.\nCLIENT\n/ None\nPROXY/GATEWAY\n/ None\n\n\n*** PART ONE: LOGON\nThe client should generate a key-pair: private key with according public\nkey. Next the client should be aware of the user's authentication\ncredentials (user/pass), and should not save these (without explicitly\nasking the user). A HTTP connection can be used to ask the sever for a\nSessionID, and at the same time send the user's authentication\ncredentials and the public key. The client can purpose a lifetime of the\nsession and for the key. Also the algorithms used must be agreed on by\nthe client and server. In case of unsecured transmission (HTTP, not for\nHTTPS), the HTTP message should be signed.\nIn case of the correct credentials are used, and the server agrees on\nthe methods/protocols used, the server sends his certificate (with\npublic key) and a session ID.\n\nIn reality this can be done over HTTPS, using Digest authentication by\npreference.\nThe client connects to a HTTP server using HTTPS. Next it sends it's\nauthentication credentials, with the password hashed (standard digest\nauthentication), but also includes the public key. No need to sign the\nmessage (already secure channel).\nAssuming the correct ID and server accepts methods used...\nThe server responds with a a SessionID, and a certificate. Also the\nserver specifies where the SessionID should/can be used and gives\nservernames with the according certificates, or a group that uses the\nsame certificate. Also expiration of the sessionID and maybe the key is\nincluded (when using 1024-bit or larger keys, this is not required).\nMesssage doesn't need be signed, because the connection is secure.\n\n\nIf the user has a certificate already, this can be used, instead of the\ncustom-generated key-pair. The entire certificate should be send in this\ncase, instead of only the public key.\nWith the appropiate methods, HTTPS will not be needed. This sample used\nthe old authentication methods in combination with the new features\nprovided by this security extension.\n\nIt is possible to use certificate authentication together with the\nuser/password authentication, but allowing a higher level of access when\nusing certificate authentication.\n\nThe server must send the certificate to identify itself, as it has no\nuser/password authentication possibility, but the certificate alone will\nbe enough for the job.\n\nNote that the rule (for certificates) is that the private key NEVER\nleaves the local computer, but only the public key. When I say, send the\ncertificate, only the certificate with the public key and withOUT the\nprivate key. This applies through all this specification. No confusion\non this...\n\n\n*** PART TWO: HTTP MESSAGE SIGNING\n\nOn unsecure channels, we sign the HTTP message.\nWe need to sign data that is affected, this includes the Request/Reponse\nline and non-modifiable headers. The headers that can be signed are\noutlined in section 13.5.2 (Non-modifiable Headers). Also the header\nincluding the sessionID should be signed.\n\nWe need a hash of the request/response line, headers and, of course, the\ncontent (data). Next the hash should be signed with the private key. The\npublic key cannot be used for signing, because the adversery also knows\nthe public key.\n\nThis means the remote side can check the data intigrity. The SessionID\nwill be used to check the user credentials.\n\n\n*** PART THREE: LOGOFF\nLogoff occurs quire simply by expiration of the SessionID or sending an\ninstruction to the server to logoff. The server can also instruct the\nclient to logoff.\nLogoff includes the expiration of the SessionID and erasing the key-pair\non the client. Most important is the expiration of the SessionID to\nsucceed on the server-side.\nThis means LOGOFF IS REQUIRED using either expiration or user-actived\nlogoff!!!\n\n\n*** PART FOUR: EXTENDING SESSION LIFETIME\nResons for ending a session without intension of the user is expiration\nof the SessionID. Before extending the session, the user should be aware\nof it, by asking him/her/it first. Closing the browser should retire the\nSessionID and key-pairs and preferably, the server should send a message\nto retire the session.\n\nA signed message with this request can be issued to the (original)\nauthentication server in order to extend the session for a specific time\n(maximum limit by the server). The server may refuse. Adding user\ncredentials again is not needed but may be requested by the server. Also\nkey rotation or sessionID rotation may be requested/done by the server.\n\n\n*** PART FIVE: WEAKNESSES/DISADVANTAGES OF THE PROTOCOL\nThe protocol requires on the strength of the security protocols used.\nKnown-plaintext attacks are possible, and security protocols used should\nbe designed to be properly (heavily) protected against this type of\nattack. Such attacks can result fast into many known-plaintexts with\nthere accompanying ciphertext.\n\nServers require significanly more performance to do the job.\n\nKey- and session-management is required and probably will get more\ncomplex.\n\nRequires browser and server support.\n\nExpire-times should be relatively short (couple minutes), to improve\nsecurity. Long expire-times give the adversery a good chance to do\ndamage. This is because delivery is still not ensured, and thus\nretirement of sessions by the user is not garanteed to happen (you can\neasily filter such messages out).\n\n\n*** PART SIX: FEATURES OF THE PROTOCOL\nProvides orgin authentication\n\nProvides data integrity\n\nExtended security over existing methods, by eliminating some severe\nweaknesses that exist in the cookies-implementation.\n\nLong sessionkeys (256-bits would probably allow you to have almost\ncertainly unique IDs)\n\nA (e.g. authentication) server can have a different certificate that the\nother servers.\n\n\n*** PART SEVEN: WORK TO DO\n- How on chunked transfers\n- How to sign headers and the request/response line.\n  These probably change due to proxies, gateways, firewalls and such\nthings.\n- .....\n\n\n\nThat was the mail for today...\nHope you like it.\n\nIf you have some other suggestions, send them...\nmaybe some protocol errors, but it probably needs to be reviewed, when\nwe want to implement such functionality.\nMaybe I forget thinks where, I did or didn't think about...\n\n\n\n- Joris\n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-16162964"}, {"subject": "RE: Logou", "content": "A stupid mistake made in the previous mail...\n\nHashing and signing the hash is always required, because it is an\nintergral part of the orgin authentication (and data integrity)\nvalidation. Corrected in this mail (I really hope)...\n\n==============================\n\nThe obvious thing that is still bad is:\n* lack of data integrity validation\n* possibility to use a 'ticket' for a (limited) time\n\nThe all-in-one solution for this problem is this (maybe can be proposed\nas a security extension to HTTP):\n[Have a good time reading - 20 kB mail message]\n\n\n===\nHope I didn't make too much typo errors. and the goal/purpose is\nclear...\nIf you find the word \"of\" and you think it should not appear here, try\nwhether \"or\" fits better. The Dutch \"of\" is the English \"or\". I tend to\nmix these up sometimes (why I don't know).\n===\n\n\nIt relies fully on public-key encryption and looks simuliar to S/MIME.\nNote that also proxies should be aware of the use of this feature. Maybe\nthey can be told using cache-control directive not to manipulate the\nmessage?\nI suppose \"Cache-control: no-transform\" can do the job, but fearing many\nproxies will not obey to the directive.\n\nIn the case that data confidence is required, using HTTPS is required.\n\n*** REQUIREMENTS\nSERVER\n* Valid certificate to ensure data integrity and orgin authentication.\n* Optional: Hardware hashing and hash signing support, to improve\nperformance dramatically.\nCLIENT\n/ None\nPROXY/GATEWAY\n/ None\n\n\n*** PART ONE: LOGON\nThe client should generate a key-pair: private key with according public\nkey. Next the client should be aware of the user's authentication\ncredentials (user/pass), and should not save these (without explicitly\nasking the user). A HTTP connection can be used to ask the sever for a\nSessionID, and at the same time send the user's authentication\ncredentials and the public key. The client can purpose a lifetime of the\nsession and for the key. Also the algorithms used must be agreed on by\nthe client and server. Next the message must be signed, as it provides a\nintegral part of the orgin authentication and data integrity check.\nOn a secure channel, in a logon message ONLY, it's not a direct\nrequirement that the message is signed\n\nIn case of the correct credentials are used, and the server agrees on\nthe methods/protocols used, the server sends his certificate (with\npublic key) and a session ID.\n\nIn reality this can be done over HTTPS, using Digest authentication by\npreference.\nThe client connects to a HTTPS server. Next it sends it's authentication\ncredentials, with the password hashed (standard digest authentication),\nbut also includes the public key. No need to sign the message (already\nsecure channel, note that this is for a logon message only).\nAssuming the correct ID and server accepts methods used...\nThe server responds with a a SessionID, and a certificate. Also the\nserver specifies where the SessionID should/can be used and gives\nservernames with the according certificates, or a group that uses the\nsame certificate. Also expiration of the sessionID and maybe the key is\nincluded (when using 1024-bit or larger keys, this is not required).\nMesssage doesn't need be signed, because the connection is secure.\n\n\nIf the user has a certificate already, this can be used, instead of the\ncustom-generated key-pair. The entire certificate should be send in this\ncase, instead of only the public key.\nWith the appropiate methods, HTTPS will not be needed. This sample used\nthe old authentication methods in combination with the new features\nprovided by this security extension.\n\nIt is possible to use certificate authentication together with the\nuser/password authentication, but allowing a higher level of access when\nusing certificate authentication.\n\nThe server must send the certificate to identify itself, as it has no\nuser/password authentication possibility, but the certificate alone will\nbe enough for the job.\n\nNote that the rule (for certificates) is that the private key NEVER\nleaves the local computer, but only the public key. When I say, send the\ncertificate, only the certificate with the public key and withOUT the\nprivate key. This applies through all this specification. No confusion\non this...\n\n\n*** PART TWO: HTTP MESSAGE SIGNING\n\nOn unsecure and SECURE channels, we sign the HTTP(S) message.\nWe need to sign data that can be affected by an adversery in a way that\nit will provide a security hole, this includes the Request/Reponse line\nand non-modifiable headers. The headers that can be signed are outlined\nin section 13.5.2 (Non-modifiable Headers). Also the header including\nthe sessionID should be signed.\n\nWe need a hash of the request/response line, headers and, of course, the\ncontent (data). Next the hash should be signed with the private key. The\npublic key cannot be used for signing, because the adversery also knows\nthe public key.\n\nThis means the remote side can check the data intigrity. The SessionID\nwill be used to check the user credentials.\n\nNote that signing is an important operation, as it provides orgin\nauthentication and data integrity.\n\n\n*** PART THREE: LOGOFF\nLogoff occurs quire simply by expiration of the SessionID or sending an\ninstruction to the server to logoff. The server can also instruct the\nclient to logoff.\nLogoff includes the expiration of the SessionID and erasing the key-pair\non the client. Most important is the expiration of the SessionID to\nsucceed on the server-side.\nThis means LOGOFF IS REQUIRED using either expiration or user-actived\nlogoff!!!\nLogoff messages must be signed.\n\n\n*** PART FOUR: EXTENDING SESSION LIFETIME\nResons for ending a session without intension of the user is expiration\nof the SessionID. Before extending the session, the user should be aware\nof it, by asking him/her/it first. Closing the browser should retire the\nSessionID and key-pairs and preferably, the server should send a message\nto retire the session.\n\nA signed message with this request can be issued to the (original)\nauthentication server in order to extend the session for a specific time\n(maximum limit by the server). The server may refuse. Adding user\ncredentials again is not needed but may be requested by the server. Also\nkey rotation or sessionID rotation may be requested/done by the server.\n\n\n*** PART FIVE: WEAKNESSES/DISADVANTAGES OF THE PROTOCOL\nThe protocol requires on the strength of the security protocols used.\nKnown-plaintext attacks are possible, and security protocols used should\nbe designed to be properly (heavily) protected against this type of\nattack. Such attacks can result fast into many known-plaintexts with\nthere accompanying ciphertext.\n\nServers require significanly more performance to do the job.\n\nKey- and session-management is required and probably will get more\ncomplex.\n\nRequires browser and server support.\n\nExpire-times should be relatively short (couple minutes), to improve\nsecurity. Long expire-times give the adversery a good chance to do\ndamage. This is because delivery is still not ensured, and thus\nretirement of sessions by the user is not garanteed to happen (you can\neasily filter such messages out).\n\n\n*** PART SIX: FEATURES OF THE PROTOCOL\nProvides orgin authentication\n\nProvides data integrity\n\nExtended security over existing methods, by eliminating some severe\nweaknesses that exist in the cookies-implementation.\n\nLong sessionkeys (256-bits would probably allow you to have almost\ncertainly unique IDs)\n\nA (e.g. authentication) server can have a different certificate that the\nother servers.\n\n\n*** PART SEVEN: WORK TO DO\n- How on chunked transfers\n- How to sign headers and the request/response line.\n  These probably change due to proxies, gateways, firewalls and such\nthings.\n- .....\n\n\n*** PART EIGTH: HACKING ATTEMPTS\nIf a false message was detected that was incorrectly signed, the server\nshould notify the actual owner of the session of this attempt. It's\nrecommended to retire the session. On the other hand, the session can\nprobably continue to exist, because this security is more difficult to\nbreak, than with the cookies.\n\n\n\nThat was the mail for today...\nHope you like it.\n\nIf you have some other suggestions, send them...\nmaybe some protocol errors, but it probably needs to be reviewed, when\nwe want to implement such functionality.\nMaybe I forget thinks where, I did or didn't think about...\n\n\n\n- Joris\n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-16182039"}, {"subject": "RE: Logou", "content": ">-----Original Message-----\n>From: Scott Lawrence [mailto:slawrence@virata.com]\n>Sent: Monday, 8 January 2001 18:33\n>To: Joris Dobbelsteen\n>Cc: WWW WG (E-mail)\n>Subject: Re: Logout\n>\n>\n>Joris Dobbelsteen wrote:\n>\n>\n>> Basic is completely insecure. Digest has some security hazards:\n>> Server sends a 'key' to use with hashing. When the same \n>'key' is used,\n>> the hashed password captured can be reused.\n>> Also doesn't digest authentication (nor basic authentication) provide\n>> data integrity.\n>\n>Actually, the Digest spec provides a content integrity mechanism \n>(qop=auth-int).  It does not protect most of the header information \n>(because of compatibility problems with proxies), but does protect \n>and authenticate the message body by including a hash of the message \n>body as an input to the response hash.\n>\nWasn't aware of the hash included of the message body.\n\n>As for alternative schemes that provide better security without \n>SSL/TLS, there was a very good spec \"The Secure HyperText Transfer \n>Protocol\" that just didn't get any traction with implementors:\n>\n>http://www.ietf.org/rfc/rfc2660.txt\n>\n>\nI will read RFC2660....\n\n\n- Joris\n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-16197169"}, {"subject": "Re: Response to &quot;On the use of HTTP as a Substrate for Other  Protocols&quot", "content": "See..\n\n  http://lists.w3.org/Archives/Public/xml-dist-app/2000Dec/0061.html\n\n..for a response (by individuals participating in the W3C XML Protocol work) \nto the IETF-wide Last Call of..\n\n  ftp://ftp.ietf.org/internet-drafts/draft-moore-using-http-01.txt\n\nThe response was sent directly to the iesg, as indicated here..\n\n  http://lists.w3.org/Archives/Public/xml-dist-app/2000Dec/0178.html\n\nI'm forwarding it to this list in case there's innarested folks here that were \notherwise unaware of it.\n\nJeffH\n\n\n\n", "id": "lists-012-16205702"}, {"subject": "can charsets be quoted", "content": "Is this legal:\n\n    Content-Type: text/html; charset=\"iso-8859-1\"\n\nSpecifically are the double quotes around the charset value\nlegal?  I assume the intent is that they are, but I believe\nthe spec as written doesn't allow for them.  I know others\n(like WebDAV) assume you can use double quotes, and I know\nit's legal in MIME (see below)\n\nIn RFC 2616 14.17 Content-Type refers you to 3.7 on media\ntypes.  3.7 defines media-type as:\n\n       media-type     = type \"/\" subtype *( \";\" parameter )\n\nand refers you to 3.6 to define parameter.  3.6 says:\n\n   Parameters are in  the form of attribute/value pairs.\n\n       parameter               = attribute \"=\" value\n       attribute               = token\n       value                   = token | quoted-string\n\nso the values can be a token or a quoted-string, great, it\nseems that charset values can be quoted.  BUT the last\nparagraph of 3.7.1 says:\n\n   The \"charset\" parameter is used with some media types to define the\n   character set (section 3.4) of the data. When no explicit charset\n   parameter is provided by the sender, media subtypes of the \"text\"\n   type are defined to have a default charset value of \"ISO-8859-1\" when\n   received via HTTP. Data in character sets other than \"ISO-8859-1\" or\n   its subsets MUST be labeled with an appropriate charset value. See\n   section 3.4.1 for compatibility problems.\n\nSpecifically referring us to section 3.4 for the definition\nof the charset parameter.  3.4 defines charset as:\n\n   HTTP character sets are identified by case-insensitive tokens. The\n   complete set of tokens is defined by the IANA Character Set registry\n   [19].\n\n       charset = token\n\nAnd \"token\" doesn't allow quotes.  Shouldn't this be:\n\n       charset = token | quoted-string\n\nor else, doesn't the spec disallow quotes around charset\nvalues?  Or should section 3.4 not offer a BNF for charset\nat all in which case it would be clear that it's just\nanother parameter and therefore the value is token or\nquoted-string?  Or, at least, section 3.4 should say that\nthis BNF is semantic and that quotes around token are used\nto delimit the parameter (see below).  \n\nIf you're trying to figure out what the spec says for\ncharset values, and you turn to section 3.4 since it defines\ncharsets, in it's current form, you get a very different\nnotion of what's allowed then I think is intended.\n\nHoward\n\n\nMIME's view of things, as best as I can find, is RFC 2045 section 5.1:\n\n   Note that the value of a quoted string parameter does not include the\n   quotes.  That is, the quotation marks in a quoted-string are not a\n   part of the value of the parameter, but are merely used to delimit\n   that parameter value.  In addition, comments are allowed in\n   accordance with RFC 822 rules for structured header fields.  Thus the\n   following two forms\n\n     Content-type: text/plain; charset=us-ascii (Plain text)\n\n     Content-type: text/plain; charset=\"us-ascii\"\n\n   are completely equivalent.\n\n\n\n", "id": "lists-012-16213744"}, {"subject": "RE: can charsets be quoted", "content": "Interresting problem\n\n>-----Original Message-----\n>From: Melman, Howard [mailto:Howard@silverstream.com]\n>Sent: Wednesday, 07 February 2001 17:46\n>To: HTTP Working Group\n>Subject: can charsets be quoted.\n>\n>\n>\n>Is this legal:\n>\n>    Content-Type: text/html; charset=\"iso-8859-1\"\n>\n>Specifically are the double quotes around the charset value\n>legal?  I assume the intent is that they are, but I believe\n>the spec as written doesn't allow for them.  I know others\n>(like WebDAV) assume you can use double quotes, and I know\n>it's legal in MIME (see below)\n>\n>In RFC 2616 14.17 Content-Type refers you to 3.7 on media\n>types.  3.7 defines media-type as:\n>\n>       media-type     = type \"/\" subtype *( \";\" parameter )\n>\n>and refers you to 3.6 to define parameter.  3.6 says:\n>\n>   Parameters are in  the form of attribute/value pairs.\n>\n>       parameter               = attribute \"=\" value\n>       attribute               = token\n>       value                   = token | quoted-string\n>\n\nTill here it seems to be all right....\n\n\n>so the values can be a token or a quoted-string, great, it\n>seems that charset values can be quoted.  BUT the last\n>paragraph of 3.7.1 says:\n>\n>   The \"charset\" parameter is used with some media types to define the\n>   character set (section 3.4) of the data. When no explicit charset\n>   parameter is provided by the sender, media subtypes of the \"text\"\n>   type are defined to have a default charset value of\n>\"ISO-8859-1\" when\n>   received via HTTP. Data in character sets other than \"ISO-8859-1\" or\n>   its subsets MUST be labeled with an appropriate charset value. See\n>   section 3.4.1 for compatibility problems.\n>\n>Specifically referring us to section 3.4 for the definition\n>of the charset parameter.  3.4 defines charset as:\n>\n>   HTTP character sets are identified by case-insensitive tokens. The\n>   complete set of tokens is defined by the IANA Character Set registry\n>   [19].\n>\n>       charset = token\n>\n>And \"token\" doesn't allow quotes.  Shouldn't this be:\n>\n>       charset = token | quoted-string\n\nWell, it doesn't point explicitly to the value, thus:\n  value = charset | token | quoted-string\n\nSomething like this would then have been in the spec\n\nI expect is to be all right what you do.\n\n>\n>or else, doesn't the spec disallow quotes around charset\n>values?  Or should section 3.4 not offer a BNF for charset\n>at all in which case it would be clear that it's just\n>another parameter and therefore the value is token or\n>quoted-string?  Or, at least, section 3.4 should say that\n>this BNF is semantic and that quotes around token are used\n>to delimit the parameter (see below).\n>\n>If you're trying to figure out what the spec says for\n>charset values, and you turn to section 3.4 since it defines\n>charsets, in it's current form, you get a very different\n>notion of what's allowed then I think is intended.\n>\n>Howard\n>\n>\n>MIME's view of things, as best as I can find, is RFC 2045 section 5.1:\n>\n>   Note that the value of a quoted string parameter does not\n>include the\n>   quotes.  That is, the quotation marks in a quoted-string are not a\n>   part of the value of the parameter, but are merely used to delimit\n>   that parameter value.  In addition, comments are allowed in\n>   accordance with RFC 822 rules for structured header fields.\n> Thus the\n>   following two forms\n>\n>     Content-type: text/plain; charset=us-ascii (Plain text)\n>\n>     Content-type: text/plain; charset=\"us-ascii\"\n>\n>   are completely equivalent.\n>\n\nHTTP has much of it's design from MIME, probably you can use the\nquoted-string, and it's compliant withe spec.\n\nHowever, I don't know if client implementation support it, but I expect they\nwill, through I'm not sure, nor have any possibility to test this. This is\nactually the issue with things like this.\n\nThe only server I found: HEAD http://www.freebsd.com/ HTTP/1.1\nreturned the value of the parameter \"charset\" without quotes.\nI would recommend to simply not use them, just in case...\n\n\n\n- Joris\n\n\n\n", "id": "lists-012-16223830"}, {"subject": "RE: can charsets be quoted", "content": "On Wednesday Feb 7, 2001, Paul Leach wrote:\n\n> I believe that the word token is being used two different ways.\n> This way:\n> value                   = token | quoted-string\n> and in this sentence:\n> HTTP character sets are identified by case-insensitive tokens.\n> \n> The first one is a formal ABNF definition, the second is not. I.e.,\n> there was no intent to say that char-set IDs have to be \"tokens\" as that\n> is specified in the HTTP ABNF.\n> \n> I would say that it is perfectly legal for them to be quoted.\n\nI agree.  But I think the spec could be clearer in two ways.\nI think the ABNF should be removed from 3.4 (since it's not\nintended) and I think the word \"token\" should be replaced\nwith the word \"name\" in all cases in section 3.4.  The IANA\ndoes not refer to charset tokens, but rather charset names.\nSee RFC 1700 or\nhttp://www.isi.edu/in-notes/iana/assignments/character-sets\n\nHoward\n\n\n\n", "id": "lists-012-16235689"}, {"subject": "RE: can charsets be quoted", "content": "Hopefully this will get on the \"errata\" page...\n\n> -----Original Message-----\n> From: Melman, Howard [mailto:Howard@silverstream.com]\n> Sent: Thursday, February 08, 2001 10:49 AM\n> To: HTTP WG\n> Cc: Joris Dobbelsteen; Paul Leach; Melman, Howard\n> Subject: RE: can charsets be quoted.\n> \n> \n> \n> On Wednesday Feb 7, 2001, Paul Leach wrote:\n> \n> > I believe that the word token is being used two different ways.\n> > This way:\n> > value                   = token | quoted-string\n> > and in this sentence:\n> > HTTP character sets are identified by case-insensitive tokens.\n> > \n> > The first one is a formal ABNF definition, the second is not. I.e.,\n> > there was no intent to say that char-set IDs have to be \"tokens\" as that\n> > is specified in the HTTP ABNF.\n> > \n> > I would say that it is perfectly legal for them to be quoted.\n> \n> I agree.  But I think the spec could be clearer in two ways.\n> I think the ABNF should be removed from 3.4 (since it's not\n> intended) and I think the word \"token\" should be replaced\n> with the word \"name\" in all cases in section 3.4.  The IANA\n> does not refer to charset tokens, but rather charset names.\n> See RFC 1700 or\n> http://www.isi.edu/in-notes/iana/assignments/character-sets\n> \n> Howard\n> \n\n\n\n", "id": "lists-012-16244749"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "> >Concerning the low implemented list:\n> >\n> >    H 10.1.2 101 Switching Protocols\n> >    H 14.42  Upgrade\n> >\n> >I'm not sure what there is to implement here... If any server does\n> >implement these, then what do they accept in the Upgrade header?\n\n> Surely we can find two different people to try to implement\n> Upgrade and Switching Protocols and test their implementations\n> against each other.\n> \n> If I write a client that attempts: \n> \n>  Upgrade: HTCPCP/0.0104\n> \n> can we get someone to create a server that returns 101 Switching Protocols\n> instead of 419?\n\nI'll volunteer a client implementation if a server implementor tells me\nwhat to send.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1624657"}, {"subject": "Re: can charsets be quoted", "content": "> Hopefully this will get on the \"errata\" page...\n\nWhy?  The spec is correct.  It takes a great deal of imagination\nto believe that the use of the word token in the text should somehow imply\nthat the HTTP syntax excludes a quoted-string.  Any token can appear inside\na quoted string.\n\n....Roy\n\n\n\n", "id": "lists-012-16254386"}, {"subject": "Re: can charsets be quoted", "content": "On Friday Feb 9, 2001, Roy T. Fielding wrote:\n\n> > Hopefully this will get on the \"errata\" page...\n> \n> Why?  The spec is correct.  It takes a great deal of imagination\n> to believe that the use of the word token in the text should somehow imply\n> that the HTTP syntax excludes a quoted-string.  Any token can appear inside\n> a quoted string.\n\nPerhaps, but it's not just the word \"token\" in text.  There\nseems to be an ABNF rule in the section which as near as I\ncan tell adds no value to the description and does add\nconfusion.  Below is the text of section 3.4:\n\nHoward\n\n================================================================\n3.4 Character Sets\n\n   HTTP uses the same definition of the term \"character set\" as that\n   described for MIME:\n\n   The term \"character set\" is used in this document to refer to a\n   method used with one or more tables to convert a sequence of octets\n   into a sequence of characters. Note that unconditional conversion in\n   the other direction is not required, in that not all characters may\n   be available in a given character set and a character set may provide\n   more than one sequence of octets to represent a particular character.\n   This definition is intended to allow various kinds of character\n   encoding, from simple single-table mappings such as US-ASCII to\n   complex table switching methods such as those that use ISO-2022's\n   techniques. However, the definition associated with a MIME character\n   set name MUST fully specify the mapping to be performed from octets\n   to characters. In particular, use of external profiling information\n   to determine the exact mapping is not permitted.\n\n      Note: This use of the term \"character set\" is more commonly\n      referred to as a \"character encoding.\" However, since HTTP and\n      MIME share the same registry, it is important that the terminology\n      also be shared.\n\n   HTTP character sets are identified by case-insensitive tokens. The\n   complete set of tokens is defined by the IANA Character Set registry\n   [19].\n\n       charset = token\n\n   Although HTTP allows an arbitrary token to be used as a charset\n   value, any token that has a predefined value within the IANA\n   Character Set registry [19] MUST represent the character set defined\n   by that registry. Applications SHOULD limit their use of character\n   sets to those defined by the IANA registry.\n\n   Implementors should be aware of IETF character set requirements [38]\n   [41].\n\n\n\n", "id": "lists-012-16262758"}, {"subject": "Force chunked transferencoding", "content": "Is there a way, issuing an HTTP 1.1 GET request to\nforce the response to be chunked, and also to ensure a\na maximum size for each chunk?\n\n__fred\n\n\n\n__________________________________________________\nDo You Yahoo!?\nGet personalized email addresses from Yahoo! Mail - only $35 \na year!  http://personal.mail.yahoo.com/\n\n\n\n", "id": "lists-012-16273840"}, {"subject": "Re: Force chunked transferencoding", "content": "fred doh wrote:\n\n> Is there a way, issuing an HTTP 1.1 GET request to\n> force the response to be chunked, and also to ensure a\n> a maximum size for each chunk?\n\nNo.  Even if 1.1 has such a mechanism, you might be talking to a\n1.0 server.\n\n--\n/================================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.  |\n|Chief Scientist |===============================================|\n|eCal Corp.      |Don't anthropomorphize computers. We don't like|\n|francis@ecal.com|it.                                            |\n\\================================================================/\n\n\n\n", "id": "lists-012-16281203"}, {"subject": "Re: Force chunked transferencoding", "content": "You can use the TE request-header to indicate what transfer-codings you are\nwilling to accept.  However, there is no means to limit the chunk size.\n\n     -Carl\n\n\n\nfred doh <fred569us@yahoo.com> on 02/12/2001 01:42:24 PM\n\nTo:   http-wg@cuckoo.hpl.hp.com\ncc:\nSubject:  Force chunked transfer-encoding?\n\n\n\nIs there a way, issuing an HTTP 1.1 GET request to\nforce the response to be chunked, and also to ensure a\na maximum size for each chunk?\n\n__fred\n\n\n\n__________________________________________________\nDo You Yahoo!?\nGet personalized email addresses from Yahoo! Mail - only $35\na year!  http://personal.mail.yahoo.com/\n\n\n\n", "id": "lists-012-16288804"}, {"subject": "RE: can charsets be quoted", "content": "\"Although HTTP allows an arbitrary token to be used as a charset value\"\n\nIt would be useful to clarify that HTTP uses charset in two contexts:\nwithin an Accept-Charset request header (in which the charset value\nis an unquoted token) and as the value of a parameter in a Content-type\nheader (within a request or response), in which case the parameter\nvalue of the charset parameter may be quoted.\n\nLarry\n-- \nhttp://larry.masinter.net\n\n\n\n", "id": "lists-012-16297011"}, {"subject": "Comments on `Common User Agent Problems' documen", "content": "First off, thanks to the authors and the W3C for writing documents\nlike this!\n\nI have some comments on the current version of\nhttp://www.w3.org/TR/2001/NOTE-cuap-20010206 , mainly about the HTTP and  \nnegotiation parts.  Quoted text below is from this current version of the \nnote.\n\n\n>1.11 Allow the user to bookmark negotiated resources.\n\nThis text makes content negotiation in HTTP sound straightforward,\nwhich it is not.  Some more info for browser implementers who want to\nsupport this would be nice.\n\nThe text fails to mention that bookmarking a particular version of a\nnegotiated resource is not always possible under HTTP semantics, because     \na) the particular version may not have its own URI and b) even if it does,\nHTTP does not guarantee that the user agent will be informed of this.\n\nHTTP/1.1 defines the Content-Location header field as the way for the\nserver to indicate the URI of the variant, and some servers do supply\nthis Content-Location when negotiation took place most of the time.\nHowever, Content-Location is also used for some other things, and its\ninclusion in a response does not necessarily mean that content\nnegotiation took place.  Under plain HTTP/1.x the user agent will have\nto use heuristics to guess if negotiation took place.  RFC 2295\ndefines the `TCN' and `Alternates' header fields, the inclusion of\nthese in a response is a sure sign that content negotiation did take\nplace.\n\nRFC 2295 also specifies (section 11.2) that if a browser knows both\nthe generic URI and the variant URI, the defaults for showing the URI\nand bookmarking the URI should be that the generic, not the variant,\nURI is used.\n\n\n>1.12 Allow the user to choose among supported transfer encodings. \n\nI think that what you _want_ to say here is that user agents should\ntry to support as many time-saving transfer encoding mechanisms\n(compression, delta-encoding) as possible, and send out TE headers\nannouncing their support.\n\nUser control over this is unnecessary.  The HTTP/1.1 transfer encoding\nnegotiation mechanism was designed to avoid the need for the end user\nto get involved.  Using the HTTP protocol, the server, proxy, and\nclient implementations among themselves will be able to choose and use\nthe most efficient transfer encoding.  Some power users might have\nenough knowledge to fine-tune this process beyond what can be done\nautomatically, if their browsers provide an interface for this, but\nthat would be a very small group of users indeed.\n\n\n>1.13 Use the user interface language as the default value for language\n      negotiation. \n\n>In case the user does not specify any language, the user agent may use\n>the language of its user interface as the value sent out.\n\nThis is wrong: a user agent should never automatically configure\nitself to send out a single language value without asking for user\napproval, because in HTTP semantics sending a single language value\nonly may very easily block convenient access to content in all other\nlanguages.\n\nCase in point: On the Apache mailing list someone recently reported\nseeing problems due to user agents being preconfigured to send out\n\n Accept-language: xy\n\n(for some language xy).  In requests for a page with many language\nvariants, but not including xy, this (currently) causes Apache to drop\ninto a `pick a language' 406 error response.  This is the best option\nunder HTTP semantics but it confuses the user, who is not aware that\nthe user agent is preconfigured to send out this very restricted\naccept-language, who will therefore not have a clue on how to correct\nthis, and was just expecting to see the English variant if xy were not\navailable.\n\nSo, I propose that the document be changed to read:\n\nIn case the user does not specify any language, the user agent may\nspecify the language of its user interface as the preferred language,\nwhile allowing other languages with a lower preference, for example by\nsending\n\n Accept-Language: dk, *;q=0.5\n\n\n>3.4 Do not treat HTTP temporary redirects as permanent redirects. \n\n>The user should be able to bookmark, copy, or link to the original\n>(persistent) URI or the result of a temporary redirect.\n>\n>Wrong: User agents usually show the user (in the user interface) the\n>URI that is the result of a temporary (302 or 307) redirect, as they\n>would do for a permanent (301) redirect.\n\nI do not like this advice to depart from current practice in showing\nthe target URI of a 302 redirect.  Changing current practice creates\nusability problems for existing web content.  Here is why: server-side\nimage maps, the maps with HTML like this:\n\n <A HREF=\"http://x.com/cgi-bin/imagemap/worldmap\">\n <IMG SRC=\"worldmap.gif\" ISMAP></A>\n\ngenerate URLs like\nhttp://your.server.name/cgi-bin/imagemap/worldmap?444,33 when clicked\n(if I recall the syntax correctly).  With an image map the server side\nthere is a CGI script (or other component) that takes the coordinates\nand translates them to the correct URL (say http://x.com/canada/)\ncorresponding to the part of the image clicked.  In all\nimplementations I have ever seen, the script uses a *302 code*\nredirect (usually created through the Location: mechanism in the CGI\ninterface) to direct the user agent to http://x.com/canada/!!!\nFollowing your proposal would mean that the ?444,33 URL, not the\nCanada URL, is shown and bookmarked, whereas the Canada URL is both\nmore informative and generally more permanent!\n\nSo at least I would like the document to make an exception to the\nabove rule for 302 redirection where imagemaps are the source.  But\nactually I'd rather see the whole rule dropped and current common\nbehavior declared correct.  Based on experience with clarifying and\nevolving the redirection codes in the http-wg, I take the view that a\ndeparture from existing practice should only be done by adding fresh\ncodes, not by `fixing' the semantics of existing ones.\n\n\n>3.6 List only supported media types in an HTTP Accept header. \n\nSee also my comments to 1.13.  There is a similar problem here: in\nnon-inline image requests the user agent should generally *not* block\nthe receipt of unsupported types by specifying an overly restrictive\nAccept header.\n\nAccording to HTTP semantics, if the user agent is willing to process\nany type (e.g. by saving it, see your point 1.3) then the Accept\nheader should either be omitted entirely or include */*;q=X for some\nlow value X.  Omitting this */*;q=X if you are willing to save to disk\nas last resort is simply bad HTTP, though most server implementations\nwill usually let you get away with it.\n\nIn Apache the negotiation module has enough workarounds for bad Accept\nheaders already, so I would not like to to see new classes of bad\nAccept headers introduced.  Point 3.6 should be rewritten to talk\nabout inline image requests only, or extended to also cover the\nnecessary subtleties in non-image requests.\n\n\nKoen.\n\n\n\n", "id": "lists-012-16304064"}, {"subject": "RFC 2616 errata: overspecified restriction on automatic redirect", "content": "Sections 10.3.2 (301 Moved Permanently) contains the paragraph\n\n   If the 301 status code is received in response to a request other\n   than GET or HEAD, the user agent MUST NOT automatically redirect the\n   request unless it can be confirmed by the user, since this might\n   change the conditions under which the request was issued.\n\nwhich fails to consider that there are many other request methods\nthat are safe to automatically redirect, and further that the user agent\nis able to make that determination based on the request method semantics.\nIn particular, the OPTIONS method is always safe to automatically redirect.\nUnfortunately, the paragraph was written long before there was OPTIONS,\nand was never updated to reflect the extensibility of methods.  The\nsame problem paragraph is found in sections 10.3.3 and 10.3.8.\n\nThe above should be replaced with\n\n   If the 301 status code is received in response to a request method\n   that is known to be \"safe\", as defined in section 9.1.1, then the\n   request MAY be automatically redirected by the user agent without\n   confirmation.  Otherwise, the user agent MUST NOT automatically\n   redirect the request unless it is confirmed by the user, since the\n   new URI might change the conditions under which the request was issued.\n\nalong with similar changes for sections 10.3.3 and 10.3.8.\nIt would also be helpful for each of the method definition sections\nto specifically define whether or not the method is safe.\nOPTIONS, GET, and HEAD are all safe in RFC 2616.\nHTTP extensions like WebDAV define additional safe methods.\n\nThis change does not impact interoperability.\n\nCheers,\n\nRoy T. Fielding, Chief Scientist, eBuilt, Inc.\n                 2652 McGaw Avenue\n                 Irvine, CA 92614-5840  fax:+1.949.609.0001\n                 (fielding@ebuilt.com)  <http://www.eBuilt.com>\n\n                 Chairman, The Apache Software Foundation\n                 (fielding@apache.org)  <http://www.apache.org/>\n\n\n\n", "id": "lists-012-16318719"}, {"subject": "HTTP client abort detection ..", "content": "I've been trying (unsuccessfully) to decide whether or not\nRFC 2616 permits a server to interpret the receipt of a FIN on\nthe incoming-request side of it's connection to a client as a \nfull client close ... allowing it to immediately stop any\nrequest processing, and possibly avoid a RST due to the arrival \nof unwanted response data at the client.\n\nUnfortunately I can't convice myself that this is safe. I can't\nsee anything in the spec which says a client isn't allowed to\nsend a request, then shutdown(fd, 1), and then still expect the\nserver to continue request processing and send a response.\n\nIs there lore on this? Are clients within their rights behaving\nin this way? Do any real clients behave this way? Are there any\nservers out there which aggressively monitor for client closes\nand abort connections in the way I described above?\n\nOpinions most welcome.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                               InterX\nInternet Systems Architect                5/6 Glenthorne Mews\n+44 (0)20 8817 4030                       London, W6 0LJ, England\nmsabin@interx.com                         http://www.interx.com/\n\n\n\n", "id": "lists-012-16327332"}, {"subject": "Re: comments on draft-ietf-http-authentication01.tx", "content": ">>>>> \"DK\" == Dave Kristol <dmk@bell-labs.com> writes:\n\nDK> Sect. 3.2.3, The Authentication-Info Header\nDK>     What should a client do if the rspauth=response-digest information\nDK>     is wrong?\n\nPL> Not accept the response.\n\nDK> How does a client, which has already read a response, \"not accept\nDK> [it]\"?  I'm picking nits here, true.  Does it mean that a browser would\nDK> show the user an error saying that the received response was in error?\nDK> Or does it just stop spinning its logo and leave on the screen what was\nDK> already there?\n\n  How does a browser indicate now when the certificate from an SSL\n  connection does not check out or the messages arriving on the\n  connection do not have valid signatures?  The User Agent should do\n  the right thing - authentication has failed.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n", "id": "lists-012-1633088"}, {"subject": "RE: HTTP client abort detection ..", "content": "Scott Lawrence wrote,\n> Miles Sabin wrote:\n> > I've been trying (unsuccessfully) to decide whether or not\n> > RFC 2616 permits a server to interpret the receipt of a FIN\n> > on the incoming-request side of it's connection to a client \n> > as a full client close ... \n>\n> I think that it would have been poor form at best for the HTTP \n> WG to ignore the Tcp semantics in that way.  In fact, the \n> client is doing you a favor by sending the FIN as soon as it \n> knows it's done with that half-connection - the server can then \n> close its half right away even if it might otherwise have left \n> it open for persistence, perhaps suffering the time-wait \n> penalty when it times it out (if the client sends the first \n> FIN, then it has to hold the time-wait, and the server \n> doesn't).\n\nIf the connection is idle (ie. no pending response), then yes, \ncertainly, the server can close immediately.\n\nBut it's not so clear if there's (part or all of) a response \npending, in which case the FIN could be interpreted as only\nimplying that no more requests will be sent over the connection,\nrather than that the client is no longer interested in the\nresponse to the current one.\n\nAs far as I can make out the only place in RFC 2616 which has any \nbearing on this scenario is 4.4 Message Length,\n\n  the transfer-length of that body is determined by one of the \n  following (in order of precedence):\n\n  5. By the server closing the connection. (Closing the \n     connection cannot be used to indicate the end of a request \n     body, since that would leave no possibility for the server \n     to send back a response.)\n\nwhich no more than hints that servers will typically treat the \nreceipt of a FIN as an abort. In context it only really rules\nout a client using shutdown(fd, 1) as a substitute for chunking\nor Content-Length.\n\nI'd be much happier if there were language along the lines of,\n\n  To ensure a reliable response to a request in progress, a \n  client SHOULD keep the transport connection open in both \n  directions until the full response has been received. A client\n  MAY close the connection earlier if it does not receive a\n  timely response.\n\n  A server SHOULD monitor the network connection for a close\n  before or while it is transmitting the response. If the server\n  sees the connection close, it SHOULD immediately cease \n  transmitting the response.\n\n  (language pinched from 8.2.2 and elsewhere)\n\nI'm sure the above is the intent, but I just can't see anything \nwhich confirms it.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                               InterX\nInternet Systems Architect                5/6 Glenthorne Mews\n+44 (0)20 8817 4030                       London, W6 0LJ, England\nmsabin@interx.com                         http://www.interx.com/\n\n\n\n", "id": "lists-012-16335511"}, {"subject": "about relative URL", "content": "Is the following a valid relative HTTP URL?\nhttp:/mypath/index.html\n\nThe intended meaning is to GET the resource \"/mypath/index.html\" from the\ncurrent server (the one from which the page with this link was retrieved).\n\nThere has been some discussion regarding Mozilla\n(<http://bugzilla.mozilla.org/show_bug.cgi?id=34648>), which gets confused by\nthis URL.  (That is, it doesn't do what I intended.)  RFC 1808 (Relative URLs)\naccepts them syntactically, then gives \"an example algorithm\" that would reject\nthem (sect. 4):\n=======\n   Step 2: Both the base and embedded URLs are parsed into their\n           component parts as described in Section 2.4.\n\n           a) If the embedded URL is entirely empty, it inherits the\n              entire base URL (i.e., is set equal to the base URL)\n              and we are done.\n\n>>         b) If the embedded URL starts with a scheme name, it is\n              interpreted as an absolute URL and we are done.\n\n           c) Otherwise, the embedded URL inherits the scheme of\n              the base URL.\n=======\n\nRFC 2396, Sect. 5, implies that the example is not relative, via the negation of\n\"A relative reference that does not begin with a scheme name or a slash\ncharacter is termed a relative-path reference.\"  Section 5.2 more explicitly\nsays (like RFC 1808):\n=======\n   3) If the scheme component is defined, indicating that the reference\n      starts with a scheme name, then the reference is interpreted as an\n      absolute URI and we are done.  Otherwise, the reference URI's\n      scheme is inherited from the base URI's scheme component.\n=======\n\nI guess I haven't been paying attention.  I have been using URLs like the\nexample for years, and they have been accepted (by liberal programs?). \nFurthermore, I don't see why the RFC so specifically declares all URLs with a\nscheme to be absolute (except that Sect. 4 says \"[t]he syntax for relative URI\nis a shortened form of that for absolute URI, where some prefix of the URI is\nmissing...\".  It seems to me that an otherwise relative URL with an explicit\nscheme can be parsed unambiguously.\n\nDave Kristol\n\n\n\n", "id": "lists-012-16345110"}, {"subject": "Re: about relative URL", "content": "  Hi Dave,\n\n> Is the following a valid relative HTTP URL?\n> http:/mypath/index.html\n\nNo.\n\n> The intended meaning is to GET the resource \"/mypath/index.html\" from the\n> current server (the one from which the page with this link was retrieved).\n> \n> There has been some discussion regarding Mozilla\n> (<http://bugzilla.mozilla.org/show_bug.cgi?id=34648>), which gets confused by\n> this URL.  (That is, it doesn't do what I intended.)  RFC 1808 (Relative URLs)\n> accepts them syntactically, then gives \"an example algorithm\" that would reject\n> them (sect. 4):\n[snip]\n> RFC 2396, Sect. 5, implies that the example is not relative, via the negation of\n> \"A relative reference that does not begin with a scheme name or a slash\n> character is termed a relative-path reference.\"  Section 5.2 more explicitly\n> says (like RFC 1808):\n> =======\n>    3) If the scheme component is defined, indicating that the reference\n>       starts with a scheme name, then the reference is interpreted as an\n>       absolute URI and we are done.  Otherwise, the reference URI's\n>       scheme is inherited from the base URI's scheme component.\n> =======\n\nThe next paragraph after that is instructive:\n\n      Due to a loophole in prior specifications [RFC1630], some parsers\n      allow the scheme name to be present in a relative URI if it is the\n      same as the base URI scheme.  Unfortunately, this can conflict\n      with the correct parsing of non-hierarchical URI.  For backwards\n      compatibility, an implementation may work around such references\n      by removing the scheme if it matches that of the base URI and the\n      scheme is known to always use the <hier_part> syntax.  The parser\n      can then continue with the steps below for the remainder of the\n      reference components.  Validating parsers should mark such a\n      misformed relative reference as an error.\n\n> I guess I haven't been paying attention.  I have been using URLs like the\n> example for years, and they have been accepted (by liberal programs?). \n> Furthermore, I don't see why the RFC so specifically declares all URLs with a\n> scheme to be absolute (except that Sect. 4 says \"[t]he syntax for relative URI\n> is a shortened form of that for absolute URI, where some prefix of the URI is\n> missing...\".  It seems to me that an otherwise relative URL with an explicit\n> scheme can be parsed unambiguously.\n\nOne of the authors will have to give you the details on the thoughts that\nwent behind this, but <scheme>:<abs_path> (ala http:/mypath/index.html)\nis a valid absolute URI (not for http, but in general for URI's\nfollowing the generic syntax), and hence a generic syntax parser would\nbe unable to distinguish between relative and absolute URI's. There's no\nreason to allow relative URI's of the form <scheme>:<abs_path> (other\nthan backwards compatibility), but there is for absolute URI's (e.g. the\nfile scheme doesn't need any host part).\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-16353934"}, {"subject": "HTTP and half close (WAS: HTTP client abort detection", "content": "Scott Lawrence wrote,\n> Miles Sabin wrote:\n> > I've been trying (unsuccessfully) to decide whether or not\n> > RFC 2616 permits a server to interpret the receipt of a FIN \n> > on the incoming-request side of it's connection to a client \n> > as a full client close ... \n>\n> I think that it would have been poor form at best for the HTTP \n> WG to ignore the Tcp semantics in that way.  In fact, the \n> client is doing you a favor by sending the FIN as soon as it \n> knows it's done with that half-connection - the server can then \n> close its half right away even if it might otherwise have left \n> it open for persistence, perhaps suffering the time-wait \n> penalty when it times it out (if the client sends the first \n> FIN, then it has to hold the time-wait, and the server \n> doesn't).\n\nThinking about this some more, I'm coming to the conclusion that\nthere's a genuine and problematic gap in RFC 2616. Just to make \nsure there's no misunderstanding here, I want to emphasize that \nI'm quite well aware of the benefits for servers of clients \nsending the first FIN.\n\nAs far as I can make out there's nothing in RFC 2616 which \nunambiguously rules out any of the following possible client and \nserver implementations,\n\n1. Clients which send a FIN early ... ie. by doing a half close\n   as soon as their last request is sent, but possibly before\n   the corresponding response has been received.\n\n2. Clients which don't (half or full) close until all pending\n   responses have been received.\n\nA. Servers which treat an early client FIN only as a half close \n   and continue processing and sending pending responses.\n\nB. Servers which treat an early client FIN as an indicator of a \n   client abort, and hence abandon processing and sending pending\n   responses.\n\nThe problem is that clients of type 1 aren't fully interoperable\nwith servers of type B ... if a type 1 client does an eager\nhalf close a type B server would abandon it's response.\n\nSo at least one of these two should be ruled out by RFC 2616. \n\nUnfortunately I don't see that either unambiguously is. And if\nneither are ruled out by the spec, then interoperability\nconsiderations mean that it's not safe for client or server\nimplementors to adopt either. In real-world terms, I'm pretty\nsure that most existing user-agents (nb. I'm not talking about \nclients generally) are of type 2, and most servers are of type\nA.\n\nHere are some pro and con considerations wrt type 1 clients and\ntype B servers.\n\n* The pragmatic good citizenship argument that a client should \n  send the first FIN, and sending it immediately after its \n  request has been sent, before any or all of the response has \n  been received, makes that likely.\n\n  On the other hand, this isn't the only way for a client to send\n  the first FIN. In scenarios which allow a client to half close\n  eagerly it should also be possible for it to send Connection: \n  close, in which case the server would know that the connection \n  is terminating. Hence the server, assuming it sends a Content-\n  Length or chunks, could quite happily defer closing after \n  sending the response, on the assumption that the client will \n  close soon and that it can close it's own end when that\n  happens.\n\n  On balance, I don't think any conclusion can be drawn from the\n  above. Early half close helps servers, but there are other\n  ways of getting the same effect.\n\n* A similar pragmatic argument that if servers are allowed to\n  treat early client FINs as client aborts, then they might be\n  able to avoid expensive response processing.\n\n  In the particular case of a proxy server, such a FIN might be \n  detected during proxy-side DNS resolution, but before the \n  initiation of a connect to an origin server. If the proxy \n  isn't going to cache the now unwanted response (either because \n  it's not a caching proxy, or because it's confident the \n  response will be uncacheable) then it would make sense not to \n  attempt to forward the request to the origin server at all.\n\n* From 8.1.4 Practical Considerations,\n\n    Servers SHOULD always respond to at least one request per \n    connection, if at all possible. Servers SHOULD NOT close a \n    connection in the middle of transmitting a response, unless a \n    network or client failure is suspected.\n\n  So long as we read 'middle' of a response liberally and allow \n  it to cover any point in time between the servers receipt of\n  the request up to the completion of the sending of the \n  response, then this seems to support type 1 clients against \n  type B servers.\n\n  Nevertheless, it's very hard to see the practical difference\n  between a 'network or client failure' and, say, the behaviour\n  of a user agent when a user hits the stop button because the\n  server hasn't managed to deliver a response sufficiently\n  quickly.\n\n* If we aren't so liberal with the reading of 'middle' in the\n  above quoted para, then we have,\n\n    A client, server, or proxy MAY close the transport connection \n    at any time. For example, a client might have started to send \n    a new request at the same time that the server has decided to \n    close the \"idle\" connection. From the server's point of view, \n    the connection is being closed while it was idle, but from \n    the client's point of view, a request is in progress.\n\n  This suggests that a server which detects an early client FIN \n  would be within it's rights in exploiting the persistent \n  connection close race condition to avoid processing or sending \n  any response at all: it's free to close the connection at any \n  time, and it's not yet started to send it's response. The only \n  difference between this scenario and the typical cases is the \n  exact location of the request message ... on the wire, in the \n  TCP receive buffers, or in a server buffer.\n\n* If type 1 clients are legitimate, this parenthetical comment in \n  4.4 Message Length becomes quite baffling,\n\n    the transfer-length of that body is determined by one of the \n    following (in order of precedence):\n\n    5. By the server closing the connection. (Closing the \n       connection cannot be used to indicate the end of a request \n       body, since that would leave no possibility for the server \n       to send back a response.)\n\n  because if an early half-close were legitimate, it would\n  provide a perfectly respectable mechanism for delimiting a\n  request body.\n\nNone of the above leaves me with any particularly clear idea of\nwhat best practice might be. Unless I can be persuaded otherwise\nI'm obliged to be cautious from an interoperability perspective\nand assume,\n\n* That a type 1 client implementation is inadvisable, because\n  it wouldn't reliably interoperate with type B servers.\n\n* That a type B server implementation is inadvisable, because\n  it wouldn't reliably interoperate with type 1 clients.\n\nCan anyone shed any more light on this?\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                               InterX\nInternet Systems Architect                5/6 Glenthorne Mews\n+44 (0)20 8817 4030                       London, W6 0LJ, England\nmsabin@interx.com                         http://www.interx.com/\n\n\n\n", "id": "lists-012-16363886"}, {"subject": "Re: HTTP and half close (WAS: HTTP client abort detection", "content": "There is a long-expired Internet-Draft, \"HTTP Connection Management\"\n(http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-connection-00.txt),\nthat has some good discussion of related issues.\n\n<!--StartFragment-->The HTTP/1.1 Proposed Standard [HTTP/1.1] specification\nis silent on\n   when, or even if, the connection should be closed (implementation\n   experience was desired before the specification was frozen on this\n   topic). So HTTP has moved from a model that closed the connection\n   after every request/response to one that might never close. Neither\n   of these two extremes deal with \"connection management\" in any\n   workable sense.\n<!--EndFragment-->\n\n     -Carl\n\n\n\nMiles Sabin <MSabin@interx.com> on 03/14/2001 07:35:00 AM\n\nTo:   http-wg@cuckoo.hpl.hp.com\ncc:\nSubject:  HTTP and half close (WAS: HTTP client abort detection)\n\n\n\nScott Lawrence wrote,\n> Miles Sabin wrote:\n> > I've been trying (unsuccessfully) to decide whether or not\n> > RFC 2616 permits a server to interpret the receipt of a FIN\n> > on the incoming-request side of it's connection to a client\n> > as a full client close ...\n>\n> I think that it would have been poor form at best for the HTTP\n> WG to ignore the Tcp semantics in that way.  In fact, the\n> client is doing you a favor by sending the FIN as soon as it\n> knows it's done with that half-connection - the server can then\n> close its half right away even if it might otherwise have left\n> it open for persistence, perhaps suffering the time-wait\n> penalty when it times it out (if the client sends the first\n> FIN, then it has to hold the time-wait, and the server\n> doesn't).\n\nThinking about this some more, I'm coming to the conclusion that\nthere's a genuine and problematic gap in RFC 2616. Just to make\nsure there's no misunderstanding here, I want to emphasize that\nI'm quite well aware of the benefits for servers of clients\nsending the first FIN.\n\nAs far as I can make out there's nothing in RFC 2616 which\nunambiguously rules out any of the following possible client and\nserver implementations,\n\n1. Clients which send a FIN early ... ie. by doing a half close\n   as soon as their last request is sent, but possibly before\n   the corresponding response has been received.\n\n2. Clients which don't (half or full) close until all pending\n   responses have been received.\n\nA. Servers which treat an early client FIN only as a half close\n   and continue processing and sending pending responses.\n\nB. Servers which treat an early client FIN as an indicator of a\n   client abort, and hence abandon processing and sending pending\n   responses.\n\nThe problem is that clients of type 1 aren't fully interoperable\nwith servers of type B ... if a type 1 client does an eager\nhalf close a type B server would abandon it's response.\n\nSo at least one of these two should be ruled out by RFC 2616.\n\nUnfortunately I don't see that either unambiguously is. And if\nneither are ruled out by the spec, then interoperability\nconsiderations mean that it's not safe for client or server\nimplementors to adopt either. In real-world terms, I'm pretty\nsure that most existing user-agents (nb. I'm not talking about\nclients generally) are of type 2, and most servers are of type\nA.\n\nHere are some pro and con considerations wrt type 1 clients and\ntype B servers.\n\n* The pragmatic good citizenship argument that a client should\n  send the first FIN, and sending it immediately after its\n  request has been sent, before any or all of the response has\n  been received, makes that likely.\n\n  On the other hand, this isn't the only way for a client to send\n  the first FIN. In scenarios which allow a client to half close\n  eagerly it should also be possible for it to send Connection:\n  close, in which case the server would know that the connection\n  is terminating. Hence the server, assuming it sends a Content-\n  Length or chunks, could quite happily defer closing after\n  sending the response, on the assumption that the client will\n  close soon and that it can close it's own end when that\n  happens.\n\n  On balance, I don't think any conclusion can be drawn from the\n  above. Early half close helps servers, but there are other\n  ways of getting the same effect.\n\n* A similar pragmatic argument that if servers are allowed to\n  treat early client FINs as client aborts, then they might be\n  able to avoid expensive response processing.\n\n  In the particular case of a proxy server, such a FIN might be\n  detected during proxy-side DNS resolution, but before the\n  initiation of a connect to an origin server. If the proxy\n  isn't going to cache the now unwanted response (either because\n  it's not a caching proxy, or because it's confident the\n  response will be uncacheable) then it would make sense not to\n  attempt to forward the request to the origin server at all.\n\n* From 8.1.4 Practical Considerations,\n\n    Servers SHOULD always respond to at least one request per\n    connection, if at all possible. Servers SHOULD NOT close a\n    connection in the middle of transmitting a response, unless a\n    network or client failure is suspected.\n\n  So long as we read 'middle' of a response liberally and allow\n  it to cover any point in time between the servers receipt of\n  the request up to the completion of the sending of the\n  response, then this seems to support type 1 clients against\n  type B servers.\n\n  Nevertheless, it's very hard to see the practical difference\n  between a 'network or client failure' and, say, the behaviour\n  of a user agent when a user hits the stop button because the\n  server hasn't managed to deliver a response sufficiently\n  quickly.\n\n* If we aren't so liberal with the reading of 'middle' in the\n  above quoted para, then we have,\n\n    A client, server, or proxy MAY close the transport connection\n    at any time. For example, a client might have started to send\n    a new request at the same time that the server has decided to\n    close the \"idle\" connection. From the server's point of view,\n    the connection is being closed while it was idle, but from\n    the client's point of view, a request is in progress.\n\n  This suggests that a server which detects an early client FIN\n  would be within it's rights in exploiting the persistent\n  connection close race condition to avoid processing or sending\n  any response at all: it's free to close the connection at any\n  time, and it's not yet started to send it's response. The only\n  difference between this scenario and the typical cases is the\n  exact location of the request message ... on the wire, in the\n  TCP receive buffers, or in a server buffer.\n\n* If type 1 clients are legitimate, this parenthetical comment in\n  4.4 Message Length becomes quite baffling,\n\n    the transfer-length of that body is determined by one of the\n    following (in order of precedence):\n\n    5. By the server closing the connection. (Closing the\n       connection cannot be used to indicate the end of a request\n       body, since that would leave no possibility for the server\n       to send back a response.)\n\n  because if an early half-close were legitimate, it would\n  provide a perfectly respectable mechanism for delimiting a\n  request body.\n\nNone of the above leaves me with any particularly clear idea of\nwhat best practice might be. Unless I can be persuaded otherwise\nI'm obliged to be cautious from an interoperability perspective\nand assume,\n\n* That a type 1 client implementation is inadvisable, because\n  it wouldn't reliably interoperate with type B servers.\n\n* That a type B server implementation is inadvisable, because\n  it wouldn't reliably interoperate with type 1 clients.\n\nCan anyone shed any more light on this?\n\nCheers,\n\n\nMiles\n\n--\nMiles Sabin                               InterX\nInternet Systems Architect                5/6 Glenthorne Mews\n+44 (0)20 8817 4030                       London, W6 0LJ, England\nmsabin@interx.com                         http://www.interx.com/\n\n\n\n", "id": "lists-012-16379102"}, {"subject": "RE: HTTP and half close (WAS: HTTP client abort detection", "content": "Carl Kugler wrote,\n> There is a long-expired Internet-Draft, \"HTTP Connection \n> Management\"\n<snip/>\n> that has some good discussion of related issues.\n\nYup, I'm aware of that doc. Unfortunately it only addresses the\nquestion of when a *server* should do a half close and why, not\nwhen or if a *client* should half close.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                               InterX\nInternet Systems Architect                5/6 Glenthorne Mews\n+44 (0)20 8817 4030                       London, W6 0LJ, England\nmsabin@interx.com                         http://www.interx.com/\n\n\n\n", "id": "lists-012-16395566"}, {"subject": "Re: HTTP and half close (WAS: HTTP client abort detection", "content": "\"Server receives a FIN\" and \"HTTP connection is closed\" are two\ndifferent things.  The first is one part of a TCP handshake for which\nHTTP has no business defining the rules (HTTP is transport independent),\nand the second is something that an application has to find out from\nwhatever network API it is using for sending/receiving HTTP bytes.\n\nUsing the typical socket API with TCP, an HTTP connection is considered\nto be closed when it receives a fatal error or EOF either:\n\n   1) when attempting to read an unfinished request and there are\n      no pending responses being sent on the connection (this is easily\n      determined because all HTTP requests are length-delimited), or\n\n   2) when writing a response.\n\nNote that the above applies to the socket API.  There may be a completely\ndifferent algorithm for determining when the connection is closed when\nusing other API, particularly event-based ones.\n\nThe reason this isn't defined in the HTTP spec is because it is not\nan interoperability issue for the application protocol -- it is an\nimplementation detail that is entirely dependent on the nature of the\nlower-layer API used by the application.  It is the stuff for a good\nbook on network programming.\n\nBTW, the reason that clients don't close the connection is because\nthey are generally written to be selfish in regard to network resources.\nSome clients (Windows-based) never send a FIN -- they always do an RST\nbecause they don't want to incur the cost of TIME_WAIT, due to an\nunusually low fixed limit on mbufs for some version of the OS.  This is\nbroken behavior, since the RST is easily lost and the server is left\nhanging til a timeout expires, but that is fairly typical of clients.\n\n\nCheers,\n\nRoy T. Fielding, Chief Scientist, eBuilt, Inc.\n                 2652 McGaw Avenue\n                 Irvine, CA 92614-5840  fax:+1.949.609.0001\n                 (fielding@ebuilt.com)  <http://www.eBuilt.com>\n\n\n\n", "id": "lists-012-16403710"}, {"subject": "RE: HTTP and half close (WAS: HTTP client abort detection", "content": "Roy T. Fielding wrote,\n> Using the typical socket API with TCP, an HTTP connection is \n> considered to be closed when it receives a fatal error or EOF \n> either:\n>\n>  1) when attempting to read an unfinished request and there \n>     are no pending responses being sent on the connection \n>     (this is easily determined because all HTTP requests are \n>     length-delimited), or\n>\n>  2) when writing a response.\n\nSo, in other words, the type 1 clients of my previous posting\n(early half close) are correct, and the type B servers of my\nprevious posting (early EOF implies client abort) are incorrect.\n\nThat's all very well, but it's only one of the possible choices \nfor the mapping of TCP/socket API behaviour on to HTTP behaviour.\nIt'd be nice to have some justification for why the choice should \nbe made this way rather than some other, and some reassurance \nthat this is the way the choice has been made in real \nimplementations. \n\nIt also leaves me puzzled about the question of delimiting\nrequest entities. If a client TCP half close *isn't* considered \nas an HTTP connection close, then why can't we use that that as \nan alternative to Content-Length or chunking? Given what you've\nsaid above, the parenthesis in,\n\n   5.By the server closing the connection. (Closing the \n     connection cannot be used to indicate the end of a request \n     body, since that would leave no possibility for the server \n     to send back a response.)\n\nsimply doesn't apply. Why not allow EOF as a request entity\ndelimiter?\n\n> Note that the above applies to the socket API.  There may be a \n> completely different algorithm for determining when the \n> connection is closed when using other API, particularly event-\n> based ones.\n\nCould you elaborate on this? It seems to imply that we might\nhave client and server HTTP over TCP implementations, one built \non the socket API, one built on something else, each with\ndifferent mappings of TCP behaviour to HTTP behaviour, hence that \nthe two might not be fully interoperable. I'm not sufficiently \nfamiliar with non-socket APIs to say whether this would be likely \nto be a problem in practice, but it certainly seems like a \nworrying prospect.\n\n> The reason this isn't defined in the HTTP spec is because it is \n> not an interoperability issue for the application protocol -- \n> it is an implementation detail that is entirely dependent on \n> the nature of the lower-layer API used by the application.  It \n> is the stuff for a good book on network programming.\n\nPerhaps it's not an issue for the abstract protocol, but it is\nan issue for it's most common concrete realization. Isn't this\nsort of interaction between the transport layer and the\napplication layer precisely the sort of thing which prompted\nJim Gettys and Alan Freiers \"HTTP Connection Management\" ID?\n\nOn the face of it, it'd be useful to have one or more transport\nlayer specific profiles as adjuncts to the abstract protocol\nspecification.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                               InterX\nInternet Systems Architect                5/6 Glenthorne Mews\n+44 (0)20 8817 4030                       London, W6 0LJ, England\nmsabin@interx.com                         http://www.interx.com/\n\n\n\n", "id": "lists-012-16413293"}, {"subject": "Re: comments on draft-ietf-http-authentication01.tx", "content": ">>>>> \"PL\" == Paul Leach <paulle@MICROSOFT.com> writes:\n\n>> From: Ronald.Tschalaer@psi.ch[SMTP:Ronald.Tschalaer@psi.ch]\n\n>> Also, sending the Authorization header unnecessarily is likely to reduce\n>> the cachability of many pages, thereby further increasing the traffic\n>> (how many responses currently contain the cache-control directive s-maxage\n>> or public? How quickly will this change?).\n\nPL> It is worth adding a note that origin servers that receive requests with\nPL> Authorization headers when authorization is not needed SHOULD send back\nPL> explicit cache-control directives to allow the page to be cached.\n\n  Our server always sends a cache-control header to any 1.1 request so\n  that it is explicit.\n\n  The really interesting question (from our perspective) is 'how soon\n  will the proxies and clients use it'?\n\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n", "id": "lists-012-1642415"}, {"subject": "Re: HTTP and half close (WAS: HTTP client abort detection", "content": "> That's all very well, but it's only one of the possible choices \n> for the mapping of TCP/socket API behaviour on to HTTP behaviour.\n> It'd be nice to have some justification for why the choice should \n> be made this way rather than some other, and some reassurance \n> that this is the way the choice has been made in real \n> implementations. \n\nThere are literally millions of theoretical possibilties that are\nnot mentioned in a protocol spec.  If they were all mentioned, then\nnobody would have time to read the spec.\n\nThe reasoning in this case is very simple: be conservative in what you\nsend and liberal in what you accept.  If you follow that principle, there\nis only one choice, and you will find it in every robust implementation of\nHTTP over TCP.  Go ahead and check the source code at apache.org.\n\n> It also leaves me puzzled about the question of delimiting\n> request entities. If a client TCP half close *isn't* considered \n> as an HTTP connection close, then why can't we use that that as \n> an alternative to Content-Length or chunking? Given what you've\n> said above, the parenthesis in,\n> \n>    5.By the server closing the connection. (Closing the \n>      connection cannot be used to indicate the end of a request \n>      body, since that would leave no possibility for the server \n>      to send back a response.)\n> \n> simply doesn't apply. Why not allow EOF as a request entity\n> delimiter?\n\nBecause there is no value obtained by allowing it, and the cost would be\nto add an HTTP requirement that is specific to those TCP API that provide\nhalf-close as an option.  If there exists a platform for which the\napplication-layer cannot discern a half-close condition from an error\ncondition, then the server on that platform cannot know if it has\nreceived the entire request or only a partial request, and since this\ncan impact the fidelity of carrying out that request, you have defined\na protocol which is less robust than what is defined in RFC 2616.\nTherefore, it is not an option allowed by HTTP/1.1.\n\n> > Note that the above applies to the socket API.  There may be a \n> > completely different algorithm for determining when the \n> > connection is closed when using other API, particularly event-\n> > based ones.\n> \n> Could you elaborate on this? It seems to imply that we might\n> have client and server HTTP over TCP implementations, one built \n> on the socket API, one built on something else, each with\n> different mappings of TCP behaviour to HTTP behaviour, hence that \n> the two might not be fully interoperable. I'm not sufficiently \n> familiar with non-socket APIs to say whether this would be likely \n> to be a problem in practice, but it certainly seems like a \n> worrying prospect.\n\nThey are interoperable right now precisely because HTTP does not\ndefine things in terms of TCP.  There are many platforms that have\nHTTP servers without any TCP -- the Web was created in a world where\nTCP was not the universal standard for transport (not that it is today).\nThe intermediary boxes that translate from non-TCP server network to\nTCP client network do not know anything about HTTP, and thus an HTTP\nmessage can only be sent through that gateway if neither side makes\nimplementation assumptions about the other beyond those carried within\nthe HTTP message.\n\n> > The reason this isn't defined in the HTTP spec is because it is \n> > not an interoperability issue for the application protocol -- \n> > it is an implementation detail that is entirely dependent on \n> > the nature of the lower-layer API used by the application.  It \n> > is the stuff for a good book on network programming.\n> \n> Perhaps it's not an issue for the abstract protocol, but it is\n> an issue for it's most common concrete realization. Isn't this\n> sort of interaction between the transport layer and the\n> application layer precisely the sort of thing which prompted\n> Jim Gettys and Alan Freiers \"HTTP Connection Management\" ID?\n> \n> On the face of it, it'd be useful to have one or more transport\n> layer specific profiles as adjuncts to the abstract protocol\n> specification.\n\nThere is absolutely no question that there is value in documenting\nimplementation profiles for protocols.  I am particularly fond of the\nones written by W. Richard Stevens.  But there is also a damn good\nreason why those profiles are better written by individual authors\nrather than within a standards organization, and that is because they\ndeal with how a standard should be implemented rather than what the\nstandard should be.\n\nCheers,\n\nRoy T. Fielding, Chief Scientist, eBuilt, Inc.\n                 2652 McGaw Avenue\n                 Irvine, CA 92614-5840  fax:+1.949.609.0001\n                 (fielding@ebuilt.com)  <http://www.eBuilt.com>\n\n\n\n", "id": "lists-012-16424357"}, {"subject": "RE: HTTP and half close (WAS: HTTP client abort detection", "content": "Miles wrote:\n>Roy T. Fielding wrote,\n>> Using the typical socket API with TCP, an HTTP connection is\n>> considered to be closed when it receives a fatal error or EOF\n>> either:\n>>\n>>  1) when attempting to read an unfinished request and there\n>>     are no pending responses being sent on the connection\n>>     (this is easily determined because all HTTP requests are\n>>     length-delimited), or\n>>\n>>  2) when writing a response.\n>\n>So, in other words, the type 1 clients of my previous posting\n>(early half close) are correct, and the type B servers of my\n>previous posting (early EOF implies client abort) are incorrect.\n>\n>That's all very well, but it's only one of the possible choices\n>for the mapping of TCP/socket API behaviour on to HTTP behaviour.\n>It'd be nice to have some justification for why the choice should\n>be made this way rather than some other, and some reassurance\n>that this is the way the choice has been made in real\n>implementations.\n>\n>It also leaves me puzzled about the question of delimiting\n>request entities. If a client TCP half close *isn't* considered\n>as an HTTP connection close, then why can't we use that that as\n>an alternative to Content-Length or chunking? Given what you've\n>said above, the parenthesis in,\n>\n>   5.By the server closing the connection. (Closing the\n>     connection cannot be used to indicate the end of a request\n>     body, since that would leave no possibility for the server\n>     to send back a response.)\n>\n>simply doesn't apply. Why not allow EOF as a request entity\n>delimiter?\n>\n\nIt was, in HTTP/1.0 or 0.9, wasn't it?  The answer I've received to this\nquestion is that EOF isn't a request entity delimiter in HTTP/1.1 because\nin HTTP/1.1 connections are persistent (by default).\n\n>> Note that the above applies to the socket API.  There may be a\n>> completely different algorithm for determining when the\n>> connection is closed when using other API, particularly event-\n>> based ones.\n>\n>Could you elaborate on this? It seems to imply that we might\n>have client and server HTTP over TCP implementations, one built\n>on the socket API, one built on something else, each with\n>different mappings of TCP behaviour to HTTP behaviour, hence that\n>the two might not be fully interoperable. I'm not sufficiently\n>familiar with non-socket APIs to say whether this would be likely\n>to be a problem in practice, but it certainly seems like a\n>worrying prospect.\n>\n\nThere is a popular API can't do half-closes:  Java.  At least until\nrecently, a close() in Java closes both sides of the connection (even if\nonly applied to the InputStream or OutputStream obtained from the Socket).\n\n>> The reason this isn't defined in the HTTP spec is because it is\n>> not an interoperability issue for the application protocol --\n>> it is an implementation detail that is entirely dependent on\n>> the nature of the lower-layer API used by the application.  It\n>> is the stuff for a good book on network programming.\n>\n>Perhaps it's not an issue for the abstract protocol, but it is\n>an issue for it's most common concrete realization. Isn't this\n>sort of interaction between the transport layer and the\n>application layer precisely the sort of thing which prompted\n>Jim Gettys and Alan Freiers \"HTTP Connection Management\" ID?\n>\n>On the face of it, it'd be useful to have one or more transport\n>layer specific profiles as adjuncts to the abstract protocol\n>specification.\n>\n>Cheers,\n>\n>\n>Miles\n>\n>--\n>Miles Sabin                               InterX\n>Internet Systems Architect                5/6 Glenthorne Mews\n>+44 (0)20 8817 4030                       London, W6 0LJ, England\n>msabin@interx.com                         http://www.interx.com/\n>\n\n     -Carl\n\n\n\n", "id": "lists-012-16436898"}, {"subject": "Problem with multipart/xmixed replac", "content": "Hi all !\n\n Could anybody tell me about the \"multipart/x-mixed-replace\" content type ?\n I try to use thie content type for my HTTPServletResponse object in my Java servlet, to keep\n outputting messages to\n the browser, but however, the browser does not interpret the headers\n properly and all the tags are visible as\n part of the browser content (I use Internet Explorer 5.0).\n\nThe following is the Java code :\npublic class RefreshingServlet extends HttpServlet{\n \n \n public void init(ServletConfig config) throws ServletException{\n  super.init(config);\n        \n }\n\npublic void service(HttpServletRequest req, HttpServletResponse res)\n    throws ServletException, IOException\n{\n    res.setContentType(\"multipart/x-mixed-replace;boundary=\\\"boundary\\\"\");\n    \n  PrintWriter out=res.getWriter();\n    // Here is the first part\n    out.print(\"\\n\\r--boundary\\n\\r\");\n    out.print(\"Content-Type: text/html\\n\\r\");\n\n    out.println(\"<H1>Waiting...</H1>\");\n    out.flush();\n\n    // Here is the long work (here, a sleep)\n    try {\n         Thread.sleep(3000);\n    }\n    catch (Exception e) {\n         // ignore\n    }\n\n    // Here is the second part\n    out.print(\"\\n\\r--boundary\\n\\r\");\n    out.print(\"Content-Type: text/html\\n\\r\");\n\n    out.println(\"<H1>Done</H1>\");\n\n    out.print(\"\\n\\r--boundary--\\n\\r\");\n    out.flush();\n}\n\n}\n\nAny loopholes for this ?\n\nRegards,\nSeema Kumar\nDatamatics Technologies Ltd.,\n(Tel: 8290829 (Ext:619))\n\n\n\n", "id": "lists-012-16448359"}, {"subject": "Re: Problem with multipart/xmixed replac", "content": "I think you will find that IE does not support  \"multipart/x-mixed-replace\"\ndespite its claim of \"User-Agent:? Mozilla/4.0 (compatible...\"\n\nYou might try asking over on www-talk (see\nhttp://lists.w3.org/Archives/Public/www-talk/).\n\n     -Carl\n\n\n\n                                                                                                                    \n                    \"Seema P Kumar\"                                                                                 \n                    <2kseema@sun20.datam       To:     <http-wg@cuckoo.hpl.hp.com>                                  \n                    atics.com>                 cc:                                                                  \n                                               Subject:     Problem with multipart/x-mixed replace                  \n                    03/30/2001 04:41 AM                                                                             \n                                                                                                                    \n                                                                                                                    \n\n\n\n\n\nHi all !\n\n?Could anybody tell me about the  \"multipart/x-mixed-replace\" content type\n?\n?I try to use thie content  type for my HTTPServletResponse object in my\nJava servlet, to  keep\n?outputting messages to\n?the browser, but however, the  browser does not interpret the headers\n?properly and all the tags are  visible as\n?part of the browser content (I use Internet Explorer  5.0).\n\nThe following is the Java code :\npublic class RefreshingServlet extends  HttpServlet{\n\n\n?public void init(ServletConfig config)  throws  ServletException{\n??super.init(config);\n\n?}\n\npublic void service(HttpServletRequest req,  HttpServletResponse res)\n??? throws ServletException,  IOException\n{\n???  res.setContentType(\"multipart/x-mixed-replace;boundary=\\\"boundary\\\"\");\n\n??PrintWriter out=res.getWriter();\n??? // Here  is the first part\n???  out.print(\"\\n\\r--boundary\\n\\r\");\n??? out.print(\"Content-Type:  text/html\\n\\r\");\n\n???  out.println(\"<H1>Waiting...</H1>\");\n???  out.flush();\n\n??? // Here is the long work (here,  a sleep)\n??? try\n????????  Thread.sleep(3000);\n??? }\n??? catch  (Exception e) {\n???????? //  ignore\n??? }\n\n??? // Here is the second  part\n???  out.print(\"\\n\\r--boundary\\n\\r\");\n??? out.print(\"Content-Type:  text/html\\n\\r\");\n\n???  out.println(\"<H1>Done</H1>\");\n\n???  out.print(\"\\n\\r--boundary--\\n\\r\");\n???  out.flush();\n}\n\n}\n\nAny loopholes for this ?\n\nRegards,\nSeema Kumar\nDatamatics Technologies  Ltd.,\n(Tel: 8290829 (Ext:619))\n\n\n\n", "id": "lists-012-16456897"}, {"subject": "Proposal (I-D) for extending HTTP to support out-oforder response", "content": "A few weeks ago, I gave a short \"Work In Progress\" talk at\nUSITS (the \"USENIX Symposium on Internet Technologies and\nSystems\") about some very early research results I had obtained\nabout the potential value of supporting out-of-order responses\nin HTTP.\n\nSeveral people came up to me after the talk and seemed very eager\nto try out this kind of mechanism.  One person had even\nimplemented the idea in his own server code while the session was\nin progress.  So it seemed like it might be a good idea to\npropose a standard extension *before* lots of people started to\ndeploy this kind of thing.  Hence, I wrote up a simple proposal\nas an Internet-Draft:\n\nTitle: Support for out-of-order responses in HTTP\nAuthor(s): J. Mogul\nFilename: draft-mogul-http-ooo-00.txt\nPages: 14\nDate: 09-Apr-01\n\nhttp://www.ietf.org/internet-drafts/draft-mogul-http-ooo-00.txt\n\n    The introduction of persistent connections and pipelining\n    into HTTP has resulted in potential performance benefits,\n    but has also exposed the problem of head-of-line blocking.\n    A simple, compatible, and optional extension to HTTP to\n    allow a server to issue responses out of order could\n    significantly reduce HOL blocking.  In this extension,\n    clients add short ID fields to their requests, and servers\n    echo these IDs back in their responses.  This extension is\n    defined as a hop-by-hop rather than end-to-end mechanism,\n    so it avoids much of the complexity of the end-to-end\n    approach.\n    \nIf members of the HTTP-WG mailing list have comments on this,\nI'd be happy to receive them.\n\nI want to stress that the research evaluation that might prove\n(or disprove) the value of this approach has NOT been finished.\nSo it does not make sense to start an argument about whether\nthis is useful (unless someone else has already done a careful\nevaluation of this kind of approach).\n\nI'm just looking for comments on whether the protocol makes sense\n(and, in particular, whether it might lead to screwups by caching\nproxies that don't comply with the HTTP specifications).\n\nThanks\n-Jeff\n\n\n\n", "id": "lists-012-16467244"}, {"subject": "Re: Proposal (I-D) for extending HTTP to support out-oforder response", "content": "That is a nice draft -- very complete and readable.  I remember discussing\nthe out-of-order question with some of the CERN folks in October 1994 at\nthe Chicago WWW conference.  The conclusion we came to was that it would\nbe easier to deploy SCP as an incompatible protocol than it would be to fix\nall of the pre-standard HTTP/1.0 implementations, with more to gain from\ndoing the former.  I suspect we are still at that state.\n\nOne thing I've toyed with in the past was to use a hierarchical request\nID as a poor man's form of transaction identifier.  That is,\n\n   TID: 1234\n\nwould be a simple request #1234, whereas\n\n   TID: 888/1234\n\nwould indicate that request 1234 was part of transaction 888, and\n\n   TID: 7/888/1234\n\nwould be transaction 7, sub-transaction 888, request 1234.  Naturally,\n\n   TID: http://example.com/7/888/1234\n\nwould be the globally unique identifier, if such were needed.  Killing\ntwo birds with one stone.\n\nCheers,\n\nRoy T. Fielding, Chief Scientist, eBuilt, Inc.\n                 2652 McGaw Avenue\n                 Irvine, CA 92614-5840  fax:+1.949.609.0001\n                 (fielding@ebuilt.com)  <http://www.eBuilt.com>\n\n\n\n", "id": "lists-012-16476761"}, {"subject": "Re: Proposal (I-D) for extending HTTP to support out-oforder response", "content": "> I'm just looking for comments on whether the protocol makes sense\n> (and, in particular, whether it might lead to screwups by caching\n> proxies that don't comply with the HTTP specifications).\n\n2 thoughts, the first is just a throw-away:\n\n1 - HTTP is stateless. indeed 2616 in its intro describes itself this\n    way.. this is a pretty fundamental change, even as an optional\n    extension.. so the intro should probably discuss it. that's all.\n\n2 - This isn't a safety/correctness thing, but it seems like its a bit of\n  a problem to effective usage..\n\ngiven: 1] non-safe methods like POST or PUT are more likely  to be\n          barrier-requests than GETs\n \n       2] POSTs and PUTs can have large request bodies with\n       significant upload latencies\n\n       3] large request bodies benefit from the expect/100-continue\n       mechanism\n\n       4] real networks often have symmetric up and down capactiy that\n       its a big win to leverage by overlapping.\n\nI'm thinking about the expect/continue mechanism and how the execution\nof a resource by a big POST or PUT is likely to be a sequence point\n(and thus a barrier request). the time spent uploading the data would\ndo well to be overlapped with any outstanding responses even if it\ncan't be executed until outstanding requests have been processed.. but\nif we mark it a sequence point we can't do that because the server\ncan't say 100-continue until all of the outstanding requests have been\nsatisfied.. it'd be nice to be able to separate the out-of-order\nproperties of the 100 with that of the final response so that a \"start\nsending\" signal could be given OOO but the resource itself not\nexecuted OOO. Obviously this requires some server side buffering, and\nif it's not willing to do that it could simply do the 100 in-order.\n\nthe obvious way to do this seems to be to associate an attribute with\n100-continue.. say ERID that works similarly to RID, but applies only\nto the 100 response, not the final one.\n\nhopefully the following case played out 3 ways will help illustrate:\n\nassume we've got a POST(d) that takes 6 units to send, and 5 responses\nthat take 4(a), 5(b.cgi), 2(c), and 2(d), 3(e) to send. throw in the\nkicker that b.cgi takes 5 units of time to calculate its response and\nd takes 2 units to process the big post.\n\nfirst, the we'll do the easy case where d is not a request-barrier.\n\n\nTime    Upstream    Downstream\n-------------    -----------\n1        GET /a, RID: 1       \n2        GET /b.cgi, RID: 2                 200 - RID : 1 (1/4)\n3        GET /c, RID: 3    200 - RID : 1 (2/4)\n4        POST /d, RID:4, Expect:..    200 - RID : 1 (3/4)\n5    200 - RID : 1 (4/4)\n6    100 - RID : 4 (1/1)\n7POST/d, RID: 4 body (1/6)    200 - RID : 3 (1/2)\n8POST/d, RID: 4 body (2/6)    200 - RID : 3 (2/2)\n9POST/d, RID: 4 body (3/6)    200 - RID : 2 (1/5)\n10POST/d, RID: 4 body (4/6)    200 - RID : 2 (2/5)\n11POST/d, RID: 4 body (5/6)    200 - RID : 2 (3/5)\n12POST/d, RID: 4 body (6/6)    200 - RID : 2 (4/5)\n13GET /e, RID: 5    200 - RID : 2 (5/5)\n14    200 - RID : 5 (1/3)\n15    200 - RID : 5 (2/3)\n16    200 - RID : 5 (3/3)\n17    200 - RID : 4 (1/2)\n18    200 - RID : 4 (2/2)\n\n18 units.. pretty cool.. \n\nbut now, what if /d (the post) is a request barrier under mogul-00?\n\nTime    Upstream    Downstream\n-------------    -----------\n1        GET /a, RID: 1       \n2        GET /b.cgi, RID: 2                 200 - RID : 1 (1/4)\n3        GET /c, RID: 3    200 - RID : 1 (2/4)\n4        POST /d,  Expect:..    200 - RID : 1 (3/4)\n5    200 - RID : 1 (4/4)\n6    200 - RID : 3 (1/2)\n7    200 - RID : 3 (2/2)\n8    200 - RID : 2 (1/5)\n9    200 - RID : 2 (2/5)\n10    200 - RID : 2 (3/5)\n11    200 - RID : 2 (4/5)\n12    200 - RID : 2 (5/5)\n13    100 continue\n14       POST/d, body (1/6)\n15       POST/d, body (2/6)\n16       POST/d, body (3/6)\n17       POST/d, body (4/6)\n18       POST/d, body (5/6)\n19       POST/d, body (6/6)\n20 GET /e, RID: 5    [wait - processing d]\n21     [wait - processing d]\n22    200   (1/2) \n23    200   (2/2) \n24    200 - RID : 5 (1/3)\n25    200 - RID : 5 (1/3)\n26    200 - RID : 5 (1/3)\n\n26 units.. not as cool.. but what if the 100-continue could be out of\norder, without making the final response out of order?\n\nTime    Upstream    Downstream\n-------------    -----------\n1        GET /a RID: 1       \n2        GET /b.cgi, RID: 2                 200 - RID : 1 (1/4)\n3        GET /c, RID: 3    200 - RID : 1 (2/4)\n4        POST /d, Expect: 100-c..; ERID=E1  200 - RID : 1 (3/4)\n5    200 - RID : 1 (4/4)\n6    100 - ERID : E1 (1/1)\n7POST/d, RID: 4 body (1/6)    200 - RID : 3 (1/2)\n8POST/d, RID: 4 body (2/6)    200 - RID : 3 (2/2)\n9POST/d, RID: 4 body (3/6)    200 - RID : 2 (1/5)\n10POST/d, RID: 4 body (4/6)    200 - RID : 2 (2/5)\n11POST/d, RID: 4 body (5/6)    200 - RID : 2 (3/5)\n12POST/d, RID: 4 body (6/6)    200 - RID : 2 (4/5)\n13GET /e, RID: 5    200 - RID : 2 (5/5)\n14    [ wait - processing d]\n15    200 (1/2)\n16    200 (1/2)\n17    200 - RID : 5 (1/3)\n18    200 - RID : 5 (2/3)\n19    200 - RID : 5 (3/3)\n\n19 units!\n\n-Patrick\n\n\n\n", "id": "lists-012-16485795"}, {"subject": "Re: Proposal (I-D) for extending HTTP to support out-oforder response", "content": "> 2 thoughts, the first is just a throw-away:\n> \n> 1 - HTTP is stateless. indeed 2616 in its intro describes itself this\n>     way.. this is a pretty fundamental change, even as an optional\n>     extension.. so the intro should probably discuss it. that's all.\n\nHTTP is stateless in the sense that all of the information needed to\ninterpret and handle a request is present in that request and not based\non a prior request's leftover state.  A RID doesn't change that in\nprinciple, though it would certainly complicate the implementation\nto handle out-of-order responses.  On the server-side, the ability\nto have out-of-order responses makes it easier to partition pipelined\nrequests out to multiple server instances, which is one of the main\nreasons why HTTP is stateless.\n\n....Roy\n\n\n\n", "id": "lists-012-16499203"}, {"subject": "Confusion regarding following a lin", "content": "Hi folks ,\n\nI have a little confusion regarding HTTP , I would appreciate if someone could help me solve it.\nThe problem is as follows:\n\ne.g. I give the following request to a browser:\nhttp://directory.google.com/Top/Computers/Algorithms/\n\nit Builds the a request that aprt from other things contains the following in which I am interested now:\n\nGET /Top/Computers/Algorithms/ HTTP/1.1\nHost: directory.google.com\n\n\nFine , the server responds back and gives an HTML page to me back .\nNow I render the HTML to my GUI .\nThe HTML contains the following link:\n\n<a href=\"/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\">Cryptography</a>\n\nNow the confusion is that if  my user \"clicks\" on the hyperlink given above , what request should I generate :\n\nWhat I used to do till now was to classify the situation into three portions:\n\n  Whenever we are currently viewing a certain page on the web , and we try \n  to follow a link to another page , there can be three cases. For all the \n  cases , the current page is say : www.abc.com/help/u1/myHelp.html\n\n  FIRST CASE:\n  The link I try to follow is : \"/yourHelp.com\"\n  Effective URL should be:\n  www.abc.com/help/u1/yourHelp.com\n\n  SECOND CASE:\n  The link I try to follow is : \"../../TopLevelHelp.com\"\n  Effective URL should be:\n  www.abc.com/TopLevelHelp.com\n\n  THIRD CASE:\n  The link I try to follow is : \"www.beta.com/OtherHelp.com\"\n  Effective URL should be:\n  www.beta.com/OtherHelp.com\n\nI had implemented a little parsing in my application which works in a way that it is given the URL of the resource currently being displayed and the link which we are trying to follow , which given in the HTML after \" <a href= \" tag. , and then it returns an Effective URL which actually has to be shown . From that URL , I separate the Host part and the relative part , and  build an HTTP request and pass it on to the server . IT used to work pretty fine till now , but I encountered an error today , that led me to believe that I was probably NOT understanding the things probably.\n\nThe problem occurred when I got to the page :\n\nhttp://directory.google.com/Top/Computers/Algorithms/\n\nThe above contains a line in HTML as :\n<a href=\"/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\">Cryptography</a>\n\nNow when  my user \"clicks\" on the hyperlink given above , according to my CASES , this thing falls into the FIRST CASE , and what I do is that the EFFECTIVE URL made is:\n\nhttp://directory.google.com/Top/Computers/Algorithms/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\n\nFine , so I remove the host and relative part and Build the HTTP request :\n\nGET /Top/Computers/Algorithms/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/  HTTP/1.1\nHost: directory.google.com\n\nThe server replies that this resource is not there .\n\nWhen I follow  the same link through MS Internet Explorer , the request it generates is :\n\nGET /Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/ HTTP/1.1\nHost: directory.google.com\n\n\nI fail to understand what are the General Rules for following links ? What portion of the RFC refers to it ?\n\nI would really appreciate if someone could explain this to me .\n\nThanks in advance ,\n\nRegards,\nTaha\n\n\n\n", "id": "lists-012-16507740"}, {"subject": "Re: Confusion regarding following a lin", "content": "* Taha Masood wrote:\n>e.g. I give the following request to a browser:\n>http://directory.google.com/Top/Computers/Algorithms/\n\n>Now I render the HTML to my GUI .\n>The HTML contains the following link:\n>\n><a href=\"/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\">Cryptography</a>\n>\n>Now the confusion is that if  my user \"clicks\" on the\n>hyperlink given above , what request should I generate:\n\nDepends on the value of the href attribute of the base element in the\nhead element :-)\n\nRequest URIs must be absolute, either\n\n/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\n\nor\n\nhttp://directory.google.com/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\n\nor some non-canonical forms of that URI.\n\n>What I used to do till now was to classify the situation into three portions:\n>\n>  Whenever we are currently viewing a certain page on the web , and we try \n>  to follow a link to another page , there can be three cases. For all the \n>  cases , the current page is say : www.abc.com/help/u1/myHelp.html\n\nLet's say it's http://www.abc.com/help/u1/myHelp.html\n\n>  FIRST CASE:\n>  The link I try to follow is : \"/yourHelp.com\"\n>  Effective URL should be:\n>  www.abc.com/help/u1/yourHelp.com\n\nhttp://www.abc.com/yourHelp.com\n\n>  SECOND CASE:\n>  The link I try to follow is : \"../../TopLevelHelp.com\"\n>  Effective URL should be:\n>  www.abc.com/TopLevelHelp.com\n\nhttp://www.abc.com/yourHelp.com\n\n>  THIRD CASE:\n>  The link I try to follow is : \"www.beta.com/OtherHelp.com\"\n>  Effective URL should be:\n>  www.beta.com/OtherHelp.com\n\nhttp://www.abc.com/www.beta.com/OtherHelp.com\n\nPlease read RFC 2396.\n\n>The problem occurred when I got to the page :\n>\n>http://directory.google.com/Top/Computers/Algorithms/\n>\n>The above contains a line in HTML as :\n><a href=\"/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\">Cryptography</a>\n>\n>Now when  my user \"clicks\" on the hyperlink given above,\n>according to my CASES , this thing falls into the FIRST CASE,\n>and what I do is that the EFFECTIVE URL made is:\n>\n>http://directory.google.com/Top/Computers/Algorithms/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\n\nNope,\nhttp://directory.google.com/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\n\n>I fail to understand what are the General Rules for following links ? What portion of the RFC refers to it ?\n>\n>I would really appreciate if someone could explain this to me .\n\nRead RFC 2396. If the relative URI (href='...') is an absolute path\nhref='/...' it completly replaces the old path. RFC 2396 has a lot of\nexamples on that.\n\nThis is no HTTP issue, www-uri@w3.org deals with URIs.\n\nPS: __Please__ wrap your lines after 68 <= x <= 80 characters.\n-- \nBj?rn H?hrmann { mailto:bjoern@hoehrmann.de } http://www.bjoernsworld.de\nam Badedeich 7 } Telefon: +49(0)4667/981028 { http://bjoern.hoehrmann.de\n25899 Dageb?ll { PGP Pub. KeyID: 0xA4357E78 } http://www.learn.to/quote/\n\n\n\n", "id": "lists-012-16518678"}, {"subject": "Re: questions regarding draft-ietf-http-authentication0", "content": ">>>>> \"PL\" == Paul Leach <paulle@MICROSOFT.com> writes:\n\n>> > 1)  Section 3.2.2, request-digest description:\n>> >\n>> >         If the \"qop\" value is \"auth\":\n>> [snip]\n>> >     Shouldn't that be\n>> >\n>> >         If the \"qop\" value is \"auth\" or \"auth-int\":\n>> [snip]\n\n  This may be a merge problem - in my working draft, the definition of\n  request-digest is followed by:\n===\n    If the \"qop\" value is \"auth-int\", then request-digest is as for\n    \"auth\", except that A2 is computed as:\n\n      A2 = method \":\" digest-uri-value \":\" H(entity-body)\n===\n  this may have been accidentally dropped merging my edits with Pauls.\n\n>> \"server\" is not defined anywhere. What is the syntax supposed to\n>> be, and what purpose does it serve?\n\nPL> Left from a previous edit. It shouldn't be there.\n\n  Agreed.\n\n>> 4) Section 3.2.2, digest-response description:\n>>\n>> cnonce\n>> An opaque quoted string value provided by the client and used by\n>> both\n>> client and server to avoid chosen plaintext attacks, to provide\n>> mutual authentication, and to provide some message integrity\n>> protection.  See the descriptions below of the calculation of\n>> the\n>> response-digest and request-digest values.\n>>\n>> nonce-count\n>> This MUST be specified if a qop attribute is sent (see above),\n>> and\n>> MUST NOT be specified if the server did not send a qop attribute\n>> in\n>> the WWW-Authenticate header field. ...\n>>\n>> I presume the \"MUST NOT\" above is to avoid problems with rfc-2068\n>> implementations which might not handle an unknown attribute correctly\n>> (although they ought to just be ignoring it)? If so, why isn't the\n>> same language used for the cnonce? I.e. something like\n>>\n>> cnonce\n>> An opaque quoted string value provided by the client and used by\n>> both\n>> client and server to avoid chosen plaintext attacks, to provide\n>> mutual authentication, and to provide some message integrity\n>> protection.  See the descriptions below of the calculation of\n>> the\n>> response-digest and request-digest values. This attribute MUST\n>> NOT\n>> be specified if the server did not send a qop attribute in the\n>> WWW-Authenticate header field.\n\nPL> Actually, I don't know why the MUST NOT is there. But it does serve the\nPL> purpose you suggest, so why not put it in both places. Sure.\n\n  I wrote that section, and yes, the idea was be explicit that the\n  behaviour should be as specified by 2069 in this case.  Perhaps it\n  is stronger than it needs to be.\n\n>> In the interest of possible future enhancements, I suggest changing\n>> the current definition to:\n>>\n>> algorithm         = \"algorithm\" \"=\" ( \"MD5\" | \"MD5-sess\" | token )\n\nPL> Sure.\n\n  Agreed.\n\n>> 8) Section 3.2.3: no words prohibit the server from sending, say, a qop\n>> attribute but not a rspauth attribute. Also, while the cnonce is\n>> required to be the same as used in the request, the nonce-count isn't.\n>> Hence I propose the following change in wording:\n>>\n>> Replace\n>>\n>> where \"Status-Code\" is the status code (e.g., \"200\") from the\n>> \"Status-Line\" of the response, as defined in section 6.1 of [2],\n>> and \"digest-uri-value\" is the value of the \"uri\" directive on the\n>> Authorization header in the request. The \"cnonce-value\" MUST be\n>> one for the client request to which this message is the response.\n>>\n>> by\n>>\n>> where \"Status-Code\" is the status code (e.g., \"200\") from the\n>> \"Status-Line\" of the response, as defined in section 6.1 of [2],\n>> and \"digest-uri-value\" is the value of the \"uri\" directive on the\n>> Authorization header in the request. The \"cnonce-value\" and\n>> \"nc-value\" MUST be the ones used in the client request to which\n>> this message is the response.\n>>\n>> The \"response-auth\", \"cnonce\", and \"nonce-count\" attributes MUST\n>> BE present if \"qop=auth\" or \"qop=auth-int\" is specified.\n\nPL> Good. Thanks for the proposed wording.\n\n  This was a cut-and-paste error, I think; the nc-value is not used in\n  the construction of the response-digest, so it need not be in the\n  syntax for the Authentication-Info header at all.  From client to\n  server the requests may be pipelined, so we needed the nonce count\n  to prevent replay, but each response is to exactly one unique\n  request, so no count is needed or used.\n\n>> 9) Section 3.2.3: Actually, why are the cnonce and nonce-count attributes\n>> sent in the Authorization-Info header? Or put differently, what makes\n>> these two attributes special, as opposed to the nonce and uri (which\n>> aren't sent back)?\n\nPL> Good question.\n\n  See above for nonce-count; I thought that returning the cnonce value\n  might make implementation in the client easier; the client can then\n  use the same kind of self-validation that servers can with nonces so\n  that they need not be actually stored while waiting for the response.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n", "id": "lists-012-1652850"}, {"subject": "Re: Confusion regarding following a lin", "content": "Thanks a lot\n\nRegards,\nTaha\n----- Original Message -----\nFrom: Bjoern Hoehrmann <derhoermi@gmx.net>\nTo: Taha Masood <taha.masood@streaming-networks.com>\nCc: <http-wg@hplb.hpl.hp.com>\nSent: Friday, April 20, 2001 12:27 AM\nSubject: Re: Confusion regarding following a link\n\n\n> * Taha Masood wrote:\n> >e.g. I give the following request to a browser:\n> >http://directory.google.com/Top/Computers/Algorithms/\n>\n> >Now I render the HTML to my GUI .\n> >The HTML contains the following link:\n> >\n> ><a\nhref=\"/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algor\nithms/\">Cryptography</a>\n> >\n> >Now the confusion is that if  my user \"clicks\" on the\n> >hyperlink given above , what request should I generate:\n>\n> Depends on the value of the href attribute of the base element in the\n> head element :-)\n>\n> Request URIs must be absolute, either\n>\n>\n/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algorithms/\n>\n> or\n>\n>\nhttp://directory.google.com/Top/Science/Math/Applications/Communication_Theo\nry/Cryptography/Algorithms/\n>\n> or some non-canonical forms of that URI.\n>\n> >What I used to do till now was to classify the situation into three\nportions:\n> >\n> >  Whenever we are currently viewing a certain page on the web , and we\ntry\n> >  to follow a link to another page , there can be three cases. For all\nthe\n> >  cases , the current page is say : www.abc.com/help/u1/myHelp.html\n>\n> Let's say it's http://www.abc.com/help/u1/myHelp.html\n>\n> >  FIRST CASE:\n> >  The link I try to follow is : \"/yourHelp.com\"\n> >  Effective URL should be:\n> >  www.abc.com/help/u1/yourHelp.com\n>\n> http://www.abc.com/yourHelp.com\n>\n> >  SECOND CASE:\n> >  The link I try to follow is : \"../../TopLevelHelp.com\"\n> >  Effective URL should be:\n> >  www.abc.com/TopLevelHelp.com\n>\n> http://www.abc.com/yourHelp.com\n>\n> >  THIRD CASE:\n> >  The link I try to follow is : \"www.beta.com/OtherHelp.com\"\n> >  Effective URL should be:\n> >  www.beta.com/OtherHelp.com\n>\n> http://www.abc.com/www.beta.com/OtherHelp.com\n>\n> Please read RFC 2396.\n>\n> >The problem occurred when I got to the page :\n> >\n> >http://directory.google.com/Top/Computers/Algorithms/\n> >\n> >The above contains a line in HTML as :\n> ><a\nhref=\"/Top/Science/Math/Applications/Communication_Theory/Cryptography/Algor\nithms/\">Cryptography</a>\n> >\n> >Now when  my user \"clicks\" on the hyperlink given above,\n> >according to my CASES , this thing falls into the FIRST CASE,\n> >and what I do is that the EFFECTIVE URL made is:\n> >\n>\n>http://directory.google.com/Top/Computers/Algorithms/Top/Science/Math/Appli\ncations/Communication_Theory/Cryptography/Algorithms/\n>\n> Nope,\n>\nhttp://directory.google.com/Top/Science/Math/Applications/Communication_Theo\nry/Cryptography/Algorithms/\n>\n> >I fail to understand what are the General Rules for following links ?\nWhat portion of the RFC refers to it ?\n> >\n> >I would really appreciate if someone could explain this to me .\n>\n> Read RFC 2396. If the relative URI (href='...') is an absolute path\n> href='/...' it completly replaces the old path. RFC 2396 has a lot of\n> examples on that.\n>\n> This is no HTTP issue, www-uri@w3.org deals with URIs.\n>\n> PS: __Please__ wrap your lines after 68 <= x <= 80 characters.\n> --\n> Bj?rn H?hrmann { mailto:bjoern@hoehrmann.de } http://www.bjoernsworld.de\n> am Badedeich 7 } Telefon: +49(0)4667/981028 { http://bjoern.hoehrmann.de\n> 25899 Dageb?ll { PGP Pub. KeyID: 0xA4357E78 } http://www.learn.to/quote/\n>\n\n\n\n", "id": "lists-012-16530591"}, {"subject": "Re: Confusion regarding following a lin", "content": "Taha Masood wrote:\n\n> Now the confusion is that if  my user \"clicks\" on the hyperlink\n> given above , what request should I generate\n\nSee RFC-2396, section 5.\n\n--\n/==============================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.|\n|Chief Scientist |=============================================|\n|eCal Corp.      |Do not suspect that I am not human.          |\n|francis@ecal.com|                                             |\n\\==============================================================/\n\n\n\n", "id": "lists-012-16543884"}, {"subject": "Server Push: Multipart/X-MixedReplac", "content": "Does anybody know the current status of this feature from netscape browsers\nprior to 6?\n\nThis is a useful feature and perhaps there should be an RFC documenting it\nso that it is preserved in mainstream browsers.\n\nOne useful application would be incremental redisplay, whereby the diffs\nare sent during the refresh.  This goes beyond the netscape spec, but this\nfacility could do it.  For example, one could provide element IDs that should be\nupdated with the new data.\n\n\n\n", "id": "lists-012-16551228"}, {"subject": "Seems like the HTTP working group is shut down..", "content": "     is that right?\n\n     Doug......................\n\n\n\n", "id": "lists-012-16558538"}, {"subject": "Re: Seems like the HTTP working group is shut down..", "content": "dillon@hns.com wrote:\n\n>      is that right?\n\nThe HTTP WG within IETF has shut down, yes.\n\nDave Kristol\n\n\n\n", "id": "lists-012-16565548"}, {"subject": "Whatever happenned to HTTP 1.1 Pipelinin", "content": "     Seems like a feature which would really help\nweb-page response time using less resources than multiple\nsimultaneous connections, but it seems to have never been\nadopted.\n\n     I know of no browser that supports it.\n\n     What are the \"GOTCHAs\" that keep it from being adopted?\n\n     The biggest one that comes to mind is fact that one\nof the responses in the pipeline may not have a content-length.\nWhat could be done about that?\n\n     Doug.......................\n\n\n\n", "id": "lists-012-16572968"}, {"subject": "RE: Whatever happenned to HTTP 1.1 Pipelinin", "content": "The latest version of Opera uses pipelining.\n\n> -----Original Message-----\n> From: dillon@hns.com [mailto:dillon@hns.com]\n> Sent: Friday, April 27, 2001 9:23 AM\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Whatever happenned to HTTP 1.1 Pipelining\n> \n> \n> \n> \n> \n>      Seems like a feature which would really help\n> web-page response time using less resources than multiple\n> simultaneous connections, but it seems to have never been\n> adopted.\n> \n>      I know of no browser that supports it.\n> \n>      What are the \"GOTCHAs\" that keep it from being adopted?\n> \n>      The biggest one that comes to mind is fact that one\n> of the responses in the pipeline may not have a content-length.\n> What could be done about that?\n> \n>      Doug.......................\n> \n> \n\n\n\n", "id": "lists-012-16580649"}, {"subject": "Re: Whatever happenned to HTTP 1.1 Pipelinin", "content": ">     The biggest one that comes to mind is fact that one\n> of the responses in the pipeline may not have a content-length.\n> What could be done about that?\n\nUse Transfer-Encoding: chunked.\n\n     -Carl\n\n\n\n", "id": "lists-012-16590110"}, {"subject": "Re: Whatever happenned to HTTP 1.1 Pipelinin", "content": "     Thankyou for digging that up, but does it\nmean that by a server SHOULD or MUST use self-defined message\nlengths? This doesn't seem to give the browser author any assurance\nthat his pipelined requests won't be aborted by a premature connection\nclose due to a no CONTENT-LENGTH response.\n\n     Seems to me that you only pipeline after you've gotton one\nrequest through a connection because you don't know whether the\nother end supports persistent connections. After that first request, which\nhandles the negotiation, you'd like to be able to pipeline with worry about the\npipeline being\ncut off.\n\n     Doug.......................\n\n\n\n\n\n\n\n\"Carl Kugler\" <kugler@us.ibm.com> on 04/27/2001 11:53:53 AM\n                                                                                \n                                                                                \n                                                                                \n\n\n\n                                                              \n                                                              \n                                                              \n To:      Doug Dillon/HNS@HNS                                 \n                                                              \n cc:                                                          \n                                                              \n                                                              \n                                                              \n Subject: Re: Whatever happenned to HTTP 1.1 Pipelining       \n                                                              \n\n\n\n\n\n\n\n\nI found this sentence in section 8.1.2.1, Negotation:\n\nIn order to remain persistent, all messages on the connection MUST have a\nself-defined message length (i.e., one not defined by closure of the\nconnection), as described in section 4.4.\n\n\n     -Carl\n\n\n\n\n\n                    dillon@hns.com\n                                         To:     Carl Kugler/Boulder/IBM@IBMUS\n                    04/27/2001           cc:\n                    08:36 AM             Subject:     Re: Whatever happenned to\nHTTP 1.1\n                                          Pipelining\n\n\n\n\n\n\n\n\n\n     I naturally thought about that, but is there anything is the RFC\nthat says that Transfer-Encoding: chunked SHOULD or MUST be used on\npersistent\nHTTP 1.1 connections instead of sending with no content-length?\n\n     I don't recall seeing that.\n\n     Doug................\n\n\n\n\n\n\"Carl Kugler\" <kugler@us.ibm.com> on 04/27/2001 10:22:42 AM\n\n\n\n\n\n\n\n\n\n To:      Doug Dillon/HNS@HNS\n\n cc:      http-wg@cuckoo.hpl.hp.com\n\n\n\n Subject: Re: Whatever happenned to HTTP 1.1 Pipelining\n\n\n\n\n\n\n\n\n\n>     The biggest one that comes to mind is fact that one\n> of the responses in the pipeline may not have a content-length.\n> What could be done about that?\n\nUse Transfer-Encoding: chunked.\n\n     -Carl\n\n\n\n", "id": "lists-012-16597818"}, {"subject": "Re: Whatever happenned to HTTP 1.1 Pipelinin", "content": "On Fri, 27 Apr 2001 dillon@hns.com wrote:\n\n>      Thankyou for digging that up, but does it\n> mean that by a server SHOULD or MUST use self-defined message\n> lengths? This doesn't seem to give the browser author any assurance\n> that his pipelined requests won't be aborted by a premature connection\n> close due to a no CONTENT-LENGTH response.\n\nThen they have to resend the requsts to which they have not received \nresponses.  This is why only idempotent requests can be pipelined.\n\n> \n>      Seems to me that you only pipeline after you've gotton one\n> request through a connection because you don't know whether the\n> other end supports persistent connections. After that first request, which\n> handles the negotiation, you'd like to be able to pipeline with worry about the\n> pipeline being\n> cut off.\n\nYou always have to worry about the possibility that the pipeline is cut\noff.\n\n\n\n", "id": "lists-012-16609211"}, {"subject": "RE: Server Push: Multipart/X-MixedReplac", "content": "Thanks, but this answer is not related to the question. We already implemented\nfrom the documentation many years ago. Moreover, netscape 6 does not\nimplement server push (more code base forgetting).\n\nDoes anybody know the current status of this feature from netscape browsers\nprior to 6?\n\nThis is a useful feature and perhaps there should be an RFC documenting it\nso that it is preserved in mainstream browsers.\n\nOne useful application would be incremental redisplay, whereby the diffs\nare sent during the refresh.  This goes beyond the netscape spec, but this\nfacility could do it.  For example, one could provide element IDs that should be\nupdated with the new data.\n\n\nAt 21:34 +0200 2001-04-24, Miguel Estrada wrote:\n>Hi,\n>\n>You can find this info in the Netscape web site, bud is not implemented in\n>Internet Explorer.\n>\n>\n>Miguel Estrada\n>\n>\n>\n>----- Original Message -----\n>From: John C. Mallery <jcma@ai.mit.edu>\n>To: <http-wg@hplb.hpl.hp.com>\n>Sent: Sunday, April 22, 2001 10:03 PM\n>Subject: Server Push: Multipart/X-Mixed-Replace\n>\n>\n>> Does anybody know the current status of this feature from netscape\n>browsers\n>> prior to 6?\n>>\n>> This is a useful feature and perhaps there should be an RFC documenting it\n>> so that it is preserved in mainstream browsers.\n>>\n>> One useful application would be incremental redisplay, whereby the diffs\n>> are sent during the refresh.  This goes beyond the netscape spec, but this\n>> facility could do it.  For example, one could provide element IDs that\n>should be\n>> updated with the new data.\n>>\n>>\n>>\n>>\n\n\n\n", "id": "lists-012-16617305"}, {"subject": "LowResponse Time Secure Web Browsin", "content": "     Colleagues,\n\n      I'm interested in having HTTPS provide\nlow-response time web browsing over long latency\nlinks, such as Satellite. As you know, the extra handshakes\nrequired by TLS tend to increase the response time.\n\n     As such, some colleagues and I have put\ntogether a test bed and are looking at sniffer traces\nfrom various browser/web server combinations to\nsee whether implementations are using the best\nfeatures of HTTP and TLS to achieve low response time.\nInitial results indicate that they are not using best\npractice.\n\n     I see from the TLS working group that there is\nalready an informational RFC (2818) regarding the\nuse of HTTP over TLS, but that this document does not\naddress performance issues.\n\n     Would anyone on this mailing list be interested\nin seeing the results of our testing as they become available?\n\n     Would anyone be interested in reviewing/contributing\nto an informational RFC recommending best practice for\nlow response time?\n\n     Doug.....................\n\n\n\n", "id": "lists-012-16626497"}, {"subject": "Re: LowResponse Time Secure Web Browsin", "content": "You find some interest on the HTTP Compliance list at\n\n     http://groups.yahoo.com/group/http-compliance\n\n\n\n\n                                                                                                                   \n                    dillon@hns.com                                                                                 \n                                         To:     http-wg@cuckoo.hpl.hp.com                                         \n                    04/30/2001           cc:                                                                       \n                    08:34 AM             Subject:     Low-Response Time Secure Web Browsing                        \n                                                                                                                   \n                                                                                                                   \n                                                                                                                   \n\n\n\n\n\n     Colleagues,\n\n      I'm interested in having HTTPS provide\nlow-response time web browsing over long latency\nlinks, such as Satellite. As you know, the extra handshakes\nrequired by TLS tend to increase the response time.\n\n     As such, some colleagues and I have put\ntogether a test bed and are looking at sniffer traces\nfrom various browser/web server combinations to\nsee whether implementations are using the best\nfeatures of HTTP and TLS to achieve low response time.\nInitial results indicate that they are not using best\npractice.\n\n     I see from the TLS working group that there is\nalready an informational RFC (2818) regarding the\nuse of HTTP over TLS, but that this document does not\naddress performance issues.\n\n     Would anyone on this mailing list be interested\nin seeing the results of our testing as they become available?\n\n     Would anyone be interested in reviewing/contributing\nto an informational RFC recommending best practice for\nlow response time?\n\n     Doug.....................\n\n\n\n", "id": "lists-012-16634270"}, {"subject": "Re: questions regarding draft-ietf-http-authentication0", "content": "> >>>>> \"PL\" == Paul Leach <paulle@MICROSOFT.com> writes:\n[snip]\n> >> 8) Section 3.2.3: no words prohibit the server from sending, say, a qop\n> >> attribute but not a rspauth attribute. Also, while the cnonce is\n> >> required to be the same as used in the request, the nonce-count isn't.\n> >> Hence I propose the following change in wording:\n[snip]\n> \n>   This was a cut-and-paste error, I think; the nc-value is not used in\n>   the construction of the response-digest, so it need not be in the\n>   syntax for the Authentication-Info header at all.\n\nI beg to differ. The response-digest is the same as the request-digest\nexcept that A2 is computed differently, and the request-digest uses the\nnc-value in the computation.\n\n>   From client to\n>   server the requests may be pipelined, so we needed the nonce count\n>   to prevent replay, but each response is to exactly one unique\n>   request, so no count is needed or used.\n>\n> >> 9) Section 3.2.3: Actually, why are the cnonce and nonce-count attributes\n> >> sent in the Authorization-Info header? Or put differently, what makes\n> >> these two attributes special, as opposed to the nonce and uri (which\n> >> aren't sent back)?\n> \n>   See above for nonce-count; I thought that returning the cnonce value\n>   might make implementation in the client easier; the client can then\n>   use the same kind of self-validation that servers can with nonces so\n>   that they need not be actually stored while waiting for the response.\n\nThe nonce-count, nonce, and uri have to be kept anyway, so I'm not sure\nhow much easier it makes things (I just compare the received nc-value\nand cnonce against the ones in the request and puke if they don't\nmatch). However, there may well implementations which do this indirect\nvalidation and which would profit from an explicit cnonce, so I think\nit can be left in.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1667604"}, {"subject": "&quot;what does Keepalive: 300&quot; mean&quot", "content": "I was looking at the header of GET request sent by Mozilla.\nI came across this header field \"keep-alive: 300\".\nThis field is not present in the GET request sent by\nnetscape 4.7 and IE 5.0 .\n\n\nOne more doubt, If this field is particular to Mozilla\nbrowser, as my observation suggests, then How the server\nis supposed to understand this field ?\n\n------------------------------------------------------------------------------------------------------------\nRegards\n-Pawan\n\n\n\n", "id": "lists-012-16700259"}, {"subject": "Re: &quot;what does Keepalive: 300&quot; mean&quot", "content": "skgupta@hss.hns.com wrote:\n\n> One more doubt, If this field is particular to Mozilla\n> browser, as my observation suggests, then How the server\n> is supposed to understand this field ?\n\nRFC-2068, section 19.7.1\n\n--\n/================================================================\\\n|John Stracke    | http://www.ecal.com |My opinions are my own.  |\n|Chief Scientist |===============================================|\n|eCal Corp.      |I used to belong to a solipsism club, but I got|\n|francis@ecal.com|bored & voted everybody else out.              |\n\\================================================================/\n\n\n\n", "id": "lists-012-16707673"}, {"subject": "RE: &quot;what does Keepalive: 300&quot; mean&quot", "content": "> > One more doubt, If this field is particular to Mozilla\n> > browser, as my observation suggests, then How the server\n> > is supposed to understand this field ?\n>\n> RFC-2068, section 19.7.1\n\nNo, it's not that.\n\nThe pre-HTTP/1.1 persistent connection mechanism uses,\n\n  Connection: Keep-Alive\n\non the request side, and on the response side,\n\n  Connection: Keep-Alive\n\nand you might also see the Apache extension,\n\n  Keep-Alive: max=???; timeout=???\n\nMozilla is the only client I've come across which uses a Keep-Alive:\nheader on the request side.\n\nPresumably it's a hint to the server that the client will close the\nconnection after the specified number of seconds. I'm not entirely \nsure what the value of this hint is supposed to be tho'. It might be\nuseful if the default value were less than 2*MSL, in which case it \nwould help a busy server to decide whether leave the client to close \nor take the time wait hit itself. But the default seems to be 300 \nsecs.\n\nCheers,\n\n\nMiles\n\n-- \nMiles Sabin                                     InterX\nInternet Systems Architect                      27 Great West Road\n+44 (0)20 8817 4030                             Middx, TW8 9AS, UK\nmsabin@interx.com                               http://www.interx.com/\n\n\n\n", "id": "lists-012-16715857"}, {"subject": "Re: Proposal (I-D) for extending HTTP to support out-oforder response", "content": "On Wed, 11 Apr 2001, Jeffrey Mogul wrote:\n\n> Title: Support for out-of-order responses in HTTP\n> Author(s): J. Mogul\n> Filename: draft-mogul-http-ooo-00.txt\n> Pages: 14\n> Date: 09-Apr-01\n> \n> http://www.ietf.org/internet-drafts/draft-mogul-http-ooo-00.txt\n\nJust read the draft, overall it looks good.  The hop-by-hop approach\nseems to be the right one.  I could not discover any potential problems\nin interacting with correctly implemented legacy caches.\n\nSome comments:\n\n1. The end of section 3.1 got me thinking a bit:\n\n   A client SHOULD NOT send an RID header field with a request if the\n   semantics of the operation might not permit reordering.  A server\n   SHOULD NOT reorder a response if the semantics of the operation might\n   not permit reordering.  For example, a DELETE request probably ought\n   not to be reordered with other requests.\n\nThis language is really too vague, as it does not spell out the criteria\nfor knowing when \"the semantics of the operation might not permit\nreordering\". In theory, as people can layer whatever they want on top of\nGET, of course *any* two operations *might* not permit reordering, so what\nis a poor proxy in the middle to do?  So what is a hard criterion one can\ndefine for not permitting reordering?\n\nThe end of Section 8.1.2.2 (Pipelining) of RFC 2616 gives some guidance\nhere:\n\n   Clients SHOULD NOT pipeline requests using non-idempotent methods or\n   non-idempotent sequences of methods (see section 9.1.2). Otherwise, a\n   premature termination of the transport connection could lead to\n   indeterminate results. A client wishing to send a non-idempotent\n   request SHOULD wait to send that request until it has received the\n   response status for the previous request.\n\nSo it looks like correctly implemented clients will already avoid\npipelining for non-idempotent requests.  So there will not be a big\npotential loss of efficiency if all non-idempotent requests are are\ndefined as `does not permit reordering'.\n\nOn the other hand, 2616 seems to imply that pipelining idempotent requests\nis OK.  Also I believe it is considered to be legal for a 1.1 proxy to\nmultiplex one incoming request stream over multiple outgoing streams to\nthe same server.  So I believe 1.1 already implies that idempotent\nrequests could arrive out-of-order at the server -- is this correct?  If\nthat is the case then just reordering the responses to non-idempotent\nrequests should also be OK.\n\nSo it looks like 2616 implies that the hard criterion for not permitting\nre-ordering should be whether the request is non-idempotent.  However the\nexample in the draft \"a DELETE request probably ought not to be reordered\nwith other requests.\" seems to even broaden this, as DELETE *is*\nidempotent, though not safe.  So this implies a preference for broadening\nthe criterion to non permitting the re-ordering of unsafe requests???  \nProbably so.\n\nOne additional observations before I propose new language for the draft..  \nIn general I do not like the specification style where both sides of the\nwire are responsible for enforcing a protocol constraint (here the\nconstraint is not being able to do response reordering in some cases).\nWhile this style potentially creates a higher tolerance against bugs in\nimplementations it also adds some software complexity and limits the\nability to override such constraints in protocol extensions.  In this case\nI would like the policing of this constraint to be the responsibility of\nthe client only -- the server should trust that the client knows what it\nis doing, as it has potentially out-of-band information about\nreorderability.\n\nSo all in all I propose this new language to replace the qouted paragraph\nabove:\n\n   A client MUST NOT send an RID header field on an unsafe\n   request (all requests except GET and HEAD requests are unsafe),\n   unless it has out-of-band information that the potential\n   re-ordering of the response to the request is not a problem.  \n   A client MAY send a RID header on any safe request.\n\n\n\n2. The language in section 3.1 does not answer these questions:\n\n2A) If a proxy server gets a sequence of (idempotent) requests, all with a\nRID header, section 3.1 is currently silent as far as I can see on whether\nthis means that the requests may be forwarded upstream in a different\norder.\n\n2B) If an origin server gets a sequence of (idempotent) requests, all with\na RID header, section 3.1 is currently silent as far as I can see on\nwhether this means that the requests may be processed (this processing may\ngenerate side effects!!!) in a different order.\n\nSection 2.1 seems to imply that the answers to both these are `yes'.  \nAccording to my reasoning about multiplexing pipelined requests above,\nHTTP/1.1 implies that answering `yes' to both 2A) and 2B) should be safe.  \nHowever I am not completely sure if this is true for all HTTP applications\nin practice...\n\nIn any case, the draft should answer the above questions in the\n`specification' section, though I am not sure if the answer to A) should\nbe yes.\n\n3. The following language at the end of section 3.3 looked scary on first\nreading:\n\n   A proxy might need to establish a new inbound transport connection in\n   order to allow continued reordering of unrelated responses while\n   preserving the ordering constraints implied by the RID-Barrier header\n   (relative to a previous request that it has forwarded without yet\n   receiving a response).\n\nOn second reading I am assuming that \"unrelated responses\" means\n`responses to requests gotten from a different client', which would make\nthe language non-scary.  Though I am not 100% sure that this is what was\nmeant, so the language probably needs to be clarified.\n\n4. The purpose of rid-barrier seems to be to separate on the whole path to\nthe origin server 2 sets of reorderable request sequences for which the\nresponses should not be mixed among the sequences. HOWEVER if a proxy\nsomwhere along this path multiplexes the request stream onto 2 (or more)\ndifferent outgoing connections (this might easily happen especially if not\nall requests are on the same origin server) then one of the streams will\nhave the rid-barrier missing, which means that the (parts of the) 2\nrequest sequences in this sequence with missing rid-barrier become\nre-orderable again. So it looks like rid-barrier will not always work for\nits intended use.\n\nAlternatives: \n\nA1) instead of sending rid-barrier have the client `drain the pipeline'\n(as HTTP/1.1 suggests you do before a non-idempotent request), this will\nprevent any mixing between the two sequences.\n\nA2) change the RID field to add an extra identifier which denotes a group\nof requests with reorderable responses.\n\ni.e.  the sequence (each line a requesr)\n\nRID: 1\nRID: 2\nRID-BARRIER:\nRID: 3\nRID: 4\n\nbecomes\n\nRID: A,1\nRID: A,2\n<non-rid normal request, could also be ommitted>\nRID: B,3\nRID: B,4\n\nand then one can put a constraint on any server that responses can only be\nreordered when the first token in the RID field is the same.\n\nI would prefer alternative A1), A2) seems to be overkill.\n\nNote that for the same reason that rid-barrier does not always work, a\nnonsafe request between two rid-sequences can also not be counted on to\nprevent re-ordering of the responses (and requests too???) between the\nsequences further upstream. It looks like to only way to be sure is to\ndrain the pipeline.  This should be documented in the draft!  If the\nanswer to question 2A above is `yes' we now have the somewhat non-obvious\nresult that if the client sends through some proxies\n\nRID: 1\nRID: 2\nRID-BARRIER or non-safe\nRID: 3\nRID: 4\n\nwithout waiting anywhere for the pipeline to drain, it is entirely\npossible that the origin server will see\n\nRID: 4\nRID: 3\nRID-BARRIER or non-safe\nRID: 2\nRID: 1\n\nbecause a proxy in the middle multiplexed the `RID-BARRIER or non-safe'\nonto a different connection than the other requests.\n\n5. About the security considerations: if a client sends\n\nGET /a RID: 1\nGET /b RID: 2\n\nthen resource /a could potentially respond with\n\n200 OK\nRID: 2\nbla bla bla\n\nthus spoofing a reponse from resource /b!!!  Resource /a might then get\nthe server to abort or hang the connection -- a client or proxy getting\nthe single response might then cache or use the response as a valid\nresponse from /b without suspecting that anything is wrong. If /b is a\nknown URL at which to get a certificate this will be a problem...  The\nattacker /a might be able to put up a web page that makes many browsers\nemit the above two requests with a high predictability. \n\n\nIn general a client cannot assume that two resources on the other end of a\nhop-to-hop connection are in the same trust domain.  Also in the\narrangement\n\nrid-aware client --- 1.1 proxy not rid-aware -- origin server with /a\n                                           \\\n                                            ---- origin server with /b\n\nunless the client has checks to prevent this, the resource /a could spoof\n/b which is on a different origin server by sending a `rid: 2' header not\nprotected by a connection header through the proxy.\n\nOverall it looks like tight sanity checking of the responses by the\nclient, and deferring the use of the response for something important\n(like caching, saving, certification) until all responses are received and\nchecked, will prevent many (maybe all?) spoofing attacks.  \nBut a more complete analysis is needed for sure.\n\n\nOK, these are all my comments.  I only thought of some of these points\nwhile writing the message, so the message has become a bit less structured\nthan I intended, for this I apologize.\n\n\nKoen.\n\n\n\n", "id": "lists-012-16724316"}, {"subject": "wrt: Web Protocols and Practice: HTTP/1.1, Networking Protocols,  Caching, and Traffic Measuremen", "content": "So, have folks on this list reviewed or otherwise seen this book?..\n\nWeb Protocols and Practice: HTTP/1.1, Networking Protocols, Caching,\nand Traffic Measurement \n Balachander Krishnamurthy \n Jennifer Rexford \n Copyright 2001, 672 pp. \n ISBN 0-201-71088-9\n\n http://www.awlonline.com/product/0,2627,0201710889,00.html\n\n\n[i note Balachander posted to this list back in Oct-2000]\n\n\nAnyway, it sounds like it is likely a \"good book\" and worth having, but I'd \nprefer to get some independent confirmation before parting with the $.\n\nTo anticipate the question -- yes, given the reprint of the \"flap blurbs\" on \nthe amazon page..\n\n  http://www.amazon.com/exec/obidos/ASIN/0201710889/\n\n..it sounds like it nominally covers the ground I'm interested in knowing more \nabout (e.g. Web caching and multimedia streaming, the Apache Web server, Squid \nproxy, and traffic measurement techniques).\n\nthanks,\n\nJeffH\n\n\n\n", "id": "lists-012-16741945"}, {"subject": "Re: wrt: Web Protocols and Practice: HTTP/1.1, Networking Protocols, Caching, and Traffic Measuremen", "content": "In message <200105181602.JAA02835@breakaway.Stanford.EDU>,\nJeff.Hodges@kingsmountain.com writes:\n>So, have folks on this list reviewed or otherwise seen this book?..\n>\n>Web Protocols and Practice: HTTP/1.1, Networking Protocols, Caching,\n>and Traffic Measurement \n> Balachander Krishnamurthy \n> Jennifer Rexford \n> Copyright 2001, 672 pp. \n> ISBN 0-201-71088-9\n>\n> http://www.awlonline.com/product/0,2627,0201710889,00.html\n>\n>Anyway, it sounds like it is likely a \"good book\" and worth having, but I'd \n>prefer to get some independent confirmation before parting with the $.\n\nI think my review is on the back cover, but not online.  This is what\nI wrote:\n\nWeb Protocols and Practice, authored by Balachander Krishnamurthy and\nJennifer Rexford, provides a comprehensive examination of the network\nprotocols and software implementations that have made the World Wide Web\nwhat it is today: the most wide-spread and pervasive application of\ncomputers ever invented.  This is the first book to delve beyond the\ntypical user experience of the Web and analyze the operation and behavior\nof Web browsers, intermediaries, and servers in the same way that a\nmechanic's manual would describe the intended operation of an automobile.\nYou need this book if you want to do more than than just poke around\nunder the hood.\n\nSufficient background material is provided for readers wishing to\nunderstand the basics of the Web infrastructure, but the book will\nprimarily benefit those who need to know more than what is defined\nin the standard Internet protocol specifications.  The chapters on\nMeasuring and Characterizing Web Traffic should be required reading\nfor anyone managing a website or developing Web software.  Likewise,\nnetwork planners and administrators will find the discussion on the\ninteraction between HTTP and other Internet protocols (IP, TCP, and DNS)\nto be invaluable for anticipating and preventing the types of network\nfailure that can cause a company to fall off the Internet.\n\n\nCheers,\n\nRoy T. Fielding, Chief Scientist, eBuilt, Inc.\n                 2652 McGaw Avenue\n                 Irvine, CA 92614-5840  fax:+1.949.609.0001\n                 (fielding@ebuilt.com)  <http://www.eBuilt.com>\n\n\n\n", "id": "lists-012-16751292"}, {"subject": "sample http forma", "content": "  I need to write from a legacy package http formatted data to a servlet. I\nwill be using tcp communication.\n\n\n Can somebody send me a real life format of the actual http generated\nmessage I must send to the\n\n  servlet?\n\n\n is it a Post method?\n\n\n               thanks,\n\n\n             Michael\n\n\n\n", "id": "lists-012-16762134"}, {"subject": "call for papers: Internet Measurement Worksho", "content": "ACM SIGCOMM Internet Measurement Workshop 2001\nNovember 1-2, 2001\nSan Francisco Bay Area\n\nThis ACM SIGCOMM Internet Measurement Workshop is a one and a half \nday event focusing on Internet measurement and analysis. Submissions \nshould contribute to the current understanding of how to collect or \nanalyze Internet measurements, or give insight into how the Internet \nbehaves. Examples of relevant topics are:\n\n* Workload characterization\n* Traffic engineering\n* Web measurements\n* Inter-domain and intra-domain routing\n* Active measurement techniques\n* Passive measurement techniques\n* Anonymization/privacy issues\n* Calibration\n* Measurement-based inference of network properties\n* Efficacy of content distribution networks\n* Reassessment/testing of previous measurement findings\n* Assessment of previous simulation/testbed findings\n\nSubmissions on new ideas and work-in-progress are encouraged. Papers\nthat do not in some fashion rely on measuring Internet properties are\nout of scope.\n\nThe workshop is sponsored by ACM SIGCOMM.  In addition to the published\nproceedings, the Program Committee may also select a few papers for\nfast-track submission for possible publication in IEEE/ACM Transactions\non Networking.\n\nAttendance will be limited to 50 participants, with priority given to\nauthors of accepted papers, program committee members, and authors of\nsubmitted papers.\n\nThe workshop is open to three forms of submissions:\n\n* full papers (up to 15 pages) should exhibit succinctness appropriate \n  to the topics and themes they discuss.\n\n* extended abstracts (up to 4 pages), conveying work expected to mature\n  somewhat between submission and presentation at the workshop.\n\n* brief abstracts (less than a single page) to be considered for a \n  possible work-in-progress session.\n\nSubmissions must be in electronic form, as plain text, Postscript,\nor PDF documents, following the instructions found at:\n\n        http://www.aciri.org/vern/sigcomm-imeas-2001.submit.html\n\nAll manuscripts must be in English. The top of the first page of each\npaper should include the title of the paper, the authors and their\naffiliations, and the full address for the contact author (e-mail, \nphone, fax, mailing address).\n\nPapers and extended abstracts accepted for presentation at the workshop\nwill be published by ACM in proceedings.\n\nImportant dates\n---------------\n\n        June 29, 2001: HARD submission deadline \nAugust 3, 2001: Notification\nAugust 31, 2001: Camera Ready Copy due\n\nSteering committee\n------------------\nChristophe Diot, Sprint ATL (cdiot@sprintlabs.com)\nBalachander Krishnamurthy, AT&T--Labs Research (bala@research.att.com)\nVern Paxson, ACIRI (vern@aciri.org)\nJennifer Rexford, AT&T--Labs Research (jrex@research.att.com)\n\nProgram committee\n-----------------\n\nMark Allman (NASA GRC, USA)\nMartin Arlitt (Hewlett-Packard Labs, USA)\nPaul Barford (U. of Wisconsin, USA)\nAnja Feldmann (Univ. Saarbruecken, Germany)\nGeoff Huston (Telstra, Australia)\nJeffrey Mogul (Compaq WRL, USA)\nHenk Uijterwaal (RIPE NCC, Netherlands)\n\n\n\n", "id": "lists-012-16769300"}, {"subject": "RE: comments on draft-ietf-http-authentication01.tx", "content": "I'm going to hold with the assumption that we will be cutting no\nfeatures from the HTTP spec. In order to avoid too long a delay,\nI'd like to see if we can process proposed editorial changes as quickly\nas possible.\n\nBecause there will not be another working group LAST CALL on the\nentire document, we should send out and review proposed wording\nchanges as the issues come up and are resolved (for the base spec).\n\nFor the authentication draft, it looks like there are sufficient\neditorial changes that we probably should do another draft (in the\nnext week or two, authors willing.)\n\nIt seems that filling out the implementation reports is causing\na careful review of the documents, which is good! \n\nI think we're on track to finish by the end of April.\n\n\n\nLarry\n\n\n\n", "id": "lists-012-1677199"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "> For the features you've listed as wanting a server to test\n> against, I've asked the 'contact' for the private reports\n> directly.\n\nThanx.\n\n> The implementation reports were:\n> \n> h 10.3.6 Use Proxy\n>          1 other, 2 clients, 1 proxy\n> h 10.3.7 Temporary Redirect\n>          1 other, 3 clients, 1 proxy, 1 combined proxy/server (tested)\n> h 10.4.9 Request Timeout\n>          1 other, 2 clients\n> h 14.37 Retry-After\n>          1 other, 1 origin\n> H 14.39 TE\n>          1 other, 2 clients\n> H 14.40 Trailer\n>          1 other, 2 clients\n> H 14.41 Transfer-Encoding\n>          1 other, 1 client, 2 origin, 1 combined\n\nWhat is \"other\"? A Gateway?\n\n> I thought I'd add some notes from the test reports:\n[snip]\n> \n> H 10.2.3, 4, 6, 10.3.1,2,4,6,7; 10.4.3, 10: \"plugin or cgi could\"\n\nHmm, cgi's could do most of the things...\n\n> 10.4.9 408 Request Timeout\n> (server) would like to have general trailer for this\n\nI don't understand this one. Is a new header field wanted? If so, what\nwould be gained?\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1686387"}, {"subject": "RE: questions regarding draft-ietf-http-authentication0", "content": "> Quite possibly. They may have renumbered the HTTP/1.1 spec while we weren't\n> noticing.\n\nAsleep at the switch I see...\n\nI did put an entry in the issues list page called XREF's, to remind you\nto check the cross references.\n> \n> > 11) What's the status of the AUTH-INFO-SYNTAX issue? The issues page\n> >     http://www.w3.org/Protocols/HTTP/Issues/\n> lists the status as\n> > \"Drafting\",\n> >     but it's not in the current draft.\n> >\n> I never saw it. I don't know why it's closed. I'll make the change it\n> requested.\n> \n>\n\nProbably my mistake on the issues list; I did a very quick edit of the \nIssues list late last week to try to bring it up to date (and try out \nthe 1.2a version of our Amaya browser/editor on it; Amaya did quite well, \neven on the Issues list tables from H--- that had previously thrown it \nfor a loop; you might take it for a test drive :-)).\n\nLet me know what issues should have which state...  (or I can tell you\nhow to use Jigedit to edit it yourself, Paul, if you want to be adventurous.\nOf course, you'll have to find a tool which supports PUT, which leaves\nthe Microsoft tools out :-(.)\n- Jim\n\n\n\n", "id": "lists-012-1694899"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "** Reply to note from Ronald.Tschalaer@psi.ch (Life is hard... and then you die.) Mon, 30 Mar 1998 00:20:01 +0200\n\nImagine for a moment that I have a proxy server.\nI start to fetch a page for a client, and return the headers.\nThere are many objects that don't come with Content-Length (e.g. FTP).\n\nIf it is not cacheable I won't buffer it; even if it is cacheable, the\nfirst client doesn't get a Content-Length because I don't buffer the\nentire object before sending the response (to reduce latency).\n\nImagine the request times out, or a shark eats the trans-Atlantic cable\nthe object is being transfered over, or whatever.  Now, I have to close\nthe connection to the client, who recieves a truncated object with no\nindication of an error (until he tries to use it and finds it is\ncorrupted).  There is no possiblity of reporting what the problem is\neither.\n\n>   \n> > 10.4.9 408 Request Timeout\n> > (server) would like to have general trailer for this\n>   \n> I don't understand this one. Is a new header field wanted? If so, what\n> would be gained?\n>   \n>   \n>   Cheers,\n>   \n>   Ronald\n>   \n> \n \n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-1704560"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "    Imagine for a moment that I have a proxy server.  I start to fetch\n    a page for a client, and return the headers.  There are many\n    objects that don't come with Content-Length (e.g. FTP).\n\n    If it is not cacheable I won't buffer it; even if it is cacheable,\n    the first client doesn't get a Content-Length because I don't\n    buffer the entire object before sending the response (to reduce\n    latency).\n\n    Imagine the request times out, or a shark eats the trans-Atlantic\n    cable the object is being transfered over, or whatever.  Now, I\n    have to close the connection to the client, who recieves a\n    truncated object with no indication of an error (until he tries to\n    use it and finds it is corrupted).  There is no possiblity of\n    reporting what the problem is either.\n    \nMaybe I'm missing something, but this seems like an ideal scenario\nfor \"Chunked\".  The proxy doesn't have to buffer the entire file,\nbut it can buffer pieces thereof (say, 8KB chunks, 1KB chunks,\nwhatever).  Since the Chunked transfer-encoding does have a mechanism\nto indicate end-of-content (i.e., sending a zero-length chunk), it's\nnow unambiguous whether the client has received a truncated object\nor not.\n\nOf course, if the client doesn't speak HTTP/1.1, then you can't\nuse \"chunked\".  But this is the situation today, right?\n\nAn HTTP/1.1 proxy could (in theory) buffer the entire entity-body\nif it knows that the client doesn't understand \"chunked\".  But I'm\nnot sure this is really the right approach, since I suspect it\nwill simply lead to bored HTTP/1.0 users hitting \"stop\" and \"reload\",\nand that isn't something we should encourage.\n\n-Jeff\n\n\n\n", "id": "lists-012-1713513"}, {"subject": "Re: Comments on draft-ietf-http-v11-spec-rev0", "content": "Jeffrey Mogul:\n>\n>Koen Holtman wrote:\n>\n>> - Section 13.10:\n>> \n>> This section introduces a new (as far as I can see) requirement:\n>> \n>> #  A cache that passes through requests for methods it does not understand\n>> #  should invalidate any entities referred to by the Request-URI.\n>> \n>> This may seem like a good safety measure on the surface but I think\n>> that it is in fact quite damaging.  First, designers of new methods\n>> cannot benefit much from the above rule because 1.0 and 2068 caches\n>> will not adhere to it.  On the other hand, the new rule introduces a\n>> performance penalty for new methods which do not in fact cause any\n>> invalidation.  One such method would be M-GET, a GET extended with a\n>> mandatory extension, for example.  The performance penalty blocks\n>> implied by the new rule makes certain ways of extending the protocol\n>> too expensive and thus shortens the lifetime of the 1.x suite.  I want\n>> the requirement to be removed.\n>\n>Dave Kristol wrote:\n>    I think I'm the instigator of this change.  While your example\n>    seems benign enough, the danger is from methods that change the\n>    underlying object, e.g., M-PUT.    The object in the cache would no\n>    longer look like the one at the origin server and must be\n>    invalidated.  In the absence of a way to tell intervening caches to\n>    invalidate their view of the object the proxy cache has to do so by\n>    default.\n>\n>    I suppose a compromise would be for a cache to mark a cached object\n>    as \"must-revalidate\" when it sees an unknown method that it passes\n>    along.  Cache experts:  would that work?\n>    \n>How does\n>mark the cached object as \"must-revalidate\"\n>differ from\n>invalidate the cached object\n>\n>except that the former propagates the change to outbound caches?\n>\n>I'm not sure that Koen would view this as a compromise :-)\n\nYou are right, I don't.\n\n>Would it work?  Well, the concept of invalidation-based protocols\n>is in general not supported by HTTP.  My preference is to err on\n>the side of transparency rather than performance, although I agree with\n>Koen that the transparency in this case might be somewhat illusory.\n\nI agree that Dave's new rule would add some extra safety to protect\nagainst outdated cache entries, but no absolute safety.  The HTTP\ncaching system was never designed to offer absolute safety.  The\nsystem is unsafe in several ways: there may be a mesh of proxy caches\nso that some caches don't see the passing M-PUT, a proxy cache may\nswitch to gateway mode, thus avoiding having to care about any rule\napplying to proxies, and legacy proxy caches won't invalidate anyway.\n\nThe amount of safety added by the new rule is so small that it does\nnot outweigh the loss in extensibility.  Especially considering that a\ndesigner of a new method can have the same extra safety that would be\nprovided by the new rule -- IF he wants it -- by including a\ncontent-location header in the response to the new method (see section\n16.3 last paragraph).\n\nSo as the safety/transparency side is very weak, I'd rather err on the\nside of extensibility.\n\n\n>But I'm not sure what the problem is; my understanding is that\n>the whole point of creating the M-GET method is to prevent\n>\"proxies that do not understand the method\" from forwarding it.\n\nHmm, as far as I recall, proxies which don't understand the method\n_will_ forward it in general.  It is origin servers which are expected\nto return an error message.\n\n>I.e., they are supposed to return 501 (Not Implemented) or act\n>as a tunnel (i.e., not cache anything).\n\nIf they act as a tunnel, they also won't invalidate anything in the\nexisting cache memory, which was the whole point of the rule.\n \n>So any caching proxy that does forward M-GET does \"understand\" it, and\n>isn't covered by the requirement that Koen objects to.\n>\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-012-1722694"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "Jeffrey Mogul:\n>\n\n[rlgray@raleigh.ibm.com:]\n>    Imagine the request times out, or a shark eats the trans-Atlantic\n>    cable the object is being transfered over, or whatever.  Now, I\n>    have to close the connection to the client, who recieves a\n>    truncated object with no indication of an error (until he tries to\n>    use it and finds it is corrupted).  There is no possiblity of\n>    reporting what the problem is either.\n>    \n>Maybe I'm missing something, but this seems like an ideal scenario\n>for \"Chunked\".\n\nHmm, I always thought it would be possible to generate a tcp-level\nerror (connection abort?) instead of closing the connection to the\nclient in the normal way.  Wouldn't that tell the client that\nsomething is wrong?\n\nKoen.\n\n\n\n", "id": "lists-012-1734192"}, {"subject": "new issue: confusing statement about HTTP/1.1 version number", "content": "Section 3.1 (HTTP Version) of the latest draft currently includes\nthis statement:\n\n  Applications sending Request or Response messages, as defined by this\n  specification, MUST include an HTTP-Version of \"HTTP/1.1\". Use of this\n  version number indicates that the sending application is at least\n  conditionally compliant with this specification.\n\nI think this is at least confusing, and possibly wrong.  For example,\nthe first sentence directly contradicts some statements in RFC2145,\nwhich is cited a few paragraphs earlier.  The parenthetical statement\nis also odd, since a completely normal HTTP/1.0 implementation could\nsend messages that are defined by \"this specification\".\n\nThe most important statement that we need to make here is the second\nsentence in the paragraph, and I think this is where the MUST needs\nto be.\n\nI propose rewriting this pararagraph to be:\n\n   An application that sends a Request or Response message that\n   includes HTTP-Version of \"HTTP/1.1\" MUST be at least conditionally\n   compliant with this specification.  Applications that are at least\n   conditionally compliant with this specification SHOULD use an\n   HTTP-Version of \"HTTP/1.1\" in their messages, and MUST do so for any\n   message that is not compatible with HTTP/1.0.  For more details on\n   when to send specific HTTP-Version values, see RFC 2145 [36].\n\nI.e.,\nyou MUST NOT say you're HTTP/1.1 unless you comply.\nif you do comply, you SHOULD say so.\nif your message isn't intelligible to an HTTP/1.0 recipient,\nsay you're HTTP/1.1.\n\n-Jeff\n\n\n\n", "id": "lists-012-1743007"}, {"subject": "new editorial issue: redundancy in 3.7.2 (Multipart Types", "content": "Section 3.7.2 includes these two paragraphs:\n  In general, HTTP treats a multipart message-body no differently than any\n  other media type: strictly as payload. The one exception is the\n  \"multipart/byteranges\" type (appendix 19.2) when it appears in a 206\n  (Partial Content) response, which will be interpreted by some HTTP\n  caching mechanisms as described in sections 13.5.4 and 14.16. In all\n  other cases, an HTTP user agent SHOULD follow the same or similar\n  behavior as a MIME user agent would upon receipt of a multipart type. If\n  an application receives an unrecognized multipart subtype, the\n  application MUST treat it as being equivalent to \"multipart/mixed\". The\n  MIME header fields within each body-part of a multipart message-body do\n  not have any significance to HTTP beyond that defined by their MIME\n  semantics.\n\n  In general, an HTTP user agent SHOULD follow the same or similar\n  behavior as a MIME user agent would upon receipt of a multipart type. If\n  an application receives an unrecognized multipart subtype, the\n  application MUST treat it as being equivalent to \"multipart/mixed\".\n\nThe phrase \"an HTTP user agent SHOULD follow the same or similar\nbehavior as a MIME user agent would upon receipt of a multipart type\"\nappears twice here, once in each sentence.  The first occurrence\nis qualified by (in effect) \"except for 'multipart/byteranges'\";\nthe second occurrence has no specific exceptions, but just says\n\"in general\".\n\nIt seems confusing to have two such similar statements, with\nslightly different qualifications.  It also seems odd to use\nthe normative term \"SHOULD\" in connection with an \"in general\".\n\nSo I propose changing the second paragraph cited above by removing\nthe first sentence, so that it reads simply:\n\n  If an application receives an unrecognized multipart subtype, the\n  application MUST treat it as being equivalent to \"multipart/mixed\".\n\n-Jeff\n\n\n\n", "id": "lists-012-1752010"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "On Mon, 30 Mar 1998, Jeffrey Mogul wrote:\n\n> Maybe I'm missing something, but this seems like an ideal scenario\n> for \"Chunked\".  The proxy doesn't have to buffer the entire file,\n> but it can buffer pieces thereof (say, 8KB chunks, 1KB chunks,\n> whatever).  Since the Chunked transfer-encoding does have a mechanism\n> to indicate end-of-content (i.e., sending a zero-length chunk), it's\n> now unambiguous whether the client has received a truncated object\n> or not.\n\nSure, but this forces a close of the client connection since there is no\ntrailer header to indicate an error.\n\nDave\n\n\n\n", "id": "lists-012-1760829"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "On Mon, 30 Mar 1998, Koen Holtman wrote:\n\n> Jeffrey Mogul:\n> >\n> \n> [rlgray@raleigh.ibm.com:]\n> >    Imagine the request times out, or a shark eats the trans-Atlantic\n> >    cable the object is being transfered over, or whatever.  Now, I\n> >    have to close the connection to the client, who recieves a\n> >    truncated object with no indication of an error (until he tries to\n> >    use it and finds it is corrupted).  There is no possiblity of\n> >    reporting what the problem is either.\n> >    \n> >Maybe I'm missing something, but this seems like an ideal scenario\n> >for \"Chunked\".\n> \n> Hmm, I always thought it would be possible to generate a tcp-level\n> error (connection abort?) instead of closing the connection to the\n> client in the normal way.  Wouldn't that tell the client that\n> something is wrong?\n\nI don't believe you can count on clients differentiating all cases and \ntreating the abort as an error.\n\nDave Morris\n\n\n\n", "id": "lists-012-1769555"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "[Richard Gray wrote:]\n> Imagine for a moment that I have a proxy server.\n> I start to fetch a page for a client, and return the headers.\n> There are many objects that don't come with Content-Length (e.g. FTP).\n> \n> If it is not cacheable I won't buffer it; even if it is cacheable, the\n> first client doesn't get a Content-Length because I don't buffer the\n> entire object before sending the response (to reduce latency).\n> \n> Imagine the request times out, or a shark eats the trans-Atlantic cable\n> the object is being transfered over, or whatever.  Now, I have to close\n> the connection to the client, who recieves a truncated object with no\n> indication of an error (until he tries to use it and finds it is\n> corrupted).  There is no possiblity of reporting what the problem is\n> either.\n\nThanx for the explanation. I guess the problem is that http provides no\nway to differentiate between an application level error (after the header\nhas been sent) and a network level error; the former must be signaled via\nthe latter. Assuming that application level errors are not very common\nthis is probably not such a major issue. However, it's not very pretty.\n\n[Jeffrey Mogule wrote:]\n> Maybe I'm missing something, but this seems like an ideal scenario\n> for \"Chunked\".  The proxy doesn't have to buffer the entire file,\n> but it can buffer pieces thereof (say, 8KB chunks, 1KB chunks,\n> whatever).  Since the Chunked transfer-encoding does have a mechanism\n> to indicate end-of-content (i.e., sending a zero-length chunk), it's\n> now unambiguous whether the client has received a truncated object\n> or not.\n\nI agree. Closing the connection prematurely on a chunked transmission\nis probably the best way to go (when talking to HTTP/1.1 clients).\n\n[Koen Holtman wrote:]\n> Hmm, I always thought it would be possible to generate a tcp-level\n> error (connection abort?) instead of closing the connection to the\n> client in the normal way.  Wouldn't that tell the client that\n> something is wrong?\n\nYes, it would, *IF* you can generate an abort - that's not always\npossible.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1779276"}, {"subject": "new issue: confusing statement about HTTP/1.1 version number", "content": "I agree.\n\n** Reply to note from Jeffrey Mogul <mogul@pa.dec.com> Mon, 30 Mar 98 14:16:30 PST\n>   \n> Section 3.1 (HTTP Version) of the latest draft currently includes\n> this statement:\n>   \n>   Applications sending Request or Response messages, as defined by this\n>   specification, MUST include an HTTP-Version of \"HTTP/1.1\". Use of this\n>   version number indicates that the sending application is at least\n>   conditionally compliant with this specification.\n>   \n> I think this is at least confusing, and possibly wrong.  For example,\n> the first sentence directly contradicts some statements in RFC2145,\n> which is cited a few paragraphs earlier.  The parenthetical statement\n> is also odd, since a completely normal HTTP/1.0 implementation could\n> send messages that are defined by \"this specification\".\n>   \n> The most important statement that we need to make here is the second\n> sentence in the paragraph, and I think this is where the MUST needs\n> to be.\n>   \n> I propose rewriting this pararagraph to be:\n>   \n>    An application that sends a Request or Response message that\n>    includes HTTP-Version of \"HTTP/1.1\" MUST be at least conditionally\n>    compliant with this specification.  Applications that are at least\n>    conditionally compliant with this specification SHOULD use an\n>    HTTP-Version of \"HTTP/1.1\" in their messages, and MUST do so for any\n>    message that is not compatible with HTTP/1.0.  For more details on\n>    when to send specific HTTP-Version values, see RFC 2145 [36].\n>   \n> I.e.,\n> you MUST NOT say you're HTTP/1.1 unless you comply.\n> if you do comply, you SHOULD say so.\n> if your message isn't intelligible to an HTTP/1.0 recipient,\n> say you're HTTP/1.1.\n>   \n> -Jeff\n>   \n> \n \n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-1788597"}, {"subject": "Re: new issue: confusing statement about HTTP/1.1 version number", "content": "I too find Jeffs wording to be an improvement, and don't believe that in\nany way changes the intent (that is, I think that it is editorial).\n\n\n\n", "id": "lists-012-1797836"}, {"subject": "Multiple ProxyAuthenticate challenges", "content": "Is there a good reason why WWW-Authenticate can have multiple\nchallenges while Proxy-Authenticate can't?\n\ndraft-ietf-http-v11-spec-rev-03:\n\n>         Proxy-Authenticate  = \"Proxy-Authenticate\" \":\" challenge\n>         WWW-Authenticate  = \"WWW-Authenticate\" \":\" 1#challenge\n\nI also think that the following sentence from the description of\nWWW-Authenticate fits better if moved to the draft-ietf-http-authentication\ndocument as it is the one that define \"challenge\".\n\n>                                                          User agents\n>  MUST take special care in parsing the WWW-Authenticate field value if it\n>  contains more than one challenge, or if more than one WWW-Authenticate\n>  header field is provided, since the contents of a challenge may itself\n>  contain a comma-separated list of authentication parameters.\n\n\nRegards,\nGisle\n\n\n\n", "id": "lists-012-1805884"}, {"subject": "More comments on draft-ietf-http-authentication01.tx", "content": "I just implemented \"Digest Authentication\" for the LWPng (the thing to\nreplace libwww-perl at some point) based on\n<draft-ietf-http-authentication-01>.  I have only tested it against a\nserver that implement the RFC 2069 spec.  I have the following\ncomments to the spec (trying to avoid things already discussed on this\nlist like how to format the 'nc-value').\n\n\n1) It is not perfectly clear to me if the namespace of realms are\nseparate for each authentication scheme or even separate when using\nthe server as a proxy and when not.  I matters to me becase I try to\nindex authentication objects in the client per \"server/realm\".\n\nThis sentence:\n\n  The realm value (case-sensitive), in combination with the canonical\n  root URL (see section 5.1.2 of [2]) of the server being accessed,\n  defines the protection space.\n\nmake me believe that I can assume that it would not be possible to\nhave to deal with all the following as separate \"protection spaces\"\nthe same time:\n\n   Authorization: Basic realm=\"foo\"\n   Authorization: Digest realm=\"foo\"\n   Proxy-Authorization: Basic realm=\"foo\"\n   Proxy-Authorization: Digest realm=\"foo\"\n\nCan this be clarified?\n\n\n\n2) How to interpret the URIs in the 'domain' attribute of the Digest\nWWW-Authenticate is not clear to me either.  Are the URIs in this\nlist path prefixes that define protection spaces or must there be\nan exact match.  If it is a prefix, what happens if the URI contains a\nquery part.\n\n\n\n3) Section 3.6 does not seem to know about the \"407 Proxy\nAuthentication Required\" status code.  I though that a\n\"Proxy-Authenticate\" header could only be present in a 407 response.\nThat is at least how I read <draft-ietf-http-v11-spec-rev-03>.  I\nwould like this paragraph to go away (it would complicate my\nimplementation if it stays):\n\n  Note that in principle a client could be asked to authenticate\n  itself to both a proxy and an end-server. It might receive an\n  \"HTTP/1.1 401 Unauthorized\" header followed by both a WWW-\n  Authenticate and a Proxy-Authenticate header. However, it can\n  never receive more than one Proxy-Authenticate header since such\n  headers are only for immediate connections and must not be passed\n  on by proxies. If the client receives both headers, it must\n  respond with both the Authorization and Proxy-Authorization\n  headers as described above, which will likely involve different\n  combinations of username, password, nonce, etc.\n\n\n\n4) Are there any servers that already implemented that that knows\nabout \"MD5-sess\" and \"auth-int\" I can test against?\n\n\n\nFor those that can read Perl, I even include the source for my\nimplementation here.  This code might even be of use to others\nimplementing Digest or perhaps somebody can tell me if I misunderstood\nsomething in the spec by looking through the code.\n\nRegards,\nGisle\n\n\n\n---------------------------------------------\npackage LWP::Authen::digest;\nuse strict;\n\n# Based on <draft-ietf-http-authentication-01>\n\nrequire MD5;\n\nsub new\n{\n    my $class = shift;\n    my $self = bless { @_ }, $class;\n    # All the WWW-Authenticate attributes are now available to the\n    # LWP::Authen::digest object as $self->{'<attr>'}.\n    $self;\n}\n\nsub _set_authorization\n{\n    my($self, $header, $req) = @_;\n    my $user = $self->{username};\n    return unless defined $user;\n    my $pass = $self->{password};\n\n    my $realm = $self->{realm};\n    $realm = \"\" unless defined $realm;\n\n    my $algorithm = lc($self->{algorithm} || \"md5\");\n    my %qops = map {$_ => 1} split(/\\s*,\\s*/, lc($self->{qop} || \"\"));\n    my $qop = \"\";\n    if ($req->has_content && $qops{'auth-int'}) {\n$qop = \"auth-int\";\n    } elsif ($qops{auth}) {\n$qop = \"auth\";\n    }\n    \n    my $uri = $req->url->full_path;\n    my $nonce = $self->{nonce};  $nonce = \"\" unless defined $nonce;\n    my $nc = sprintf \"%08x\", ++$self->{nonce_count};\n    my $cnonce = sprintf \"%x\", rand(0x1000000);\n\n    my $a1;\n    if ($algorithm eq \"md5\") {\n# A1 = unq(username-value) \":\" unq(realm-value) \":\" passwd\n$a1 = MD5->hexhash(\"$user:$realm:$pass\");\n    } elsif ($algorithm eq \"md5-sess\") {\n# The following A1 value should only be computed once\n$a1 = $self->{'A1'};\nunless ($a1) {\n    # A1 = H( unq(username-value) \":\" unq(realm-value) \":\" passwd )\n    #         \":\" unq(nonce-value) \":\" unq(cnonce-value)\n    $a1 = MD5->hexhash(\"$user:$realm:$pass\") . \":$nonce:$cnonce\";\n    $a1 = MD5->hexhash($a1);\n    $self->{'A1'} = $a1;\n}\n    } else {\nreturn;\n    }\n\n    my $a2 = $req->method . \":\" . $uri;\n    $a2 .= MD5->hexhash($req->content) if $qop eq \"auth-int\";\n    $a2 = MD5->hexhash($a2);\n\n    # at this point $a1 is really H(A1) and $a2 is H(A2)\n\n    my $response;\n    if ($qop eq \"auth\" || $qop eq \"auth-int\") {\n#  KD(H(A1), unq(nonce-value)\n#            :\" nc-value\n#            \":\" unq(cnonce-value)\n#            \":\" unq(qop-value)\n#            \":\" H(A2)\n#    )\n$response = MD5->hexhash(\"$a1:$nonce:$nc:$cnonce:$qop:$a2\");\n    } else {\n# compatibility with RFC 2069\n# KD ( H(A1), unq(nonce-value) \":\" H(A2) )\n$response = MD5->hexhash(\"$a1:$nonce:$a2\");\nundef($cnonce);\n    }\n\n    my @h;\n    push(@h, [\"username\" => $user],\n     [\"realm\"    => $realm],\n     [\"nonce\"    => $nonce],\n     [\"uri\"      => $uri],\n             [\"response\" => $response]);\n    push(@h, [\"_algorithm\" => $self->{algorithm}]) if $self->{algorithm};\n    push(@h, [\"cnonce\"   => $cnonce]) if $cnonce;\n    push(@h, [\"opaque\"   => $self->{opaque}]) if exists $self->{opaque};\n    \n    push(@h, [\"_qop\"     => $qop]) if $self->{qop};\n    push(@h, [\"_nc\"      => $nc]);\n\n    my $h = \"Digest \" . join(\", \",\n     map {\n my($k,$v) = @$_;\n unless ($k =~ s/^_//) {\n     $v =~ s/([\\\\\\\"])/\\\\$1/g;\n     $v = qq(\"$v\");\n }\n \"$k=$v\";\n                             } @h);\n    \n    $req->header($header => $h);\n}\n\n\nsub set_authorization\n{\n    shift->_set_authorization(\"Authorization\", @_);\n}\n\n\nsub set_proxy_authorization\n{\n    shift->_set_authorization(\"Proxy-Authorization\", @_);\n}\n\n\n\n", "id": "lists-012-1813539"}, {"subject": "``canonical'' root URL of a server", "content": "What is the significance of including the word ``canonical'' in the following\nsentence in draft-ietf-http-authentication-01 section 1.2?  The cited section\nof the HTTP/1.1 draft defines the \"root\" URL of a server, but the word\ncanonical doesn't appear there.  Is this an editorial bug in one spec or the\nother?\n\n``The realm value (case-sensitive), in combination with the canonical root URL\n(see section 5.1.2 of [2]) of the server being accessed, defines the protection\nspace.''\n\n\n\n", "id": "lists-012-1827582"}, {"subject": "Re: ``canonical'' root URL of a server", "content": "To:http-wg@cuckoo.hpl.hp.com\n\n\n\nMight be an artifact of moving the syntactical definition of a URL to be a\nreference so that the definition of canonical got lost?\n\nDave Morris\n\n\nOn Fri, 3 Apr 1998 Mike_Spreitzer.PARC@xerox.com wrote:\n\n> What is the significance of including the word ``canonical'' in the following\n> sentence in draft-ietf-http-authentication-01 section 1.2?  The cited section\n> of the HTTP/1.1 draft defines the \"root\" URL of a server, but the word\n> canonical doesn't appear there.  Is this an editorial bug in one spec or the\n> other?\n> \n> ``The realm value (case-sensitive), in combination with the canonical root URL\n> (see section 5.1.2 of [2]) of the server being accessed, defines the protection\n> space.''\n> \n> \n\n\n\n", "id": "lists-012-1835870"}, {"subject": "RE: Multiple ProxyAuthenticate challenges", "content": "none. Just forgot to update the one when I was updating the other.  I'll fix\nit in the next draft.\n\nBTW: there isn't any good reason not to have multuple challenges for the\nsame mechanism, either (with different realms).\n\nJim: can we log this an an issue?\n\n> ----------\n> From: Gisle Aas[SMTP:gisle@aas.no]\n> Sent: Friday, April 03, 1998 2:02 AM\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Multiple Proxy-Authenticate challenges?\n> \n> Is there a good reason why WWW-Authenticate can have multiple\n> challenges while Proxy-Authenticate can't?\n> \n> draft-ietf-http-v11-spec-rev-03:\n> \n> >         Proxy-Authenticate  = \"Proxy-Authenticate\" \":\" challenge\n> >         WWW-Authenticate  = \"WWW-Authenticate\" \":\" 1#challenge\n> \n> I also think that the following sentence from the description of\n> WWW-Authenticate fits better if moved to the\n> draft-ietf-http-authentication\n> document as it is the one that define \"challenge\".\n> \n> >                                                          User agents\n> >  MUST take special care in parsing the WWW-Authenticate field value if\n> it\n> >  contains more than one challenge, or if more than one WWW-Authenticate\n> >  header field is provided, since the contents of a challenge may itself\n> >  contain a comma-separated list of authentication parameters.\n> \n> \n> Regards,\n> Gisle\n> \n\n\n\n", "id": "lists-012-1844694"}, {"subject": "Re: More comments on draft-ietf-http-authentication01.tx", "content": "[Gisle Aas wrote:]\n> 1) It is not perfectly clear to me if the namespace of realms are\n> separate for each authentication scheme or even separate when using\n> the server as a proxy and when not.  I matters to me becase I try to\n> index authentication objects in the client per \"server/realm\".\n> \n> This sentence:\n> \n>   The realm value (case-sensitive), in combination with the canonical\n>   root URL (see section 5.1.2 of [2]) of the server being accessed,\n>   defines the protection space.\n> \n> make me believe that I can assume that it would not be possible to\n> have to deal with all the following as separate \"protection spaces\"\n> the same time:\n> \n>    Authorization: Basic realm=\"foo\"\n>    Authorization: Digest realm=\"foo\"\n>    Proxy-Authorization: Basic realm=\"foo\"\n>    Proxy-Authorization: Digest realm=\"foo\"\n\nHmm, I assumed the protection space to be defined by the tuple\n(host, port, scheme, realm), but you are right that it's not clearly\nspecified. However, I would not differentiate between Authorization and\nProxy-Authorization, although at least one popular server will allow\ndifferent auth info to be set up for a proxy and server on the same\nhost and port.\n\n> 2) How to interpret the URIs in the 'domain' attribute of the Digest\n> WWW-Authenticate is not clear to me either.  Are the URIs in this\n> list path prefixes that define protection spaces or must there be\n> an exact match.  If it is a prefix, what happens if the URI contains a\n> query part.\n\nAn exact match. Although in my implementation I actually use these to\ncreate new protection space tuples (if the host or port is different)\nand to update a cache of URL's. This cache is then used to guess if a\nspecific URL needs authentication (for preemptive auth header\ngeneration). I think this fits the intent too.\n\n> 3) Section 3.6 does not seem to know about the \"407 Proxy\n> Authentication Required\" status code.  I though that a\n> \"Proxy-Authenticate\" header could only be present in a 407 response.\n> That is at least how I read <draft-ietf-http-v11-spec-rev-03>.  I\n> would like this paragraph to go away (it would complicate my\n> implementation if it stays):\n> \n>   Note that in principle a client could be asked to authenticate\n>   itself to both a proxy and an end-server. It might receive an\n>   \"HTTP/1.1 401 Unauthorized\" header followed by both a WWW-\n>   Authenticate and a Proxy-Authenticate header. However, it can\n>   never receive more than one Proxy-Authenticate header since such\n>   headers are only for immediate connections and must not be passed\n>   on by proxies. If the client receives both headers, it must\n>   respond with both the Authorization and Proxy-Authorization\n>   headers as described above, which will likely involve different\n>   combinations of username, password, nonce, etc.\n\nHow come this would complicate the implementation? Instead of doing\n\n  if (status == 401)\n      handle_auth_header\n  if (status == 407)\n      handle_proxy-auth_header\n\nit becomes\n\n  if (status == 401)\n      handle_auth_header\n      handle_proxy-auth_header\n  if (status == 407)\n      handle_proxy-auth_header\n\nActually, now that you mention it, I'm not even really sure why the 407\nis needed at all. The Authorization vs. Proxy-Authorization header gives\nyou enough info on where the challenge is coming from. Can anybody shed\nsome light on why 407 exists?\n\n> 4) Are there any servers that already implemented that that knows\n> about \"MD5-sess\" and \"auth-int\" I can test against?\n\nI waiting too...\n\n> For those that can read Perl, I even include the source for my\n> implementation here.  This code might even be of use to others\n> implementing Digest or perhaps somebody can tell me if I misunderstood\n> something in the spec by looking through the code.\n\nI think there is a problem with the nonce-count and the MD5-sess\nimplementation. Assuming that _set_authorization is only called upon\nreceipt of a Authentication or Proxy-Authentication header (i.e. it's\nnot used for preemptively sending auth info) you need the following\nchanges:\n\n> sub _set_authorization\n> {\n[snip]\n>     my $uri = $req->url->full_path;\n>     my $nonce = $self->{nonce};  $nonce = \"\" unless defined $nonce;\n>     my $nc = sprintf \"%08x\", ++$self->{nonce_count};\n\nYou need to reset the counter when a new nonce is received. Something\nlike\n\n      if ($self->{prev_nonce} ne $self->{nonce}) { $self->{nonce_count} = 0; }\n      $self->{prev_nonce} = $self->{nonce};\n      my $nc = sprintf \"%08x\", ++$self->{nonce_count};\n\n>     my $cnonce = sprintf \"%x\", rand(0x1000000);\n> \n>     my $a1;\n>     if ($algorithm eq \"md5\") {\n> # A1 = unq(username-value) \":\" unq(realm-value) \":\" passwd\n> $a1 = MD5->hexhash(\"$user:$realm:$pass\");\n>     } elsif ($algorithm eq \"md5-sess\") {\n> # The following A1 value should only be computed once\n> $a1 = $self->{'A1'};\n> unless ($a1) {\n\nshould be\n\nunless ($a1  &&  ($self->{stale} eq \"true\"))\n\n(I believe this will be clarified in the next draft)\n\n>     # A1 = H( unq(username-value) \":\" unq(realm-value) \":\" passwd )\n>     #         \":\" unq(nonce-value) \":\" unq(cnonce-value)\n>     $a1 = MD5->hexhash(\"$user:$realm:$pass\") . \":$nonce:$cnonce\";\n>     $a1 = MD5->hexhash($a1);\n>     $self->{'A1'} = $a1;\n> }\n>     } else {\n> return;\n>     }\n[snip]\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1854939"}, {"subject": "Re: More comments on draft-ietf-http-authentication01.tx", "content": "Ronald.Tschalaer@psi.ch (Life is hard... and then you die.) writes:\n\n> > 2) How to interpret the URIs in the 'domain' attribute of the Digest\n> > WWW-Authenticate is not clear to me either.  Are the URIs in this\n> > list path prefixes that define protection spaces or must there be\n> > an exact match.  If it is a prefix, what happens if the URI contains a\n> > query part.\n> \n> An exact match.\n\nWould not that make this attribute too long to be useful for any real\nuse?  If you try to protect one specific server script you have to\nmake a domain attribute that lists all possible parameters (query\npart) that this script takes.\n\n\n> > 3) Section 3.6 does not seem to know about the \"407 Proxy\n> > Authentication Required\" status code.  I though that a\n> > \"Proxy-Authenticate\" header could only be present in a 407 response.\n> > That is at least how I read <draft-ietf-http-v11-spec-rev-03>.  I\n> > would like this paragraph to go away (it would complicate my\n> > implementation if it stays):\n> > \n> >   Note that in principle a client could be asked to authenticate\n> >   itself to both a proxy and an end-server. It might receive an\n> >   \"HTTP/1.1 401 Unauthorized\" header followed by both a WWW-\n> >   Authenticate and a Proxy-Authenticate header. However, it can\n> >   never receive more than one Proxy-Authenticate header since such\n> >   headers are only for immediate connections and must not be passed\n> >   on by proxies. If the client receives both headers, it must\n> >   respond with both the Authorization and Proxy-Authorization\n> >   headers as described above, which will likely involve different\n> >   combinations of username, password, nonce, etc.\n> \n> How come this would complicate the implementation? Instead of doing\n> \n>   if (status == 401)\n>       handle_auth_header\n>   if (status == 407)\n>       handle_proxy-auth_header\n> \n> it becomes\n> \n>   if (status == 401)\n>       handle_auth_header\n>       handle_proxy-auth_header\n>   if (status == 407)\n>       handle_proxy-auth_header\n\nThe problem for me is that handle_auth_header() will actually send a\nfollow-up request (usually after asking the user for a new\nusername/password).  I just have to rethink how to deal with this I\nguess.\n\n\n> Actually, now that you mention it, I'm not even really sure why the 407\n> is needed at all. The Authorization vs. Proxy-Authorization header gives\n> you enough info on where the challenge is coming from. Can anybody shed\n> some light on why 407 exists?\n\nPerhaps some old clients that get confused if they get a 401 without\nany WWW-Authenticate?\n\n\n> I think there is a problem with the nonce-count and the MD5-sess\n> implementation. Assuming that _set_authorization is only called upon\n> receipt of a Authentication or Proxy-Authentication header (i.e. it's\n> not used for preemptively sending auth info) you need the following\n> changes:\n\nThe intent is to use it preemptively (assuming that you by this mean\nto send a Authentication to a URL before having seen any\nWWW-Authenticate for it).  Is there problem with this?\n\nThis also brings up another question on how nonce-count and domain\ninteract.  If you have a Digest space that spans several servers,\nshould the nonce-count be incremented seperately for these servers or\nshould they share it?\n\nRegards,\nGisle\n\n\n\n", "id": "lists-012-1868096"}, {"subject": "RE: Multiple ProxyAuthenticate challenges", "content": "> View: Browse HTML    Browse Raw Text\n> From: Paul Leach <paulle@microsoft.com>\n> Date: Fri, 3 Apr 1998 17:18:36 -0800\n> To: http-wg@cuckoo.hpl.hp.com,\n> \"'Gisle Aas'\" <gisle@aas.no>\n> Cc: \"'Jim Gettys'\" <jg@w3.org>\n> Subject: RE: Multiple Proxy-Authenticate challenges?\n> \n> none. Just forgot to update the one when I was updating the other.  I'll fix\n> it in the next draft.\n> \n> BTW: there isn't any good reason not to have multuple challenges for the\n> same mechanism, either (with different realms).\n> \n> Jim: can we log this an an issue?\n> \n\nSure.  I'll update the issues list sometime in the next couple days\nwith the issues that have come in since last call.\n- Jim\n\n\n\n", "id": "lists-012-1879494"}, {"subject": "Request for a copy of the documen", "content": "Hi,\n\nI am a new member in this list.\n\nHow can I get a copy HTTP/1.1 draft  ?\n\nThanx.\n\nJiten\n\n\n\n", "id": "lists-012-1889336"}, {"subject": "RE: Multiple ProxyAuthenticate challenges", "content": "The official working group last call period ended\nover a week ago; at this point, as soon as there\nare new drafts, I will send them onto the IESG.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n> -----Original Message-----\n> From: Jim Gettys [mailto:jg@pa.dec.com]\n> Sent: Monday, April 06, 1998 7:36 AM\n> To: Paul Leach\n> Cc: http-wg@cuckoo.hpl.hp.com; Gisle Aas; Jim Gettys\n> Subject: RE: Multiple Proxy-Authenticate challenges?\n>\n>\n>\n> > View: Browse HTML    Browse Raw Text\n> > From: Paul Leach <paulle@microsoft.com>\n> > Date: Fri, 3 Apr 1998 17:18:36 -0800\n> > To: http-wg@cuckoo.hpl.hp.com,\n> > \"'Gisle Aas'\" <gisle@aas.no>\n> > Cc: \"'Jim Gettys'\" <jg@w3.org>\n> > Subject: RE: Multiple Proxy-Authenticate challenges?\n> >\n> > none. Just forgot to update the one when I was updating the other.  I'll fix\n> > it in the next draft.\n> >\n> > BTW: there isn't any good reason not to have multuple challenges for the\n> > same mechanism, either (with different realms).\n> >\n> > Jim: can we log this an an issue?\n> >\n>\n> Sure.  I'll update the issues list sometime in the next couple days\n> with the issues that have come in since last call.\n> - Jim\n>\n>\n\n\n\n", "id": "lists-012-1896364"}, {"subject": "RE: Multiple ProxyAuthenticate challenges", "content": "The official working group last call period ended\nover a week ago; at this point, as soon as there\nare new drafts, I will send them onto the IESG.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n> -----Original Message-----\n> From: Jim Gettys [mailto:jg@pa.dec.com]\n> Sent: Monday, April 06, 1998 7:36 AM\n> To: Paul Leach\n> Cc: http-wg@cuckoo.hpl.hp.com; Gisle Aas; Jim Gettys\n> Subject: RE: Multiple Proxy-Authenticate challenges?\n>\n>\n>\n> > View: Browse HTML    Browse Raw Text\n> > From: Paul Leach <paulle@microsoft.com>\n> > Date: Fri, 3 Apr 1998 17:18:36 -0800\n> > To: http-wg@cuckoo.hpl.hp.com,\n> > \"'Gisle Aas'\" <gisle@aas.no>\n> > Cc: \"'Jim Gettys'\" <jg@w3.org>\n> > Subject: RE: Multiple Proxy-Authenticate challenges?\n> >\n> > none. Just forgot to update the one when I was updating the other.  I'll fix\n> > it in the next draft.\n> >\n> > BTW: there isn't any good reason not to have multuple challenges for the\n> > same mechanism, either (with different realms).\n> >\n> > Jim: can we log this an an issue?\n> >\n>\n> Sure.  I'll update the issues list sometime in the next couple days\n> with the issues that have come in since last call.\n> - Jim\n>\n>\n\n\n\n", "id": "lists-012-1907672"}, {"subject": "Re: HTTP features w/ low 'implemented' and 'tested", "content": "FWIW, I've now provided a URL where you can test the use of Expect\nand 100 Continue for POSTs.  See\nhttp://portal.research.bell-labs.com:8000/\nfor a link.\n\nDave Kristol\n\n\n\n", "id": "lists-012-1918715"}, {"subject": "Copyright and cach", "content": "I 'm working in resolve the problem of copyright, that is violated with\ncatching. \nIf I pay for a subscription, and connect to get information copyrighted,\nthe page will be cached in the proxy, and every one else could see it.\n\nIf the server sends an http header with \"no-cache\", the problem would be\nresolved?\nPlease, let me know how to get extra information about this issue.\n\nThanks\nM Eugenia Riggi\n\n\n\n", "id": "lists-012-1925865"}, {"subject": "RE: ``canonical'' root URL of a server", "content": "I thought URLs had a caonical form -- bad chars coverted to %xx, etc. Maybe\nit's in the URL RFC?\n\n-----Original Message-----\nFrom: David W. Morris [mailto:dwm@xpasc.com]\nSent: Friday, April 03, 1998 3:55 PM\n\nMight be an artifact of moving the syntactical definition of a URL to be a\nreference so that the definition of canonical got lost?\n\nDave Morris\n\n\nOn Fri, 3 Apr 1998 Mike_Spreitzer.PARC@xerox.com wrote:\n\n> What is the significance of including the word ``canonical'' in the\nfollowing\n> sentence in draft-ietf-http-authentication-01 section 1.2?  The cited\nsection\n> of the HTTP/1.1 draft defines the \"root\" URL of a server, but the word\n> canonical doesn't appear there.  Is this an editorial bug in one spec or\nthe\n> other?\n> \n> ``The realm value (case-sensitive), in combination with the canonical root\nURL\n> (see section 5.1.2 of [2]) of the server being accessed, defines the\nprotection\n> space.''\n> \n> \n\n\n\n", "id": "lists-012-1933207"}, {"subject": "RE: ``canonical'' root URL of a server", "content": "To:http-wg@cuckoo.hpl.hp.com\n\nTo the original post ... sounds like an editorial bug in the\nauthentication draft.  After Larry's post I went digging 'cause I knew\nI'd seen a definition in the past of a 'canonical form'.  Here is what\nI found in rfc1945 ... (in section 3.2.2):\n\n   The canonical form for \"http\" URLs is obtained by converting any\n   UPALPHA characters in host to their LOALPHA equivalent (hostnames are\n   case-insensitive), eliding the [ \":\" port ] if the port is 80, and\n   replacing an empty abs_path with \"/\".  \n\nIn rfc2068 (3.2.3) we provide a definition of URI Comparison which is\nprobably a better approach.\n\nHence I think removal of 'canonical' from the authentication draft\nwill not change the intent and will align the documents.\n\nDave Morris\n\nOn Tue, 7 Apr 1998, Larry Masinter wrote:\n\n> In general, URLs do _not_ have a canonical form. However, HTTP\n> defines some equivalences for URLs (e.g., that http://host is \n> equivalent to http://host/, and by using the generic\n> syntax for host names, the host part is case insensitive).\n> \n> Some particular HTTP servers MAY define other equivalences,\n> e.g., that http://host/dir is equivalent to http://host/dir/\n> and to http://host/dir/index.html.\n> \n> I'm less sure how equivalence is turned into canonicalization for\n> the purpose of creating a 'canonical root', though.\n> \n> Larry\n> --\n> http://www.parc.xerox.com/masinter\n>  \n> \n> > -----Original Message-----\n> > From: Paul Leach [mailto:paulle@microsoft.com]\n> > Sent: Tuesday, April 07, 1998 1:35 PM\n> > To: 'David W. Morris'; http-wg@cuckoo.hpl.hp.com\n> > Cc: http-wg@cuckoo.hpl.hp.com\n> > Subject: RE: ``canonical'' root URL of a server?\n> > \n> > \n> > I thought URLs had a caonical form -- bad chars coverted to %xx, etc. Maybe\n> > it's in the URL RFC?\n> > \n> > -----Original Message-----\n> > From: David W. Morris [mailto:dwm@xpasc.com]\n> > Sent: Friday, April 03, 1998 3:55 PM\n> > \n> > Might be an artifact of moving the syntactical definition of a URL to be a\n> > reference so that the definition of canonical got lost?\n> > \n> > Dave Morris\n> > \n> > \n> > On Fri, 3 Apr 1998 Mike_Spreitzer.PARC@xerox.com wrote:\n> > \n> > > What is the significance of including the word ``canonical'' in the\n> > following\n> > > sentence in draft-ietf-http-authentication-01 section 1.2?  The cited\n> > section\n> > > of the HTTP/1.1 draft defines the \"root\" URL of a server, but the word\n> > > canonical doesn't appear there.  Is this an editorial bug in one spec or\n> > the\n> > > other?\n> > > \n> > > ``The realm value (case-sensitive), in combination with the canonical root\n> > URL\n> > > (see section 5.1.2 of [2]) of the server being accessed, defines the\n> > protection\n> > > space.''\n> > > \n> > > \n> > \n> > \n> \n\n\n\n", "id": "lists-012-1942658"}, {"subject": "RE: ``canonical'' root URL of a server", "content": "In general, URLs do _not_ have a canonical form. However, HTTP\ndefines some equivalences for URLs (e.g., that http://host is \nequivalent to http://host/, and by using the generic\nsyntax for host names, the host part is case insensitive).\n\nSome particular HTTP servers MAY define other equivalences,\ne.g., that http://host/dir is equivalent to http://host/dir/\nand to http://host/dir/index.html.\n\nI'm less sure how equivalence is turned into canonicalization for\nthe purpose of creating a 'canonical root', though.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n> -----Original Message-----\n> From: Paul Leach [mailto:paulle@microsoft.com]\n> Sent: Tuesday, April 07, 1998 1:35 PM\n> To: 'David W. Morris'; http-wg@cuckoo.hpl.hp.com\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: RE: ``canonical'' root URL of a server?\n> \n> \n> I thought URLs had a caonical form -- bad chars coverted to %xx, etc. Maybe\n> it's in the URL RFC?\n> \n> -----Original Message-----\n> From: David W. Morris [mailto:dwm@xpasc.com]\n> Sent: Friday, April 03, 1998 3:55 PM\n> \n> Might be an artifact of moving the syntactical definition of a URL to be a\n> reference so that the definition of canonical got lost?\n> \n> Dave Morris\n> \n> \n> On Fri, 3 Apr 1998 Mike_Spreitzer.PARC@xerox.com wrote:\n> \n> > What is the significance of including the word ``canonical'' in the\n> following\n> > sentence in draft-ietf-http-authentication-01 section 1.2?  The cited\n> section\n> > of the HTTP/1.1 draft defines the \"root\" URL of a server, but the word\n> > canonical doesn't appear there.  Is this an editorial bug in one spec or\n> the\n> > other?\n> > \n> > ``The realm value (case-sensitive), in combination with the canonical root\n> URL\n> > (see section 5.1.2 of [2]) of the server being accessed, defines the\n> protection\n> > space.''\n> > \n> > \n> \n> \n\n\n\n", "id": "lists-012-1954895"}, {"subject": "HTTP 1.1 rev 3 permissio", "content": "Dear HTTP Working Group,\n\nI work in the editorial group within Addison-Wesley's computer &\nengineering publishing team. We're publishing a book called, Programming\nApplications with Netscape Servers by Kaveh Gh. Bassiri in August 1998.\nWe're including a CD-ROM with this book, and would like to request\npermission to use the full text of the HTTP/1.1 draft revision 3 available\nonline at:\n\nhttp://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-rev-03.txt\n\nThe author references this material in the book, and we thought that\nincluding it would be valuable to readers. Would you please let me know if\nyou will allow us to do this? I need to secure all permissions for this\nCD-ROM by April 15, so your quick attention to this matter would be much\nappreciated! I have attached a permissions release to this email: if you\nfind our use of your draft acceptable, please sign and fax this form to me\nat 781-942-3076. \n\nThank you again. I look forward to hearing from you.\n\nBest regards,\n\n\nElizabeth \n\n\n\n\n\n\nElizabeth SpainhourTel: (781) 944-3700 ext. 2256\nAddison Wesley LongmanFax: (781) 942-3076\nOne Jacob Wayelizabeth.spainhour@awl.com\nReading, MA 01867http://www.awl.com/cseng/\n\n\n\n\n\n\napplication/msword attachment: Bassiri_permissions.doc\n\n\n\n\n", "id": "lists-012-1965859"}, {"subject": "Digest auth:  what if client omits qop=", "content": "I'm just starting to think about implementing the qop= part of\nDigest authentication in draft-ietf-http-authentication-01.\n\nIt would appear that a client that understands Digest could\nwillfully or accidentally omit the qop= and response=\nattribute/values, which would bypass the checks based on them.\nOr, presumably, an intermediate malicious agent could delete\nthem.\n\nWhat are the consequences?\n\n1) If the server sends qop=\"auth\" www-Authenticate for its own benefit,\nit still has to accept a response with no qop=\"auth\" in Authorization,\nto allow for older Digest implementations.\n\n2) A client can send qop=\"auth\" in Authorization only if it got\nqop=\"auth\" in WWW-Authenticate.  By sending a cnonce, the client could\ngain some assurance that its request arrived unchanged at the server.\nBut if the qop/response/cnonce attributes got deleted by an agent in\nthe middle, the server wouldn't know it and would just assume it was\ndealing with an older client.\n\nSo what, exactly, is the threat that qop=\"auth\" guards against?  This\nfeature only has value if both the client and server understand and use\nqop=\"auth\".  But the \"security\" of qop=\"auth\" seems no greater than\nwhat's achieved without it.  The same logic would seem to apply to\nqop=\"auth-int\", too.\n\nI assume I'm missing something.  What?\n\nDave Kristol\n\n\n\n", "id": "lists-012-1974451"}, {"subject": "RE: Digest auth:  what if client omits qop=", "content": "> -----Original Message-----\n> From: Dave Kristol [mailto:dmk@research.bell-labs.com]\n> Sent: Wednesday, April 08, 1998 3:05 PM\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Digest auth: what if client omits qop=?\n> \n> \n> I'm just starting to think about implementing the qop= part of\n> Digest authentication in draft-ietf-http-authentication-01.\n> \n> It would appear that a client that understands Digest could\n> willfully or accidentally omit the qop= and response=\n> attribute/values, which would bypass the checks based on them.\n> Or, presumably, an intermediate malicious agent could delete\n> them.\n> \n> What are the consequences?\n> \n> 1) If the server sends qop=\"auth\" www-Authenticate for its \n> own benefit,\n> it still has to accept a response with no qop=\"auth\" in Authorization,\n> to allow for older Digest implementations.\n\nNo -- it will allow a response with no qop=auth or qop=auth-int only if that\nmeets its security policy.\n\n> \n> 2) A client can send qop=\"auth\" in Authorization only if it got\n> qop=\"auth\" in WWW-Authenticate.  By sending a cnonce, the client could\n> gain some assurance that its request arrived unchanged at the server.\n> But if the qop/response/cnonce attributes got deleted by an agent in\n> the middle, the server wouldn't know it and would just assume it was\n> dealing with an older client.\n\nIn which case, when the client eventually checks the Auth-info header's\n\"response=\" directive, the check will fail.\n\n> \n> So what, exactly, is the threat that qop=\"auth\" guards against?  This\n> feature only has value if both the client and server \n> understand and use\n> qop=\"auth\".  But the \"security\" of qop=\"auth\" seems no greater than\n> what's achieved without it.  The same logic would seem to apply to\n> qop=\"auth-int\", too.\n> \n> I assume I'm missing something.  What?\n\nEither or both sides can insist on it. Obviously, if they allow a weaker\nauthentication menchanism, they can't guarantee the stronger security.\n\nPaul\n\n\n\n", "id": "lists-012-1982818"}, {"subject": "Re: Digest auth: what if client omits qop=", "content": "> I'm just starting to think about implementing the qop= part of\n> Digest authentication in draft-ietf-http-authentication-01.\n> \n> It would appear that a client that understands Digest could\n> willfully or accidentally omit the qop= and response=\n> attribute/values, which would bypass the checks based on them.\n> Or, presumably, an intermediate malicious agent could delete\n> them.\n> \n> What are the consequences?\n\nIf the response attribute is missing then authentication must fail.\nAfter all, this is \"core\" attribute (the rest are just to help\nunderstand how the \"response\" was calculated), akin to the encoded\nuser-pass for Basic.\n\nIf qop is removed, then the server will calculate the response attribute\ndifferently than the client did, and authentication will fail too.\n\n> 1) If the server sends qop=\"auth\" www-Authenticate for its own benefit,\n> it still has to accept a response with no qop=\"auth\" in Authorization,\n> to allow for older Digest implementations.\n\nYup.\n\n> 2) A client can send qop=\"auth\" in Authorization only if it got\n> qop=\"auth\" in WWW-Authenticate.  By sending a cnonce, the client could\n> gain some assurance that its request arrived unchanged at the server.\n> But if the qop/response/cnonce attributes got deleted by an agent in\n> the middle, the server wouldn't know it and would just assume it was\n> dealing with an older client.\n\nNo. RFC-2069 also has the \"response\" attribute. So if it's missing, the\nserver must assume a MIM removed it and disallow access.\n\n> So what, exactly, is the threat that qop=\"auth\" guards against?  This\n> feature only has value if both the client and server understand and use\n> qop=\"auth\".  But the \"security\" of qop=\"auth\" seems no greater than\n> what's achieved without it.  The same logic would seem to apply to\n> qop=\"auth-int\", too.\n\nHmm, I suppose a MIM could remove the qop attribute from the challenge,\nthereby forcing rfc-2069 behaviour. I'll leave this to the crypto\nexperts to discuss.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-1992974"}, {"subject": "Re: More comments on draft-ietf-http-authentication01.tx", "content": "> Ronald.Tschalaer@psi.ch (Life is hard... and then you die.) writes:\n> \n> > > 2) How to interpret the URIs in the 'domain' attribute of the Digest\n> > > WWW-Authenticate is not clear to me either.  Are the URIs in this\n> > > list path prefixes that define protection spaces or must there be\n> > > an exact match.  If it is a prefix, what happens if the URI contains a\n> > > query part.\n> > \n> > An exact match.\n> \n> Would not that make this attribute too long to be useful for any real\n> use?  If you try to protect one specific server script you have to\n> make a domain attribute that lists all possible parameters (query\n> part) that this script takes.\n\nEither that, or hope the client does some intelligent guessing. Though\nI don't think that the situation is that bad. I'd expect most clients\nto note the new protection spaces (i.e. the tuples (host, port, scheme,\nrealm) ) in any case, so that at worst you get a needless round-trip for\nthe 401; but at least the user won't be prompted for the username/password\nagain (which I believe is the main reason for the domain attribute).\n\n> This also brings up another question on how nonce-count and domain\n> interact.  If you have a Digest space that spans several servers,\n> should the nonce-count be incremented seperately for these servers or\n> should they share it?\n\nI've assumed a nonce per protection space (i.e. separate nonces and\nnonce-counts above). However, I can't find anything specific in the\nspec, so I believe this should be clarified too.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-2001819"}, {"subject": "Re: Copyright and cach", "content": "     > I 'm working in resolve the problem of copyright, that is violated\n     > with catching. If I pay for a subscription, and connect to get\n     > information copyrighted, the page will be cached in the proxy, and\n     > every one else could see it.\n     \n     > If the server sends an http header with \"no-cache\", the problem\n     > would be resolved?\n     \n     > Please, let me know how to get extra information about this issue.\n     \n     > Thanks\n     > M Eugenia Riggi\n     \n     This problem affects any recources that have access restrictions \n     applied to them. As you mention, a pragma: no-cache header (HTTP/1.0) \n     solves the problem, but means that you can not cache the data \n     privately. A cache-control: private header (HTTP/1.1) is better \n     because the data can now be cached on private caches (usually the \n     browser). In the future, an extension to the private cache control \n     header may be avialbale so that shared caches can cache data that has \n     access restrictions, and ensure that that data is not forwarded to \n     people that do not have access rights (see \n     <http://www.ics.uci.edu/pub/ietf/http/draft-melve-cachecontrol-00.txt> \n     for info.).\n     \n     However, you should also consider that if the data is sent unencrypted \n     that people snooping the network connections or those with the ability \n     to snoop the caching directories on the proxies cache, will also be \n     able to view the data. If the data is extremely sensitive, encryption \n     should be used, and the cache-control: no-store header should be used \n     so that the data is not stored on non-volatile storage in an \n     unencrypted form, and is removed from volatile storage as soon as is \n     possible.\n     \n     Hope thats of use,\n     \n     \n     Dominic.\n**********************************************************************\n\nThis email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. \nIf you have received this email in error please notify Content Technologies \non +44 118 9301300.\n\nThis message has been generated by MIMEsweeper and certifies that the message and attachments have been swept for all known and recorded computer viruses. \nMIMEsweeper 3.x protects your organization from content borne threats and malicious intent. Combined with firewalls MIMEsweeper provides a comprehensive network security solution.\n\nFor information regarding the MIMEsweeper family of products:\n\nPhone:  +44 118 9301300\nFax:    +44 118 9301301\nEmail:  info@mimesweeper.com\nSupport:msw.support@mimesweeper.com\nWorld Wide Web: http://www.mimesweeper.com\n\nMIMEsweeper: Content Security for Networks \n**********************************************************************\n\n\n please notify Content Technologies \non +44 118 9301300.\n\nThis message has been generated by MIMEsweeper and certifies that the message and attachments have been swept for all known and recorded computer viruses. \nMIMEsweeper 3.x protects your organization from content borne threats and malicious intent. Combined with firewalls MIMEsweeper provides a comprehensive network security solution.\n\nFor information regarding the MIMEsweeper family of products:\n\nPhone:  +44 118 9301300\nFax:    +44 118 9301301\nEmail:  info@mimesweeper.com\nSupport:msw.support@mimesweeper.com\nWorld Wide Web: http://www.mimesweeper.com\n\nMIMEsweeper: Content Security for Networks \n**********************************************************************\n\n\n\n", "id": "lists-012-2010719"}, {"subject": "Implementors' List", "content": "Not sure if this list is still active or not...\n\nI was wondering if there was an HTTP implementors' list out there\nsomewhere that I could post a few questions to. \n\nJosh\n\n\n\n", "id": "lists-012-2021827"}, {"subject": "more Digest auth questions/comment", "content": "More stupid Digest authentication questions/comments (and some nits).\n\nDave Kristol\n=============\n\nSubstantive:\n1) Suppose a client wants to get back an entity digest on a GET.\n    a) C->S\nGET /foo HTTP/1.1\n...\n\n    b) C<-S\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: Digest qop=\"auth, auth-int\", s-other-stuff\n...\n\n    c) C->S\nGET /foo HTTP/1.1\nAuthorization: Digest qop=\"auth-in\", c-other-stuff\n...\n\n    Now, for qop=\"auth-int\", the client must include H(entity-body) in\n    the calculation of A2.  But there is no entity-body.  Does the\n    client use the null string when it calculates A2?\n\n2) Same example.  Suppose the server decides, for whatever reason, that\nit *can't* calculate the response-digest for AuthenticationInfo.  How\nshould the server respond?  Error code?  (Which one?)  AuthenticateInfo\nheader with no rspauth attribute?\n\nNits:\n\n3.2.1 The WWW-Authenticate Response Header\n    has this notation:\n  time-stamp H(time-stamp \":\" ETag \":\" private-key)\n    and this notation:\n  KD(secret, data) = H(concat(secret, \":\", data))\n\n    Since the second example is the only instance of concat(), I suggest\n    it be changed to be like the rest:\n  KD(secret, data) = H(secret \":\" data)\n\n3.2.2 The Authorization Request Header\n\n    \"absoluteURL\". The \"cnonce-value\" is an optional  client-chosen\n        ^-- delete\n    value whose purpose is to foil chosen plaintext attacks.\n\n\n\n", "id": "lists-012-2028540"}, {"subject": "RE: HTTP 1.1 rev 3 permissio", "content": "Elizabeth:\n\nI think it is inappropriate to publish the current draft. The next\none should have the official Internet Drafts copyright attached,\nwhich will give you the permission you need.\n\n\n   Copyright (C) The Internet Society 1998.  All Rights Reserved.\n \n   This document and translations of it may be copied and furnished to\n   others, and derivative works that comment on or otherwise explain it\n   or assist in its implementation may be prepared, copied, published\n   and distributed, in whole or in part, without restriction of any\n   kind, provided that the above copyright notice and this paragraph are\n   included on all such copies and derivative works.  However, this\n   document itself may not be modified in any way, such as by removing\n   the copyright notice or references to the Internet Society or other\n   Internet organizations, except as needed for the purpose of\n   developing Internet standards in which case the procedures for\n   copyrights defined in the Internet Standards process must be\n   followed, or as required to translate it into languages other than\n   English.\n \n   The limited permissions granted above are perpetual and will not be\n   revoked by the Internet Society or its successors or assigns.\n \n   This document and the information contained herein is provided on an\n   \"AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.\n\n\n\n", "id": "lists-012-2036881"}, {"subject": "RE: Implementors' List", "content": "> I was wondering if there was an HTTP implementors' list out there\n> somewhere that I could post a few questions to. \n\nIf you can couch your questions in the form \"I read the latest draft\nand I still couldn't tell how to implement ...\", then this is the\nappropriate forum.\n\nRegards,\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-2045546"}, {"subject": "Re: more Digest auth questions/comment", "content": "Scott Lawrence wrote:\n> \n> On Thu, 9 Apr 1998, Dave Kristol wrote:\n> > [...]\n> >     Now, for qop=\"auth-int\", the client must include H(entity-body) in\n> >     the calculation of A2.  But there is no entity-body.  Does the\n> >     client use the null string when it calculates A2?\n> \n>   It uses the hash of the null string, actually.\n\nI think it would be wise to add an explicit statement to that effect to\nthe draft.\n\n> \n> > 2) Same example.  Suppose the server decides, for whatever reason, that\n> > it *can't* calculate the response-digest for AuthenticationInfo.  How\n> > should the server respond?  Error code?  (Which one?)  AuthenticateInfo\n> > header with no rspauth attribute?\n> \n>   500 Internal Server Error\n> \n>   It said that it supported auth-int and now it is backing out - this is\n>   just a bug.\n\nI don't entirely agree that it's a bug.  Suppose the client sends a\npreemptive Authorization header with qop=\"auth-int\" for a URL that\nretrieves dynamic content, more particularly something like an NPH CGI\n(a CGI that presumes to handle all output itself, including headers,\nbypassing the server).  The server will never get the opportunity to\nexamine and calculate the digest for the returned content (because of\nthe NPH architecture).  But the server can probably assume the CGI will\nnot send an AuthenticateInfo header.  And, 500 sounds like the right\nresponse here.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2052881"}, {"subject": "still more Digest auth questions/comment", "content": "3.2.1 The WWW-Authenticate Response Header\n\nWe define function KD as:\n      KD(secret, data) = H(concat(secret, \":\", data))\n\n3.2.2 The Authorization Request Header\n\nThe spec. says:\n    If the \"qop\" directive is not present (this construction is for\n    compatibility with RFC 2069):\n\n       request-digest  =\n  <\"> < KD ( H(A1), unq(nonce-value) \":\" H(A2) ) > <\">\n\n    see below for the defintions for A1 and A2.\n\n    If the \"qop\" value is \"auth\":\n\n       request-digest  = <\"> < KD ( H(A1),     unq(nonce-value)\n   \":\" nc-value\n   \":\" unq(cnonce-value)\n   \":\" unq(qop-value)\n   \":\" H(A2)\n   ) <\">\n\nNote that in neither of these uses of KD() are there two arguments!\n\nI believe the first use of KD() is actually incorrect, although the\ndescription is inherited from RFC 2069.  I think it should be H(), not\nKD().\n\nIt's not clear to me whether the other use of KD() is correct, or\nwhether it, too, should be H().  If it should be H() (and I think so),\nthen we should remove all references to KD(), which is not actually\nused.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2061231"}, {"subject": "RE: still more Digest auth questions/comment", "content": "Read carefully, both of the text samples below have a comma after H(A1).\nEverything after the comma is an implicit concatenation.  I believe the\nsamples below are entirely correct.\n\n-----Original Message-----\nFrom: Dave Kristol [mailto:dmk@research.bell-labs.com]\nSent: Friday, April 10, 1998 8:49 AM\nTo: http-wg@cuckoo.hpl.hp.com\nSubject: still more Digest auth questions/comments\n\n\n3.2.1 The WWW-Authenticate Response Header\n\nWe define function KD as:\n      KD(secret, data) = H(concat(secret, \":\", data))\n\n3.2.2 The Authorization Request Header\n\nThe spec. says:\n    If the \"qop\" directive is not present (this construction is for\n    compatibility with RFC 2069):\n\n       request-digest  =\n  <\"> < KD ( H(A1), unq(nonce-value) \":\" H(A2) ) > <\">\n\n    see below for the defintions for A1 and A2.\n\n    If the \"qop\" value is \"auth\":\n\n       request-digest  = <\"> < KD ( H(A1),     unq(nonce-value)\n   \":\" nc-value\n   \":\" unq(cnonce-value)\n   \":\" unq(qop-value)\n   \":\" H(A2)\n   ) <\">\n\nNote that in neither of these uses of KD() are there two arguments!\n\nI believe the first use of KD() is actually incorrect, although the\ndescription is inherited from RFC 2069.  I think it should be H(), not\nKD().\n\nIt's not clear to me whether the other use of KD() is correct, or\nwhether it, too, should be H().  If it should be H() (and I think so),\nthen we should remove all references to KD(), which is not actually\nused.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2069591"}, {"subject": "Re: still more Digest auth questions/comment", "content": "Whoops!  I really blew that.  H(A1) is, in fact, the secret for KD().\n\nDave Kristol\n==============\nDave Kristol wrote:\n> \n> 3.2.1 The WWW-Authenticate Response Header\n> \n> We define function KD as:\n>       KD(secret, data) = H(concat(secret, \":\", data))\n> \n> 3.2.2 The Authorization Request Header\n> \n> The spec. says:\n>     If the \"qop\" directive is not present (this construction is for\n>     compatibility with RFC 2069):\n> \n>        request-digest  =\n>                   <\"> < KD ( H(A1), unq(nonce-value) \":\" H(A2) ) > <\">\n> \n>     see below for the defintions for A1 and A2.\n> \n>     If the \"qop\" value is \"auth\":\n> \n>        request-digest  = <\"> < KD ( H(A1),     unq(nonce-value)\n>                                            \":\" nc-value\n>                                            \":\" unq(cnonce-value)\n>                                            \":\" unq(qop-value)\n>                                            \":\" H(A2)\n>                                    ) <\">\n> \n> Note that in neither of these uses of KD() are there two arguments!\n> \n> I believe the first use of KD() is actually incorrect, although the\n> description is inherited from RFC 2069.  I think it should be H(), not\n> KD().\n> \n> It's not clear to me whether the other use of KD() is correct, or\n> whether it, too, should be H().  If it should be H() (and I think so),\n> then we should remove all references to KD(), which is not actually\n> used.\n> \n> Dave Kristol\n\n\n\n", "id": "lists-012-2079423"}, {"subject": "Issue: warning header should be general header, not response heade", "content": "Currently the Warning header field is a Response header field:\n\n14.46 Warning\n\nThe Warning response-header field is used to carry\nadditional information about the status of a response\nwhich may not be reflected by the response status code.\nThis information is typically, though not exclusively,\nused to warn about a possible lack of semantic transparency\nfrom caching operations.\n\nHowever, in section 13.1.2, it is said that \n\nA proxy MUST NOT modify or add any of the following fields\nin a response that contains the no-transform Cache-Control\ndirective, or in any request:\n\no Content-Encoding\no Content-Range\no Content-Type\n\nA non-transparent proxy MAY modify or add these fields in\na response that does not include no-transform, but if it\ndoes so, it MUST add a Warning 114 (Transformation applied)\nif one does not already appear in the response.\n\nThe Warning header field must be a general header field to be able to\nindicate that a message in a PUT has been modified. This can for example be\nused to support advanced image processing in the proxy which is not\navailable in the client doing the PUT. I admit that the above wording is\nquite confusing. I would say:\n\nA proxy MUST NOT modify or add any of the following fields\nin a message that contains the no-transform Cache-Control\ndirective:\n\no Content-Encoding\no Content-Range\no Content-Type\n\nA non-transparent proxy MAY modify or add these fields to\na message that does not include no-transform, but if it\ndoes so, if not already present, it MUST add a Warning 114\n(Transformation applied).\n\nThis would also make it a lot easier to use the Warining header field by\nMandatory, for example.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-2087818"}, {"subject": "4/13/98 http-authentication01.txt comment", "content": "(It's gotten so I have to date them! :-)\n\n\n3.2.2 The Authorization Request Header\n\n    response         = \"response\" \"=\" request-digest\n\nLater there's this text:\n    The definition of request-digest above indicates the encoding for\n    its value. The following definitions show how the value is\n    computed.\n\nUnfortunately, there *is* no \"definition ... above\".  The non-terminal\nrequest-digest has no syntactic definition.  I suspect it should be\n\nrequest-digest = <\"> *LHEX <\">\n\n\n3.2.3 The Authentication-Info Header\n\ncnonce and qop are used in the calculation of response-digest.  The\nclient is not required to send either cnonce= or auth=.  So I assume\n(correct?) that the null string is used for values for omitted\nattributes in the calculation.\n\nIf (to use cnonce as the example) cnonce was omitted, should\nAuthentication-Info omit cnonce, or should it send cnonce=\"\"?  Same\nquestion for auth.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2096645"}, {"subject": "Digest testing site availabl", "content": "My Digest test is still meager at\n<http://portal.research.bell-labs.com:8000/>, but it now supports\nhttp-authentication-01 style for qop=auth (but not auth-int yet).\nAt least I think it does.  Testers welcome.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2104605"}, {"subject": "HTTP-authentication01.txt comment", "content": "On Mon, 13 Apr 1998, Dave Kristol wrote:\n\n> \n> 3.2.3 The Authentication-Info Header\n> \n> cnonce and qop are used in the calculation of response-digest.  The\n> client is not required to send either cnonce= or auth=.  So I assume\n> (correct?) that the null string is used for values for omitted\n> attributes in the calculation.\n> \n> If (to use cnonce as the example) cnonce was omitted, should\n> Authentication-Info omit cnonce, or should it send cnonce=\"\"?  Same\n> question for auth.\n> \n\nIt might be better to say that Authentication-Info should only be\nsent if qop (and hence cnonce) are present.\n\n\nAnother question: Unless I am mistaken, at one point in the long\nsequence of digest drafts, the Authentication-Info header could be\nsupplied by either the server or the client.  It would be useful\nfor the client to be able to supply the digest of POSTed data\nor a file which is PUT.  Being able to assure the integrity of\nclient supplied data would be very useful.  Did this fall through\nthe cracks, or am I just missing this functionality somewhere in\nthe draft?\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-2111223"}, {"subject": "Re: HTTP-authentication01.txt comment", "content": "John Franks wrote:\n> \n> On Mon, 13 Apr 1998, Dave Kristol wrote:\n> \n> >\n> > 3.2.3 The Authentication-Info Header\n> >\n> > cnonce and qop are used in the calculation of response-digest.  The\n> > client is not required to send either cnonce= or auth=.  So I assume\n> > (correct?) that the null string is used for values for omitted\n> > attributes in the calculation.\n> >\n> > If (to use cnonce as the example) cnonce was omitted, should\n> > Authentication-Info omit cnonce, or should it send cnonce=\"\"?  Same\n> > question for auth.\n> >\n> \n> It might be better to say that Authentication-Info should only be\n> sent if qop (and hence cnonce) are present.\n\nBut cnonce is not required, even when qop is specified.  Only\nnonce-count is required.\n\n> \n> Another question: Unless I am mistaken, at one point in the long\n> sequence of digest drafts, the Authentication-Info header could be\n> supplied by either the server or the client.  It would be useful\n> for the client to be able to supply the digest of POSTed data\n> or a file which is PUT.  Being able to assure the integrity of\n> client supplied data would be very useful.  Did this fall through\n> the cracks, or am I just missing this functionality somewhere in\n> the draft?\n\nHmmm.  There does not seem to be a way for the client to send a digest\nof the entity-body.  If it could, though, there's an ambiguity about\nqop=auth-int:\n\n1) C<-S\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: ... qop=\"auth,auth-int\", ...\n\n2) C->S (speculative)\nPOST /some/entity HTTP/1.1\nHost: blah\nAuthorization: ... qop=auth-int, ...\nAuthentication-Info:  reqauth=<some suitable digest>, ...\n\n3) C<-S (problematic)\nHTTP/1.1 200 OK\nAuthentication-Info:  qop=auth-int, rspauth=<entity digest>\n\nThe problem is that the client chose (this is speculative -- the spec.\ndoesn't read this way) \"auth-int\", in order to send an entity digest.\nBut the server is obliged to respond in kind, which means it must do a\ndigest of what is probably not a very interesting response.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2120107"}, {"subject": "4/14/98 http-authentication01 comment", "content": "The specification provides this suggestion:\n\n3.2.1 The WWW-Authenticate Response Header\n\n  The contents of the nonce are implementation dependent. The quality\n  of the implementation depends on a good choice. A nonce might, for\n  example, be constructed as the base 64 encoding of\n\n      time-stamp H(time-stamp \":\" ETag \":\" private-key)\n  where time-stamp is a server-generated time or other non-repeating\n  value, ETag is the value of the HTTP ETag header associated with the\n  requested entity, and private-key is data known only to the server.\n  With a nonce of this form a server would recalculate the hash portion\n  after receiving the client authentication header and reject the\n  request if it did not match the nonce from that header or if the\n  time-stamp value is not recent enough. In this way the server can\n  limit the time of the nonce's validity. The inclusion of the ETag\n  prevents a replay request for an updated version of the resource.\n  (Note: including the IP address of the client in the nonce would\n  appear to offer the server the ability to limit the reuse of the\n  nonce to the same client that originally got it. However, that would\n  break proxy farms, where requests from a single user often go through\n  different proxies in the farm. Also, IP address spoofing is not that\n  hard.)\n\nI think this example for nonce is a poor one, for the following reasons:\n\n1) The nonce would not be reusable for other entities, because it's\ntied to a particular URI by Etag.\n\n2) Etag may be difficult or impossible to calculate.  For example, in\nmy server implementation, access control is done based on the raw URL,\nbefore a URL gets mapped to a file.  When I discover that\nauthentication has failed, my server hasn't done the URL -> file\nmapping, and it would be difficult to calculate Etag when generating\nWWW-Authenticate.\n\nIn other contexts, such as dynamically generated content, there may not\n*be* an Etag.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2129583"}, {"subject": "Re: HTTP-authentication01.txt comment", "content": "> From: John Franks <john@math.nwu.edu>\n\n> Another question: Unless I am mistaken, at one point in the long\n> sequence of digest drafts, the Authentication-Info header could be\n> supplied by either the server or the client.  It would be useful\n> for the client to be able to supply the digest of POSTed data\n> or a file which is PUT.  Being able to assure the integrity of\n> client supplied data would be very useful.  Did this fall through\n> the cracks, or am I just missing this functionality somewhere in\n> the draft?\n\n  It is in the Authorization header now.  In section 3.2.2, it says:\n\n\n]    request-digest  = <\"> < KD ( H(A1),     unq(nonce-value)\n]                                        \":\" nc-value\n]                                        \":\" unq(cnonce-value)\n]                                        \":\" unq(qop-value)\n]                                        \":\" H(A2)\n]                                ) <\">\n   ...\n] If the \"qop\" directive's value is \"auth\" or is unspecified, then A2 is:\n]\n]    A2       = Method \":\" digest-uri-value\n]\n] If the \"qop\" value is \"auth-int\", then A2 is:\n]\n]    A2       = Method \":\" digest-uri-value \":\" H(entity-body)\n\n  So A2 in the response digest (poor name, that, because it is the\n  response to the challenge, but it appears in a request) contains the\n  hash of the body.  That way we don't need another header.  I don't\n  believe that we specified that this may appear in a trailer,\n  though.\n\n\n\n", "id": "lists-012-2137960"}, {"subject": "Re: HTTP-authentication01.txt comment", "content": "On Tue, 14 Apr 1998, Dave Kristol wrote:\n\n> John Franks wrote:\n> > \n> > On Mon, 13 Apr 1998, Dave Kristol wrote:\n> > \n> > >\n> > > If (to use cnonce as the example) cnonce was omitted, should\n> > > Authentication-Info omit cnonce, or should it send cnonce=\"\"?  Same\n> > > question for auth.\n> > >\n> > \n> > It might be better to say that Authentication-Info should only be\n> > sent if qop (and hence cnonce) are present.\n> \n> But cnonce is not required, even when qop is specified.  Only\n> nonce-count is required.\n> \n\nI think this is an oversight.  I suspect the intent was that\ncnonce MUST be supplied if qop is.\n\n> > \n> > Another question: Unless I am mistaken, at one point in the long\n> > sequence of digest drafts, the Authentication-Info header could be\n> > supplied by either the server or the client.  It would be useful\n> > for the client to be able to supply the digest of POSTed data\n> > or a file which is PUT.  Being able to assure the integrity of\n> > client supplied data would be very useful.  Did this fall through\n> > the cracks, or am I just missing this functionality somewhere in\n> > the draft?\n> \n> Hmmm.  There does not seem to be a way for the client to send a digest\n> of the entity-body.  If it could, though, there's an ambiguity about\n> qop=auth-int:\n> \n\nActually Scott Lawrence straightened me out about this.  The client's\nPOSTed or PUT data must be MD5 digested if qop=\"auth-int\".  The digest\nresult is part or the response digest.  It is included in \"A2\" in\nthe case that qop=\"auth-int\".\n\n> 1) C<-S\n> HTTP/1.1 401 Unauthorized\n> WWW-Authenticate: ... qop=\"auth,auth-int\", ...\n> \n> 2) C->S (speculative)\n> POST /some/entity HTTP/1.1\n> Host: blah\n> Authorization: ... qop=auth-int, ...\n> Authentication-Info:  reqauth=<some suitable digest>, ...\n> \n> 3) C<-S (problematic)\n> HTTP/1.1 200 OK\n> Authentication-Info:  qop=auth-int, rspauth=<entity digest>\n> \n> The problem is that the client chose (this is speculative -- the spec.\n> doesn't read this way) \"auth-int\", in order to send an entity digest.\n> But the server is obliged to respond in kind, which means it must do a\n> digest of what is probably not a very interesting response.\n> \n\nActually I think the spec DOES read this way and the server is\nrequired to respond with a digest of what may be an uninteresting\nresponse.  The choice is not entirely with the client.  The server\nmay only offer one of \"auth\" or \"auth-int\".  But if it offers both\nthe client gets to pick and the server must digest its response.\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-2147350"}, {"subject": "Unidentified subject", "content": "In rev3, language was added at the end this section which states:\n\n\"The Content-Length field of a request or response is added or deleted\naccording to the rules in section 4.4. A cache or non-caching proxy MUST\npreserve the entity-length (section 7.2.2) of the entity-body, although\nit MAY change the transfer-length (section 4.4).\"\n\nI believe the second sentence should read:\n\n\"A transparent proxy MUST preserve the entity-length....\"\n\ngiven that the definition of \"proxy\" specifically mentions media type\ntransformation as a service that non-transparent proxies may provide.\n(Media type transformations usually do change the entity length....)\n\n\n\n", "id": "lists-012-2158220"}, {"subject": "Implementing Hostname recognition in HTTP Server", "content": "Hi all --\n\nI'm currently wrestling with a moderately thorny issue while in the\nprocess of implementing HTTP/1.1 support for my server.\n\nThe RFC specifies that the HTTP server MUST accept URLs of the form\nhttp://www.foo.org/index.html, and that they SHOULD (or is it MUST?)\nfurthermore verify that the hostname passed in such requests is\nactually one that is served by the HTTP server.\n\nImplicit in this specification, it would seem, is the ability to do\nsome sort of multisite hosting using hostnames to discriminate.\n(i.e. different responses to http://www.foo.org and\nhttp://www.bar.org, even if they're the same machine)\n\nThis all seems easy enough, until I consider how to treat those poor\nHTTP/1.0 clients that don't know to pass a hostname as part of the\nrequest.  I have devised a method that I think will work for dealing\nwith this, involving a default hostname and some sneaky URL\nmanipulation, but I'm not 100% happy with it.\n\nSo, I was wondering:\n\nIs there a convention for doing this that should be followed in order\nto avoid pitfalls?  It's soemwhat outside the pruview of the RFCs, but\nI wonder if people know of other servers that have this functionality,\nand perhaps more importantly how widely this functionality is\ndeployed... \n\nThanks for any help you can offer.\n\nJosh Bluestein\njosh@epilogue.com\nEpilogue Technology Corporation\nAn Integrated Systems Company\n\n\n\n", "id": "lists-012-2165292"}, {"subject": "Digest auth and Range and qop=authin", "content": "Just want to do a reality check.  Suppose we have\n\n1) C<-S\n    HTTP/1.1 401 Unauthorized\n    WWW-Authenticate: Digest ... qop=\"auth,auth-int\", ...\n\n2) C->S\n    GET / HTTP/1.1\n    Host: foo\n    Range: bytes=0-1,2-3\n    Authorization: Digest ... qop=auth-int, ...,\n    response=\"d1837789f989729e19578e86cadbd06e\", ...\n\n3) C<-S\n    HTTP/1.1 206 Partial Content\n    Date: Wed, 15 Apr 1998 15:26:04 GMT\n    Server: DMKHTD/1.06c\n    Content-type: multipart/byteranges; boundary=xyzzy\n    Last-modified: Wed, 27 Nov 1996 22:36:24 GMT\n    Etag: \"1e7d0a-6ac-329cc268\"\n    Authentication-Info: ... rspauth=<stuff>, ...\n\n    --xyzzy\n    Content-type: text/html\n    Content-range: bytes 0-1/1708\n\n    <H\n    --xyzzy\n    Content-type: text/html\n    Content-range: bytes 2-3/1708\n\n    TM\n    --xyzzy--\n\nNow, response-digest computes\n   A2       = Status-Code \":\" digest-uri-value \":\" H(entity-body)\n\nI assume that entity-body in this context comprises all the stuff that\ngets returned as a response, including multipart boundaries and\nembedded headers in each part.\n\nSounds like fun to implement.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2173355"}, {"subject": "Re: Digest auth and Range and qop=authin", "content": "On Wed, 15 Apr 1998, Dave Kristol wrote:\n\n> Just want to do a reality check.  Suppose we have\n> \n> 1) C<-S\n>     HTTP/1.1 401 Unauthorized\n>     WWW-Authenticate: Digest ... qop=\"auth,auth-int\", ...\n> \n> 2) C->S\n>     GET / HTTP/1.1\n>     Host: foo\n>     Range: bytes=0-1,2-3\n>     Authorization: Digest ... qop=auth-int, ...,\n>     response=\"d1837789f989729e19578e86cadbd06e\", ...\n> \n> 3) C<-S\n>     HTTP/1.1 206 Partial Content\n>     Date: Wed, 15 Apr 1998 15:26:04 GMT\n>     Server: DMKHTD/1.06c\n>     Content-type: multipart/byteranges; boundary=xyzzy\n>     Last-modified: Wed, 27 Nov 1996 22:36:24 GMT\n>     Etag: \"1e7d0a-6ac-329cc268\"\n>     Authentication-Info: ... rspauth=<stuff>, ...\n> \n>     --xyzzy\n>     Content-type: text/html\n>     Content-range: bytes 0-1/1708\n> \n>     <H\n>     --xyzzy\n>     Content-type: text/html\n>     Content-range: bytes 2-3/1708\n> \n>     TM\n>     --xyzzy--\n> \n> Now, response-digest computes\n>    A2       = Status-Code \":\" digest-uri-value \":\" H(entity-body)\n> \n> I assume that entity-body in this context comprises all the stuff that\n> gets returned as a response, including multipart boundaries and\n> embedded headers in each part.\n> \n> Sounds like fun to implement.\n> \n\nI think that one way to do this would be to chunk the entire \nmultipart/byteranges response and calculate the MD5 digest while\nchunking.  The Authentication-Info header could then be put in\na trailer of the chunking.\n\nIs there any reason not to chunk a multipart/byterange?  Is doing\nso consistent with the spec?\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-2181806"}, {"subject": "Re: Digest auth and Range and qop=authin", "content": "John Franks <john@math.nwu.edu> wrote:\n  > [in response to DMK]\n\n  > I think that one way to do this would be to chunk the entire \n  > multipart/byteranges response and calculate the MD5 digest while\n  > chunking.  The Authentication-Info header could then be put in\n  > a trailer of the chunking.\n  > \n  > Is there any reason not to chunk a multipart/byterange?  Is doing\n  > so consistent with the spec?\n\nThat's the only approach I could imagine using, short of precomputing\nthe entire response.  But I wasn't altogether sure it was valid, though\nneither can I think of a reason why it is not.  That's why I asked.\n\nI'm in the process of implementing auth-int and it is, to be charitable,\n\"challenging\".  Has anyone else implemented server-side auth-int?  Is\nanyone else *planning* to?\n\nDave Kristol\n\n\n\n", "id": "lists-012-2191408"}, {"subject": "Re: Digest auth and Range and qop=authin", "content": "On Thu, 16 Apr 1998, Dave Kristol wrote:\n\n> I'm in the process of implementing auth-int and it is, to be charitable,\n> \"challenging\".  Has anyone else implemented server-side auth-int?  Is\n> anyone else *planning* to?\n\n  Havn't gotten to it yet, but yes, I do plan to implement it.\n\n\n\n", "id": "lists-012-2200165"}, {"subject": "Confused about persistent connection for old client", "content": "OK, I too, am confused about why proxies MUST NOT establish\npersistent connections with 1.0 clients.  If the client and\norigin server connections are handled separately, and if the\nproxy understands the 1.0 Keep-alive, what's the danger?\n\n> 8.1.3Proxy Servers\n>\n> It is especially important that proxies correctly implement \n> the properties of the Connection header field as specified in 14.2.1.\n> The proxy server MUST signal persistent connections \n> separately with its clients and the origin servers (or other \n> proxy servers) that it connects to. Each persistent \n> connection applies to only one transport link.\n>\n> A proxy server MUST NOT establish a persistent connection \n> with an HTTP/1.0 client (but see RFC 2068 for information \n> about the Keep-Alive header implemented by many HTTP/1.0 clients).\n\n\n\n", "id": "lists-012-2208685"}, {"subject": "Re: Confused about persistent connection for old client", "content": ">>>>> \"EW\" ==  <ewindes@spyglass.com> writes:\n\nEW> OK, I too, am confused about why proxies MUST NOT establish\nEW> persistent connections with 1.0 clients.  If the client and\nEW> origin server connections are handled separately, and if the\nEW> proxy understands the 1.0 Keep-alive, what's the danger?\n\n  I believe that the intent is that the proxy server not assume\n  1.1-style persistent connections.  The prohibition does not apply to\n  the non-standard Keep-Alive mechanism; that is why it is noted in\n  the text you quoted:\n\n>> A proxy server MUST NOT establish a persistent connection\n>> with an HTTP/1.0 client (but see RFC 2068 for information\n>> about the Keep-Alive header implemented by many HTTP/1.0 clients).\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n", "id": "lists-012-2216983"}, {"subject": "Re: Confused about persistent connection for old client", "content": "     \n     >>>>> \"EW\" ==  <ewindes@spyglass.com> writes:\n     \n     EW> OK, I too, am confused about why proxies MUST NOT establish\n     EW> persistent connections with 1.0 clients.  If the client and\n     EW> origin server connections are handled separately, and if the\n     EW> proxy understands the 1.0 Keep-alive, what's the danger?\n     \n     Persistent connections in HTTP/1.0 must be established on a hop-by-hop \n     basis, so that intermediaries (proxies) are not forced into persistent \n     connections if they do not understand them. The connection: keep-alive \n     header allowed a client to request a persistent connection from an \n     origin server without consideration for intermediaries (end-to-end).\n     \n     To prevent this problem, clients connecting to origin servers via \n     intermediaries were required to send a proxy-connection: keep-alive \n     header instead which the proxy could convert to a plain connection: \n     keep-alive header if it also implemented persistent connections. \n     Origin servers only respond to the plain connection: keep-alive \n     headers, knowing that if they receive a proxy-connection: keep-alive \n     header that the proxy does not support persistent connections. This \n     works fine in this scenario:\n     \n     C <--> P <--> OS\n     \n     Proxy chains were, in theory, to be supported by ensuring that only \n     proxies at the end of the chain could perform keep-alive header \n     conversions so that upstream proxies were not unwittingly forced into \n     agreeing a persistent connection. Where P* is a proxy that will \n     perform keep-alive header conversions, then a typical scenario might \n     be:\n     \n     C <--> P <--> P <--> P* <--> OS\n     \n     Finally, the reason origin servers should not provide persistent \n     connections to clients is that if the last proxy in the chain supports \n     persistent connections, then the entire connection will be persistent \n     regardless of whether all the downstream proxies support persistent \n     connections.\n     \n     Proxy chains constructed in this way will never work because any proxy \n     that does not support persistent connections will wait for an end of \n     connection from the upstream proxy that will never arrive. If the \n     first proxy in our chain did not support persistent connections the \n     data flow would be:\n     \n      _       _       _       __       __\n     |C| --> |P| --> |P| --> |P*| --> |OS|\n     | |     | | <-- | | <-- |  | <-- |  |\n     \n     Hope that makes sense (it's not easy to explain without a drawing \n     board).\n     \n     \n     Cheers,\n     \n     \n     Dominic\n**********************************************************************\n\nThis email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. \nIf you have received this email in error please notify Content Technologies \non +44 118 9301300.\n\nThis message has been generated by MIMEsweeper and certifies that the message and attachments have been swept for all known and recorded computer viruses. \nMIMEsweeper 3.x protects your organization from content borne threats and malicious intent. Combined with firewalls MIMEsweeper provides a comprehensive network security solution.\n\nFor information regarding the MIMEsweeper family of products:\n\nPhone:  +44 118 9301300\nFax:    +44 118 9301301\nEmail:  info@mimesweeper.com\nSupport:msw.support@mimesweeper.com\nWorld Wide Web: http://www.mimesweeper.com\n\nMIMEsweeper: Content Security for Networks \n**********************************************************************\n\n\n\n", "id": "lists-012-2226101"}, {"subject": "Re: Confused about persistent connection for old client", "content": "The fallacy in the attached description is the implication that\na persistent connection is established on any basis except \nhop-hop. I would suggest reading section 19.7.1 in rfc2068. There can be \nno such thing as \"unwittingly forced into agreeing to a persistent\nconnection\" and I think the RFC2068 section explains the issues\nmore accurately.\n\nDave Morris\n\nOn Fri, 17 Apr 1998 Dominic.Chambers@mimesweeper.com wrote:\n\n>      \n>      >>>>> \"EW\" ==  <ewindes@spyglass.com> writes:\n>      \n>      EW> OK, I too, am confused about why proxies MUST NOT establish\n>      EW> persistent connections with 1.0 clients.  If the client and\n>      EW> origin server connections are handled separately, and if the\n>      EW> proxy understands the 1.0 Keep-alive, what's the danger?\n>      \n>      Persistent connections in HTTP/1.0 must be established on a hop-by-hop \n>      basis, so that intermediaries (proxies) are not forced into persistent \n>      connections if they do not understand them. The connection: keep-alive \n>      header allowed a client to request a persistent connection from an \n>      origin server without consideration for intermediaries (end-to-end).\n>      \n>      To prevent this problem, clients connecting to origin servers via \n>      intermediaries were required to send a proxy-connection: keep-alive \n>      header instead which the proxy could convert to a plain connection: \n>      keep-alive header if it also implemented persistent connections. \n>      Origin servers only respond to the plain connection: keep-alive \n>      headers, knowing that if they receive a proxy-connection: keep-alive \n>      header that the proxy does not support persistent connections. This \n>      works fine in this scenario:\n>      \n>      C <--> P <--> OS\n>      \n>      Proxy chains were, in theory, to be supported by ensuring that only \n>      proxies at the end of the chain could perform keep-alive header \n>      conversions so that upstream proxies were not unwittingly forced into \n>      agreeing a persistent connection. Where P* is a proxy that will \n>      perform keep-alive header conversions, then a typical scenario might \n>      be:\n>      \n>      C <--> P <--> P <--> P* <--> OS\n>      \n>      Finally, the reason origin servers should not provide persistent \n>      connections to clients is that if the last proxy in the chain supports \n>      persistent connections, then the entire connection will be persistent \n>      regardless of whether all the downstream proxies support persistent \n>      connections.\n>      \n>      Proxy chains constructed in this way will never work because any proxy \n>      that does not support persistent connections will wait for an end of \n>      connection from the upstream proxy that will never arrive. If the \n>      first proxy in our chain did not support persistent connections the \n>      data flow would be:\n>      \n>       _       _       _       __       __\n>      |C| --> |P| --> |P| --> |P*| --> |OS|\n>      | |     | | <-- | | <-- |  | <-- |  |\n>      \n>      Hope that makes sense (it's not easy to explain without a drawing \n>      board).\n>      \n>      \n>      Cheers,\n>      \n>      \n>      Dominic\n> **********************************************************************\n> \n> This email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. \n> If you have received this email in error please notify Content Technologies \n> on +44 118 9301300.\n> \n> This message has been generated by MIMEsweeper and certifies that the message and attachments have been swept for all known and recorded computer viruses. \n> MIMEsweeper 3.x protects your organization from content borne threats and malicious intent. Combined with firewalls MIMEsweeper provides a comprehensive network security solution.\n> \n> For information regarding the MIMEsweeper family of products:\n> \n> Phone:  +44 118 9301300\n> Fax:    +44 118 9301301\n> Email:  info@mimesweeper.com\n> Support:msw.support@mimesweeper.com\n> World Wide Web: http://www.mimesweeper.com\n> \n> MIMEsweeper: Content Security for Networks \n> **********************************************************************\n> \n> \n\n\n\n", "id": "lists-012-2238149"}, {"subject": "RE: Digest auth and Range and qop=authin", "content": "So are we.\n\n> -----Original Message-----\n> From: Scott Lawrence [mailto:lawrence@agranat.com]\n> Sent: Thursday, April 16, 1998 12:03 PM\n> To: Dave Kristol\n> Cc: john@math.nwu.edu; http-wg@cuckoo.hpl.hp.com;\n> http-wg@hplb.hpl.hp.com\n> Subject: Re: Digest auth and Range and qop=auth-int\n> \n> \n> \n> \n> On Thu, 16 Apr 1998, Dave Kristol wrote:\n> \n> > I'm in the process of implementing auth-int and it is, to \n> be charitable,\n> > \"challenging\".  Has anyone else implemented server-side \n> auth-int?  Is\n> > anyone else *planning* to?\n> \n>   Havn't gotten to it yet, but yes, I do plan to implement it.\n> \n> \n\n\n\n", "id": "lists-012-2252189"}, {"subject": "Of possible interest: ID on &quot;Duplicate Suppression in HTTP&quot", "content": "This is not within the charter of the HTTP-WG, but it might be\nof interest to people working on HTTP protocol design issues.\n\n-Jeff    \n    \n    From: Internet-Drafts@ietf.org\n    Subject: I-D ACTION:draft-mogul-http-dupsup-00.txt\n    Date: Fri, 17 Apr 1998 10:27:18 -0400\n    \n    A New Internet-Draft is available from the on-line Internet-Drafts\n    directories.\n    \nTitle: Duplicate Suppression in HTTP\nAuthor(s): J. Mogul, A. van Hoff\nFilename: draft-mogul-http-dupsup-00.txt\nPages: 24\nDate: 16-Apr-98\n\n        A significant fraction of Web content is often exactly\n        duplicated under several different URIs.  This duplication\n        can lead to suboptimal use of network bandwidth, and\n        unnecessary latency for users.  Much of this duplication\n        can be avoided through the use of a simple mechanism,\n        described here, which allows a cache to efficiently\n        substitute one byte-for-byte identical value for another.\n        By doing so, the cache avoids some or all of the network\n        costs associated with retrieving the duplicate value.\n\n    ftp://ftp.ietf.org/internet-drafts/draft-mogul-http-dupsup-00.txt\n\n\n\n", "id": "lists-012-2262507"}, {"subject": "Section 10.3.3 (302 Found", "content": "Section 10.3.3 (\"302 Found\") of the current HTTP/1.1 draft still says\nthat the conversion of POST to GET is \"erroneous\".  It thus still contradicts\nvirtually universal \"current practice\", and the decisions reached at the\nWashington, DC IETf meeting and subsequent discussion on this list.  This\nundoubtedly is an oversight, but should be corrected before it becomes a\nDraft Standard.\n\n-- \nFoteos Macrides (macrides@sci.wfbr.edu)\n\n\n\n", "id": "lists-012-2270529"}, {"subject": "Status of HTTPWG document", "content": "On Fri 3/13/98 5:28 PM, the 'last LAST CALL' for three documents\n(v11-spec-rev, authentication, state-man-mec) was issued, and scheduled\nto expire March 27.\n\nWe've since had several relatively minor issues raised with\nthe first document (v11-spec-rev), but, more importantly,\nwe're still looking for more complete implementation reports\nof a few features before forwarding this document on as Draft\nStandard.\n\nThe second document (authentication) has had substantial comments,\nwhich I believe will require a revision and another (brief) last\ncall. More importantly, we need better substantiation of testing\nof interoperability of the independent implementations. We're still\nhoping for Draft Standard status for these two documents together,\nsince v11-spec-rev should not move forward without a credible\nauthentication mechanism at the same maturity level.\n\nThe last document (state-man-mec) has had no substantial comments,\nexcept that there are not independent interoperable implementations,\nand it seems possible that we just forward it on as Proposed Standard,\nwhich I will do forthwith.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-2277428"}, {"subject": "Last Call: HTTP State Management Mechanism to Proposed Standar", "content": "The IESG has received a request from the HyperText Transfer Protocol\nWorking Group to consider HTTP State Management Mechanism\n<draft-ietf-http-state-man-mec-08.txt> as a Proposed Standard.\n\nThe IESG plans to make a decision in the next few weeks, and solicits\nfinal comments on this action.  Please send any comments to the\niesg@ietf.org or ietf@ietf.org mailing lists by May 6, 1998.\n\nFiles can be obtained via\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-08.txt\n\n\n\n", "id": "lists-012-2285576"}, {"subject": "Digest test site availabl", "content": "I've upgraded my server at <http://portal.research.bell-labs.com:8000>\nto support Digest authentication with qop=auth-int.  The home page\nthere points to some useful links for testing odds and ends.\n\nN.B.  qop=auth-int is supported only on HTTP/1.1 requests.\n\nHave at it!\n\nDave Kristol\n\n\n\n", "id": "lists-012-2293113"}, {"subject": "Re: Digest test site availabl", "content": "On Wed, 22 Apr 1998, Dave Kristol wrote:\n\n> I've upgraded my server at <http://portal.research.bell-labs.com:8000>\n> to support Digest authentication with qop=auth-int.  The home page\n> there points to some useful links for testing odds and ends.\n> \n> N.B.  qop=auth-int is supported only on HTTP/1.1 requests.\n> \n\nI am somewhat behind Dave in implementation (and likely to stay that\nway given my current schedule).  I have implemented the new\nspec, but so far only with qop=auth.  However, I do have \nContent-MD5 implemented for static documents.\n\nBoth of these have barely been tested so I would appreciate\nfeedback.  The URL is\n\n     http://hopf.math.nwu.edu/testpage/\n\nThat document has Content-MD5.  It contains a link to a digest\nprotected document.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-2300269"}, {"subject": "Re: Status of HTTPWG document", "content": "On Wed, 22 Apr 1998, Larry Masinter wrote:\n\n> On Fri 3/13/98 5:28 PM, the 'last LAST CALL' for three documents\n> (v11-spec-rev, authentication, state-man-mec) was issued, and scheduled\n> to expire March 27.\n> \n> The second document (authentication) has had substantial comments,\n> which I believe will require a revision and another (brief) last\n> call. More importantly, we need better substantiation of testing\n> of interoperability of the independent implementations. We're still\n> hoping for Draft Standard status for these two documents together,\n> since v11-spec-rev should not move forward without a credible\n> authentication mechanism at the same maturity level.\n\nThe previous \"last call\" was pretty sudden. \n\nOne thing that I don't like is\n\n    nc-value         = 8LHEX\n\nIt should be \n\n    nc-value         = *LHEX\n\n\nAlso in Authentication-Info I think it should be \n\n     auth-info          = 1#( [nextnonce] | [ message-qop ]\n                            | [ response-auth ] | [ cnonce ]\n                            | [nonce-count] )\n\n\ninstead of \n\n     auth-info          = 1#(nextnonce | [ message-qop ]\n                            | [ response-auth ] | [ cnonce ]\n                            | [nonce-count] )\n\nwith [] around nextnonce indicating that it is optional.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-2308516"}, {"subject": "question in caching in the HTTP1.", "content": "Hi\nmy question concern caching in http 1.1 about the expiration model\nrfc 2068 page 53\nyou get the corrected_initial_age by adding  corrected_received_age to a new\nterm which is (now-request_time)\nI think this will give some misleading results because now-request time is\nequal to the transaction period. \nEample\n *a---------------------------*b_______*c------------------------*d\n\na is the user agent\nb,c are x caching servers (clients)\nd is the origin server\n\nif a send a request 1 to server d and then we calculate the\ncorrected_recieved_age.\nwhich is equal to the period between sending the response from the server\ntill this moment, it will be enough to indicate the real age of the\nresponse. \nbut if we add now - request time it will be overhead which is not the real\nvalue of the age of entry\nNUMERIC exampe\nif a send the request at time 0 and d received the request at time 6 and\nsend the respond at time 8 and a recieves the response at time 10, the date\nvalue will be 8 and now will be 10 so the corrected recieved age will be 2,\nnow let us say that this response was created at the server at time 7 and\nwith life_time (max age) equals to 11 i.e if the age is less than 10 for our\ncase we should not try to reload and the response for any farther request\nbefore the time 18 (7+11) should be fresh, so no more request to the origin\nserver.\nnow let us go to step two and calculate the corrected_inital_age (which I\ndon't agree with) the value will be the correct age + (now - reaquest time)\nnow = 11 < 18 should not reload\nbut due to the calulations\nage = 2 ( initial date) + (11 - 0) = 13 > max age then stale ##\n\n\nso I propose to remove this overhead from the age calculation\n\nAhmed Mokhtar\n\n\n\n", "id": "lists-012-2318066"}, {"subject": "Re: Status of HTTPWG document", "content": "On Wed, 22 Apr 1998, John Franks wrote:\n\n> One thing that I don't like is\n> \n>     nc-value         = 8LHEX\n> \n> It should be \n> \n>     nc-value         = *LHEX\n\nI was the one that specified the fixed size (implying leading zeros)\noriginally (though I think that I used 4 rather than 8); my thinking was\nthat it would avoid any potential problems with one side using the\nleading zeros and the other not.  I can live with it either way.\n\n> Also in Authentication-Info I think it should be \n> \n>      auth-info          = 1#( [nextnonce] | [ message-qop ]\n>                             | [ response-auth ] | [ cnonce ]\n>                             | [nonce-count] )\n> \n> \n> instead of \n> \n>      auth-info          = 1#(nextnonce | [ message-qop ]\n>                             | [ response-auth ] | [ cnonce ]\n>                             | [nonce-count] )\n> \n> with [] around nextnonce indicating that it is optional.\n\n  Agreed.\n\n\n\n", "id": "lists-012-2327040"}, {"subject": "Re: Status of HTTPWG document", "content": "On Thu, 23 Apr 1998, Scott Lawrence wrote:\n\n> \n> On Wed, 22 Apr 1998, John Franks wrote:\n> \n> > One thing that I don't like is\n> > \n> >     nc-value         = 8LHEX\n> > \n> > It should be \n> > \n> >     nc-value         = *LHEX\n> \n> I was the one that specified the fixed size (implying leading zeros)\n> originally (though I think that I used 4 rather than 8); my thinking was\n> that it would avoid any potential problems with one side using the\n> leading zeros and the other not.  I can live with it either way.\n> \n\nHow about doing what is done for the chunked TE and using\n\n       lhex-no-zero    = <LHEX excluding \"0\">\n       nc-value        = lhex-no-zero  *LHEX\n\nto forbid the leading zeros.  I think that at least 99% of the time the\nnc-value will be a single digit.  It is kind of wasteful and silly to\nrequire an enormous number of \n\n        nc=00000001\n\nfields.  I suppose for backwards compatibility we should as\nservers to tolerate the leading zeros.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-2335691"}, {"subject": "Re: question in caching in the HTTP1.", "content": "MOKHTAR Ahmed CNET/DSM/REN <ahmed.mokhtar@cnet.francetelecom.fr> writes:\n    my question concern caching in http 1.1 about the expiration model\n    rfc 2068 page 53 you get the corrected_initial_age by adding\n    corrected_received_age to a new term which is (now-request_time) I\n    think this will give some misleading results because now-request\n    time is equal to the transaction period.\n\nFirst of all, you should refer to draft-ietf-http-v11-spec-rev-03,\nnot RFC2068, for any discussion of the text of the HTTP/1.1\nspecification.\n\nIn any case, the goal of this Age calculation is to support the\n\"transparency\" requirements for caching, described at the beginning\nof Section 13 of the specification.  In particular, we have \n\n  A basic principle is that it must be possible for the clients to\n  detect any potential relaxation of semantic transparency.\n\nThe goal of the age calculation is to discover whether a response\nis fresh.  In order to meet this principle, we need to err on the\nside of caution; that is, we would rather treat a fresh response\nas stale (and needed revalidation) than treating a stale response\nas fresh (thereby presenting a user with out-of-date information).\n\nGiven that there is no way to calculate the precise age of a\nresponse in a distributed system with (possibly) unsynchronized\nclocks, the Age calculation described in the HTTP/1.1 specification\nis designed to overestimate the Age, rather than to underestimate\nit.  This preserves the property that clients can detect potentially\nstale responses; it may cause a few unnecessary reloads, but generally\nif the remaining freshness lifetime of a response is so close to\nthe error in the Age calculation that it causes an unnecessary\nreload, then probably we are dealing with a response whose original\nfreshness lifetime is short ... this means that someone really wants\nto avoid a stale response in this case.\n\n-Jeff\n\n\n\n", "id": "lists-012-2344176"}, {"subject": "Proxies and gethostbynam", "content": "Clarification is needed on what error is to be returned if a Proxy\ntimes out looking up a hostname.  The spec is silent on what is a very\ncommon failure.\n\nMy current opinion is that returning 504 Gateway Timout is correct, but \nthat clarification to the spec is in order.  But other options include \nintroducting other error codes, or less likely, some other existing error \ncode.\n\nOther opinions?\n- Jim Gettys\n\n> View: Browse HTML    Browse Raw Text\n> From: jg@pa.dec.com (Jim Gettys)\n> Date: Mon, 27 Apr 1998 07:27:49 -0700\n> To: Ari Luotonen <luotonen@netscape.com>\n> Cc: erik@netscape.com, montulli@netscape.com,\n> phadnis@netscape.com\n> Cc: fielding@ics.uci.edu, mogul@pa.dec.com,\n> frystyk@w3.org,\n>         masinter@parc.xerox.com\n> Subject: Re: proxies and gethostbyname [RESEND]\n> \n> Yup.  I'm surprised the spec isn't clear on this point as well, given\n> how common this problem is....\n> \n> We need to clarify this for draft standard, in my opinion.  This is at\n> least an editorial problem that must get fixed.\n> \n> My initial reaction is also that it should be a 500 class error.\n> \n> The closest code currently defined is 504 Gateway Timeout.  Even there,\n> clarification is really needed.\n> \n> Do you have any problems with me taking the discussion to the\n> working group mailing list for resolution?\n>                                 - Jim\n> \n> \n> > Sender: luotonen@netscape.com (Ari\n> > Luotonen)\n> > From: Ari Luotonen <luotonen@netscape.com>\n> > Date: Fri, 24 Apr 1998 20:33:00 -0700\n> > To: erik@netscape.com\n> > Cc: montulli@netscape.com, phadnis@netscape.com,\n> > jg@pa.dec.com\n> > Subject: Re: proxies and gethostbyname [RESEND]\n> >\n> > [Resending with JG's e-mail corrected.]\n> >\n> > Eric,\n> >\n> > I'm Cc'ing Jim Gettys of W3C re: \"should there be an HTTP status\n> > code for proxy failure to DNS resolve\".\n> >\n> > Jim,\n> >\n> > Please see below. --Thanks!!\n> >\n> > -- Cheers, Ari --\n> >\n> > Erik van der Poel wrote:\n> > > Ari Luotonen wrote:\n> > >> Erik van der Poel wrote:\n> > >>>\n> > >>> Hi Lou and Ari,\n> > >>>\n> > >>> If Navigator is using an HTTP proxy, and it asks the proxy to\n> > >>> resolve a URL that contains a non-existent hostname (i.e.\n> > >>> gethostbyname() fails in the proxy), what is the proxy\n> > >>> supposed to return? Is it supposed to return 404, or 400, or\n> > >>> what?\n> > >>\n> > >> Our UNIX proxy returns 500.  It shouldn't be 404 because I\n> > >> think HTTP status codes refer to objects within servers, not\n> > >> the existence or non-existence of servers themselves.  Since\n> > >> the proxy is not the originator of the data, it cannot\n> > >> definitively determine the existence or non-existence of a\n> > >> server (or an object within a server) -- in a case where it\n> > >> fails to connect to one (regardless of whether the reason is\n> > >> DNS, server down, or midstream proxy down).  Only an origin\n> > >> server can say something like \"404 Not found\" or \"410 Gone\".\n> > >>\n> > >> It shouldn't be 400 because the request format itself was ok.\n> > >> I further think that it shouldn't be any 4xx class status code\n> > >> (client's request bad), since the problem may be a temporary\n> > >> DNS problem, or a problem on the proxy itself.  I would\n> > >> therefore continue to vote that 500 (or some other 5xx class\n> > >> status code) is the correct code for this case.\n> > >\n> > > This all sounds very reasonable. I guess I'm just a little\n> > > surprised that the HTTP spec itself does not specify the error\n> > > number to use in this case. We seem to be doing the right\n> > > thing: i.e. we are passing the absolute URL to the proxy, and\n> > > asking the proxy to retrieve that object. Is the omission of\n> > > the error number for this case a \"bug\" in the HTTP spec?\n\n\nattached mail follows:\nYup.  I'm surprised the spec isn't clear on this point as well, given\nhow common this problem is....\n\nWe need to clarify this for draft standard, in my opinion.  This is at\nleast an editorial problem that must get fixed.\n\nMy initial reaction is also that it should be a 500 class error.\n\nThe closest code currently defined is 504 Gateway Timeout.  Even there,\nclarification is really needed.\n\nDo you have any problems with me taking the discussion to the\nworking group mailing list for resolution?\n- Jim\n\n\n> View: Browse HTML    Browse Raw Text\n> Sender: luotonen@netscape.com (Ari\n> Luotonen)\n> From: Ari Luotonen <luotonen@netscape.com>\n> Date: Fri, 24 Apr 1998 20:33:00 -0700\n> To: erik@netscape.com\n> Cc: montulli@netscape.com, phadnis@netscape.com,\n> jg@pa.dec.com\n> Subject: Re: proxies and gethostbyname [RESEND]\n> \n> [Resending with JG's e-mail corrected.]\n> \n> Eric,\n> \n> I'm Cc'ing Jim Gettys of W3C re: \"should there be an HTTP status\n> code for proxy failure to DNS resolve\".\n> \n> Jim,\n> \n> Please see below. --Thanks!!\n> \n> -- Cheers, Ari --\n> \n> Erik van der Poel wrote:\n> > Ari Luotonen wrote:\n> >> Erik van der Poel wrote:\n> >>>\n> >>> Hi Lou and Ari,\n> >>>\n> >>> If Navigator is using an HTTP proxy, and it asks the proxy to\n> >>> resolve a URL that contains a non-existent hostname (i.e.\n> >>> gethostbyname() fails in the proxy), what is the proxy\n> >>> supposed to return? Is it supposed to return 404, or 400, or\n> >>> what?\n> >>\n> >> Our UNIX proxy returns 500.  It shouldn't be 404 because I\n> >> think HTTP status codes refer to objects within servers, not\n> >> the existence or non-existence of servers themselves.  Since\n> >> the proxy is not the originator of the data, it cannot\n> >> definitively determine the existence or non-existence of a\n> >> server (or an object within a server) -- in a case where it\n> >> fails to connect to one (regardless of whether the reason is\n> >> DNS, server down, or midstream proxy down).  Only an origin\n> >> server can say something like \"404 Not found\" or \"410 Gone\".\n> >>\n> >> It shouldn't be 400 because the request format itself was ok.\n> >> I further think that it shouldn't be any 4xx class status code\n> >> (client's request bad), since the problem may be a temporary\n> >> DNS problem, or a problem on the proxy itself.  I would\n> >> therefore continue to vote that 500 (or some other 5xx class\n> >> status code) is the correct code for this case.\n> >\n> > This all sounds very reasonable. I guess I'm just a little\n> > surprised that the HTTP spec itself does not specify the error\n> > number to use in this case. We seem to be doing the right\n> > thing: i.e. we are passing the absolute URL to the proxy, and\n> > asking the proxy to retrieve that object. Is the omission of\n> > the error number for this case a \"bug\" in the HTTP spec?\n\n\n\n", "id": "lists-012-2353494"}, {"subject": "Re: Proxies and gethostbynam", "content": "    Clarification is needed on what error is to be returned if a Proxy\n    times out looking up a hostname.  The spec is silent on what is a\n    very common failure.\n\n    My current opinion is that returning 504 Gateway Timout is correct,\n    but that clarification to the spec is in order.  But other options\n    include introducting other error codes, or less likely, some other\n    existing error code.\n    \nBefore we charge off on a detailed discussion of what to do when a\nproxy times out on a DNS translation, we might want to consider whether\nis error is part of a larger class.\n\nFor example,\nwhat if the proxy has no route to the server?  (i.e.,\nit gets a \"host unreachable\" error).  This may also\nbe a transient error (cf. the paper in USITS '97 on\nthe observed routing instability in the Internet).  Is\nthis the same as a \"504 Gateway Timeout\"?\n\nI'm not sure whether there are any other errors in this class.\nBut if these are the only two that we can think of, perhaps\nthe proper name for the error is\n\n504 Temporarily Unable to Forward Request\n\nwith an entity body that explains the reason in more detail.\nI.e., the relevant question is not \"was this a timeout or not?\"\nbut \"is the error (probably) transient or is it permanent?\"\n\nIf people think that these cases are actually different enough\nto warrant separate error codes, then I think we ought to add\na \"No route to next-hop server\" code if we add a \"DNS timeout\" code.\n\n-Jeff\n\n\n\n", "id": "lists-012-2372737"}, {"subject": "Re: Proxies and gethostbynam", "content": "On Mon, 27 Apr 1998, Jim Gettys wrote:\n\n> Clarification is needed on what error is to be returned if a Proxy\n> times out looking up a hostname.  The spec is silent on what is a very\n> common failure.\n> \n> My current opinion is that returning 504 Gateway Timout is correct, but \n> that clarification to the spec is in order.  But other options include \n> introducting other error codes, or less likely, some other existing error \n> code.\n> \n> Other opinions?\n\nI agree that it is a 5xx error, but I believe that overloading an\nexisting error is wrong. It is important to give the network support\ninfrastructure more information as well as allowing the browser the\nchance to provide a decent message to the user.  \n\n504 Gateway Timeout ... means the gateway timed out waiting for the\n               resource itself\n\n5xx Gateway DNS Timeout     ... means that the gateway timed out waiting\n                    for a response from the DNS\n5xy Gateway DNS Unknown     ... means that the DNS provide a definate\n                    response that the host name is unknown\n5xz Gateway Request Refused ... means that the gateway's attempt to\n                    connect the remote server was refused \n\nConsidering this issue and looking overthe 5xx errors, I notice what to\nme is a glaring oversight .... no clear differentiation between \nerrors reported by server and errors reported by a proxy about its\nattempt to contact a server. This makes intelligent problem\ndetermination more difficult.\n\nDave Morris\n\n\n\n", "id": "lists-012-2381074"}, {"subject": "Re: Proxies and gethostbynam", "content": "On Mon, 27 Apr 1998, Jeffrey Mogul wrote:\n\n>     Clarification is needed on what error is to be returned if a Proxy\n>     times out looking up a hostname.  The spec is silent on what is a\n>     very common failure.\n> \n>     My current opinion is that returning 504 Gateway Timout is correct,\n>     but that clarification to the spec is in order.  But other options\n>     include introducting other error codes, or less likely, some other\n>     existing error code.\n>     \n> Before we charge off on a detailed discussion of what to do when a\n> proxy times out on a DNS translation, we might want to consider whether\n> is error is part of a larger class.\n> \n> For example,\n> what if the proxy has no route to the server?  (i.e.,\n> it gets a \"host unreachable\" error).  This may also\n> be a transient error (cf. the paper in USITS '97 on\n> the observed routing instability in the Internet).  Is\n> this the same as a \"504 Gateway Timeout\"?\n> \n> I'm not sure whether there are any other errors in this class.\n> But if these are the only two that we can think of, perhaps\n> the proper name for the error is\n> \n> 504 Temporarily Unable to Forward Request\n> \n> with an entity body that explains the reason in more detail.\n> I.e., the relevant question is not \"was this a timeout or not?\"\n> but \"is the error (probably) transient or is it permanent?\"\n> \n> If people think that these cases are actually different enough\n> to warrant separate error codes, then I think we ought to add\n> a \"No route to next-hop server\" code if we add a \"DNS timeout\" code.\n\nAs I tried to say, there are more errors than just a timeout between\nthe proxy and the down stream server and they need to be differentiated.\n\nDave\n\n\n\n", "id": "lists-012-2389910"}, {"subject": "Re: Proxies and gethostbynam", "content": ">My current opinion is that returning 504 Gateway Timeout is correct, but \n>that clarification to the spec is in order.  But other options include \n>introducting other error codes, or less likely, some other existing error \n>code.\n\nDitto.  504 is sufficient, but we might do better.  The general question\nin deciding on a response code is whether the \"here's what you should do\nnext\" semantics are equivalent to an existing response code, and whether\nthe \"next\" thing can be accomplished automatically or only by some real\nperson looking at the error response message.\n\nIn my opinion, a failure to resolve a DNS name is equivalent to a\nfailure to connect to the resolved IP address -- the reason being that\nboth are components of the resource resolution process.  A more general\nstatus code would therefore be a new \"5aa Unable to Resolve URI\", with\nthe explanation of why being included in the response message.  This is\ndifferent from 404 Not Found since it does not imply an authoritative\nresponse.  Likewise, we would gain nothing from further differentiation\nof all possible resolution failure causes into separate error codes,\nsince there is nothing whatsoever that the client can do \"next\" aside\nfrom display the cause or try a different proxy (i.e., all of those\nfailures are equivalent).\n\n....Roy\n\n\n\n", "id": "lists-012-2399364"}, {"subject": "Re: Proxies and gethostbynam", "content": "On Tue, 28 Apr 1998, Roy T. Fielding wrote:\n\n> >My current opinion is that returning 504 Gateway Timeout is correct, but \n> >that clarification to the spec is in order.  But other options include \n> >introducting other error codes, or less likely, some other existing error \n> >code.\n> \n> Ditto.  504 is sufficient, but we might do better.  The general question\n> in deciding on a response code is whether the \"here's what you should do\n> next\" semantics are equivalent to an existing response code, and whether\n> the \"next\" thing can be accomplished automatically or only by some real\n> person looking at the error response message.\n> \n> In my opinion, a failure to resolve a DNS name is equivalent to a\n> failure to connect to the resolved IP address -- the reason being that\n> both are components of the resource resolution process.  A more general\n> status code would therefore be a new \"5aa Unable to Resolve URI\", with\n> the explanation of why being included in the response message.  This is\n> different from 404 Not Found since it does not imply an authoritative\n> response.  Likewise, we would gain nothing from further differentiation\n> of all possible resolution failure causes into separate error codes,\n> since there is nothing whatsoever that the client can do \"next\" aside\n> from display the cause or try a different proxy (i.e., all of those\n> failures are equivalent).\n\nI disagree ... there is a large difference between not being able to\nresolve a name and not being able to connect to an IP address.\nThe first order problem resolution for the DNS failure will in may cases\nbe the user checking the URL they typed for correctness. Failure to\nconnect to the resolved IP address is also different from connection\nrefused in terms of recovery strategy.  All of those distinctions are\nuseful for further problem determination by human analysists. And even\nwhen the automated portion of a client can't distinguish, the ability to\ncollect failure statistics with distinct failure reasons may be helpful.\n\nDave Morris\n\n\n\n", "id": "lists-012-2407910"}, {"subject": "Re: Proxies and gethostbynam", "content": ">I disagree ... there is a large difference between not being able to\n>resolve a name and not being able to connect to an IP address.\n\nrom the point of view of the proxy, yes.  From the user's perspective,\nboth indicate a failure of the proxy to map the identifier to a service.\nWhether that failure was due to a down IP interface or a bad DNS entry or\na nonexistant DNS entry or a very slow DNS gateway may be useful for\nfurther diagnostics (and the proxy is certainly capable of recording\nthose facts), but none of these can be used by the user agent to\nautomatically handle the failure in a distinguishable manner, nor\nis it the user agent's responsibility to track the performance\ncharacteristics if the proxy's external network environment.  Even if\nsuch a thing were desirable for a given proxy, the design principles of\nsimplicity and separation of concerns suggest that it be transmitted\nseparately from the main protocol stream (e.g., via SNMP monitoring).\n\n....Roy\n\n\n\n", "id": "lists-012-2418211"}, {"subject": "Re: Proxies and gethostbynam", "content": "On Tue, 28 Apr 1998, Roy T. Fielding wrote:\n\n> >I disagree ... there is a large difference between not being able to\n> >resolve a name and not being able to connect to an IP address.\n> \n> >From the point of view of the proxy, yes.  From the user's perspective,\n> both indicate a failure of the proxy to map the identifier to a service.\n\nNo,  one is a failure to map to the service, the second is a failure of\nthe service. Failure to map is often under the user's control or in the\ncase of some UAs which try multiple URLs, the UA's control.\n\nDave\n\n\n\n", "id": "lists-012-2426432"}, {"subject": "TE: identity; q=", "content": "So, I'm thinking about implementing this, and it occurs to me that it's\ncrazy to allow TE: identity; q=0.\n\nConsider:  The HTTP/1.1 spec. already says that \"chunked\" is always\nacceptable.  If \"identity\" were also always acceptable, then any server\nthat just implements those two (a common case?) could ignore the TE\nheader altogether, thus saving processing and code space.\n\nMost content gets returned as \"identity\".  But as the spec. now stands,\na fully conforming server must check the TE header for \"identity; q=0\",\njust so it knows to return a 406 (Not Acceptable).  That seems crazy to\nme.  What earthly reason could a client have for *not* accepting\nidentity?\n\nDave Kristol\n\n\n\n", "id": "lists-012-2434061"}, {"subject": "Digest Authentication:  nc", "content": "In my implementation of Digest, I don't verify that a given nonce count\n(nc=) has appeared only once.  My server simply doesn't retain that\nkind of state.\n\nSo I'm wondering, *is* anyone planning to implement checks on reuse\nof nc?\n\nDave Kristol\n\n\n\n", "id": "lists-012-2441429"}, {"subject": "Re: Digest Authentication:  nc", "content": "On Wed, 29 Apr 1998, Dave Kristol wrote:\n\n> In my implementation of Digest, I don't verify that a given nonce count\n> (nc=) has appeared only once.  My server simply doesn't retain that\n> kind of state.\n> \n> So I'm wondering, *is* anyone planning to implement checks on reuse\n> of nc?\n\n  Our internal nonce handling already has a nonce use count, so yes, I\nexpect that when I do this I'll be checking it.\n\n\n\n", "id": "lists-012-2448426"}, {"subject": "On te, and on case in the digest example", "content": "1) Dave Kristol  wrote\n>So, I'm thinking about implementing this, and it occurs to me that it's\ncrazy to allow TE: identity; q=0.\n\nFor what it's worth, it does seem silly to me to.\n\nIn my implementation, I only check TE for 1.1 clients (perhaps that's a\nmistake?), and when identity;q=0,  I  chunk it and send it back.\n\n2) The digest example (on pg 17 of the digest authentication draft):\n\n\n\n", "id": "lists-012-2456299"}, {"subject": "On te, and on case in the digest example (2nd send", "content": "2) The example on pg 17 of the digest authentication draft\nshould mention a  few gotchas:\n\ni) method of GET is used (GET, not get, is used)\n\nii) the 32 hex character md5 (not the 128 bit) is used in H(), with\nlower case abcdef characters used.   Given that the content-md5\nheader uses a pack64 of the 128 bit hash, reiterating that the\nexample uses a \"lower case 32 hex-char\" hash might save a \nfew headaches.\n\niii)The example nonce (pg 9)\n time-stamp H(time-stamp \":\" ETag \":\" private-key)   \nwas a bit hard to read -- at least I missed that it meant\n\"concatenate time-stamp with  H(time-stamp \":\" ETag \":\" private-key),\nand then you can use the unhashed time-stamp to verify the\nnonce.\n\n\n\n", "id": "lists-012-2462994"}, {"subject": "Re: TE: identity; q=", "content": "Dave Kristol writes:\n\n    So, I'm thinking about implementing this, and it occurs to me that it's\n    crazy to allow TE: identity; q=0.\n    \n    Consider:  The HTTP/1.1 spec. already says that \"chunked\" is always\n    acceptable.  If \"identity\" were also always acceptable, then any server\n    that just implements those two (a common case?) could ignore the TE\n    header altogether, thus saving processing and code space.\n    \n    Most content gets returned as \"identity\".  But as the spec. now stands,\n    a fully conforming server must check the TE header for \"identity; q=0\",\n    just so it knows to return a 406 (Not Acceptable).  That seems crazy to\n    me.  What earthly reason could a client have for *not* accepting\n    identity?\n    \nWe had a similar discussion around Accept-Encoding several years ago.\n\nThe reason for a client to say \"I don't want you to send me an\nidentity content-coding\" was that it would presumably be doing\nthis in a context where compression was not only acceptable but\nalso highly desirable.  E.g.,\n\nAccept-Encoding: gzip, identity; q=0\n\nFor example, the client might be paying based on bytes received, and\nwould rather give the user a chance to intervene before doing a\nlarge and expensive transfer.  (This is my recollection of the\ntheory behind this.)  If delta-encoding ever becomes widely\nused, the difference in cost is potentially even larger than with\ncompression.\n\nFor transfer-coding, the situation is made murkier by the requirement\nthat \"chunked\" is always acceptable.  This means that a client sending\n\nTE: gzip, identity; q=0\n\nin hopes of getting a compressed result, or nothing, could legally\nbe sent a response that is chunked but not compressed (i.e., even\nmore bytes than the identity encoding!)  And you can't even say\nsomething like\n\nTE: gzip, identity; q=0, chunked; q=0\n\nto force the use of gzip, because we're requiring the use of\n\"chunked\" with any non-identity transfer-coding.\n\nIn short, the rationale that applied to Accept-Encoding probably\ndoes not apply here.  I'm not 100% sure that there might not be\nanother rationale (for TE) that someone had in mind, though.\n\n-Jeff\n\n\n\n", "id": "lists-012-2470094"}, {"subject": "TR: question in caching in the HTTP1.", "content": "> ----------\n> De : MOKHTAR Ahmed CNET/DSM/REN\n> Date?:jeudi 30 avril 1998 08:22\n> A :'Jeffrey Mogul'\n> Objet :RE: question in caching in the HTTP1.1 \n> \n> I didn't understand clearly what is the relation between transparency and\n> the error on the age calculation. \n> about the synch. between caches, may be the SCSP solves the problem at\n> some time in the future.\n> and talking about the transparency, what do you think about CARP\n> effeciency in that matter. I mean don't you think that CARP violate the\n> transparency.\n> \n> Ahmed\n> \n> ----------\n> De : Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n> Date?:jeudi 23 avril 1998 21:36\n> A :MOKHTAR Ahmed CNET/DSM/REN\n> Cc :'http-wg@cuckoo.hpl.hp.com'\n> Objet :Re: question in caching in the HTTP1.1 \n> \n> MOKHTAR Ahmed CNET/DSM/REN <ahmed.mokhtar@cnet.francetelecom.fr> writes:\n>     my question concern caching in http 1.1 about the expiration model\n>     rfc 2068 page 53 you get the corrected_initial_age by adding\n>     corrected_received_age to a new term which is (now-request_time) I\n>     think this will give some misleading results because now-request\n>     time is equal to the transaction period.\n> \n> First of all, you should refer to draft-ietf-http-v11-spec-rev-03,\n> not RFC2068, for any discussion of the text of the HTTP/1.1\n> specification.\n> \n> In any case, the goal of this Age calculation is to support the\n> \"transparency\" requirements for caching, described at the beginning\n> of Section 13 of the specification.  In particular, we have \n> \n>   A basic principle is that it must be possible for the clients to\n>   detect any potential relaxation of semantic transparency.\n> \n> The goal of the age calculation is to discover whether a response\n> is fresh.  In order to meet this principle, we need to err on the\n> side of caution; that is, we would rather treat a fresh response\n> as stale (and needed revalidation) than treating a stale response\n> as fresh (thereby presenting a user with out-of-date information).\n> \n> Given that there is no way to calculate the precise age of a\n> response in a distributed system with (possibly) unsynchronized\n> clocks, the Age calculation described in the HTTP/1.1 specification\n> is designed to overestimate the Age, rather than to underestimate\n> it.  This preserves the property that clients can detect potentially\n> stale responses; it may cause a few unnecessary reloads, but generally\n> if the remaining freshness lifetime of a response is so close to\n> the error in the Age calculation that it causes an unnecessary\n> reload, then probably we are dealing with a response whose original\n> freshness lifetime is short ... this means that someone really wants\n> to avoid a stale response in this case.\n> \n> -Jeff\n> \n> \n\n\n\n", "id": "lists-012-2479148"}, {"subject": "Comments on draft HTTP/1.1 spec, v", "content": "Hello working group\n\nI sent this Sun, 26 Apr but received no reply:\n\n\nNinety computer science graduate students of mine are \nimplementing partial HTTP/1.1 servers.  They have been \nreading and analyzing the draft specification, versions 2 \nand 3, very carefully.  Overall the specification is \nvery strong.  however, we find a slight amount of confusion and \nambiguity, and would like to share our comments.\n\n5.3\nambiguity on application of request header semantics.\nSuppose a request contains an error.  Should all semantics \nof all correct headers in the request be applied?  For \nexample, consider\n\n  HEAD /index.html HTTP/1.1\n  Host: 128.122.238.94:9999\n  Connection: close\n  TE: chunked\n  If-Modified_since: foo\n \n\"foo\" is an invalid value for If-Modified, and therefore the \none and only correct response to this message will be 400. \nShould the response, containing a 400 bad request error, \nreturn the error chunked and then close the connection?  In \nmy view the answer is yes, and we should consider adding the \nfollowing sentence to the end of the first paragraph in \nsection 5.3.  (I do not fully understand the meaning of \n\"semantics equal to the parameters on a programming language \nmethod invocation.\")\n\"The semantics of all valid headers SHOULD be applied.\"  \n\n8.1.4\nin general, we're confused whether the specification wants \nservers to try to maintain persistent connections when \nerrors occur.  I believe that performance considerations \nmake it desirable for servers to maintain persistent \nconnections whenever possible. reasoning that a persistent \nconnection is simply a sequence of one-request connections, \nit makes sense to keep using the persistent one, just as an \nHTTP client keeps making requests following an error \nresponse.  Inserting the following sentence as the sixth \nparagraph in 8.1.4 would clarify this point.  \"If a server \ndetects an error (status 4xx) in a client request on a \npersistent connection, then the server SHOULD try to parse \nand respond to subsequent requests on the connection.\"\n\n10.4.9\nWe do not understand the semantics of the request time-out \n408 status code.  We have assumed a meaning which would be \nclarified by inserting the following sentence after the \nfirst sentence in section 10.4.9.  \"When a server times-out \na connection it SHOULD send a response with a request \ntime-out status before closing the connection.\"  It would \nalso be helpful to clarify the meaning of \"graceful \nclose\" in paragraph 2 of 8.1.4 by inserting the phrase \"by \nsending a 408 message\" following the word 'close'.\n\nWe hope these comments are helpful.\nThank you\n\n\n-- \nArthur P. Goldberg\nClinical Associate Professor of Computer Science\nartg@cs.nyu.edu       http://www.cs.nyu.edu/cs/faculty/artg\nFEB and MARCH, 1998: excuse my brief emails, RSI's limiting my typing\n715 Broadway, Room 711, Computer Science Department\nCourant Institute of Mathematical Science\nNew York University\nNew York, NY 10003-6806\n212 998-3014   fax 995-4123\n\n\n\n", "id": "lists-012-2489844"}, {"subject": "Slight editorial discrepancy in HTTP/1.1 Rev ", "content": "Hi,\n\nWhile reviewing Rev 3 in conjunction with some testing I found this slight\ndiscrepancy:\n\nIn Section 10.4.17 (416 Requested Range Not Satisfiable) it is stated:\n\n\"When this status code is returned for a byte-range request, the response\nMUST include a Content-Range entity-header field specifying the current\nlength of the selected resource (see section 14.16).\"\n\nIn section 14.16 (Content-Range) it is stated:\n\n\"A server sending a response with status code 416 (Requested range not\nsatisfiable) SHOULD include a Content-Range field with a\nbyte-range-resp-spec of \"*\"\"\n\nThe discrepancy is between the directives MUST and SHOULD.\n\nRegards,\n\nJohn Chamberlain\nIris Associates\nmailto:john_chamberlain@iris.com\n\n\n\n", "id": "lists-012-2500250"}, {"subject": "Re: TE: identity; q=", "content": "Jeffrey Mogul wrote:\n> \n> Dave Kristol writes:\n> \n>     So, I'm thinking about implementing this, and it occurs to me that it's\n>     crazy to allow TE: identity; q=0.\n>     [...]\n>     Most content gets returned as \"identity\".  But as the spec. now stands,\n>     a fully conforming server must check the TE header for \"identity; q=0\",\n>     just so it knows to return a 406 (Not Acceptable).  That seems crazy to\n>     me.  What earthly reason could a client have for *not* accepting\n>     identity?\n> \n> We had a similar discussion around Accept-Encoding several years ago.\n> \n> The reason for a client to say \"I don't want you to send me an\n> identity content-coding\" was that it would presumably be doing\n> this in a context where compression was not only acceptable but\n> also highly desirable.  E.g.,\n> \n>         Accept-Encoding: gzip, identity; q=0\n> [...]\n> For transfer-coding, the situation is made murkier by the requirement\n> that \"chunked\" is always acceptable.  This means that a client sending\n> \n>         TE: gzip, identity; q=0\n> \n> in hopes of getting a compressed result, or nothing, could legally\n> be sent a response that is chunked but not compressed (i.e., even\n> more bytes than the identity encoding!)  And you can't even say\n> something like\n> \n>         TE: gzip, identity; q=0, chunked; q=0\n> \n> to force the use of gzip, because we're requiring the use of\n> \"chunked\" with any non-identity transfer-coding.\n> \n> In short, the rationale that applied to Accept-Encoding probably\n> does not apply here.  I'm not 100% sure that there might not be\n> another rationale (for TE) that someone had in mind, though.\n\nIn the Accept-Encoding case, the client declares what it does and does\nnot understand, and it gives preferences.  In the TE case it does the\nsame.  But as you point out, the alternative to \"identity\" may be even\nmore bytes (using \"chunked\"), and the goal of reduced bytes transferred\nwill have been thwarted.  And I don't think there's any question that a\nclient can understand \"identity\" in all circumstances.\n\nI propose changing the wording so \"identity\" is always acceptable, just\nlike chunked.  The presence of q=0 (or q=0.001) would clue the server\nthat an alternative (as in your second TE example above) is really,\nreally, preferred, but not mandatory.\n\n[This raises a related question for Accept-Encoding:  does it really\nmake sense to suppress \"identity\" with \"q=0\"?  Wouldn't it be better to\nreceive the content without an encoding than not to receive it at all?]\n\nDave Kristol\n\n\n\n", "id": "lists-012-2508236"}, {"subject": "Re: Comments on draft HTTP/1.1 spec, v", "content": "Arthur P. Goldberg wrote:\n> \n> Hello working group\n> \n> I sent this Sun, 26 Apr but received no reply:\n\nThe working group archive does not show the earlier message, as far as I\ncan see.\n\n> \n> Ninety computer science graduate students of mine are\n> implementing partial HTTP/1.1 servers.  They have been\n> reading and analyzing the draft specification, versions 2\n> and 3, very carefully.  Overall the specification is\n> very strong.  however, we find a slight amount of confusion and\n> ambiguity, and would like to share our comments.\n\nThat's great!\n\n> \n> 5.3\n> ambiguity on application of request header semantics.\n> Suppose a request contains an error.  Should all semantics\n> of all correct headers in the request be applied?  For\n> example, consider\n> \n>   HEAD /index.html HTTP/1.1\n>   Host: 128.122.238.94:9999\n>   Connection: close\n>   TE: chunked\n>   If-Modified_since: foo\n> \n> \"foo\" is an invalid value for If-Modified, and therefore the\n> one and only correct response to this message will be 400.\n\nActually, no.  The most recent draft,\ndraft-ietf-http-v11-spec-rev-03.txt (and earlier ones, too), is quite\nclear on this.  In 14.25 If-Modified-Since the spec. says, \"If the\nrequest would normally result in anything other than a 200 (OK) status,\nor if the passed If-Modified-Since date is invalid, the response is\nexactly the same as for a normal GET.\"  Your example is the \"invalid\ndate\" case.\n\n> Should the response, containing a 400 bad request error,\n> return the error chunked and then close the connection?  In\n\nTE is a request header that provides advice from the client to the\nserver.  I think you meant Transfer-Encoding: chunked.\n\nThe server may send a chunked (error) response, and it need not close\nthe connection.\n\n> my view the answer is yes, and we should consider adding the\n> following sentence to the end of the first paragraph in\n> section 5.3.  (I do not fully understand the meaning of\n> \"semantics equal to the parameters on a programming language\n> method invocation.\")\n\nI think that's just trying to draw a parallel between HTTP and\nobject-oriented systems.  HTTP methods are similar to object methods,\nand the request headers are similar to the parameters one supplies in a\nmethod invocation.\n\n> \"The semantics of all valid headers SHOULD be applied.\"\n\nI'm not sure what, if anything, that improves.  I think it's already\nimplicit throughout the spec.\n\n> \n> 8.1.4\n> in general, we're confused whether the specification wants\n> servers to try to maintain persistent connections when\n> errors occur.  I believe that performance considerations\n> make it desirable for servers to maintain persistent\n> connections whenever possible. reasoning that a persistent\n\nYes, whenever possible.  It's considered very important.\n\n> connection is simply a sequence of one-request connections,\n> it makes sense to keep using the persistent one, just as an\n> HTTP client keeps making requests following an error\n> response.  Inserting the following sentence as the sixth\n> paragraph in 8.1.4 would clarify this point.  \"If a server\n> detects an error (status 4xx) in a client request on a\n> persistent connection, then the server SHOULD try to parse\n> and respond to subsequent requests on the connection.\"\n\nAgain, I think that's redundant.  If a server keeps a connection open,\nit does so presumably because it's prepared to accept further requests. \nOne reason for closing a connection, in fact, is that an error forces\nthe server to lose sync with the input stream.\n\n> \n> 10.4.9\n> We do not understand the semantics of the request time-out\n> 408 status code.  We have assumed a meaning which would be\n> clarified by inserting the following sentence after the\n> first sentence in section 10.4.9.  \"When a server times-out\n> a connection it SHOULD send a response with a request\n> time-out status before closing the connection.\"  It would\n\n\nI think your suggestion is redundant by harmless.  In general if there's\nan (error) response code for condition X, there's the implication that\nif X happens, then the server will send an X response.  Timeout is but\none example.\n\n> also be helpful to clarify the meaning of \"graceful\n> close\" in paragraph 2 of 8.1.4 by inserting the phrase \"by\n> sending a 408 message\" following the word 'close'.\n\nActually that phrase meant something quite different.  (The paragraph\nmentions both clients and servers, and a client can't send a 408\nmessage.)  The phrase means that the agent should try to close its\nconnection in a way that lets any packets in transit reach their\ndestination.  It's worded elliptically to avoid making it too\nTCP-centric, but what it really wants to say is \"avoid forcing a TCP\nreset on the connection prematurely, because the receiving end will miss\nbytes in transit\".  Those bytes could even include a 408 Request Timeout\nmessage.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2517619"}, {"subject": "Re: Comments on draft HTTP/1.1 spec, v", "content": "Thanks for the comments.\n\nI'll see if we can't clarify things in the next (hopefully last) draft.\n- Jim\n\n\n\n", "id": "lists-012-2530009"}, {"subject": "Re: Slight editorial discrepancy in HTTP/1.1 Rev ", "content": "John Chamberlain <jchamberlain@iris.com> writes:\n\n    While reviewing Rev 3 in conjunction with some testing I found this slight\n    discrepancy:\n    \n    In Section 10.4.17 (416 Requested Range Not Satisfiable) it is stated:\n    \n    \"When this status code is returned for a byte-range request, the response\n    MUST include a Content-Range entity-header field specifying the current\n    length of the selected resource (see section 14.16).\"\n    \n    In section 14.16 (Content-Range) it is stated:\n    \n    \"A server sending a response with status code 416 (Requested range not\n    satisfiable) SHOULD include a Content-Range field with a\n    byte-range-resp-spec of \"*\"\"\n    \n    The discrepancy is between the directives MUST and SHOULD.\n\nHmm.  It looks like this is my doing; see\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0226.html\nwhich proposed both of these paragraphs, later amended by\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0089.html\n\nI'm not sure why everyone failed to catch this discrepancy last year.\n\nI think the MUST in 10.4.17 probably ought to be changed to be a\nSHOULD.  MUST is probably too strong for this situation, since the\nunderlying goal is to avoid wasting a round-trip for the client\nto discover the actual length.  I.e., it's not mandatory for\ncorrect interoperability.\n\n-Jeff\n\n\n\n", "id": "lists-012-2537233"}, {"subject": "Re: Comments on draft HTTP/1.1 spec, v", "content": "> > ambiguity on application of request header semantics.\n> > Suppose a request contains an error.  Should all semantics\n> > of all correct headers in the request be applied?  For\n> > example, consider\n> > \n> >   HEAD /index.html HTTP/1.1\n> >   Host: 128.122.238.94:9999\n> >   Connection: close\n> >   TE: chunked\n> >   If-Modified_since: foo\n> > \n> > \"foo\" is an invalid value for If-Modified, and therefore the\n> > one and only correct response to this message will be 400.\n> \n> Actually, no.  The most recent draft,\n> draft-ietf-http-v11-spec-rev-03.txt (and earlier ones, too), is quite\n> clear on this.  In 14.25 If-Modified-Since the spec. says, \"If the\n> request would normally result in anything other than a 200 (OK) status,\n> or if the passed If-Modified-Since date is invalid, the response is\n> exactly the same as for a normal GET.\"  Your example is the \"invalid\n> date\" case.\n\nThe use of If-Modified-Since is an unfortunate example since its handling in the \ncase of an error is explicitly stated in the draft.  The question is more general:  \nif a request is sent and one of its headers contains an error resulting in a 400 \nBad Request status code, should the remaining, valid headers be parsed and \ntreated as usual?  That is, to consider an example similar to that above, \nconsider a request:\n\nGET /index.html HTTP/1.1\nTE: chunked\nConnection: close\n\nThis is a bad request due to the lack of the Host header field.  Should the \nserver respond with chunked data and then close the connection?  That is, \nshould the semantics of the valid headers apply?\n\n> > Should the response, containing a 400 bad request error,\n> > return the error chunked and then close the connection?  In\n> \n> TE is a request header that provides advice from the client to the\n> server.  I think you meant Transfer-Encoding: chunked.\n\nNo, he meant TE.  The client sent TE to request chunked transfer encoding, \nbut the request itself was bad.  Should the server nonetheless send the error \nbody in a chunked format (and then close the connection, as per the \nConnection: close header)?  You say yes, but we had trouble determining this \nfrom the current draft standard.\n\n\nAdam Donahue\n\n\nmailto:adam@cyber-guru.com\n\n\n\n", "id": "lists-012-2900441"}, {"subject": "Re: Comments on draft HTTP/1.1 spec, v", "content": "Adam M. Donahue wrote:\n> [...]\n> The use of If-Modified-Since is an unfortunate example since its handling in the\n> case of an error is explicitly stated in the draft.  The question is more general:\n> if a request is sent and one of its headers contains an error resulting in a 400\n> Bad Request status code, should the remaining, valid headers be parsed and\n> treated as usual?  That is, to consider an example similar to that above,\n> consider a request:\n> \n> GET /index.html HTTP/1.1\n> TE: chunked\n> Connection: close\n> \n> This is a bad request due to the lack of the Host header field.  Should the\n> server respond with chunked data and then close the connection?  That is,\n> should the semantics of the valid headers apply?\n\nTE is a bad example, too, for two reasons.  First, in your example,\n\"identity\" would still be allowed, and \"chunked\" is always allowed.  To\nsuppress \"identity\", if that's what you meant to convey, you would say\n\"TE: identity;q=0\".  Second, it's my opinion that \"identity\" should\nalways be allowed.  But that's another discussion.\n\n> \n> > > Should the response, containing a 400 bad request error,\n> > > return the error chunked and then close the connection?  In\n> >\n> > TE is a request header that provides advice from the client to the\n> > server.  I think you meant Transfer-Encoding: chunked.\n> \n> No, he meant TE.  The client sent TE to request chunked transfer encoding,\n> but the request itself was bad.  Should the server nonetheless send the error\n\nWell, TE doesn't work that way, exactly.  But let's focus on...\n\n> body in a chunked format (and then close the connection, as per the\n> Connection: close header)?  You say yes, but we had trouble determining this\n> from the current draft standard.\n\nYes, I overlooked what is a reasonable question, namely, must the\nserver, in returning the error, honor \"Connection: close\"?  And, in\ngeneral, must the server's error responses be consistent with all the\nheaders (and, for that matter, the method)?\n\nMumble.  I think the answer is yes.  Or at least the server should do\nits best.  Sometimes the headers could be contradictory, in which case\nthe server has to do its best to honor them.\n\nThat leads me to my own question:  If there's an error on a HEAD\nrequest, should the server return an entity, or just the headers. \n(Apparently the latter.)  Example:\n\nHEAD / HTTP/1.1\n<CRLF>\n\nThere's no Host header, so the server responds \"400 Bad Request\".  With,\nor without, entity?\n\nDave Kristol\n\n\n\n", "id": "lists-012-2909558"}, {"subject": "Re: Comments on draft HTTP/1.1 spec, v", "content": "On Fri, 1 May 1998, Dave Kristol wrote:\n\n> That leads me to my own question:  If there's an error on a HEAD\n> request, should the server return an entity, or just the headers. \n> (Apparently the latter.)  Example:\n> \n> HEAD / HTTP/1.1\n> <CRLF>\n> \n> There's no Host header, so the server responds \"400 Bad Request\".  With,\n> or without, entity?\n\nThe only reliable answer is without. But beyond that, the spec is pretty\nclear.  Same as GET except for no message-body. So if the client wants\nto know why\n   GET / HTTP/1.1\n   <CRLF>\nwould have failed, it must try a GET.\n\n\nThe flaw in the HEAD request design is that it should have returned its\nresult as an entity rather than as a simple modified GET response. But\nthat change is too late.\n\nDave Morris\n\n\n\n", "id": "lists-012-2919882"}, {"subject": "Re: Comments on draft HTTP/1.1 spec, v", "content": "On Fri, 1 May 1998, Dave Kristol wrote:\n\n> That leads me to my own question:  If there's an error on a HEAD\n> request, should the server return an entity, or just the headers. \n> (Apparently the latter.)  Example:\n> \n> HEAD / HTTP/1.1\n> <CRLF>\n> \n> There's no Host header, so the server responds \"400 Bad Request\".  With,\n> or without, entity?\n\n  I asked essentially this question a year or so ago - the consensus\nthen was that a response to HEAD _never_ gets a body.\n\n\n\n", "id": "lists-012-2928800"}, {"subject": "Re: Comments on draft HTTP/1.1 spec, v", "content": "> TE is a bad example, too, for two reasons.  First, in your example,\n> \"identity\" would still be allowed, and \"chunked\" is always allowed.  To\n> suppress \"identity\", if that's what you meant to convey, you would say\n> \"TE: identity;q=0\".  Second, it's my opinion that \"identity\" should\n> always be allowed.  But that's another discussion.\n\nHmm.  I should have used another encoding I guess.  I see what you're saying.\n\n> > body in a chunked format (and then close the connection, as per the\n> > Connection: close header)?  You say yes, but we had trouble determining this\n> > from the current draft standard.\n> \n> Yes, I overlooked what is a reasonable question, namely, must the\n> server, in returning the error, honor \"Connection: close\"?  And, in\n> general, must the server's error responses be consistent with all the\n> headers (and, for that matter, the method)?\n\nYes, this is exactly what I'm asking.\n\n> Mumble.  I think the answer is yes.  Or at least the server should do\n> its best.  Sometimes the headers could be contradictory, in which case\n> the server has to do its best to honor them.\n\nWe were having the same problem deciding for sure how to handle the above \ncases. \n\n> That leads me to my own question:  If there's an error on a HEAD\n> request, should the server return an entity, or just the headers. \n> (Apparently the latter.)  Example:\n> \n> HEAD / HTTP/1.1\n> <CRLF>\n> \n> There's no Host header, so the server responds \"400 Bad Request\".  With,\n> or without, entity?\n\nI think the spec is somewhat clear about this.  In section 10.4:\n\n    Except when responding to a HEAD request, the server SHOULD include an \nentity containing an explanation of the error situation, and whether it is a \ntemporary or permanent condition.\n\nI'd gather from the above that the HEAD example you specify would not return \nan entity body.\n\nAdam\n\nmailto:adam@cyber-guru.com\n\n\n\n", "id": "lists-012-2936443"}, {"subject": "What is the REAL Address of this Lis", "content": "In recent weeks I've received quite a few posts where the original\nincluded both:\n   http-wg@cuckoo.hpl.hp.com and\n   http-wg@hplb.hpl.hp.com\n\nReplying without deleting the second address results in a bounce message\nfor unknown user.  Where is this stray address coming from and can we\ntry and stop including it?\n\nThe evidence would seem to be that the convoluted ?UUCP? gateway address\nwhich seems to be in my address book from historical times works but isn't\nnecessary?\n\nDave Morris\n\n\n\n", "id": "lists-012-2945863"}, {"subject": "Digest auth and domain", "content": "    3.2.1 The WWW-Authenticate Response Header\n\n    domain\n      A space-separated list of URIs, as specified in RFC XURI [7]. The\n      intent is that the client could use this information to know the set\n      of URIs for which the same authentication information should be sent.\n      The URIs in this list may exist on different servers. If this keyword\n      is omitted or empty, the client should assume that the domain\n      consists of all URIs on the responding server.\n\nI'm uncomfortable with what the words say, and whether they say what\nthey're meant to say.  In truth I'm concerned about how much they\n*don't* say.\n\nI believe one intent is that something like\ndomain=\"/dir/\"\nmeans the credentials should be applied to all URIs of the form /dir/*.\nBut I don't think the words say that.\n\nI also wonder whether implementers think that\ndomain=\"/xyz\"\nmeans \"URI /xyz and all /xyz/*\", or just the URI /xyz.  The notion of\n\"prefix\" (which I think is implied here) is poorly defined (well,\ncompletely undefined), and I don't know what the consensus opinion is.\nMoreover, the consensus opinion should be made explicit.\n\nDave Kristol\n\n\n\n", "id": "lists-012-2953447"}, {"subject": "Re: Digest auth and domain, agai", "content": "Given that most clients are likely to  associate the realm information with\na \"prefix\", I'm wondering if  it's worth the trouble to\n(as a general rule) send the domain information (my current plans\nare not to bother).\n\n\n\n", "id": "lists-012-2960893"}, {"subject": "Re: Digest auth and domain, agai", "content": "On Fri, 1 May 1998, Daniel Hellerstein wrote:\n\n> Given that most clients are likely to  associate the realm information with\n> a \"prefix\", I'm wondering if  it's worth the trouble to\n> (as a general rule) send the domain information (my current plans\n> are not to bother).\n\nAs far as I know there are no general client implementations so it would\nseem unwise to assume what they would do and instead provide the protocol\nelement which gives more power.  With basic, the client which wants to\noptimize must make assumptions AND has no way to avoid prompting a user\nfor multiple servers.  Clients seem to assume that all URLs in the\ndirectory for which authentication was requested and below share the\nsame credentials.  Thus, there is currently no way other than an\nadditional round trip for the client to learn that the real world is\nbroader or narrower than the current heuristic.\n\nDave\n\n\n\n", "id": "lists-012-2967775"}, {"subject": "caching vs revalidation in http1.", "content": "There may be cases where an origin server has semi-permanent\nresources (that change every few hours or days).  If these change\nirregularly, allowing proxies to cache this may be problemmatic (i.e.;\none can not know a proper maxage value)\n\nA second best solution would be to allow proxies to cache, but\ninsist they perform a conditional get (an If-modified of If-no-match)\nbefore using the cached item.\n\nrom my reading of the ver 3 spec, there's no way of doing this\n(\"must-revalidate\" sounds like it should, but it's really a \"stale\" modifier).\n\nAm I missing something, or is there a notion that well designed caches\nwill make such an option unnecessary, or ....\n\n\n\n", "id": "lists-012-2976141"}, {"subject": "Re: caching vs revalidation in http1.", "content": "Daniel Hellerstein <danielh@MAILBOX.ECON.AG.GOV> writes:\n    There may be cases where an origin server has semi-permanent\n    resources (that change every few hours or days).  If these change\n    irregularly, allowing proxies to cache this may be problemmatic\n    (i.e.; one can not know a proper maxage value)\n\n    A second best solution would be to allow proxies to cache, but\n    insist they perform a conditional get (an If-modified of\n    If-no-match) before using the cached item.\n\n    From my reading of the ver 3 spec, there's no way of doing this\n    (\"must-revalidate\" sounds like it should, but it's really a \"stale\"\n    modifier).\n\n    Am I missing something, or is there a notion that well designed\n    caches will make such an option unnecessary, or ....\n\nThe main problem is a vague definition of the verb \"to cache\".\n\nThe naive (but perhaps linguistically reasonable) meaning of\n\"to cache\" is that the caching agent stores a response for\nlater use.  This is the normal way of looking at, say, the\ncache lines in a CPU's data cache: a line is removed from\na CPU cache if it's not legal to use it in the future.\n\nHowever, if you read the HTTP/1.1 specification, you will actually\nfind language like \"whether a cache may use the response to reply\nto a subsequent request without revalidation.\"  That is because\nwe want to be able to insist that a cache entry is valid before\nbeing \"used to reply to a subsequent request\", but (with some\nexeceptions noted below) we don't insist that this period of\nvalidity is continuous from the moment the cache entry is created\nto the moment that it is used.\n\nThat is: unlike a CPU data cache, an HTTP cache has a means to\ndetermine if a cache entry is valid or not ... hence, it need\nnot delete a cache entry whose validity is suspect.  \"Stale\"\nis another way of saying \"not sure if it is still valid.\"\n\nSo, when you write:\n\n    A second best solution would be to allow proxies to cache, but\n    insist they perform a conditional get (an If-modified of\n    If-no-match) before using the cached item.\n\nI would rephrase that as:\n\n    A second best solution would be to allow proxies to *store*\n    responses, but insist they perform a conditional get (an\n    If-modified of If-no-match) before using the cached item.\n\nIn fact, this is exactly what the\n\nCache-control: s-maxage=0\n\ndirective does.  It says that a proxy MUST consider the response stale\nimmediately, and that a proxy MUST revalidate the response  before\nevery use.  (\"The s-maxage directive also implies the semantics of the\nproxy-revalidate directive\".)\n\nHowever, s-maxage does not apply to non-shared (i.e., browser)\ncaches.  If you really meant:\n\n    A second best solution would be to allow *caches* to store\n    responses, but insist they perform a conditional get (an\n    If-modified of If-no-match) before using the cached item.\n\nthen the proper response header would be\n\nCache-control: must-revalidate, maxage=0\n\nIt says that any cache MUST treat the response as immediately\nstale, and further that it cannot honor other advice (from the\nclient's request or from a local configuration option) to ignore\nthis staleness.  Thus, it MUST revalidate before every use.\n\nThe asymmetry between s-maxage and maxage is because we realized\nthe need for this kind of semantics somewhat too late in the\nprocess to change the meaning of maxage.\n\n-Jeff\n\n\n\n", "id": "lists-012-2983492"}, {"subject": "Content-Encoding and ContentType in trailer", "content": "I have searched the open issues and believe that this is a new issue.\n\nIt seems that including Content-Encoding or Content-Type in the\ntrailer of a chunked transfer would be incorrect.  Allowing this would\nrequire that the entire message body be buffered until the trailer is\nreceived, so that the content codings may be removed, or the content\ninterpreted.  There doesn't seem to be any need for these headers to\nappear in the trailer, since this information should be known before the\n\nmessage transfer begins.  As far as I can tell, libwww would not\ncorrectly\nprocess a message with Content-Encoding headers in the trailer.\n\nIf I am incorrect in this, or if you are not soliciting public comments\nat this time, please accept my apologies.\n\nThank you.\n--\nPerry R. Ross     pross@platinum.com     800-526-9096 X4986\n\n\n\n", "id": "lists-012-2993693"}, {"subject": "Protection spaces and proxy server", "content": "     Background\n     ----------\n     \n     According to draft-ietf-http-authentication \"The protection space \n     determines the domain over which credentials can be automatically \n     applied. If a prior request has been authorized, the same credentials \n     MAY be reused for all other requests within that protection space..\".\n     \n     The protection space is defined as \"The realm value, in combination \n     with the canonical root URL of the server being accessed, defines the \n     protection space.\". Also, \"a single protection space cannot extend \n     outside the scope of its server\".\n     \n     \n     Query?\n     ------\n     \n     What is the protection space for a proxy server which forces \n     authentication? Does the \"..canonical root URL of the server being \n     accessed..\" refer to the proxie servers URL, or the origin servers \n     URL. The later would imply that the client should stop sending proxy \n     authorization headers whenever the protection space of the origin \n     server changes, even though the proxy has not changed.\n     \n     If proxy servers request authorization, it is likely that the same \n     authorization will be required for all/most resources accessed through \n     the proxy, and I must suppose that the protection space refers to the \n     proxies URL, and therefore all requests a client makes via that proxy \n     must require authorization as long as the realm remains constant. Is \n     this the case?\n     \n     \n     Thanks, Dominic.\n**********************************************************************\n\nThis email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. \nIf you have received this email in error please notify Content Technologies \non +44 118 9301300.\n\nThis message has been generated by MIMEsweeper and certifies that the message and attachments have been swept for all known and recorded computer viruses. \nMIMEsweeper 3.x protects your organization from content borne threats and malicious intent. Combined with firewalls MIMEsweeper provides a comprehensive network security solution.\n\nFor information regarding the MIMEsweeper family of products:\n\nPhone:  +44 118 9301300\nFax:    +44 118 9301301\nEmail:  info@mimesweeper.com\nSupport:msw.support@mimesweeper.com\nWorld Wide Web: http://www.mimesweeper.com\n\nMIMEsweeper: Content Security for Networks \n**********************************************************************\n\n\n\n", "id": "lists-012-3001594"}, {"subject": "caching &amp; revalidation, followup &ndash;&ndash; a question to proxy server author", "content": "Yesterday I inquired about how an origin server may inform\ndownstream caches (say, on proxy servers) that a given\nresponse may change suddenly -- implying that a cache\nmay store the content, but should always revalidate before using.\n\nrom the responses I've recieved (thanks Jeff), it appears that a \n   cache-control: max-age=0,must-revalidate\nor \n  cache-control: nocache\ncan have this effect. However,  this is subject to implementation,\nwith perhaps the latter hinting that \"it's not worth storing this response\". \n\nMy question is to current or potential authors of proxy servers\n(or other caches).:\n\n Should a conscientious web server use a particular response\nto hint that \n  a) this is subject to change -- so store it but always revalidate\nas opposed to \n b) don't bother storing this -- it's request specific?\n\nFor example, would a max-age=1,must-revalidate be a reasonable\nway of signaling this (with a tiny chance of sending a no longer\nvalid stored response)?   Or should origin servers not worry -- let the\ncaches manage their disk space as they see fit?\n\n\n\n", "id": "lists-012-3011537"}, {"subject": "RE: Protection spaces and proxy server", "content": "> -----Original Message-----\n> From: Dominic.Chambers@mimesweeper.com\n> [mailto:Dominic.Chambers@mimesweeper.com]\n> Sent: Tuesday, May 05, 1998 4:40 AM\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Protection spaces and proxy servers\n> \n>      \n>      Query?\n>      ------\n>      \n>      What is the protection space for a proxy server which forces \n>      authentication? Does the \"..canonical root URL of the \n> server being \n>      accessed..\" refer to the proxie servers URL, or the \n> origin servers \n>      URL. The later would imply that the client should stop \n> sending proxy \n>      authorization headers whenever the protection space of \n> the origin \n>      server changes, even though the proxy has not changed.\n>      \n>      If proxy servers request authorization, it is likely \n> that the same \n>      authorization will be required for all/most resources \n> accessed through \n>      the proxy, and I must suppose that the protection space \n> refers to the \n>      proxies URL, and therefore all requests a client makes \n> via that proxy \n>      must require authorization as long as the realm remains \n> constant. Is \n>      this the case?\n>      \n>      \nI beleive this is correct.  There are some complicated cases\nwhere this definition may fail to be true when cascaded proxies\nare used.  Ideally, each proxy in a chain can use a different realm,\nbut to the client it would appear as two different realms on\nthe same proxy since the client doesn't necessarily see the higher\nlevel proxies.\n\n---\nJosh Cohen <josh@microsoft.com>\nProgram Manager IE - Networking Protocols \n\n\n\n", "id": "lists-012-3019212"}, {"subject": "Re: Proxies and gethostbynam", "content": "At 12:53 4/28/98 -0700, Roy T. Fielding wrote:\n\n>>From the point of view of the proxy, yes.  From the user's perspective,\n>both indicate a failure of the proxy to map the identifier to a service.\n\nI think there is one situation that 504 doesn't cover and that is if the\nproxy doesn't *want* to proxy a certain URI, maybe because it can't\n(firewall etc.) or because it is not within it's trust domain etc. What are\nexisting proxies doing in this situation? Adding this would clarify the\nsituation a lot:\n\n418 Not Proxying\n\nThe server is not willing to proxy or gateway the Request-URI. No\nindication is given of whether the condition is temporary or permanent. The\nclient MAY repeat the request using either a different proxy or no proxy at\nall.\n\nThe 504 (Gateway Timeout) status code SHOULD be used if the server can not\nserve the request due to upstream errors.\n\nHenrik\n\n\n\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-3029823"}, {"subject": "Re: Proxies and gethostbynam", "content": "On Mon, 11 May 1998, Henrik Frystyk Nielsen wrote:\n\n> I think there is one situation that 504 doesn't cover and that is if the\n> proxy doesn't *want* to proxy a certain URI, maybe because it can't\n\nI continue to disagree ... there is a distinct difference from the \nperspective of recovery whether the DNS consulted by proxy is unable\nto resolve the host name or whether the host fails to respond. Hiding\nall failures under a single code does nothing for usability or\nperceived reliability.\n\n\n> 418 Not Proxying\n> \n> The server is not willing to proxy or gateway the Request-URI. No\n> indication is given of whether the condition is temporary or permanent. The\n> client MAY repeat the request using either a different proxy or no proxy at\n> all.\n> \n> The 504 (Gateway Timeout) status code SHOULD be used if the server can not\n> serve the request due to upstream errors.\n\nWhy bother .... there is a whole lot less an end user will be able to do\nwith this distinction over a simple not authorized response than over\na mistyped host name in a URL ... the suggested 418 may be useful to know\nbut on a priority scale less important to know than better granularity\non conditions covered by 504.\n\nDave Morris\n\n\n\n", "id": "lists-012-3038479"}, {"subject": "Re: Proxies and gethostbynam", "content": "David W. Morris:\n>\n>\n>On Mon, 11 May 1998, Henrik Frystyk Nielsen wrote:\n>\n>> I think there is one situation that 504 doesn't cover and that is if the\n>> proxy doesn't *want* to proxy a certain URI, maybe because it can't\n>\n>I continue to disagree ... there is a distinct difference from the \n>perspective of recovery whether the DNS consulted by proxy is unable\n>to resolve the host name or whether the host fails to respond. Hiding\n>all failures under a single code does nothing for usability or\n>perceived reliability.\n\nI agree with David that these two errors are different and should have\ndifferent codes.\n\nTo inject some current practice into this debate: I just tested two\ndifferent proxy servers and both return a 500 with an explanatory\nresponse body when the URL does not have a DNS entry.\n\nKoen.\n\n\n\n", "id": "lists-012-3047525"}, {"subject": "outstanding issues: what's happening", "content": "This mailing list has been pretty quiet.  I posed a bunch of questions\nover the past month or so, but I haven't seen resolutions.  I'll\nsummarize:\n\n1) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0031.html>\n\n(Digest)\na) No definition for non-terminal request-digest.\nb) In Authorization, the client can omit cnonce=.  If qop=auth-int\nand cnonce is omitted, should Authentication-Info in the server's\nresponse say 'cnonce=\"\"', or should cnonce be omitted there, too?\n\n2) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0035.html>\n\nRecommending that the (Digest) nonce include Etag seems like a bad\nidea -- it makes the nonce non-reusable for other entities.\n\n3) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0040.html>\n\n(Digest) In a response that sends multipart/byteranges, does the\ndigest-uri-value of A2 digest the MIME headers and separators?\n\n4) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0069.html>\n\nI think allowing TE: identity; q=0 is a bad idea.  I think \"identity\"\nshould always be allowed.\n\nDave Kristol\n\n\n\n", "id": "lists-012-3055720"}, {"subject": "5/13/98 http-authentication01.txt comment", "content": "3.2.2 The Authorization Request Header\n\ndigest-response  = 1#( username | realm | nonce | digest-uri |\n   response | [ algorithm ] | [cnonce] |\n   [opaque] | [server] | [message-qop] |\n   [ nonce-count ] )\n\nI don't think the non-terminal \"server\" is defined anywhere.\nWhat is it supposed to be?\n\nAlso, the typography for \"[ nonce-count ]\" is inconsistent with the\nother items (extra spaces).\n\nDave Kristol\n\n\n\n", "id": "lists-012-3064326"}, {"subject": "Re: outstanding issues: what's happening", "content": "On Wed, 13 May 1998, Dave Kristol wrote:\n\n> This mailing list has been pretty quiet.  I posed a bunch of questions\n> over the past month or so, but I haven't seen resolutions.  I'll\n> summarize:\n> \n> 1) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0031.html>\n> \n> (Digest)\n> a) No definition for non-terminal request-digest.\n\nShould be\n     request-digest    = <\"> *LHEX <\">\n\n> b) In Authorization, the client can omit cnonce=.  If qop=auth-int\n> and cnonce is omitted, should Authentication-Info in the server's\n> response say 'cnonce=\"\"', or should cnonce be omitted there, too?\n> \n\nIt should be illegal to omit cnonce with qop=auth-int\n\n> 2) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0035.html>\n> \n> Recommending that the (Digest) nonce include Etag seems like a bad\n> idea -- it makes the nonce non-reusable for other entities.\n> \n\nA note should say that ETag should only be used with one-time nonces.\n\n> 3) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0040.html>\n> \n> (Digest) In a response that sends multipart/byteranges, does the\n> digest-uri-value of A2 digest the MIME headers and separators?\n> \n\nI would think yes.\n\n\nPaul Leach is doing the editing of this.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-3071185"}, {"subject": "RE: outstanding issues: what's happening", "content": "Thanks, Dave. I was just telling Jim that we needed a revised Issues List,\nand you've gone and written one. The background focus has been on getting\ntested interoperable implementations, including implementations of Digest;\nwhen we have enough implementations, then some of the answers to your questions\nwill be clearer.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-3080565"}, {"subject": "Re: outstanding issues: what's happening", "content": "> 4) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0069.html>\n> \n> I think allowing TE: identity; q=0 is a bad idea.  I think \"identity\"\n> should always be allowed.\n\nI thought Jerrfey Mogul made a good point about wanting to forbid \nidentity when compression would be highly desireable (as in the \ncase where bandwidth is very expensive).  However, I agree with \nyou that since chunked encoding is _always_ acceptable it seems \nsomewhat silly to disallow identity in that case.  So...\n\nPerhaps in some cases the amount of data to send is so large that \nthe receiving computer won't have enough space allocated in its \ninternal representation of the content length to store the Content-\nLength header's value. ;-)  I guess also that if you're thinking of \nperformance, maybe the idea is that the hex representation of \nchunked transfer is more efficient to convert to binary.  And then \nthere's the case of an interrupted transfer.  It's probably easier for a \nclient to figure out which characters were received successfully in \norder to request the rest if those characters come in little chunks \nrather than one long stream...\n\nOK, these are a stretch.\n\nAdam \n\nAdam\n\n\n\n\nmailto:adam@cyber-guru.com\n\n\n\n", "id": "lists-012-3088426"}, {"subject": "RE: Etag in nonc", "content": "Dave Kristol wrote:\n>2) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0035.html>\n>\n>Recommending that the (Digest) nonce include Etag seems like a bad\n>idea -- it makes the nonce non-reusable for other entities.\n\nI think that allowing, but not recommending, the Etag in the nonce is\nthe best course.  There is some (slight?) security gain by doing so, but\nwith the major disadvantage of non-reusability.  (This should likely be\ndocumented...)\n==========================================================\nMark Leighton Fisher          Thomson Consumer Electronics\nfisherm@indy.tce.com          Indianapolis, IN\n\"Browser Torture Specialist, First Class\"\n\n\n\n", "id": "lists-012-3097218"}, {"subject": "RE: Etag in nonc", "content": "On Thu, 14 May 1998, Fisher Mark wrote:\n\n> I think that allowing, but not recommending, the Etag in the nonce is\n> the best course.\n\n  Anything at all is _allowed_ in the nonce except a double quote\ncharacter.\n\n\n\n", "id": "lists-012-3104958"}, {"subject": "Re: outstanding issues: what's happening", "content": "\"Adam M. Donahue\" <adam@cyber-guru.com> writes:\n\n    > 4) <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0069.html>\n    > \n    > I think allowing TE: identity; q=0 is a bad idea.  I think \"identity\"\n    > should always be allowed.\n    \n    I thought Jerrfey Mogul made a good point about wanting to forbid \n    identity when compression would be highly desireable (as in the \n    case where bandwidth is very expensive).  However, I agree with \n    you that since chunked encoding is _always_ acceptable it seems \n    somewhat silly to disallow identity in that case.  So...\n    \nI probably wasn't clear enough in that message.  For Accept-Encoding,\n\"identity; q=0\" is presumably justified by the point I made, but\nI finished that message by saying \"the rationale that applied to\nAccept-Encoding probably does not apply here.\"  I.e., while I'm\nnot sure I agree with Dave Kristol that \"TE: identity; q=0\" is a\n\"bad idea\", I didn't mean to imply that I thought it was necessarily\na good idea, either.\n\n-Jeff\n\nP.S.: I've never seen my name spelled like *that* before :-)\n\n\n\n", "id": "lists-012-3111787"}, {"subject": "Deployed use of HOST Header", "content": "Is anyone aware of any effort to derrive a list of client software\npackages which include the \"Host:\" header with requests? It would be\nreally helpful for my efforts to push server usage away from discrete\nIP addresses to be able to speak with some degree of authority.\n\nNice to know what percentages of requests to major net destinations like\nYahoo, Altavista, Infoseek, Excite, Netscape, IBM, Microsoft, etc. now\ninclude the HOST: header.\n\nDave Morris\n\n\n\n", "id": "lists-012-3119716"}, {"subject": "Implementation report: does anyone implement nonCRLF text charsets", "content": "Section 3.7.1 and appendix 19.4.2 allude to the fact that HTTP\nsupports the use of charset parameters with text types where the\ncharset does not use octets 13 and 10 for end-of-line, e.g.,\nUTF-16. However, are there any implementations that support this,\nand has the interoperability of, e.g., UTF-16 been tested?\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-3127181"}, {"subject": "Nonorder processing in persistent connection", "content": "Hi, Every body:\n \nHTTP/1.1 says when web server deals with the pipelined requests in persistent connections, it must return responses in same order that they recieved. I think that is an idea not good enough, at least in some special situations.\n \nI am working on a web server and write my master degree thesis about HTTP. My implementation supports both multi-thread and persistent connections. But with my test,  in some situation , ordered processing in pipeline requests costs long waiting time .\n \nFor example: when a request in the pipeline needs a long time processing (like retrieve data from a database) , so the later responses must wait even if they has been ready for being sent away.\n \nIn this situation , web server has to deal with the ordered messages at the cost of a long response time and more resource for holding the ready response in memory.\n \nIn fact, we can make some minor alteration to http server's performance. For example, we can add a header to the messages to indicate the request/response pair, something like 'Request-ID'. A client may create a unique digital ID for every request to the same server ,  then the server can return responses with this ID or without but use the same order. To name this mechanism , I call it as 'non-order processing'.\n \nAny question and comment about this 'non-order processing', and my web server or my thesis as well, please contact with me: zhoukang@cheerful.com  . I can share source code of my web server with others.\n \nZhou Kang\nComputer Department,\nSichuan University of P.R.China\n \n\n\n\n", "id": "lists-012-3134387"}, {"subject": "Re: Nonorder processing in persistent connection", "content": "Hi,\n\nZhou Kang wrote:\n> In fact, we can make some minor alteration to http server's\n> performance. For example, we can add a header to the messages to\n> indicate the request/response pair, something like 'Request-ID'. A\n> client may create a unique digital ID for every request to the same\n> server ,  then the server can return responses with this ID or without\n> but use the same order. To name this mechanism , I call it as\n> 'non-order processing'.\n\nI work on a project for anonymous WWW access. It will base on HTTP/1.1\nIf you send a *big* packet through a MIX-Net (like Mixmaster, can\nonly work with a constant block size) a Cache-Proxy must connect the\nsmaller\nparts. You can use the Content-Range Field but if some anonymous users\nuse\nthe same Range than they will get wasted data.\nSo we need also a Message-ID or 'Request-ID'.\nThis Message-ID should be defined for Request and Response and\n*randomly* choose by the client/server. If a server, (user?),\nproxy or MIX detect a collision\nthan a warning or error can be response to the client (may be\nserver?).   \n\nBye\nUwe Danz\n(my English is too bad: same mistakes corrected)\n\n\n\n", "id": "lists-012-3143595"}, {"subject": "RE: Nonorder processing in persistent connection", "content": "Thanks for your comments.   Out of order responses in HTTP is a topic\nwhich has come up more than a few times.  I beleive that most people\nagree that out of order responses or transacation identification would\nbe a good thing.  However, specifying exactly how this would behave\nisn't a simple task.  I think that because of this, the consensus is\nthat it is beyond the scope of the 1.1 version of HTTP.  \nThat doesn't mean the future versions of http wouldn't have this,\nbut http/1.1 is \"feature complete\" at this point.\n \n\n-----Original Message-----\nFrom: Zhou Kang [mailto:ZhouKang@cheerful.com]\nSent: Saturday, May 16, 1998 10:09 AM\nTo: HTTP-WG\nSubject: Non-order processing in persistent connections\n\n\nHi, Every body:\n \nHTTP/1.1 says when web server deals with the pipelined requests in\npersistent connections, it must return responses in same order that they\nrecieved. I think that is an idea not good enough, at least in some special\nsituations.\n \nI am working on a web server and write my master degree thesis about HTTP.\nMy implementation supports both multi-thread and persistent connections. But\nwith my test,  in some situation , ordered processing in pipeline requests\ncosts long waiting time .\n \nFor example: when a request in the pipeline needs a long time processing\n(like retrieve data from a database) , so the later responses must wait even\nif they has been ready for being sent away.\n \nIn this situation , web server has to deal with the ordered messages at the\ncost of a long response time and more resource for holding the ready\nresponse in memory.\n \nIn fact, we can make some minor alteration to http server's performance. For\nexample, we can add a header to the messages to indicate the\nrequest/response pair, something like 'Request-ID'. A client may create a\nunique digital ID for every request to the same server ,  then the server \n \nAny question and comment about this 'non-order processing', and my web\nserver or my thesis as well, please contact with me: zhoukang@cheerful.com\n<mailto:zhoukang@cheerful.com>   . I can share source code of my web server\nwith others.\n \nZhou Kang\nComputer Department,\nSichuan University of P.R.China\n \n\n\n\n", "id": "lists-012-3152151"}, {"subject": "Re: Nonorder processing in persistent connection", "content": "Josh Cohen:\n>\n>Thanks for your comments.   Out of order responses in HTTP is a topic\n>which has come up more than a few times.  I beleive that most people\n>agree that out of order responses or transacation identification would\n>be a good thing.  However, specifying exactly how this would behave\n>isn't a simple task.  I think that because of this, the consensus is\n>that it is beyond the scope of the 1.1 version of HTTP.  \n>That doesn't mean the future versions of http wouldn't have this,\n>but http/1.1 is \"feature complete\" at this point.\n\nYes, going beyond pipelining was generally considered too complex to\nput in http/1.1.  \n\nI'd like to add though that, as far as I know, most people do not\nconsider out-of-order responses to be the logical next step in\nimproving 1.1.  The preferred next step seems to be to add\nmultiplexing of several HTTP connections over a single TCP connection.\nMultiplexing has some additional advantages over out-of-order\nresponses.  See http://www.w3.org/Protocols/MUX/ for one effort in\nthis direction.\n\nKoen.\n\n\n\n", "id": "lists-012-3163162"}, {"subject": "RE: Nonorder processing in persistent connection", "content": "> -----Original Message-----\n> From: koen@win.tue.nl [mailto:koen@win.tue.nl]\n> Sent: Tuesday, May 19, 1998 12:50 AM\n> To: Josh Cohen\n> Cc: ZhouKang@cheerful.com; http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Non-order processing in persistent connections\n\n> I'd like to add though that, as far as I know, most people do not\n> consider out-of-order responses to be the logical next step in\n> improving 1.1.  The preferred next step seems to be to add\n> multiplexing of several HTTP connections over a single TCP connection.\n> Multiplexing has some additional advantages over out-of-order\n> responses.  See http://www.w3.org/Protocols/MUX/ for one effort in\n> this direction.\n> \nI totally agree with you in preferring mux over out-of-order responses..\nWhile out-of-order would be on the 'good' list of things, effort\nspent on MUX would be more worthwhile, IMHO.  A MUX allows much\nmore than out of order responses and the work on MUX can be leveraged\nfor other things besides just HTTP.\n\n> Koen.\n> \n\n\n\n", "id": "lists-012-3172047"}, {"subject": "Re: Nonorder processing in persistent connection", "content": "Josh Cohen wrote:\n\n> I totally agree with you in preferring mux over out-of-order responses..\n> While out-of-order would be on the 'good' list of things, effort\n> spent on MUX would be more worthwhile, IMHO.  A MUX allows much\n> more than out of order responses and the work on MUX can be leveraged\n> for other things besides just HTTP.\n\nSpeaking of multiplexing protocols, a group of us at UC Berkeley and IBM\nhave developed a TCP implementation that does many of the same good\nthings as MUX (multiplexing many logical connections over a single TCP\nconnection), but in a way that does not require applications to be\nmodified or relinked with a new socket library. We do this by\nintegrating congestion control and loss recovery of simultaneous TCP\nconnections with the same destination at the sender. This implementation\nis completely compatible with current TCP receivers and only requires\nmodifications to the sender-side networking stack.\n\nWe have a paper describing this implementation, which we call TCP-INT,\nat http://www.cs.berkeley.edu/~stemm/publications/infocom98.ps.gz.\n\nI'd appreciate your comments.\n\n--Mark\n\n\n\n", "id": "lists-012-3182601"}, {"subject": "Re: Deployed use of HOST Header", "content": "Dave Morris writes:\n    Is anyone aware of any effort to derrive a list of client software\n    packages which include the \"Host:\" header with requests? It would\n    be really helpful for my efforts to push server usage away from\n    discrete IP addresses to be able to speak with some degree of\n    authority.\n\n    Nice to know what percentages of requests to major net destinations\n    like Yahoo, Altavista, Infoseek, Excite, Netscape, IBM, Microsoft,\n    etc. now include the HOST: header.\n    \nI thought about asking the folks at AltaVista for this info, but then I\nrealized that it would probably require reprogramming their server, and\nthis is something they like to avoid.\n\nBut by a strange coincidence, this week one of the other researchers\nhere yesterday started logging all of the HTTP headers through our\nproxy here in Palo Alto.  And so I put together an AWK script to figure\nout which User-Agents send Host.\n\nBy the way, we're not going to release these logs, no matter how nicely\npeople ask ... so please don't ask.  And also don't ask me what\nfraction of requests were HTTP/1.1; the logs apparently don't include\nthat tidbit.\n\nOut of 1963154 requests logged yesterday (May 18), 1946195 (99.1%) had\nUser-Agent headers.  For these requests, I looked for a Host header\n(but without checking on its syntactic or semantic validity!), and then\nmade a list of User-Agent values associated with requests with or\nwithout Host.\n\nIn some cases, the same User-Agent was seen with and without the Host\nheader.  I'm not really keen on doing the analysis to figure out what\nreally happened in all cases, but I looked at one (more or less random)\nexample; it looks like when that particular browser is invoked by\nQuicken, it changes the request headers fairly significantly.  I\nsuspect that in other cases, some intervening proxy (we have several\nlayers of internal proxies within Digital) either added or removed Host\nheaders.\n\nIn any case, this means that these results should be taken with a large\ndose of skepticism, since it appears that one cannot simply assume that\nuse of a given User-Agent will always result in the delivery of a Host\nheader to the origin server.\n\nFor these lists, I used only the first \"word\" (whitespace-delimited\nstring of characters) in the User-Agent header.  I tried analyzing a\nsubset of the log using the entire User-Agent header; it doesn't seem\nto add much information, but it slows things down a lot.\n\nDisclaimer: nothing here is meant as a criticism of any User-Agent\nimplementation, especially since my analysis could be erroneous.\n\n-Jeff\n\nUser-agents that were never seen with a Host header:\n\n    0101500608win16001\n    0101600719win16001\n    0101600719win16014\n    0101600719win16042\n    0101600720win16001\n    0102001290win32001\n    AVSMCPX\n    Crescent\n    Enhanced_Mosaic\n    FFiNet32.DLL/3.1\n    Go-Ahead-Got-It/1.1\n    Lotus\n    Lynx/2.3\n    Lynx/2.3-FM\n    Lynx/2.3.6\n    Marimba\n    Mozilla/1.12I\n    Mozilla/1.2\n    Mozilla/1.22\n    Mozilla/3.0b9Gold\n    NCSA\n    NSPlayer/2.0\n    PCNviewer\n    Proxy\n    QFNApp/1.0\n    TSTPAV\n    Tuner/1.1.1\n    Tuner/2.0.2\n    Update\n    VXtreme,\n    Visto-Assistant/Commercial-Release-2.0\n    WebCopy/0.98b7\n    www.pl/961205.24\n\nUser-agents that were always seen with a Host header:\n\n    0102001735win32090\n    0102001737win32001\n    0102001737win32011\n    0102001737win32024\n    0102001790win32001\n    0102001790win32051\n    0102001790win32090\n    0102001792win32001\n    0102001792win32015\n    0102001792win32024\n    0102001792win32042\n    0102001792win32043\n    0102001792win32073\n    0102502226win32001\n    @%146%01L%146%01%7c%146%01%94%146%01\n    Alexa\n    Alexa/1.1.4.0%3bMicrosoft\n    AlphaCONNECT\n    BW-C-2.0\n    CSymWebPage\n    Caching-Manager/2.1\n    Conveyer\n    DMS-NetLink-GetLink\n    HotJava/1.1\n    InstallShield\n    Investor\n    Java1.0.2\n    Java1.1\n    Java1.1.4\n    LiveUpdate\n    Lotus-Notes/4.5\n    Lynx\n    Lynx/2.5FM\n    Lynx/2.6\n    Lynx/2.7.1\n    Lynx/2.7.2\n    Lynx/2.8rel.2\n    MFC_Tear_Sample\n    MSFrontPage/2.0\n    MSFrontPageWpp/3.0\n    MSInvestor\n    MSN\n    MSNBC-News-Alert/2.2\n    MSNBC-News-Browser-IE/2.1\n    MSWebPostPostInfoProcessor/1.5\n    Mozilla/1.1\n    Mozilla/2.01Gold\n    Mozilla/3.01-C-MACOS8\n    Mozilla/3.01C-DH397\n    Mozilla/3.01C-KIT\n    Mozilla/3.01C-WorldNet\n    Mozilla/3.03\n    Mozilla/3.04GoldC\n    Mozilla/3.0C-NC320\n    Mozilla/3.0C-WorldNet\n    Mozilla/4.04j2\n    NeoPlanet\n    NetAttache/2.5\n    Net_Vampire/2.4\n    OilChange\n    PhotoImpact\n    PrimeNet3Win32\n    RealPlayer\n    Registration\n    Scooter/1.1\n    ServerComm\n    SunLab's\n    Teleport\n    UPDATEIT\n    WebTrends/3.0\n    WebZIP/2.0\n    Wget/1.4.5\n    contype\n    d%f0D\n    libwww-perl/5.20\n    xmcd/v2.2PL1\n    \nUser-agents that were sometimes, but not always, seen with a Host header:\n\n    GetRight/3.1\n    Microsoft\n    Mozilla/2.0\n    Mozilla/2.01\n    Mozilla/2.02\n    Mozilla/2.02Gold\n    Mozilla/3.0\n    Mozilla/3.01\n    Mozilla/3.01C-POLNET\n    Mozilla/3.01C-SI304A01\n    Mozilla/3.01Gold\n    Mozilla/3.02\n    Mozilla/3.02Gold\n    Mozilla/3.03Gold\n    Mozilla/3.04\n    Mozilla/3.04Gold\n    Mozilla/3.0Gold\n    Mozilla/4.0\n    Mozilla/4.01\n    Mozilla/4.02\n    Mozilla/4.03\n    Mozilla/4.04\n    Mozilla/4.05\n    NSPlayer/3.0.0.2437\n    Tuner/2.1.2\n\n\n\n", "id": "lists-012-3191909"}, {"subject": "Re: Nonorder processing in persistent connection", "content": "Clearly a good thing...  Congestion and loss recovery should clearly\nbe done on a host pair basis, and that existing TCPs don't is clearly\nwrong...\n\nThere are still several other things I suspect that SMUX does that your \nmodified TCP does not (I couldn't access your web site this instant; got \na DNS lookup failure on the site for your paper): SMUX can pack more than \none fragment into a single packet.  As there are quite a few web objects \n(and HTTP protocol requests) that are quite small, this saves quite a \nfew packets.  It also serves as a record marking protocol, which can be \nuseful in many applications.\n\nAnd it can be deployed alot faster than getting everyone to agree on fixing\ntheir TCPs (which should be done in any case).\n\nSo I don't think this is one or the other; I'd certainly like to see\na better TCP (particularly one that didn't throw away data after a reset;\nthis causes us more than a little pain!).\n- Jim\n\n\n\n", "id": "lists-012-3203711"}, {"subject": "Re: Content-Encoding and ContentType in trailer", "content": "> From: \"Perry R. Ross\" <pross@platinum.com>\n> Resent-From: Andy Norman <ange@hplb.hpl.hp.com>\n> Date: Mon, 4 May 1998 19:47:21 +0100 (BST)\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Content-Encoding and Content-Type in trailer?\n> -----\n> I have searched the open issues and believe that this is a new issue.\n> \n> It seems that including Content-Encoding or Content-Type in the\n> trailer of a chunked transfer would be incorrect.  Allowing this would\n> require that the entire message body be buffered until the trailer is\n> received, so that the content codings may be removed, or the content\n> interpreted.  There doesn't seem to be any need for these headers to\n> appear in the trailer, since this information should be known before the\n> \n> message transfer begins.  As far as I can tell, libwww would not\n> correctly\n> process a message with Content-Encoding headers in the trailer.\n> \n> If I am incorrect in this, or if you are not soliciting public comments\n> at this time, please accept my apologies.\n\nI talked with Henrik about this...\n\nThis is not a correctness problem; it might be painful, but you have all \nthe information you need to deal with the message (chunked is self \ndelimiting).  What is more, the client would have to have asked for it \nusing TE, so it seems unlikely you would do this unless you had a good \nreason. \n- Jim\n\n\n\n", "id": "lists-012-3212966"}, {"subject": "New issues list published..", "content": "As always, it is:\nhttp://www.w3.org/Protocols/HTTP/Issues/\n\nNote that all issues from before last call have been moved to a separate\npage to reduce confusion.\n\nhttp://www.w3.org/Protocols/HTTP/Issues/BeforeLastCall.html\n\nPlease check to see if your favorite problem has been recorded, and let\nme know if I've missed anything.\n\nWe anticipate submitting one more draft of each document  based on these \nissues and as the interoperability testing completes, forwarding to the \nIESG for draft standard.\n\nAt the moment, it looks like Digest is the pacing item.\n\n- Jim Gettys\nDigital Equipment Corporation\nVisiting Scientist, W3C\nHTTP/1.1 editor\n\n\n\n", "id": "lists-012-3222092"}, {"subject": "redirection issue", "content": "Jim Gettys <jg@pa.dec.com> wrote regarding \"New issues list published\":\n>As always, it is:\n>http://www.w3.org/Protocols/HTTP/Issues/\n>\n>Note that all issues from before last call have been moved to a\nseparate\n>page to reduce confusion.\n>\n>http://www.w3.org/Protocols/HTTP/Issues/BeforeLastCall.html\n\n Note that Issue #he9 concerning 302 redirection in the new issues list\nis not really a new \"open\" issue, but simply an editorial matter of\nacting\nfully on the closed Issue #23 in what is now the BeforeLastCall.html\nlist.\nThe current draft has changed the status string to \"Found\" and appended\nthe\n\"Note:\" alerting readers that 302 has been changed from what was in RFCs\n1945 and 2068, but the actual text has not been changed to \"... [ text\nfor\n302, amended to allow UAs \"converting\" to GET ] ...\".\n\n\n I three times posted concern about 305 Use Proxy as it presently\nstands in the draft allowing a proxy to redirect a POST without explicit\nconfirmation from the user/UA which submitted the form, which is a\nserious security/privacy issue.  There still has been no discussion\nabout\nthis, nor is it recorded in the new issues list.  Two possibilities are\n(1) require that 305 for a POST be returned to the UA which submitted\nthe form and not be acted upon by any interposed proxies, or (2) drop\n305 together with 306 Set Proxy from the Draft Standard.  Note with\nrespect to the need for two independent implementations, that I dropped\nactive support for 305 in Lynx v2.7.2, so that the body of a 305 is\ndisplayed to the user, and this is still the behavior in Lynx v2.8.\nRestoring the behavior of acting on a 305 if the method is safe, and\nprompting the user about whether to act on it for a POST, would be\nsimple, but I'm not personally keen on retaining 305 until/unless the\nissue of what is appropriate behavior within proxy chains is resolved.\n\n\n 300 Multiple Choices in the draft does not say what should be\ndone if the method is not GET.  Since 300 also is the default for\nredirection statuses which a UA does not know, it would be a good\nidea to specify what should be done in the the case of POST.  When\nthis issue was raised before, Koen pointed out that all existing\nimplementations would convert the POST to a GET, i.e., that is\nuniversal current practice, so it shouldn't be a problem specifying\nthat explicitly in the Draft Standard.  In most cases, I suspect,\nthe UA will in fact end up showing the body of the unknown 3xx\nresponse, and for an actual 300 it would be expecting additional\ninformation associated with content negotiation, but whenever it\nacts on a 300 rather than showing the body, it would seem that a\nPOST should be converted to GET (without need for user confirmation\nit that conversion is done).\n\n     Fote\n--\nFoteos Macrides\n\n\n\n", "id": "lists-012-3228957"}, {"subject": "Re: Nonorder processing in persistent connection", "content": "Jim Gettys wrote:\n> \n> Clearly a good thing...  Congestion and loss recovery should clearly\n> be done on a host pair basis, and that existing TCPs don't is clearly\n> wrong...\n> \n> There are still several other things I suspect that SMUX does that your\n> modified TCP does not (I couldn't access your web site this instant; got\n> a DNS lookup failure on the site for your paper): SMUX can pack more than\n> one fragment into a single packet.  As there are quite a few web objects\n> (and HTTP protocol requests) that are quite small, this saves quite a\n> few packets.  It also serves as a record marking protocol, which can be\n> useful in many applications.\n> \n> And it can be deployed alot faster than getting everyone to agree on fixing\n> their TCPs (which should be done in any case).\n> \nYou couldn't grab the paper because power to the UCB campus was out for\nmost of yesterday...sigh. Anyway, it's back now.\n\nI agree that SMUX has advantages in terms of increasing the size of\npackets and decreasing the number of packets per transfer. It would be\ninteresting to quantify the effect that this has in reducing response\ntime or network congestion over our approach. However, I think I\ndisagree in terms of ease of deployment. It obviously takes more work to\nupgrade the networking stack of a single host than upgrade a web server\nor browser, but there are a lot less web servers than web clients in the\ninternet, and because our approach is limited to the stack at the sender\nand is compatible with existing receivers, the total work necessary to\nget our approach deployed is probably less. \n\nIf deployment pain P= N*p, where N=number of hosts modified and p= pain\nper host, TCP-INT has a larger p but much smaller N :).\n\n--Mark\n\n\n\n", "id": "lists-012-3238245"}, {"subject": "Re: Nonorder processing in persistent connection", "content": "> From http-wg-request@hplb.hpl.hp.com Tue May 19 12:20:30 1998\n> Date: Tue, 19 May 1998 12:11:41 -0700\n> From: jg@pa.dec.com (Jim Gettys)\n> To: Mark Stemm <stemm@CS.Berkeley.EDU>\n> Cc: Josh Cohen <joshco@microsoft.com>, koen@win.tue.nl, ZhouKang@cheerful.com,\n>         http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Non-order processing in persistent connections\n> \n> \n> Clearly a good thing...  Congestion and loss recovery should clearly\n> be done on a host pair basis, and that existing TCPs don't is clearly\n> wrong...\n\nThere is an in-depth discussion of this issue in RFC 2140, BTW. :-)\n\nThe methods in the Infocom paper are only one way to mux connections;\nthere are policy issues to be considered. There is also the opportunity to \nshare state among different hosts on the same LAN, described in the RFC.\n\n> There are still several other things I suspect that SMUX does that your \n> modified TCP does not (I couldn't access your web site this instant; got \n> a DNS lookup failure on the site for your paper): SMUX can pack more than \n> one fragment into a single packet.  As there are quite a few web objects \n> (and HTTP protocol requests) that are quite small, this saves quite a \n> few packets.  It also serves as a record marking protocol, which can be \n> useful in many applications.\n\nThis can be considered detrimental. Keeping the data separated allows emerging\nintegrated services mechanisms to apply. How would the OS schedule a packet\nthat is part video, part audio, and part HTML? \n\n> And it can be deployed alot faster than getting everyone to agree on fixing\n> their TCPs (which should be done in any case).\n\nAn HTTP solution requires everyone agreeing to fix their web browsers\nto be compatible; the state sharing on TCP works wherever deployed (it\nis backward compatible even when activated).\n\nAs a result, the TCP solution is easier to incrementally deploy and gain incremental\nbenefit.\n\nJoe\n\n\n\n", "id": "lists-012-3248093"}, {"subject": "Cache Control must &gt; MUS", "content": "I suppose this falls under issue \"h1\" (MUST-MAY-SHOULD):\n\nIn section 14.9, the first sentence after the first note says:\n\nCache directives must be passed through a proxy or gateway...\n\nI think the \"must\" ought to be \"MUST\".\n\nRichard L. Gray\nwill code for chocolate\n\n\nReferer had no value.  Add a value, or\nremove the header altogether, and their server worked okay.\n\nI was all set to go off in high dudgeon about how the specification\n*says* that headers that aren't understood should be ignored.  What it\nsays (7.1 Entity Header Fields) is that \"unrecognized header fields\nSHOULD be ignored....\"\n\nThe question I have is, what does \"unrecognized\" mean?  Does it just\nmean a header whose name is unfamiliar, or does it also mean a\nrecognized header for which the value is in some way invalid (such as\nmy example above)?  I realize that \"be liberal in what you accept\" is\non my side, here, but it's not clear that the *letter* of the\nspecification is also on my side.\n\nDave Kristol\n\n\n\n", "id": "lists-012-3259670"}, {"subject": "Re: what does &quot;unrecognized header fields&quot; mean", "content": "Dave Kristol <dmk@research.bell-labs.com> writes:\n\n>The question I have is, what does \"unrecognized\" mean?  Does it just\n>mean a header whose name is unfamiliar,\n\nThat's certainly the implication.  There are 12 uses of the term in the\n-03 level of HTTP 1.1, and they all focus on \"unrecognized\" as \"not\nknown to the implementation\" (as opposed to \"not properly constructed\").\n\n>                                        or does it also mean a\n>recognized header for which the value is in some way invalid (such as\n>my example above)?  I realize that \"be liberal in what you accept\" is\n>on my side, here, but it's not clear that the *letter* of the\n>specification is also on my side.\n\nThe letter appears to be against you in two ways.  In the case of\nhandling \"unrecognized\" headers, \"urecognized\" appears to mean \"not\nknown to the implementation\").  Section 3.7.2 uses the term to clearly\nmean \"a MIME type that this implementation does not know of\", as the\nrequirement is to treat unrecognized subtypes of MULTIPART as\nMULTIPART/MIXED.  Sections 4.5, 5.3, and 6.2 discuss general, request\nand, response header fields by name and require an implementation to\ntreat unrecognized headers as entity-headers.  Section 5.1.1 discusses\nhandling of unrecognized methods in a context that implies it to mean\nunknown.  Likewise section 6.1.1 when discussing status codes.  Section\n7.1, upon which your argument would hang, implcitly defines\n\"unrecognized\" in terms of the BNF for <extension-header>, when it says:\n\n   \"The extension-header mechanism allows additional entity-header\n   fields to be defined without changing the protocol, but these\n   fields cannot be assumed to be recognizable by the recipient.\n   Unrecognized header fields SHOULD be ignored by the recipient\n   and MUST be forwarded by transparent proxies.\"\n\nSection 14.9.6 makes much the same implication.\n\nThe second point to make is that an empty REFERER: header is illegal.\nThe BNF in section 14.36 reads:\n\n   \"Referer        = \"Referer\" \":\" ( absoluteURI | relativeURI )\"\n\nThere is a required value.  Surely the RFC can't make a normative\nstatement expecting a compliant implementation to ignore mal-formed\nheaders - that way would lie madness and miscommunication.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-3267059"}, {"subject": "Re: what does &quot;unrecognized header fields&quot; mean", "content": "Ross Patterson wrote:\n> [...]\n> The second point to make is that an empty REFERER: header is illegal.\n> The BNF in section 14.36 reads:\n> \n>    \"Referer        = \"Referer\" \":\" ( absoluteURI | relativeURI )\"\n> \n> There is a required value.  Surely the RFC can't make a normative\n> statement expecting a compliant implementation to ignore mal-formed\n> headers - that way would lie madness and miscommunication.\n\nI don't think it's so crazy for an implementation to treat a malformed\nheader as though it were simply missing.  If the header is required for\nproper interpretation of the request, then the request will fail.  If\nthe header is just optional fluff (User-Agent, Referer), then the\nactually or implied missing header can be ignored, no harm done.\n\nI agree that the empty Referer header violates the specified syntax. \nThe question is, what should the recipient do about it?  A server could\nparse all headers strictly according to the specified syntax and return\na 400 Bad Request response if it sees an error.  Or, it could adopt the\nmore common tolerant Internet behavior and ignore the malformed header.\n\nI seem to have encountered, for the first time of which I'm aware, an\nexample of the former, and it surprised me.\n\nDave Kristol\n\n\n\n", "id": "lists-012-3277336"}, {"subject": "response to protocol violations (was: what does &quot;unrecognized header fields&quot; mean?", "content": "I agree that in general, implementing \"common tolerant Internet behavior\" is\na good thing. However, there are times that for simplicity, security, or\ncorrectness reasons, certain protocol violations must cause error responses\nfrom servers. One that comes to mind is when CTL chars are included in the\nmessage header without first being encoded or inside of a quote string. I've\nseen applications (MapQuest might still do this) that attempt to use control\ncharacters in the URL. For security reasons, our proxy refuses to retrieve\nrequests that violate the protocol in that fashion.\n\n-Rob Polansky\n\n> -----Original Message-----\n> From: dmk@research.bell-labs.com [mailto:dmk@research.bell-labs.com]On\n> Behalf Of Dave Kristol\n> Sent: Tuesday, May 26, 1998 2:41 PM\n> To: Ross Patterson\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: what does \"unrecognized header fields\" mean?\n>\n[...snip...]\n>\n> I don't think it's so crazy for an implementation to treat a malformed\n> header as though it were simply missing.  If the header is required for\n> proper interpretation of the request, then the request will fail.  If\n> the header is just optional fluff (User-Agent, Referer), then the\n> actually or implied missing header can be ignored, no harm done.\n>\n> I agree that the empty Referer header violates the specified syntax.\n> The question is, what should the recipient do about it?  A server could\n> parse all headers strictly according to the specified syntax and return\n> a 400 Bad Request response if it sees an error.  Or, it could adopt the\n> more common tolerant Internet behavior and ignore the malformed header.\n>\n> I seem to have encountered, for the first time of which I'm aware, an\n> example of the former, and it surprised me.\n>\n> Dave Kristol\n>\n>\n\n\n\n", "id": "lists-012-3286583"}, {"subject": "Comments on section 9.8, TRAC", "content": "Two of my students offer the following comments on TRACE:\n\nFrom: Louis Discepola <disc7701@sparky.cs.nyu.edu>\nThe protocol states that \"a TRACE request MUST NOT include \nan entity\".  For someone writing a server program, this \nrequirement implies that the server program must check that \nthis condition is upheld.  But after reading a line of data \nfrom a client, there is no way for the server program to \ndistinguish whether it received the first line of an entity \nbody or the fist line of a badly formatted request line.  It \ncannot therefore reply with an accurate response code.  I \nsuggest that the HTTP/1.1 specification clearly force the client \nside to enforce this requirement.\n \nFrom: Catalin Floristean <floriste@slinky.cs.nyu.edu>\nThe specification states that if the request is \"successful\" \nit should return a 200 message containing the whole \noriginal request as message-body but does not define \nwhat successful means.  Understandably, it should be checked \nfor correct syntax and semantics, but how far should the \nsemantic check go?  Specifically, in the case where the \nRequest-URI is the URL of a resource and not \"*\", should \nthe origin server also make sure that the resource exists \nand is readable?  Since the intention is just to provide a \nloop-back trace method (otherwise a HEAD request would do), \nprobably a syntax check and a basic semantic check (the \nprotocol version, presence of the Host header containing \nthe correct host/port in case of HTTP/1.1) should suffice \nand this fact should be clearly stated. \nAlso, it would help if it were stated clearly that the \nrequest headers apply to the trace response (e.g. a TRACE \nrequest with a \"TE: chunked\" will have it response's \nmessage body -- the trace -- chunked) and that a \"*\" URI is \nallowed here (although this could be implied from other \nsections).\n\n-- \nArthur P. Goldberg\nClinical Associate Professor of Computer Science\nartg@cs.nyu.edu       http://www.cs.nyu.edu/cs/faculty/artg\n715 Broadway, Room 711, Computer Science Department\nCourant Institute of Mathematical Science\nNew York University\nNew York, NY 10003-6806\n212 998-3014   fax 995-4123\n\n\n\n", "id": "lists-012-3297057"}, {"subject": "Re: Comments on section 9.8, TRAC", "content": "Arthur P. Goldberg wrote:\n> \n> Two of my students offer the following comments on TRACE:\n> \n> From: Louis Discepola <disc7701@sparky.cs.nyu.edu>\n> The protocol states that \"a TRACE request MUST NOT include\n> an entity\".  For someone writing a server program, this\n> requirement implies that the server program must check that\n> this condition is upheld.  But after reading a line of data\n> from a client, there is no way for the server program to\n> distinguish whether it received the first line of an entity\n> body or the fist line of a badly formatted request line.  It\n> cannot therefore reply with an accurate response code.  I\n> suggest that the HTTP/1.1 specification clearly force the client\n> side to enforce this requirement.\n\n1) The presence of an entity is signaled by the presence of an entity\nheader, such as Content-Length or Content-Type.  Without some indication\nthat there's an entity there, the server must assume there is none.\n\n2) I don't necessarily agree with the assertion that the \"server program\nmust check\".  The statement that \"a TRACE request MUST NOT include an\nentity\" is more a requirement on the client.  It equally well means that\nif a client does send an entity, then there's no guarantee about how the\nserver might/must respond.\n\nIn particular, consider three scenarios:\na) If the server is written with the assumption that there's never an\nentity, then, on a persistent connection, it would assume that the next\nline that follows the end of the previous request would be a new request\nline, not part of an entity.\nb) OTOH, for that same example, if there happen to be entity headers in\nthe TRACE request (but there's no entity) and the server is written to\nread an entity, even though one is not defined by the specification,\nthen the second request would be interpreted as an entity, not as a\nrequest.\nc) The server in the second example could instead return a 400 Bad\nRequest because there's an entity implied by the headers, but TRACE does\nnot permit one.\n\nThe specification is quite imprecise about what to do in many error\ncases.  It is mostly concerned with what's supposed to happen if you do\nthings the right way.\n\n3) The specification can't *force* the client (or server) to do\nanything.  The \"MUST NOT\" you cite is about as good as you can do.  How\ncan you make the client \"enforce this requirement\"?  Either it does the\nright thing or it doesn't.\n\n> \n> From: Catalin Floristean <floriste@slinky.cs.nyu.edu>\n> The specification states that if the request is \"successful\"\n> it should return a 200 message containing the whole\n> original request as message-body but does not define\n> what successful means.  Understandably, it should be checked\n> for correct syntax and semantics, but how far should the\n> semantic check go?  Specifically, in the case where the\n> Request-URI is the URL of a resource and not \"*\", should\n> the origin server also make sure that the resource exists\n> and is readable?  Since the intention is just to provide a\n> loop-back trace method (otherwise a HEAD request would do),\n> probably a syntax check and a basic semantic check (the\n> protocol version, presence of the Host header containing\n> the correct host/port in case of HTTP/1.1) should suffice\n> and this fact should be clearly stated.\n\n1) I think the wording \"If successful, the response...\" is poor\nEnglish.  (It isn't the response that's successful!)  Perhaps it should\nsay \"If the request is valid, the response....\"  What constitutes a\nvalid request is sprinkled through the specification.\n\n2) I think the student is trying to read too much into the\nspecification.  The description of the response says only that the\nrequest message should be returned as an entity.  It says nothing about\nchecking the Request-URI or the resource so-identified  The topic of\nwhat to do when something is unstated often comes up in discussions here\n(and I often raise them :-).  If the specification doesn't say to do\nsomething anywhere, then don't do it.  In this case, a server might or\nmight not check the syntax of the Request-URI, but it need not check the\nresource so-identified.\n\n> Also, it would help if it were stated clearly that the\n> request headers apply to the trace response (e.g. a TRACE\n> request with a \"TE: chunked\" will have it response's\n> message body -- the trace -- chunked) and that a \"*\" URI is\n> allowed here (although this could be implied from other\n> sections).\n\nI think we've been here before.  \"TE: chunked\" doesn't say the server\n*must* chunk the response.  It says the client would accept \"chunked\". \nBut for an HTTP/1.1 request, that fact is implied and, therefore, the\nheader is redundant.\n\nDave Kristol\n\n\n\n", "id": "lists-012-3307400"}, {"subject": "response to protocol violations (was: what does &quot;unrecognized header fields&quot; mean?", "content": "We ignore empty header fields (including Referer).\nA _really_ anal reading of the BNF for URIs allows \"\".\nLynx (at least some versions used to) for example send empty Referer\nheaders, which was the problem that bit us when we were too rigid in\nour error checking.\n\nI would agree that unescaped CTLs is bad enough to reject.\n\n** Reply to note from \"Rob Polansky\" <polansky@raptor.com> Tue, 26 May 1998 15:27:03 -0400\n>   \n> I agree that in general, implementing \"common tolerant Internet behavior\" is\n> a good thing. However, there are times that for simplicity, security, or\n> correctness reasons, certain protocol violations must cause error responses\n> from servers. One that comes to mind is when CTL chars are included in the\n> message header without first being encoded or inside of a quote string. I've\n> seen applications (MapQuest might still do this) that attempt to use control\n> characters in the URL. For security reasons, our proxy refuses to retrieve\n> requests that violate the protocol in that fashion.\n>   \n> -Rob Polansky\n>   \n> > -----Original Message-----\n> > From: dmk@research.bell-labs.com [mailto:dmk@research.bell-labs.com]On\n> > Behalf Of Dave Kristol\n> > Sent: Tuesday, May 26, 1998 2:41 PM\n> > To: Ross Patterson\n> > Cc: http-wg@cuckoo.hpl.hp.com\n> > Subject: Re: what does \"unrecognized header fields\" mean?\n> >\n> [...snip...]\n> >\n> > I don't think it's so crazy for an implementation to treat a malformed\n> > header as though it were simply missing.  If the header is required for\n> > proper interpretation of the request, then the request will fail.  If\n> > the header is just optional fluff (User-Agent, Referer), then the\n> > actually or implied missing header can be ignored, no harm done.\n> >\n> > I agree that the empty Referer header violates the specified syntax.\n> > The question is, what should the recipient do about it?  A server could\n> > parse all headers strictly according to the specified syntax and return\n> > a 400 Bad Request response if it sees an error.  Or, it could adopt the\n> > more common tolerant Internet behavior and ignore the malformed header.\n> >\n> > I seem to have encountered, for the first time of which I'm aware, an\n> > example of the former, and it surprised me.\n> >\n> > Dave Kristol\n> >\n> >\n>   \n> \n \n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-3320408"}, {"subject": "Re: Comments on section 9.8, TRAC", "content": "> From: Louis Discepola <disc7701@sparky.cs.nyu.edu>\n> The protocol states that \"a TRACE request MUST NOT include \n> an entity\".  For someone writing a server program, this \n> requirement implies that the server program must check that \n> this condition is upheld.  But after reading a line of data \n> from a client, there is no way for the server program to \n> distinguish whether it received the first line of an entity \n> body or the fist line of a badly formatted request line.  It \n> cannot therefore reply with an accurate response code.  I \n> suggest that the HTTP/1.1 specification clearly force the client \n> side to enforce this requirement.\n\nIt can distinguish this with a CRLF pair.  Because a TRACE request \ncannot contain an entity as per the specification, then anything after \nthe CRLF pair (not including additional CRLFs) would have to be \nconsidered the beginning of a new request.  However, another \nquestion to ask would be how a server should handle a TRACE \nrequest when a Content-Length header, for example, is included.  \nShould the server simply read in the entity in any event even though \nit's forbidden by the spec (I'd think so, in keeping with being \nlenient), and then just not return it with the response?  Or should \nthe server respond with the entire trace?  Or should the trace, in \nthis case, be answered with a 400 status code?\n\n> From: Catalin Floristean <floriste@slinky.cs.nyu.edu>\n> Also, it would help if it were stated clearly that the \n> request headers apply to the trace response (e.g. a TRACE \n> request with a \"TE: chunked\" will have it response's \n> message body -- the trace -- chunked) and that a \"*\" URI is \n> allowed here (although this could be implied from other \n> sections).\n\nI think we all missed the point of TE.  \"TE: chunked\" is never \nrequired since chunked encoding is *always* acceptable, correct, \nworking group?\n\nAdam\n\n\n\nmailto:adam@cyber-guru.com\n\n\n\n", "id": "lists-012-3331919"}, {"subject": "Re: Comments on section 9.8, TRAC", "content": "Catalin Floristean wrote:\n> \n> I would just make a general comment here: it is true that in most\n> cases it is easy to infer\n> common sense answers to these questions, e.g. it is common sense not\n> to check the resource\n> identified by the Request-URI of a TRACE request. The point is though,\n> for some reason I still believe\n> that a good, solid specification should rely less on common sense\n> answers and more on clearly stated\n> definitions, descriptions, terms etc. (even if this implies a\n> reasonable amount of redundancy). I believe\n> this leads to a rapid and reliable implementation (and that's how you\n> get a big, ugly, hard-to-read yet\n> complete ANSI standerd :).\n\nIf you're referring to the ANSI C standard, I had a small hand in that.\n:-)\n\nThe danger with adding redundancy, as I found out from the ANSI C\neffort, is that it's very easy to introduce inconsistencies.  If you\nsay, \"Do a, b, and c\" in one place and \"Do a and b\" in another, you\ninvite incompatibilities.  The safest way to write a standard that's\nconsistent is to say each thing only once, if possible.\n\nAs with the ANSI C specification, you cannot understand the HTTP\nspecification by looking at just one part.  Unfortunately you have to\nunderstand all or most of it as a unit.\n\nDave Kristol\n\n\n\n", "id": "lists-012-3342029"}, {"subject": "[Fwd: I-D ACTION:draft-kristol-http-proxy-state00.txt,.ps", "content": "Much later than I promised, I've submitted the first Internet Draft for\nproxy cookies.\n\nDiscussion should be conducted on the http-state mailing list.  See\n<http://www.bell-labs.com/mailing-lists/http-state/> for information\n(and archives).\n\nI've also started a web page about proxy cookies:\n<http://portal.research.bell-labs.com/~dmk/pcookies/>.\n\nDave Kristol\n\n\nattached mail follows:\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\n\n\nTitle: HTTP Proxy State Management Mechanism\nAuthor(s): D. Kristol\nFilename: draft-kristol-http-proxy-state-00.txt,.ps\nPages: 8\nDate: 27-May-98\n\nThis document specifies a way to create a stateful session, using HTTP\nrequests and responses, between an HTTP proxy server and its client.  It\ndescribes two new headers, Pcookie and Set-Pcookie, which carry state\ninformation between participating HTTP proxy servers and their clients.\n\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-kristol-http-proxy-state-00.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-kristol-http-proxy-state-00.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-kristol-http-proxy-state-00.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-3350970"}, {"subject": "416 implementation incompatibility V", "content": "While working with our implementations of HTTP/1.1, we've come across\na compatibility problem with the 416 range unsatisfiable response.\n\nBrowsers implemented based on 2068 prior to the 416 behavior's introduction\ndon't understand the 416 response.  The previous behavior was that an \nunsatisfiable range request was ignored and a 200 response followed.\n\nOur problem is that IE4, which supports range requests, would, in certain\ncases, depend on this behavior.  In tests with 416 server implementations,\nthe browser receives this unexpected response which it cant handle\ngracefully. (just receive the whole file again, and go on with life as\nusual instead of interrupting the operation and the user with an error).\n\nThe crux is that we end up in a situation with a cached document\nwhich is marked partial because, event though we've actually read \nas much as the content length indicates, we didn't actually \"read to EOF\".\nOn the next retrieve the browser attempts to fetch the rest of the document\nwith a range request which begins with the byte after the last byte\nin the file.  In older implementations we just get the whole file again,\nbut in newer server implementations we get a 416, which is an error.\n\nI'll avoid going into the exact details of how we get into this\nsituation and just say that at the time we implemented it,\nthis area of the spec wasn't completely clear and our behavior\nis probably a bug in our implementation.\n\nBeing that IE4 has long since been shipped, we are unsure how to\nproceed.  Our discussions have revisited the reasoning for why\nan unsatisfiable range request should get an error while an\ninvalid request (syntax) would be ignored.\n\nIt seems to me that we have introduced a feature in http/1.1\nwhich is incompatible with the initial spec for http/1.1.  In general\nthis is something we should avoid and makes me wonder if we should\nstrike the 416 from http/1.1.\n\nTaking that one step further, I'm inclined to wonder if the right\nthing may be to take 416, mandatory and options, drop them in \na spec with the version number bumped.  This would let new\nextensions or features get standardized in a separate fashion\nvia the extension framework.\n\nyour thoughts?\n\n\n\n", "id": "lists-012-3360466"}, {"subject": "RE: Comments on section 9.8, TRAC", "content": "I would just make a general comment here: it is true that in most cases\nit is easy to infer\ncommon sense answers to these questions, e.g. it is common sense not to\ncheck the resource\nidentified by the Request-URI of a TRACE request. The point is though,\nfor some reason I still believe \nthat a good, solid specification should rely less on common sense\nanswers and more on clearly stated\ndefinitions, descriptions, terms etc. (even if this implies a reasonable\namount of redundancy). I believe\nthis leads to a rapid and reliable implementation (and that's how you\nget a big, ugly, hard-to-read yet \ncomplete ANSI standerd :).\n\n--Catalin\n\n\n> -----Original Message-----\n> From:Dave Kristol [SMTP:dmk@bell-labs.com]\n> Sent:Wednesday, May 27, 1998 12:15 PM\n> To:artg@cs.nyu.edu\n> Cc:http-wg@cuckoo.hpl.hp.com; Louis Discepola; Catalin Floristean\n> Subject:Re: Comments on section 9.8, TRACE\n> \n> ...\n>  \n> 2) I think the student is trying to read too much into the\n> specification.  The description of the response says only that the\n> request message should be returned as an entity.  It says nothing\n> about\n> checking the Request-URI or the resource so-identified  The topic of\n> what to do when something is unstated often comes up in discussions\n> here\n> (and I often raise them :-).  If the specification doesn't say to do\n> something anywhere, then don't do it.  In this case, a server might or\n> might not check the syntax of the Request-URI, but it need not check\n> the\n> resource so-identified.\n> \n> ...\n\n\n\n", "id": "lists-012-3369564"}, {"subject": "Implications of introducing new scheme and port for existing HTTP server", "content": "Hi,\n\nAs most of you know already, the Internet Printing Protocol (IPP) WG has\nsuggested using HTTP as \"transport\", with the payload in the form of a MIME\nobject passed with the POST method.\n\nAs part of the onging IESG review process, the Application Area Director\nKeith Moore has suggested to distinguish IPP traffic from \"normal\" HTTP\ntraffic by: \n\n1) the introduction of a new scheme called \"ipp\"\n2) the introduction a new default port number for IPP servers.\n\nBefore the IPP WG responds to those suggestions, the IPP WG would like to\nget some advice from the HTTP WG on the implications of such a change.\nIn particular, we want some feedback on how easy or difficult it would be\nto configure existing web servers to accomodate the suggested changes.\n\nPlease note that many printer vendors are not in the business of developing\nweb servers or HTTP servers and are dependent on getting those compoments\nfrom other vendors.\n\nPlease respond back to the IPP DL at:\n\nipp@pwg.org\n\nThanks,\n\nCarl-Uno Manros\nChair of the IETF IPP WG\n\n\n\n", "id": "lists-012-3380333"}, {"subject": "RE: IPP&gt; Re: Implications of introducing new scheme and port for  existing HTTP server", "content": "I think its fine to have a new default dest port \nassociated with IPP, but a new URL scheme seems like more\ntrouble than may be apparent.\n\nFor one, even though IPP is a different service than HTTP,\nan IPP client *is* speaking HTTP, IMHO.  HTTP is used as\na layer underneath IPP.  So, I think the URL scheme\nshould continue to be http://..\n\nUsing a new URL scheme will certainly break compatibility\nwith existing proxies.  Proxy server's encountering a new\nscheme will fail unless they are modified to understand it.\n\nAs I've stated before, I think the best way to differentiate\nthe service and remain compatible with existing proxy servers\nis to use a new method on the request line.\n\n\n> -----Original Message-----\n> From: hardie@thornhill.arc.nasa.gov\n> [mailto:hardie@thornhill.arc.nasa.gov]\n> Sent: Monday, June 01, 1998 10:31 AM\n> To: Carl-Uno Manros; http-wg@hplb.hpl.hp.com\n> Cc: ipp@pwg.org\n> Subject: IPP> Re: Implications of introducing new scheme and port for\n> existing HTTP servers\n> \n> \n> Carl-Uno,\n> By \"scheme\" in the text below, do you mean a\n> new HTTP method, parallel to GET and POST, or something\n> else?\n> regards,\n> Ted Hardie\n> NASA NIC\n> \n> > 1) the introduction of a new scheme called \"ipp\"\n> > 2) the introduction a new default port number for IPP servers.\n> >\n> > Before the IPP WG responds to those suggestions, the IPP WG \n> would like to\n> > get some advice from the HTTP WG on the implications of \n> such a change.\n> > In particular, we want some feedback on how easy or \n> difficult it would be\n> > to configure existing web servers to accomodate the \n> suggested changes.\n> \n\n\n\n", "id": "lists-012-3389645"}, {"subject": "RE: IPP&gt; Re: Implications of introducing new scheme and port for  existing HTTP server", "content": "If this gets down to a point where we HAVE to modify our specification,\nthen I agree with Josh, it would be much better to differentiate based\non HTTP method than on URL scheme, (IMHO). (But I think its ok as it\nstands now)\n\nRandy\n\n\n-----Original Message-----\nFrom:Josh Cohen [SMTP:joshco@MICROSOFT.com]\nSent:Monday, June 01, 1998 10:54 AM\nTo:'http-wg@cuckoo.hpl.hp.com'\nSubject:RE: IPP> Re: Implications of introducing new\nscheme and port for  existing HTTP servers\n\nI think its fine to have a new default dest port \nassociated with IPP, but a new URL scheme seems like more\ntrouble than may be apparent.\n\nFor one, even though IPP is a different service than HTTP,\nan IPP client *is* speaking HTTP, IMHO.  HTTP is used as\na layer underneath IPP.  So, I think the URL scheme\nshould continue to be http://..\n\nUsing a new URL scheme will certainly break compatibility\nwith existing proxies.  Proxy server's encountering a new\nscheme will fail unless they are modified to understand it.\n\nAs I've stated before, I think the best way to differentiate\nthe service and remain compatible with existing proxy servers\nis to use a new method on the request line.\n\n\n> -----Original Message-----\n> From: hardie@thornhill.arc.nasa.gov\n> [mailto:hardie@thornhill.arc.nasa.gov]\n> Sent: Monday, June 01, 1998 10:31 AM\n> To: Carl-Uno Manros; http-wg@hplb.hpl.hp.com\n> Cc: ipp@pwg.org\n> Subject: IPP> Re: Implications of introducing new scheme and\nport for\n> existing HTTP servers\n> \n> \n> Carl-Uno,\n> By \"scheme\" in the text below, do you mean a\n> new HTTP method, parallel to GET and POST, or something\n> else?\n> regards,\n> Ted Hardie\n> NASA NIC\n> \n> > 1) the introduction of a new scheme called \"ipp\"\n> > 2) the introduction a new default port number for IPP\nservers.\n> >\n> > Before the IPP WG responds to those suggestions, the IPP WG \n> would like to\n> > get some advice from the HTTP WG on the implications of \n> such a change.\n> > In particular, we want some feedback on how easy or \n> difficult it would be\n> > to configure existing web servers to accomodate the \n> suggested changes.\n> \n\n\n\n", "id": "lists-012-3401279"}, {"subject": "RE: IPP&gt; Implications of introducing new scheme and port ", "content": "To restate, I disagree.\nStandardizing a default destination port for IPP doesnt\naffect routing through proxy servers.\nThe proxy listening port doesnt need to change.\nJust like when doing 'normal' http, the proxy listening\nport has nothing to do with the destination port in the URL.\n\nIf you have a URL:\nhttp://mywebserver.domain.com:8000/index.html\nor\nhttp://myprinter:5150/queue1/\n\nand a proxy setting of \nhttp://myproxy:8080/\n\nthe proxy's listening port typically is 8080 or 80,\nbut does not matter.  The client http library knows\nvia some config setting which port and host to connect\nto when speaking the 'http proxy mechanism'.\n\nThe only restraint is if your proxy is configured to\nrestrict URLS to only port 80.\neg proxied URLs of the type:\nhttp://server/index.html \nor\nhttp://server:80/index.html\n\nwhich due to the popularity of servers running\non ports other than 80 (perhaps for a user non-root server\non a unix machine), is difficult to maintain.\n\n> -----Original Message-----\n> From: PETER_E_MELLQUIST@hp-roseville-om3.om.hp.com\n> [mailto:PETER_E_MELLQUIST@hp-roseville-om3.om.hp.com]\n> Sent: Monday, June 01, 1998 11:54 AM\n> To: manros@cp10.es.xerox.com\n> Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org\n> Subject: Re: IPP> Implications of introducing new scheme and port f\n> \n> \n>      Regarding item #2,\n>      \n>      Use of alternative HTTP ports, other than port 80, \n> effects the ability \n>      to move through proxies and firewalls. Using alternative \n> port #'s will \n>      require reconfiguration of security infrastructure in \n> order to allow \n>      for HTTP connections. \n>      \n>      HP has gone through similar work in the definition and \n> standardization \n>      of HTTP port 280 for Web Based Management Purposes ( see \n> IANA port \n>      list ). Currently port 280 is IANA approved for usage of \n> HTTP for \n>      network management. This works fine for Intranet usage, \n> but issues as \n>      described above result when operating in a secure environments.\n>      \n>      The other issue is that of configuring HTTP servers and \n> proxies to \n>      listen on alternative port #s. While easy to do \n> programatically, not \n>      all commercial HTTP servers allow listening on multiple ports \n>      concurrently.\n>      \n>      Considering these two issues, partitioning of the URI \n> space for IPP on \n>      HTTP port 80 or HTTP-S (HTTP/(SSL |TLS)) on port 443 \n> makes better \n>      sense.\n>      \n>      Peter\n> \n> \n> ______________________________ Reply Separator \n> _________________________________\n> Subject: IPP> Implications of introducing new scheme and port for e\n> Author:  Non-HP-manros (manros@cp10.es.xerox.com) at \n> HP-Roseville,mimegw4\n> Date:    6/1/98 10:20 AM\n> \n> \n> Hi,\n>      \n> As most of you know already, the Internet Printing Protocol \n> (IPP) WG has \n> suggested using HTTP as \"transport\", with the payload in the \n> form of a MIME \n> object passed with the POST method.\n>      \n> As part of the onging IESG review process, the Application \n> Area Director \n> Keith Moore has suggested to distinguish IPP traffic from \n> \"normal\" HTTP \n> traffic by: \n>      \n> 1) the introduction of a new scheme called \"ipp\"\n> 2) the introduction a new default port number for IPP servers.\n>      \n> Before the IPP WG responds to those suggestions, the IPP WG \n> would like to \n> get some advice from the HTTP WG on the implications of such a change.\n> In particular, we want some feedback on how easy or difficult \n> it would be \n> to configure existing web servers to accomodate the suggested changes.\n>      \n> Please note that many printer vendors are not in the business \n> of developing \n> web servers or HTTP servers and are dependent on getting \n> those compoments \n> from other vendors.\n>      \n> Please respond back to the IPP DL at:\n>      \n>         ipp@pwg.org\n>      \n> Thanks,\n>      \n> Carl-Uno Manros\n> Chair of the IETF IPP WG\n> \n\n\n\n", "id": "lists-012-3414268"}, {"subject": "Byte range", "content": "I recently received the message below.  Looking at\ndraft-ietf-http-v11-spec-rev-03 it seems the description\nof multipart/byteranges leaves a lot to be desired on the\nsubject of where <CRLF> should occur.  Is there a definitive\nanswer to this?\n\nJohn Franks\njohn@math.nwu.edu\n\n---------- Forwarded message ----------\nDate: Mon, 1 Jun 1998 16:04:33 -0600 (MDT)\nFrom: Alex Rousskov <rousskov@nlanr.net>\nTo: John Franks <john@dehn.math.nwu.edu>\nSubject: Re: Server: WN/2.0.0pre\n\nHi John,\n\nIn implementing multipart range responses you probably followed RFC\n2046 (prepending boundaries with <CRLF>). However, HTTP RFC (2068) gives an\nexample where boundaries are _not_ prepended with <CRLF> (note that there is\nonly one empty line after headers):\n\n   HTTP/1.1 206 Partial content\n   Date: Wed, 15 Nov 1995 06:25:24 GMT\n   Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n   Content-type: multipart/byteranges; boundary=THIS_STRING_SEPARATES\n\n   --THIS_STRING_SEPARATES\n   Content-type: application/pdf\n   Content-range: bytes 500-999/8000\n\n   ...the first range...\n   --THIS_STRING_SEPARATES\n   Content-type: application/pdf\n   Content-range: bytes 7000-7999/8000\n\n   ...the second range\n   --THIS_STRING_SEPARATES--\n\n\nHow did you decide which RFC to follow? Any \"well-known\" clarifications that\nI am not aware about? \n\nThanks!\n\nAlex.\n\n\n\n", "id": "lists-012-3429501"}, {"subject": "Re: Implications of introducing new scheme and port for existing  HTTP server", "content": "I would strongly urge you to NOT introduce a new url scheme. I would\nassert that any existing proxy which doesn't reject an unknown scheme\nbadly broken. At the very minimum, proxies would have to be configured\nto accept this new scheme and interpret it as HTTP. A requirement which\nwill be fraught with failure in the implementation. I'm not aware of any\nproxies which would be this simple to adapt so this is speculation of the\nlower bound of difficulty generated by using a new scheme.\n\n(I'm also not wild about new HTTP methods as I know of existing proxies\nwhich will reject unknown methods. Don't know of any which will accept\nunknown methods. I'm also unaware of any firewall software which examines\nthe HTTP request method as part of its algorithm but then I'm not a \nfirewall expert.)\n\nA new default port may be OK but I believe unnecessary.  In any case, an\ninstallation should be allowed to overload port 80 etc. if desired by\nover-riding the default if it isn't 80.\n\nBut then I don't buy the basic premis that its necessary to distinguish \nbetween IPP (or any other HTTP-based content protocol) and HTTP.\n\nDave Morris\n\nOn Mon, 1 Jun 1998, Carl-Uno Manros wrote:\n\n> Hi,\n> \n> As most of you know already, the Internet Printing Protocol (IPP) WG has\n> suggested using HTTP as \"transport\", with the payload in the form of a MIME\n> object passed with the POST method.\n> \n> As part of the onging IESG review process, the Application Area Director\n> Keith Moore has suggested to distinguish IPP traffic from \"normal\" HTTP\n> traffic by: \n> \n> 1) the introduction of a new scheme called \"ipp\"\n> 2) the introduction a new default port number for IPP servers.\n> \n> Before the IPP WG responds to those suggestions, the IPP WG would like to\n> get some advice from the HTTP WG on the implications of such a change.\n> In particular, we want some feedback on how easy or difficult it would be\n> to configure existing web servers to accomodate the suggested changes.\n> \n> Please note that many printer vendors are not in the business of developing\n> web servers or HTTP servers and are dependent on getting those compoments\n> from other vendors.\n> \n> Please respond back to the IPP DL at:\n> \n> ipp@pwg.org\n> \n> Thanks,\n> \n> Carl-Uno Manros\n> Chair of the IETF IPP WG\n> \n> \n\n\n\n", "id": "lists-012-3438740"}, {"subject": "RE: Implications of introducing new scheme and port for existing  HTTP server", "content": "I know of at least one :-) firewall that not only rejects unknown methods\nbut also examines the HTTP request method as part of its \"algorithm\". From a\nprotocol and security perspective, it appears to be the right thing to do.\nIf you don't understand the method, how can you properly proxy it? Take the\nCONNECT method as an example.\n\nIn summary, any proxy that is more than a simple packet passer (supports\nCONNECT, protocol conversion, proxy authentication, etc.) runs the risk of\nfailing to pass IPP if it uses a new scheme and/or a new method. Not that\nthat's a bad thing... :-)\n\n-Rob Polansky\n\n> -----Original Message-----\n> From: David W. Morris [mailto:dwm@xpasc.com]\n> Sent: Monday, June 01, 1998 10:34 PM\n> To: Carl-Uno Manros\n> Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org; http-wg@hplb.hpl.hp.com\n> Subject: Re: Implications of introducing new scheme and port for\n> existing HTTP servers\n>\n> (I'm also not wild about new HTTP methods as I know of existing proxies\n> which will reject unknown methods. Don't know of any which will accept\n> unknown methods. I'm also unaware of any firewall software which examines\n> the HTTP request method as part of its algorithm but then I'm not a\n> firewall expert.)\n>\n\n\n\n", "id": "lists-012-3450480"}, {"subject": "Re: Byte range", "content": "On Tue, 2 Jun 1998, Dave Kristol wrote:\n\n> It appears that RFC 2046 (Sect. 5.1.1) treats the CRLF that precedes a\n> boundary as *part* of the boundary:\n> \n>    The boundary delimiter MUST occur at the beginning of a line, i.e.,\n>    following a CRLF, and the initial CRLF is considered to be attached\n>    to the boundary delimiter line rather than part of the preceding\n>    part.\n>    [...]\n>    NOTE:  The CRLF preceding the boundary delimiter line is conceptually\n>    attached to the boundary so that it is possible to have a part that\n>    does not end with a CRLF (line  break).  Body parts that must be\n>    considered to end with line breaks, therefore, must have two CRLFs\n>    preceding the boundary delimiter line, the first of which is part of\n>    the preceding body part, and the second of which is part of the\n>    encapsulation boundary.\n> \n...\n> \n> To me it appears that RFC 2068 conflicts with RFC 2046 in its letter,\n> but follows it in the spirit.  I think we need a MIME guru to pass\n> judgement.  Or we can add another note to Section 19.4 of the HTTP/1.1\n> spec. about this difference beteen HTTP and MIME.\n> \n\nYou understand the problem correctly.\n\nI don't think that RFC is specific.  There is no discussion of CRLF.\nIt gives an example in which all CRLF's are invisible and that is all.\nI personally would not want to draw conclusions based on the number of\nblank lines in the formating of the spec document! I don't think it is\nsufficient to just refer to a MIME RFC either.\n\nI don't care how it is done, but we need to be more precise.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-3461476"}, {"subject": "Re: Byte range", "content": "John Franks wrote:\n> \n> I recently received the message below.  Looking at\n> draft-ietf-http-v11-spec-rev-03 it seems the description\n> of multipart/byteranges leaves a lot to be desired on the\n> subject of where <CRLF> should occur.  Is there a definitive\n> answer to this?\n> [...]\n> ---------- Forwarded message ----------\n> Date: Mon, 1 Jun 1998 16:04:33 -0600 (MDT)\n> From: Alex Rousskov <rousskov@nlanr.net>\n> [...]\n>         In implementing multipart range responses you probably followed RFC\n> 2046 (prepending boundaries with <CRLF>). However, HTTP RFC (2068) gives an\n> example where boundaries are _not_ prepended with <CRLF> (note that there is\n> only one empty line after headers):\n> \n>    HTTP/1.1 206 Partial content\n>    Date: Wed, 15 Nov 1995 06:25:24 GMT\n>    Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n>    Content-type: multipart/byteranges; boundary=THIS_STRING_SEPARATES\n> \n> [...]\n\nI'd like to be sure I understand the question.  Please correct me if\nI've got it wrong.\n\nIt appears that RFC 2046 (Sect. 5.1.1) treats the CRLF that precedes a\nboundary as *part* of the boundary:\n\n   The boundary delimiter MUST occur at the beginning of a line, i.e.,\n   following a CRLF, and the initial CRLF is considered to be attached\n   to the boundary delimiter line rather than part of the preceding\n   part.\n   [...]\n   NOTE:  The CRLF preceding the boundary delimiter line is conceptually\n   attached to the boundary so that it is possible to have a part that\n   does not end with a CRLF (line  break).  Body parts that must be\n   considered to end with line breaks, therefore, must have two CRLFs\n   preceding the boundary delimiter line, the first of which is part of\n   the preceding body part, and the second of which is part of the\n   encapsulation boundary.\n\nIn the example Alex cites, by RFC 2046, there should be two CRLFs in a\nrow:  1) to separate the HTTP response headers from the response body;\nand 2) to precede (and be considered part of) the multipart boundary.  \n\nOn the other hand, the purpose of the second CRLF is to ensure that the\nboundary occurs at the beginning of a line, which we already know to be\ntrue for the first boundary in an HTTP response (and in email\nmessages?).\n\nTo me it appears that RFC 2068 conflicts with RFC 2046 in its letter,\nbut follows it in the spirit.  I think we need a MIME guru to pass\njudgement.  Or we can add another note to Section 19.4 of the HTTP/1.1\nspec. about this difference beteen HTTP and MIME.\n\n(FWIW, the output from my server looks like the RFC 2068 example, with\none blank line between the HTTP response headers and the first\nboundary.)\n\nDave Kristol\n\n\n\n", "id": "lists-012-3471096"}, {"subject": "Re: IPP&gt; RE: Implications of introducing new scheme and port for existing  HTTP server", "content": "The past few comments about firewalls do not (IMHO) appear to pose a\nproblem for IPP deployment. If the majority of the installed base of\nfirewall products do not do HTTP method inspection then thats ok.\neverything would work. When the \"next-generation\" products that can perform\nthis type of inspection, then during installation of this new\ninfrastructure, the administrator will then enable IPP (or WEBDAV) or\nwhatever at that time.\n\nUltimately, I believe firewall admins will explicitly enable internet\nprinting or faxing or whatever, and I don't think a firewall issue should\nimpose undue design constraints on what we (the WG) want to do.\nFirewall admins already do this explicitly enabling/disabling of\napplication protocols (POP, FTP, IMAP, etc.) and I think we're just another\napplication. I don't think these protocol designers were too bogged down in\nfirewall issues during the development process. At least with the\nCheckpoint Firewall-1 product, it takes about 45 seconds to bring up the\nconsole and enable or disable a particular application protocol.\n\nJust my (possibly more than) $0.02\n\nRandy\n\n\n\nAt 08:15 AM 6/2/98 -0700, Vinod Valloppillil (Exchange) wrote:\n>Rob's argument is broadly correct -- as a long term firewall design issue,\n>method inspection (and occasionally payload inspection) will become the\n>rule.\n>\n>However, as a small carrot to today's protocol designers, the vast majority\n>of the installed base of firewalls do no method / payload inspection on HTTP\n>data being passed through.   Purely from the perspective of 'reach' there's\n>no impediment to IPP using it's own method in the short run.\n>\n>> -----Original Message-----\n>> From:Rob Polansky [SMTP:polansky@raptor.com]\n>> Sent:Tuesday, June 02, 1998 6:06 AM\n>> To:David W. Morris\n>> Cc:http-wg; ipp@pwg.org\n>> Subject:RE: Implications of introducing new scheme and port for\n>> existing  HTTP servers\n>> \n>> I know of at least one :-) firewall that not only rejects unknown methods\n>> but also examines the HTTP request method as part of its \"algorithm\". From\n>> a\n>> protocol and security perspective, it appears to be the right thing to do.\n>> If you don't understand the method, how can you properly proxy it? Take\n>> the\n>> CONNECT method as an example.\n>> \n>> In summary, any proxy that is more than a simple packet passer (supports\n>> CONNECT, protocol conversion, proxy authentication, etc.) runs the risk of\n>> failing to pass IPP if it uses a new scheme and/or a new method. Not that\n>> that's a bad thing... :-)\n>> \n>> -Rob Polansky\n>> \n>> > -----Original Message-----\n>> > From: David W. Morris [mailto:dwm@xpasc.com]\n>> > Sent: Monday, June 01, 1998 10:34 PM\n>> > To: Carl-Uno Manros\n>> > Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org; http-wg@hplb.hpl.hp.com\n>> > Subject: Re: Implications of introducing new scheme and port for\n>> > existing HTTP servers\n>> >\n>> > (I'm also not wild about new HTTP methods as I know of existing proxies\n>> > which will reject unknown methods. Don't know of any which will accept\n>> > unknown methods. I'm also unaware of any firewall software which\n>> examines\n>> > the HTTP request method as part of its algorithm but then I'm not a\n>> > firewall expert.)\n>> >\n> \n\n\n\n", "id": "lists-012-3481367"}, {"subject": "Re: Byte range", "content": "On Tue, 2 Jun 1998, Dave Kristol wrote:\n\n> It appears that RFC 2046 (Sect. 5.1.1) treats the CRLF that precedes a\n> boundary as *part* of the boundary:\n\nExactly.\n \n> In the example Alex cites, by RFC 2046, there should be two CRLFs in a\n> row:  1) to separate the HTTP response headers from the response body;\n> and 2) to precede (and be considered part of) the multipart boundary.  \n\nThat's correct. According to MIME, there also should be a <CRLF> before other\nboundaries (2nd, 3rd, etc). However, the example is not detailed enough to\nsee if those <CRLF>s are there. \n \n> On the other hand, the purpose of the second CRLF is to ensure that the\n> boundary occurs at the beginning of a line, \n\nI could understand why MIME cares about message \"appearance\". I see no reason\nwhy HTTP should be human-oriented. Too late for that though :). \n\n> which we already know to be\n> true for the first boundary in an HTTP response (and in email\n> messages?).\n\nRight, but to ease implementation (both generation and parsing) and to be\nconsistent, it would be much better if HTTP would explicitly require CRLF to\nprepend _all_ boundaries rather than all but the first one. Alternatively,\nHTTP could explicitly prohibit those extra CRLFs.\n \n> To me it appears that RFC 2068 conflicts with RFC 2046 in its letter,\n> but follows it in the spirit.\n\nHTTP says nothing about CRLFs prepending the boundaries. Thus, it encourages\nus to follow MIME specs. MIME requires CRLFs. However, the example in HTTP\ndoes not follow MIME requirements. Since MIME contains a lot of requirements\nthat HTTP overwrites, it is not clear if the intention was to overwrite this\nrequirement as well or not. \n\nIMHO, HTTP should minimize references to MIME format and simply provide its\nown BNF for multipart messages. Personally, I find \"refer-to-MIME and\nlist-the-differences\" approach error prone. Such an approach leads to\nquestions like \"did they forget to list that difference?\"... \n\n> (FWIW, the output from my server looks like the RFC 2068 example, with\n> one blank line between the HTTP response headers and the first\n> boundary.)\n\nThus, we already have two serves implementing multiparts differently: NW and\nyours. :)\n\nThank you,\n\nAlex.\n\n\n\n", "id": "lists-012-3496010"}, {"subject": "Re: Implications of introducing new scheme and port for exi", "content": "     I would agree with Josh Cohen and David Morris that introducing a new \n     URL scheme will affect proxies and is a bad idea. I also agree with \n     David Morris that a new HTTP scheme should be avoided for the same \n     reasons.\n     \n     Using a new default port is okay. The stuff about firewalls being \n     affected is true to the extent that they also had to be opened up for \n     the default ports used for other protocols - It really isn't a big \n     hassle reconfiguring them.\n     \n     However, I side with David Morris in questioning the need distinguish \n     between IPP and HTTP. Is there one?\n     \n     Dominic\n     \n     As a private person\n\n\n\n", "id": "lists-012-3505950"}, {"subject": "RE: Implications of introducing new scheme and port for existing  HTTP server", "content": "Rob's argument is broadly correct -- as a long term firewall design issue,\nmethod inspection (and occasionally payload inspection) will become the\nrule.\n\nHowever, as a small carrot to today's protocol designers, the vast majority\nof the installed base of firewalls do no method / payload inspection on HTTP\ndata being passed through.   Purely from the perspective of 'reach' there's\nno impediment to IPP using it's own method in the short run.\n\n> -----Original Message-----\n> From:Rob Polansky [SMTP:polansky@raptor.com]\n> Sent:Tuesday, June 02, 1998 6:06 AM\n> To:David W. Morris\n> Cc:http-wg; ipp@pwg.org\n> Subject:RE: Implications of introducing new scheme and port for\n> existing  HTTP servers\n> \n> I know of at least one :-) firewall that not only rejects unknown methods\n> but also examines the HTTP request method as part of its \"algorithm\". From\n> a\n> protocol and security perspective, it appears to be the right thing to do.\n> If you don't understand the method, how can you properly proxy it? Take\n> the\n> CONNECT method as an example.\n> \n> In summary, any proxy that is more than a simple packet passer (supports\n> CONNECT, protocol conversion, proxy authentication, etc.) runs the risk of\n> failing to pass IPP if it uses a new scheme and/or a new method. Not that\n> that's a bad thing... :-)\n> \n> -Rob Polansky\n> \n> > -----Original Message-----\n> > From: David W. Morris [mailto:dwm@xpasc.com]\n> > Sent: Monday, June 01, 1998 10:34 PM\n> > To: Carl-Uno Manros\n> > Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org; http-wg@hplb.hpl.hp.com\n> > Subject: Re: Implications of introducing new scheme and port for\n> > existing HTTP servers\n> >\n> > (I'm also not wild about new HTTP methods as I know of existing proxies\n> > which will reject unknown methods. Don't know of any which will accept\n> > unknown methods. I'm also unaware of any firewall software which\n> examines\n> > the HTTP request method as part of its algorithm but then I'm not a\n> > firewall expert.)\n> >\n\n\n\n", "id": "lists-012-3513877"}, {"subject": "Re: IPP&gt; Implications of introducing new scheme and port ", "content": "     Regarding item #2,\n     \n     Use of alternative HTTP ports, other than port 80, effects the ability \n     to move through proxies and firewalls. Using alternative port #'s will \n     require reconfiguration of security infrastructure in order to allow \n     for HTTP connections. \n     \n     HP has gone through similar work in the definition and standardization \n     of HTTP port 280 for Web Based Management Purposes ( see IANA port \n     list ). Currently port 280 is IANA approved for usage of HTTP for \n     network management. This works fine for Intranet usage, but issues as \n     described above result when operating in a secure environments.\n     \n     The other issue is that of configuring HTTP servers and proxies to \n     listen on alternative port #s. While easy to do programatically, not \n     all commercial HTTP servers allow listening on multiple ports \n     concurrently.\n     \n     Considering these two issues, partitioning of the URI space for IPP on \n     HTTP port 80 or HTTP-S (HTTP/(SSL |TLS)) on port 443 makes better \n     sense.\n     \n     Peter\n\n\n______________________________ Reply Separator _________________________________\nSubject: IPP> Implications of introducing new scheme and port for e\nAuthor:  Non-HP-manros (manros@cp10.es.xerox.com) at HP-Roseville,mimegw4\nDate:    6/1/98 10:20 AM\n\n\nHi,\n     \nAs most of you know already, the Internet Printing Protocol (IPP) WG has \nsuggested using HTTP as \"transport\", with the payload in the form of a MIME \nobject passed with the POST method.\n     \nAs part of the onging IESG review process, the Application Area Director \nKeith Moore has suggested to distinguish IPP traffic from \"normal\" HTTP \ntraffic by: \n     \n1) the introduction of a new scheme called \"ipp\"\n2) the introduction a new default port number for IPP servers.\n     \nBefore the IPP WG responds to those suggestions, the IPP WG would like to \nget some advice from the HTTP WG on the implications of such a change.\nIn particular, we want some feedback on how easy or difficult it would be \nto configure existing web servers to accomodate the suggested changes.\n     \nPlease note that many printer vendors are not in the business of developing \nweb servers or HTTP servers and are dependent on getting those compoments \nfrom other vendors.\n     \nPlease respond back to the IPP DL at:\n     \n        ipp@pwg.org\n     \nThanks,\n     \nCarl-Uno Manros\nChair of the IETF IPP WG\n\n\n\n", "id": "lists-012-3527009"}, {"subject": "Re: Byte range", "content": "For what it's worth: I'm pretty sure I was the one who wrote\nthe example for the multipart/byteranges in the HTTP/1.1 spec.\n\nI'm *very* sure that I knew (and know) nothing about MIME rules,\nnor have I ever read RFC2046.  Since I knew that I was ignorant,\nI asked a few MIME experts to check the specification and the\nexample, and left it at that.  (I can't remember who I asked,\nso I won't try to assign any blame for not spotting the ambiguity.)\n\nAnd, as John Franks alludes, the formatting of the document\nhas been somewhat at the whim of a well-known (and somewhat\nunpredictable) word procesing program, so it's not at all\nclear whether whatever example was originally written is the\none that now appears in the draft.  (However, I don't think\nthis one has been changed.)\n\nBottom line: we should not be putting too much weight on this\nspecific example.  If the text of the HTTP/1.1 spec is ambiguous,\nwe need to fix that.  Then we can revise the example to match\nthe text, perhaps with a note to be cautiously liberal about accepting\nmultipart/byteranges with unexpected numbers of CRLFs.\n\nAnd with a note to the RFC editor to be careful about the formatting\nof the example :-)\n\n-Jeff\n\n\n\n", "id": "lists-012-3537501"}, {"subject": "finishing testing of HTTP/1.1 (help requested)...", "content": "I've added recent reports from Lester Waters of Microsoft,\nRobert Polansky of Raptor (Axent), Mike Belshe of Netscape; my thanks\nto them for filing them.  There have been over 20 reports filed to date.\n\nI encourage others to submit reports.  There are still some significant\n1.1 implementations out there for which I don't have data.\n\nTo make it easier to update reports, if people who have filed wish to \ndrop me a note saying things like \"I finished testing feature XXX\", or \n\"I found that my testing of feature XXX was broken or in error\", I am \nwilling to edit the existing reports to save people the effort of filling\nout the very long form from scratch...\n\nAnyone who can knock of features that don't have enough tested\nimplementations would be highly appreciated... \n\nI've automated generating the reports so that updating that page\nis now easy for me to do.\n\nSee: http://www.w3.org/Protocols/HTTP/Forum/Reports/ for the\ncurrent state of reports.  It has a rollup of all the data,\nand a list of features for which we are not \n\nUsing the on-line data in the reprot should make it easier to figure out \nwho has implementations to test against.\n\nThere are a few features where I am skeptical proper testing has\nbeen done, which I plan to understand further to make myself confident we've\nmet IETF rules.\n\nThe list of untested implentations on the base spec continues to shrink. \nSome of the leftover features  below are actually incorrect, as there \nare often no special requirements on proxies; we don't really need proxy \nimplementations for those features (though we wouldn't mind having them; \nin fact, one might claim that a proxy that just passes the response back \nfrom the origin server is correct), and that therefore we have \nimplementations in hand in any case...  \n\nFeatures that do not have requirements on proxies include 202 Accepted, \n203 Non-Authoritative Information, 205 Reset Content, 300 Multiple Choices, \n303 See Other, 305 Multiple Choices, 410 Gone, 411 Length Required, 412 \nPrecondition Failed, 413 Request Entity Too Large, 414 Request-URI Too \nLong, Content-MD5, TE, Trailer (if my quick look at the spec is correct).\n\nClients         Servers         Proxies         Feature\n 1t  3y  7n  2-  3t  2y 12n  0-  1t  1y  6n  0- H 10.2.3        202 Accepted\n 1t  2y  8n  2-  2t  2y 13n  0-  1t  1y  6n  0- H 10.2.4        203 Non-Authoritative Information\n 1t  2y  8n  2-  2t  1y 14n  0-  1t  1y  6n  0- H 10.2.6        205 Reset Content\n 2t  2y  8n  1-  2t  3y 12n  0-  0t  2y  6n  0- H 10.3.1        300 Multiple Choices\n 3t  4y  6n  0-  2t  3y 12n  0-  0t  3y  5n  0- H 10.3.4        303 See Other\n 2t  4y  7n  0-  1t  3y 12n  1-  1t  2y  5n  0- H 10.3.6        305 Use Proxy\n 2t  3y  7n  1-  2t  2y 13n  0-  1t  1y  6n  0- H 10.4.11       410 Gone\n 3t  5y  5n  0-  2t  6y  9n  0-  1t  3y  4n  0- H 10.4.12       411 Length Required\n 3t  4y  5n  1-  3t  7y  7n  0-  1t  3y  4n  0- H 10.4.13       412 Precondition Failed\n 2t  4y  6n  1-  3t  3y 11n  0-  1t  2y  5n  0- H 10.4.14       413 Request Entity Too Large\n 2t  3y  7n  1-  2t  2y 13n  0-  1t  1y  6n  0- H 10.4.15       414 Request-URI Too Long\n 1t  4y  7n  1-  2t  5y 10n  0-  1t  2y  5n  0- H 10.4.18       417 Expectation Failed\n 1t  5y  6n  1-  3t  5y  9n  0-  1t  3y  4n  0- H 13.3.3        Weak entity tags\n 2t  2y  9n  0-  1t  7y  9n  0-  1t  2y  5n  0- H 14.15 Content-MD5\n 3t  4y  4n  2-  5t  7y  5n  0-  1t  4y  3n  0- H 14.24 If-Match\n 3t  3y  6n  1-  4t  7y  6n  0-  1t  3y  4n  0- H 14.26 If-None-Match\n 3t  1y  8n  1-  2t  6y  9n  0-  1t  1y  6n  0- H 14.27 If-Range\n 3t  3y  7n  0-  1t  2y 14n  0-  1t  1y  6n  0- H 14.39 TE\n 3t  2y  8n  0-  0t  2y 15n  0-  0t  2y  6n  0- H 14.40 Trailer\n 3t  4y  4n  2-  1t  4y 11n  1-  1t  4y  3n  0- H 14.46 Warning\n\nAuthentication is in poorer shape, where we need another pretty complete\nimplementation to meet requirements. Our thanks to Ronald Tschal?r,\nwho has done a complete client implementation (though hasn't had a proxy\nto test the proxy authentication against).\n\nClients         Servers         Proxies         Feature\n 2t  0y 11n  0-  3t  5y  9n  0-  1t  0y  7n  0- A 3.2.1 WWW-Authenticate Digest\n 1t  0y 12n  0-  0t  1y 16n  0-  0t  0y  8n  0- A 3.2.1 qop-options auth\n 1t  0y 12n  0-  0t  0y 17n  0-  0t  0y  8n  0- A 3.2.1 qop-options auth-int\n 2t  0y 11n  0-  2t  5y 10n  0-  1t  0y  7n  0- A 3.2.2 Authorization Digest\n 1t  0y 12n  0-  0t  1y 16n  0-  0t  0y  8n  0- A 3.2.2 request qop auth\n 1t  0y 12n  0-  0t  0y 17n  0-  0t  0y  8n  0- A 3.2.2 request qop auth-int\n 2t  0y 11n  0-  2t  2y 13n  0-  1t  0y  7n  0- A 3.2.3 Authentication-Info Digest\n 1t  0y 12n  0-  0t  2y 15n  0-  0t  0y  8n  0- A 3.2.3 response qop auth\n 1t  0y 12n  0-  0t  0y 17n  0-  0t  0y  8n  0- A 3.2.3 response qop auth-int\n 2t  0y 11n  0-  1t  1y 11n  4-  1t  0y  7n  0- A 4.2   Proxy-Authenticate Digest\n 0t  1y 12n  0-  0t  0y 13n  4-  0t  0y  8n  0- A 4.2   Proxy qop-options auth\n 0t  1y 12n  0-  0t  0y 13n  4-  0t  0y  8n  0- A 4.2   Proxy qop-options auth-int\n 2t  0y 11n  0-  1t  1y 11n  4-  1t  0y  7n  0- A 4.2   Proxy Authorization Digest\n 0t  1y 12n  0-  0t  0y 13n  4-  0t  0y  8n  0- A 4.2   Proxy request qop auth\n 0t  1y 12n  0-  0t  0y 13n  4-  0t  0y  8n  0- A 4.2   Proxy request qop auth-int\n 1t  1y 11n  0-  1t  0y 12n  4-  1t  0y  7n  0- A 4.2   Proxy Authentication-Info Digest\n 0t  1y 12n  0-  0t  0y 13n  4-  0t  0y  8n  0- A 4.2   Proxy response qop auth\n 0t  1y 12n  0-  0t  0y 13n  4-  0t  0y  8n  0- A 4.2   Proxy response qop auth-int\n\n- Jim Gettys\n\n\n\n", "id": "lists-012-3545066"}, {"subject": "RE: IPP&gt; RE: Implications of introducing new scheme and port for existing  HTTP server", "content": "This kind of flamebait is not really helpful to our discussion. Firewalls\nserve legitimate technical and business needs as our friends from Microsoft\nknow, and those firewalls with application proxies look at protocols from a\ndifferent point of view than your typical caching proxies. The beauty of it\nis that protocol compliant implementations from either the firewall or the\ncache point of view will interoperate. The difference is when they encounter\nsomething unexpected. Firewalls by definition must \"fail closed\" so as not\nto make their protected resources vulnerable to attacks; most other software\nmakes a best effort to pass data. I don't see anything wrong with that\ndifference.\n\nOnce again, if IPP uses existing methods and schemes, it should be passable\nthrough all proxies without trouble. Add a new method and/or scheme i.e.\nCHANGE THE STANDARD, and you should expect that existing implemenations will\nnot understand it and some (not many) may not pass it.\n\n-Rob Polansky\n\n> -----Original Message-----\n> From: Paul Moore [mailto:paulmo@microsoft.com]\n> Sent: Tuesday, June 02, 1998 8:37 PM\n> To: 'Randy Turner'; Vinod Valloppillil (Exchange); 'Rob Polansky'; David\n> W. Morris\n> Cc: http-wg; ipp@pwg.org\n> Subject: RE: IPP> RE: Implications of introducing new scheme and port\n> for existing HTTP servers\n>\n>\n> The issue is proxies - enablers - not firewalls - disablers. If\n> you replace\n> my proxy by a passthrough cable I cannot do anything, if you replace my\n> firewall by a cable you can do anything.\n>\n> > -----Original Message-----\n> > From:Randy Turner [SMTP:rturner@sharplabs.com]\n> > Sent:Tuesday, June 02, 1998 8:34 AM\n> > To:Vinod Valloppillil (Exchange); 'Rob Polansky'; David W. Morris\n> > Cc:http-wg; ipp@pwg.org\n> > Subject:Re: IPP> RE: Implications of introducing new scheme and port\n> > for existing  HTTP servers\n> >\n> >\n> > The past few comments about firewalls do not (IMHO) appear to pose a\n> > problem for IPP deployment. If the majority of the installed base of\n> > firewall products do not do HTTP method inspection then thats ok.\n> > everything would work. When the \"next-generation\" products that can\n> > perform\n> > this type of inspection, then during installation of this new\n> > infrastructure, the administrator will then enable IPP (or WEBDAV) or\n> > whatever at that time.\n> >\n> > Ultimately, I believe firewall admins will explicitly enable internet\n> > printing or faxing or whatever, and I don't think a firewall\n> issue should\n> > impose undue design constraints on what we (the WG) want to do.\n> > Firewall admins already do this explicitly enabling/disabling of\n> > application protocols (POP, FTP, IMAP, etc.) and I think we're just\n> > another\n> > application. I don't think these protocol designers were too bogged down\n> > in\n> > firewall issues during the development process. At least with the\n> > Checkpoint Firewall-1 product, it takes about 45 seconds to bring up the\n> > console and enable or disable a particular application protocol.\n> >\n> > Just my (possibly more than) $0.02\n> >\n> > Randy\n> >\n> >\n> >\n> > At 08:15 AM 6/2/98 -0700, Vinod Valloppillil (Exchange) wrote:\n> > >Rob's argument is broadly correct -- as a long term firewall design\n> > issue,\n> > >method inspection (and occasionally payload inspection) will become the\n> > >rule.\n> > >\n> > >However, as a small carrot to today's protocol designers, the vast\n> > majority\n> > >of the installed base of firewalls do no method / payload inspection on\n> > HTTP\n> > >data being passed through.   Purely from the perspective of 'reach'\n> > there's\n> > >no impediment to IPP using it's own method in the short run.\n> > >\n> > >> -----Original Message-----\n> > >> From:Rob Polansky [SMTP:polansky@raptor.com]\n> > >> Sent:Tuesday, June 02, 1998 6:06 AM\n> > >> To:David W. Morris\n> > >> Cc:http-wg; ipp@pwg.org\n> > >> Subject:RE: Implications of introducing new scheme and port for\n> > >> existing  HTTP servers\n> > >>\n> > >> I know of at least one :-) firewall that not only rejects unknown\n> > methods\n> > >> but also examines the HTTP request method as part of its \"algorithm\".\n> > From\n> > >> a\n> > >> protocol and security perspective, it appears to be the\n> right thing to\n> > do.\n> > >> If you don't understand the method, how can you properly\n> proxy it? Take\n> > >> the\n> > >> CONNECT method as an example.\n> > >>\n> > >> In summary, any proxy that is more than a simple packet passer\n> > (supports\n> > >> CONNECT, protocol conversion, proxy authentication, etc.)\n> runs the risk\n> > of\n> > >> failing to pass IPP if it uses a new scheme and/or a new method. Not\n> > that\n> > >> that's a bad thing... :-)\n> > >>\n> > >> -Rob Polansky\n> > >>\n> > >> > -----Original Message-----\n> > >> > From: David W. Morris [mailto:dwm@xpasc.com]\n> > >> > Sent: Monday, June 01, 1998 10:34 PM\n> > >> > To: Carl-Uno Manros\n> > >> > Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org; http-wg@hplb.hpl.hp.com\n> > >> > Subject: Re: Implications of introducing new scheme and port for\n> > >> > existing HTTP servers\n> > >> >\n> > >> > (I'm also not wild about new HTTP methods as I know of existing\n> > proxies\n> > >> > which will reject unknown methods. Don't know of any which will\n> > accept\n> > >> > unknown methods. I'm also unaware of any firewall software which\n> > >> examines\n> > >> > the HTTP request method as part of its algorithm but then I'm not a\n> > >> > firewall expert.)\n> > >> >\n> > >\n>\n\n\n\n", "id": "lists-012-3557124"}, {"subject": "apologie", "content": "Perhaps I misread Paul's previous message and replied with redundant and\nunnecessary verbiage. So I'm sorry for wasting your bandwidth. Now back to\nour regularly scheduled programs...\n\nHey mister, your shopping cart is rolling down the highway!\nhttp://www.ultranet.com/~polansky/\nhttp://nairobi.raptor.com/users/rpolansky/\nAIM: polanskyr\n\n\n\n\n\napplication/x-pkcs7-signature attachment: smime.p7s\n\n\n\n\n", "id": "lists-012-3575010"}, {"subject": "RE: IPP&gt; RE: Implications of introducing new scheme and port for  existing  HTTP server", "content": "When you say \"change the standard\" are you referring to RFC 2068 or the IPP\nstandard?\n\nThanks,\nYaron\n\n> -----Original Message-----\n> From: Rob Polansky [mailto:polansky@raptor.com]\n> Sent: Wednesday, June 03, 1998 5:55 AM\n> To: Paul Moore\n> Cc: http-wg; ipp@pwg.org\n> Subject: RE: IPP> RE: Implications of introducing new scheme and port\n> for existing HTTP servers\n> \n> \n> This kind of flamebait is not really helpful to our \n> discussion. Firewalls\n> serve legitimate technical and business needs as our friends \n> from Microsoft\n> know, and those firewalls with application proxies look at \n> protocols from a\n> different point of view than your typical caching proxies. \n> The beauty of it\n> is that protocol compliant implementations from either the \n> firewall or the\n> cache point of view will interoperate. The difference is when \n> they encounter\n> something unexpected. Firewalls by definition must \"fail \n> closed\" so as not\n> to make their protected resources vulnerable to attacks; most \n> other software\n> makes a best effort to pass data. I don't see anything wrong with that\n> difference.\n> \n> Once again, if IPP uses existing methods and schemes, it \n> should be passable\n> through all proxies without trouble. Add a new method and/or \n> scheme i.e.\n> CHANGE THE STANDARD, and you should expect that existing \n> implemenations will\n> not understand it and some (not many) may not pass it.\n> \n> -Rob Polansky\n> \n> > -----Original Message-----\n> > From: Paul Moore [mailto:paulmo@microsoft.com]\n> > Sent: Tuesday, June 02, 1998 8:37 PM\n> > To: 'Randy Turner'; Vinod Valloppillil (Exchange); 'Rob \n> Polansky'; David\n> > W. Morris\n> > Cc: http-wg; ipp@pwg.org\n> > Subject: RE: IPP> RE: Implications of introducing new \n> scheme and port\n> > for existing HTTP servers\n> >\n> >\n> > The issue is proxies - enablers - not firewalls - disablers. If\n> > you replace\n> > my proxy by a passthrough cable I cannot do anything, if \n> you replace my\n> > firewall by a cable you can do anything.\n> >\n> > > -----Original Message-----\n> > > From:Randy Turner [SMTP:rturner@sharplabs.com]\n> > > Sent:Tuesday, June 02, 1998 8:34 AM\n> > > To:Vinod Valloppillil (Exchange); 'Rob Polansky'; \n> David W. Morris\n> > > Cc:http-wg; ipp@pwg.org\n> > > Subject:Re: IPP> RE: Implications of introducing new \n> scheme and port\n> > > for existing  HTTP servers\n> > >\n> > >\n> > > The past few comments about firewalls do not (IMHO) \n> appear to pose a\n> > > problem for IPP deployment. If the majority of the \n> installed base of\n> > > firewall products do not do HTTP method inspection then thats ok.\n> > > everything would work. When the \"next-generation\" \n> products that can\n> > > perform\n> > > this type of inspection, then during installation of this new\n> > > infrastructure, the administrator will then enable IPP \n> (or WEBDAV) or\n> > > whatever at that time.\n> > >\n> > > Ultimately, I believe firewall admins will explicitly \n> enable internet\n> > > printing or faxing or whatever, and I don't think a firewall\n> > issue should\n> > > impose undue design constraints on what we (the WG) want to do.\n> > > Firewall admins already do this explicitly enabling/disabling of\n> > > application protocols (POP, FTP, IMAP, etc.) and I think \n> we're just\n> > > another\n> > > application. I don't think these protocol designers were \n> too bogged down\n> > > in\n> > > firewall issues during the development process. At least with the\n> > > Checkpoint Firewall-1 product, it takes about 45 seconds \n> to bring up the\n> > > console and enable or disable a particular application protocol.\n> > >\n> > > Just my (possibly more than) $0.02\n> > >\n> > > Randy\n> > >\n> > >\n> > >\n> > > At 08:15 AM 6/2/98 -0700, Vinod Valloppillil (Exchange) wrote:\n> > > >Rob's argument is broadly correct -- as a long term \n> firewall design\n> > > issue,\n> > > >method inspection (and occasionally payload inspection) \n> will become the\n> > > >rule.\n> > > >\n> > > >However, as a small carrot to today's protocol \n> designers, the vast\n> > > majority\n> > > >of the installed base of firewalls do no method / \n> payload inspection on\n> > > HTTP\n> > > >data being passed through.   Purely from the perspective \n> of 'reach'\n> > > there's\n> > > >no impediment to IPP using it's own method in the short run.\n> > > >\n> > > >> -----Original Message-----\n> > > >> From:Rob Polansky [SMTP:polansky@raptor.com]\n> > > >> Sent:Tuesday, June 02, 1998 6:06 AM\n> > > >> To:David W. Morris\n> > > >> Cc:http-wg; ipp@pwg.org\n> > > >> Subject:RE: Implications of introducing new \n> scheme and port for\n> > > >> existing  HTTP servers\n> > > >>\n> > > >> I know of at least one :-) firewall that not only \n> rejects unknown\n> > > methods\n> > > >> but also examines the HTTP request method as part of \n> its \"algorithm\".\n> > > From\n> > > >> a\n> > > >> protocol and security perspective, it appears to be the\n> > right thing to\n> > > do.\n> > > >> If you don't understand the method, how can you properly\n> > proxy it? Take\n> > > >> the\n> > > >> CONNECT method as an example.\n> > > >>\n> > > >> In summary, any proxy that is more than a simple packet passer\n> > > (supports\n> > > >> CONNECT, protocol conversion, proxy authentication, etc.)\n> > runs the risk\n> > > of\n> > > >> failing to pass IPP if it uses a new scheme and/or a \n> new method. Not\n> > > that\n> > > >> that's a bad thing... :-)\n> > > >>\n> > > >> -Rob Polansky\n> > > >>\n> > > >> > -----Original Message-----\n> > > >> > From: David W. Morris [mailto:dwm@xpasc.com]\n> > > >> > Sent: Monday, June 01, 1998 10:34 PM\n> > > >> > To: Carl-Uno Manros\n> > > >> > Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org; \n> http-wg@hplb.hpl.hp.com\n> > > >> > Subject: Re: Implications of introducing new scheme \n> and port for\n> > > >> > existing HTTP servers\n> > > >> >\n> > > >> > (I'm also not wild about new HTTP methods as I know \n> of existing\n> > > proxies\n> > > >> > which will reject unknown methods. Don't know of any \n> which will\n> > > accept\n> > > >> > unknown methods. I'm also unaware of any firewall \n> software which\n> > > >> examines\n> > > >> > the HTTP request method as part of its algorithm but \n> then I'm not a\n> > > >> > firewall expert.)\n> > > >> >\n> > > >\n> >\n> \n\n\n\n", "id": "lists-012-3582374"}, {"subject": "RE: IPP&gt; RE: Implications of introducing new scheme and port for  existing  HTTP server", "content": "Rob clarified in personal e-mail that he meant the latest rev of the HTTP\ndraft.\n\nOne of the innovations of HTTP in respect to many other protocols is that\nyou do not need to modify the HTTP standard in order to add new methods for\nuse with HTTP. Rather HTTP defines exactly how one is to act if one receives\nan unknown method. Thus one can safely add new methods and know that at the\nworst one will simply receive a method unknown error from servers/firewalls\nand be tunneled by proxies.\n\nFirewall designers understood this simple design goal and thus allowed\nadministrators to specify which methods they did and did not want to allow\nthrough their firewall. Thus the issue is not forcing administrators to rev\ntheir firewalls to be compliant with some new standard but rather having\nadministrators brought into the loop in order to approve the use of a new\nmethod through their firewall. Once they approve they need only change\nsettings on their firewall to permit the new method. It is this very process\nthat firewalls were introduced to enforce. Administrators realized they\ncould not possibly control what every server in their network did. Rather\nthan chasing after every user in order to ensure they were not running\n\"unapproved\" or \"insecure\" software they configured their network such that\nanyone wishing to do anything external to the network had to run through\ntheir firewall.\n\nThus were IPP to use a new method there would be absolutely no need to alter\nthe HTTP standard and IPP would be acting in a manner consistent with the\nexpress desires of the administrative community.\n\nYaron\n\n> -----Original Message-----\n> From: Yaron Goland \n> Sent: Wednesday, June 03, 1998 11:03 AM\n> To: 'Rob Polansky'; Paul Moore\n> Cc: http-wg; ipp@pwg.org\n> Subject: RE: IPP> RE: Implications of introducing new scheme and port\n> for existing HTTP servers\n> \n> \n> When you say \"change the standard\" are you referring to RFC \n> 2068 or the IPP standard?\n> \n> Thanks,\n> Yaron\n> \n> > -----Original Message-----\n> > From: Rob Polansky [mailto:polansky@raptor.com]\n> > Sent: Wednesday, June 03, 1998 5:55 AM\n> > To: Paul Moore\n> > Cc: http-wg; ipp@pwg.org\n> > Subject: RE: IPP> RE: Implications of introducing new \n> scheme and port\n> > for existing HTTP servers\n> > \n> > \n> > This kind of flamebait is not really helpful to our \n> > discussion. Firewalls\n> > serve legitimate technical and business needs as our friends \n> > from Microsoft\n> > know, and those firewalls with application proxies look at \n> > protocols from a\n> > different point of view than your typical caching proxies. \n> > The beauty of it\n> > is that protocol compliant implementations from either the \n> > firewall or the\n> > cache point of view will interoperate. The difference is when \n> > they encounter\n> > something unexpected. Firewalls by definition must \"fail \n> > closed\" so as not\n> > to make their protected resources vulnerable to attacks; most \n> > other software\n> > makes a best effort to pass data. I don't see anything \n> wrong with that\n> > difference.\n> > \n> > Once again, if IPP uses existing methods and schemes, it \n> > should be passable\n> > through all proxies without trouble. Add a new method and/or \n> > scheme i.e.\n> > CHANGE THE STANDARD, and you should expect that existing \n> > implemenations will\n> > not understand it and some (not many) may not pass it.\n> > \n> > -Rob Polansky\n> > \n> > > -----Original Message-----\n> > > From: Paul Moore [mailto:paulmo@microsoft.com]\n> > > Sent: Tuesday, June 02, 1998 8:37 PM\n> > > To: 'Randy Turner'; Vinod Valloppillil (Exchange); 'Rob \n> > Polansky'; David\n> > > W. Morris\n> > > Cc: http-wg; ipp@pwg.org\n> > > Subject: RE: IPP> RE: Implications of introducing new \n> > scheme and port\n> > > for existing HTTP servers\n> > >\n> > >\n> > > The issue is proxies - enablers - not firewalls - disablers. If\n> > > you replace\n> > > my proxy by a passthrough cable I cannot do anything, if \n> > you replace my\n> > > firewall by a cable you can do anything.\n> > >\n> > > > -----Original Message-----\n> > > > From:Randy Turner [SMTP:rturner@sharplabs.com]\n> > > > Sent:Tuesday, June 02, 1998 8:34 AM\n> > > > To:Vinod Valloppillil (Exchange); 'Rob Polansky'; \n> > David W. Morris\n> > > > Cc:http-wg; ipp@pwg.org\n> > > > Subject:Re: IPP> RE: Implications of introducing new \n> > scheme and port\n> > > > for existing  HTTP servers\n> > > >\n> > > >\n> > > > The past few comments about firewalls do not (IMHO) \n> > appear to pose a\n> > > > problem for IPP deployment. If the majority of the \n> > installed base of\n> > > > firewall products do not do HTTP method inspection then \n> thats ok.\n> > > > everything would work. When the \"next-generation\" \n> > products that can\n> > > > perform\n> > > > this type of inspection, then during installation of this new\n> > > > infrastructure, the administrator will then enable IPP \n> > (or WEBDAV) or\n> > > > whatever at that time.\n> > > >\n> > > > Ultimately, I believe firewall admins will explicitly \n> > enable internet\n> > > > printing or faxing or whatever, and I don't think a firewall\n> > > issue should\n> > > > impose undue design constraints on what we (the WG) want to do.\n> > > > Firewall admins already do this explicitly enabling/disabling of\n> > > > application protocols (POP, FTP, IMAP, etc.) and I think \n> > we're just\n> > > > another\n> > > > application. I don't think these protocol designers were \n> > too bogged down\n> > > > in\n> > > > firewall issues during the development process. At \n> least with the\n> > > > Checkpoint Firewall-1 product, it takes about 45 seconds \n> > to bring up the\n> > > > console and enable or disable a particular application protocol.\n> > > >\n> > > > Just my (possibly more than) $0.02\n> > > >\n> > > > Randy\n> > > >\n> > > >\n> > > >\n> > > > At 08:15 AM 6/2/98 -0700, Vinod Valloppillil (Exchange) wrote:\n> > > > >Rob's argument is broadly correct -- as a long term \n> > firewall design\n> > > > issue,\n> > > > >method inspection (and occasionally payload inspection) \n> > will become the\n> > > > >rule.\n> > > > >\n> > > > >However, as a small carrot to today's protocol \n> > designers, the vast\n> > > > majority\n> > > > >of the installed base of firewalls do no method / \n> > payload inspection on\n> > > > HTTP\n> > > > >data being passed through.   Purely from the perspective \n> > of 'reach'\n> > > > there's\n> > > > >no impediment to IPP using it's own method in the short run.\n> > > > >\n> > > > >> -----Original Message-----\n> > > > >> From:Rob Polansky [SMTP:polansky@raptor.com]\n> > > > >> Sent:Tuesday, June 02, 1998 6:06 AM\n> > > > >> To:David W. Morris\n> > > > >> Cc:http-wg; ipp@pwg.org\n> > > > >> Subject:RE: Implications of introducing new \n> > scheme and port for\n> > > > >> existing  HTTP servers\n> > > > >>\n> > > > >> I know of at least one :-) firewall that not only \n> > rejects unknown\n> > > > methods\n> > > > >> but also examines the HTTP request method as part of \n> > its \"algorithm\".\n> > > > From\n> > > > >> a\n> > > > >> protocol and security perspective, it appears to be the\n> > > right thing to\n> > > > do.\n> > > > >> If you don't understand the method, how can you properly\n> > > proxy it? Take\n> > > > >> the\n> > > > >> CONNECT method as an example.\n> > > > >>\n> > > > >> In summary, any proxy that is more than a simple \n> packet passer\n> > > > (supports\n> > > > >> CONNECT, protocol conversion, proxy authentication, etc.)\n> > > runs the risk\n> > > > of\n> > > > >> failing to pass IPP if it uses a new scheme and/or a \n> > new method. Not\n> > > > that\n> > > > >> that's a bad thing... :-)\n> > > > >>\n> > > > >> -Rob Polansky\n> > > > >>\n> > > > >> > -----Original Message-----\n> > > > >> > From: David W. Morris [mailto:dwm@xpasc.com]\n> > > > >> > Sent: Monday, June 01, 1998 10:34 PM\n> > > > >> > To: Carl-Uno Manros\n> > > > >> > Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org; \n> > http-wg@hplb.hpl.hp.com\n> > > > >> > Subject: Re: Implications of introducing new scheme \n> > and port for\n> > > > >> > existing HTTP servers\n> > > > >> >\n> > > > >> > (I'm also not wild about new HTTP methods as I know \n> > of existing\n> > > > proxies\n> > > > >> > which will reject unknown methods. Don't know of any \n> > which will\n> > > > accept\n> > > > >> > unknown methods. I'm also unaware of any firewall \n> > software which\n> > > > >> examines\n> > > > >> > the HTTP request method as part of its algorithm but \n> > then I'm not a\n> > > > >> > firewall expert.)\n> > > > >> >\n> > > > >\n> > >\n> > \n> \n\n\n\n", "id": "lists-012-3604303"}, {"subject": "Re: IPP&gt; RE: Implications of introducing new scheme and port for  existing  HTTP server", "content": "Yaron Goland wrote:\n> \n> Rob clarified in personal e-mail that he meant the latest rev of the HTTP\n> draft.\n> \n> One of the innovations of HTTP in respect to many other protocols is that\n> you do not need to modify the HTTP standard in order to add new methods for\n> use with HTTP. Rather HTTP defines exactly how one is to act if one receives\n> an unknown method. Thus one can safely add new methods and know that at the\n> worst one will simply receive a method unknown error from servers/firewalls\n> and be tunneled by proxies.\n\nHow can one \"know that at the worst one will ... be tunneled by\nproxies\"?  I can't find anything in the HTTP/1.1 spec. that instructs\nproxies to tunnel unknown methods.  I think at worst the request will be\nrejected.\n\nDave Kristol\n\n\n\n", "id": "lists-012-3630281"}, {"subject": "RE: IPP&gt; RE: Implications of introducing new scheme and port for  existing  HTTP server", "content": "You're right, a non-transparent proxy could reject an unknown method.\nHowever the point was that sending methods not specified in the HTTP spec is\nnot a protocol violation.\n\nYaron\n\n> -----Original Message-----\n> From: Dave Kristol [mailto:dmk@bell-labs.com]\n> Sent: Wednesday, June 03, 1998 1:17 PM\n> To: Yaron Goland\n> Cc: 'http-wg'; 'ipp@pwg.org'\n> Subject: Re: IPP> RE: Implications of introducing new scheme and port\n> for existing HTTP servers\n> \n> \n> Yaron Goland wrote:\n> > \n> > Rob clarified in personal e-mail that he meant the latest \n> rev of the HTTP\n> > draft.\n> > \n> > One of the innovations of HTTP in respect to many other \n> protocols is that\n> > you do not need to modify the HTTP standard in order to add \n> new methods for\n> > use with HTTP. Rather HTTP defines exactly how one is to \n> act if one receives\n> > an unknown method. Thus one can safely add new methods and \n> know that at the\n> > worst one will simply receive a method unknown error from \n> servers/firewalls\n> > and be tunneled by proxies.\n> \n> How can one \"know that at the worst one will ... be tunneled by\n> proxies\"?  I can't find anything in the HTTP/1.1 spec. that instructs\n> proxies to tunnel unknown methods.  I think at worst the \n> request will be\n> rejected.\n> \n> Dave Kristol\n> \n\n\n\n", "id": "lists-012-3639850"}, {"subject": "IBM patents tunneling HTTP through another protoca", "content": "Sigh.\n\n<URL:http://www.patents.ibm.com/details?patent_number=5754774>\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-012-3651300"}, {"subject": "RE: IBM patents tunneling HTTP through another protoca", "content": "hmm..\nWouldnt a prior work be tunneling HTTP through SSL, as\noriginated by Netscape ?\nThis is how https works...\n\njosh\n\n\n> -----Original Message-----\n> From: Benjamin \"Snowhare\" Franz \n> [mailto:snowhare@xmission.xmission.com]\n> Sent: Thursday, June 04, 1998 11:20 AM\n> To: HTTP Working Group\n> Subject: IBM patents tunneling HTTP through another protocal\n> \n> \n> Sigh.\n> \n> <URL:http://www.patents.ibm.com/details?patent_number=5754774>\n> \n> -- \n> Benjamin Franz\n> \n\n\n\n", "id": "lists-012-3660117"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "In a message dated 6/4/98 11:25:01 PM Eastern Daylight Time,\njoshco@microsoft.com writes:\n\n> hmm..\n>  Wouldnt a prior work be tunneling HTTP through SSL, as\n>  originated by Netscape ?\n>  This is how https works...\n\nProtocol tunnels have been used for at least twenty\nyears.  Hard to believe IBM's develop is non-obvious\nto someone skilled in the art or that it constitutes a\nnew combination of old ideas.\n\nJoachim Martillo\n\n\n\n", "id": "lists-012-3669868"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "Note that on the Mac, we have been running HTTP over AppleTalk since\n1995 with CL-HTTP.\n\nAt 12:20 PM -0600 98-06-04, Benjamin \\\"Snowhare\\\" Franz wrote:\n>Content-Type: TEXT/PLAIN; CHARSET=us-ascii\n>Content-ID: <Pine.GSO.3.96.980604121712.6544C@xmission.xmission.com>\n>\n>Sigh.\n>\n><URL:http://www.patents.ibm.com/details?patent_number=5754774>\n>\n>-- \n>Benjamin Franz\n\n\n\n", "id": "lists-012-3677931"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "Steve Lewontin and Mary Ellen Zurko, \"The DCE Web: Providing\nAuthorization and Other Distributed Services to the World\nWide Web\", WWW Conference 1994,\nhttp://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/Security/lewontin/Web_DCE_Conf_94.html\n\n\n\n", "id": "lists-012-3686546"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "On Fri, 5 Jun 1998, John C. Mallery wrote:\n\n> Note that on the Mac, we have been running HTTP over AppleTalk since\n> 1995 with CL-HTTP.\n\nIs that anything more than substituting AppleTalk for TCP/IP?  An ability\nwhich some members of this WG have long insisted on maintaining. I\nwouldn't consider that quite the same. I haven't managed to decode the\nIBM patent yet so I haven't a clue as to whether it represented something\nnew when it was filed in 1995.  Nor do I have any idea how broad the\nimpact would be. \n\nDave Morris\n\n\n\n", "id": "lists-012-3694709"}, {"subject": "RE: IBM patents tunneling HTTP through another protoca", "content": "> > hmm..\n> >  Wouldnt a prior work be tunneling HTTP through SSL, as\n> >  originated by Netscape ?\n> >  This is how https works...\n> \n> Protocol tunnels have been used for at least twenty\n> years.  Hard to believe IBM's develop is non-obvious\n> to someone skilled in the art or that it constitutes a\n> new combination of old ideas.\n\nBut the patent seems only to apply to:\n\"a method of increasing the performance\"\n\nProtocol tunnels for security/routing reasons wouldn't be\napplicable here.\n\nI guess tunnelling over SSL doesn't apply since there is\ncertainly no performance enhancement.\n\nTo prove a prior work you need to find an example of\ntunnelling over a protocol which dynamically compresses??\nDoes AppleTalk compress?\n\nObviously there are other ways to increase the performance\nother than compress - to use a different protocol than TCP/IP..\n\nAnyway, is it true to say, if any performance improvements\nare introduced *into* the HTTP standard, this patent doesn't\napply?\n\n- Sean\n\n\n\n", "id": "lists-012-3703765"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "> \n> > > hmm..\n> > >  Wouldnt a prior work be tunneling HTTP through SSL, as\n> > >  originated by Netscape ?\n> > >  This is how https works...\n> > \n> > Protocol tunnels have been used for at least twenty\n> > years.  Hard to believe IBM's develop is non-obvious\n> > to someone skilled in the art or that it constitutes a\n> > new combination of old ideas.\n> \n> But the patent seems only to apply to:\n> \"a method of increasing the performance\"\n> \n> Protocol tunnels for security/routing reasons wouldn't be\n> applicable here.\n> \n> I guess tunnelling over SSL doesn't apply since there is\n> certainly no performance enhancement.\n> \n> To prove a prior work you need to find an example of\n> tunnelling over a protocol which dynamically compresses??\n> Does AppleTalk compress?\n> \n> Obviously there are other ways to increase the performance\n> other than compress - to use a different protocol than TCP/IP..\n> \n> Anyway, is it true to say, if any performance improvements\n> are introduced *into* the HTTP standard, this patent doesn't\n> apply?\n> \n> - Sean\n\nSSH surely does compress the stream, if you ask it to do so. It is also\npossible to use compression with no encryption, which does enhance performance\nin some cases (slow lines, fast systems).\n\nI think one of the key issues here is whether there is prior work which\ntranslates HTTP on one end of the stream into something else (thereby removing\nthe HTTP protocol overhead), and recreates the HTTP request at the other end.\nSSH does not touch the protocol, it only transports the data stream (compressed\nand/or encrypted).\n\nCheers//Frank\n\n  WWWWW      ___________________________\n ## o o\\    /       Frank de Lange       \\    =================================\n }#   \\|   /      +31-70-3712708 day      \\   #   WARNING: Do not add these   #\n  ##---# _/      +31-320-252965 night      \\  # addresses to any mass mailing #\n   ####   \\frank.de.lange@inet.unisource.nl/  #  list without prior approval  #\n           \\  frank.de.lange@net.info.nl  /   #     of the address owner.     #\n            ------------------------------    =================================\n\n\n\n", "id": "lists-012-3713047"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "rom a quick look at the patent, I expect that previous prior work is \nwidespread; I think this may be an example of overeager patent applications \nvs. patent examiners that don't know enough computing history.  But I haven't\nread the patent carefully, so take this comment with a block of salt.\n\nExamples:\n\n1) The tunneling of the X Window System protocol through various compression \nservers, which know how to translate that protocol to a more compact\nrepresentation and back again.\n\nThis was done by a Stanford prof (name slips my mind, but I can regenerate\nit) years ago (late 80's is in my mind), and a product version appeared \nas the Serial Line X work that came out of NCD and the X Consortium well \nbefore the application date of the IBM patent.\n\nThe patent claims of \"Differencing, caching or protocol reduction techniques \nincrease performance over the external communication link. \" are certainly\npresent in this system, both differencing and caching.\n\n2) the tunneling of the IP protocol through chaosnet protocols, done\nwidely at MIT in the 1983 time frame.  Here TCP's behavior in the\nface of packet loss (particularly at the time) was mitigated somewhat\nby Chaos Net's more agressive retransmission strategy.\n\n3) Both SLIP and PPP do differencing and caching on the TCP/IP headers\nto increase performance; these have been around for many years.\n\nThe idea, as I understand it, has been around for a long time.  Examples\n1 and 3 above apply most closely, that occur to me without much thought,\nand long predate the patent application.\n- Jim Gettys\n\n\n\n", "id": "lists-012-3724786"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "In a message dated 6/5/98 11:43:31 AM Eastern Daylight Time,\nsean.mcdermott@fmr.com writes:\n\n> > > hmm..\n>  > >  Wouldnt a prior work be tunneling HTTP through SSL, as\n>  > >  originated by Netscape ?\n>  > >  This is how https works...\n \n>  > Protocol tunnels have been used for at least twenty\n>  > years.  Hard to believe IBM's develop is non-obvious\n>  > to someone skilled in the art or that it constitutes a\n>  > new combination of old ideas.\n  \n>  But the patent seems only to apply to:\n>  \"a method of increasing the performance\"\n  \n>  Protocol tunnels for security/routing reasons wouldn't be\n>  applicable here.\n  \n>  I guess tunnelling over SSL doesn't apply since there is\n>  certainly no performance enhancement.\n  \n>  To prove a prior work you need to find an example of\n>  tunnelling over a protocol which dynamically compresses??\n>  Does AppleTalk compress?\n  \n>  Obviously there are other ways to increase the performance\n>  other than compress - to use a different protocol than TCP/IP..\n\n>  Anyway, is it true to say, if any performance improvements\n>  are introduced *into* the HTTP standard, this patent doesn't\n>  apply?\n\nTunneling IPX over IP being compressed onto a WAN like\nsounds hardly different.  If I am not mistaken, Cisco brouters,\nACC brouters and <A HREF=\"http://members.aol.com/Telford001/\">TTT's own VLAN\nRouter</A> have such capabilities.\n\nJoachim Martillo\n\n\n\n", "id": "lists-012-3734890"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "On Fri, 5 Jun 1998, Jim Gettys wrote:\n\n> The idea, as I understand it, has been around for a long time.  Examples\n> 1 and 3 above apply most closely, that occur to me without much thought,\n> and long predate the patent application.\n\nBut isn't it the building and describing of a 'machine' which receives the\npatent and not the concept or idea? \n\nDave\n\n\n\n", "id": "lists-012-3744352"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "> From: \"David W. Morris\" <dwm@xpasc.com>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Fri, 5 Jun 1998 09:42:34 -0700 (PDT)\n> To: http working group <http-wg@cuckoo.hpl.hp.com>\n> Subject: Re: IBM patents tunneling HTTP through another protocal\n> -----\n> On Fri, 5 Jun 1998, Jim Gettys wrote:\n> \n> > The idea, as I understand it, has been around for a long time.  Examples\n> > 1 and 3 above apply most closely, that occur to me without much thought,\n> > and long predate the patent application.\n> \n> But isn't it the building and describing of a 'machine' which receives the\n> patent and not the concept or idea?\n> \nBoth the X compression and SLIP/PPP stuff have been instantiated into\n\"machines\" and used for years.  The idea has been around for even longer\nthan the implementations.....  \n- Jim\n\n\n\n", "id": "lists-012-3752546"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "In a message dated 6/5/98 11:53:41 AM Eastern Daylight Time,\nfrank@rintintin.gv-itf.unisource.nl writes:\n\n> SSH surely does compress the stream, if you ask it to do so. It is also\n>  possible to use compression with no encryption, which does enhance \n> performance\n>  in some cases (slow lines, fast systems).\n>  \n>  I think one of the key issues here is whether there is prior work which\n>  translates HTTP on one end of the stream into something else (thereby \n> removing the HTTP protocol overhead), and recreates the HTTP request at the\nother end.\n\n>  SSH does not touch the protocol, it only transports the data stream (\n> compressed and/or encrypted).\n\nThe VLAN Router in certain remote bridging applications has for years\nstripped off MAC headers, sent the data and recreated the MAC\nheaders on the remote side.  Telnet <-->VTAM and Telnet<-->PAD\ntranslation has been common for a decade.\n\nJoachim Martillo\n\n\n\n", "id": "lists-012-3762102"}, {"subject": "Re: Implications of introducing new scheme and port for exi", "content": ">     I would agree with Josh Cohen and David Morris that introducing a new\n>     URL scheme will affect proxies and is a bad idea. I also agree with\n>     David Morris that a new HTTP scheme should be avoided for the same\n>     reasons.\n>\n>     Using a new default port is okay. The stuff about firewalls being\n>     affected is true to the extent that they also had to be opened up for\n>     the default ports used for other protocols - It really isn't a big\n>     hassle reconfiguring them.\n>\n>     However, I side with David Morris in questioning the need distinguish\n>     between IPP and HTTP. Is there one?\n\nI'm not an expert in this area, but I'd speculate that the suggestion of\nintroducing a new URL scheme and a new port number, comes from\nconsiderations of firewalls and security. If one is implementing a policy\nof \"deny-everything-not-explicitly-allowed\", it might be regarded as a\n\"feature\" that IPP would not go thru existing firewalls or proxies until\nthey were reconfigured.\n\nI guess the reasonableness of this condition depends on the intended market\nfor IPP, and the default security policy it should be expected to have.\n\nI'd argue that a new URL scheme is a reasonable way to introduce a\ndifferent default port; but again the price is that a lot of existing web\nclients will give you dumb looks if handed a new URL scheme. But this may\nnot matter if HTTP is just seen as a transport for IPP.\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-012-3770957"}, {"subject": "RE: IPP&gt; RE: Implications of introducing new scheme and port for  existing  HTTP server", "content": "The issue is proxies - enablers - not firewalls - disablers. If you replace\nmy proxy by a passthrough cable I cannot do anything, if you replace my\nfirewall by a cable you can do anything. \n\n> -----Original Message-----\n> From:Randy Turner [SMTP:rturner@sharplabs.com]\n> Sent:Tuesday, June 02, 1998 8:34 AM\n> To:Vinod Valloppillil (Exchange); 'Rob Polansky'; David W. Morris\n> Cc:http-wg; ipp@pwg.org\n> Subject:Re: IPP> RE: Implications of introducing new scheme and port\n> for existing  HTTP servers\n> \n> \n> The past few comments about firewalls do not (IMHO) appear to pose a\n> problem for IPP deployment. If the majority of the installed base of\n> firewall products do not do HTTP method inspection then thats ok.\n> everything would work. When the \"next-generation\" products that can\n> perform\n> this type of inspection, then during installation of this new\n> infrastructure, the administrator will then enable IPP (or WEBDAV) or\n> whatever at that time.\n> \n> Ultimately, I believe firewall admins will explicitly enable internet\n> printing or faxing or whatever, and I don't think a firewall issue should\n> impose undue design constraints on what we (the WG) want to do.\n> Firewall admins already do this explicitly enabling/disabling of\n> application protocols (POP, FTP, IMAP, etc.) and I think we're just\n> another\n> application. I don't think these protocol designers were too bogged down\n> in\n> firewall issues during the development process. At least with the\n> Checkpoint Firewall-1 product, it takes about 45 seconds to bring up the\n> console and enable or disable a particular application protocol.\n> \n> Just my (possibly more than) $0.02\n> \n> Randy\n> \n> \n> \n> At 08:15 AM 6/2/98 -0700, Vinod Valloppillil (Exchange) wrote:\n> >Rob's argument is broadly correct -- as a long term firewall design\n> issue,\n> >method inspection (and occasionally payload inspection) will become the\n> >rule.\n> >\n> >However, as a small carrot to today's protocol designers, the vast\n> majority\n> >of the installed base of firewalls do no method / payload inspection on\n> HTTP\n> >data being passed through.   Purely from the perspective of 'reach'\n> there's\n> >no impediment to IPP using it's own method in the short run.\n> >\n> >> -----Original Message-----\n> >> From:Rob Polansky [SMTP:polansky@raptor.com]\n> >> Sent:Tuesday, June 02, 1998 6:06 AM\n> >> To:David W. Morris\n> >> Cc:http-wg; ipp@pwg.org\n> >> Subject:RE: Implications of introducing new scheme and port for\n> >> existing  HTTP servers\n> >> \n> >> I know of at least one :-) firewall that not only rejects unknown\n> methods\n> >> but also examines the HTTP request method as part of its \"algorithm\".\n> From\n> >> a\n> >> protocol and security perspective, it appears to be the right thing to\n> do.\n> >> If you don't understand the method, how can you properly proxy it? Take\n> >> the\n> >> CONNECT method as an example.\n> >> \n> >> In summary, any proxy that is more than a simple packet passer\n> (supports\n> >> CONNECT, protocol conversion, proxy authentication, etc.) runs the risk\n> of\n> >> failing to pass IPP if it uses a new scheme and/or a new method. Not\n> that\n> >> that's a bad thing... :-)\n> >> \n> >> -Rob Polansky\n> >> \n> >> > -----Original Message-----\n> >> > From: David W. Morris [mailto:dwm@xpasc.com]\n> >> > Sent: Monday, June 01, 1998 10:34 PM\n> >> > To: Carl-Uno Manros\n> >> > Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org; http-wg@hplb.hpl.hp.com\n> >> > Subject: Re: Implications of introducing new scheme and port for\n> >> > existing HTTP servers\n> >> >\n> >> > (I'm also not wild about new HTTP methods as I know of existing\n> proxies\n> >> > which will reject unknown methods. Don't know of any which will\n> accept\n> >> > unknown methods. I'm also unaware of any firewall software which\n> >> examines\n> >> > the HTTP request method as part of its algorithm but then I'm not a\n> >> > firewall expert.)\n> >> >\n> > \n\n\n\n", "id": "lists-012-3779890"}, {"subject": "RE: IPP&gt; RE: Implications of introducing new scheme and port for  existing  HTTP server", "content": "I just want to be clear on what I am trying to say:- \n\nFirewalls are a completely legitimate and valuable tool in the Internet and\nI think that issues associtaed with IPP and its penetration or otherwise\nthrough them are extremely important. Having said that I think that the\nproxy issue is the more important one because:-\n\na) This is how most businesses users access the Internet \nb) They invert the firewall model (a firewall blocks, a proxy enables)\n\nIf we use a mechanism not carried by the current proxy installed base then\nthe majority of business users will not be able to use IPP to talk to\n'printers' on the Internet. This would be a big change. I do not place a\nvalue judement on this - I merely point out the enormity of the change that\nwould occur.\n\nI should point out that at one of the first IPP meeting attended by MS we\npointed out that using HTTP primarily as a means of 'stealthing' through\nfirewalls was totally bogus and this continues to be our position. The PWG\ncame to an uneasy agreement that the 'firewall issue' was no longer open for\ndebate since the group divided in two on it and would not be reconciled.\n\n> -----Original Message-----\n> From:Rob Polansky [SMTP:polansky@raptor.com]\n> Sent:Wednesday, June 03, 1998 5:55 AM\n> To:Paul Moore\n> Cc:http-wg; ipp@pwg.org\n> Subject:RE: IPP> RE: Implications of introducing new scheme and port\n> for existing  HTTP servers\n> \n> This kind of flamebait is not really helpful to our discussion. Firewalls\n> serve legitimate technical and business needs as our friends from\n> Microsoft\n> know, and those firewalls with application proxies look at protocols from\n> a\n> different point of view than your typical caching proxies. The beauty of\n> it\n> is that protocol compliant implementations from either the firewall or the\n> cache point of view will interoperate. The difference is when they\n> encounter\n> something unexpected. Firewalls by definition must \"fail closed\" so as not\n> to make their protected resources vulnerable to attacks; most other\n> software\n> makes a best effort to pass data. I don't see anything wrong with that\n> difference.\n> \n> Once again, if IPP uses existing methods and schemes, it should be\n> passable\n> through all proxies without trouble. Add a new method and/or scheme i.e.\n> CHANGE THE STANDARD, and you should expect that existing implemenations\n> will\n> not understand it and some (not many) may not pass it.\n> \n> -Rob Polansky\n> \n> > -----Original Message-----\n> > From: Paul Moore [mailto:paulmo@microsoft.com]\n> > Sent: Tuesday, June 02, 1998 8:37 PM\n> > To: 'Randy Turner'; Vinod Valloppillil (Exchange); 'Rob Polansky'; David\n> > W. Morris\n> > Cc: http-wg; ipp@pwg.org\n> > Subject: RE: IPP> RE: Implications of introducing new scheme and port\n> > for existing HTTP servers\n> >\n> >\n> > The issue is proxies - enablers - not firewalls - disablers. If\n> > you replace\n> > my proxy by a passthrough cable I cannot do anything, if you replace my\n> > firewall by a cable you can do anything.\n> >\n> > > -----Original Message-----\n> > > From:Randy Turner [SMTP:rturner@sharplabs.com]\n> > > Sent:Tuesday, June 02, 1998 8:34 AM\n> > > To:Vinod Valloppillil (Exchange); 'Rob Polansky'; David W.\n> Morris\n> > > Cc:http-wg; ipp@pwg.org\n> > > Subject:Re: IPP> RE: Implications of introducing new scheme and port\n> > > for existing  HTTP servers\n> > >\n> > >\n> > > The past few comments about firewalls do not (IMHO) appear to pose a\n> > > problem for IPP deployment. If the majority of the installed base of\n> > > firewall products do not do HTTP method inspection then thats ok.\n> > > everything would work. When the \"next-generation\" products that can\n> > > perform\n> > > this type of inspection, then during installation of this new\n> > > infrastructure, the administrator will then enable IPP (or WEBDAV) or\n> > > whatever at that time.\n> > >\n> > > Ultimately, I believe firewall admins will explicitly enable internet\n> > > printing or faxing or whatever, and I don't think a firewall\n> > issue should\n> > > impose undue design constraints on what we (the WG) want to do.\n> > > Firewall admins already do this explicitly enabling/disabling of\n> > > application protocols (POP, FTP, IMAP, etc.) and I think we're just\n> > > another\n> > > application. I don't think these protocol designers were too bogged\n> down\n> > > in\n> > > firewall issues during the development process. At least with the\n> > > Checkpoint Firewall-1 product, it takes about 45 seconds to bring up\n> the\n> > > console and enable or disable a particular application protocol.\n> > >\n> > > Just my (possibly more than) $0.02\n> > >\n> > > Randy\n> > >\n> > >\n> > >\n> > > At 08:15 AM 6/2/98 -0700, Vinod Valloppillil (Exchange) wrote:\n> > > >Rob's argument is broadly correct -- as a long term firewall design\n> > > issue,\n> > > >method inspection (and occasionally payload inspection) will become\n> the\n> > > >rule.\n> > > >\n> > > >However, as a small carrot to today's protocol designers, the vast\n> > > majority\n> > > >of the installed base of firewalls do no method / payload inspection\n> on\n> > > HTTP\n> > > >data being passed through.   Purely from the perspective of 'reach'\n> > > there's\n> > > >no impediment to IPP using it's own method in the short run.\n> > > >\n> > > >> -----Original Message-----\n> > > >> From:Rob Polansky [SMTP:polansky@raptor.com]\n> > > >> Sent:Tuesday, June 02, 1998 6:06 AM\n> > > >> To:David W. Morris\n> > > >> Cc:http-wg; ipp@pwg.org\n> > > >> Subject:RE: Implications of introducing new scheme and port\n> for\n> > > >> existing  HTTP servers\n> > > >>\n> > > >> I know of at least one :-) firewall that not only rejects unknown\n> > > methods\n> > > >> but also examines the HTTP request method as part of its\n> \"algorithm\".\n> > > From\n> > > >> a\n> > > >> protocol and security perspective, it appears to be the\n> > right thing to\n> > > do.\n> > > >> If you don't understand the method, how can you properly\n> > proxy it? Take\n> > > >> the\n> > > >> CONNECT method as an example.\n> > > >>\n> > > >> In summary, any proxy that is more than a simple packet passer\n> > > (supports\n> > > >> CONNECT, protocol conversion, proxy authentication, etc.)\n> > runs the risk\n> > > of\n> > > >> failing to pass IPP if it uses a new scheme and/or a new method.\n> Not\n> > > that\n> > > >> that's a bad thing... :-)\n> > > >>\n> > > >> -Rob Polansky\n> > > >>\n> > > >> > -----Original Message-----\n> > > >> > From: David W. Morris [mailto:dwm@xpasc.com]\n> > > >> > Sent: Monday, June 01, 1998 10:34 PM\n> > > >> > To: Carl-Uno Manros\n> > > >> > Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org;\n> http-wg@hplb.hpl.hp.com\n> > > >> > Subject: Re: Implications of introducing new scheme and port for\n> > > >> > existing HTTP servers\n> > > >> >\n> > > >> > (I'm also not wild about new HTTP methods as I know of existing\n> > > proxies\n> > > >> > which will reject unknown methods. Don't know of any which will\n> > > accept\n> > > >> > unknown methods. I'm also unaware of any firewall software which\n> > > >> examines\n> > > >> > the HTTP request method as part of its algorithm but then I'm not\n> a\n> > > >> > firewall expert.)\n> > > >> >\n> > > >\n> >\n\n\n\n", "id": "lists-012-3797786"}, {"subject": "MOD  What is a Firewall", "content": "I have been trying to figure out how we can get the discussion about how to\ndistinguish IPP in firewalls a little more structured and not talk past each\nother. Let me try to sketch up a simple model of how I think firewalls work,\nand where the different proposals come in.\n\nNOTE, that there is no standard what-so-ever for firewalls, so whatever\nmodel you come up with will not fit every firewall implementation. If there\nwas a firewall standard in the IETF, we would not have this discussion.\n\nI think a common feature of all firewalls is that they have a hierachy,\nwhich sometimes is shallow and sometimes is deep. Here is my try at\ndescribing the more important \"layers\".\n\n1) Host address    TCP/IP address\n2) Port number     Default 80 for HTTP\n3) Protocol        \"http\" for HTTP\n4) Method          POST etc. for HTTP\n5) Content         HTML etc.\n\nFiltering in the firewall can be done on any of these layers. Usually the\nfirewall only let things through that it can identify and refuses the rest.\n\nKeith Moore suggests that we need to change both layer 2) and 3) above to\ngive the firewall a chance to distinguish IPP from HTPP traffic.\n\nMS experts and a couple of others have suggested that the filtering takes\nplace on layer 4), by allocating a new PRINT method for IPP and we do not\nneed to touch layers 2) and 3).\n\nIn discussions that I had with firewall experts last year, they indicated\nthat they had no problem to filter on layer 5), e.g. distinguishing IPP from\nHTML etc. by identifying the content as an \"application/ipp\" MIME type.\n\nSo what it all boils down to is how versatile the firewall implementation is.\n\nTo make a concious decision about filtering in/out IPP from other HTTP\ntraffic, any current firewall will need to be reconfigured or modified in\nsame way.\n\nLooking at my hierachy, I suggest that if firewall do all levels, there is\nNO need to modify anything in the current IPP specs. If we move up (or down)\nto level 4), then we should go along with the MS approach, etc.\n\nMy 2 cents,\n\nCarl-Uno\n\n\n\n", "id": "lists-012-3819203"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "There is rather more direct prior art here than that being alluded to-\nmore specifically the HTTP over SCP work performed by myself and Andy\nNorman of Hewlett Packard in which was presented a long time before the\nfiling date (even at W3 Boston on two stages simultaneously :-)\n\nTO make matters worse, the claimed inventors are from IBM RTP. I'm going\nto try and find out what they're actually claiming.\n\nSimon\n\nNow available - The Freddy Hayek Kayak            | \"Pass me another elf\nPaddle Your Own Canoe! Be Rowed To Surfdom!       | Sergeant- this one's\nFrom The Taco Institute for Dyslexic Libertarians | split\"\n  Moments ago I had everything. Now, there's a cow in my nose - La Salla\n\n\n\n", "id": "lists-012-3828444"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "Further- I just called up and spoke to Barron Housel,one of the inventors\nlisted on the patent, and he agreed that the HTTP over SCP work is prior\nart; he's going to try and get the issue resolved and get some\nclarification posted to the working group mailing list\n\nSimon\n----\nNow available - The Freddy Hayek Kayak            | \"Pass me another elf\nPaddle Your Own Canoe! Be Rowed To Surfdom!       | Sergeant- this one's\nFrom The Taco Institute for Dyslexic Libertarians | split\"\n  Moments ago I had everything. Now, there's a cow in my nose - La Salla\n\n\n\n", "id": "lists-012-3837908"}, {"subject": "Re: IBM patents tunneling HTTP through another protoca", "content": "I won't attempt to inject an opinion into the debate over whether there\nis \"prior art\" that might invalidate this patent (#5,754,774).  (I will\npoint out that people who know nothing about patent law are often\nsurprised by what is or is not a valid patent.)\n\nBut because it's usually pretty difficult to figure out what a patent\nis really describing (especially one with 129 claims!), I thought it\nmight help to point out that several of the authors of this patent\nhave already published a readable and fairly complete paper:\n\nBarron C. Housel and David B. Lindquist\n\"WebExpress: A System for Optimizing Web\nBrowsing in a Wireless Environment\"\nProc. 2nd Annual Intl. Conf. on Mobile Computing and\nNetworking\nACM, New York, November 1996, 108-116\n\n(from which one can infer that they probably came up with\nthe invention somewhat before the filing date on the patent).\n\nAs far as I know, this is the first publication to describe\nthe idea of \"delta encoding\" for HTTP (although I don't think\nthey used this term).\n\nThere have been several related documents published by others,\nincluding myself:\n\nGaurav Banga, Fred Douglis, and Michael Rabinovich\n\"Optimistic Deltas for WWW Latency Reduction\"\nProc. 1997 USENIX Technical Conference\nAnaheim, CA, January 1997, pages 289-303\n\nArthur van Hoff, John Giannandrea, Mark Hapner, Steve Carter,\nand Milo Medin.  The HTTP Distribution and Replication\nProtocol.  Technical Report NOTE-DRP, World Wide Web\nConsortium, August, 1997.\nhttp://www.w3.org/TR/NOTE-drp-19970825.html.\n\nJeffrey C. Mogul, Fred Douglis, Anja Feldmann,\n and Balachander Krishnamurthy.\n\"Potential benefits of delta encoding and data compression\nfor HTTP\"\nProc. SIGCOMM '97 Conference\nCannes, France, September, 1997, pages 181-194\nExpanded version:\n http://www.research.digital.com/wrl/techreports/abstracts/97.4.html\n\nFred Douglis, Antonio Haro, and Michael Rabinovich.  HPP: HTML\nMacro-Preprocessing to Support Dynamic Document Caching.\nProc.  USENIX Symposium on Internet Technologies and Systems,\nUSENIX, Monterey, CA, December, 1997, pp. 83-94.\n\nJ. Mogul, Y. Goland, A. van Hoff, F. Douglis, A. Feldmann,\nB. Krishnamurthy\n\"Delta encoding in HTTP\",\ndraft-mogul-http-delta-00.txt\n13 Jan 1998\n\n\nOur SIGCOMM '97 paper has some discussion of the related work in\nthis area.\n\n-Jeff\n\n\n\n", "id": "lists-012-3846968"}, {"subject": "RE: IBM patents tunneling HTTP through another protoca", "content": "> On Fri, 5 Jun 1998, Jim Gettys wrote:\n>\n> > The idea, as I understand it, has been around for a long time.  Examples\n> > 1 and 3 above apply most closely, that occur to me without much thought,\n> > and long predate the patent application.\n>\n> But isn't it the building and describing of a 'machine' which receives the\n> patent and not the concept or idea?\n\nIn theory yes, in practice no.\n\nPeople inside the PTO tell me that since getting sued ten years ago they\nhave pretty much had a policy of eventually permitting any patent that\nisn't a perpetual motion machine or an anti-gravity device.\n\nThey have realised that the more patents they grant the more inventive\nthe US congress believes the US people to be and hence the better satisfied\nthey are with the patent system as the cause of all this 'inventiveness'.\nAlso the more patents that are filed the more need other folk have for\npatent collateral to be used in defence.\n\nA guy recently got a 'patent' covering the PEM hierarchical trust system.\nThe examiner obviously didn't read (or understand) the documents referenced.\n\nFolks may be interested to look at a current patent application by some\nfolks from OpenMarket. There is an international application which cites a\nUS patent application. By my reading these folks are claiming to have\ninvented Kerberos, Lotus Notes and the idea of a proxy gateway.\n\n\nNeedless to say I'm not impressed by such games. They cost real money\nin legal fees, regardless of how vexatious the patent is. The best way\nto develop a partner relationship is not to file a ridiculous patent\nby a long chalk.\n\n\nYou don't have to be a patent attorney to realize that there is a lot\nof serious malpractice going on.\n\nPhill\n\n\n\n", "id": "lists-012-3856598"}, {"subject": "RE: IBM patents tunneling HTTP through another protoca", "content": "another example..\nI think the product was eventually bought up\nby cisco, but it was a tunneling software\npackage which let HTTP (And maybe tcp/ip in general)\nrun over Novell IPX/SPX.  It was inteded for networks\nand desktops which are still running Novell networking\nstacks instead of TCP/IP.\n\nAnother which has been mentioned is the tunneling\nof TCP type stuff over VTAM and SNA networks.\nA related (but inverted) protocol is from IBM\nas well, DLS and DLSw, which tunnel the \nSNA family (LU62, APPC/APPN) over IP router\nnetworks.\nThis also improves performance by providing\na routed architecture instead of bridged\nWAN architectures previously available.\n\njosh\n----\nJosh Cohen <josh@microsoft.com>\nProgram Manager - Internet Protocols\n \n\n> -----Original Message-----\n> From: Phillip Hallam-Baker [mailto:hallam@ai.mit.edu]\n> Sent: Friday, June 05, 1998 1:36 PM\n> To: David W. Morris; http working group\n> Subject: RE: IBM patents tunneling HTTP through another protocal\n> \n> \n> \n> \n> > On Fri, 5 Jun 1998, Jim Gettys wrote:\n> >\n> > > The idea, as I understand it, has been around for a long \n> time.  Examples\n> > > 1 and 3 above apply most closely, that occur to me \n> without much thought,\n> > > and long predate the patent application.\n> >\n> > But isn't it the building and describing of a 'machine' \n> which receives the\n> > patent and not the concept or idea?\n> \n> In theory yes, in practice no.\n> \n> People inside the PTO tell me that since getting sued ten \n> years ago they\n> have pretty much had a policy of eventually permitting any patent that\n> isn't a perpetual motion machine or an anti-gravity device.\n> \n> They have realised that the more patents they grant the more inventive\n> the US congress believes the US people to be and hence the \n> better satisfied\n> they are with the patent system as the cause of all this \n> 'inventiveness'.\n> Also the more patents that are filed the more need other folk have for\n> patent collateral to be used in defence.\n> \n> A guy recently got a 'patent' covering the PEM hierarchical \n> trust system.\n> The examiner obviously didn't read (or understand) the \n> documents referenced.\n> \n> Folks may be interested to look at a current patent \n> application by some\n> folks from OpenMarket. There is an international application \n> which cites a\n> US patent application. By my reading these folks are claiming to have\n> invented Kerberos, Lotus Notes and the idea of a proxy gateway.\n> \n> \n> Needless to say I'm not impressed by such games. They cost real money\n> in legal fees, regardless of how vexatious the patent is. The best way\n> to develop a partner relationship is not to file a ridiculous patent\n> by a long chalk.\n> \n> \n> You don't have to be a patent attorney to realize that there is a lot\n> of serious malpractice going on.\n> \n> Phill\n> \n> \n> \n> \n\n\n\n", "id": "lists-012-3865899"}, {"subject": "RE: MOD  What is a Firewall", "content": "> -----Original Message-----\n> From: Carl-Uno Manros [mailto:carl@manros.com]\n> Sent: Friday, June 05, 1998 11:51 AM\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: MOD - What is a Firewall?\n> \n> \n> 1) Host address    TCP/IP address\n> 2) Port number     Default 80 for HTTP\n> 3) Protocol        \"http\" for HTTP\n> 4) Method          POST etc. for HTTP\n> 5) Content         HTML etc.\n> \nLets add a level, so its:\n\n 1) Host address    TCP/IP address\n 2) Port number     Default 80 for HTTP\n 3) Protocol        \"http\" for HTTP\n 4) Method          POST etc. for HTTP\n 5) Content-type       text/HTML etc.\n 6) content body filtering (Firewall/proxy attempts to parse the IPP body)\n\nI wasnt sure if you meant for 5 to be my 5 or 6.\nIts much easier to filter by the http header content-type: than\nto parse the body and try to filter that way, although both can\ntechnically be done.\n\nSome proxies can filter the body content, it can, for example,\nstrip unwanted HTML tags like embedded scripts or Java references.\nThough it is possible in these products, the task of parsing\nthe bodies is such a performance hit, virtually no one uses it\nand proxy implementors tend to stick to the guideline that proxies\ndo not parse the entity-body in HTTP.\n(At least the implementors I worked with)\n\n\n\n", "id": "lists-012-3877706"}, {"subject": "Patents, IETF, and HTT", "content": "RFC 2028 lays out the ground rules for participation in IETF working groups.\nIt might be good to review those guidelines:\n\n#   To ensure a fair and open process, participants in the IETF and its\n#   Working Groups must be able to disclose, and must disclose to the\n#   Working Group chairs any relevant current or pending intellectual\n#   property rights that are reasonably and personally known to the\n#   participant if they participate in discussions about a specific\n#   technology.\n\nIf you've participated in discussions about HTTP and reasonablly and personally\nknow about current or pending intellectual property right claims (patents)\nthat are relevant to the HTTP/1.1 spec, please let me know.\n\nI think before we have a lengthy discussion about prior art, we need to\njudge whether the patent is relevant, e.g., would an implementation of\nthe proposed or draft standard require a license of the patent in order to\nimplement the standard. And even if we might reasonably believe the patent \nisn't \"valid\" based on prior art, I'm not sure the working group mailing\nlist is the right place to discuss it, and certainly not the general issues\nof patent law and its application to software.\n\nThis isn't our primary focus, which is to get the documentation of independent\ninteroperable implementations out. I hope we can close this off and finish up\nany remaining issues prior to the next IETF meeting.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-3886693"}, {"subject": "Connection token", "content": "There is no clear definition of the connection tokens supported by the \nConnection general-header field. Section 14.10 remains vague:\n\n    connection-token = token\n\nand gives \"close\" as an example of token, a few lines further.\n\nRFC 2068 refers to the following connection tokens:\n\n  close: pp. 44, 45, 109 and 161\n  Keep-Alive: p. 161\n  Persist: p. 161\n\n\"Keep-Alive\" and \"Persist\" are described in RFC 2068 section 19.7.1 as the \"HTTP/1.0 form of persistent connections\". But they are specified in neither RFC 1945 nor RFC 2068. In Rev-03, references to\" Keep-Alive\" and \"Persist\" have been deleted from section 19.6.2.\n\nI interpret this as meaning that the sole connection-token supported in HTTP/1.1 is \"close\", and that the experimental tokens \"Keep-Alive\" and \"Persist\" have been deprecated. This should be stated clearly in the spec. I suggest 2 ways of doing this:\n\n\n1) Rephrase section 14.10 as follows:\n\n  14.10 Connection\n\n  The Connection general-header field allows the sender to specify options\n  that are desired for that particular connection and MUST NOT be\n  communicated by proxies over further connections.\n\n  The Connection header has the following grammar:\n\n         Connection = \"Connection\" \":\" 1#(connection-token)\n         connection-token  = token\n\n  HTTP/1.1 proxies MUST parse the Connection header field before a message\n  is forwarded and, for each connection-token in this field, remove any\n  header field(s) from the message with the same name as the connection-\n  token. Connection options are signaled by the presence of a connection-\n  token in the Connection header field, not by any corresponding\n  additional header field(s), since the additional header field may not be\n  sent if there are no parameters associated with that connection option.\n\n  Message headers listed in the Connection header MUST NOT include end-to-\n  end headers, such as Cache-Control.\n\n  The only connection-token defined by HTTP/1.1 is \"close\". Tokens\n  \"Keep-Alive\" and \"Persist\", which were used in some HTTP/1.0\n  experimental implementations of persistent connections, have been\n  deprecated. The \"close\" token allows the sender to signal that the\n  connection will be closed after completion of the response. For example,\n\n         Connection: close\n\n  in either the request or the response header fields indicates that the\n  connection should not be considered `persistent' (section 8.1) after the\n  current request/response is complete.\n\n  HTTP/1.1 applications that do not support persistent connections MUST\n  include the \"close\" connection option in every message.\n\n  A system receiving an HTTP/1.0 (or lower-version) message that includes\n  a Connection header MUST, for each connection-token in this field,\n  remove and ignore any header field(s) from the message with the same\n  name as the connection-token. This protects against mistaken forwarding\n  of such header fields by pre-HTTP/1.1 proxies.\n\n\n2) Add a new section between sections 3.8 and 3.9:\n\n    Connection Tokens\n\n    Connection tokens are used to control the persistence of connections.\n    The only connection-token defined by HTTP/1.1 is \"close\" (sections\n    8.1 and 14.10).\n    \n    connection-token = \"close\"\n\n    Tokens \"Keep-Alive\" and \"Persist\", which were used in some HTTP/1.0\n    experimental implementations of persistent connections, have been\n    deprecated.\n\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-3894562"}, {"subject": "Multiple or unsolicited responses are forbidde", "content": "I was wondering whether persistent HTTP/1.1 connections could be used as \nbi-directional sockets, where both the client and the server can send data \nasynchronously to the other side. In other words:\n\n* Can an HTTP server send data asynchronously to an HTTP client (response \nwithout request)?\n\n* Can an HTTP server send multiple responses to an HTTP client, without \nthe client knowing in advance how many responses it will receive, or how \nlong it will take for the server to send all its responses?\n\nThis would have allowed all socket-based client/server applications to \ntunnel traffic over HTTP, and therefore go across a firewall easily.\n\nImplicitly, section 1.4 answers no to all these questions. I think this \nshould be stated explicitly (HTTP is clearly not primarily geared towards \nthat, but more and more people are using HTTP outside the Web context).\n\nHere is the first paragraph of section 1.4:\n\n  The HTTP protocol is a request/response protocol. A client sends a\n  request to the server in the form of a request method, URI, and protocol\n  version, followed by a MIME-like message containing request modifiers,\n  client information, and possible body content over a connection with a\n  server. The server responds with a status line, including the message's\n  protocol version and a success or error code, followed by a MIME-like\n  message containing server information, entity metainformation, and\n  possible entity-body content. The relationship between HTTP and MIME is\n  described in appendix 19.4.\n\nI suggest to add the following right after:\n\n  The server must not send more than one response message to a given\n  request message (that is, multiple responses are forbidden). The server\n  must not send unsolicited messages to a client: all messages sent by\n  the server are responses to requests previously issued by this client.\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-3905059"}, {"subject": "RE: Multiple or unsolicited responses are forbidde", "content": "> I was wondering whether persistent HTTP/1.1 connections could be used as \n> bi-directional sockets, where both the client and the server can send data \n> asynchronously to the other side.\n\nThere are so MANY things that you COULD do that aren't spelled\nout as explicitly prohibited, although they are not clearly\npart of the protocol. Why don't we just add a general \"everything\nthat is not explicitly allowed is prohibited\" and be done with it?\n\n\n\n", "id": "lists-012-3914394"}, {"subject": "RE: Multiple or unsolicited responses are forbidde", "content": "> -----Original Message-----\n> From: J.P. Martin-Flatin [mailto:martin-flatin@epfl.ch]\n> Sent: Sunday, June 14, 1998 11:10 AM\n> To: http-wg@cuckoo.hpl.hp.com\n> Cc: martin-flatin@epfl.ch\n> Subject: Multiple or unsolicited responses are forbidden\n> \n> \n> * Can an HTTP server send multiple responses to an HTTP \n> client, without \n> the client knowing in advance how many responses it will \n> receive, or how \n> long it will take for the server to send all its responses?\n> \nYes, this is possible.  the 100 continue response can be sent\nrepeatedly in addition to the 'final' response code.\nWhile its possible, its intended use appears to be \nto provide an intermediary status response on long or\nexpensive operations..\n\n\n\n", "id": "lists-012-3922531"}, {"subject": "Minor typos in Rev0", "content": "1) Section 3.2.1, replace \"[42] [42]\" with \"[42]\".\n\n2) Section 8.1.1, replace:\n\n  Analysis of these performance\n  problems are available [30]; analysis and results from a prototype\n  implementation are in [26].\n\nwith:\n\n  Analyses of these performance problems and results from a prototype\n  are available [26] [30].\n\n3) Section 8.1.1, 4th bullet, replace \";\" with \",\" before \"since\".\n\n4) Section 8.2.4, 2nd bullet of \"Requirements for HTTP/1.1 origin \nservers\", replace:\n\n    .  An origin server MAY omit a 100 (Continue) response if has already\n       received some or all of the request body for the corresponding\n       request.\n\nwith:\n\n    .  An origin server MAY omit a 100 (Continue) response if it has\n       already received some or all of the request body for the\n       corresponding request.\n\n5) Section 14.20, skip a line between the definition of expect-params and \nthe sentence \"The server MUST respond with a 417...\".\n\n6) Section 14.20, replace:\n\n  Comparison of expectation values is case-insensitive unquoted tokens\n  (including the 100-continue token), and is case-sensitive for quoted-\n  string expectation-extensions.\n\nwith:\n\n  Comparison of expectation values is case-insensitive for unquoted\n  tokens (including the 100-continue token), and is case-sensitive for\n  quoted-string expectation-extensions.\n\n7) Section 19.6.3.1, remove white space at the beginning of items 6 and 7.\n\n8) In the index, alignment problems with entries \"byte-range-resp-spec\", \n\"bytes-unit\", \"Content-Encoding\", \"Expect\", \"Content-Language\", \n\"entity-body\", \"IANA\", \"Methods\", \"qdtext\", and \"RFC 1738\".\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-3932178"}, {"subject": "100 Continu", "content": "There is a contradiction in section 8.2.4, in \"Requirements for HTTP/1.1 \norigin servers\", between the 2nd bullet:\n\n    .  An origin server SHOULD NOT send a 100 (Continue) response if the\n       request message does not include an Expect request-header field\n       with the \"100-continue\" expectation, and MUST NOT send a 100\n       (Continue) response if such a request comes from an HTTP/1.0 (or\n       earlier) client.\n\nand 4 paragraphs further:\n\n  For compatibility with RFC 2068, a server MAY send a 100 (Continue)\n  status in response to an HTTP/1.1 PUT or POST request that does not\n  include an Expect request-header field with the \"100-continue\"\n  expectation. This exception, the purpose of which is to minimize any\n  client processing delays associated with an undeclared wait for 100\n  (Continue) status, applies only to HTTP/1.1 requests, and not to\n  requests with any other HTTP-version value.\n\nTo correct this, I propose to append the last paragraph to the 2nd bullet, \nlike this:\n\n    .  An origin server SHOULD NOT send a 100 (Continue) response if the\n       request message does not include an Expect request-header field\n       with the \"100-continue\" expectation, and MUST NOT send a 100\n       (Continue) response if such a request comes from an HTTP/1.0 (or\n       earlier) client. There is an exception to this rule, though: for\n       compatibility with RFC 2068, a server MAY send a 100 (Continue)\n       status in response to an HTTP/1.1 PUT or POST request that does not\n       include an Expect request-header field with the \"100-continue\"\n       expectation. This exception, the purpose of which is to minimize any\n       client processing delays associated with an undeclared wait for 100\n       (Continue) status, applies only to HTTP/1.1 requests, and not to\n       requests with any other HTTP-version value.\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-3940927"}, {"subject": "Advantages of persistent connection", "content": "Regarding the advantages of persistent connections listed in section \n8.1.1, under heading \"Persistent HTTP connections have a number of \nadvantages\", one is missing (latency), and one is not always true (memory \nsavings). I therefore suggest 2 changes.\n\n1) Add a bullet between current 3rd and 4th bullet:\n\n    .  Latency is reduced because network congestion is reduced, so\n       less retransmissions are needed, so the elapsed transmission\n       time is shorter.\n\n2) Replace:\n\n    .  By opening and closing fewer TCP connections, CPU time is saved,\n       and memory used for TCP protocol control blocks is also saved.\n\nwith:\n\n    .  By opening and closing fewer TCP connections, CPU time is saved\n       in routers and hosts (clients, servers, proxies, gateways, tunnels,\n       or caches), and memory used for TCP protocol control blocks can\n       be saved in hosts.\n\n\nRegarding memory savings, the original statement is not true in its \ngenerality. The number of TCP control blocks using up memory at any point \nin time depends on many parameters, among which we find:\n\n* timeout value of persistent connections\n* average number of simultaneous users\n* activity of users:\n    - whether they typically retrieve lots of objects (pages, inline\n      images) and then become quiet\n    - whether they typically download a few objects every N minutes\n      with N inferior to the TCP connection timeout value\n    - whether they typically download a few objects every N minutes\n      with N superior to the TCP connection timeout value\n    - whether they typically poll a server (e.g., to get the latest\n      results of a sporting event)\n* etc.\n\nBy varying these values, one can easily show that the number of TCP control blocks can be higher with persistent connections in some cases, smaller in others. Persistent connections often allow to save memory, but not always.\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-3949220"}, {"subject": "Drawbacks of persistent connection", "content": "Section 8.1.1 may come across as slightly biaised, because it lists only \nadvantages of persistent connections. In practice, these are balanced by \ndrawbacks. For instance, if the timeout value of persistent connections is \nlarger than the TCP connection timeout, denial-of-service attacks are more \neffective: by using up all possible connections, a malicious user can \nprevent access to a targeted server for a longer period of time. Perhaps a \nquick mention of this issue would make sense in section 8.1.4 (Practical \nConsiderations)?\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-3958393"}, {"subject": "RE: Advantages of persistent connection", "content": "> -----Original Message-----\n> From: J.P. Martin-Flatin [mailto:martin-flatin@epfl.ch]\n> Sent: Sunday, June 14, 1998 4:42 PM\n> To: http-wg@cuckoo.hpl.hp.com\n> Cc: martin-flatin@epfl.ch\n> Subject: Advantages of persistent connections\n> \n> \n> Regarding the advantages of persistent connections listed in section \n> 8.1.1, under heading \"Persistent HTTP connections have a number of \n> advantages\", one is missing (latency), and one is not always \n> true (memory \n> savings). I therefore suggest 2 changes.\n> \nWhile it may not be explicitly stated, I think the real latency improvement\nis due to #2.  Indirect network health improvement by use of persistent\nconnections will lead to many things like reduced congestion and therefore\nless latency, but from the perspective of the user of a browser, for\nexample,\nit is the avoidance of the re-establishment of connections which contributes\nto the reduction of latency.\n\nSo, from the server side, the big win is the CPU, memory and system call\nreduction\nwhich allows it to scale better.  From the client side, it is the latency\nreduction\nby not having to do a 3way handshake for every request which makes it a\nfaster\nuser experience.\n\nIf we are to specifically highlight latency reduction in the draft, I think\nit should be described in terms of the connection re-use and 3way handshake \navoidance, not the general network health example.\n\n> 1) Add a bullet between current 3rd and 4th bullet:\n> \n>     .  Latency is reduced because network congestion is reduced, so\n>        less retransmissions are needed, so the elapsed transmission\n>        time is shorter.\n> \n> 2) Replace:\n> \n>     .  By opening and closing fewer TCP connections, CPU time \n> is saved,\n>        and memory used for TCP protocol control blocks is also saved.\n> \n> with:\n> \n>     .  By opening and closing fewer TCP connections, CPU time is saved\n>        in routers and hosts (clients, servers, proxies, \n> gateways, tunnels,\n>        or caches), and memory used for TCP protocol control blocks can\n>        be saved in hosts.\n> \n> \n\n\n\n", "id": "lists-012-3966077"}, {"subject": "Re: Advantages of persistent connection", "content": "In a message dated 98-06-14 19:43:03 EDT, martin-flatin@epfl.ch writes:\n\n> Regarding the advantages of persistent connections listed in section \n>  8.1.1, under heading \"Persistent HTTP connections have a number of \n>  advantages\", one is missing (latency), and one is not always true (memory \n>  savings). I therefore suggest 2 changes.\n\n>  1) Add a bullet between current 3rd and 4th bullet:\n  \n>      .  Latency is reduced because network congestion is reduced, so\n>         less retransmissions are needed, so the elapsed transmission\n>         time is shorter.\n  \n>  2) Replace:\n  \n>      .  By opening and closing fewer TCP connections, CPU time is saved,\n>         and memory used for TCP protocol control blocks is also saved.\n  \n>  with:\n  \n>      .  By opening and closing fewer TCP connections, CPU time is saved\n>         in routers and hosts (clients, servers, proxies, gateways, tunnels,\n>         or caches), and memory used for TCP protocol control blocks can\n>         be saved in hosts.\n  \n>  Regarding memory savings, the original statement is not true in its \n>  generality. The number of TCP control blocks using up memory at any point \n>  in time depends on many parameters, among which we find:\n  \n>  * timeout value of persistent connections\n>  * average number of simultaneous users\n>  * activity of users:\n>      - whether they typically retrieve lots of objects (pages, inline\n>        images) and then become quiet\n>      - whether they typically download a few objects every N minutes\n>        with N inferior to the TCP connection timeout value\n>      - whether they typically download a few objects every N minutes\n>        with N superior to the TCP connection timeout value\n>      - whether they typically poll a server (e.g., to get the latest\n>        results of a sporting event)\n>  * etc.\n  \n>  By varying these values, one can easily show that the number of TCP control\n> blocks can be higher with persistent connections in some cases, smaller in \n> others. Persistent connections often allow to save memory, but not always.\n\nI am not sure exactly what is meant by CPU time, but as pointed out\npersistent connections do not correlate with opening and closing fewer \nTCP connections.  In fact persistent connections may result in greater\nsearch times for protocol control blocks as the number of active protocol\ncontrol blocks in many sorts of typical types of web use will increase\nwith persistent TCP connections.\n\nTelford Tools does a lot of work in optimizing web server performance. \nWe have the impression that the use of persistent TCP \nconnections for HTTP was addressing the problem of the excessive \ncost of opening a TCP connection in certain TCP implementations.  \n\nAs the use of persistent TCP connections is not guaranteed \nto decrease the number of TCP connections opened per \nsecond in typical web server or usage (although \nthe number of connections opened per second in certain \nperformance tests may decrease), the persistent TCP connection \nfeature while relatively harmless is irrelevant to Web Server\nor Web Client performance except in certain contrived\nsituations.  In many situations persistent TCP connections\nwill not decrease the number of connections opened per \nsecond but will decrease the number of http transactions\nper second of which a given servier is capable because\nat any given time the time to find a protocol control block\nmay in crease because the number of active protocol\ncontrol blocks at any given moment is likely to increase\nwith the use persistent TCP connections.\n\nIn the general case, if better Web Server\nor Web Client performance is desired, the correct\napproach is to bite the bullet and improve the performance\nof the TCP virtual circuit open code.\n\nJoachim Martillo\n<A HREF=\"http://members.aol.com/Telford001/\">Telford Tools, Inc.</A> \n\n\n\n", "id": "lists-012-3976680"}, {"subject": "Re: Drawbacks of persistent connection", "content": "On Mon, 15 Jun 1998 10:31:25 -0700, Jim Gettys wrote:\n> \n> > Presumably, the timeout of persistent connections will be longer than\n> > the TCP connection timeout (that is, the recommended time to maintain\n> > TCP TIME_WAIT state, generally 4 minutes). So even though the\n> > technique used for the attack is the same, the effect will be\n> > amplified in the case of persistent connections with long timeouts.\n> \n> No, actually, most of the benefit from persistent connections appears\n> to be in the first 30 seconds to a minute...\n> \n> I don't think many busy servers will likely keep that long a timeout,\n> even with persistent connections.  Mogul's research showed that most\n> the value for \"click ahead\" occurs in the first few minutes, so a\n> reasonable timeout for a busy server (one which will likely have to\n> time out connections at all) is likely shorter than the TCP TIME_WAIT\n> state.\n\nDo you refer to:\n\n    Jeffrey C. Mogul. \"The Case for Persistent-Connection HTTP\".\n    WRL Research Report 95/4, Digital, Palo Alto, CA, USA, May 1995.\n\nor one of its variants (Proc. WWW2 and Proc. SIGCOMM'95)?\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-3988646"}, {"subject": "RE: Minor typos in Rev03 etc", "content": "Thank you for your careful review of the HTTP/1.1 specification\nand your many comments. While there is naturally some push-back\non some of your proposals, it is this level of careful\nreview of the document that will make the standard stronger.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-3996833"}, {"subject": "Re: Multiple or unsolicited responses are forbidde", "content": "On Sun, 14 Jun 1998 13:25:10 -0700, Josh Cohen wrote:\n> \n> > * Can an HTTP server send multiple responses to an HTTP \n> > client, without \n> > the client knowing in advance how many responses it will \n> > receive, or how \n> > long it will take for the server to send all its responses?\n> \n> Yes, this is possible.  the 100 continue response can be sent\n> repeatedly in addition to the 'final' response code.\n> While its possible, its intended use appears to be \n> to provide an intermediary status response on long or\n> expensive operations..\n\nI just checked Rev-03 again, especially sections 8.2.4 and 10.1.1, and I \ncan't see this use of 100 continue described. Could you please elaborate \non this?\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-4004294"}, {"subject": "Re: Drawbacks of persistent connection", "content": "On Mon, 15 Jun 1998 09:48:50 -0700, Jim Gettys wrote:\n> \n> > Section 8.1.1 may come across as slightly biaised, because it lists\n> > only advantages of persistent connections. In practice, these are\n> > balanced by drawbacks. For instance, if the timeout value of\n> > persistent connections is larger than the TCP connection timeout,\n> > denial-of-service attacks are more effective: by using up all possible\n> > connections, a malicious user can prevent access to a targeted server\n> > for a longer period of time. Perhaps a quick mention of this issue\n> > would make sense in section 8.1.4 (Practical Considerations)?\n> \n> The denial of service attack is the same between persistent connections\n> and non-persistent connections.  I can see no difference between the\n> two situations; the attacker does exactly the same thing in either case,\n> with the same result.\n\nPresumably, the timeout of persistent connections will be longer than the \nTCP connection timeout (that is, the recommended time to maintain TCP \nTIME_WAIT state, generally 4 minutes). So even though the technique used \nfor the attack is the same, the effect will be amplified in the case of \npersistent connections with long timeouts.\n\n> In general, denial of service attacks are very difficult to deal with.\n\nAgreed.\n\nJean-Philippe Martin-Flatin\n\n\n\n", "id": "lists-012-4012237"}, {"subject": "Re: Drawbacks of persistent connection", "content": "> Sender: jpmf@tcomhp20.epfl.ch\n> From: \"J.P. Martin-Flatin\" <martin-flatin@epfl.ch>\n> Date: Mon, 15 Jun 1998 19:13:43 +0200\n> To: jg@pa.dec.com (Jim Gettys)\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Drawbacks of persistent connections\n> -----\n> On Mon, 15 Jun 1998 09:48:50 -0700, Jim Gettys wrote:\n> >\n> > > Section 8.1.1 may come across as slightly biaised, because it lists\n> > > only advantages of persistent connections. In practice, these are\n> > > balanced by drawbacks. For instance, if the timeout value of\n> > > persistent connections is larger than the TCP connection timeout,\n> > > denial-of-service attacks are more effective: by using up all possible\n> > > connections, a malicious user can prevent access to a targeted server\n> > > for a longer period of time. Perhaps a quick mention of this issue\n> > > would make sense in section 8.1.4 (Practical Considerations)?\n> >\n> > The denial of service attack is the same between persistent connections\n> > and non-persistent connections.  I can see no difference between the\n> > two situations; the attacker does exactly the same thing in either case,\n> > with the same result.\n> \n> Presumably, the timeout of persistent connections will be longer than the\n> TCP connection timeout (that is, the recommended time to maintain TCP\n> TIME_WAIT state, generally 4 minutes). So even though the technique used\n> for the attack is the same, the effect will be amplified in the case of\n> persistent connections with long timeouts.\n> \n\nNo, actually, most of the benefit from persistent connections appears\nto be in the first 30 seconds to a minute...\n\n\nI don't think many busy servers will likely keep that long a timeout,\neven with persistent connections.  Mogul's research showed that most\nthe value for \"click ahead\" occurs in the first few minutes, so a reasonable\ntimeout for a busy server (one which will likely have to time out connections\nat all) is likely shorter than the TCP TIME_WAIT state.\n\nAs I said, the attack is the same, and the effect is the same...\n- Jim\n\n\n\n", "id": "lists-012-4021120"}, {"subject": "Re: Advantages of persistent connection", "content": "> From: Telford001@aol.com\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Sun, 14 Jun 1998 23:08:24 EDT\n> To: martin-flatin@epfl.ch, http-wg@cuckoo.hpl.hp.com\n> Cc: Telford001@aol.com, Bodzia@aol.com\n> Subject: Re: Advantages of persistent connections\n> -----\n> In a message dated 98-06-14 19:43:03 EDT, martin-flatin@epfl.ch writes:\n> \n> > Regarding the advantages of persistent connections listed in section\n> >  8.1.1, under heading \"Persistent HTTP connections have a number of\n> >  advantages\", one is missing (latency), and one is not always true (memory\n> >  savings). I therefore suggest 2 changes.\n> \n> >  1) Add a bullet between current 3rd and 4th bullet:\n> \n> >      .  Latency is reduced because network congestion is reduced, so\n> >         less retransmissions are needed, so the elapsed transmission\n> >         time is shorter.\n> \n> >  2) Replace:\n> \n> >      .  By opening and closing fewer TCP connections, CPU time is saved,\n> >         and memory used for TCP protocol control blocks is also saved.\n> \n> >  with:\n> \n> >      .  By opening and closing fewer TCP connections, CPU time is saved\n> >         in routers and hosts (clients, servers, proxies, gateways, tunnels,\n> >         or caches), and memory used for TCP protocol control blocks can\n> >         be saved in hosts.\n> \n> >  Regarding memory savings, the original statement is not true in its\n> >  generality. The number of TCP control blocks using up memory at any point\n> >  in time depends on many parameters, among which we find:\n> \n> >  * timeout value of persistent connections\n> >  * average number of simultaneous users\n> >  * activity of users:\n> >      - whether they typically retrieve lots of objects (pages, inline\n> >        images) and then become quiet\n> >      - whether they typically download a few objects every N minutes\n> >        with N inferior to the TCP connection timeout value\n> >      - whether they typically download a few objects every N minutes\n> >        with N superior to the TCP connection timeout value\n> >      - whether they typically poll a server (e.g., to get the latest\n> >        results of a sporting event)\n> >  * etc.\n> \n> >  By varying these values, one can easily show that the number of TCP control\n> > blocks can be higher with persistent connections in some cases, smaller in\n> > others. Persistent connections often allow to save memory, but not always.\n> \n> I am not sure exactly what is meant by CPU time, but as pointed out\n> persistent connections do not correlate with opening and closing fewer\n> TCP connections.  In fact persistent connections may result in greater\n> search times for protocol control blocks as the number of active protocol\n> control blocks in many sorts of typical types of web use will increase\n> with persistent TCP connections.\n\nNot true; please see Jeff Mogul's research in this area; since there are \nmany fewer connections used, you have many fewer PCB's in time-wait state \nafter the connections close, so research  (using traces) indicates that \nfewer total PCB's are needed.\n> \n> Telford Tools does a lot of work in optimizing web server performance.\n> We have the impression that the use of persistent TCP\n> connections for HTTP was addressing the problem of the excessive\n> cost of opening a TCP connection in certain TCP implementations. \n> \n> As the use of persistent TCP connections is not guaranteed\n> to decrease the number of TCP connections opened per\n> second in typical web server or usage (although\n> the number of connections opened per second in certain\n> performance tests may decrease), the persistent TCP connection\n> feature while relatively harmless is irrelevant to Web Server\n> or Web Client performance except in certain contrived\n> situations.  In many situations persistent TCP connections\n> will not decrease the number of connections opened per\n> second but will decrease the number of http transactions\n> per second of which a given servier is capable because\n> at any given time the time to find a protocol control block\n> may in crease because the number of active protocol\n> control blocks at any given moment is likely to increase\n> with the use persistent TCP connections.\n> \n> In the general case, if better Web Server\n> or Web Client performance is desired, the correct\n> approach is to bite the bullet and improve the performance\n> of the TCP virtual circuit open code.\n> \n\nI have no problems with optimizing code, wherever....\n\nI think you'll find others who've been looking into this problem\ncarefully hold other opinions to those you state in this message...\n\nJoachim, I think you need to read some of the research papers that\nmade the case for HTTP/1.1; you'll find them referenced at the\nend of the HTTP spec.   They include simulations of the effects of\npersistent connections on TCP PCB's, and some running code work\nthat we've done as well.\n- Jim\n\n\n\n", "id": "lists-012-4031950"}, {"subject": "Re: Drawbacks of persistent connection", "content": "> Sender: jpmf@tcomhp20.epfl.ch\n> From: \"J.P. Martin-Flatin\" <martin-flatin@epfl.ch>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Mon, 15 Jun 1998 01:44:25 +0200\n> To: http-wg@cuckoo.hpl.hp.com\n> Cc: martin-flatin@epfl.ch\n> Subject: Drawbacks of persistent connections\n> -----\n> Section 8.1.1 may come across as slightly biaised, because it lists only\n> advantages of persistent connections. In practice, these are balanced by\n> drawbacks. For instance, if the timeout value of persistent connections is\n> larger than the TCP connection timeout, denial-of-service attacks are more\n> effective: by using up all possible connections, a malicious user can\n> prevent access to a targeted server for a longer period of time. Perhaps a\n> quick mention of this issue would make sense in section 8.1.4 (Practical\n> Considerations)?\n> \n\nThe denial of service attack is the same between persistent connections and\nnon-persistent connections.  I can see no difference between the two situations;\nthe attacker does exactly the same thing in either case, with the same result.\n\nIn general, denial of service attacks are very difficult to deal with.\n- Jim\n\n\n\n", "id": "lists-012-4046176"}, {"subject": "Re: draft-ietf-http-state-man-mec09.txt,.p", "content": "The newly announced draft is the same as state-man-mec-08, except for a\nReferences section, added at IANA's request.  The usual stuff is\navailable at <http://portal.research.bell-labs.com/~dmk/cookie.html>.\n\nDave Kristol\n\n\n\n", "id": "lists-012-4055979"}, {"subject": "I-D ACTION:draft-ietf-http-state-man-mec09.txt,.p", "content": "Note: This revision reflects comments received during the last call period.\n\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-09.txt,.ps\nPages: 22\nDate: 15-Jun-98\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal [Netscape], but it can interoperate with\nHTTP/1.0 user agents that use Netscape's method.  (See the HISTORICAL\nsection.)\n \nThis document reflects implementation experience with RFC 2109 [RFC2109]\nand obsoletes it.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-09.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-09.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-09.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-4062905"}, {"subject": "http 1.1 RF", "content": "     To: Tim Berners-Lee (http-wg@cuckoo.hpl.hp.com)\n     From: Peter Lenahan (pjl@ibi.com)\n     \n     May I ask for a feature in the next version HTTP 1.1\n     \n     I have read the HTTP 1.1 spec and searched for this and couldn't find \n     a solution.\n     \n     The closest thing that I found was\n     \n     Content-Location: URI \n     \n     This is what I would like:\n     \n     Content-Name: URI\n     \n     The Content-Name would formally name the data, this would give the \n     browser a name to save a file under when the user is prompted in the \n     browser's save dialog box.\n     \n     Reason for the request: I have a CGI generating a Microsoft Excel \n     Spread sheet dynamically from a database.  The user get's back a \n     Content-type: application/x-xls that it may or may not understand.  \n     The user is asked to open or save the data. Currently there doesn't \n     appear to be an HTTP way of setting the name of the data to save to \n     the local machine.\n     \n     I thought that the browser companies may have added extension-header's \n     as the spec suggests, but I was not that lucky.\n     \n     \n     Thanks,\n     \n     Peter Lenahan\n     pjl@ibi.com\n     \n\n\n\n", "id": "lists-012-4071867"}, {"subject": "Re: http 1.1 RF", "content": "peter_lenahan@ibi.com:\n>\n>\n>     To: Tim Berners-Lee (http-wg@cuckoo.hpl.hp.com)\n>     From: Peter Lenahan (pjl@ibi.com)\n>     \n>     May I ask for a feature in the next version HTTP 1.1\n>     \n>     I have read the HTTP 1.1 spec and searched for this and couldn't find \n>     a solution.\n[...]     \n>     The Content-Name would formally name the data, this would give the \n>     browser a name to save a file under when the user is prompted in the \n>     browser's save dialog box.\n\nHi Peter,\n\nThe upcoming update of the HTTP 1.1 specification RFC includes a\ndescription of the feature you want.  This is not officially part of\nthe standard but it is widely adopted in current browsers.\n\nI have included the draft text (from\ndraft-ietf-http-v11-spec-rev-03.txt) below.\n\nKoen.\n\n\n--snip--\n\n  19.5.1 Content-Disposition\n\n  The Content-Disposition response-header field has been proposed as a\n  means for the origin server to suggest a default filename if the user\n  requests that the content is saved to a file. This usage is derived\n  from the definition of Content-Disposition in RFC 1806 [35].\n\n          content-disposition = \"Content-Disposition\" \":\"\n                                disposition-type *( \";\"\n                                    disposition-parm )\n          disposition-type = \"attachment\" | disp-extension-token\n          disposition-parm = filename-parm | disp-extension-parm\n          filename-parm = \"filename\" \"=\" quoted-string\n          disp-extension-token = token\n          disp-extension-parm = token \"=\" ( token | quoted-string )\n\n  An example is\n\n          Content-Disposition: attachment; filename=\"fname.ext\"\n  \n  The receiving user agent should not respect any directory path\n  information that may seem to be present in the filename-parm\n  parameter, which is the only parameter believed to apply to HTTP\n  implementations at this time. The filename should be treated as a\n  terminal component only.\n       \n  [If this header is used in a response with the\n  application/octet-stream content-type,(*)] the implied suggestion is\n  that the user agent should not display the response, but directly\n  enter a `save response as...' dialog.\n\n  See section 15.5 for Content-Disposition security issues.\n\n (*) This half-sentence seems to have been dropped from the 03\n revision, it was still in the 01 revision.  Editing error?  I'll raise\n this as an issue.\n \n\n\n\n", "id": "lists-012-4080297"}, {"subject": "New issue: DISPOSITION_EDI", "content": "I just noticed that half of a sentence disappeared in section 19.5.1\n(Content-Disposition) of draft-ietf-http-v11-spec-rev-0[23].txt.\n\nMy original draft\n(http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q2/0151.html) and\nrevision 01 had:\n\n   If this header is used in a response with the\n   application/octet-stream content-type, the implied suggestion is\n   that the user agent should not display the response, but directly\n   enter a `save response as..' dialog.\n\nRevisions 02 and 03 have:\n\n  The implied suggestion is that the user agent should not display the\n  response, but directly enter a `save response as...' dialog.\n\nI guess there has been some type of editorial error.  The original\ntext is correct, the new text is incorrect as a description of\nexisting practice.  User agents (at least the ones I tested) will\n*not* directly enter a 'save as' dialog if Content-Disposition is\npresent on, say, a normal text/html response.  The text/html will be\ndisplayed as a normal response and the filename in Content-Disposition\nis only used if the user does 'save as' manually.\n\nKoen.\n\n\n\n", "id": "lists-012-4090579"}, {"subject": "rev03 editorial crossref glitc", "content": "page 22 reads:\n\n \"All transfer-coding values are case-insensitive. HTTP/1.1 uses transfer\n  coding values in the TE header field (section 14.39) and in the\n  Transfer-Encoding header field (section 14.39).\"\n\nTransfer-Encoding is 14.41\n\n-P\n\n\n\n", "id": "lists-012-4098768"}, {"subject": "draft-ietf-http-authentication01.txt client imp", "content": "Does anybody have a client implementation of Digest Auth from the\n\"specification formerly known as RFC 2069\"?\n\nUser-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\nand \nUser-Agent: W3CCommandLine/5.1m libwww/5.1m\n\nseem tied to Basic only..\n\nthanks,\n-P\n\n\n\n", "id": "lists-012-4105410"}, {"subject": "RE: draft-ietf-http-authentication01.txt client imp", "content": "> Does anybody have a client implementation of Digest Auth from the\n> \"specification formerly known as RFC 2069\"?\n> \n> User-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\n> and \n> User-Agent: W3CCommandLine/5.1m libwww/5.1m\n> \n> seem tied to Basic only..\n\n\nContact-EmailRonald.Tschalaer@psi.ch\nResubmissionresubmit\nPublicationpublic\nApp-NameHTTPClient\nApp-VersionV0.4-dev, running on JDK 1.1 or later\nApp-Typeclient\nDescriptionClient library in Java\n\nincludes implementation of all features.\n\nContact-Emailjcma@ai.mit.edu\nResubmissionnew_submit\nPublicationpublic\nApp-NameCL-HTTP\nApp-Version67.47\nApp-Typecombined\nDescriptionAlso includes client and web walker\n\nimplements:\n\nA 3.2.1WWW-Authenticate Digestt\n\nbut not all authentication draft features.\n\nThe authentication features are holding us up, and it is important\nto have more client implementations.\n\n\n\n", "id": "lists-012-4112927"}, {"subject": "Implementations and implementation reports", "content": "This is another plea for implementation reports and/or updates to existing\nimplementation reports, so we can get these specifications to draft standard.\n\nIf we don't have at least 2 interoperable implementations of each feature \nby, say, the August IETF meeting, we'll have to strongly consider dropping \nthose features from the protocol. This is required by IETF rules for draft \nstandard. Note we need 2 interoperable implementations of each feature; \nthere is no requirement that these implementations be shipped as product \nat the time of reporting, or for that matter, ever.  The point of the \nrequirement is to show that the specification can be independently \nimplemented and that the protocol as specified works.\n\nWe've asked for time at the IETF meeting to discuss any such cases where \nwe don't meet IETF requirements.  There aren't many more features that \nneed implementation and testing, though Digest authentication is more \nof a problem than the base specification.  \n\nData on all reported implementations can be found at:\nhttp://www.w3.org/Protocols/HTTP/Forum/Reports/ ; the report is\nresonably up to date (there is one report I haven't added yet).\n\nMost reports are public; in general, you should be able to find\nsomeone to test features against given the data available on that\npage.\n\nPlease let me know of any new implementations, or any features that\nyou've tested since your report (I'm willing to update an existing report\nfor new features, rather than require you to fill out the very long form\nyet again to report a few changes to the status of tested features).\n\nThanks again to all who have contributed...\nYour editor (of the base HTTP spec)\nJim Gettys\n\n\n\n", "id": "lists-012-4122035"}, {"subject": "Re: Implementations and implementation reports", "content": "Jim Gettys wrote:\n> We've asked for time at the IETF meeting to discuss any such cases where\n> we don't meet IETF requirements.  There aren't many more features that\n> need implementation and testing, though Digest authentication is more\n> of a problem than the base specification.\n\nIf someone wants to remind me where the latest digest auth spec lives,\nI'll have a crack at bringing Apache's digest auth up to speed (if it\nisn't already - it's not clear to me from the report whether it is, but\nI could've sworn there were some changes since it was last touched).\n\nIf there's anything else critical Apache can help with, let me know what\nand I'll see what I can do.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686| Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org/\nand Technical Director|Email: ben@algroup.co.uk |\nA.L. Digital Ltd,     |Apache-SSL author     http://www.apache-ssl.org/\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache/\n\nWE'RE RECRUITING! http://www.aldigital.co.uk/recruit/\n\n\n\n", "id": "lists-012-4130975"}, {"subject": "what does &quot;unrecognized header fields&quot; mean", "content": "The LPWA proxy truncates the Referer header, in most cases, to read\n        Referer:\nwith no value, and therefore violates the HTTP/1.1 specification.\n\nI received a report about a site that didn't work with the LPWA HTTP\nproxy.  After much investigation, I discovered that the site's server\nsoftware was barfing because Referer had no value.  Add a value, or\nremove the header altogether, and their server worked okay.\n\nI was all set to go off in high dudgeon about how the specification\n*says* that headers that aren't understood should be ignored.  What it\nsays (7.1 Entity Header Fields) is that \"unrecognized header fields\nSHOULD be ignored....\"\n\nThe question I have is, what does \"unrecognized\" mean?  Does it just\nmean a header whose name is unfamiliar, or does it also mean a\nrecognized header for which the value is in some way invalid (such as\nmy example above)?  I realize that \"be liberal in what you accept\" is\non my side, here, but it's not clear that the *letter* of the\nspecification is also on my side.\n\nDave Kristol\n\n\n\n", "id": "lists-012-4139455"}, {"subject": "Fwd: HTTP 1.1 question on &quot;#rule&quot; exampl", "content": "To document for the mailing list this editorial issue.\n- Jim\n\n\nattached mail follows:\nIn the definition of \"#rule\", draft-ietf-http-v11-spec-rev-03 states:\n    \n  This makes the usual form of lists very easy; a rule such as\n  \"( *LWS element *( *LWS \",\" *LWS element )) \" can be shown as\n  \"1#element\".\n    \nI'm confused by the presence of the outer set of parentheses in the\nformer but not the latter rule.  Wouldn't it be clearer to rewrite it as\n\"( 1#element )\"?\n\n-- \nPaul McJones <mcjones@pa.dec.com>\nCompaq Systems Research Center, 130 Lytton Ave., Palo Alto, CA 94301 USA\nvoice: 1 650 853 2255, fax: 1 650 853 2104\nhttp://www.research.digital.com/SRC/personal/Paul_McJones/\n\nattached mail follows:\n>than as metasyntactic characters.  Instead, only the inner quotes\n>(around the comma) designate literal text.  Perhaps you could rewrite\n>this with the two rules displayed on separate lines and not surrounded\n>by outer quotes:\n\nYes, or just use single-quote marks on the outside.\n\n....Roy\n\n\n\n", "id": "lists-012-4147697"}, {"subject": "Re: Architectural Issues  Additional Commen", "content": "Dear Members of the IESG,\n\nAn added comment to my previous post, is that I consider that Keith Moore \nhas indeed recently given the IPP WG more clear guidance on these issues.\n\nHowever, the IPP WG has spent several months of sometimes almost religious \ndiscussions on these subjects (shared with the HTTP WG), so if the IESG \ncould come up with some general guidelines for these kind of issues, it \nmight save other WGs considerable time and effort in the future.\n\nCarl-Uno Manros\n\n\nAt 02:04 PM 7/6/98 PDT, you wrote:\n>Dear Members of the IESG,\n>\n>In recent discussions with Keith Moore in his role as Applications Area\n>Director, a couple of rather fundamental questions about Internet protocol\n>architecture have come up. As chair of one of the Application Area WGs, I\n>have had some difficulty to understand the current policy within the IESG\n>and the IAB on the following two aspects, and might have given my WG wrong\n>advice on the acceptability of certain technical solutions vs. others from\n>an IESG/IAB perspective. \n>\n>Issue 1 - Firewalls\n>===================\n>\n>Although I have been unable to find much said about firewalls in the IETF\n>RFCs (RFC1579 and RFC2356 are the only references that come up), there\n>seems to be some undocumented views within the IESG about what is\n>appropriate and what is not when it comes to distinguishing different\n>applications in firewalls. If such criteria are indeed used by the IESG, I\n>think it is urgently needed to document them. They should distinguish\n>between outgoing vs. incoming firewalls and should clearly state on which,\n>and how many  \"parameters\", filtering must be possible (such as TCP/IP\n>address, scheme, port, method, content-type).\n>\n>Issue 2 - Layering of Applications\n>==================================\n>\n>It has also been discussed whether layering one application on another is\n>allowed, and if so, which kind of things can be layered on what, and which\n>combinations would be disallowed. This has resulted in debates such as if\n>HTTP is specific to web traffic or a more generic transport protocol. I\n>think it is particularly important to answer this question in anticipation\n>of the HTTP-NG protocol, which is planned for introduction in the IETF\n>later this year. To my knowledge, the designers of that protocol have\n>explicitly wanted to make a protocol that is a more genereric than the\n>current HTTP. Would that be in conflict with the IESGs ideas about what is\n>allowed or not over that protocol?  Again, any criteria that the IESG will\n>be using for this kind of layering decisions should be clearly documented,\n>so the WGs have a reasonable chance to stay within the boundaries of what\n>the IESG considers to be \"correct\" design.\n>\n>Thankful for your feedback on this,\n>\n>Carl-Uno Manros\n>Chair of IETF WG on IPP\n>\n>\n> \n>\n>Carl-Uno Manros\n>Principal Engineer - Advanced Printing Standards - Xerox Corporation\n>701 S. Aviation Blvd., El Segundo, CA, M/S: ESAE-231\n>Phone +1-310-333 8273, Fax +1-310-333 5514\n>Email: manros@cp10.es.xerox.com\n>\n>\n\n\n\n", "id": "lists-012-4155672"}, {"subject": "Updated issues list... (NOTE THE MUST/SHOULD/MAY AUDIT!", "content": "As always, the list is at:\nhttp://www.w3.org/Protocols/HTTP/Issues/\n\nIn particular, please review the MUST/SHOULD/MAY issue for any problems.\n\nAlso let me know if I'm mising any.\n- Jim\n\n\n\n", "id": "lists-012-4166890"}, {"subject": "Fwd: A suggestion for correction on HTTP/1.1 draf", "content": "For some reason, Ravi had trouble posting this.\n- Jim\n\n\nattached mail follows:\nHi Jim,\nI tried sending the attached email to the 'http-wg@cuckoo.hpl.hp.com' as\nsuggested in the HTTP/1.1 Internet-Draft, but it came back with a\n'connection timed out' error. Since you are one of the primary authors for\nthis draft, I thought I could forward it to you.\nThanks,\nRavi Badrachalam\nAndersen Consulting\n(Embedded image moved to file: PIC07364.PCX)\nRavi Badrachalam\n07/03/98 03:48 PM\n\nTo:   http-wg@cuckoo.hpl.hp.com\ncc:\nSubject:  A suggestion for correction on HTTP/1.1 draft\n\nWhile browsing through your HTTP Internet-Draft \"\ndraft-ietf-http-v11-spec-rev-03\", I came across a minor anomaly.\n\nThe first paragraph under section \"9.5 POST' starts like this:\n\"The POST method is used to request that the destination server accept the\nentity ........\"\n\nThis is the only place in the whole document that the phrase \"destination\nserver\" is used, while every where else the phrase \"origin server\" is used\nto represent \"The server on which a given resource resides or is to be\ncreated.\"(definition from the terminology).\nWithin the same section(9.5),  a few paragraphs later, a new paragraph\nstarts like this:\n\"If a resource has been created on the origin server, ........\"  which\nimplies that we are referring to the same server in the same context by two\ndifferent names (I called it an anomaly since 'origin' and 'destination'\nhave conflicting meanings).\n\nIn the context of POST, the phrase \"destination server\" makes more sense,\nbut I was wondering for the sake of consistency, we should either change it\nto \"origin server\", or document the new phrase \"destination server\" in the\nterminology section as something that makes sense only under \"POST\" action.\n\nOverall, this Internet-Draft is excellent and outstanding because it has\nbeen kept very simple and easy to understand, yet  providing a complete\nspecification. Great work!\n\nThanks,\nRavi Badrachalam\nAndersen Consulting\n\n\n\nReceived: by src-mail.pa.dec.com; id AA02185; Sun, 5 Jul 1998 18:14:00 -0700\nReceived: from mail1.digital.com by pobox1.pa.dec.com (5.65v3.2/1.1.10.5/07Nov97-1157AM)\nid AA30252; Sun, 5 Jul 1998 18:13:48 -0700\nReceived: from www10.w3.org (www10.w3.org [18.23.0.20])\nby mail1.digital.com (8.8.8/8.8.8/WV1.0f) with ESMTP id SAA03669\nfor <jg@pa.dec.com>; Sun, 5 Jul 1998 18:13:47 -0700 (PDT)\nFrom: ravi.badrachalam@ac.com\nReceived: from sxhab.compuserve.net (sxhab.compuserve.net [149.174.177.79]) by www10.w3.org (8.8.5/8.7.3) with ESMTP id VAA27623 for <jg@w3.org>; Sun, 5 Jul 1998 21:07:04 -0400 (EDT)\nReceived: from aamta.compuserve.net (nthnsaab.ibmnotes.compuserve.com [149.174.177.77]) by sxhab.compuserve.net (8.8.8/8.6.12) with SMTP id VAA05049.; Sun, 5 Jul 1998 21:03:00 -0400 (EDT)\nReceived: by aamta.compuserve.net(Lotus SMTP MTA SMTP v4.6 (462.2 9-3-1997))  id 85256639.000584DF ; Sun, 5 Jul 1998 21:00:16 -0400\nX-Lotus-Fromdomain: ACIN@CSERVE\nTo: jg@w3.org\nMessage-Id: <86256639.00051A11.00@aamta.compuserve.net>\nDate: Sun, 5 Jul 1998 20:00:23 -0500\nSubject: A suggestion for correction on HTTP/1.1 draft\nMime-Version: 1.0\nContent-Type: multipart/mixed; \nBoundary=\"0__=4BgnYlZpYlCMd2U2qgX7lZYCGguljFCmJN8zz9pddvpq4weSnN82oCo6\"\n\n--0__=4BgnYlZpYlCMd2U2qgX7lZYCGguljFCmJN8zz9pddvpq4weSnN82oCo6\nContent-type: text/plain; charset=us-ascii\n\nHi Jim,\nI tried sending the attached email to the 'http-wg@cuckoo.hpl.hp.com' as\nsuggested in the HTTP/1.1 Internet-Draft, but it came back with a\n'connection timed out' error. Since you are one of the primary authors for\nthis draft, I thought I could forward it to you.\nThanks,\nRavi Badrachalam\nAndersen Consulting\n(Embedded image moved to file: PIC07364.PCX)\nRavi Badrachalam\n07/03/98 03:48 PM\n\nTo:   http-wg@cuckoo.hpl.hp.com\ncc:\nSubject:  A suggestion for correction on HTTP/1.1 draft\n\nWhile browsing through your HTTP Internet-Draft \"\ndraft-ietf-http-v11-spec-rev-03\", I came across a minor anomaly.\n\nThe first paragraph under section \"9.5 POST' starts like this:\n\"The POST method is used to request that the destination server accept the\nentity ........\"\n\nThis is the only place in the whole document that the phrase \"destination\nserver\" is used, while every where else the phrase \"origin server\" is used\nto represent \"The server on which a given resource resides or is to be\ncreated.\"(definition from the terminology).\nWithin the same section(9.5),  a few paragraphs later, a new paragraph\nstarts like this:\n\"If a resource has been created on the origin server, ........\"  which\nimplies that we are referring to the same server in the same context by two\ndifferent names (I called it an anomaly since 'origin' and 'destination'\nhave conflicting meanings).\n\nIn the context of POST, the phrase \"destination server\" makes more sense,\nbut I was wondering for the sake of consistency, we should either change it\nto \"origin server\", or document the new phrase \"destination server\" in the\nterminology section as something that makes sense only under \"POST\" action.\n\nOverall, this Internet-Draft is excellent and outstanding because it has\nbeen kept very simple and easy to understand, yet  providing a complete\nspecification. Great work!\n\nThanks,\nRavi Badrachalam\nAndersen Consulting\n\n\n--0__=4BgnYlZpYlCMd2U2qgX7lZYCGguljFCmJN8zz9pddvpq4weSnN82oCo6\nContent-type: application/octet-stream; \nname=\"PIC07364.PCX\"\nContent-transfer-encoding: base64\n\nCgUBCAAAAAB+AQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAABfwEBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAD/D/8P/w//D/8P4w/RD8kPxA/CDw//BP8E/wT/BP8E4wTRBMkExATCBAT/\nBP8E/wT/BP8E4wTRBMkExATCBAT/BP8E/wT/BP8E4wTRBMkExATCBAQMAAAAgAAAAIAAgIAAAACA\ngACAAICAgICAwMDA/wAAAP8A//8AAAD//wD/AP//////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAA\n\n--0__=4BgnYlZpYlCMd2U2qgX7lZYCGguljFCmJN8zz9pddvpq4weSnN82oCo6--\n\n\n\n\napplication/octet-stream attachment: PIC07364.PCX\n\n\n\n\n", "id": "lists-012-4173521"}, {"subject": "Progress on testing HTTP/1.1", "content": "I've recieved a new report from Patrick McManus of Applied Theory\nCommunications for a new server.  Our thanks.\n\nAlso, Yves Lafon and Ronald Tschalaer have finished testing Content-MD5\nin Jigsaw, and they interoperate with the Apache implementation, so\nthat is also off the \"hit list\".\n\nYves Lafon has also finished his testing of Warning, which also\ngives us the second proxy implementation for Warning needed.\n\nBelow is the remaining features needed for testing.  This list is\na bit pessimistic, as a some features have no proxy related functions\nand the report below calls out anything that does not have two implementations\nfor clients, servers and proxies.\n\nUpdates to data greatfully accepted....  Lets see if we can cancel\nour meeting in Chicago by getting everything tested by then, so we won't\nneed to have the discussion on what to drop from the specs.\n- Jim Gettys\n\nClients         Servers         Proxies         Feature\n 2t  3y  7n  2-  4t  2y 12n  0-  1t  1y  6n  0- H 10.2.3        202 Accepted\n 1t  3y  8n  2-  3t  2y 13n  0-  1t  1y  6n  0- H 10.2.4        203 Non-Authoritative Information\n 1t  3y  8n  2-  3t  1y 14n  0-  1t  1y  6n  0- H 10.2.6        205 Reset Content\n 2t  3y  8n  1-  2t  4y 12n  0-  0t  2y  6n  0- H 10.3.1        300 Multiple Choices\n 3t  5y  6n  0-  2t  4y 12n  0-  0t  3y  5n  0- H 10.3.4        303 See Other\n 2t  4y  7n  1-  3t  1y 14n  0-  1t  1y  6n  0- H 10.4.11       410 Gone\n 2t  4y  7n  1-  2t  2y 14n  0-  1t  1y  6n  0- H 10.4.15       414 Request-URI Too Long\n 1t  5y  7n  1-  3t  5y 10n  0-  1t  3y  4n  0- H 13.3.3        Weak entity tags\n 3t  3y  8n  0-  2t  2y 14n  0-  1t  1y  6n  0- H 14.39 TE\n 3t  2y  9n  0-  1t  2y 15n  0-  0t  2y  6n  0- H 14.40 Trailer\n 2t  0y 12n  0-  4t  5y  9n  0-  1t  0y  7n  0- A 3.2.1 WWW-Authenticate Digest\n 1t  0y 13n  0-  2t  1y 15n  0-  0t  0y  8n  0- A 3.2.1 qop-options auth\n 1t  0y 13n  0-  1t  0y 17n  0-  0t  0y  8n  0- A 3.2.1 qop-options auth-int\n 2t  0y 12n  0-  4t  5y  9n  0-  1t  0y  7n  0- A 3.2.2 Authorization Digest\n 1t  0y 13n  0-  2t  1y 15n  0-  0t  0y  8n  0- A 3.2.2 request qop auth\n 1t  0y 13n  0-  1t  0y 17n  0-  0t  0y  8n  0- A 3.2.2 request qop auth-int\n 2t  0y 12n  0-  3t  2y 13n  0-  1t  0y  7n  0- A 3.2.3 Authentication-Info Digest\n 1t  0y 13n  0-  1t  2y 15n  0-  0t  0y  8n  0- A 3.2.3 response qop auth\n 1t  0y 13n  0-  1t  0y 17n  0-  0t  0y  8n  0- A 3.2.3 response qop auth-int\n 2t  0y 12n  0-  1t  1y 12n  4-  1t  0y  7n  0- A 4.2   Proxy-Authenticate Digest\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy qop-options auth\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy qop-options auth-int\n 2t  0y 12n  0-  1t  1y 12n  4-  1t  0y  7n  0- A 4.2   Proxy Authorization Digest\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy request qop auth\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy request qop auth-int\n 1t  1y 12n  0-  1t  0y 13n  4-  1t  0y  7n  0- A 4.2   Proxy Authentication-Info Digest\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy response qop auth\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy response qop auth-int\n\n\n--\nJim Gettys\nDigital Industry Standards and Consortia\nCompaq Computer Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n", "id": "lists-012-4190053"}, {"subject": "Re: Fwd: A suggestion for correction on HTTP/1.1 draf", "content": "On Mon, 6 Jul 1998, Jim Gettys wrote:\n\n> \n> For some reason, Ravi had trouble posting this.\n> - Jim\n\nAnd for some reason, my mail program refuses to quote attachments ...\ngrrr.\n\nAnyway, I would note that destination server would be a good english\nterm for PUT as well. \n\nBut I think that the phrase origin server has a meaning in the\nspecification beyond direction of the transaction data and is used\nin contrast to proxies, etc.  Understood to be one end-point of the\nlogical transaction where the response originates. Hence I'd vote for\nremoving destination server and using origin server.\n\nDave Morris\n\n\n\n", "id": "lists-012-4200660"}, {"subject": "ISSUE: revalidatio", "content": "Having read both\nhttp://www.ics.uci.edu/pub/ietf/http/draft-mogul-http-revalidate-01.txt, and\nthe diff version of rev-03, I am now confused about the Cache-control\nrevalidation directives, and their intended interaction with the Authorization\nmechanism.\n\nThe text in 14.8 appears to allow me, as a proxy, to serve objects requiring\nauthorization, without first validating the user's credentials, as long as the\nobject is fresh.\n\n14.9.4 seems to say that must-revalidate is not unconditional, but rather that\nit only requires revalidation if the object is stale.\nIt further seems to say that proxy-revalidate can be used to require shared\nproxies to authenticate each user.\n\nSo, an origin server should send both \"proxy-revalidate\" and \"public\" to force\nrevalidation?\nIf so, I think at the very least this should be added to the list in 14.8, and\nthat proxies ought to be required to revalidate in this case.\nIf not, I need educating.\n\nThank You,\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-4208913"}, {"subject": "Re: ISSUE: revalidatio", "content": "If the spec is so murky as to need a tutorial, perhaps it is not good enough to\nproduce interoperable implementations?\n\nWe intend to implment \"must-revalidate\", \"proxy-revalidate\", \"max-age\", and\n\"s-maxage\" the same if credentials are present; that is, _always_ revalidate,\nusing the current credentials.\n\nAnd I just noticed that 14.8 Authorization, special case (1) mentions\n\"proxy-maxage\".\nI assume that is intended to be \"proxy-revalidate\"?\n\nRichard L. Gray\nwill code for chocolate\n\n\n\nmogul@pa.dec.com on 07-14-98 01:57:23 PM\nPlease respond to mogul@pa.dec.com\nTo: Richard Gray/Raleigh/IBM@ibmus\ncc: http-wg@cuckoo.hpl.hp.com\nSubject: Re: ISSUE: revalidation\n\n\nRichard Gray <rlgray@us.ibm.com> writes:\n    Having read both\n    http://www.ics.uci.edu/pub/ietf/http/draft-mogul-http-revalidate-01.txt,\n    and the diff version of rev-03, I am now confused about the\n    Cache-control revalidation directives, and their intended\n    interaction with the Authorization mechanism.\n\nI have no opinions about Authorization, so you'll have to wait\nfor someone else to discuss the \"intended interaction\".  But\nregarding the basic revalidation mechanism:\n\n    14.9.4 seems to say that must-revalidate is not unconditional, but\n    rather that it only requires revalidation if the object is stale.\n    It further seems to say that proxy-revalidate can be used to\n    require shared proxies to authenticate each user.\n\nThe difference between must-revalidate and proxy-revalidate is\nthat the former applies to all caches, but the latter does not\napply to non-shared caches (e.g., browser caches).  If you are\nwilling to assume that a given browser cache is uniquely associated\nwith a specific user, then you could use proxy-revalidate instead\nof must-revalidate (if your goal is to force re-authentication.)\n\n    So, an origin server should send both \"proxy-revalidate\" and\n    \"public\" to force revalidation?  If so, I think at the very least\n    this should be added to the list in 14.8, and that proxies ought to\n    be required to revalidate in this case.  If not, I need educating.\n\nThe intended way to force revalidation (by shared caches) is to use\n\n Cache-control: s-maxage=0, proxy-revalidate\n\nI.e., tell the shared caches that the response is immediately\nstale, and that shared caches are not allowed to ignore this.\n*HOWEVER*, since this is rather verbose, and since the \"s-maxage\"\ndirective was added specifically to support this case, the\nspec says \"s-maxage directive also implies ... proxy-revalidate\",\nso you really only need to send\n\n Cache-control: s-maxage=0\n\nto force a shared cache to revalidate on every access.  It's\na no-op for private caches.\n\nBottom line: I don't think this is an issue, except perhaps\nfor \"the specification is complicated and someone should\nwrite a tutorial\".\n\n-Jeff\n\n\n\n", "id": "lists-012-4216570"}, {"subject": "New issue: PROXY-MAXAGETYP", "content": "    And I just noticed that 14.8 Authorization, special case (1)\n    mentions \"proxy-maxage\".  I assume that is intended to be\n    \"proxy-revalidate\"?\n    \nNo, \"proxy-maxage\" was the original proposed name for \"s-maxage\".\n(Roy pointed out that the relevant consideration was whether or\nnot a cache was \"shared\", not whether or not it was a \"proxy\".)\nThis section seems to have been missed during the rewording.\n\nJim, please make sure that all instances of \"proxy-maxage\"\nhave been replaced by \"s-maxage\", in both the HTTP/1.1 spec\nand in the Authentication spec.\n\nThanks\n-Jeff\n\n\n\n", "id": "lists-012-4227458"}, {"subject": "Re: ISSUE: revalidatio", "content": "    We intend to implment \"must-revalidate\", \"proxy-revalidate\",\n    \"max-age\", and \"s-maxage\" the same if credentials are present; that\n    is, _always_ revalidate, using the current credentials.\n\nI believe that this is a compliant implementation of the spec.\n\n-Jeff\n\n\n\n", "id": "lists-012-4235217"}, {"subject": "Re: ISSUE: revalidatio", "content": "Richard Gray <rlgray@us.ibm.com> writes:\n    Having read both\n    http://www.ics.uci.edu/pub/ietf/http/draft-mogul-http-revalidate-01.txt,\n    and the diff version of rev-03, I am now confused about the\n    Cache-control revalidation directives, and their intended\n    interaction with the Authorization mechanism.\n\nI have no opinions about Authorization, so you'll have to wait\nfor someone else to discuss the \"intended interaction\".  But\nregarding the basic revalidation mechanism:\n\n    14.9.4 seems to say that must-revalidate is not unconditional, but\n    rather that it only requires revalidation if the object is stale.\n    It further seems to say that proxy-revalidate can be used to\n    require shared proxies to authenticate each user.\n\nThe difference between must-revalidate and proxy-revalidate is\nthat the former applies to all caches, but the latter does not\napply to non-shared caches (e.g., browser caches).  If you are\nwilling to assume that a given browser cache is uniquely associated\nwith a specific user, then you could use proxy-revalidate instead\nof must-revalidate (if your goal is to force re-authentication.)\n\n    So, an origin server should send both \"proxy-revalidate\" and\n    \"public\" to force revalidation?  If so, I think at the very least\n    this should be added to the list in 14.8, and that proxies ought to\n    be required to revalidate in this case.  If not, I need educating.\n    \nThe intended way to force revalidation (by shared caches) is to use\n\nCache-control: s-maxage=0, proxy-revalidate\n\nI.e., tell the shared caches that the response is immediately\nstale, and that shared caches are not allowed to ignore this.\n*HOWEVER*, since this is rather verbose, and since the \"s-maxage\"\ndirective was added specifically to support this case, the\nspec says \"s-maxage directive also implies ... proxy-revalidate\",\nso you really only need to send\n\nCache-control: s-maxage=0\n\nto force a shared cache to revalidate on every access.  It's\na no-op for private caches.\n\nBottom line: I don't think this is an issue, except perhaps\nfor \"the specification is complicated and someone should\nwrite a tutorial\".\n\n-Jeff\n\n\n\n", "id": "lists-012-4242334"}, {"subject": "chunking and trailer", "content": "If my proxy, while processing a chunked response, exceeds the amount of data I\nam willing to buffer, does anything break by me throwing away the contents of\nthe trailer?  I think, from previous discussion, that the answer should be\n\"no\", but I am concerned about the effects of discarding \"Authentication-Info\"\nand \"Content-MD5\".\n\nThank You,\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-4251371"}, {"subject": "Re: chunking and trailer", "content": "Then, I am forced to wonder why it is allowed to occur in the Trailer.\n\nSince the spec does not appear to anywhere specify that clients MUST honour\nheader fields found in chunked trailers, nor that proxies MUST NOT remove and\ndiscard header fields found in chunked trailers, I think that it would be a bad\nidea for a server to place, in a chunked trailer, a header field that it did\nnot want to risk loss of.\n\nRichard L. Gray\nwill code for chocolate\n\n\n\nlawrence@agranat.com on 07-21-98 04:04:53 PM\nPlease respond to lawrence@agranat.com\nTo: Richard Gray/Raleigh/IBM@ibmus\ncc: http-wg@cuckoo.hpl.hp.com\nSubject: Re: chunking and trailers\n\n\nRichard Gray wrote:\n>\n> If my proxy, while processing a chunked response, exceeds the\n> amount of data I am willing to buffer, does anything break by\n> me throwing away the contents of the trailer?  I think, from\n> previous discussion, that the answer should be \"no\", but I am\n> concerned about the effects of discarding \"Authentication-Info\"\n> and \"Content-MD5\".\n\n Throwing away Authentication-Info would certainly break Digest\nAuthentication.\n\n--\nScott Lawrence            Consulting Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.   Embedded Web Technology     http://www.agranat.com/\n\n\n\n", "id": "lists-012-4258321"}, {"subject": "Re: chunking and trailer", "content": "To:http-wg@cuckoo.hpl.hp.com\n\n\n\nOn Mon, 20 Jul 1998, Richard Gray wrote:\n\n> If my proxy, while processing a chunked response, exceeds the amount of data I\n> am willing to buffer, does anything break by me throwing away the contents of\n> the trailer?  I think, from previous discussion, that the answer should be\n> \"no\", but I am concerned about the effects of discarding \"Authentication-Info\"\n> and \"Content-MD5\".\n\nI believe you violate the protocol if you delete header values which\naren't explictly described as removed by a proxy. Whether the value is\nprovided in the headers or the trailer. You have the option removing\nchunking and promoting the trailer values to the 'real' header.\nIn a normal situation, the trailers should be relatively small so if\nyour proxy can't tolerate the additional data, then it suggests to me\nthat some form of denial of service attack is underway and the proxy\nshould just discard the whole response.\n\nDave Morris\n\n\n\n", "id": "lists-012-4267450"}, {"subject": "Re: chunking and trailer", "content": "Richard L. Gray wrote:\n    Since the spec does not appear to anywhere specify that clients\n    MUST honour header fields found in chunked trailers, nor that\n    proxies MUST NOT remove and discard header fields found in chunked\n    trailers, I think that it would be a bad idea for a server to\n    place, in a chunked trailer, a header field that it did not want to\n    risk loss of.\n    \nYour premise is wrong.  Section 13.5.1 (End-to-end and Hop-by-hop\nHeaders) defines:\n\n       End-to-end headers, which must be transmitted to the ultimate\n       recipient of a request or response.\n\n(that \"must\" should be a MUST; see issue-list MMS_AUDIT_ITEM_110).\nand says:\n\nAll other headers defined by HTTP/1.1 are end-to-end headers.\n\nafter a short list of specifically hop-by-hop headers.  Thus, proxies\nMUST forward all but a specific and small set of headers.\n\nThere is no exception here for header fields carried in the\ntrailer of a chunked encoding; this encoding is hop-by-hop\nand so one clearly can't use it as an excuse to delete a header\nthat would otherwise have to be sent end-to-end.\n\n-Jeff\n\n\n\n", "id": "lists-012-4275364"}, {"subject": "Minutes of HTTP/1.1 editorial teleconference, July 23, 199", "content": "Attending, Larry Masinter, Jim Gettys, Henrik Frystyk, Jeff Mogul, Dave\nKristol, Roy Fielding, Paul Leach.  Minutes by Henrik Frystyk, reviewed\nand expanded by Jim Gettys.  Any errors are now my responsibility.\n\nAs always, current issues and status are found at\nat: http://www.w3.org/Protocols/HTTP/Issues/\n\nIssues discussed (and usually resolved): MUST-MAY-SHOULD, TE-IDENTITY, \nPROXY-DNS, ERRORS, RANGEDELIM, IEBUG, MISTAKES For detail, see the minutes \nbelow. \n\nThe conference call was made difficult due to the passage of a severe \nthunderstorm through the Boston area (I was in Carlisle, MA, 25 miles \nwest of Boston, and Henrik in Boston), which slowed us down and made things \ndifficult; the storm forced me to unplug my home machine and deal with \na portable phone through 7! power-failures.  Consequently, we didn't finish \ngoing over the issues list today, but hope to finish with another call \nin the next few days.\n\nWe plan to discuss in the next (hopefully last) teleconference:\nIEBUG, remaining questions in MISTAKES, WARNGEN, and VERSION. \n\nDraft 04 is in preparation; I plan to submit it the middle of next week \n(well in advance of the IETF ID deadline; I have a family reunion which \nforces my completion well in advance this time).\n\nYou should see mail following up from the designated people below on proposed \nlanguage on the working group list in the next two days.\n\nActions\n\n* h1  MUST-MAY-SHOULD. \nThis was classed as technical only as it affects normative language.\nThere have been no comments on this issue in the list, despite a\ncall for any comments.  I will apply the edits, doing a final sanity check.\n\nJim: Editorial\n\n* h2  TE-IDENTITY. Henrik: Add OPTIONAL for how the server handle it. That\nis, it is optional for the server to look at it but if it does then it\nSHOULD send 406 if it can't respond in the transfer encoding.\n\n* h3  PROXY-DNS.\n\n a) Paul: Broaden the scope of 504 to contain DNS timeouts as well and\ndescribe this as a minor clarification.\n\n b) What about authoritative errors in proxies? Too late to handle.  Existing\nproxies do at least three different things.  Grist for the HTTP-Extension\nworking group, who may find additional error codes useful.\n\n c) Larry: Put in a note for clients saying that to expect in a 400 code\nand maybe 500 as well.\n\n* h4  ERRORS. No resolution required - do what the spec says for the\nvarious features.\n\n* h5  RANGEDELIM. Mime-Multipart allows text to be in the area before \nthe first boundary. MM says that there should be an extra CRLF so that \nthere is an empty line after the boundaries. Dave: Clarify 19.2 that the \ndefinition of the MM body is defined by RFC 2046 (?). Also make the example \nlegal - it misses some CRLFs.\n\n* h6  IEBUG. Believe that if this becomes an important practical problem\nthen servers will deal with this; it appeared to be an infrequent failure,\non a buggy implementation, without an easy solution not keyed to the\nparticular browser.\n\nI am slightly nervous about the correctness of this resolution so we'll \ndiscuss it again in the next phone call, after refreshing our memory on \nexactly what problem 416 solves (no one on the call remembered all the \ndetails of the problem that caused 416 to be added to the protocol).\n\n* h6a  MISTAKES\n\na) Section 8.2.3: We believe the current SHOULD should become a MAY; only \na client ends up having enough information as to whether a retry makes \nsense or not.  Koen's example is a good one, to retry a GET of 1 megabyte \nthat failed partially is something only a user agent will know if it wants \nthe user control over.  The first sentence also needs editorial work to \nmake clear we mean the connection closing before the end of the response \nhas been recieved.\n\nb) The consensus was the invalidation was in fact correct, has been a \nrequirement for a long time given language existing even in 1945, and \nit would be dangerous to remove it.  At least, that is what I remember\nwhile in the basement with a scared 3 year old near by over the static\nin the phone.\n\nc) At this point, the storm arrived in Boston, Henrik went to save his \nmachine, and several other attendees had to leave; we'll pick up again \nwith Koen's message about section 14.2.  As the storm was mostly over\nat this point in Carlisle, this says the storm was about 25 miles across...\nDefinately the most exciting of any of these teleconferences, at least\nfor Henrik and me.\n\n- Jim Gettys\n\n\n\n", "id": "lists-012-4283382"}, {"subject": "Rev03 changes to 19.2, multipart/byterange", "content": "I propose here corrective wording for open issue RANGEDELIM (described\nin <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0141.html>).\n\nRFC 2046 defines multipart messages.  In section 5.1.1, it says,\n\n   The boundary delimiter MUST occur at the beginning of a line, i.e.,  \n   following a CRLF, and the initial CRLF is considered to be attached\n   to the boundary delimiter line rather than part of the preceding\n   part....\n\nNow, HTTP introduces an interesting ambiguity, because the first line\nof an entity body could well be considered the beginning of a line, and\nit follows a CRLF.  However, since we must ignore the CRLF that\nseparates the headers from the body, the body does not really begin\nwith a CRLF.  So by my reading of RFC 2046, an HTTP multipart entity\nwould have to include an extra CRLF preceding the boundary.  I have\namended Section 19.2 of the HTTP spec. accordingly, along with the\nexample.\n\nDave Kristol\n\n\n  19.2 Internet Media Type multipart/byteranges\n\n  When an HTTP 206 (Partial Content) response message includes the\n  content of multiple ranges (a response to a request for multiple\n  non-overlapping ranges), these are transmitted as a multipart\n  message-body (RFC 2046). The media type for this purpose is called\n  \"multipart/byteranges\".\n\n  The multipart/byteranges media type includes two or more parts, each\n  with its own Content-Type and Content-Range fields. The required\n  boundary parameter specifies the boundary string used to separate each\n  body-part.\n\n         Media Type name:         multipart\n         Media subtype name:      byteranges\n         Required parameters:     boundary\n         Optional parameters:     none\n         Encoding considerations: only \"7bit\", \"8bit\", or \"binary\" are\n                                  permitted\n         Security considerations: none\n\n\n  For example:\n\n     HTTP/1.1 206 Partial Content\n     Date: Wed, 15 Nov 1995 06:25:24 GMT\n     Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n     Content-type: multipart/byteranges; boundary=THIS_STRING_SEPARATES\n\n\n[Note:  two CRLF's above.  Second one is new. :-)]\n     --THIS_STRING_SEPARATES\n     Content-type: application/pdf\n     Content-range: bytes 500-999/8000\n\n     ...the first range...\n     --THIS_STRING_SEPARATES\n     Content-type: application/pdf\n     Content-range: bytes 7000-7999/8000\n\n     ...the second range\n     --THIS_STRING_SEPARATES--\n\n  Cautions:\n  1) Although RFC 2046 mandates that a CRLF precede each boundary, some\n  existing implementations omit the mandated CRLF that precedes the\n  first boundary in the body.\n\n  2) Although RFC 2046 permits the boundary string to be quoted, some\n  existing implementations handle a quoted boundary string incorrectly.\n\n  19.2.1 Multipart/x-byteranges\n  ...\n\nNote to Jim Gettys:  need to add reference to RFC 2046:\n    Freed, N., and N. Borenstein. \"Multipurpose Internet Mail\n    Extensions (MIME) Part Two: Format of Internet Message Bodies.\" RFC\n    2046, Innosoft, First Virtual, November 1996.\n\n\n\n", "id": "lists-012-4297052"}, {"subject": "Digest Authentication Challenge Orderin", "content": "This is a message I've been meaning to send for a while, but other\nprojects have a way of getting in the way.. I apologize for a certain\nlack of precision that will follow, but I wanted to raise the issue.\n\nRonald and I did a bit of interoperability testing wrt Digest\nAuthentication.. and we discovered what might be a problem (From the\nauth draft)\n\n::     4.6 Weakness Created by Multiple Authentication Schemes\n:: \n::     An HTTP/1.1 server may return multiple challenges with a 401\n::     (Authenticate) response, and each challenge may use a different scheme.\n::     The order of the challenges returned to the user agent is the order that\n::     the server would prefer they be chosen. The server should order its\n::     challenges with the \"most secure\" authentication scheme first. A user\n::     agent should choose to use  the first challenge it understands and\n::     request credentials from the user based upon that challenge.\n:: \n\nMy problem is that if my server doesn't list Basic as the first choice\n(but does list is as less preferred to Digest) some existing 1.0 clients\nthat can't do digest but can do basic don't realize that basic is an\noption.. and if I do list basic first then I am suggesting to clients\n(as per 4.6) that I want that more than digest, which is definitely\nnot the case.\n\n                                   (basic,digest)     (digest, basic)\nlynx 2.8.1 dev 17(and prev)             Y       N\nnetscape 4.05Y       Y\nmsie 4Y       N\n\nY = interoperable\n\n(netscape has some strange behavior.. it will send basic responses no\nmatter the challenge type... even if only digest!)\n\nI'm generally in favor of 1 of two paths to resolve the issue: \n    1] remove the notion of server specified preference.. the\n    credentials belong to the client, it seems to me they should\n    understand what the risks are in sending them out on the\n    network. In this way the challenge specifes understandable\n    methods only.\n\n    2] introduce q values\n\nthoughts?\n\n-P\n\n     \n\n\n\n", "id": "lists-012-4307642"}, {"subject": "Re: Minutes of HTTP/1.1 editorial teleconference, July 23, 199", "content": "Jim Gettys:\n>\n[...]\n>* h6a  MISTAKES\n[...]\n>b) The consensus was the invalidation was in fact correct, has been a \n>requirement for a long time given language existing even in 1945, and \n>it would be dangerous to remove it.  At least, that is what I remember\n>while in the basement with a scared 3 year old near by over the static\n>in the phone.\n\nNeedless to say I disagree with the consensus of the phone\nconference.  I can't find any language about this matter in 1945 or\n2068, so please point me to it.  Also, the requirement has no teeth\nanyway because a system is always free to act as a tunnel, not as a\ncache, for certain requests.  \n\nI could change my mind about this issue if someone would provide some\nevidence that current practice actually conforms to the proposed new\nrequirment.  As far as I know currently deployed systems do not, and\nthe new requirement would only shorten the useful lifetime of the\nprotocol.\n\nKoen.\n\n\n\n", "id": "lists-012-4317207"}, {"subject": "Re: Rev03 changes to 19.2, multipart/byterange", "content": "Last week I wrote:\n> \n> I propose here corrective wording for open issue RANGEDELIM (described\n> in <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0141.html>).\n> [...]\n> Now, HTTP introduces an interesting ambiguity, because the first line\n> of an entity body could well be considered the beginning of a line, and\n> it follows a CRLF.  However, since we must ignore the CRLF that\n> separates the headers from the body, the body does not really begin\n> with a CRLF.  So by my reading of RFC 2046, an HTTP multipart entity\n> would have to include an extra CRLF preceding the boundary.  I have\n> amended Section 19.2 of the HTTP spec. accordingly, along with the\n> example.\n\nAfter discussions over the weekend, especially exchanges with Ned Freed,\nco-author of RFC 2046, I am amending my proposed changes.  In\nparticular, most of them go away.  It turns out that a careful reading\nof RFC 2046 shows that an extra leading CRLF is unnecessary, and that\nthe HTTP example was correct.  (Details:  the multipart-body grammar\nbegins \"[preamble CRLF]\", which makes the cruft that often precedes\nmultipart bodies, plus the CRLF that ends it, optional.)\n\nThus the only changes that remain are to reference RFC 2046, to caution\nabout quoted boundary strings, and to add 2046 to the reference section.\n\nDave Kristol\n===================\n\n   19.2 Internet Media Type multipart/byteranges\n \n   When an HTTP 206 (Partial Content) response message includes the\n   content of multiple ranges (a response to a request for multiple\n   non-overlapping ranges), these are transmitted as a multipart\n   message-body (RFC 2046). The media type for this purpose is called\n   \"multipart/byteranges\".\n \n[Following the example:]\nCaution:  Although RFC 2046 permits the boundary string to be quoted,\nsome existing implementations handle a quoted boundary string\nincorrectly.\n \n   19.2.1 Multipart/x-byteranges\n   ...\n \n Note to Jim Gettys:  need to add reference to RFC 2046:\n     Freed, N., and N. Borenstein. \"Multipurpose Internet Mail\n     Extensions (MIME) Part Two: Format of Internet Message Bodies.\" RFC\n     2046, Innosoft, First Virtual, November 1996.\n\n\n\n", "id": "lists-012-4327848"}, {"subject": "Re: Rev03 changes to 19.2, multipart/byterange", "content": "John Franks <john@math.nwu.edu> wrote:\n  > On Mon, 27 Jul 1998, Dave Kristol wrote:\n  > > [...]\n  > > After discussions over the weekend, especially exchanges with Ned Freed,\n  > > co-author of RFC 2046, I am amending my proposed changes.  In\n  > > particular, most of them go away.  It turns out that a careful reading\n  > > of RFC 2046 shows that an extra leading CRLF is unnecessary, and that\n  > > the HTTP example was correct.  (Details:  the multipart-body grammar\n  > > begins \"[preamble CRLF]\", which makes the cruft that often precedes\n  > > multipart bodies, plus the CRLF that ends it, optional.)\n  > > \n  > \n  > Enough people have been confused by this that it is important to\n  > have an explicit warning that the leading CRLF is allowed and optional.\n\nI agree it's confusing, and subtle.  How about if there are two cautions:\n\nCautions:\n  1) Additional CRLFs may precede the first boundary string in the entity.\n\n  2) Although RFC 2046 permits the boundary string to be quoted, some\n  existing implementations handle a quoted boundary string incorrectly.\n\nDave Kristol\n\n\n\n", "id": "lists-012-4337630"}, {"subject": "Re: Rev03 changes to 19.2, multipart/byterange", "content": "On Mon, 27 Jul 1998, Dave Kristol wrote:\n\n> Last week I wrote:\n> > \n> > I propose here corrective wording for open issue RANGEDELIM (described\n> > in <http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0141.html>).\n> > [...]\n> > Now, HTTP introduces an interesting ambiguity, because the first line\n> > of an entity body could well be considered the beginning of a line, and\n> > it follows a CRLF.  However, since we must ignore the CRLF that\n> > separates the headers from the body, the body does not really begin\n> > with a CRLF.  So by my reading of RFC 2046, an HTTP multipart entity\n> > would have to include an extra CRLF preceding the boundary.  I have\n> > amended Section 19.2 of the HTTP spec. accordingly, along with the\n> > example.\n> \n> After discussions over the weekend, especially exchanges with Ned Freed,\n> co-author of RFC 2046, I am amending my proposed changes.  In\n> particular, most of them go away.  It turns out that a careful reading\n> of RFC 2046 shows that an extra leading CRLF is unnecessary, and that\n> the HTTP example was correct.  (Details:  the multipart-body grammar\n> begins \"[preamble CRLF]\", which makes the cruft that often precedes\n> multipart bodies, plus the CRLF that ends it, optional.)\n> \n\nEnough people have been confused by this that it is important to\nhave an explicit warning that the leading CRLF is allowed and optional.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-4346522"}, {"subject": "Fwd: HTTP working group mailing list... is flaky; use httpwg%cuckoo.hpl.hp.com&#64;hplb.hpl.hp.co", "content": "Use http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com until Andy gets this fixed.\n\nI'll try to forward mail that might have bounced later today.\n- Jim\n\n--\nJim Gettys\nDigital Industry Standards and Consortia\nCompaq Computer Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\nattached mail follows:\nHi Jim.\n\nRecently you wrote:\n\n> Has gotten very flaky the last few weeks...\n\nYep.  I'll kick a sysadmin or two.  In the meantime, better to use:\n\n  http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n\n-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-012-4356717"}, {"subject": "Charset issue from issue MISTAKE", "content": "Koen,\n\nIn http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0520.html\n\nyou wrote about the removal of the sentence:\n   The ISO-8859-1 character set can be assumed to be acceptable to all user\n   agents.\n\narguing that it was a mistake, but that you couldn't \"remember that there was\nany rationale or discussion on the list for deleting it.\"\n\nBut in http://www.findmail.com/list/http-wg/7596.html (November 26, 1997)\n\nyou said, in response to a suggestion to remove this sentence:\n\n> I agree, this sencence should be removed.\n\nI think the earlier rationale is still valid, and that section 14.2\ndoes not require any additional edits.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-4365563"}, {"subject": "resend: LAST last meeting of HTTP WG, August 24 19302200, Chicag", "content": "(resend to http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com)\n\nIt looks as if there are enough items to discuss that I requested\na working group meeting at the upcoming IETF meeting in Chicago.\nWe're scheduled for\n\nMonday, August 24 at 1930-2200\n     other groups scheduled at that time: usefor, policy-BOF, pktway, entmib\n\nThe agenda will focus on review of the implementation reports, and\nany area director or IESG comments.\n\nIt is my hope that \n - final versions of both drafts will be submitted\n - all of the changes since the last 'last call' will have been discussed\n   in the working group in sufficient detail that an additional working\n   group last call will not be necessary\n - we will have forwarded the documents to the area director(s) for review\n - Those IESG members who have potential concerns about progressing HTTP/1.1\n   to Draft Standard will attend and express those concerns.\n\nDetails about IETF meetings are at http://www.ietf.org.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-4373763"}, {"subject": "Re: Digest Authentication Challenge Orderin", "content": "Patrick McManus wrote:\n\n> My problem is that if my server doesn't list Basic as the first choice\n> (but does list is as less preferred to Digest) some existing 1.0 clients\n> that can't do digest but can do basic don't realize that basic is an\n> option.. \n\nBut those are buggy even with respect to 2068; it's one thing for us to aim\nfor compatibility with software that conformed to the Proposed Standard, but\nto maintain compatibility with something that was broken with respect to\nthem can be too difficult a standard to meet.\n\n> I'm generally in favor of 1 of two paths to resolve the issue:\n\n>     1] remove the notion of server specified preference.. the\n>     credentials belong to the client, it seems to me they should\n>     understand what the risks are in sending them out on the\n>     network. In this way the challenge specifes understandable\n>     methods only.\n\nIt is only a preference, and does not mandate anything - a browser could\nsend basic when it might have sent digest; since the server cannot tell what\nits complete capabilities are, it makes no difference.  Either basic is\nacceptable or not.  Eventually, I hope that digest will be supported well\nenough in browsers that I can recommend to my customers that they turn off\nsupport for basic, but we've a long way to go...\n\n>     2] introduce q values\n\nThat only makes the situation worse by increasing the number of browsers\nthat don't understand what you are doing.\n\n-- \nScott Lawrence            Consulting Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.   Embedded Web Technology     http://www.agranat.com/\n\n\n\n", "id": "lists-012-4381975"}, {"subject": "Authentication issue REQUESTDIGES", "content": "Dave Kristol wrote in\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0031.html\n\n\n# Unfortunately, there *is* no \"definition ... above\".  The non-terminal\n# request-digest has no syntactic definition.  I suspect it should be\n# request-digest = <\"> *LHEX <\">\n\nIn lieu of any objection, this seems like a reasonable resolution of this issue.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-4391190"}, {"subject": "Authentication issue CNONCE: Proposed resolutio", "content": "(I'm going through the Authentication issue list\nhttp://www.w3.org/Protocols/HTTP/Issues/ \nseeing if there are actually proposed resolutions of the open issues):\n\nIn http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0031.html\nDave Kristol wrote:\n\n# 3.2.3 The Authentication-Info Header\n# cnonce and qop are used in the calculation of response-digest.  The\n# client is not required to send either cnonce= or auth=.  So I assume\n# (correct?) that the null string is used for values for omitted\n# attributes in the calculation.\n\nI suggest that this be the correct interpretation, that the null\nstring is used for values for omitted attributes in the calculation.\n\n# If (to use cnonce as the example) cnonce was omitted, should\n# Authentication-Info omit cnonce, or should it send cnonce=\"\"?  Same\n# question for auth.\n\nI propose that either MAY be allowed, since they are equivalent.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-4398312"}, {"subject": "Authentication issue: NONCEETAG proposed resolution (to leave as is", "content": "In http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0035.html\nDave Kristol wrote about problems with the example nonce\ngiven in section 3.2.1.\n\n\"I think this example for nonce is a poor one...\"\n\nciting two reasons: (a) using ETag ties the nonce to a given URI\nand (b) some resources may not have an ETag.\n\nHowever, this is just an example of what a nonce might be, rather than\nnormative text, and the drawbacks that Dave cites don't affect the\nsecurity of the nonce, but rather the performance of nonce reuse,\nand the domain of applicability of the example.\n\nSo I propose that we leave the text as is. I considered recommending\na disclaimer, but I consider the existing disclaimer\n\n# The contents of the nonce are implementation dependent. The quality\n# of the implementation depends on a good choice. \n\nsufficient.\n\nProposed resolution: leave as is.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-4407187"}, {"subject": "MUST-MAYSHOULD (MMS) audit..", "content": "There has been no comments on Jeff and Scott's massive audit.  I suspect \neveryone who has looked at them has had their eyes glaze over; I know \nI have.  But driving the keyboard means I actually had to pay attention \nto all of the diffs (all 222 of them!). I noted a few problems.  All, in \nall, I think they did a great job. I encourage other brave hearted people\nto review their work as well.\n\nHere's what I noted of significance that differs from their recommendation:\n\nMMS 023:\nMessage bodies are often NOT optional in the protocol (they may in fact\nbe required for certain responses), so the use of optional in this\ncontext is confusing. So turning optional into OPTIONAL is clearly\nwrong, as it may not be optional. I said \"possibly a message body\" to\nremove the confusion.\n\nMMS 036:\n\nThis is really a SHOULD requirement that the client may presume unless\notherwise told in the specification that connections persist: and I added\na clarification: \"even after error responses from the server.\", as for\nevolution's sake, it is important that you be able to deliberatly try\nsomething that will return an error, and then continue.  This may\ncost a round trip, but at least avoids another TCP connection, which\nhistory has shown can generate major problems. So it reads:\n\n\"That is, unless otherwise indicated, the client SHOULD assume that the \nserver will maintain a persistent connection, even after error responses \nfrom the server.\"\n\nThis also provides clarification for Art Goldberg.\n\nMMS 144:\n\"Users MAY\" doesn't make sense, and can't be normative; we have no\ncontrol over people's behavior.\n\nMMS 168:\nScott's comments are probably correct, that this is so stupid as not\nto be worth being in the spec. As I remember, it was to quell\npeople who were afraid there would be some requirement to implement\nmultipart range requests, pointing out that if they don't ask\nfor them, they won't get them in return.\n\nBut for now, it is... \nHowever:\n\"should not\" -> \"MUST NOT\"; if a client is stupid enough to ask for\na multipart byte range and can't understand it, it will in fact cause\nhim not to be able to understand the response, and lose in various ways.\nSo the should not should be a MUST NOT.\n\nMMS 184:\n\nI believe Scott is correct: this SHOULD should be a MUST.  If it is\nleft a SHOULD it would be inconsistent with the following MUST NOT and\nMUST requiring correct behavior when Max-Forwards is being used.  If it\nisn't a MUST, an implementation might still forward expired requests,\ndefeating the debugging purpose of Max-Forwards in the first place.\n\nMMS 190:\n\nI think Scott is wrong on this one.  The should->SHOULD is in text which \nis recommendations to developers (that they support ranges), not requirements \non the implementation.  Putting a SHOULD into a clearly optional feature, \nwhen the previous sentence says it MAY be ingnored, is clearly wrong.\n\nMMS 209:\n\nThe first should is really recommendation, and can't be normative as\n\"should be conservative in offering accept header configuration\noptions to end users\" doesn't have any meaning that could be enforced.\n\nThe second should can be a SHOULD, as this warning is possible, and\nwhat \"which provide a high degree of header configurability\" is left\nto rational people (are implementer's rational? :-)) to decide.\n\nMMS 219:\n\nThe should is NOT normative to our specification (it is normative in MHTML); \nit is just pointing out to implementers that they should be careful and \nshould pay attention to the MHTML spec.  I reworded it to avoid the use \nof should. \n\n- Jim\n\n\n\n", "id": "lists-012-4415054"}, {"subject": "MISTAKES, section 8.2.", "content": "In the process of changing the SHOULD to a MAY, and the cleanup of the \nfirst sentence, I realized that most of the section needs rewrite to properly \ncover pipelining of a sequence of requests; as written, it only talked \nabout individual requests.  Here's my rewrite for sanity's sake.\n\nLet me know if anyone sees a problem.\n- Jim\n\nOld:\n8.2.3 Automatic Retrying of Requests\n\nIf a user agent sees the transport connection close before it receives \na final response to its request, if the request method is idempotent (see \nsection 9.1.2), the user agent SHOULD retry the request without user \ninteraction. If the request method is not idempotent, the user agent SHOULD \nNOT retry the request without user confirmation. (Confirmation by user-agent \nsoftware with semantic understanding of the application MAY substitute \nfor user confirmation.)\n\nNew:\n8.2.3 Automatic Retrying of Requests\n\nIf a user agent sees the transport connection close before it receives \nall of the final response to its request or sequence of requests, if the \nrequests or sequence are idempotent (see section 9.1.2), the user agent \nMAY retry the request or sequence without user interaction. If the request \nmethod or sequence is not idempotent, the user agent SHOULD NOT retry \nthe request without user confirmation. (Confirmation by user-agent software \nwith semantic understanding of the application MAY substitute for user \nconfirmation.)\n\n--\nJim Gettys\nDigital Industry Standards and Consortia\nCompaq Computer Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n", "id": "lists-012-4425311"}, {"subject": "PROXYDNS resolution", "content": "Unless I hear complaint, I plan to apply this to rev-04.\n- Jim\n\nFrom: Paul Leach <paulle@microsoft.com>\nDate: Mon, 27 Jul 1998 14:04:46 -0700\nTo: \"'jg@pa.dec.com'\" <jg@pa.dec.com>, frystyk@w3.org,\nmasinter@parc.xerox.com,\n        mogul@pa.dec.com, lawrence@agranat.com, fielding@ics.uci.edu,\n        dmk@research.bell-labs.com\nSubject: RE: URGENT....  I need your drafts by the end of today (Monday)..\n        .\n-----\nOLD:\n10.5.5 504 Gateway Timeout\n  The server, while acting as a gateway or proxy, did not receive a timely\n  response from the upstream server it accessed in attempting to complete\n  the request.\n\nNEW:\n10.5.5 504 Gateway Timeout\n  The server, while acting as a gateway or proxy, did not receive a timely\n  response from the upstream server specifued by the URI (e.g., HTTP, FTP,\nLDAP)\n  or some other auxiliary server (e.g., DNS) it needed to access in\nattempting\n  to complete the request.\n        Note: Implementers should be aware that some deployed proxies\n        are known to return 400 or 500 when DNS lookups time out.\n\n\n\n", "id": "lists-012-4433875"}, {"subject": "Re: Minutes of HTTP/1.1 editorial teleconference, July 24, 199", "content": "Several of us managed to get together on a teleconference again today to\ndiscuss remaining issues: these were myself, Henrik Frystyk, Dave Kristol,\nand Paul Leach.\n\nIssues discussed (and usually resolved): IEBUG, MISTAKES, WARNGEN, VERSION \nsee the minutes below.\n\nIEBUG: We discussed IEBUG some more; Paul will check further with Josh Cohen\non this.  It isn't clear any action should be taken; but we don't\nfully remember why the response was made a 416 code rather than a 200 series\ncode, and this is nagging us.  Jeff Mogul was not available for the call,\nand he drafted the 416 description.  I will also check with Jeff to see\nif he remembers the reason.\n\nMISTAKES: \nSection 14.2: This change was made due to bug CHARSET-NIT editorial issue\nand applied in Rev-02; you should have been able to catch this one\nearlier.  I don't keep such a detailed issues list just for my health,\nyou know; it is for all our benefits as a sanity check on possible mistakes\nslipping in.\n\nSee in the old issues list at \nhttp://www.w3.org/Protocols/HTTP/Issues/BeforeLastCall.html#CHARSET-NIT. \nI might have made a mistake here, but Larry wasn't on our call, and he \nfields our character set questions.  Let this serve as a reminder that \nall issues, including ones I class as editorial, should be looked over.  \nPlease explain why I was wrong (if indeed I was)...  At the time, it looked \nto me like Howard was clearly right, and that was the best solution, but \nI'm not a character set guru.\n\nSection 4, bullet 4 vs. 14.35.1.  We discussed this at length, and agree\nthat there is an implicit assumption here that may be best to make explicit.\nPaul Leach agreed to try to draft some words on this one, and a way to\nexplain why the requirement exists.  As usual, this is a subtle problem\ninvolving various versions of proxies, to keep it complicated.\n\nWARNGEN:\nIt is really the same as the WARN-GEN editorial issue further down; there\nwas one place where in applying the changes, that a bit of care in the\nchanging words needed to be applied; Henrik and I figured out exactly which\nword to change in this case.  This is now an editorial issue.\n\nVERSION:\nUnless there is objection, consider this a last call for the rewrite in\nJeff's message.\n\nWe also discussed the problem that when you do a PUT, you have no way\nto get an entity tag back in the response to the initial PUT.  This\nleaves a race where someone might update a resource before you can\nget your hands on the Etag to do your own conditional operations.\nHenrik, Dave and Paul continued discussing this after I had to go\nto a doctor's appointment.  I suspect that this will be deferred to\na future working group, but don't know the outcome.\n\n\n\n", "id": "lists-012-4442942"}, {"subject": "Re: Authentication issue CNONCE: Proposed resolutio", "content": "Larry Masinter wrote:\n\n> In http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0031.html\n> Dave Kristol wrote:\n> \n> # 3.2.3 The Authentication-Info Header\n> # cnonce and qop are used in the calculation of response-digest.  The\n> # client is not required to send either cnonce= or auth=.  So I assume\n> # (correct?) that the null string is used for values for omitted\n> # attributes in the calculation.\n> \n> I suggest that this be the correct interpretation, that the null\n> string is used for values for omitted attributes in the calculation.\n> \n> # If (to use cnonce as the example) cnonce was omitted, should\n> # Authentication-Info omit cnonce, or should it send cnonce=\"\"?  Same\n> # question for auth.\n> \n> I propose that either MAY be allowed, since they are equivalent.\n\nI think that this is an acceptable resolution, but that the Security\nConsiderations section will need a short paragraph on the implications of\nleaving this out - the server is then not authenticated to the user agent.\n\n-- \nScott Lawrence            Consulting Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.   Embedded Web Technology     http://www.agranat.com/\n\n\n\n", "id": "lists-012-4454921"}, {"subject": "Re: Progress on testing HTTP/1.1", "content": "Further testing has taken place; I urge anyone who has not submitted\nimplementation reports to do so.  For the core specification, we are\ndown to weak tags, TE, and Trailers at this instant, and may in fact\nptentially be within the rules of IESG as soon as we get a second client\nto finish weak entity tag testing.  In any case, the core spec is almost\ndone.  We still need work for the authentication spec.  My thanks to those\nwho are sending updates to their reports.\n\nI've set up a number of tests on http://zap.w3.org using Apache.  Yves\nLafon has also been setting up some tests on http://jigsaw.w3.org/HTTP.\n\nSee: http://www.w3.org/Protocols/HTTP/Forum/Reports/\n\nThe list is now down to:\nClients         Servers         Proxies         Feature\n 1t  5y  7n  1-  3t  5y 10n  0-  1t  3y  4n  0- H 13.3.3        Weak entity tags\n 3t  3y  8n  0-  2t  2y 14n  0-  1t  1y  6n  0- H 14.39 TE\n 3t  2y  9n  0-  1t  2y 15n  0-  0t  2y  6n  0- H 14.40 Trailer\n 2t  0y 12n  0-  4t  5y  9n  0-  1t  0y  7n  0- A 3.2.1 WWW-Authenticate Digest\n 1t  0y 13n  0-  2t  1y 15n  0-  0t  0y  8n  0- A 3.2.1 qop-options auth\n 1t  0y 13n  0-  1t  0y 17n  0-  0t  0y  8n  0- A 3.2.1 qop-options auth-int\n 2t  0y 12n  0-  4t  5y  9n  0-  1t  0y  7n  0- A 3.2.2 Authorization Digest\n 1t  0y 13n  0-  2t  1y 15n  0-  0t  0y  8n  0- A 3.2.2 request qop auth\n 1t  0y 13n  0-  1t  0y 17n  0-  0t  0y  8n  0- A 3.2.2 request qop auth-int\n 2t  0y 12n  0-  3t  2y 13n  0-  1t  0y  7n  0- A 3.2.3 Authentication-Info Digest\n 1t  0y 13n  0-  1t  2y 15n  0-  0t  0y  8n  0- A 3.2.3 response qop auth\n 1t  0y 13n  0-  1t  0y 17n  0-  0t  0y  8n  0- A 3.2.3 response qop auth-int\n 2t  0y 12n  0-  1t  1y 12n  4-  1t  0y  7n  0- A 4.2   Proxy-Authenticate Digest\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy qop-options auth\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy qop-options auth-int\n 2t  0y 12n  0-  1t  1y 12n  4-  1t  0y  7n  0- A 4.2   Proxy Authorization Digest\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy request qop auth\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy request qop auth-int\n 1t  1y 12n  0-  1t  0y 13n  4-  1t  0y  7n  0- A 4.2   Proxy Authentication-Info Digest\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy response qop auth\n 0t  1y 13n  0-  0t  0y 14n  4-  0t  0y  8n  0- A 4.2   Proxy response qop auth-int\n\n- Jim\n\n\n\n", "id": "lists-012-4463881"}, {"subject": "ISSUE: Warning as a general header...", "content": "At 11:39 7/25/98 -0700, Jim Gettys wrote:\n>I've done one scan trying to catch all the Warning usages that should\n>change...\n\nAs we talked about, when the warning header is described directly in\nconnection with a cache then it should be \"response\" and not message as a\ncache by definition only works on responses. That is, 1xx codes only make\nsense for response, 2xx codes can be applied both to requests and responses.\n\nI don't know if it is of any use to define directions on future codes - I\ndon't like the codes in the first place. Had they been URIs then I could\nhave used them in Mandatory and 118n problems would have been the same as\nfor any other resource. If anybody else would like to do that then feel free!\n\nSection 13.1.2\n\nWhenever a cache returns a response that is neither first-hand nor \"fresh\nenough\" (in the sense of condition 2 in section 13.1.1), it must attach a\nwarning to that effect, using a Warning response-header. This warning\nallows clients to take appropriate action\n\nThe \"must\" should be a MUST in the paragraph above. \"response-header\"\nshould be \"general-header\".\n\nIn the section a bit further down:\n\nHTTP/1.0 caches will cache all Warnings, without deleting the ones in the\nfirst category. Warnings that are passed to HTTP/1.0 caches carry an extra\nwarning-date field, which prevents a future HTTP/1.1 recipient from\nbelieving an erroneously cached Warning.\n\nChange \"HTTP/1.0 caches will cache all Warnings\" to \"HTTP/1.0 caches will\ncache all Warnings in responses\".\n\nChange \"Warnings that are passed to HTTP/1.0 caches\" to \"Warnings in\nresponses that are passed to HTTP/1.0 caches\"\n\nSection 13.5.2\n\nA non-transparent proxy MAY modify or add these fields in a response that\ndoes not include no-transform, but if it does so, it MUST add a Warning 114\n(Transformation applied) if one does not already appear in the response.\n\nChange \"in a response\" to \"in a message\" and \"if one does not already\nappear in the response\" to \"if one does not already appear in the message\"\n\nSection 14.46\n\nThe Warning response-header field is used to carry additional information\nabout the status of a response which may not be reflected by the response\nstatus code. This information is typically, though not exclusively, used to\nwarn about a possible lack of semantic transparency from caching operations.\n\nChange \"The Warning response-header field is used to carry additional\ninformation about the status of a response which may not be reflected by\nthe response status code\" to \"The Warning general-header field is used to\ncarry additional information about the status or transformation of a\nmessage which may not be otherwise reflected in the message.\"\n\nChange \"This information is typically, though not exclusively, used to warn\nabout a possible lack of semantic transparency from caching operations.\" to\n\"This information is typically used to warn about a possible lack of\nsemantic transparency from caching operations or of transformations applied\nto the entity body of the message.\"\n\nFurther down\n\nAny server or cache may add Warning headers to a response. New Warning\nheaders should be added after any existing Warning headers. A cache MUST\nNOT delete any Warning header that it received with a response. However, if\na cache successfully validates a cache entry, it SHOULD remove any Warning\nheaders previously attached to that entry except as specified for specific\nWarning codes. It MUST then add any Warning headers received in the\nvalidating response. In other words, Warning headers are those that would\nbe attached to the most recent relevant response.\n\nChange the whole paragraph to\n\nWarning headers can in general be applied to any message, however some\nspecific warn-codes are specific to caches and can only be applied to\nresponse messages. New Warning headers should be added after any existing\nWarning headers. A cache MUST NOT delete any Warning header that it\nreceived with a message. However, if a cache successfully validates a cache\nentry, it SHOULD remove any Warning headers previously attached to that\nentry except as specified for specific Warning codes. It MUST then add any\nWarning headers received in the validating response. In other words,\nWarning headers are those that would be attached to the most recent\nrelevant response.\n\nNext paragraph\n\nWhen multiple Warning headers are attached to a response, the user agent\nSHOULD display as many of them as possible, in the order that they appear\nin the response. If it is not possible to display all of the warnings, the\nuser agent should follow these heuristics:\n  - Warnings that appear early in the response take priority over those\nappearing later in the response.\n  - Warnings in the user's preferred character set take priority over\nwarnings in other character sets but with identical warn-codes and\nwarn-agents.\n\nI am surprised that none of the GUI folks have complained about this\nSHOULD. It isn't a protocol requirements and interferes with GUI policies.\nIf we want this paragraph then it SHOULD at least be a lowercase \"should\".\n\nIn the paragraph\n\n1XXWarnings that describe the freshness or revalidation status of the\nresponse, and so MUST be deleted after a successful revalidation.\n\nAdd the sentence\n\n1XX warn-codes MAY be generated by a cache only when validating a cached\nentry. It MUST NOT be generated by clients.\n\nComments?\n\nHenrik\n\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4473009"}, {"subject": "ISSUE: Returning metainformation on a 201 (Created) respons", "content": " This was discussed between Paul Leech, Dave Kristol, and myself in\nFriday's teleconference.\n\nPROBLEM: When doing a PUT on a resource that already exists, the server can\nsend back a 200 or a 204 response. The 204 response allows the server to\nsend back an updated etag and other entity information about the resource\nthat has been affected by the PUT operation. This allows the client to do\nthe next PUT using the If-Match precondition to ensure that edits are not\nlost.\n\nThere were some discussion of whether the etag is the *only* piece of\ninformation that the client doesn't know (not included in the request). The\nfeeling was that in general this can not be assumed as the operation may\ntrigger server side dependencies that the server wants to communicate back\nto the client.\n\nWhen doing a PUT for *the*first*time* the resource is created on the server\nand a 201 response is sent back. The semantics of the 201 indicates that it\ncan include an entity body. In section 9.5 (although this is in the\ndescription of POST), it is stated that \n\nIf a resource has been created on the origin server, the response SHOULD be\n201 (Created) and contain an entity which describes the status of the\nrequest and refers to the new resource, and a Location header (see section\n14.30).\n\nHence, the server can not send information back on a newly created resource\nusing 201 as it can when updating an existing resource using 204.\n\nIt was discussed whether anybody does include an entity in a 201 response\nas it would be a challenge for the editor to deal with the response entity\n(it would have to be displayed in a new window, for example).\n\nFour solutions were discussed:\n\n1) When receiving a 201 response, the client can issue a HEAD request to\nget the updated information. However, the cost of this is one additional\nRTT and it may lead to race conditions if another client comes in a does a\nPUT between the first PUT and the follow-up HEAD request. This could cause\nthe client to apply inconsistent metainformation to its version of the\nentity body.\n\n2) Introduce a new status code which has the semantics of 204 except that\nit can only be used when a resource has been created. The problem here is\nthat existing editors will not understand this code and will default to 200\nwhich is will not indicate that a resource has been created.\n\n3) Do not allow entity bodies in a 201 response and say that all\ninformation included in the response is about the newly created resource.\nThe problem here is that we don't know if anyone already sends back entity\nbodies in a 201 response.\n\n4) Change the definition of 201 to distinguish between the case when there\nis an entity body and when there isn't:\n\nIf the response does not contain an entity-body but includes new or updated\nmetainformation in the form of entity-headers, then these header fields\nSHOULD be associated with the created requested variant. If an entity body\nis present in the response then all entity header fields apply to that\nentity body.\n\nAny 201 response can contain an etag indicating the current value of the\nentity tag for the created requested variant, see [14.19].\n\n\nSee [14.30] for use of the Location response header field in a 201 response.\n\nThe consensus among us, I believe, is to propose 4).\n\nComments?\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4485417"}, {"subject": "RE: MISTAKES, section 8.2.", "content": "> If a user agent sees the transport connection close before it receives \n> all of the final response to its request or sequence of requests, if the \n> requests or sequence are idempotent (see section 9.1.2), the user agent \n> MAY retry the request or sequence without user interaction. If the request \n> method or sequence is not idempotent, the user agent SHOULD NOT retry \n> the request without user confirmation. (Confirmation by user-agent software \n> with semantic understanding of the application MAY substitute for user \n> confirmation.)\n\nIf blah-blah-1, if blah-blah-2, then yadda-yadda-a. If blah-blah-3, then\nyadda-yadda-b. (Parenthetical-c).\n\n-->\n\nIf blah-blah-1:\n     If blah-blah-2 then yadda-yadda-a.\n     If blah-blah-3 then yadda-yadda-b. (Parenthetical-c).\n\nwould be clearer.\n\n\n\n", "id": "lists-012-4496900"}, {"subject": "ISSUE: Expect Header Field Proble", "content": " The current wording in section 14.20 is nonsense as it renders all\nexisting HTTP/1.1 servers not compliant:\n\nThe Expect request-header field is used to indicate that particular server\nbehaviors are required by the client. A server that does not understand or\nis unable to comply with any of the expectation values in the Expect field\nof a request MUST respond with appropriate error status.\n\nPeople  not having read the spec before or not knowing about HTTP\nimplementations will get very surprised if they expect this behavior. The\nMUST MUST be changed to something like this:\n\nThe Expect request-header field can be used to indicate that particular\nserver behaviors are preferred by the client. The client is not guaranteed\nthat the server recognizes the information in an Expect request header field.\n\nThe stronger semantics of a MUST is captured by the Mandatory proposal\nwhich is under way on the HTTP-EXT mailing list.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4505311"}, {"subject": "Issue: TEIDENTIT", "content": " ISSUE: TE-IDENTITY. Action on Henrik: Add OPTIONAL for how the server\nhandle it. That is, it is optional for the server to look at it but if it\ndoes then it SHOULD send 406 if it can't respond in the transfer encoding.\nWe shouldn't disallow identity; q=0.\n\nIn section 14.39\n\nChange\n\nThe TE request-header field is similar to Accept-Encoding, but restricts\nthe transfer-codings (section 3.6) that are acceptable in the response.\n\nto\n\nThe TE request-header field is similar to Accept-Encoding, but indicates\nthe transfer-codings (section 3.6) that are preferred in the response. The\nclient is not guaranteed that the server recognizes the information in a TE\nrequest header field.\n\nComments?\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4513629"}, {"subject": "ISSUE CREATE: Proposed resolutio", "content": "After further discussions with Jim, we propose the following resolution:\n\nClarify the existing possibility of including an etag in a 201 response and\ndon't do anything about potential other metainformation as this is a\nproblem in many other places as well. Basically, in HTTP it's hard to know\nwhat is metainformation and what is transaction information.\n\nAction: Add this clarification to section 10.2.2:\n\nA 201 response MAY contain an Etag response header field\nindicating the current value of the entity tag for the\ncreated requested variant, see [14.19].\n\nAny objections or comments?\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4521390"}, {"subject": "Re: ISSUE: Returning metainformation on a 201 (Created) respons", "content": "At 09:23 7/27/98 -0700, Jim Gettys wrote:\n>\n>a) I'm nervous about adding anything like this at this date.  I'd be happier\n>with a separate RFC describing what should happen...  This should have\n>been dealt with months ago.\n\nI guess that applies to many of the edits performed now - the reason is\nprobably that nobody has implemented PUT with etag preconditions until now\n- if you want to play with it then you can try out my implementation using\nthe Web Commander - a libwww sample application [1].\n\n>b) 4) seems a gross hack to me, adding another HEAD class special case\n>to the protocol.\n\nI didn't say that it is beautiful but it provides the needed functionality\nwithout breaking existing applications. If people don't care then we can\ndream up a new scheme altogether and don't have to be concerned about it here.\n\n>c) I suggest (described in a separate ID) a new header that conveys the\n>ETAG.  This solves the race condition.  To get any additional metadata\n>that might have changed, you'd perform a HEAD right after the PUT.\n\nThis solution doesn't quite work the same way and doesn't have as useful\nsemantics (special cases etags and allows race conditions on other\nmetainformation).\n\n>d) you should go see if the WEBDAV folks have worried about this case;\n>they are sloppy enough they may have missed it, but then again, they\n>may have found it and have another fix for it...\n\nI have forwarded the mail to the webdav mailing list.\n\nActually the current wording of etags in 14.19 is adequate as (if?) the\nrequested variant in a first time PUT can be said to be the \"requested\nvariant\". Whether that is obvious is another question - and of course it\ndoesn't work with other metainformation:\n\nThe ETag response-header field provides the current value of the entity tag\nfor the requested variant.\n\nHenrik\n\n[1] http://www.w3.org/WinCom/\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4529821"}, {"subject": "IEBU", "content": "    IEBUG: We discussed IEBUG some more; Paul will check further with\n    Josh Cohen on this.  It isn't clear any action should be taken; but\n    we don't fully remember why the response was made a 416 code rather\n    than a 200 series code, and this is nagging us.  Jeff Mogul was not\n    available for the call, and he drafted the 416 description.  I will\n    also check with Jeff to see if he remembers the reason.\n    \nI'm not sure we gave any deep theoretical analysis to the choice\nbetween 2xx and 4xx.\n\nTo review the reason for including the 416 response: without a specific\n\"Range not satisfiable\" error, the client cannot tell the difference\nbetween (inadvertently) requesting something beyond the end of the\nresource, and simply failing to get any response at all.  I believe\nthat Henrik discovered this problem while doing one of his\nimplementations.\n\nWhy might the client legitimately read past the end of a resource?\nWell, consider the case of a client that wants to find out the\nscreen-size of a GIF image (to allow early rendering of text) without\nblocking while the entire image downloads.  Assume that the client has\nseen some or all of the enclosing HTML file, which turns out to have\ntwo imbedded GIFs.  The client could do these requests (please excuse\nthe short-hand notation!) in a pipeline (i.e., without waiting\nfor any server replies):\n\nGET imagea.gif/Range: 0-1023\nGET imageb.gif/Range: 0-1023\nGET imagea.gif/Range: 1023-\nGET imageb.gif/Range: 1023-\n\nThis would be done with the assumption that all of the necessary\nscreen layout information is contained in the first 1024 bytes\nof the GIF file (actually, the number is probably a lot smaller,\nbut I'm using this as an arbitrary example).\n\nNow, suppose that these are really just tiny images (bullets, \nhorizontal rules, whatever) and imageb.gif is actually only\n512 bytes long.  We don't want return a \"hard\" error to the\nclient, such as 404 Not Found, since that could imply instead\nthat the entire resource is now bogus.  So this is why we wanted\na separate code.  Note that by the time that the client receives\nthe 416 response for the second request for imageb.gif, it\nhas already received the first response, and so between the\ntwo responses (and the resource-length information in each)\nit can infer exactly why it got the 416 response.\n\nOK, back to 2xx vs. 4xx.  As I said, I don't think we gave it\nmuch thought at the time.\n\nRemember that the scenario above is not the only way to get\na 416 error.  It might be the most common one, but imagine\nthat we have a client interface that allows the user to select\na specific part of a document, and the user enters a bogus\nrange.  So, in some cases, this is really an \"error.\"\n\nSection 6.1.1 (Status Code and Reason Phrase) lists the\ncategories like this:\n\n    .  1xx: Informational - Request received, continuing process\n\n    .  2xx: Success - The action was successfully received, understood,\n       and accepted\n\n    .  3xx: Redirection - Further action must be taken in order to\n       complete the request\n\n    .  4xx: Client Error - The request contains bad syntax or cannot be\n       fulfilled\n\n    .  5xx: Server Error - The server failed to fulfill an apparently\n       valid request\n\nFor \" Requested range not satisfiable\" (RRNS, I'll call it), 1xx\nand 5xx are clearly not the right categories.  3xx \"Redirection\"\ndoesn't sound right.\n\n2xx is described as \"The action was successfully received, understood,\nand accepted\", but this seems to contradict the plain intent of\n416 (which is that the action was NOT accepted).  4xx is described\nas \"The request ... cannot be fulfilled\", which seems like it hits\nthe nail on the head.  So, from the plain language of the description\nin 6.1.1, 4xx is indeed the right category, and this is presumably\nwhat led us to pick 416.\n\nI'm not necessarily arguing that, if we had had the foresight\nto consider the implications of interoperating with a RFC2068-compliant\nclient that sends Range requests (and rather clever one, at that),\nthat we might not have gone against this basic distinction between\n2xx and 4xx ... although I can't for sure say that this would have\nsolved the problem for IE, and it might have created other ones.\n\nBut I think we would be making a mistake to try to change the\ncode number now.  Right now, we are in this situation:\n(1) The 416 code was introduced to solve a real problem.\n(2) The design in -rev-03 is at least internally consistent\n(3) We are aware of the problem with IE, but we can also\nhypothesize some workarounds\n\nIf we change the code, then we may end up with problems that we\ndon't understand, and I think this would be worse.  I.e., rather\nthe devil you know than the (potentially several) devils you\ndon't know.  (Note to Paul: I am using \"devil\" here as part of\na common proverb, not in any more specific sense.)\n\nWhat workarounds?  I recall that someone (Roy?) suggested on\nThursday that the server could have special code saying\nif (about to send 416) then\n    if (User-Agent == \"IE4\") then\ndon't send 416\n    endif\n    send 416\nendif\n\nThe outer condition might be something more elaborate, such\nas \"about to send 416 and requested range is just after\nthe end of the resource.\"\n\nI don't remember what was suggested instead of sending 416, but I'm\nsure that Josh can tell us what he would like to have happen\nthere.\n\nYes, I know this is a pain, but the alternatives are also painful.\n\n-Jeff\n\n\n\n", "id": "lists-012-4540085"}, {"subject": "Re: ISSUE CREATE: Proposed resolutio", "content": "Henrik Frystyk Nielsen wrote:\n\n> Action: Add this clarification to section 10.2.2:\n> \n>         A 201 response MAY contain an Etag response header field\n>         indicating the current value of the entity tag for the\n>         created requested variant, see [14.19].\n\nSince I hadn't noticed that this might not have been allowed before and our\nimplementation does it, I fully support this change :-)\n\n-- \nScott Lawrence            Consulting Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.   Embedded Web Technology     http://www.agranat.com/\n\n\n\n", "id": "lists-012-4553730"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "Henrik Frystyk Nielsen wrote:\n> \n>  The current wording in section 14.20 is nonsense as it renders all\n> existing HTTP/1.1 servers not compliant:\n> \n> The Expect request-header field is used to indicate that particular\n> server behaviors are required by the client. A server that does not \n> understand or is unable to comply with any of the expectation values \n> in the Expect field  of a request MUST respond with appropriate error\n> status.\n\nIn what sense, Henrik?  My server responds with Expectation Failed if you\nsend a token in Expect: that it doesn't recognize...\n\nYes, this was not in 2068, but this is not the only thing we've added.\n\nI believe that the MUST should stand; making it a SHOULD renders the Expect\nfeature almost useless.\n\n-- \nScott Lawrence            Consulting Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.   Embedded Web Technology     http://www.agranat.com/\n\n\n\n", "id": "lists-012-4561050"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "> At 19:20 7/28/98 +0000, Scott Lawrence wrote:\n> \n> >I believe that the MUST should stand; making it a SHOULD renders the \n> >Expect feature almost useless.\n\nHenrik Frystyk Nielsen replied:\n\n> I agree, but isn't this in fact the case in practice? Wouldn't it be\n> better to let Mandatory handle this as it has a stronger mechanism \n> for enforcing client based requirements based on the M- method name\n> prefix?\n\n... but have you tested what happens with old servers (and CGI programs) if\nyou send them new methods?  At least in the case of CGIs they often ignore\nthe method - in short, you have exactly the same situation faced by Expect,\nbut more complex.  Don't get me wrong - I like the Mandatory mechanism\nbecause it is so much more descriptive, but I don't think that it is any\nbetter from a backward compatibility point of view (and cannot be made any\nbetter).\n\n-- \nScott Lawrence            Consulting Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.   Embedded Web Technology     http://www.agranat.com/\n\n\n\n", "id": "lists-012-4569801"}, {"subject": "Rev04 production has started...", "content": "About a day late, but it has started.  And then I go on vacation\nfor a while.\n\nFurther suggestions, pleas, or discussions may fall on deaf ears.\n- Jim\n\n--\nJim Gettys\nDigital Industry Standards and Consortia\nCompaq Computer Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n", "id": "lists-012-4578516"}, {"subject": "Submission of draft-ietf-http-v11-spec-rev04.txt has occurred", "content": "Rev04 is available in the usual place (off the issues list\npage).  Thank you all for your help.\nJim Gettys\nHTTP/1.1 editor.\nAbstract\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol\nfor distributed, collaborative, hypermedia information systems. It is a\ngeneric, stateless, protocol which can be used for many tasks, such as\nname servers and distributed object management systems, through\nextension of its request methods. A feature of HTTP is the typing and\nnegotiation of data representation, allowing systems to be built\nindependently of the data being transferred.\n\nHTTP has been in use by the World-Wide Web global information initiative\nsince 1990. This specification defines the protocol referred to as\n\"HTTP/1.1\", and is an update to RFC2068 [33].\n\nAt the time of submission, there were no remaining outstanding issues\nafter a working group last call.  The HTTP/1.1 issues list can be found\nat http://www.w3.org/Protocols/HTTP/Issues/.  Linked from this page are\nalso Postscript and Microsoft Word versions (with versions with change\nbars) of this document which are easier to review than this text\ndocument.\n\n--\nJim Gettys\nDigital Industry Standards and Consortia\nCompaq Computer Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n", "id": "lists-012-4585556"}, {"subject": "Re: Issue: TEIDENTIT", "content": "Henrik writes:\n\n    In section 14.39\n\n    Change\n\n    The TE request-header field is similar to Accept-Encoding, but\n    restricts the transfer-codings (section 3.6) that are acceptable in\n    the response.\n\n    to\n\n    The TE request-header field is similar to Accept-Encoding, but\n    indicates the transfer-codings (section 3.6) that are preferred in\n    the response. The client is not guaranteed that the server\n    recognizes the information in a TE request header field.\n\nConvoluted wording.  How about\n\n    The TE request-header field is similar to Accept-Encoding, but\n    indicates the transfer-codings (section 3.6) that are preferred in\n    the response.  The server MAY ignore the TE request-header field.\n\nIsn't that what you mean?\n\n-Jeff\n\n\n\n", "id": "lists-012-4593872"}, {"subject": "Re: MISTAKES, section 8.2.", "content": "If at first you don't succeed, try try again...  Actually, the problem\nis a missing \"and\" connecting the phrases.\n\nHere's take two:\n\n8.2.3 Automatic Retrying of Requests\n\nIf a user agent sees the transport connection close before it receives \nall of the final response to its request or sequence of requests, and \nif the requests or sequence are idempotent (see section 9.1.2), the user \nagent MAY retry the request or sequence without user interaction. If the \nrequest method or sequence is not idempotent, the user agent SHOULD NOT \nretry the request without user confirmation. (Confirmation by user-agent \nsoftware with semantic understanding of the application MAY substitute \nfor user confirmation.)\n\n- Jim\n\n\n\n", "id": "lists-012-4601456"}, {"subject": "Re: ISSUE: Returning metainformation on a 201 (Created) respons", "content": "a) I'm nervous about adding anything like this at this date.  I'd be happier\nwith a separate RFC describing what should happen...  This should have\nbeen dealt with months ago.\n\nb) 4) seems a gross hack to me, adding another HEAD class special case\nto the protocol.\n\nc) I suggest (described in a separate ID) a new header that conveys the\nETAG.  This solves the race condition.  To get any additional metadata\nthat might have changed, you'd perform a HEAD right after the PUT.\n\nd) you should go see if the WEBDAV folks have worried about this case;\nthey are sloppy enough they may have missed it, but then again, they\nmay have found it and have another fix for it...\n\nOne person's opinion...,\n\n- Jim\n\n\n\n", "id": "lists-012-4609250"}, {"subject": "Re: Connection token", "content": "On Fri, 17 Jul 1998 11:02:55 -0700, Jim Gettys wrote:\n> \n> A couple things wrong with this suggested rewrite:\n> \n> 1) saying it is vague is unfair; the spec says that 1.1 defines the\n> token \"close\" very clearly.\n\nI agree that the token \"close\" is defined clearly.\n\n> 2) Keep-Alive is a header.\n\nYou missed my point: in RFC 2068, section 19.7.1 states that Keep-Alive \nand Persist are connection-token's. Here's the original text:\n\n   When it connects to an origin server, an HTTP client MAY send the\n   Keep-Alive connection-token in addition to the Persist connection-\n   token:\n\n          Connection: Keep-Alive\n\n> 3) as HTTP/1.1 may be extended in ways that are just as valid as\n> the base spec, saying that close is the only connection token is\n> I believe misleading, as others may want/need to define other tokens;\n> I don't want the base spec to imply that some other spec isn't as valid\n> that is extending HTTP/1.1.\n\nI disagree: I don't think different implementations should be allowed to \nuse their own proprietary tokens. Allowing this would result in \ninteroperability issues.\n\n> I did add cross references in the section to section 19.6.2, and\n> vice versa, where persistent connection compatibility with HTTP/1.1\n> is discussed, which should reduce confusion.\n\nThanks\nJean-Philippe\n\n____________________________________________________________________\nJean-Philippe Martin-Flatin, EPFL-DI-ICA, 1015 Lausanne, Switzerland\nEmail: martin-flatin@epfl.ch                    Fax: +41-21-693-6610\nWeb: http://icawww.epfl.ch/~jpmf/\n\n\n\n", "id": "lists-012-4618457"}, {"subject": "Re: Issue: TEIDENTIT", "content": "At 11:43 7/28/98 MDT, Jeffrey Mogul wrote:\n\n>Isn't that what you mean?\n\nMuch better.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4627816"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "At 19:20 7/28/98 +0000, Scott Lawrence wrote:\n\n>I believe that the MUST should stand; making it a SHOULD renders the Expect\n>feature almost useless.\n\nI agree, but isn't this in fact the case in practice? Wouldn't it be better\nto let Mandatory handle this as it has a stronger mechanism for enforcing\nclient based requirements based on the M- method name prefix?\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4634806"}, {"subject": "[Fwd: I-D ACTION:draft-ietf-http-state-man-mec10.txt,.ps", "content": "[This announcement failed to make it to the mailing list, or, at least,\nto the http-wg archive.]\n\nDave Kristol\n\n\nattached mail follows:\nNote: This revision reflects comments received during the last call period.\n\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-10.txt,.ps\nPages: 22\nDate: 29-Jul-98\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal [Netscape], but it can interoperate with\nTTP/1.0 user agents that use Netscape's method.  (See the HISTORICAL\nsection.)\n \nThis document reflects implementation experience with RFC 2109 [RFC2109]\nand obsoletes it.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-10.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-10.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-10.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-4642867"}, {"subject": "[Fwd: Re: I-D ACTION:draft-ietf-http-state-man-mec10.txt,.ps", "content": "[This message failed to make it to http-wg or, at least, to the mailing\nlist archive.]\n\nDave Kristol\n\n\nattached mail follows:\nThese drafts reflect changes that address feedback from the IESG.  The\nusual set of drafts, including ones with change marks, is available from\n<http://portal.research.bell-labs.com/~dmk/cookie-ver.html>.\n\nThe biggest change:  addition of MAY/MUST/... etc. notations for\nnormative parts.\n\nDave Kristol\n\n\n\n", "id": "lists-012-4652079"}, {"subject": "Re: Digest Authentication Challenge Orderin", "content": "On Tue, 28 Jul 1998, Scott Lawrence wrote:\n\n> Patrick McManus wrote:\n> \n> > My problem is that if my server doesn't list Basic as the first choice\n> > (but does list is as less preferred to Digest) some existing 1.0 clients\n> > that can't do digest but can do basic don't realize that basic is an\n> > option.. \n> \n> But those are buggy even with respect to 2068; it's one thing for us to aim\n> for compatibility with software that conformed to the Proposed Standard, but\n> to maintain compatibility with something that was broken with respect to\n> them can be too difficult a standard to meet.\n\nHeck, they're buggy even with respect to rfc-1945. However, I feel we\nshould try and find a solution because otherwise I fear deployment of\nDigest might be hampered too much.\n\nLet me explain why I see a problem. As a webmaster I'd like to set up\nprotectecd areas such that if the browser supports Digest it will use\nDigest, else it will use Basic. Implicit in this is the emphasis that\nthis should work with the current (broken) browsers. Now, going by the\ncurrent authentication spec this would mean the server must send\n\nWWW-Authenticate: Digest ..., Basic ...\n\nHowever, because this breaks most currently used browsers I have to\ninstead configure\n\nWWW-Authenticate: Basic ..., Digest ...\n\nBut unfortunately this then means that _all_ browsers (including the\nones implementing the current auth spec) will use Basic too - no one\nwill use Digest. This means we have no reasonable upgrade path and I\nbelieve this will therefore definitely hamper the demployment of\nDigest.\n\nI see basically three solutions (ignoring the q-value approach given\nby Patrick because I don't see how to do it without breaking current\nbrowsers):\n\n1) Reverse the order, i.e. have servers list from least preferable to\n   most preferable.\n2) Make Basic the exception: list Basic first (if it's used), and then\n   from most preferable to least preferable\n3) Let the client make decisision (i.e the order is irrelevant).\n\n1) would be the simplest and most elegant solution. However, I think\nthis might cause problems with deployed M$ implementations (both clients\nand servers) who use Basic and NTLM auth schemes - anyone from M$ care\nto comment? 2) is ugly, and I don't think it's any better than 1) with\nrespect to current implementations.\n\n3) is doable (i.e. clients will probably have a reasonable notion of\nwhat is the most secure/prefered of the methods it implements), but\nmay not be acceptable to server folks who would like to specify the\npreference. As an example I'm thinking of NTLM vs. Digest - some\nwebmasters might prefer that the client to use Digest instead of NTLM\n(if implemented) because of it's better security, however some might\nwish for NTLM to be used (if implemented) because of the greater\n\"transparency\" (users don't have to enter username/password).\n\n> > I'm generally in favor of 1 of two paths to resolve the issue:\n> \n> >     1] remove the notion of server specified preference.. the\n> >     credentials belong to the client, it seems to me they should\n> >     understand what the risks are in sending them out on the\n> >     network. In this way the challenge specifes understandable\n> >     methods only.\n> \n> It is only a preference, and does not mandate anything - a browser could\n> send basic when it might have sent digest; since the server cannot tell what\n> its complete capabilities are, it makes no difference.  Either basic is\n> acceptable or not.  Eventually, I hope that digest will be supported well\n> enough in browsers that I can recommend to my customers that they turn off\n> support for basic, but we've a long way to go...\n\nI agree. However, if the spec stays at it is and browsers implement it\nthat way then I wonder if we'll ever get there. I'd prefer not to force\nor encourage more \"this site only works with the foobar browser\" crap.\n\n> >     2] introduce q values\n> \n> That only makes the situation worse by increasing the number of browsers\n> that don't understand what you are doing.\n\nI agree.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-4659095"}, {"subject": "CHALLENGEORDER: proposed chang", "content": "[resent after bounce on first attempt]\n\nI don't believe that leaving the choice of schemes to the browser creates\nany problems that are not there anyway, so I propose the following\nreplacement for 4.6 (I could not find any other section that had any text on\nthis - please point it out if I missed it).  The first paragraph is changed\nto remove the semantics associated with the offered order, and to add some\nnormative language about not sending replayable credentials.  The second is\nunchanged except for striking the last sentence.\n\n    4.6 Weakness Created by Multiple Authentication Schemes\n    \n    An HTTP/1.1 server MAY return multiple challenges with a 401\n    (Unauthorized) response, and each challenge MAY use a different\n    scheme.  The user is free to choose from among the offered challenges\n    it understands and request credentials from the user based upon that\n    challenge.  The user agent SHOULD choose the scheme it considers to be\n    most secure; the Basic scheme, or any other scheme which transmits\n    credentials in a way that allows for replay of those credentials,\n    SHOULD NOT be used if there is an alternative available. \n    \n    When the server offers choices of authentication schemes using the WWW-\n    Authenticate header, the strength of the resulting authentication is\n    only as good as that of the of the weakest of the authentication\n    schemes. See section 4.8 below for discussion of particular attack\n    scenarios which exploit multiple authentication schemes.\n\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-4670427"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "At 20:47 7/28/98 +0000, Scott Lawrence wrote:\n\n>... but have you tested what happens with old servers (and CGI programs) if\n>you send them new methods?  At least in the case of CGIs they often ignore\n>the method - in short, you have exactly the same situation faced by Expect,\n>but more complex. \n\nDoes the server ignore them as well and just hands off the script regardless?\n\nI don't have an exhaustive list - do you have some data? Is so then I can\ntry and check around to see about other servers as well.\n\nThanks,\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4679541"}, {"subject": "Re: CHALLENGEORDER: proposed chang", "content": "David W. Morris wrote:\n\n> If I were a server owner, I would be very inclined to examine the HTTP\n> user-agent field and not offer multiple choices but rather the best\n> choice known to be supported by the UA. I wonder if some or\n> all of the following or other implementations suggestions should be\n> documented:\n> \n> 1.  Tell the end user what the authentication scheme is ...\n> 2.  Provide a user-agent configuration option to allow the user to\n>     refuse authentication using basic\n\nIn 4.8 we have:\n\nUser agents should consider measures such as presenting a visual\nindication at the time of the credentials request of what authentication\nscheme is to be used, or remembering the strongest authentication scheme\never requested by a server and produce a warning message before using a\nweaker one.  It might also be a good idea for the user agent to be\nconfigured to demand Digest authentication in general, or from specific\nsites.\n\n> 3.  Provide server owners with the ability to restrict basic usage\n>     to UAs based on UA identity .. perhaps not much better in MIM case\n>     but it would insure that a UA which could use digest would use it.\n\nThe standard does not say anything now about what criteria may be used by\nthe server to choose the offered schemes, and to my way of thinking that\nallows for any criteria at all, including UA identity (personally, keeping\nup with what browsers send for identity is way too much trouble for me). \n\n> 4.  Provide applications with the ability to differentiate the level of\n>     access based on authentication type. Read via basic but create/update\n>     only by digest or better.\n\nAgain, I don't think that there is anything in the spec that forbids such a\nthing, and it is therefor allowed (and, I think, is a good idea).\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-4687724"}, {"subject": "Re: Digest Authentication Challenge Orderin", "content": "Ronald.Tschalaer@psi.ch wrote:\n> [...]\n> Heck, they're buggy even with respect to rfc-1945. However, I feel we\n> should try and find a solution because otherwise I fear deployment of\n> Digest might be hampered too much.\n> \n> Let me explain why I see a problem. As a webmaster I'd like to set up\n> protectecd areas such that if the browser supports Digest it will use\n> Digest, else it will use Basic. Implicit in this is the emphasis that\n> this should work with the current (broken) browsers. Now, going by the\n> current authentication spec this would mean the server must send\n> \n> WWW-Authenticate: Digest ..., Basic ...\n> \n> However, because this breaks most currently used browsers I have to\n> instead configure\n> \n> WWW-Authenticate: Basic ..., Digest ...\n> \n> But unfortunately this then means that _all_ browsers (including the\n> ones implementing the current auth spec) will use Basic too - no one\n> will use Digest. This means we have no reasonable upgrade path and I\n> believe this will therefore definitely hamper the demployment of\n> Digest.\n\nI think I made similar observations as far back as when Digest was\n\"SimpleMD5\".\n\n> \n> I see basically three solutions (ignoring the q-value approach given\n> by Patrick because I don't see how to do it without breaking current\n> browsers):\n\n> [Description of three options, plus commentary, deleted.]\n\nExpressing preferences with \"least-preferred-first\" does seem to be an\nelegant solution.  Unfortunately, we remain at the mercy of the browser\nimplementation.  We (webmasters) are relying on the user agent to \"do\nthe right thing\" and choose the more secure authentication method.  If\nDigest Auth is a required part of HTTP/1.1 (clients), then it's possible\nto test whether the client complies (by sending Basic and Digest\nchallenges and looking for the Digest response).  In that case I would\nsupport \"least-preferred-first\".\n\nDave Kristol\n\n\n\n", "id": "lists-012-4697126"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "Henrik Frystyk Nielsen wrote:\n> >At least in the case of CGIs they often ignore\n> >the method\n\n> Does the server ignore them as well and just hands off the script \n> regardless?\n\nThe CGI spec (either the existing 1.1 spec or the new 1.2) just makes the\nmethod available to the program - if the program doesn't look, it doesn't. \nSome of the CGI libraries make this easy to do as well - parameters are\nparsed by the library from a GET query string or a POST body transparently,\nfor example.  I've tried bouncing TRACE and OPTIONS requests off various\nURLs that were obviously scripts and many respond as though the request were\na GET. \n\nI just took a (very) quick peek at the Apache documentation for adding\nmodules, and found a similar API - the method is passed by the server to the\nmodule, so the core server itself doesn't even appear to have a way to know\nwhat methods might be handled or not.  Pretty good design if you are\noptimizing for flexibility.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-4706593"}, {"subject": "Re: I-D ACTION:draft-ietf-http-state-man-mec10.txt,.p", "content": "These drafts reflect changes that address feedback from the IESG.  The\nusual set of drafts, including ones with change marks, is available from\n<http://portal.research.bell-labs.com/~dmk/cookie-ver.html>.\n\nThe biggest change:  addition of MAY/MUST/... etc. notations for\nnormative parts.\n\nDave Kristol\n\n\n\n", "id": "lists-012-4715460"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "At 17:52 7/31/98 +0000, Scott Lawrence wrote:\n\n>The CGI spec (either the existing 1.1 spec or the new 1.2) just makes the\n>method available to the program - if the program doesn't look, it doesn't. \n>Some of the CGI libraries make this easy to do as well - parameters are\n>parsed by the library from a GET query string or a POST body transparently,\n>for example.  I've tried bouncing TRACE and OPTIONS requests off various\n>URLs that were obviously scripts and many respond as though the request were\n>a GET. \n\nYou are right - they indeed seem to be largely indifferent except that I\nsee an added 100 code on PUTs and HEAD responses don't include a body.\n \n>I just took a (very) quick peek at the Apache documentation for adding\n>modules, and found a similar API - the method is passed by the server to the\n>module, so the core server itself doesn't even appear to have a way to know\n>what methods might be handled or not.  Pretty good design if you are\n>optimizing for flexibility.\n\nSure if you have a way of describing what you mean by a new method and that\nyou have a mechanism for ensuring that it is handled probably. The current\nsituation is just a recipe for evolutional disaster in HTTP/1.x and it\nindeed breaks HTTP/1.1 for any method (including unknown ones) except GET,\nHEAD and POST.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-4722858"}, {"subject": "Re: ISSUE: Returning metainformation on a 201 (Created) respons", "content": ">a) I'm nervous about adding anything like this at this date.  I'd be happier\n>with a separate RFC describing what should happen...  This should have\n>been dealt with months ago.\n\nYes.  Actually, I think we had the same discussion about a year ago,\nresulting in a substantial change to the definition of the Etag field\nfrom what it meant for RFC 2068.  I am still queesy about that change,\nbut it does support the usage in 201 that Henrik suggests.\n\nThe contrarian in me finds it necessary to point out that this is\njust one of many protocol changes that require a version number bump\nto HTTP/1.2.  It is no longer the same protocol, and this is precisely\nwhy we have that second number.\n\n>b) 4) seems a gross hack to me, adding another HEAD class special case\n>to the protocol.\n\nYep.  A more appropriate definition is to include the new properties\nof the resource in the body of the response, either as a multipart or\nin XML.  But that would require a lot more definition of the response\nformat, which is something only WebDAV has done so far.\n\n....Roy\n\n\n\n", "id": "lists-012-4731975"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": ">In what sense, Henrik?  My server responds with Expectation Failed if you\n>send a token in Expect: that it doesn't recognize...\n>\n>Yes, this was not in 2068, but this is not the only thing we've added.\n>\n>I believe that the MUST should stand; making it a SHOULD renders the Expect\n>feature almost useless.\n\nWe cannot do that and still call it HTTP/1.1.  The protocol versioning\nrules exist to prevent the HTTP-WG from screwing over early implementers\nof the protocol, such that the protocol can be deployed and remain\ninteroperable even though some definitions and requirements will,\nby necessity, change over time.\n\n....Roy\n\n\n\n", "id": "lists-012-4740534"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "David W. Morris wrote:\n\n> The problem starts with the fact we've carefully never acknowledged\n> the CGI interface as being part of HTTP.  What I think is needed is a\n> MUST requirement that a HTTP server never delegate handling of any\n> method other then GET, HEAD, or POST to any part of the server which\n> isn't known to understand / properly handle the unknown method.\n\nI think that crosses the line from being a protocol spec to being a\nfunctional spec for a server, which it should not try to be.  There are few\ncases where we have put in requirements outside the wire protocol, but they\nare mostly governing caching to preserve correctness, or are related to the\nsecurity behaviours.  \n\nrom the point of view of this spec, \"the server\" is whatever is generating\nthe response to a request - it encompasses the CGI or *API program, and\nthose components are as much bound by its requirements as any other part. \nYes, I understand the real world implications of this - and I believe that\nthose working on a new CGI spec do too. \n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-4748380"}, {"subject": "Re: CHALLENGEORDER: proposed chang", "content": "On Fri, 31 Jul 1998, Scott Lawrence wrote:\n\n> [resent after bounce on first attempt]\n> \n> I don't believe that leaving the choice of schemes to the browser creates\n> any problems that are not there anyway, so I propose the following\n> replacement for 4.6 (I could not find any other section that had any text on\n> this - please point it out if I missed it).  The first paragraph is changed\n> to remove the semantics associated with the offered order, and to add some\n> normative language about not sending replayable credentials.  The second is\n> unchanged except for striking the last sentence.\n> \n>     4.6 Weakness Created by Multiple Authentication Schemes\n>     \n>     An HTTP/1.1 server MAY return multiple challenges with a 401\n>     (Unauthorized) response, and each challenge MAY use a different\n>     scheme.  The user is free to choose from among the offered challenges\n                  ^^^^^^ client or user-agent but not user\n>     it understands and request credentials from the user based upon that\n>     challenge.  The user agent SHOULD choose the scheme it considers to be\n>     most secure; the Basic scheme, or any other scheme which transmits\n>     credentials in a way that allows for replay of those credentials,\n>     SHOULD NOT be used if there is an alternative available. \n>     \n>     When the server offers choices of authentication schemes using the WWW-\n>     Authenticate header, the strength of the resulting authentication is\n>     only as good as that of the of the weakest of the authentication\n>     schemes. See section 4.8 below for discussion of particular attack\n>     scenarios which exploit multiple authentication schemes.\n\nIf I were a server owner, I would be very inclined to examine the HTTP\nuser-agent field and not offer multiple choices but rather the best\nchoice known to be supported by the UA. I wonder if some or\nall of the following or other implementations suggestions should be\ndocumented:\n\n1.  Tell the end user what the authentication scheme is ... \n2.  Provide a user-agent configuration option to allow the user to\n    refuse authentication using basic\n3.  Provide server owners with the ability to restrict basic usage\n    to UAs based on UA identity .. perhaps not much better in MIM case\n    but it would insure that a UA which could use digest would use it.\n4.  Provide applications with the ability to differentiate the level of\n    access based on authentication type. Read via basic but create/update\n    only by digest or better.\n\nDave Morris\n\n\n\n", "id": "lists-012-4757234"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "On Fri, 31 Jul 1998, Henrik Frystyk Nielsen wrote:\n\n> At 17:52 7/31/98 +0000, Scott Lawrence wrote:\n> \n> >The CGI spec (either the existing 1.1 spec or the new 1.2) just makes the\n> >method available to the program - if the program doesn't look, it doesn't. \n> >Some of the CGI libraries make this easy to do as well - parameters are\n> >parsed by the library from a GET query string or a POST body transparently,\n> >for example.  I've tried bouncing TRACE and OPTIONS requests off various\n> >URLs that were obviously scripts and many respond as though the request were\n> >a GET. \n> \n> You are right - they indeed seem to be largely indifferent except that I\n> see an added 100 code on PUTs and HEAD responses don't include a body.\n>  \n> >I just took a (very) quick peek at the Apache documentation for adding\n> >modules, and found a similar API - the method is passed by the server to the\n> >module, so the core server itself doesn't even appear to have a way to know\n> >what methods might be handled or not.  Pretty good design if you are\n> >optimizing for flexibility.\n> \n> Sure if you have a way of describing what you mean by a new method and that\n> you have a mechanism for ensuring that it is handled probably. The current\n> situation is just a recipe for evolutional disaster in HTTP/1.x and it\n> indeed breaks HTTP/1.1 for any method (including unknown ones) except GET,\n> HEAD and POST.\n\nThe problem starts with the fact we've carefully never acknowledged\nthe CGI interface as being part of HTTP.  What I think is needed is a\nMUST requirement that a HTTP server never delegate handling of any\nmethod other then GET, HEAD, or POST to any part of the server which\nisn't known to understand / properly handle the unknown method.\n\nI think this requirement is implicit in the spec in any case so it \nisn't a protocol change. It does mean that a conforming HTTP/1.1\nserver can't handoff an unknown method to a CGI (or *API) program\nif there isn't some kind of configuration switch to prevent handing\nsuch requests to an old CGI program. With a server having the ability\nto distinguish 'old' methods from new methods and controlling which\napplication code each will be sent to, the server can't even fix a\nfailure by disabling the new methods from the old CGI.\n\nDave Morris\n\n\n\n", "id": "lists-012-4768086"}, {"subject": "Re: Connection token", "content": ">I disagree: I don't think different implementations should be allowed to \n>use their own proprietary tokens. Allowing this would result in \n>interoperability issues.\n\nWell, I completely disagree with that opinion.  The reason we added\nConnection is because there was no way to indicate connection-only\nextensions in HTTP.  We didn't need it for standard additions, except\nfor the obvious problem that all new standards look like extensions\nto an already deployed implementation.\n\nAllowing this is what solves the interoperability issue.\n\n....Roy\n\n\n\n", "id": "lists-012-4778819"}, {"subject": "Re: MUST-MAYSHOULD (MMS) audit..", "content": "Jim Gettys:\n>\n>\n>There has been no comments on Jeff and Scott's massive audit.  I suspect \n>everyone who has looked at them has had their eyes glaze over; I know \n>I have.\n\nI managed to go through Jeff's audit and get halfway through Scott's\naudit so far.  Here are some comments.\n\n- Jim: I looked at your corrections to their corrections and they seem\n  correct to me.\n\n- Generic editorial warning: I seem to remember (though I don't know\n  where I learnt it) that notes in an RFC are always non-normative by\n  convention, so there should be no MMS terminology in them.  Jeff's\n  corrections seem to take the opposite view, whereas Scott makes\n  judgment calls on a note by note basis.  I don't have a direct\n  connection to any RFC repository now so I can't check whether there\n  are generic conventions with respect to notes, but somebody should\n  check this.\n\n- MMS 025: I am not sure if Jeff is right in his assumption as to what\n  the term \"common form\" is supposed to mean.  Maybe \"common form\"\n  means `not extended over multiple lines' here.\n\n- MMS 117: The musts in the two items define conditions to be met for\n  a MAY to apply, so they should be lowercase (or preferably\n  rephrased), not uppercase.\n\n- MMS 125: in the definition of \"invalidate an entity\", the two\n  shoulds are defining a term, they are are not keywords specifying a\n  requirement, so they should be rephrased, e.g. from `should' to\n  `will'.  [This is a bit of a judgment call: I believe a phrase like\n\n       \"MUST cause a cache to invalidate the entity\"\n\n  (which is used in the paragraph following the definition) was meant\n  to expand to\n\n       \"MUST cause a cache to either remove all instances or mark them\n       as invalid\"\n\n  and not to \n\n       \"MUST cause a cache to apply the rule that it SHOULD either\n        remove all instances or mark them as invalid\"\n\n  which is only a SHOULD level requirement at the core.]\n\nKoen.\n\n\n\n", "id": "lists-012-4786051"}, {"subject": "I-D ACTION:draft-ietf-http-v11-spec-rev04.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: Hypertext Transfer Protocol -- HTTP/1.1\nAuthor(s): J. Mogul, T. Berners-Lee, L. Masinter, \n                          P. Leach, R. Fielding, H. Nielsen, J. Gettys\nFilename: draft-ietf-http-v11-spec-rev-04.txt\nPages: 155\nDate: 29-Jul-98\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol\nfor distributed, collaborative, hypermedia information systems. It is a\ngeneric, stateless, protocol which can be used for many tasks, such as\nname servers and distributed object management systems, through\nextension of its request methods. A feature of HTTP is the typing and\nnegotiation of data representation, allowing systems to be built\nindependently of the data being transferred.\n \nHTTP has been in use by the World-Wide Web global information initiative\nsince 1990. This specification defines the protocol referred to as\n'HTTP/1.1', and is an update to RFC2068 [33].\n \nAt the time of submission, there were no remaining outstanding issues\nafter a working group last call.  The HTTP/1.1 issues list can be found\nat http://www.w3.org/Protocols/HTTP/Issues/.  Linked from this page are\nalso Postscript and Microsoft Word versions (with versions with change\nbars) of this document which are easier to review than this text\ndocument.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-v11-spec-rev-04.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-v11-spec-rev-04.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-v11-spec-rev-04.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-4795448"}, {"subject": "I-D ACTION:draft-ietf-http-state-man-mec10.txt,.p", "content": "Note: This revision reflects comments received during the last call period.\n\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-10.txt,.ps\nPages: 22\nDate: 29-Jul-98\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal [Netscape], but it can interoperate with\nTTP/1.0 user agents that use Netscape's method.  (See the HISTORICAL\nsection.)\n \nThis document reflects implementation experience with RFC 2109 [RFC2109]\nand obsoletes it.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-10.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-10.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-10.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-4805001"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "On Fri, 31 Jul 1998, Scott Lawrence wrote:\n\n> David W. Morris wrote:\n> \n> > The problem starts with the fact we've carefully never acknowledged\n> > the CGI interface as being part of HTTP.  What I think is needed is a\n> > MUST requirement that a HTTP server never delegate handling of any\n> > method other then GET, HEAD, or POST to any part of the server which\n> > isn't known to understand / properly handle the unknown method.\n> \n> I think that crosses the line from being a protocol spec to being a\n> functional spec for a server, which it should not try to be.  There are few\n> cases where we have put in requirements outside the wire protocol, but they\n> are mostly governing caching to preserve correctness, or are related to the\n> security behaviours.  \n\n\nOK, then lets stop worrying about the fact that servers invoke brain dead\ncgi programs. In any case, the requirement I suggested is there to insure\ncorrect behavior. I tried to use very general language as to the\nrequirement that base servers take responsibility for what they deliver\non the wire.\n\n> \n> >From the point of view of this spec, \"the server\" is whatever is generating\n> the response to a request - it encompasses the CGI or *API program, and\n> those components are as much bound by its requirements as any other part. \n> Yes, I understand the real world implications of this - and I believe that\n> those working on a new CGI spec do too. \n\nIt is easy enough to expect the base server to enforce protocol when \ninvoking API programs.  The new CGI spec. is only intended to be an\ninformational RFC. Whether or not a server implements the new CGI\nspecification, it must ensure that what it returns as an HTTP/1.1\nresponse is infact a HTTP/1.1 response. \n\nDave Morris\n\n\n\n", "id": "lists-012-4814590"}, {"subject": "Re: ISSUE: Expect Header Field Proble", "content": "David W. Morris wrote:\n\n> [...] Whether or not a server implements the new CGI\n> specification, it must ensure that what it returns as an HTTP/1.1\n> response is infact a HTTP/1.1 response.\n\nAnd the spec does that now - it does not make any special allowance for\nresponses that originate with some other component or gatewayed system - if\nthe response is labelled HTTP/1.1 then the spec makes certain requirements\nof it.  What more is needed than that?  When we say MUST, we don't need to\nthen provide a list of circumstances under which the MUST still applies -\nunless we allow for exceptions, there are none.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-4824182"}, {"subject": "CNONCE: proposed resolutio", "content": "In http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0031.html and\nsubsequent messages, the question was raised of how the server should\ncalculate the various digests if qop=auth or qop=auth-int was sent by the\nclient, but no cnonce attribute is supplied.\n\nI propose the following clarification for this;\n\nin section 3.2.2 (The Authorization Request Header), append the following to\nthe description of the cnonce:\n\n    If not present, the null string should be used for this value\n    in any digest calculation where 'cnonce' is used.\n\nand add the following text to the end of 4.3 (Limited Use Nonce Values):\n\n\n   The client generated 'cnonce' value is optional; however, clients\n   choosing not to use this mechanism or which do not change the cnonce\n   value used cannot authenticate the server, and do not have any message\n   integrity protection for responses.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-4832394"}, {"subject": "Re: CNONCE: proposed resolutio", "content": "Paul Leach wrote:\n\n> I think that absence of cnonce should be illegal if qop=auth or\n> qop=auth-int is selected by the client; if the client really _demands_ \n> to be totally braindead, it can send a constant as its cnonce.\n\nI'm also happy with that solution.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-4840827"}, {"subject": "Re: CNONCE: proposed resolutio", "content": "Scott Lawrence wrote:\n> \n> Paul Leach wrote:\n> \n> > I think that absence of cnonce should be illegal if qop=auth or\n> > qop=auth-int is selected by the client; if the client really _demands_\n> > to be totally braindead, it can send a constant as its cnonce.\n> \n> I'm also happy with that solution.\n\nJust what does \"illegal\" mean?  What should a server do if it gets such\nan \"illegal\" request?\n\nDave Kristol\n\n\n\n", "id": "lists-012-4848910"}, {"subject": "Re: CNONCE: proposed resolutio", "content": "Paul Leach wrote:\n> \n> > -----Original Message-----\n> > From: Dave Kristol [mailto:dmk@bell-labs.com]\n> > Sent: Monday, August 03, 1998 2:54 PM\n> \n> > Just what does \"illegal\" mean?  What should a server do if it\n> > gets such\n> > an \"illegal\" request?\n> \n> Send back 400 Bad Request, just like with all ill-formed requests.\n> \n> Dave -- it would help if you'd indicate when your questions are Socratic --\n> I know you've been doing this long enough you know the answers.\n\nIn truth, it was not intended as a Socratic question, and I really\nwasn't sure whether 400 Bad Request was the intended response.  I think\nthe spec. should be clear about what the server should do, rather than\nbe coy about it.\n\nDave Kristol\n\n\n\n", "id": "lists-012-4856590"}, {"subject": "Re: CHALLENGEORDER: proposed chang", "content": "good morning,\n\nIn a previous episode Scott Lawrence said...\n\n:: I don't believe that leaving the choice of schemes to the browser creates\n:: any problems that are not there anyway, so I propose the following\n\nthat's a key point.. in the end the browser needs to make the decision\nof whether or not sending their credentials onto the wire is within\ntheir security policy.\n\n::     4.6 Weakness Created by Multiple Authentication Schemes\n::     \n::     An HTTP/1.1 server MAY return multiple challenges with a 401\n::     (Unauthorized) response, and each challenge MAY use a different\n::     scheme.  The user is free to choose from among the offered challenges\n::     it understands and request credentials from the user based upon that\n::     challenge.  The user agent SHOULD choose the scheme it considers to be\n::     most secure; the Basic scheme, or any other scheme which transmits\n::     credentials in a way that allows for replay of those credentials,\n::     SHOULD NOT be used if there is an alternative available. \n\nI'd scratch the last portion of that (\"; the Basic scheme, ...\") as\nbeing redundant. \n\nrelated point: It's important to me to keep UA based decisions out of\nthe spec. They're messy, non-scalable, and inevitably become\nhistorical cruft you can never quite get rid of. I've currently only\ngot one in (Content-Encodings with respect to some unix versions of\nnetscape) this codebase, and would like to keep it that way.\n\nThe above proposed change satisfies my need while allowing an auth\nupgrade path for clients.\n\n-P\n\n\n\n", "id": "lists-012-4864594"}, {"subject": "Re: CNONCE: proposed resolutio", "content": "On Mon, 3 Aug 1998, Scott Lawrence wrote:\n\n> Paul Leach wrote:\n> \n> > I think that absence of cnonce should be illegal if qop=auth or\n> > qop=auth-int is selected by the client; if the client really _demands_ \n> > to be totally braindead, it can send a constant as its cnonce.\n> \n> I'm also happy with that solution.\n> \n\nI prefer this also.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-4873385"}, {"subject": "Administrivia: list address chang", "content": "Folks,\n\nThe http-wg and http-wg-d mailing lists have moved from:\n\n  http-wg[-d]@cuckoo.hpl.hp.com\n\nto:\n\n  http-wg[-d]@hplb.hpl.hp.com\n\nHopefully this will solve the timeouts people have been reporting.\n\n-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-012-4881716"}, {"subject": "Re: MUST-MAYSHOULD (MMS) audit..", "content": "Koen Holtman writes:\n    \n    Generic editorial warning: I seem to remember (though I don't know\n    where I learnt it) that notes in an RFC are always non-normative by\n    convention, so there should be no MMS terminology in them.  Jeff's\n    corrections seem to take the opposite view, whereas Scott makes\n    judgment calls on a note by note basis.  I don't have a direct\n    connection to any RFC repository now so I can't check whether there\n    are generic conventions with respect to notes, but somebody should\n    check this.\n\nIt has indeed been our policy to avoid normative statements in\n\"Notes\".  I basically followed this policy, although there are\na few places where it seemed like a secondary priority.  Also,\nthere are place where it's not clear whether a paragraph is\nreally a \"Note\" or not.  In almost every case, I did point out\nthat the Note in question contained apparently normative\nlanguage.  I didn't want to delete requirements simply because\nthey appeared in Notes, but I also didn't want to spend the\ntime to rewrite large chunks of text.\n\nMMS_AUDIT_ITEM_008:\nThe paragraph starts with the word \"Note\", but it's not\nindented like the other \"Notes\".  The content is clearly\nnormative, but it might just be restating other normative\nlanguage.\n\nMMS_AUDIT_ITEM_010:\nAn actual \"Note\"; I changed a \"should\" (ambiguous) to\nan \"ought\" (non-normative).\n\nissue MMS_AUDIT_ITEM_013\nI pointed out that this is a case where normative\nlanguage appears in a Note, and it's not clear whether\nit was meant to be normative or not.  Someone with\nan interest should comment on the substance, not just\nthe form.\n\nissue MMS_AUDIT_ITEM_018:\nDefinitely in a Note, but also possibly an\ninteroperability requirement.  Again, review by\na qualified person would be helpful.\n\nissue MMS_AUDIT_ITEM_024:\nA Note that restates a normative requirement.\n\nissue MMS_AUDIT_ITEM_062:\nI removed a \"should not\" from a Note.\n\nissue MMS_AUDIT_ITEM_065:\nissue MMS_AUDIT_ITEM_066:\nNotes that contain what appear to be normative\nrequirements (aids to interoperability)\n\nissue MMS_AUDIT_ITEM_075:\nissue MMS_AUDIT_ITEM_104:\nI removed two \"mays\" from each of these Notes.\n\nIn other words, I'm not sure why Koen thinks that I \"took an\nopposite view\" rather than making \"judgement calls on a note\nby note basis.\"  Nevertheless, if we really want to be consistent\non this formality, then someone should spend some effort\nsplitting some of these Notes into normative and true-Note parts.\n\n    MMS 025: I am not sure if Jeff is right in his assumption as to\n    what the term \"common form\" is supposed to mean.  Maybe \"common\n    form\" means `not extended over multiple lines' here.\n\nAs I wrote, we don't have a definition for \"common form\" in general, so\nwe can argue until the end of the millenium about what this paragraph\nactually means.  The right solution is to either define a general\n\"common form\", or make it clear which specific \"common forms\" are\nintended.  I tried to do the latter.\n\n    MMS 117: The musts in the two items define conditions to be met for\n    a MAY to apply, so they should be lowercase (or preferably\n    rephrased), not uppercase.\n\nI think your logic is faulty.  An analogy: in this sentence,\n\nYou MAY pilot an airliner, but in order to do so, you\nMUST have a pilot's license.\n\nit is pretty clear that the phrase \"you MUST have a pilot's\nlicense\" does not apply to everyone; it only applies to pilots,\nbut then it is an \"absolute requirement of the specification\"\n(to quote from RFC2119).  I think you are arguing that one should\ninstead say\n\nYou MAY pilot an airliner, but in order to do so, you\nought to have a pilot's license.\n\nwhich has a very different meaning (and one that would convince\nme to travel by train).\n\n    MMS 125: in the definition of \"invalidate an entity\", the two\n    shoulds are defining a term, they are are not keywords specifying a\n    requirement, so they should be rephrased, e.g. from `should' to\n    `will'.\n\nI believe that this language basically defines a \"subroutine\"\nrather than simply a \"term\".  It's giving a specific meaning to a\nverb-phrase when that phrase is used in this specification.\n\nUsing a word like \"will\" sidesteps the issue of whether this\nis a conditional or unconditional requirement.  Which points\nout that since I didn't look at the following text as carefully\nas you have, where it says\n  \n  Some HTTP methods MUST cause a cache to invalidate an entity.\n\nit seems (by my argument re: MMS 117) that these SHOULDS really\nneed to be MUSTs (or that the latter MUST has to be a SHOULD).\n\nOtherwise, we're in the position of saying that doing something\nis an absolute requirement of the specification, but doing it\naccording to our definition is only a conditional requirement.\nI.e., the implementor has to do something, but is allowed (for\ngood cause) to ignore our specification of what that something is.\nSo this is probably a bug in the language, and needs to be resolved.\n\n-Jeff\n\n\n\n", "id": "lists-012-4888226"}, {"subject": "RE: Authentication issue CNONCE: Proposed resolutio", "content": "How about -- if auth= or auth-int= are specified, cnonce= is required and\nMUST be a value never used before by the client?\n\n> -----Original Message-----\n> From: Scott Lawrence [mailto:lawrence@agranat.com]\n> Sent: Tuesday, July 28, 1998 11:13 AM\n> To: Larry Masinter\n> Cc: HTTP Working Group\n> Subject: Re: Authentication issue CNONCE: Proposed resolution\n> \n> \n> Larry Masinter wrote:\n> \n> > In http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0031.html\n> > Dave Kristol wrote:\n> > \n> > # 3.2.3 The Authentication-Info Header\n> > # cnonce and qop are used in the calculation of \n> response-digest.  The\n> > # client is not required to send either cnonce= or auth=.  \n> So I assume\n> > # (correct?) that the null string is used for values for omitted\n> > # attributes in the calculation.\n> > \n> > I suggest that this be the correct interpretation, that the null\n> > string is used for values for omitted attributes in the calculation.\n> > \n> > # If (to use cnonce as the example) cnonce was omitted, should\n> > # Authentication-Info omit cnonce, or should it send \n> cnonce=\"\"?  Same\n> > # question for auth.\n> > \n> > I propose that either MAY be allowed, since they are equivalent.\n> \n> I think that this is an acceptable resolution, but that the Security\n> Considerations section will need a short paragraph on the \n> implications of\n> leaving this out - the server is then not authenticated to \n> the user agent.\n> \n> -- \n> Scott Lawrence            Consulting Engineer        \n> <lawrence@agranat.com>\n> Agranat Systems, Inc.   Embedded Web Technology     \n> http://www.agranat.com/\n> \n\n\n\n", "id": "lists-012-4900348"}, {"subject": "ISSUE: Protection spac", "content": "In section 3.2.1, The WWW-Authenticate Response Header\n\nOLD:\n\ndomain\n\nA space-separated list of URIs, as specified in RFC XURI [7]. The intent is\nthat the client could use this information to know the set of URIs for which\nthe same authentication information should be sent. The URIs in this list\nmay exist on different servers. If this keyword is omitted or empty, the\nclient should assume that the domain consists of all URIs on the responding\nserver.\n\nNEW:\n\ndomain\n\nA space-separated list of URIs, as specified in RFC XURI [7] that define the\nprotection space.  If a URI is relative, it is relative to canonical root\nURL (see section 5.1.2 of [2]) of the server being accessed. The URIs in\nthis list may refer to different servers. The client can use this list to\ndetermine the set of URIs for which the same authentication information may\nbe sent: any URI that has a URI in this list as a prefix (after both have\nbeen made absolute) may be assumed to be in the same protection space. If\nthis keyword is omitted or empty, the client should assume that the\nprotection space consists of all URIs on the responding server.\n\nRATIONALE:\nThe terminology of \"protection space\" was not used for Digest. The means for\ndetermining when Digest clients could use the same credentials was\nunder-specified. \n\n\n\n", "id": "lists-012-4911132"}, {"subject": "Proxy Auth??", "content": "Is Proxy-Authorization only sent after 407, or can it also be sent after\n401? Section 3.6 (entitled Proxy-Authentication and Proxy-Authorization)\nsays that:\n \nUpon receiving a request which requires authentication, the proxy/server\nmust issue the \"HTTP/1.1 401 Unauthorized \" response with a\n\"Proxy-Authenticate\" header.\n\nSection 1.2 says:\n\nThe 401 (Unauthorized) response message is used by an origin server to\nchallenge the authorization of a user agent. This response MUST include a\nWWW-Authenticate header field containing at least one challenge applicable\nto the requested resource. The 407 (Proxy Authentication Required) response\nmessage is used by a proxy to challenge the authorization of a client and\nMUST include a Proxy-Authenticate header field containing a challenge\napplicable to the proxy for the requested resource.\n\n\n\n", "id": "lists-012-4919527"}, {"subject": "RE: Digest Authentication Challenge Orderin", "content": "I propose that the user-agent MUST choose the strongest auth-scheme it\nunderstands. This permits the server to put Basic first for old browsers (if\nit finds Basic acceptably secure). The order really doesn't matter, since\nthe server is only supposed to offer minimally acceptable schemes.\n\n\n\n", "id": "lists-012-4927064"}, {"subject": "RE: questions regarding draft-ietf-http-authentication0", "content": "> -----Original Message-----\n> From: Scott Lawrence [mailto:lawrence@agranat.com]\n> Sent: Sunday, March 29, 1998 10:50 AM\n> To: Paul Leach\n> Cc: HTTP-WG@cuckoo.hpl.hp.com; 'Ronald.Tschalaer@psi.ch'\n> Subject: Re: questions regarding draft-ietf-http-authentication-01\n> \n> \n> >> 8) Section 3.2.3: no words prohibit the server from \n> sending, say, a qop\n> >> attribute but not a rspauth attribute. Also, while the cnonce is\n> >> required to be the same as used in the request, the \n> nonce-count isn't.\n> >> Hence I propose the following change in wording:\n> >>\n> >> Replace\n> >>\n> >> where \"Status-Code\" is the status code (e.g., \"200\") from the\n> >> \"Status-Line\" of the response, as defined in section 6.1 of [2],\n> >> and \"digest-uri-value\" is the value of the \"uri\" directive on the\n> >> Authorization header in the request. The \"cnonce-value\" MUST be\n> >> one for the client request to which this message is the response.\n> >>\n> >> by\n> >>\n> >> where \"Status-Code\" is the status code (e.g., \"200\") from the\n> >> \"Status-Line\" of the response, as defined in section 6.1 of [2],\n> >> and \"digest-uri-value\" is the value of the \"uri\" directive on the\n> >> Authorization header in the request. The \"cnonce-value\" and\n> >> \"nc-value\" MUST be the ones used in the client request to which\n> >> this message is the response.\n> >>\n> >> The \"response-auth\", \"cnonce\", and \"nonce-count\" attributes MUST\n> >> BE present if \"qop=auth\" or \"qop=auth-int\" is specified.\n> \n> PL> Good. Thanks for the proposed wording.\n> \n>   This was a cut-and-paste error, I think; the nc-value is not used in\n>   the construction of the response-digest, so it need not be in the\n>   syntax for the Authentication-Info header at all.  From client to\n>   server the requests may be pipelined, so we needed the nonce count\n>   to prevent replay, but each response is to exactly one unique\n>   request, so no count is needed or used.\n\nI don't understand -- the spec says that if qop=auth or auth-int,\n   request-digest  = <\"> < KD ( H(A1),     unq(nonce-value) \n                                       \":\" nc-value \n                                       \":\" unq(cnonce-value) \n                                       \":\" unq(qop-value) \n                                       \":\" H(A2)\n                               ) <\">\nso nc-value is used, and is needed in the Auth-info header.\n\nSo, I did the edits that Ron suggested.\n\nPaul\n\n\n\n", "id": "lists-012-4935073"}, {"subject": "Re: Authentication issue CNONCE: Proposed resolutio", "content": "Paul Leach wrote:\n> \n> How about -- if auth= or auth-int= are specified, cnonce= is required and\n> MUST be a value never used before by the client?\n\nI concur with the first part.  Is the second part a requirement on the\nclient, to avoid sending; on the server, to reject if it sees a\nduplicate; or both?  I oppose a MUST requirement on the server to reject\na set of credentials that includes a cnonce value that it had seen\nbefore.\n\nBTW, if this is a requirement on the client, is this a prohibition\nagainst sending the same cnonce value to different servers?\n\nDave Kristol\n\n\n\n", "id": "lists-012-4947107"}, {"subject": "Re: ISSUE: Protection spac", "content": "Paul Leach wrote:\n> \n> In section 3.2.1, The WWW-Authenticate Response Header\n> \n> OLD:\n> \n> domain\n> \n> A space-separated list of URIs, as specified in RFC XURI [7]. The intent is\n> that the client could use this information to know the set of URIs for which\n> the same authentication information should be sent. The URIs in this list\n> may exist on different servers. If this keyword is omitted or empty, the\n> client should assume that the domain consists of all URIs on the responding\n> server.\n> \n> NEW:\n> \n> domain\n> \n> A space-separated list of URIs, as specified in RFC XURI [7] that define the\n> protection space.  If a URI is relative, it is relative to canonical root\n> URL (see section 5.1.2 of [2]) of the server being accessed. The URIs in\n> this list may refer to different servers. The client can use this list to\n> determine the set of URIs for which the same authentication information may\n> be sent: any URI that has a URI in this list as a prefix (after both have\n> been made absolute) may be assumed to be in the same protection space. If\n> this keyword is omitted or empty, the client should assume that the\n> protection space consists of all URIs on the responding server.\n> \n> RATIONALE:\n> The terminology of \"protection space\" was not used for Digest. The means for\n> determining when Digest clients could use the same credentials was\n> under-specified.\n\nI agree the \"protection space\" for Digest needed to be specified.  I\nhave a problem with the proposed words above:\n\nI assume \"5.1.2 of [2]\" refers to the HTTP/1.1 spec.  The words\n\"canonical root URL\" do not appear there, and I am therefore unsure what\nwas meant.  Since all URLs on a server are implicitly descended from \"/\"\n(no?), wouldn't it be easier just to say that relative URLs are taken to\nbe relative to \"/\"?\n\nDave Kristol\n\n\n\n", "id": "lists-012-4955562"}, {"subject": "Re: Proxy Auth??", "content": "Paul Leach wrote:\n> \n> Is Proxy-Authorization only sent after 407, or can it also be sent after\n> 401? Section 3.6 (entitled Proxy-Authentication and Proxy-Authorization)\n> says that:\n> \n> Upon receiving a request which requires authentication, the proxy/server\n> must issue the \"HTTP/1.1 401 Unauthorized \" response with a\n> \"Proxy-Authenticate\" header.\n> \n> Section 1.2 says:\n> \n> The 401 (Unauthorized) response message is used by an origin server to\n> challenge the authorization of a user agent. This response MUST include a\n> WWW-Authenticate header field containing at least one challenge applicable\n> to the requested resource. The 407 (Proxy Authentication Required) response\n> message is used by a proxy to challenge the authorization of a client and\n> MUST include a Proxy-Authenticate header field containing a challenge\n> applicable to the proxy for the requested resource.\n\nSounds like a bug in the spec. to me.  WWW-Authenticate goes with 401,\nProxy-Authenticate goes with 407.\n\nThe paragraph at the end of 3.6 seems wrong.  I don't think you can get\nboth WWW-Authenticate *and* Proxy-Authenticate in one response.  First\nyou would get a 407 from the proxy, then a 401 from the origin server. \nBoth could occur, of course, on one request.\n\nDave Kristol\n\n\n\n", "id": "lists-012-4964449"}, {"subject": "Re: Digest Authentication Challenge Orderin", "content": "Paul Leach wrote:\n> \n> I propose that the user-agent MUST choose the strongest auth-scheme it\n> understands. This permits the server to put Basic first for old browsers (if\n> it finds Basic acceptably secure). The order really doesn't matter, since\n> the server is only supposed to offer minimally acceptable schemes.\n\nI concur.  But the specifications for various authenticate schemes also\nmust rank them by strength relative to the others.  (Yes, of course it's\neasy when we have just two, and their relative strengths are obvious.)\n\nDave Kristol\n\n\n\n", "id": "lists-012-4972342"}, {"subject": "Re: Authentication issue CNONCE: Proposed resolutio", "content": "Paul Leach wrote:\n> \n> How about -- if auth= or auth-int= are specified, cnonce= is required and\n> MUST be a value never used before by the client?\n\nI like requiring cnonce because it makes the implementation simpler, but the\nadvice about changing it should be just that - advice.  It does not affect\ninteroperability.  Put something in the Security Considerations.\n\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-4980345"}, {"subject": "RE: Digest Authentication Challenge Orderin", "content": "I know what Paul is trying to say, and I agree that it would be a good\nthing. My question is, is \"strongest\" unambiguous? Does it just mean\n\"maximum key length\"?\n\nI'm not trying to be pedantic - this is an important part of protecting\nagainst \"drop your shields\" man-in-the-middle attacks, and I'd like to see\nthe spec be pretty precise about a user's exposure to server selection of a\n\"weaker\" authentication scheme when a stronger scheme could be used. But I\ncan't define \"weak\" and \"strong\" either!\n\nSpencer\n\n> -----Original Message-----\n> From:Paul Leach [SMTP:paulle@MICROSOFT.com]\n> Sent:Friday, August 07, 1998 2:57 AM\n> To:'http-wg@hplb.hpl.hp.com'\n> Subject:RE: Digest Authentication Challenge Ordering\n> \n> I propose that the user-agent MUST choose the strongest auth-scheme it\n> understands. This permits the server to put Basic first for old browsers\n> (if\n> it finds Basic acceptably secure). The order really doesn't matter, since\n> the server is only supposed to offer minimally acceptable schemes.\n\n\n\n", "id": "lists-012-4988208"}, {"subject": "RE: Authentication issue CNONCE: Proposed resolutio", "content": "This is a MUST on the client in order for it to ensure its own security, not\nin order to interoperate. It imposes no burden on servers.\n\nIn order to be safe, it is indeed true that the client should never send the\nsame value, even to different servers. If a server can predict what the\nclient will send, then we're back in chosen-plaintext-attack land.\n\n-----Original Message-----\nFrom: Dave Kristol [mailto:dmk@bell-labs.com]\nSent: Friday, August 07, 1998 6:52 AM\nTo: Paul Leach\nCc: 'Scott Lawrence'; Larry Masinter; HTTP Working Group\nSubject: Re: Authentication issue CNONCE: Proposed resolution\n\n\nPaul Leach wrote:\n> \n> How about -- if auth= or auth-int= are specified, cnonce= is required and\n> MUST be a value never used before by the client?\n\nI concur with the first part.  Is the second part a requirement on the\nclient, to avoid sending; on the server, to reject if it sees a\nduplicate; or both?  I oppose a MUST requirement on the server to reject\na set of credentials that includes a cnonce value that it had seen\nbefore.\n\nBTW, if this is a requirement on the client, is this a prohibition\nagainst sending the same cnonce value to different servers?\n\nDave Kristol\n\n\n\n", "id": "lists-012-4997526"}, {"subject": "RE: ISSUE: Protection spac", "content": "> -----Original Message-----\n> From: Dave Kristol [mailto:dmk@bell-labs.com]\n> Sent: Friday, August 07, 1998 7:05 AM\n> To: Paul Leach\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: ISSUE: Protection space\n> \n>\n> > RATIONALE:\n> > The terminology of \"protection space\" was not used for \n> Digest. The means for\n> > determining when Digest clients could use the same credentials was\n> > under-specified.\n> \n> I agree the \"protection space\" for Digest needed to be specified.  I\n> have a problem with the proposed words above:\n> \n> I assume \"5.1.2 of [2]\" refers to the HTTP/1.1 spec.  The words\n> \"canonical root URL\" do not appear there, and I am therefore \n> unsure what was meant. \n\nMumph. I just copied that text from section 1 of the spec. I think it means\nthat if the server is \"www.foo.com\" then the cononincal root URL is\n\"http://www.foo.com/\"\n\n> Since all URLs on a server are implicitly \n> descended from \"/\"\n> (no?), wouldn't it be easier just to say that relative URLs \n> are taken to\n> be relative to \"/\"?\n\nThe list allows absolute URIs with host names other than that of the server\nsending the \"domain\" directive.\n\nPaul\n\n\n\n", "id": "lists-012-5007655"}, {"subject": "Re: Authentication issue CNONCE: Proposed resolutio", "content": "Paul Leach wrote:\n> \n> This is a MUST on the client in order for it to ensure its own \n> security, not in order to interoperate. It imposes no burden on \n> servers.\n> \n> In order to be safe, it is indeed true that the client should never \n> send the same value, even to different servers. If a server can \n> predict what the client will send, then we're back in \n> chosen-plaintext-attack land.\n\nrom RFC 2119 RFC Key Words:\n\n6. Guidance in the use of these Imperatives\n\n  Imperatives of the type defined in this memo must be used with care\n  and sparingly.  In particular, they MUST only be used where it is\n  actually required for interoperation or to limit behavior which has\n  potential for causing harm (e.g., limiting retransmisssions)  For\n  example, they must not be used to try to impose a particular method\n  on implementors where the method is not required for\n  interoperability.\n\n7. Security Considerations\n\n  These terms are frequently used to specify behavior with security\n  implications.  The effects on security of not implementing a MUST or\n  SHOULD, or doing something the specification says MUST NOT or SHOULD\n  NOT be done may be very subtle. Document authors should take the time\n  to elaborate the security implications of not following\n  recommendations or requirements as most implementors will not have\n  had the benefit of the experience and discussion that produced the\n  specification.\n\nI read that to say that no matter how we write it up we have to have\nsomething in the security considerations section that says that if you blow\nthis part you are messing yourself up.  It is my personal preference that we\nnot use MUST where there is no harm to interoperability or to other parties\n- shooting yourself in the foot should be allowed.  I am concerned that (as\nDaves query exemplifies) using a MUST may mislead servers into thinking that\nthere is something there to enforce, which is not our intent.\n\nThat having been said - you make the call Paul; I can live with it either\nway.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-5017107"}, {"subject": "Re: ISSUE: Protection spac", "content": "Paul Leach wrote:\n> [...]\n> > [DMK]\n> > Since all URLs on a server are implicitly\n> > descended from \"/\"\n> > (no?), wouldn't it be easier just to say that relative URLs\n> > are taken to\n> > be relative to \"/\"?\n> \n> The list allows absolute URIs with host names other than that of the server\n> sending the \"domain\" directive.\n\nAre we talking about two different things?  I'm not concerned with\nabsolute URLs.  For them the protected set of URLs is obvious.\n\nHere's the wording at issue (Sect. 3.2.1):\nIf a URI is relative, it is relative to [the] canonical root URL of the\nserver being accessed.\n\nMy notion of a relative URL is one that does not begin with '/'.  For\nsuch a URL, wouldn't it make sense to give them an implicit '/' prefix?\n\nDave Kristol\n\n\n\n", "id": "lists-012-5027133"}, {"subject": "RE: ISSUE: Protection spac", "content": "> -----Original Message-----\n> From: Dave Kristol [mailto:dmk@bell-labs.com]\n> Sent: Friday, August 07, 1998 11:04 AM\n> To: Paul Leach\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: ISSUE: Protection space\n> \n> \n> Paul Leach wrote:\n> > [...]\n> > > [DMK]\n> > > Since all URLs on a server are implicitly\n> > > descended from \"/\"\n> > > (no?), wouldn't it be easier just to say that relative URLs\n> > > are taken to\n> > > be relative to \"/\"?\n> > \n> > The list allows absolute URIs with host names other than \n> that of the server\n> > sending the \"domain\" directive.\n> \n> Are we talking about two different things?\n\nMaybe. See next comment.\n\n  I'm not concerned with\n> absolute URLs.  For them the protected set of URLs is obvious.\n\nI think of two kinds of \"relative URLs\"  -- \"dir/foo.html\" and\n\"/dir/foo.html\". The latter is relative to (e.g.) http://www.xxx.com, the\nformer to the URL of page in which it appears (typically). I don't think the\nformer belong in a domain list. \n\n> Here's the wording at issue (Sect. 3.2.1):\n> If a URI is relative, it is relative to [the] canonical root \n> URL of the\n> server being accessed.\n> \n> My notion of a relative URL is one that does not begin with '/'.  For\n> such a URL, wouldn't it make sense to give them an implicit \n> '/' prefix?\n\nHow about I say that URI in \"domain=URI...\" must be an \"http_UTL\" or\n\"abs_path\" as defined in section 3.2.2 of the HTTP/1.1 spec? \nThe former is the usual \"http://www.xxx.com:port/dir/foo.html\" type; the\nlatter is \"/dir/foo.html\".\n\nPaul\n\n\n\n", "id": "lists-012-5034910"}, {"subject": "Re: ISSUE: Protection spac", "content": "Paul Leach wrote:\n>> [...]\n> I think of two kinds of \"relative URLs\"  -- \"dir/foo.html\" and\n> \"/dir/foo.html\". The latter is relative to (e.g.) http://www.xxx.com, the\n> former to the URL of page in which it appears (typically). I don't think the\n> former belong in a domain list.\n> \n> > Here's the wording at issue (Sect. 3.2.1):\n> > If a URI is relative, it is relative to [the] canonical root\n> > URL of the\n> > server being accessed.\n> >\n> > My notion of a relative URL is one that does not begin with '/'.  For\n> > such a URL, wouldn't it make sense to give them an implicit\n> > '/' prefix?\n> \n> How about I say that URI in \"domain=URI...\" must be an \"http_UTL\" or\n> \"abs_path\" as defined in section 3.2.2 of the HTTP/1.1 spec?\n> The former is the usual \"http://www.xxx.com:port/dir/foo.html\" type; the\n> latter is \"/dir/foo.html\".\n\nThat's much clearer.  Sold.\n\nDave Kristol\n\n\n\n", "id": "lists-012-5044912"}, {"subject": "Re: ISSUE: Protection spac", "content": ">How about I say that URI in \"domain=URI...\" must be an \"http_UTL\" or\n>\"abs_path\" as defined in section 3.2.2 of the HTTP/1.1 spec? \n>The former is the usual \"http://www.xxx.com:port/dir/foo.html\" type; the\n>latter is \"/dir/foo.html\".\n\nIt's odd that it isn't relative to the requested resource.  Has someone\nchecked the implementations to see what is normally returned?\n\nIn any case, \"http_URL\" is too restrictive.  That should be \"absoluteURI\",\nsince it would allow for the use of HTTP with different schemes\n(think http, https, ipp, ...).\n\n....Roy\n\n\n\n", "id": "lists-012-5052949"}, {"subject": "Re: MUST-MAYSHOULD (MMS) audit..", "content": "Jeffrey Mogul:\n>\n[....]\n> In almost every case, I did point out\n>that the Note in question contained apparently normative\n>language.\n\nUsually this apparently normative language merely explains or restates\nactual normative language in the main text.  I recall that at least\none editor (Roy?) had the policy of stating all normative requirements\nexactly once, in order to reduce the risks of\nself-contradictions. This policy leads to the editorial device of\nusing a note whenever you want to explain or restate requirements\nalready stated once.\n\n>  I didn't want to delete requirements simply because\n>they appeared in Notes, but I also didn't want to spend the\n>time to rewrite large chunks of text.\n\nWhenever reviewing a note in the past I have always kept in mind that\nit could not contain a normative requirement, so I am pretty sure that\nthe set of the requirements in the draft won't change if we leave all\nnotes non-normative.\n\n[...]\n>In other words, I'm not sure why Koen thinks that I \"took an\n>opposite view\" rather than making \"judgement calls on a note\n>by note basis.\"  \n\nJust an overall impression I got.  I may have missed some exceptions.\n\n>Nevertheless, if we really want to be consistent\n>on this formality, then someone should spend some effort\n>splitting some of these Notes into normative and true-Note parts.\n\nI believe being consistent on this is very necessary, so I would like\none of the editors to take the effort.\n\n>    MMS 025: I am not sure if Jeff is right in his assumption as to\n>    what the term \"common form\" is supposed to mean.  Maybe \"common\n>    form\" means `not extended over multiple lines' here.\n>\n>As I wrote, we don't have a definition for \"common form\" in general, so\n>we can argue until the end of the millenium about what this paragraph\n>actually means. \n\nHmm, I was hoping that the original author of that sentence, or maybe\nsomeone knowledgeable of email/news header terminology, would come\nforward and clarify.\n\n[...]\n>    MMS 117: The musts in the two items define conditions to be met for\n>    a MAY to apply, so they should be lowercase (or preferably\n>    rephrased), not uppercase.\n>\n>I think your logic is faulty.  An analogy: in this sentence,\n>\n>You MAY pilot an airliner, but in order to do so, you\n>MUST have a pilot's license.\n>\n>it is pretty clear that the phrase \"you MUST have a pilot's\n>license\" does not apply to everyone; it only applies to pilots,\n>but then it is an \"absolute requirement of the specification\"\n>(to quote from RFC2119).  I think you are arguing that one should\n>instead say\n>\n>You MAY pilot an airliner, but in order to do so, you\n>ought to have a pilot's license.\n>\n>which has a very different meaning (and one that would convince\n>me to travel by train).\n\nNo, I am arguing that you should say\n\n  If you have a pilot's license you MAY pilot an airliner.\n\nwhich avoids the whole problem.  Compare this to the following piece\nof text from the spec:\n\n       If the response includes the \"must-revalidate\" Cache-Control\n       directive, the cache MAY use that response in replying to a\n       subsequent request.\n\nThe logical structure of such things is\n\n  if 'condition' then ( 'subject' MAY 'action' )\n\nMy sense of grammar tells me that the 'condition' part, if it includes\nthe verbs must, may, or should, should never have these capitialised\nas keywords, because in the condition parts these verbs do not signal\nan actual requirement on a subject in the sense of rfc2219.  I got\nmost of my grammatical intuition from reading Latin and writing C, so\nI may be wrong, but I don't think so.\n\n>    MMS 125: in the definition of \"invalidate an entity\", the two\n>    shoulds are defining a term, they are are not keywords specifying a\n>    requirement, so they should be rephrased, e.g. from `should' to\n>    `will'.\n>\n>I believe that this language basically defines a \"subroutine\"\n>rather than simply a \"term\".  It's giving a specific meaning to a\n>verb-phrase when that phrase is used in this specification.\n\nWell, I agree that this could be a \"subroutine\" too, but as you also\nnote, this does not solve the problem in the spec, so we should fix\nthis one way or the other.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-012-5060600"}, {"subject": "Fwd: Closures for remaining technical Digest Issue", "content": "This mail should clearly have gone to the general mailing list....\n\nI'm updating the issues list to reflect this.\n- Jim\n\n\nattached mail follows:\na1 REQUEST-DIGEST:   \n    http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0031.html\n\n    I believe that the suggested resolution in the mail is correct \n    - the syntax should just be:\n\n        request-digest = <\"> *LHEX <\">\n\na2 CNONCE:\n\n    I just posted by suggestion about this - use the null string, add\n    a little text to security considerations about why it is a bad idea.\n\na3 NONCE-ETAG:\n\n    I believe that Larrys message on this was on the mark\n    ( http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q3/0028.html )\n    to leave it as is.  There has been no objection, so leave it.\n\na4 DIGEST-MULTIPART \n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0040.html\n\n   is, I think, not an issue - if you think that we need some text   \n   clarifying what is meant by entity-body in a multipart response, then\n   I guess we could add some, but I can't think where it belongs - ideas?\n\na5 CHALLENGE-ORDER\n   I posted my proposed fix as:\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q3/0057.html\n\n   it has been suggested that the statement I put in about not using\n   basic or other replayable scheme is redundant.  I like it, and think\n   that it will make the IESG happier, but use your own judgement.  There\n   has been no other substantive comment.\n\nI think that closes all the technical issues.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\nattached mail follows:\nScott Lawrence <lawrence@agranat.com> wrote:\n  > a4 DIGEST-MULTIPART \n  >    http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0040.html\n  > \n  >    is, I think, not an issue - if you think that we need some text   \n  >    clarifying what is meant by entity-body in a multipart response, then\n  >    I guess we could add some, but I can't think where it belongs - ideas?\n\nI, for one, think it deserves a clarification.  (If it had been clear,\nI wouldn't have asked my question.)\n\nI need to leave for the day, but I can try to suggest words and\npinpoint a place to put them tomorrow, unless someone else beats me to\nit.\n\nDave\n\nattached mail follows:\n  >   > a4 DIGEST-MULTIPART \n  >   >    http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q2/0040.html\n  >   > \n  >   >    is, I think, not an issue - if you think that we need some text   \n  >   >    clarifying what is meant by entity-body in a multipart response, then\n  >   >    I guess we could add some, but I can't think where it belongs - ideas?\n  > \n  > I, for one, think it deserves a clarification.  (If it had been clear,\n  > I wouldn't have asked my question.)\n  > \n  > I need to leave for the day, but I can try to suggest words and\n  > pinpoint a place to put them tomorrow, unless someone else beats me to\n  > it.\n\nAs promised, a proposal:\nIn \"3.2.2 The Authorization Request Header\", amend this paragraph:\n\n    Also note that if integrity protection is applied (qop=auth-int),\n    the H(entity-body) is the hash of the entity body, not the\n    message body - it is computed before any transfer encoding is\n    applied by the sender and after it has been removed by the\n    recipient.\n\nAdd to it:\n    Further note that, if the entity comprises a multipart message-body,\n    H(entity-body) is a hash of the entire multi-part message-body,\n    including its MIME header parts.\n\nattached mail follows:\n\nOn Tue, 4 Aug 1998, Dave Kristol wrote:\n\n> Add to it:\n>     Further note that, if the entity comprises a multipart message-body,\n>     H(entity-body) is a hash of the entire multi-part message-body,\n>     including its MIME header parts.\n\nSounds good.\n\n\n\n", "id": "lists-012-5072459"}, {"subject": "Re: MUST-MAYSHOULD (MMS) audit..", "content": ">Usually this apparently normative language merely explains or restates\n>actual normative language in the main text.  I recall that at least\n>one editor (Roy?) had the policy of stating all normative requirements\n>exactly once, in order to reduce the risks of\n>self-contradictions. This policy leads to the editorial device of\n>using a note whenever you want to explain or restate requirements\n>already stated once.\n\nThe editorial policy of using a note to explain non-normative design\ndecisions comes from the MIME specs (or at least that's where I got\nit from).  Anything that is normative should just be changed to a\nnon-note.\n\n>>    MMS 025: I am not sure if Jeff is right in his assumption as to\n>>    what the term \"common form\" is supposed to mean.  Maybe \"common\n>>    form\" means `not extended over multiple lines' here.\n>>\n>>As I wrote, we don't have a definition for \"common form\" in general, so\n>>we can argue until the end of the millenium about what this paragraph\n>>actually means. \n\nWhen I originally wrote \"common form\" it wasn't associated with a\nrequirement -- it was implementation advice.  \"common form\" means\njust that -- whatever is commonly used by other applications at the\ntime of implementation, which might have changed over time.\n\nIf you want to spell it out, I'd say the current common form is\n\n   fieldname: fieldvalue\n\nwith no extra space or line continuation folding.\n\n....Roy\n\n\n\n", "id": "lists-012-5083949"}, {"subject": "Protocol Action: Uniform Resource Identifiers (URI): Generic Syntax and Semantics to Draft Standar", "content": "Note that the HTTP/1.1 specifications as currently drafted have dependencies\non this document.\n\nThis dependency is now resolved; as soon as our own interoperability testing\nis complete, we will be able to forward our specs for Draft Standard\n(previously, we would have had to remove the dependencies, which would\nhave resulted in duplication between the HTTP/1.1 specs and the URI specs,\nwhich would not have been a good situation.\n- Jim\n\n---------------- Forwarded  Message ----------------------\n\n<headers snipped>\n\n\nThe IESG has approved the Internet-Draft 'Uniform Resource Identifiers\n(URI): Generic Syntax and Semantics' <draft-fielding-uri-syntax-04.txt>\nas a Draft Standard. This has been reviewed in the IETF but is not the\nproduct of an IETF Working Group.\n\nThis document updates RFC1808 and RFC1738.\n\nThe IESG contact persons are Patrik Faltstrom and Keith Moore.\n\n\nTechnical Summary\n\nThis document revises and replaces the generic definitions in RFC 1738 and\nRFC 1808. Revision was needed because of last years experience in\nimplementations of various URL schemes, aswell as the creation of the URN\nspecification. Clearifications were needed regarding many basic assumptions\nin the old documents which was not spelled out explicitely. All significant\nchanges from the prior RFCs are noted in Appendix G.\n\nWorking Group Summary\n\nDiscussion has been very intense regarding if the document should be about\nURL schemes, or URI schemes. Even when consensus was found on this issue,\nit was obvious that various reviewers had different opinions on basic\ndefinitions. Alternative papers were produced, but the discussion converged\nto an updated version of this document.\n\nDefinitions of functions and methods that can only be applicable to some\nURI schemes were also up for debate. Some parties wanted those\n(specifically fragments and relative URIs) in separate documents extracted\nfrom this general syntax document, while others claimed a need for\ndescribing not only syntax but also algorithms and methods for for example\ncalculating the resulting URI from a relative URI existing inside a\ndocument.\n\nThe conclusion was at the end that the description of the methods for some\nbasic algorithms (like relative URIs) should stay in the document because\nit can be used in several URI schemes, and in those cases relative URIs\nshould be the same.\n\n\nProtocol Quality\n\nPatrik Faltstrom reviewed the specification for IESG.\n\nIt has been tested on:\n\n    Mozilla/4.03 [en] (X11; U; SunOS 5.5 sun4u; Nav)\n    Lynx/2.7.1 libwww-FM/2.14\n    MSIE 3.01; Windows 95\n    NCSA_Mosaic/2.6 (X11;SunOS 4.1.2 sun4m) libwww/2.12\n\n\nRFC Editor:\n\nPlease insert the following text as an IESG Note:\n\nThis paper describes some kind of \"superset\" of all functions and methods that\ncan be applied to URIs. It consists of both a grammar and a description of\nbasic\nfunctionality for URIs. To understand what is a valid URI, both the grammar\nand the apropriate description have to be studied. Also, some functions and\nmethods described only works in some URI schemes, and some only with\ncertain content types (i.e. regardless of scheme used).\n\n\n\n", "id": "lists-012-5092403"}, {"subject": "RE: ISSUE: Expect Header Field Proble", "content": "If Expect continues in the spec then there can be only two reasonable\noptions:\n\n1. Require the client to perform discovery on the resource (by asking for\nOPTIONS and looking for a particular header, for example) before using\nExpect with that resource.\n\n2. Change the version number to HTTP/1.2.\n\nYaron\n\n> -----Original Message-----\n> From: Scott Lawrence [mailto:lawrence@agranat.com]\n> Sent: Tuesday, July 28, 1998 12:21 PM\n> To: Henrik Frystyk Nielsen\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: ISSUE: Expect Header Field Problem\n> \n> \n> Henrik Frystyk Nielsen wrote:\n> > \n> >  The current wording in section 14.20 is nonsense as it renders all\n> > existing HTTP/1.1 servers not compliant:\n> > \n> > The Expect request-header field is used to indicate that particular\n> > server behaviors are required by the client. A server that does not \n> > understand or is unable to comply with any of the \n> expectation values \n> > in the Expect field  of a request MUST respond with \n> appropriate error\n> > status.\n> \n> In what sense, Henrik?  My server responds with Expectation \n> Failed if you\n> send a token in Expect: that it doesn't recognize...\n> \n> Yes, this was not in 2068, but this is not the only thing we've added.\n> \n> I believe that the MUST should stand; making it a SHOULD \n> renders the Expect\n> feature almost useless.\n> \n> -- \n> Scott Lawrence            Consulting Engineer        \n> <lawrence@agranat.com>\n> Agranat Systems, Inc.   Embedded Web Technology     \nhttp://www.agranat.com/\n\n\n\n", "id": "lists-012-5102281"}, {"subject": "I-D ACTION:draft-ietf-http-authentication02.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: HTTP Authentication: Basic and Digest Access \n                          Authentication\nAuthor(s): J. Franks, E. Sink, P. Leach, J. Hostetler, \n                          P. Hallam-Baker, L. Stewart, S. Lawrence\nFilename: draft-ietf-http-authentication-02.txt\nPages: 29\nDate: 11-Aug-98\n\n'HTTP/1.0' includes the specification for a Basic Access Authentication^M\nscheme. This scheme is not considered to be a secure method of user^M\nauthentication (unless used in conjunction with some external secure^M\nsystem such as SSL [5]), as the user name and password are passed over^M\nthe network as cleartext.^M\n \nThis document also provides the specification for HTTP's authentication^M\nframework, the original Basic authentication scheme and a scheme based^M\non cryptographic hashes, referred to as 'Digest Access Authentication'.^M\nIt is therefore also intended to serve as a replacement for RFC 2069^M\n[6].  Some optional elements specified by RFC 2069 have been removed^M\nfrom this specification due to problems found since its publication;^M\nother new elements have been added -for compatibility, those new^M\nelements have been made optional, but are strongly recommended.^M\n \nLike Basic, Digest access authentication verifies that both parties to a^M\ncommunication know a shared secret (a password); unlike Basic, this^M\nverification can be done without sending the password in the clear,^M\nwhich is Basic's biggest weakness. As with most other authentication^M\nprotocols, the greatest sources of risks are usually found not in the^M\ncore protocol itself but in policies and procedures surrounding its use.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-authentication-02.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-authentication-02.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-authentication-02.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-5112614"}, {"subject": "Editorial ISSUE: Warning x1", "content": "(Sorry Jim, I didn't catch this in rev-03).\n\nSection 13.5.2 refers to warning _114_, transformation applied.\nSection 14.46 however says that transformation applied is _214_.\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-5122436"}, {"subject": "Fwd: draft-ietf-http-v11-spec-rev04 comment", "content": "Paul Bennet has done a complete, nit-picking, read of draft 04... My thanks\nto him for the read.\n\nHaving recently become convinced that there will (almost certainly) be\na draft 05,  I'm forwarding this list to the mailing list so it\nwill get archived.\n- Jim\n\n\nattached mail follows:\nJim,\n\nI've just completed reviewing the '-04' revision of the HTTP/1.1\nspec, and have some comments for you.\n\nMost of these comments are simple editorial matters.  There are a\nfew genuine errors I noticed; I've listed things in decreasing order\nof severity (I'd understand if you never get to the end of this\nlist, but I'd rather you dealt with the first couple).\n\nThe spec's looking in good shape; I bet you'll be pleased to see it\nreach Draft Standard.\n\n\nPaul.\n\n\nOut and out mistakes\n\n * Page 2, last sentence: \"See section 18 for the full copyright\n   notice.\" - that should be section *20*.  (I believe Henrik has\n   already notified you of this one.)\n\n * Section 2.1 \"Augmented BNF\", p15, last sentence of first\n   paragraph (\"implied *LWS\" rule): '(for the definition of\n   \"token\" above)' - you mean, *below* (the def's in section 2.2).\n\n * Section 5.1.2 \"Request-URI\", p33, second para talks about\n   replacing 'a null abs_path with \"*\"' - surely it's replaced\n   with a \"/\".\n\n * Section 8.1.3 \"Proxy Servers\", first paragraph: \"... as specified\n   in section 14.2.1.\" - no such section exists; maybe you mean\n   section 14.10?\n\n * Section 13.2.4 \"Expiration Calculations\", end of second paragraph:\n   \"(see section 14.10.\" - missing right bracket, and the section\n   number's wrong - should be 14.9.3.\n\n * Section 13.5.2 \"Non-modifiable Headers\", page 79, last sentence\n   on page refers to \"Warning 114 (Transformation Applied)\".  That\n   should be \"214\", according to section 14.46, page 126.  This\n   sentence also includes repetition: \"..., if not already present,\n   ... if one does not already appear in the message.\".\n\n * Section 14.24 \"If-None-Match\", third para on p112, last sentence:\n   \"... then the server MUST not return a 304 ...\"; the \"not\" should\n   be capitalised: \"... the server MUST NOT return a 304 ...\".\n\n * Section 14.31 \"Max-Forwards\" refers to section 14.31 for the\n   TRACE method; this should be section 9.8.\n\n * Section 19.4.2 \"Conversion to Canonical Form\" refers to \"Appendix\n   G of RFC 2045\" - such an appendix doesn't exist!  The closest I\n   can find is section 6.6... or maybe you mean something else.\n\n * Section 13.5.1 \"End-to-end and Hop-by-hop Headers\" refers to the\n   Keep-Alive field, which doesn't exist any more.\n\n\nClarifications\n\n * Section 2.2 \"Basic Rules\", \"TEXT\" rule: \"<any OCTET except CTLs,\n   but including LWS>\".  This excludes the optional \"CRLF\" of \"LWS\"\n   by virtue of those two octets being CTLs, but it took a second\n   reading to see this; perhaps \"<any OCTET except CTLs, but\n   including HT and SP>\" would be clearer.\n\n * Section 2.2 \"Basic Rules\", \"Note:\" in \"quoted-pair\" description:\n   I *think* I get what the example's on about: it's the\n   significance of an HT and/or SP before \"a\" and \"quoted-string\",\n   right?  But the indentation of the example doesn't make this\n   clear; perhaps indent those two lines more than the first of\n   the example, or add more description to the text.\n\n * Sections 9.3 \"GET\", 9.4 \"HEAD\" and 9.7 \"DELETE\" don't explicitly\n   rule out the presence of an entity-body, as section 9.8 \"TRACE\"\n   does.\n\n * Section 10.2.2 \"201 Created\" says nothing about the format of the\n   response entity.  (section 10.3.1 \"300 Multiple Choices\" in a\n   similar position is explicitly non-committal.\n\n * Section 10.3.6 \"305 Use Proxy\": \"The Location field gives the URI\n   of the proxy.\"  But nowhere can I find a specification for this\n   URI form; presumably it's an HTTP URI without an abs_path part.\n\n\nTypos\n\n * Section 3.6 \"Transfer Codings\", third paragraph: \"section\n   14.4114.41\" clearly should read \"section 14.41\".\n\n * Section 3.7.2 \"Multipart Types\", third paragraph is a repetition\n   of the penultimate sentence of the second paragraph (but maybe you\n   intended the repetition to emphasise the point!).\n\n * Section 10.1 \"Informational 1xx\": \"There are no required headers\n   for this class of status codes.\"  Should be \"code\" singular, I\n   think.\n\n * Section 10.2.7 \"206 Partial Content\", first bullet point: \"If a\n   Content-Length header field is present in the response MUST match\n   ...\" words missing!  Probably want \"... in the response, its value\n   MUST match ...\".\n\n * Section 13.2.4 \"Expiration Calculations\", page 72, second para:\n   weird additional whitespace in first line of this paragraph.\n   Also, the last sentence includes repetition: \"If the value is\n   greater than 24 hours, ... whose age is more than 24 hours ...\".\n\n * Section 13.3.3 \"Weak and Strong Validators\", first two sentences\n   should be a single one (the first one doesn't make sense alone;\n   perhaps replace \"... different entities.  One normally ...\" with\n   \"... different entities, one normally ...\".\n\n * Section 13.5.3 \"Combining Headers\", second paragraph starts \"In\n   the status code is 304\" - that should be \"If the status code ...\".\n   The last sentence of this paragraph also repeats \"see 13.5.4\".\n\n * Section 13.5.4 \"Combining Byte Ranges\", last paragraph: \"If either\n   requirement is not meant ...\" should be \"not met\", presumbaly.\n\n * Section 13.13 \"History Lists\", fourth paragraph: \"This is not be\n   construed to prohibit\" - should be \"This is not construed ...\"\n\n * Section 14.8 \"Authorization\", last sentence of first para on p91:\n   \"(assuming that the authentication schemed ...\" should be\n   \"scheme\" (spurious 'd').\n\n * Section 14.21 \"Expires\", last paragraph \"... an response ...\"\n   should be \"... a response ...\"\n\n * Section 14.25 \"If-None-Match\", end of first sentence: \"... used\n   with a method to make the method conditional.\" should be \"... used\n   with a method to make it conditional.\".  This brings the wording\n   in line with that of the other If-* headers, as well as saving\n   bytes :-)\n\n * Section 14.35.2 \"Range Retrieval Requests\", second para on p118:\n   \"intermediate caches ought tosupport\" - space missing between \"to\"\n   and \"support\".\n\n * Section 14.45 \"Via\", first para: spurious whitespace at start\n   of fifth line.\n\n * Section 14.46 \"Warning\", first sentence: \"transformation of a\n   message whichmight\" - missing space.  Also spurious extra space\n   at the start of the fourth para on p125: \" Warning headers can\n   ...\".\n\n * Section 19.6.3 \"Changes from RFC 2068\", p146, third from last\n   paragraph: extra right bracket: \"A new error code (416))\".\n\n\nThings you've probably debated to death\n\n * Throughout: \"inbound\" vs. \"upstream\" - these terms seem to refer\n   to the same concept, but are not defined or used consistently.\n     \"inbound\":   13.11, 14.31, 14.35.2, 19.6.2\n     \"outbound\":  13.2.3, 14.34\n     \"upstream\":  10.5.3, 10.5.5, 14.45, 19.6.3,\n     \"downstram\": 14.33\n\n * Section 3.6.1 \"Chunked Transfter Coding\", \"chunk-data\"\n   production: \"chunk-size(OCTET)\" at a casual glance *might* imply\n   \"1*HEX OCTET\", when presumably you mean \"n*n OCTET ; where n is\n   the value of chunk-size\".\n\n * Section 15.1.2 \"Transfer of Sensitive Information\": given recent\n   well-publicised security holes in specific user agents, perhaps\n   \"User-agent\" should be added to the list of sensitive fields.\n\n\nCross-references\n\n * Throughout: I'd like to see more cross-referencing.  Particularly,\n   against most occurrances of method names, field names, status and\n   warning codes.\n\n * Section 1.2 \"Requirements\": the key words \"SHALL\", \"REQUIRED\",\n   \"MAY\" and \"OPTIONAL\" are mentioned, but requirements for\n   (un)conditionally compliant implementations with respect to\n   these words are not laid down.  Suggest: replace \"the MUST\n   requirements\" with \"the MUST, REQUIRED and SHALL requirements\".\n\n * Section 3.2.2 \"http URL\", second paragraph: \"... and the\n   Request-URI ...\" a forward reference to section 5.1.2 (for\n   Request-URI) would be helpful.\n\n * Section 3.3.1 \"Full Date\": needs forward-reference to section\n   19.3 (concerning the \"year 2000 problem\").\n\n * Section 8.2.1 \"Persistent Connections and Flow Control\":\n   reference to section 3.6 (\"chunked encoding\") should more\n   accurately be a reference to section 3.6.1.\n\n * Section 13.6 \"Caching Negotiated Responses\", first sentence refers\n   to section 12; 12.1 would be more accurate.\n\n * Section 14.4 \"Accapt-Language\" should refer to section 3.10.\n\n * Section 14.5 \"Accept-Range\" should refer to section 3.12.\n\n * Section 14.15 \"Content-Encoding\", second para on p100 refers to\n   14.15 itself!\n\n * Section 14.15 \"Content-Range\" should refer to section 3.12.\n\n * Section 14.21 \"Expires\", third paragraph refers to section 3.3;\n   3.3.1 would be more accurate.\n\n * Section 14.24 \"If-Match\" should refer so section 3.11.\n\n\nReally petty stuff\n\n * Section 4.5 \"General Header Fields\": the BNF is *almost* in\n   alphabetical order (shift \"Trailer\" up two places and you're\n   there).  Also: comment part of \"Warning\" isn't quite aligned\n   with the other comments.\n\n * Section 5.3, \"Request Header Fields\": more alphabetical BNF:\n   \"If-Match\" should appear before \"If-Modified-Since\".\n\n * Section 10.2.5 \"204 No Content\" and section 10.2.6 \"205 Reset\n   Content\", last sentence of each, differ in their terminology:\n   204 says \"The 204 response MUST NOT include a message-body\";\n   205 says \"The response MUST NOT include an entity\".\n\n * Section 10.4.15 \"414 Request URI Too Long\": spurious extra\n   space before the full-stop at the end of the first sentence.\n\n * Section 13.1.4 \"Explicit User Agent Warnings\": spurious extra\n   full-stop at the end of the first paragraph.\n\n * Section 19.6.3 \"Changes from RFC 2068\", top of p146: spurious\n   extra space at start of first line: \" Content-Base was\n   deleted ...\".  Also, spurious extra whitespace at the start\n   of points six and sever (page 147).\n\n\nThings you know about, but I'll mention anyway\n\n * Throughout: The [jg] footnotes\n\n * Throughout: \"HTTP Authentication: Basic and Digest Access\n   Authentication\"; presumably you're waiting for an RFC number\n   (but you might also flag these as [43]).  This appears at least\n   in these sections: 10.4.2, 10.4.8, 11, 14.8, 14.33, 14.34, 14.47\n\n * Section 14.16 \"Content-Range\", second para on p104, \"See appendix\n   Error! Reference source not found\".\n\n * Section 19.1 \"Internet Media Type message/http and\n   application/http\": screwy line-wrapping in the Encoding\n   considerations section of application/http.\n\n * Section 19.6 \"Compatibility with Previous Versions\", last\n   sentence: \"Error! Reference source not found\" - presumably this\n   should instead refer to RFC2068 since Keep-Alive has now gone.\n\n * Section 21 \"Index\": the index is broken - at least, a good\n   percentage of the referenced pages are irrelevant, which is\n   a real shame because the index looks great and is *vital* in\n   a document this size :-(\n\n\n_____________________________________________________________\nNotice:  This contribution is the personal view of the author \nand does not necessarily reflect the technical nor commercial \ndirection of British Telecommunications plc.\n_____________________________________________________________\n\n\n\n", "id": "lists-012-5128918"}, {"subject": "ISSUE: transformation", "content": "The wordage around application of transformations to entities [by\nproxies] seems to deal only with what headers can be changed\n(presumably interoperability issues are thought to only be possible \nin that case).\n\nHowever, imagine for a moment that my proxy applies a transformation\nthat neither changes the Content-Type nor the Content-Encoding (for the\nsake of argument, call it the \"ee cummings\" transform: on text types\nonly, lowercase and remove punctuation).\nEven for the example used in the spec, if one were to change the\nresolution of a medical image, it would be a potentially bad thing.\n\nNow, for the questions:\n\n1. Content-MD5, for example, is an end-to-end header (which, according\nto 13.5.1, MUST be stored and forwarded, and according to 14.15 MUST\nNOT be generated by proxies.  If I leave it in, it will be wrong.  If I\nremove it, I run the risk of breaking an application.  Gaaack.\n\n2. This would lead me to think that I should add a \"Warning: 214\" even\nif I do not change the Content-Type or Content-Encoding (and that\nperhaps the spec should be changed to require this).\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-5148420"}, {"subject": "Re: ISSUE: transformation", "content": "    The wordage around application of transformations to entities [by\n    proxies] seems to deal only with what headers can be changed\n    (presumably interoperability issues are thought to only be possible\n    in that case).\n\n    However, imagine for a moment that my proxy applies a\n    transformation that neither changes the Content-Type nor the\n    Content-Encoding (for the sake of argument, call it the \"ee\n    cummings\" transform: on text types only, lowercase and remove\n    punctuation).  Even for the example used in the spec, if one were\n    to change the resolution of a medical image, it would be a\n    potentially bad thing.\n    \nI agree, the normative wording seems to be insufficient, and\nnot consistent with the rationale given just before it.  In\nsection 14.9.5 (No-Transform Directive), the rationale says:\n\n  Serious operational problems occur, however, when these\n  transformations are applied to entity bodies intended for certain\n  kinds of applications. For example, applications for medical imaging,\n  scientific data analysis and those using end-to-end authentication,\n  all depend on receiving an entity body that is bit for bit identical\n  to the original entity-body.\n\nbut the normative wording is:\n\n  Therefore, if a message includes the no-transform directive, an\n  intermediate cache or proxy MUST NOT change those headers that are\n  listed in section 13.5.2 as being subject to the no-transform\n  directive. This implies that the cache or proxy MUST NOT change any\n  aspect of the entity-body that is specified by these headers.\n\nwith the 13.5.2 list of headers as:\n  .  Content-Encoding\n  .  Content-Range\n  .  Content-Type\n\nFor example, this means that a response that looks like this\n\nHTTP/1.1 200 OK\nContent-Type: image/gif\ncache-control: no-transform\n\ncould be modified so that the image is still a GIF file, but\nhas had 99% of its bits removed.  Not really consistent with\nthe \"bit for bit identical\" criterion in the rationale!\n\nMy recollection is that we fully intended \"no-transform\"\nto prevent such transformations, as well as the ones\ncurrently specified.  Someone (maybe me) must have screwed\nup when it came to writing the normative wording.\n\n    1. Content-MD5, for example, is an end-to-end header (which,\n    according to 13.5.1, MUST be stored and forwarded, and according to\n    14.15 MUST NOT be generated by proxies.  If I leave it in, it will\n    be wrong.  If I remove it, I run the risk of breaking an\n    application.  Gaaack.\n\nContent-MD5 can't be changed or generated by proxies.  But there is no\nprohibition that a response with a Content-MD5 header has to be\nforwarded without transformation.  I.e., the spec allows a proxy to\ntransform a message with a Content-MD5 header in such a way that the\nContent-MD5 value no longer matches the contents.\n\nOne might argue that this is OK.  I.e., if the Content-MD5 is wrong,\nthen the recipient can assume that the message has been transformed\n(making a Warning superfluous).\n\nOne might argue that a response with both \"no-transform\" and\na Content-MD5 shouldn't be transformed in such a way as to\nchange the MD5 hash of the body.  We could add a normative\nMUST NOT along these lines (say, to the definition of\n\"no-transform\", section 14.9.5):\n\n    If a message contains the no-transform directive and also\n    includes a Content-MD5 header, an intermediate cache or\n    proxy MUST NOT change the value of the message body.\n\nAlternatively, one could make a minor modification to the\nexisting text:\n\n  Therefore, if a message includes the no-transform directive, an\n  intermediate cache or proxy MUST NOT change those headers that are\n  listed in section 13.5.2 as being subject to the no-transform\n  directive. This implies that the cache or proxy MUST NOT change any\n  aspect of the entity-body that is specified by these headers,\n  [or by the Content-MD5 header field, if present].\n\nThis means that one way to prevent a transformation would be\nto attach a Content-MD5 header field to the message.  However,\nthe computational cost of doing this, while not excessive, is\nnot negligible, and so it's kind of a kludge.\n\nAnother alternative would be to say:\n\n  Therefore, if a message includes the no-transform directive, an\n  intermediate cache or proxy MUST NOT change those headers that are\n  listed in section 13.5.2 as being subject to the no-transform\n  directive. This implies that the cache or proxy MUST NOT change any\n  aspect of the entity-body that is specified by these headers,\n  including the value of the entity-body itself.\n\nwhich is what intuition suggests that \"no-transform\" ought to mean,\nand probably what we really meant to write.\n\nNote that the utility of Content-MD5 is perhaps suspect, since\nit applies to the message and not to the underlying thing.  So\nit totally breaks down when trying to reassemble something at\nan intermediate proxy cache out of byte-range responses, for\nexample, or from the proposed delta-encoded responses.  Which\nis why some of us have proposed adding headers for \"instance digests\":\n   http://search.ietf.org/internet-drafts/draft-mogul-http-digest-00.txt\n\nIn order for HTTP/1.1 proxies to \"do the right thing\" if this\nkind of extension is adopted later, the meaning of \"no-transform\"\nought to be independent of whether or not the proxy knows that\na set of header names is somehow special.  Based on that, I would\nsay that we should adopt the final correction that I proposed;\nnot only is it the most intuitive interpretation of \"no-transform\",\nbut it's also the most extensible.\n\n    2. This would lead me to think that I should add a \"Warning: 214\"\n    even if I do not change the Content-Type or Content-Encoding (and\n    that perhaps the spec should be changed to require this).\n    \nRight.  I think the specification here should be changed from\n\n    214 Transformation applied\n      MUST be added by an intermediate cache or proxy if it applies any\n      transformation changing the content-coding (as specified in the\n      Content-Encoding header) or media-type (as specified in the Content-\n      Type header) of the response, unless this Warning code already\n      appears in the response.\n\nto\n\n    214 Transformation applied\n      MUST be added by an intermediate cache or proxy if it applies any\n      transformation changing the content-coding (as specified in the\n      Content-Encoding header) or media-type (as specified in the\n      Content-Type header), or the entity-body of the response, unless\n      this Warning code already appears in the response.\n\nsince otherwise there is no way to know if a proxy has, for\nexample, removed 99% of the bits from a GIF file.\n\n-Jeff\n\n\n\n", "id": "lists-012-5156322"}, {"subject": "Re: Fwd: draft-ietf-http-v11-spec-rev04 comment", "content": "Some comments on Paul Bennet's comments:\n[It's reassuring to come upon someone else who can be as nit-picky as I\nam. :-)]\n\n>  * Section 5.1.2 \"Request-URI\", p33, second para talks about\n>    replacing 'a null abs_path with \"*\"' - surely it's replaced\n>    with a \"/\".\n\nI was about to say, vigorously, \"no\", but now I think I'm just confused\nabout just what the point is of that paragraph.  The paragraph says:\n\n       Request-URI    = \"*\" | absoluteURI | abs_path\n[...]\nIn requests that they forward, transparent proxies MUST NOT rewrite the\n\"abs_path\" part of a Request-URI in any way except as noted above to\nreplace a null abs_path with \"*\", no matter what the proxy does in its\ninternal implementation.\n\nIf \"abs_path\" in the paragraph refers to the non-terminal in the\nRequest-URI production, then I have to ask, how can it be null in the\nfirst place?  When is Request-URI null?  OTOH, if we're talking about\nthe \"abs_path\" that's part of the \"absoluteURI\", then I might be\ninclined to agree with Paul.\n\n>  * Sections 9.3 \"GET\", 9.4 \"HEAD\" and 9.7 \"DELETE\" don't explicitly\n>    rule out the presence of an entity-body, as section 9.8 \"TRACE\"\n>    does.\n\nThat's because we've discussed the possibility that, some time in the\nfuture, it might make sense to include an entity-body for them.\n\nDave Kristol\n\n\n\n", "id": "lists-012-5169796"}, {"subject": "Re: ISSUE: transformation", "content": "** Reply to note from Jeffrey Mogul <mogul@pa.dec.com> Wed, 12 Aug 98 16:37:56 MDT\n\nI agree with your suggestions, which is why I didn't quote them.\n\nOn the topic of what to do with Content-MD5, you pointed out (around\nlast December I think) that one use is to detect packet-splicing errors\non ATM links (where the TCP ones-complement checksum is not\nsufficient).\n\nThis, combined with the transformation issue, leads me to think that\nthere could (or ought to) be a distinction between hop-by-hop and\nend-to-end message integrity checks (MICs).  I would think that if\nCache-control: no-transform is present, one would use an end-to-end\nMIC, otherwise a hop-by-hop MIC could be used.\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n", "id": "lists-012-5178920"}, {"subject": "Re: ISSUE: transformation", "content": "    On the topic of what to do with Content-MD5, you pointed out\n    (around last December I think) that one use is to detect\n    packet-splicing errors on ATM links (where the TCP ones-complement\n    checksum is not sufficient).\n    \nThis is a slight misinterpretation of what I wrote, if you're\nreferring to:\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0117.html\n\nThe main point of this message was that transport-level checksums\ndo not necessarily provide an \"error-free transport layer\", and\nso higher-level checksums are useful.\n\nBut in the same message, I also wrote:\n\n    Unfortunately, the [Content-]MD5 checksum covers just the message\n    body, and so if one is reassembling a document from several\n    messages (e.g., using Range retrievals) one can still have\n    undetected errors.  This is why I speculated that Content-MD5 is\n    \"not even particularly useful\" ... it's end-to-end as far as the\n    HTTP messages go, but it's not end-to-end as far as the actual\n    documents (or whatever) are concerned.\n    \nI.e., I still believe that Content-MD5 is not actually the\nright solution for the problem.\n\n    This, combined with the transformation issue, leads me to think\n    that there could (or ought to) be a distinction between hop-by-hop\n    and end-to-end message integrity checks (MICs).  I would think that\n    if Cache-control: no-transform is present, one would use an\n    end-to-end MIC, otherwise a hop-by-hop MIC could be used.\n    \nThe so-called \"End-to-End Argument\" (see \"End-To-End Arguments in\nSystem Design\", J.H. Saltzer, D.P.Reed, D.D.Clark, ACM TOCS, Vol 2,\nNumber 4, November 1984, pp 277-288. - probably not available online,\nalas!) implies that if you care about end-to-end integrity, then\nyou need an end-to-end MIC.  All you can get from hop-by-hop MICs\nis some added efficiency if (1) you have the ability to do\nautomatic hop-by-hop retransmissions if the MIC detects and error,\nand (2) such errors are relatively frequent, or (3) the hop-by-hop\nMIC is specifically suited to the kinds of errors seen on that hop.\nBut none of these conditions seems to be true for HTTP.  (The\nEthernet CRC is useful because Ethernet-level errors are common,\ndue to CSMA/CD, and the CRC is designed for the kinds of\nerrors that do occur.)\n\nIn any case, it's hard to think of a way to specify \"no-transform\" in\nsuch a way that it depends on the presence of specific kinds of headers\ncarrying MICs, especially if we want to be able to extend the set of\nMICs in the future.  I think this is a rathole, and we're best off just\nsaying \"no-transform means no transforms of the entity-body, period.\"\n\n-Jeff\n\n\n\n", "id": "lists-012-5187085"}, {"subject": "Agenda items for HTTPWG, IET", "content": "I have the following items for the HTTP working group meeting in Chicago:\n\n- Review status of implementation reports\n   Before we go to draft standard, we must document independent\n   interoperable implementations of every feature.\n\n   We should review the current implementation reports and decide\n   whether they are sufficient to claim 'draft standard' status.\n\n-  Open technical issues?\n   If there are any, we must close them ASAP.\n\n-  HTTP/1.2\n   There've been some discussions of whether we should increment\n   the version number, because of new normative language since\n   RFC 2068.\n\nAre there other open items for this (the really last) meeting of\nHTTP-WG?\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-5196215"}, {"subject": "Chunking and trailers (IMPORTANT question for implementers", "content": "We've come across a protocol interoperability problem with chunking and \ntrailers; Jeff Mogul is working on a description of this problem for the \nworking group and discussion of the possible fixes (it is subtle enough \nto want a careful exposition). He hopes to have it out in the next couple\ndays.\n\nWe need data from implementers to make a good recommendation of how to \nproceed.  This question is needed to get data on whether the problem actually \noccurs in practice in any deployed code, which will affect what the right \nfix might be (if the situation never exists in any deployed code, then \nthe simplest, cleanest fix involves outlawing the protocol condition that\ncan generate the problem in the first place).\n\nSection 3.6.1 (Chunked Transfer Coding) currently says:\n\n     A server using chunked transfer-coding in a response MUST NOT\n     use the trailer for other header fields than Content-MD5 and\n     Authentication-Info unless the \"chunked\" transfer-coding is\n     present in the request as an accepted transfer-coding in the TE\n     field (section 14.39). The Authentication-Info header is\n      defined by RFC 2069 [32] or its successor [43].\n\nThis exception for Authentication-Info and Content-MD5 leads\nto a potential interoperability bug.\n\nAre there any deployed HTTP servers or proxies that actually\nsend Authentication-Info or Content-MD5 in the trailer of\na chunked encoding when the request does not include \"chunked\"\nin a TE request-header field?\n\nIf so, please respond ASAP.  Otherwise, this exception might\nbe removed from the final draft of the specification.\n\nThanks greatly,\n- Jim Gettys\n\n\n\n", "id": "lists-012-5203431"}, {"subject": "Re: Chunking and trailers (IMPORTANT question for implementers", "content": "Jim Gettys wrote:\n> \n> We've come across a protocol interoperability problem with chunking and\n> trailers; Jeff Mogul is working on a description of this problem for the\n> working group and discussion of the possible fixes (it is subtle enough\n> to want a careful exposition). He hopes to have it out in the next couple\n> days.\n> ...if the situation never exists in any deployed code, then\n> the simplest, cleanest fix involves outlawing the protocol condition that\n> can generate the problem in the first place).\n>...\n> If so, please respond ASAP.  Otherwise, this exception might\n> be removed from the final draft of the specification.\n\nNot in deployed code, no - because I'm implementing it now (in between\nreading IDs for the Chicago meeting).  Taking it out creates a problem too -\nthat's why I (and others) thought that the exception for Authentication-Info\nshould be there in the first place (I don't care about Content-MD5).\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-5212428"}, {"subject": "Re: Chunking and trailers (IMPORTANT question for implementers", "content": "In a previous episode Jim Gettys said...\n:: \n:: \n:: We need data from implementers to make a good recommendation of how to \n:: proceed.\n[..]\n:: \n:: Are there any deployed HTTP servers or proxies that actually\n:: send Authentication-Info or Content-MD5 in the trailer of\n:: a chunked encoding when the request does not include \"chunked\"\n:: in a TE request-header field?\n\nyes - my origin server will send md5 trailers under some conditions.. the\nserver is currently in use with a few development projects and shipped\n(embedded) to one customer.. it could be altered, but at some cost.\n\nbecause our server never generates trailers other than md5 it isn't\naware at this stage whether or not a TE request was sent or not, only\nwhether or not it is in chunked mode.. it will go into chunked mode\nanytime there is a 1.1 request and the server feels buffering latency\ncould be improved by chunking.. when md5 is turned on by the resource\n(about 10% of the time in the deployed ap) this almost always results\nin chunking to occur..\n\nflip side: a simpleminded (embedded) 1.1 client that interacts with above\nmentioned server doesn't send TE but does check the checksum of the\nresponse.. a change to the spec would make that info unavailable to\nit.. and it's probably the only client that cares.. other uas (msie,\nnetscape, etc..) also interact with the server, but they probably\ndon't check the sums..\n\nopinion: I haven't had time to follow why this is a problem so won't\ncomment on that.. I will say that this feature is fairly important to\nme to preserve.. it lets me apply the MIC without impacting\nlatency.. though I guess the penalty of having the client send TE to\npreserve this is doable..\n\n-P\n\n\n\n", "id": "lists-012-5221386"}, {"subject": "Re: Chunking and trailers (IMPORTANT question for implementers", "content": "Jim Gettys wrote:\n> [...]\n> We need data from implementers to make a good recommendation of how to\n> proceed.  This question is needed to get data on whether the problem actually\n> occurs in practice in any deployed code, which will affect what the right\n> fix might be (if the situation never exists in any deployed code, then\n> the simplest, cleanest fix involves outlawing the protocol condition that\n> can generate the problem in the first place).\n> [...]\n> Are there any deployed HTTP servers or proxies that actually\n> send Authentication-Info or Content-MD5 in the trailer of\n> a chunked encoding when the request does not include \"chunked\"\n> in a TE request-header field?\n\nSorry for the delay --- I've been away a few days.\n\nI don't know whether my server could be considered \"deployed\", but\nhere's what it does.  For *any* digest response to qop=auth-int, the\nserver *always* sends a trailer (and chunked encoding) that contains\nAuthentication-Info.  The server ignores the TE header.  (You may recall\nI argued (issue h2, TE-IDENTITY) that \"chunked\" should always be\nallowed.  This support of Digest auth-int is why.)\n\nDave Kristol\n\n\n\n", "id": "lists-012-5230848"}, {"subject": "(revised) Agenda, HTTPW", "content": "HTTP Working Group, IETF Chicago, Monday, 8/24/98 7:30 PM\n\n- 10 Agenda review: Masinter\n- 20 Implementation status reports: review & discussion; Masinter/Gettys\n   Before we go to draft standard, we must document independent\n   interoperable implementations of every feature.\n\n   Review the current implementation reports and decide\n   whether they are sufficient to claim 'draft standard' status.\n    http://www.w3.org/Protocols/HTTP/Forum/Reports/\n\n- 30 Status of main draft & open issues review; Gettys\n   ftp://ftp.ietf.org/internet-drafts/draft-ietf-http-spec-v11-rev-04.txt\n    http://www.w3.org/Protocols/HTTP/Issues/ \n\n- 30 Status of Authentication draft & review; Lawrence\n   ftp://ftp.ietf.org/internet-drafts/draft-ietf-http-authentication-02.txt\n\n- 15 HTTP/1.2 Increment version number?\n- 15 Closing working group: administrative issues; Masinter\n\n\n\n", "id": "lists-012-5239164"}, {"subject": "(Nit) Comments on draft-ietf-http-authentication02.tx", "content": "I have some comments about the latest draft of HTTP Authentication:\nBasic and Digest Access Authentication.  This message contains just\neditorial comments.  Some of the problems seem to arise from the MS\nWord to .txt conversion.  Some further messages will (individually)\naddress substantive stuff.\n\nDave Kristol\n-----------------\n\n1) Authors:\n    J. Hostetler, Spyglass, Inc.\nshould be AbiSource, Inc.\n\n2) Sect. 2.\n\n    If a client wishes to send the same userid and password to a proxy, it\n    would use the Proxy-Authorization header field. See section 4 for\n    security considerations associated with Basic authentication.\n\nTo my brain, this reads funny, particularly in context, where \"the\nsame\" seems to refer to the preceding paragraph.  I suggest new wording:\n\n    When it sends a request to a proxy, a client may reuse a userid and\n    password in the Proxy-Authorization header field without receiving\n    another challenge from the proxy server. See section 4 for security\n    considerations associated with Basic authentication.\n\n3) Sect. 3.1.1\n                                                        This document\n    provides the specification for such a scheme, which does not send the\n    password in cleartext.\n\nReword to:\n                                                        This document\n    provides the specification for a scheme that does not send the\n    password in cleartext.\n\n4) Sect. 3.1.2\n    value. A valid response contains a checksum (by default the MD5\n        insert ',' ----^\n\n5) Sect. 3.2.1 (under \"domain\")\n  to canonical root URL (see section 1.2 above) of the server being\n    ^-- add \"the\"\n\n6) Sect. 3.2.1 (under \"algorithm\")\n\n  and a checksum. If this is not present it is assumed to be \"MD5\". If\n  the algorithm is not understood, the challenge should be ignored (and\n  a different one used, if there is more than one). In this document\n  the string obtained by applying the digest algorithm to the data\n\nI think \"In this document\" should begin a new paragraph.\n\n6) Sect. 3.2.1 (under \"algorithm\")\n  intended to allow efficient 3rd party authentication servers;\n  for the difference in usage, see the description .\n\n  ^\nThere's an extra space here; was there a reference?\n\n6) Sect. 3.2.1 (under \"qop-options\")\n  this choice. Unrecognized options MUST be ignored.\n      ------- -> change to qop-options\n\n7) Sect. 3.2.3\n    nonce value to be used for a limited time to permit request\n    pipelining. Use of\n\nThis (indented) paragraph just trails off....\n\n7) Sect. 3.2.3 (just before 3.3)\n     The Authentication-Info header is allowed in the trailer of an\n    ^-- extra space at beginning of sentence.\n\n8) Sect. 4.1\n    essentially cleartext transmission of the user?s password over the\n    non-ASCII character --^\n\n9) Sect. 4.2\n    directive values (see section 0) are  protected.  Most header\n      --------- -> bad reference?\n\n\n    authentication is both useful and appropriate (any service in\n    present use that uses Basic should be switched to Digest as soon\n    as practical).\nChange to\n    authentication is both useful and appropriate.  (Any service in\n    present use that uses Basic should be switched to Digest as soon\n    as practical.)\n\n10) Sect. 4.3\n    The Digest scheme uses a server-specified nonce to seed the generation\n    of the response-digest value (as specified in section 0).  As shown in\n      --------- -> missing ref.\n    the example in 0, the server is free to construct the nonce such that it\n                   - -> missing ref.\n\n11) Sect. 4.4 (last sentence)\n    limited by the servers choice of nonce.\n       ------- -> server's\n\n12) Sect. 4.5\n    on the use of  the integrity protection of qop=auth-int, an\n      ^-- extra space\n\n13) Sect. 4.7\n    This attack can be mitigated by checking the password against a\n    dictionary when a user tries to change it and disallowing passwords that\n    are in the dictionary.\n\nI think this wording is unclear; it doesn't say who does the checking\nand what \"it\" is.  Suggestion:\n\n    A server can mitigate the risk of this attack.  When a user asks\n    the server to change a password, the server can disallow passwords\n    that are in the dictionary.\n\n14) Sect. 4.9\n    The countermeasure against this attack is to for clients to be\n          -- -> delete\n\n15) Sect. 4.11\n    very quickly ? reports exist of searching all passwords with six or\n     ^-- non-ASCII character\n\n\n\n", "id": "lists-012-5247728"}, {"subject": "authentication-02: #authparam", "content": "Section 1.2 shows:\n\n    credentials = auth-scheme #auth-param\n\nShouldn't that more correctly be\n\n    credentials = auth-scheme 1#auth-param\n\nIt's hard for me to imagine an auth-scheme that can work correctly\nand usefully with zero auth-param's in the credentials.\n\nDave Kristol\n\n\n\n", "id": "lists-012-5258868"}, {"subject": "authentication-02: digestchallenge orde", "content": "     digest-challenge  = 1#( realm | [ domain ] | nonce |\n                         [ opaque ] |[ stale ] | [ algorithm ] |\n                         [ qop-options ] | [auth-param] )\n\nI'm just a tad nervous that an implementor might assume that \"realm\"\nwill be the first auth-param in a digest-challenge, based on\n\n- analogy to Basic (which has only one auth-param)\n- the fact that \"realm\" is listed first in the syntax\n\nIs it worth adding a note that the syntax for digest-challenge allows\nthe auth-param's to appear in any order?  There other places in this\nspec. where we say, in effect, \"don't be stupid\", so there is precedent.\n\nDave Kristol\n\n\n\n", "id": "lists-012-5266157"}, {"subject": "authentication-02: use of digestur", "content": "Section 3.2.2.5\n\n    The authenticating server must assure that the document\n    designated by the \"uri\" parameter is the same as the document\n    served. The purpose of duplicating information from the request\n    URL in this field is to deal with the possibility that an\n    intermediate proxy may alter the client's request. This altered\n    (but presumably semantically equivalent) request would not result\n    in the same digest as that calculated by the client.\n\nLet's say the URL is \"http://example.com/foobar\".  digest-uri-value\ncan be any HTTP/1.1 request-uri.  Should the client set uri=\"/foobar\"\nor uri=\"http://example.com/foobar\"?  Does the answer depend on whether\nthe client connects to example.com through a proxy.  (I hope not.)\n\nAn HTTP/1.1 server at example.com can legitimately get either of these\nrequest lines:\nGET /foobar HTTP/1.1\norGET http://example.com/foobar HTTP/1.1\n\n(An HTTP/1.1 proxy could, for example, pass the second line to the\norigin server.)\n\nThe origin server is supposed to compare the digest-uri-value and the\nrequest-uri.  It could thus find itself comparing the abs_path and\nabsoluteURI forms, one from uri=, the other from reguest-uri.  At the\nvery least, I think the spec. should alert implementors to that\npossibility.\n\n(And I assume the correct behavior is to respond with 400 Bad Request\nif there's a mismatch.)\n\nDave Kristol\n\n\n\n", "id": "lists-012-5273222"}, {"subject": "authentication02: threat of snooped passwor", "content": "    If a server permits users to select their own passwords, then the threat\n    is not only illicit access to documents on the server but also illicit\n    access to the accounts of all users who have chosen to use their account\n    password. If users are allowed to choose their own password that also\n    means the server must maintain files containing the (presumably\n    encrypted) passwords. Many of these may be the account passwords of\n    users perhaps at distant sites. The owner or administrator of such a\n    system could conceivably incur liability if this information is not\n    maintained in a secure fashion.\n\nThis paragraph surprises me a little.  It seems to me that if I choose\nas a password some kind of account password, then the threat is only to\nme and all the accounts that share the password.  I don't see how this\nallows \"illicit access to the accounts of all users who have chosen to\nuse their account password.\"  If an adversary grabs my password, how\ndoes that open a risk to other users?\n\nI think what was meant here is said better and more succinctly in\nSection 4.4:\n\n    The greatest threat to the type of transactions for which these\n    protocols are used is network snooping. This kind of transaction\n    might involve, for example, online access to a database whose use\n    is restricted to paying subscribers. With Basic authentication an\n    eavesdropper can obtain the password of the user. This not only\n    permits him to access anything in the database, but, often worse,\n    will permit access to anything else the user protects with the\n    same password.\n\nDave Kristol\n\n\n\n", "id": "lists-012-5281152"}, {"subject": "RE: authentication-02: #authparam", "content": "Always allow unless you have a good reason to deny.\nYaron\n\n> -----Original Message-----\n> From: Dave Kristol [mailto:dmk@research.bell-labs.com]\n> Sent: Thursday, August 20, 1998 11:54 AM\n> To: http-wg@hplb.hpl.hp.com\n> Subject: authentication-02: #auth-param?\n> \n> \n> Section 1.2 shows:\n> \n>     credentials = auth-scheme #auth-param\n> \n> Shouldn't that more correctly be\n> \n>     credentials = auth-scheme 1#auth-param\n> \n> It's hard for me to imagine an auth-scheme that can work correctly\n> and usefully with zero auth-param's in the credentials.\n> \n> Dave Kristol\n> \n\n\n\n", "id": "lists-012-5289221"}, {"subject": "web replication/caching bo", "content": "I thought HTTP folks might be interested in this BOF.\n\n-Keith\n\nWeb Caching and Replication BOF (webrepl)\n  Thursday, August 27 at 1530-1730\nChair: Patrik Faltstrom  <paf@swip.net>\n\nDESCRIPTION: Ideas on how to replicate contents of webservers (including \ncaches) have been floating around in the web community. Replication includes \nseveral steps -- from prefetching information (pushing or polling), to \nannouncement of the availability of new content (again pushing or polling). \nThis gets even more complicated when starting to use hierarichal caches.\n\nIt is also the case that in this area, the community don't want proprietary\nprotocols, as one cache should be able to communicate with another one --\neven though it is developed by a different vendor.\n\nThis BOF is ment to check the interest in the community on this matter, and\nsee of there is a possibility to try to focus some work on some few items,\nthat have to be standardized.\n\n\n\n", "id": "lists-012-5297549"}, {"subject": "Re: authentication-02: #authparam", "content": ">It's hard for me to imagine an auth-scheme that can work correctly\n>and usefully with zero auth-param's in the credentials.\n\nIt is for an auth-scheme that sends its parameters in separate\nheader field(s) rather than as auth-param's.\n\n....Roy\n\n\n\n", "id": "lists-012-5305065"}, {"subject": "Re: Fwd: draft-ietf-http-v11-spec-rev04 comment", "content": ">>  * Section 5.1.2 \"Request-URI\", p33, second para talks about\n>>    replacing 'a null abs_path with \"*\"' - surely it's replaced\n>>    with a \"/\".\n>\n>I was about to say, vigorously, \"no\", but now I think I'm just confused\n>about just what the point is of that paragraph.  The paragraph says:\n>\n>       Request-URI    = \"*\" | absoluteURI | abs_path\n>[...]\n>In requests that they forward, transparent proxies MUST NOT rewrite the\n>\"abs_path\" part of a Request-URI in any way except as noted above to\n>replace a null abs_path with \"*\", no matter what the proxy does in its\n>internal implementation.\n\nThat is a leftover from when Max-Forwards was not allowed to be used\nwith an OPTIONS request.  That stuff should be removed.  Unfortunately,\nI don't have time to make a diff to give better guidance about where\n\"that stuff\" is precisely -- one more thing to do on the plane to Chicago.\n\n....Roy\n\n\n\n", "id": "lists-012-5312134"}, {"subject": "Fwd: Analysis of &quot;chunking, trailers, and buffering&quot; problem (CHUNKEDTRAILERS", "content": "Here is the proposed resolution to this issue for the general mailing list,\nper the discussion at the working group meeting.  I'll update the issues list\nwith a pointer to this mail when I get a chance...\n- Jim\n\n\nFrom: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\nDate: Fri, 21 Aug 1998 16:36:12 -0700\nTo: Jeffrey Mogul <mogul@pa.dec.com>\nCc: Larry Masinter <masinter@parc.xerox.com>,\n        Paul Leach <paulle@microsoft.com>, jg@w3.org, frystyk@w3.org,\n        josh@microsoft.com, luotonen@netscape.com,\n        Scott Lawrence <lawrence@agranat.com>,\n        Richard Gray <rlgray@us.ibm.com>\nSubject: Re: Analysis of \"chunking, trailers, and buffering\" problem\n-----\nI really really really dislike the continuing use of TE to indicate\nanything about trailers.  In order for trailers to be usable, the\nentire response path must be willing to forward them or be willing\nto buffer the entire message or be allowed to discard them.  TE cannot\nindicate that because it is a hop-by-hop field, so we'd be better off\nremoving the exception entirely and leaving that functionality to\nbe specified properly in the future.\n\nLikewise, making specific exceptions for Content-MD5 and Authentication-Info\nis unnecessary if we just make a general statement for the condition\nwhere trailer fields can be included if it is permissible to ignore them.\n\nIn other words, I believe the following is substantially better:\n\nIn section 3.6.1 (Chunked Transfer Coding), change this paragraph:\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].\n\nto read\n\n    A server using chunked transfer-coding in a response MUST NOT\n    use the trailer for any header fields unless at least one of\n    the following are true:\n\n       a) there is no Via header field (indicating a connection without\n          intermediary proxies);\n\n       b) all of the protocols listed in the Via header field are\n          HTTP/1.1 or later; or,\n\n       c) the server grants the recipient(s) the right and ability to\n          discard those trailer fields without forwarding them to any\n          downstream recipient of the remaining message.  This is\n          only possible if the trailer fields consist entirely of\n          optional metadata that is not necessary for the recipient\n          to understand in order to use the message.\n\n    The above requirement exists to avoid an interoperabilty paradox\n    when the message is being received by an HTTP/1.1 (or later) proxy\n    and forwarded to an HTTP/1.0 recipient.  It avoids a situation\n    where compliance with the protocol would have necessitated an\n    infinite buffer on the proxy.\n\nIn section 14.40 (Trailer), change this paragraph\n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nto   \n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields.  See section 3.6.1 for restrictions\n    on the use of trailer fields in a \"chunked\" transfer-coding.\n\n....Roy\n\n\nattached mail follows:\nI really really really dislike the continuing use of TE to indicate\nanything about trailers.  In order for trailers to be usable, the\nentire response path must be willing to forward them or be willing\nto buffer the entire message or be allowed to discard them.  TE cannot\nindicate that because it is a hop-by-hop field, so we'd be better off\nremoving the exception entirely and leaving that functionality to\nbe specified properly in the future.\n\nLikewise, making specific exceptions for Content-MD5 and Authentication-Info\nis unnecessary if we just make a general statement for the condition\nwhere trailer fields can be included if it is permissible to ignore them.\n\nIn other words, I believe the following is substantially better:\n\nIn section 3.6.1 (Chunked Transfer Coding), change this paragraph:\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].\n\nto read\n\n    A server using chunked transfer-coding in a response MUST NOT\n    use the trailer for any header fields unless at least one of \n    the following are true:\n\n       a) there is no Via header field (indicating a connection without\n          intermediary proxies);\n\n       b) all of the protocols listed in the Via header field are\n          HTTP/1.1 or later; or,\n\n       c) the server grants the recipient(s) the right and ability to\n          discard those trailer fields without forwarding them to any\n          downstream recipient of the remaining message.  This is\n          only possible if the trailer fields consist entirely of\n          optional metadata that is not necessary for the recipient\n          to understand in order to use the message.\n\n    The above requirement exists to avoid an interoperabilty paradox\n    when the message is being received by an HTTP/1.1 (or later) proxy\n    and forwarded to an HTTP/1.0 recipient.  It avoids a situation\n    where compliance with the protocol would have necessitated an\n    infinite buffer on the proxy.\n\nIn section 14.40 (Trailer), change this paragraph\n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nto    \n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields.  See section 3.6.1 for restrictions\n    on the use of trailer fields in a \"chunked\" transfer-coding.\n\n....Roy\n\n\n\n", "id": "lists-012-5320461"}, {"subject": "Re: Fwd: Analysis of &quot;chunking, trailers, and buffering&quot; problem (CHUNKEDTRAILERS", "content": "At 08:55 26/08/1998 -0700, Jim Gettys wrote:\n>I really really really dislike the continuing use of TE to indicate\n>anything about trailers.  In order for trailers to be usable, the\n>entire response path must be willing to forward them or be willing\n>to buffer the entire message or be allowed to discard them.  TE cannot\n>indicate that because it is a hop-by-hop field, so we'd be better off\n>removing the exception entirely and leaving that functionality to\n>be specified properly in the future.\n\nRegardless of the solution chosen, I would like to point out that TE indeed\ncan be used for this and indeed it was (at least by me, intended) for this\npurpose.\n\nTrailers must be dealt with on a hop-by-hop basis by all the parties down\nthe chain. That is, it is a \"repeated hop-by-hop\" because each step has to\nagree and not only the \"end\".\n\nA \"TE: chunked\" header field means for any particular hop that \"I accept\nfull featured chunked encoding with trailers and not just the boiled down\nversion without trailers\". This was a compromise necessary to do was\nexisting implementations only support non-trailered chunked encoding.\n\nIf a client is willing to accept data in the trailer, it indicates this\nusing the \"TE: chunked\" header field.\n\nThis works in all cases - regardless of whether there are HTTP/1.0 proxies,\nexisting limited HTTP/1.1 chunking proxies or HTTP/1.1 chunking proxies\nthat understand trailers. For example, if a proxy doesn't see this in a\nrequest then it can either say:\n\n1) I don't want to risk having to buffer so I don't send TE: chunked\n2) I don't mind buffering and include a TE: chunked.\n\nand if it sees it and understands trailers then it can just repeat the \"TE:\nchunked\" in its request to the next server.\n\nThe other part of TE was Trailer which the server uses to indicate which\nheader fields are included in the response or not. This makes it very easy\nfor a proxy to see up front whether it has to buffer or not (if it\nindicated that it was willing to buffer).\n\nThis also works for arbitrary header fields *except* content-md5 and\nauthentication-info which were already in the spec at the time, TE was\nintroduced. *Removing* the special case for these two header fields will\nsolve the problem and does not require any additional checks in the server\nlike looking for the via field.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-5337610"}, {"subject": "Fwd: Analysis of &quot;chunking, trailers, and buffering&quot; problem (CHUNKEDTRAILERS", "content": "Bala Krishanmurthy asked I post the (long) discussion  of possibilities \nthat was off-line the main list; I think this is a good idea as not everyone \nwas at the working group meeting this week.\n- Jim\n\n\n\nattached mail follows:\nRichard Gray pointed out an interoperability bug in the current\nspecification:\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q3/0011.html\n\nThis lead to a lengthy off-line discussion between Richard, Jim,\nmyself, and (later on) Henrik.  After thinking about it for perhaps\na little too long, we've convinced ourselves that this is an actual\ninteroperability bug, and therefore requires resolution.\n\nJim and I are both somewhat embarrassed to admit that the bug is\na result of ignoring a Protocol Design 101 lesson that we both\nshould have remembered somewhat earlier in the process.\n\nJim asked me to write up an analysis of the problem, and of the\nvarious proposed solutions, and to submit it to the Editorial\nGroup for additional analysis.  The goal is to avoid proposing\na solution to the entire HTTP-WG until we've had a chance to \ngive it some review.  (Larry/Jim: if I've left off any members\nof the Editorial Group, please rectify this!)\n\nMy draft analysis is appended.  Constructive comment are\nappreciated.  I'm not entirely satisfied with any of the possible\nsolutions (which is why the draft includes two alternative\n\"recommended solutions\"), but after mulling it over for a while,\nI'm not sure we can do any better.\n\n-Jeff\n\n(A) THE SCENARIO:\n\nConsider this configuration:\n\n                 hop1            hop2             hop3\n   OriginServer -------- ProxyA --------- ProxyB ---------- Client\n   HTTP/1.1             HTTP/1.1         HTTP/1.0         HTTP/1.1\n\n\nand suppose that the OriginServer sends a response that\n(1) is very large (X bytes long)\n(2) is sent used a chunked encoding\n(3) is sent with an Authentication-Info header field\n(4) the Authentication-Info field is sent in the trailer\nof the chunked encoding.\n\nNow, this is all reasonable.  The origin server has received\nthe request via an HTTP/1.1 proxy (ProxyA), so it knows that it is\nlegal to send a chunked response.  Since it is a long response,\nthe origin server doesn't want to buffer the whole response\nin order to place the Authentication-Info field before the\nmessage-body; putting it in the trailer avoids this buffering\nstep.  And section 3.6.1 (Chunked Transfer Coding)  says:\n\n     A server using chunked transfer-coding in a response MUST NOT\n     use the trailer for other header fields than Content-MD5 and\n     Authentication-Info unless the \"chunked\" transfer-coding is\n     present in the request as an accepted transfer-coding in the TE\n     field (section 14.39).\n\nTherefore, the origin server has not broken any of the rules\nas specified in draft-ietf-http-v11-spec-rev-04.txt.\n\nBut suppose ProxyA is unable to buffer an arbitrarily long\nmessage; in particular, suppose that it cannot buffer an\nmessage body of length X.  Now we have a problem:\n(1) ProxyA cannot forward the message in chunked encoding,\nbecause ProxyB is HTTP/1.0 and doesn't understand chunked.\n(2) ProxyA cannot put the Authentication-Info field\ninto the header of a non-chunked message, because to\ndo that requires buffer X bytes of message body, and\nwe just said that ProxyA can't do that.\n(3) ProxyA cannot simply drop the Authentication-Info field,\nbecause it's an End-To-End header and proxies can't\narbitrarily drop such headers.  In this case, doing so\nwould totally break the authentication mechanism.\n(4) ProxyA cannot automatically retry the request\n(using HTTP/1.0 this time), because it has no way\nto know whether the operation is idempotent.\nIn fact, there is no way for the end-hosts in this scenario\n(Client or OriginServer) to even know that something has gone\nwrong.\n\nWe could insist that ProxyA never send its requests using HTTP/1.1,\nbecause then this particular problem would never arise.  But then\nwe would be tossing out all of the advantages of HTTP/1.1 on the\nhop between ProxyA and the OriginServer, for all requests, even\nthough this scenario might be extremely rare.  (Note that there\ndoes not seem to be any way for ProxyA to realize, when it is\nabout to forward the request, that the response might fall into\nthis category.)\n\n(B) THE UNDERLYING PROBLEM:\n\nWhat has gone wrong here?  The bug is that we have apparently\nignored a fundamental aspect of protocol design: the sender\nof a message needs to know, in general, whether the recipient\ncan actually buffer it or not.  For example, TCP has the\nnotion of a maximum segment size (MSS) and a receiver window\nsize, precisely to avoid the possibility that a TCP sender\nwill send a bigger datagram, or more data, than the TCP receiver\ncan buffer.  But HTTP has no such mechanism (unless you squint\nat the byte-range feature, which doesn't solve the case at hand).\n\nThis is actually a bug, at some level, in HTTP as a whole.\nI.e., the browser normally has to be able to buffer the\nentire response (in order to do things like render it,\nscroll back and forth, and all those things that we take\nfor granted).  But www.contentprovider.com has no sure way of\nknowing whether the response it sends to a client is going to\nbe too big to fit in the browser's buffer.\n\nThe observed fact that we haven't been confronted with this\nproblem, in actual practice, no doubt is because most browsers\nhave relatively large buffers, and most Web values are small.\nI mean, the mean and median response sizes are on the order\nof 10K, and there are very few outliers.\n\nSo the Web \"works\" because of an apparently unstated understanding\nbetween content providers and browser implementors about the\nbrowser's buffer size.  (And it probably fails to \"work\" for\nbrowsers on PDAs, except that with slow modems, very few users\nwould bother to wait for even a smallish PDA to fill up.)\n\nBut when we introduce a proxy with a constrained buffer size,\nand then encounter a situation that requires the proxy to buffer\nthe entire message, we run straight into this problem.\n\nTo summarize: the introduction of trailer fields into HTTP/1.1\nmeans that there will be times when either a sender or a recipient\nwill have to buffer the entire message.  This is unavoidable;\nour only choice is whether to force the buffering to occur at\nthe sender or at the recipient.  The current specification\nforces whole-message buffering at the recipient, making it\nimpossible to recover from buffer overflow.\n\n(C) POSSIBLE SOLUTIONS:\n\nWe can see several possible solutions, of varying degrees of\ndesirability:\n\n(1) Ignore it.  Assume that nobody cares if Authentication-Info\nsometimes mysteriously gets lost from large responses.\n\n(1a) Ignore it, but add a note warning implementors that this\nmight be a problem ... although giving no real guidance about\nwhat to do about it.\n\n(2) Insist that all HTTP/1.1 proxies must have (effectively)\ninfinite buffer capacities.\n\n(3) Modify the spec to prohibit the use of Authentication-Info\nor Content-MD5 in a trailer unless specifically authorized by\nthe presence of a TE header field in the request.  This means\nthat, absent such a TE field, the origin server must be willing\nto buffer the entire response.\n\n(4) Change the spec to say that proxy implementations SHOULD\nbe able to buffer up to N bytes of message-body, and that\nservers SHOULD NOT put Authentication-Info or Content-MD5\nin the trailer of a message longer than N bytes (without\npermission in the form of a TE header).\n\n(4a) Argue for a while about what N should be.\n\n(5) Add a requirement that servers/proxies SHOULD NOT put\nAuthentication-Info or Content-MD5 in the trailer of any message\nif the request includes a Via field that lists an HTTP/1.0\nforwarder.\n\n(6) Add end-to-end and hop-by-hop buffer-size negotiation\nto HTTP/1.1.\n\n(D) ANALYSIS OF POSSIBLE SOLUTIONS:\n\n    (1) Ignore it.\n    \nProbably a mistake.  It's not such an unlikely scenario, since\nDigest Authentication allows the server to (optionally) include\na digest of any arbitrarily-long response.  Also, it violates our\nduty to ensure interoperability of all of the protocol features\n(rather than simply the interoperability of features in isolation).\n\n    (1a) Ignore it, but add a note.\n    \nNot really an improvement, since it doesn't really eliminate\nthe interoperability problem.\n\n    (2) Insist that HTTP/1.1 proxies have infinite buffer capacities.\n    \nNot a practical option.  At least one proxy implementor intends\nto ship a system with a 50KB limit, whether the spec allows it\nor not.  Remember, not all proxies have caches, so not all proxies\nneed (or can include) mass storage.\n\n    (3) Remove the exception for Authentication-Info or Content-MD5.\n    \nThis would solve the problem.  It might not be feasible if there\nare many already-deployed servers that send Authentication-Info or\nContent-MD5 in the trailer.\n\nIt does force the origin server to buffer the entire message,\nunder certain circumstances.  It has been argued that this\njust shifts the problem from the proxy to the origin server.\nHowever, there is a significant difference: the origin server\ncan therefore detect that there is a problem.  I.e., the origin\nserver is potentially able to restructure its response to\navoid having to buffer the whole message (even if this requires\ngenerating the response twice, once to compute the hash value\nand once to stream it to the network), or to generate a meaningful\nerror code.  But the proxy is unable to do anything except generate\na relatively useless error message.\n\n    (4) Add a SHOULD-level N-byte limit to the spec.\n    \nThis would solve the problem.  It would again force the origin\nserver to buffer the message, in some cases; see #3 for more\ndetails.  But not quite as often as #3.\n\nAs with #3, it would not be feasible if already-deployed servers send\nlonger messages that contain trailers.\n\n    (4a) What should N be?\n\nOn the other hand, choosing a value for N requires negotiation.\nIf the value chosen is larger than the buffer size used by\nany significant number of deployed proxies, then the \"solution\"\nfails.  If the number is too small, it degenerates into solution\n#3 (i.e., always buffer at the origin server), but with more\ncomplexity in the specification.  However, making N too small\ncan't cause interoperability failures; it can only reduce the\namount of \"optimization\" available relative to solution #3.\n    \n    (5) Check the Via header for HTTP/1.0 proxies.\n\nThis approach depends on the proper implementation of the\nVia header by any proxy with limited buffer sizes.  It also\nrepresents an added complication of the spec.  But it does\nallow the most optimistic use of trailers, and should (if\nVia headers are correctly implemented) directly avoid the\nproblem.\n\nIt still requires the origin server to buffer the entire\nmessage in some, but not all, cases.\n\n    (6) Add end-to-end and hop-by-hop buffer-size negotiation.\n\nPresumably too much of a change to add to HTTP/1.1 at this stage.\nHowever, it might be useful to pursue this as an add-on extension,\nespecially since it could solve the problem of sending over-sized\nmessages to small clients, such as PDAs.\n\n(E)  RECOMMENDATION FROM THE HTTP/1.1 EDITORIAL GROUP\n\nSolution #3, removing the exception for Authentication-Info\nor Content-MD5 in a trailer, seems to be the simplest solution\nand will probably work (depending on whether existing deployed\nservers would fail to comply).  Among other things, it simplifies\nthe specification (by removing an exception); all of the other\nchanges add complexity.\n\nThe proposal would be to change, in section 3.6.1 (Chunked\nTransfer Coding), this paragraph:\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].\n\nto read\n\nA server using chunked transfer-coding in a response MUST NOT\nuse the trailer for any header fields unless the \"chunked\"\ntransfer-coding is present in the request as an accepted\ntransfer-coding in the TE field (section 14.39).\n\nIn section 14.39 (TE), change this paragraph:\n\n    If no TE field is present, the sender MAY assume that the recipient\n    will accept the \"identity\" and \"chunked\" transfer-codings.\n    \nto read\n\n    If no TE field is present, the sender MAY assume that the recipient\n    will accept the \"identity\" and \"chunked\" transfer-codings, but\n    MUST NOT send any trailer fields in a \"chunked\" transfer-coding.    \n\nIn section 14.40 (Trailer), change this paragraph\n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nto    \n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields.  A server MUST NOT send any trailer\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nNote: when a trailer field is sent without observing the\nrequirements of the previous paragraph, it could be silently\ndropped by an intervening proxy, if the message is being\nforwarded to an HTTP/1.0 recipient.\n\nThe removal of the exception allowing the sender to omit Content-MD5\nor Authentication-Info from the Trailer header field also solves\na related problem: under the existing specification, even a proxy\nwith sufficient buffer space cannot know until it has seen the\nend of the entire message whether or not it will have to move\nthese fields from the trailer to the header (when forwarding to\nan HTTP/1.0 recipient).  In other words, the current spec requires\na proxy to buffer every chunked message before forwarding it to\nan HTTP/1.0 recipient, whether or not it includes a trailer field.\n\n\n(E1) ALTERNATIVE RECOMMENDATION FROM THE HTTP/1.1 EDITORIAL GROUP\n\nIf it is felt that the recommendation above is too restrictive\n(because it eliminates the use of trailer fields in certain\nall-HTTP/1.1 paths, where the buffering problem is non-fatal),\nwe could instead adopt a more complicated change involving checks\non the Via header.\n\nThis version would change, in section 3.6.1 (Chunked Transfer\nCoding), this paragraph:\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].\n\nto read\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].  A server\nMUST NOT send any trailer fields if the request includes\na Via header field listing one or more HTTP/1.0 hops.\n\nIn section 14.40 (Trailer), change this paragraph\n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nto    \n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.  If the\n    request includes a Via header field listing one or more HTTP/1.0\n    hops, the trailer MUST NOT include any header fields.\n\nNote: when a trailer field is sent without observing the\nrequirements of the previous paragraph, it could be silently\ndropped by an intervening proxy, if the message is being\nforwarded to an HTTP/1.0 recipient.\n\n[End]\n\nattached mail follows:\nNice analysis.  However, it isn't necessary to prevent a feature\njust because some applications are incapable of using it.  I think\nthe protocol needs to allow these things (Content-MD5 in particular)\nwhen the sender doesn't care if the recipient has no choice but\nto ignore the trailer.  My choice would be a combination of your\ntwo E proposals, something like the following:\n\n>The proposal would be to change, in section 3.6.1 (Chunked\n>Transfer Coding), this paragraph:\n>\n>        A server using chunked transfer-coding in a response MUST NOT\n>        use the trailer for other header fields than Content-MD5 and\n>        Authentication-Info unless the \"chunked\" transfer-coding is\n>        present in the request as an accepted transfer-coding in the TE\n>        field (section 14.39). The Authentication-Info header is\n>        defined by RFC 2069 [32] or its successor [43].\n>\n>to read\n\n    A server using chunked transfer-coding in a response MUST NOT\n    use the trailer for any header fields unless at least one of \n    the following are true:\n\n       a) the \"chunked\" transfer-coding is present in the request\n          as an accepted transfer-coding in the TE field (section 14.39);\n\n       b) there is no Via header field (indicating a connection without\n          intermediary proxies);\n\n       c) all of the protocols listed in the Via header field are\n          HTTP/1.1 or later; or,\n\n       d) the server grants the recipient the right and ability to\n          discard those trailer fields without forwarding them to any\n          downstream recipient of the remaining message.  This is\n          only possible if the trailer fields consist entirely of\n          optional metadata that is not necessary for the recipient\n          to understand in order to use the message.\n\n    The above requirement exists to avoid an interoperabilty paradox\n    when the message is being received by an HTTP/1.1 (or later) proxy\n    and forwarded to an HTTP/1.0 recipient.  It avoids a situation\n    where compliance with the protocol would have necessitated an\n    infinite buffer on the proxy.\n\n[Aside:\nActually, I don't understand why the presence or lack of the TE field\nshould have anything to do with this requirement.  TE doesn't indicate\nanything useful for trailer fields, not even the extent to which an\nimplementation might be prepared to handle trailer fields, and it\nmost certainly can't handle this case because TE is hop-by-hop and\nthe exception will only work if chunked will be sent end-to-end.\nI suggest we drop part (a) completely.\n\nIf we want to control the trailer with TE, then it needs a separate\nname for it (like \"trailer\"), and it needs to be propagated inbound\njust like the old Pragma directives.  In fact, it would probably work\nbetter as a Pragma directive and not in TE at all.]\n\n>In section 14.39 (TE), change this paragraph:\n>\n>    If no TE field is present, the sender MAY assume that the recipient\n>    will accept the \"identity\" and \"chunked\" transfer-codings.\n>    \n>to read\n\n     If no TE field is present, the sender MAY assume that the recipient\n     will accept the \"identity\" and \"chunked\" transfer-codings.  See\n     section 3.6.1 for restrictions on the use of trailer fields in a\n     \"chunked\" transfer-coding.    \n\n[Aside: this is true even if the TE field is present and indicates that\n \"identity\" and \"chunked\" are not acceptable.  In other words, this\n paragraph already contradicts how we will implement HTTP.  I thought\n this was cleaned up after the discussion with Dave's issues.]\n\n>In section 14.40 (Trailer), change this paragraph\n>\n>    If no Trailer header field is present, the trailer SHOULD NOT\n>    include any header fields other than Content-MD5 and\n>    Authentication-Info.  A server MUST NOT include any other header\n>    fields unless the \"chunked\" transfer-coding is present in the\n>    request as an accepted transfer-coding in the TE field.\n>\n>to    \n\n     If no Trailer header field is present, the trailer SHOULD NOT\n     include any header fields.  See section 3.6.1 for restrictions\n     on the use of trailer fields in a \"chunked\" transfer-coding.\n\n \nRationale:  This option trades off the exception for Authentication-Info\nand Content-MD5, which was an unnecessary complication to begin with,\nwith a more complex check of the conditions under which a trailer field\nis allowed.  Condition (d) takes care of all current implementations\nof the chunked encoding that use trailer fields.  The other conditions\nare sufficient to prevent the paradox and still allow future applications\nto make use of the features.  In my opinion, this is also easier for\na server developer to implement, since the requirement is quite explicit.\nI also dislike repeating the same requirement in multiple places -- the\nreader needs a cross-reference to where the requirement is stated\nalong with its rationale, not repetition of a MUST sentence.\n\nOf course, Jeff is quite right that this doesn't solve the basic\nproblem of HTTP needing a way to negotiate a maximum message size,\nparticularly when we consider extensions that would require the\nproxy to verify the message content (or something else in the trailer)\nbefore forwarding the message.  But I think that is a general failure\nof single-stream/single-response protocols.  To solve it we would need\na method of splitting a message into separately digestable chunks,\nwhich is something we can't do in HTTP/1.x [actually, that is why we\nhave a chunk-ext field after every chunk-size, but trying to get people\nto define AA protocols that use such things is even harder than getting\nthem to implement Digest].\n\nThe notion of a buffer negotiation protocol extension doesn't really work.\nUse of chunked indicates that the server is not aware of the content\nlength before sending the data, so it wouldn't know if it could send it\nall in one buffer-limited size or not.  We would have to do something\nweird with the response code like in byte-ranges responses, but in this\ncase indicating that the response may or may not be complete up to\na certain length.  Actually, an easier way to accomplish that would\nhave been to define a special option for the ending zero-length chunk\nsuch that it would indicate an incomplete but self-consistent response,\nunlike the premature termination of the response when closing the connection.\n\nAh, crap.  If someone would please go back in time two years and insert a\n\n   chunk-data\n   0;end-code=533\n   trailers\n\ninto the spec I'd really appreciate it.  I suppose we could enable that\nwith a hop-by-hop buffer limit extension.\n\n....Roy\n\nattached mail follows:\n>                  hop1            hop2             hop3\n>    OriginServer -------- ProxyA --------- ProxyB ---------- Client\n>    HTTP/1.1             HTTP/1.1         HTTP/1.0         HTTP/1.1\n>\n> ... there is no way for the end-hosts in this scenario\n> (Client or OriginServer) to even know that something has gone\n> wrong.\n\n  They'll know when the authentication fails.\n\n  My conclusion is that there are some aspects of 1.1, including\n  digest authentication and content-md5 that just won't end up working\n  in this scenario and that the proposed cures are worse than that\n  problem given that there are widely deployed 1.1 user agents that\n  won't provide the hints the solutions need.\n\n  The problem with any proposed solution that requires the TE field is\n  that at least some current deployed 1.1 user agents don't send it.\n  The following is a header capture from IE4.0 (with some manual line\n  wrapping):\n\n     GET / HTTP/1.1\n     Accept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg,\n             application/vnd.ms-excel, application/msword,\n             application/vnd.ms-powerpoint, */*\n     Accept-Language: en-us\n     Accept-Encoding: gzip, deflate\n     User-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\n     Host: birdcage:4042\n     Connection: Keep-Alive\n\n  Note the lack of 'TE: chunked'.\n\n  We may have a problem with deployed origin servers and proxies, but\n  any such problem pales in comparison to replacing all the IE 1.1\n  browsers out there.\n\n> (5) Add a requirement that servers/proxies SHOULD NOT put\n> Authentication-Info or Content-MD5 in the trailer of any message\n> if the request includes a Via field that lists an HTTP/1.0\n> forwarder.\n\n> Also, it violates our duty to ensure interoperability of all of the\n> protocol features (rather than simply the interoperability of\n> features in isolation).\n\n  I don't believe that we have a duty to ensure that all 1.1 features\n  will work in the face of an undetected 1.0 proxy - that is too\n  restrictive a test.  I believe that in this situation all that is\n  required is that it be possible to detect that an error has occured\n  (easy - the expected response field [A-I or C-MD5] will be missing)\n  and why.\n\n  I support solution 1a - do not change the protocol at this very late\n  date, add a note describing the potential backward compatibility\n  problem in the face of certain 1.0 proxy situations.\n\n  If that won't fly, how about this - add a new 4xx error code:\n\n    10.4.19 418 Path Unsuitable\n\n    The request could not be satisfied because the path over which it\n    was received is in some way unsuitable.  The response body SHOULD\n    include an explanation of the problem to aid the recipient.   For\n    example, the response requires the use of chunked encoding with a\n    Content-MD5 header field in the trailer, but the origin server\n    detected a 1.0 proxy server in the request path.\n\n  this is backward compatible because it will be interpreted as 400 by\n  anything and the response body will explain the problem.\n\nattached mail follows:\nRoy T. Fielding wrote:\n>        c) all of the protocols listed in the Via header field are\n>           HTTP/1.1 or later; or,\n\nNote that the \"Via\" header was a new addition to HTTP/1.1.  HTTP/1.0\nproxies wouldn't add a \"Via\" header, and you therefore you wouldn't be\nable to tell by just looking at the \"Via\" header if there is an HTTP/1.0\nproxy in between.\n\n-- \nAri LuotonenOpinions my own, not Netscape's.\nNetscape Communications Corp.\n\nattached mail follows:\n\nOn Tue, 18 Aug 1998, Ari Luotonen wrote:\n\n> Note that the \"Via\" header was a new addition to HTTP/1.1.  HTTP/1.0\n> proxies wouldn't add a \"Via\" header, and you therefore you wouldn't be\n> able to tell by just looking at the \"Via\" header if there is an HTTP/1.0\n> proxy in between.\n\n  I was confused about this too, but what proxies are supposed to put in\nthe Via header is the version they _received_ from the downstream side,\nso a 1.1 proxy forwarding a request from a 1.0 proxy from a 1.1 browser\nwould put 1.0 in Via (unless the 1.0 proxy is so broken that it\nforwarded the 1.1 from the browser - which I believe we have seen\ninstances of).\n\n\n\nattached mail follows:\nYour 418 proposal fails because, in the absence of a method to indicate - on a\nnon-chunked message - the final status code, it would require the proxy to\nbuffer the entire object (at which time the point is moot).\n\nRegards,\nRichard L. Gray\nwill code for chocolate\n\n\n\nlawrence@agranat.com on 08-18-98 11:12:12 AM\nPlease respond to lawrence@agranat.com\nTo: mogul@pa.dec.com\ncc: Richard Gray/Raleigh/IBM@ibmus, luotonen@netscape.com, josh@microsoft.com,\nfielding@ics.uci.edu, frystyk@w3.org, jg@w3.org, paulle@microsoft.com,\nmasinter@parc.xerox.com\nSubject: Re: Analysis of \"chunking, trailers, and buffering\" problem\n\n\n>                  hop1            hop2             hop3\n>    OriginServer -------- ProxyA --------- ProxyB ---------- Client\n>    HTTP/1.1             HTTP/1.1         HTTP/1.0         HTTP/1.1\n>\n> ... there is no way for the end-hosts in this scenario\n> (Client or OriginServer) to even know that something has gone\n> wrong.\n\n  They'll know when the authentication fails.\n\n  My conclusion is that there are some aspects of 1.1, including\n  digest authentication and content-md5 that just won't end up working\n  in this scenario and that the proposed cures are worse than that\n  problem given that there are widely deployed 1.1 user agents that\n  won't provide the hints the solutions need.\n\n  The problem with any proposed solution that requires the TE field is\n  that at least some current deployed 1.1 user agents don't send it.\n  The following is a header capture from IE4.0 (with some manual line\n  wrapping):\n\n     GET / HTTP/1.1\n     Accept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg,\n             application/vnd.ms-excel, application/msword,\n             application/vnd.ms-powerpoint, */*\n     Accept-Language: en-us\n     Accept-Encoding: gzip, deflate\n     User-Agent: Mozilla/4.0 (compatible; MSIE 4.01; Windows NT)\n     Host: birdcage:4042\n     Connection: Keep-Alive\n\n  Note the lack of 'TE: chunked'.\n\n  We may have a problem with deployed origin servers and proxies, but\n  any such problem pales in comparison to replacing all the IE 1.1\n  browsers out there.\n\n> (5) Add a requirement that servers/proxies SHOULD NOT put\n> Authentication-Info or Content-MD5 in the trailer of any message\n> if the request includes a Via field that lists an HTTP/1.0\n> forwarder.\n\n> Also, it violates our duty to ensure interoperability of all of the\n> protocol features (rather than simply the interoperability of\n> features in isolation).\n\n  I don't believe that we have a duty to ensure that all 1.1 features\n  will work in the face of an undetected 1.0 proxy - that is too\n  restrictive a test.  I believe that in this situation all that is\n  required is that it be possible to detect that an error has occured\n  (easy - the expected response field [A-I or C-MD5] will be missing)\n  and why.\n\n  I support solution 1a - do not change the protocol at this very late\n  date, add a note describing the potential backward compatibility\n  problem in the face of certain 1.0 proxy situations.\n\n  If that won't fly, how about this - add a new 4xx error code:\n\n    10.4.19 418 Path Unsuitable\n\n    The request could not be satisfied because the path over which it\n    was received is in some way unsuitable.  The response body SHOULD\n    include an explanation of the problem to aid the recipient.   For\n    example, the response requires the use of chunked encoding with a\n    Content-MD5 header field in the trailer, but the origin server\n    detected a 1.0 proxy server in the request path.\n\n  this is backward compatible because it will be interpreted as 400 by\n  anything and the response body will explain the problem.\n\n\n\nattached mail follows:\n>Note that the \"Via\" header was a new addition to HTTP/1.1.  HTTP/1.0\n>proxies wouldn't add a \"Via\" header, and you therefore you wouldn't be\n>able to tell by just looking at the \"Via\" header if there is an HTTP/1.0\n>proxy in between.\n\nThat is why the proxies list the incoming protocol in Via instead of\ntheir own.  Either 1.0 will be listed or the request itself will be HTTP/1.0\n(which isn't one of the cases because you can't send chunked to a 1.0\nrequest).\n\n....Roy\n\nattached mail follows:\nI wrote (admitedly in haste):\n\n>     10.4.19 418 Path Unsuitable\n> \n>     The request could not be satisfied because the path over which it\n>     was received is in some way unsuitable.  The response body SHOULD\n>     include an explanation of the problem to aid the recipient.   For\n>     example, the response requires the use of chunked encoding with a\n>     Content-MD5 header field in the trailer, but the origin server\n>     detected a 1.0 proxy server in the request path.\n> \n>   this is backward compatible because it will be interpreted as 400 by\n>   anything and the response body will explain the problem.\n\nRichard Gray replies:\n\n> Your 418 proposal fails because, in the absence of a method to \n> indicate - on a non-chunked message - the final status code,\n> it would require the proxy to buffer the entire object (at which \n> time the point is moot).\n\nI was perhaps unclear - my intent was that the origin server could decide to\nrespond to the original request with a 418 when it saw the 1.0 in the Via\nheader on a request it knew that it would need to send trailers for (in the\ncase of my server, any qop=auth-int response).  The proxy needs no special\nhandling.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\nattached mail follows:\n    >                  hop1            hop2             hop3\n    >    OriginServer -------- ProxyA --------- ProxyB ---------- Client\n    >    HTTP/1.1             HTTP/1.1         HTTP/1.0         HTTP/1.1\n    >\n    > ... there is no way for the end-hosts in this scenario\n    > (Client or OriginServer) to even know that something has gone\n    > wrong.\n    \n      They'll know when the authentication fails.\n\nWhen the Authentication-Info header field appears in a response,\nwill the *client* know whether authentication has failed?  Or\nwill it simply become vulnerable to the very man-in-the-middle\nattack that the response-digest is supposed to prevent?  (I\nadmit that I don't really understand the details of Digest Auth.)\n    \nBut if there is no true security problem that arises when the\nAuthentication-Info field is silently dropped, then I suppose\nwe could just allow proxies to silently drop it whenever they\nfeel like it.\n\n      The problem with any proposed solution that requires the TE field\n      is that at least some current deployed 1.1 user agents don't send\n      it.\n\nNot really.  The new use of the TE field is not required; it\nis only required to allow the optimization that avoids buffering\nthe entire message at the origin server.\n\nRemember, this inevitably comes down to: someone has to buffer\nthe entire message - is it the origin server or one of the proxies?\n    \nAnd we could couple this with \"or no Via header in the request\", and\nsolve the problem for all proxy-free paths.\n\n      If that won't fly, how about this - add a new 4xx error code:\n    \n10.4.19 418 Path Unsuitable\n    \nThe request could not be satisfied because the path over which it\nwas received is in some way unsuitable.  The response body SHOULD\ninclude an explanation of the problem to aid the recipient.   For\nexample, the response requires the use of chunked encoding with a\nContent-MD5 header field in the trailer, but the origin server\ndetected a 1.0 proxy server in the request path.\n    \n      this is backward compatible because it will be interpreted as 400 by\n      anything and the response body will explain the problem.\n\nAlternatively (since it's a matter of opinion whether the\n\"unsuitable\" part of the path is the lack of buffer space in the\nproxy, or the lack of buffer space in the origin server), how\nabout just sending\n\n    10.5.1 500 Internal Server Error\n\n    The server encountered an unexpected condition which prevented it\n    from fulfilling the request.\n\nwhich is, after all, an accurate description of what has happened :-)\n\n-Jeff\n\nattached mail follows:\nThis is a bit of a digression, since it's pretty clear that we\naren't going to follow this path for HTTP/1.1 ... but Roy writes:\n\n    The notion of a buffer negotiation protocol extension doesn't\n    really work.  Use of chunked indicates that the server is not aware\n    of the content length before sending the data, so it wouldn't know\n    if it could send it all in one buffer-limited size or not.\n\nI think you have made the incorrect assumption that the server\nonly has two choices:\n\n(1) chunk the response and add a trailer to carry the digest.\n(2) generate an error because it doesn't know the length\nand digest before starting to send the data.\n\nbut there is a third choice (however much Scott objects to it)\n\n(3) buffer the entire response and then send it with\nthe digest in a header.\n\nOr even a fourth choice \n\n(4) generate a shorter-than-normal response (i.e.,\nan odd form of content-negotiation).\n\nwhich is especially useful if the limited-size buffer is\nat the ultimate recipient (e.g., a PDA).  (I would argue,\nas I wrote in my original analysis, for separate end-to-end\nand hop-by-hop buffer size negotiations, since these are\nreally separate problems.)\n\nThe nice thing about a negotiation protocol is that, when it\nsays there *is* enough buffer space at the receiver, the\nsender can safely use choice #1 and know that the message\nwon't be lost due to buffering problems.  And when there\nis *not* enough buffer space, the server can make the choice\nbetween choice #2 and choice #3, rather than leaving the\nresult up to chance.\n\nMost disk-based servers could easily implement #3; embedded\nservers might have to use choice #2.  But without some sort\nof explicit negotiation, it's hard to know whether you can\nsafely implement #1.\n\n-Jeff\n\nattached mail follows:\nJeffrey Mogul wrote:\n\n> When the Authentication-Info header field appears in a response,\n> will the *client* know whether authentication has failed?  Or\n> will it simply become vulnerable to the very man-in-the-middle\n> attack that the response-digest is supposed to prevent?  (I\n> admit that I don't really understand the details of Digest Auth.)\n\nIn the latest version of Digest, the client also provides a nonce to be used\nby the server, so yes - the client can detect the failure. \n\n> But if there is no true security problem that arises when the\n> Authentication-Info field is silently dropped, then I suppose\n> we could just allow proxies to silently drop it whenever they\n> feel like it.\n\nIf the Authenitcation-Info field (whether in the header or the trailer) is\nremoved, then the only indication of a problem is that something that had\nbeen assumed to require authentication now appears not to - this is possible\nwith anyway due to MITM attacks.  Ultimately, that situation must be caught\nby the human user by noticing that the security attributes have changed;\nhence the recommendations in Security Considerations that there be an\nindication.\n\n>       The problem with any proposed solution that requires the TE field\n>       is that at least some current deployed 1.1 user agents don't send\n>       it.\n> \n> Not really.  The new use of the TE field is not required; it\n> is only required to allow the optimization that avoids buffering\n> the entire message at the origin server.\n\nBut the point is that because it is not sent by current 1.1 browsers it\ndoesn't even do that.  The proposed change (for the optimization) is based\non the premise that the buffering problem can be avoided, but without the TE\nit can't be.\n\n> Remember, this inevitably comes down to: someone has to buffer\n> the entire message - is it the origin server or one of the proxies?\n\nNeither - all we need is a way to indicate the problem.  I actually don't\nthink it will come up much or persist long.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\nattached mail follows:\n>I think you have made the incorrect assumption that the server\n>only has two choices:\n>\n>(1) chunk the response and add a trailer to carry the digest.\n>(2) generate an error because it doesn't know the length\n>and digest before starting to send the data.\n>\n>but there is a third choice (however much Scott objects to it)\n>\n>(3) buffer the entire response and then send it with\n>the digest in a header.\n\nI discarded that choice because it assumes the server knows that the response\nwill be larger than what the downstream client has negotiated.  The server\ncan't buffer all responses since that presents an unacceptable amount\nof user-perceived latency, so it would need to know ahead of time which\nones should be buffered, but if it knew that it wouldn't need chunked.\n\n>Or even a fourth choice \n>\n>(4) generate a shorter-than-normal response (i.e.,\n>an odd form of content-negotiation).\n\nThat's what I meant by the trailing end-code on a chunked.  It works\nfrom both the performance and negotiation sides of the issue, but it\nwould require that the trailer fields be specific to the amount of\ncontent actually sent in that message.  It would thus also require a\npartial content response delivered across the 1.0 hop that made clear\nwhich metadata applies to only the partial content.  We could use such\na thing in any case for multihop request chains when the response\nis only partially available due to a dropped connection or partial cache.\n\nBut such a concept would need its own I-D.\n\n....Roy\n\nattached mail follows:\nJim asked me to try to send something out to the HTTP-WG\nbefore the IETF meeting.  However, he waited too long, and\nI may have spent too much time in meetings today before\nsending this.  Anyway, I wanted to see if we have any\nconsensus among ourselves regarding a solution.\n\nHow about the following:\n\n(E2) CONSENSUS RECOMMENDATION FROM THE HTTP/1.1 EDITORIAL GROUP\n\nUse of the Via header to detect potentially incompatible\nproxying paths seems to be the least objectionable solution\nat this point.  Also, it seems unlikely that we could\nenforce a new MUST-level requirement on either servers\nor proxies, since certain implementors of both kinds of\nsoftware believe that they would have to violate it anyway.\n\nHere are the proposed changes for this solution:\n\nIn section 3.6.1 (Chunked Transfer Coding), change this paragraph:\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].\n\nto read\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].  A server\nSHOULD NOT send any trailer fields if the request includes\na Via header field listing one or more HTTP/1.0 hops,\nregardless of the content of any TE field in the request.\n\n    Note: a server that sends a trailer field in response\n    to a request with a Via field listing an HTTP/1.0 hop\n    is taking a risk that the trailer will not be delivered,\n    especially if the message is long.  If the loss of\n    the trailer would lead to significant and unrecoverable\n    problems, and if the server is unable to buffer the\n    entire response (in order to insert an Authentication-Info\n    or Content-MD5 header prior to the message-body), then\n    it might be appropriate to response with an error status,\n    such as 500 (Internal Server Error).\n\n    Note: the use of the \"TE: chunked\" request-header to\n    indicate a client's willingness to accept trailer fields\n    other than Authentication-Info or Content-MD5 is an\n    artifact of history.\n    \nIn section 14.40 (Trailer), change this paragraph\n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nto    \n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nNote: section 3.6.1 recommends against the use of any\ntrailer fields in a response to a request with a Via\nheader field listing one or more HTTP/1.0 hops.  Trailers\nsent in violation of this requirement could be silently\ndropped by an intervening proxy, if the message is being\nforwarded to an HTTP/1.0 recipient.\n\nNB: respecting Koen's recent complaints, the change to 14.40\ndoes not restate the normative requirement in 3.6.1, but does\nremind the implementor why it is there.\n\n[End]\n\nattached mail follows:\nI really really really dislike the continuing use of TE to indicate\nanything about trailers.  In order for trailers to be usable, the\nentire response path must be willing to forward them or be willing\nto buffer the entire message or be allowed to discard them.  TE cannot\nindicate that because it is a hop-by-hop field, so we'd be better off\nremoving the exception entirely and leaving that functionality to\nbe specified properly in the future.\n\nLikewise, making specific exceptions for Content-MD5 and Authentication-Info\nis unnecessary if we just make a general statement for the condition\nwhere trailer fields can be included if it is permissible to ignore them.\n\nIn other words, I believe the following is substantially better:\n\nIn section 3.6.1 (Chunked Transfer Coding), change this paragraph:\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].\n\nto read\n\n    A server using chunked transfer-coding in a response MUST NOT\n    use the trailer for any header fields unless at least one of \n    the following are true:\n\n       a) there is no Via header field (indicating a connection without\n          intermediary proxies);\n\n       b) all of the protocols listed in the Via header field are\n          HTTP/1.1 or later; or,\n\n       c) the server grants the recipient(s) the right and ability to\n          discard those trailer fields without forwarding them to any\n          downstream recipient of the remaining message.  This is\n          only possible if the trailer fields consist entirely of\n          optional metadata that is not necessary for the recipient\n          to understand in order to use the message.\n\n    The above requirement exists to avoid an interoperabilty paradox\n    when the message is being received by an HTTP/1.1 (or later) proxy\n    and forwarded to an HTTP/1.0 recipient.  It avoids a situation\n    where compliance with the protocol would have necessitated an\n    infinite buffer on the proxy.\n\nIn section 14.40 (Trailer), change this paragraph\n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nto    \n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields.  See section 3.6.1 for restrictions\n    on the use of trailer fields in a \"chunked\" transfer-coding.\n\n....Roy\n\nattached mail follows:\n       a) there is no Via header field (indicating a connection without\n          intermediary proxies);\n       b) all of the protocols listed in the Via header field are\n          HTTP/1.1 or later; or,\n       c) the server grants the recipient(s) the right and ability to\n          discard those trailer fields without forwarding them to any\n          downstream recipient of the remaining message.  This is\n          only possible if the trailer fields consist entirely of\n          optional metadata that is not necessary for the recipient\n          to understand in order to use the message.\n\nEditorially, it may be simpler if we combine (a) & (b). Either you're\nHTTP/1.1 through all proxies (if there are any) or else trailer fields\nmay be discarded by recipients or proxies.\n\nLarry\n\n\n\n", "id": "lists-012-5349058"}, {"subject": "Re: Fwd: Analysis of &quot;chunking, trailers, and buffering&quot; problem (CHUNKEDTRAILERS", "content": ">A \"TE: chunked\" header field means for any particular hop that \"I accept\n>full featured chunked encoding with trailers and not just the boiled down\n>version without trailers\". This was a compromise necessary to do was\n>existing implementations only support non-trailered chunked encoding.\n\nIt does not say that in the definition of TE in the spec, nor should\nit say such a thing.  Creating a special-case definition within an\nunimplemented header field is not an appropriate design.  TE was\nadded to the spec to fix one and only one bug -- the desire to add\ndeflate as a transfer coding without breaking deployed systems.\nIf that is not its purpose, then I want TE removed from the spec right now.\n\nThere are multiple bugs in the definition of the TE header field.\n\nThe current definition says that:\n\n   If an TE field is present in a request, and if a server cannot send a\n   response which is acceptable according to the TE header field, then the\n   server SHOULD send an error response with the 406 (Not Acceptable)\n   status code.\n\nwhich is wrong because the 406 response code indicates a failure\nin content negotiation on the origin server, not an unacceptable\ntransfer coding at any particular hop.  If you want that functionality\nit must be defined as a new 5xx status code.  Furthermore, the above\nadds a requirement to HTTP/1.1 that is not implemented in deployed\nsystems and is not necessary for interoperability.\n\nThe reality, however, is that no server worth deploying will ever\nobey such a requirement.  From the server's point of view, assuming that\nthe client is broken is preferable to dealing with user complaints\nabout the inability to get anything but errors with no body content.\nTE should not be able to change the acceptability of the identity and\nchunked encodings.\n\nThere are multiple ways that we could design an EXTENSION to HTTP/1.1\nthat provided a hop-by-hop method of allowing a maximum buffer size\nfor chunked content that includes a trailer.  It is not necessary for\nus to do it with TE, nor is it appropriate to do so as a last minute\nchange before moving to draft standard.\n\n....Roy\n\n\n\n", "id": "lists-012-5409807"}, {"subject": "CHUNKEDTRAILERS, attempted fix ", "content": "Henrik and I sat down and discussed the various options and meaning\nbehind the existing sections.  As a result, here is a proposed fix\nthat better fits current practice and allows for future improvements.\n\nMuch of the confusion of TE is in the use of \"chunked\" to indicate\nthat trailers are okay (it serves no other purpose in TE).  If we\nreplace that token with \"trailers\", much of the confusion goes away\nand we can ensure that the client sending it does fully understand\nwhat it is saying when it says it agrees to accept trailers.\n\nThe other part is we both agree that the use of 406 is wrong.\nSince it is only used when identity;q=0 is present, and there\nhas yet to be any real need for that with transfer codings, the\neasiest solution is to just remove it instead of replacing it\nwith a new 5xx code.\n\nSo, here is another proposed solution:\n\nIn section 3.6.1 (Chunked Transfer Coding), change this paragraph:\n\n        A server using chunked transfer-coding in a response MUST NOT\n        use the trailer for other header fields than Content-MD5 and\n        Authentication-Info unless the \"chunked\" transfer-coding is\n        present in the request as an accepted transfer-coding in the TE\n        field (section 14.39). The Authentication-Info header is\n        defined by RFC 2069 [32] or its successor [43].\n\nto read\n\n    A server using chunked transfer-coding in a response MUST NOT\n    use the trailer for any header fields unless at least one of \n    the following is true:\n\n       a) the request included a TE header field that indicates\n          \"trailers\" is acceptable in the transfer-coding of the\n          response, as described in section 14.39; or,\n\n       b) the server grants the recipient(s) the right and ability to\n          discard those trailer fields without forwarding them to any\n          downstream recipient of the remaining message.  This is\n          only possible if the trailer fields consist entirely of\n          optional metadata that is not necessary for the recipient\n          to understand in order to use the message.\n\n    The above requirement exists to avoid an interoperabilty paradox\n    when the message is being received by an HTTP/1.1 (or later) proxy\n    and forwarded to an HTTP/1.0 recipient.  It avoids a situation\n    where compliance with the protocol would have necessitated an\n    infinite buffer on the proxy.\n\nIn section 14.40 (Trailer), change this paragraph\n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields other than Content-MD5 and\n    Authentication-Info.  A server MUST NOT include any other header\n    fields unless the \"chunked\" transfer-coding is present in the\n    request as an accepted transfer-coding in the TE field.\n\nto    \n\n    If no Trailer header field is present, the trailer SHOULD NOT\n    include any header fields.  See section 3.6.1 for restrictions\n    on the use of trailer fields in a \"chunked\" transfer-coding.\n\nIn section 3.6, replace\n\n    Whenever a transfer-coding other than \"identity\" is applied to a\n    message-body, the set of transfer-codings MUST include \"chunked\", unless\n    the message is terminated by closing the connection. When the \"chunked\"\n    transfer-coding is used, it MUST be the last transfer-coding applied to\n    the message-body. The \"chunked\" transfer-coding MUST NOT be applied more\n    than once to a message-body. These rules allow the recipient to\n    determine the transfer-length of the message (section 4.4).\n\nwith\n\n    Whenever a transfer-coding is applied to a message-body, the set of\n    transfer-codings MUST include \"chunked\", unless the message is\n    terminated by closing the connection. When the \"chunked\"\n    transfer-coding is used, it MUST be the last transfer-coding applied to\n    the message-body. The \"chunked\" transfer-coding MUST NOT be applied more\n    than once to a message-body. These rules allow the recipient to\n    determine the transfer-length of the message (section 4.4).\n\nDelete section 3.6.2:\n\n    3.6.2 Identity Transfer-Coding\n\n    The identity transfer-encoding is the default (identity) encoding; the\n    use of no transformation whatsoever. The identity transfer-coding is\n    used only in the TE header field, and SHOULD NOT be used in any\n    Transfer-Encoding header field.\n\nIn section 14.39 (TE), replace as follows:\n\n    14.39 TE\n\n    The TE request-header field indicates what extension transfer-codings\n    it is willing to accept in the response and whether or not it is\n    willing to accept trailer fields in a chunked transfer-coding.\n    Its value may consist of the keyword \"trailers\" and/or a\n    comma-separated list of extension transfer-coding names with\n    optional accept parameters (as described in section 3.9).\n\n           TE         = \"TE\" \":\" #( t-codings )\n           t-codings  = \"trailers\" | ( transfer-extension [ accept-params ] )\n\n    The presence of the keyword \"trailers\" indicates that the client is\n    willing to accept trailer fields in a chunked transfer-coding, as\n    defined in section 3.9.1.  This keyword is reserved for use with\n    transfer-coding values even though it does not itself represent a\n    transfer-coding.\n\n    Examples of the use of the TE field:\n    \n           TE: deflate\n           TE:\n           TE: trailers, deflate;q=0.5\n\n    The TE header field only applies to the immediate connection. Therefore,\n    the keyword MUST be supplied within a Connection header field (section\n    14.10) whenever TE is present in an HTTP/1.1 message.\n\n    A server tests whether a transfer-coding is acceptable, according to a\n    TE field, using these rules:\n\n      1. The \"chunked\" transfer-coding is always acceptable.  If the keyword\n         \"trailers\" is listed, the client indicates that it is willing to\n         accept trailer fields in the chunked response on behalf of itself\n         and any downstream clients.  The implication is that, if given, the\n         client is stating that either all downstream clients are willing\n         to accept trailer fields in the forwarded response, or that it\n         will attempt to buffer the response on behalf of downstream\n         recipients.\n\n         Note: HTTP/1.1 does not define any means to limit the size of\n         a chunked response such that a client can be assured of buffering\n         the entire response.\n\n      2. If the transfer-coding being tested is one of the transfer-codings\n         listed in the TE field, then it is acceptable unless it is\n         accompanied by a qvalue of 0. (As defined in section 3.9, a\n         qvalue of 0 means \"not acceptable.\")\n\n      3. If multiple transfer-codings are acceptable, then the acceptable\n         transfer-coding with the highest non-zero qvalue is preferred.\n         The \"chunked\" transfer-coding always has a qvalue of 1.\n\n      4. If the TE field-value is empty or if no TE field is present, the\n         only transfer-coding is \"chunked\".  A message with no\n         transfer-coding is always acceptable.\n\n[end of section]\n\nIn 19.6.3 (Changes from RFC 2068), delete\n\n    A content-coding of \"identity\" was introduced, to solve problems\n    discovered in caching. (section 3.5)\n\nIn 19.7.1 (Transfer-coding Values), replace\n\n    This document defines a new class of registry for its transfer-coding\n    values as part of the solution to solve problems discovered in RFC 2068\n    with the caching of transfer encoded documents. Initially, the registry\n    should contain the following tokens: \"chunked\" (section 3.6.1),\n    \"identity\" (section 3.6.2), \"gzip\" (section 3.5), \"compress\" (section\n    3.5), and \"deflate\" (section 3.5). ...\n\nwith\n\n    This document defines a new class of registry for its transfer-coding\n    values as part of the solution to solve problems discovered in RFC 2068\n    with the caching of transfer encoded documents. Initially, the registry\n    should contain the following tokens: \"chunked\" (section 3.6.1),\n    \"gzip\" (section 3.5), \"compress\" (section 3.5), \"deflate\" (section 3.5),\n    and the special keyword \"trailers\" (section 14.39). ...\n\n\n....Roy and Henrik\n\n\n\n", "id": "lists-012-5420429"}, {"subject": "Re: CHUNKEDTRAILERS, attempted fix ", "content": "In the interest of not equating silence with agreement:\n\nThis seems a reasonable and workable compromise.\n\n\n\n", "id": "lists-012-5437113"}, {"subject": "Re: CHUNKEDTRAILERS, attempted fix ", "content": "I'm comfortable with Roy & Henrik's proposal, but I have a small qualm\nabout this wording:\n\n    A server using chunked transfer-coding in a response MUST NOT\n    use the trailer for any header fields unless at least one of\n    the following is true:\n\n       a) ....\n\n       b) the server grants the recipient(s) the right and ability to\n          discard those trailer fields without forwarding them to any\n          downstream recipient of the remaining message.  This is\n          only possible if the trailer fields consist entirely of\n          optional metadata that is not necessary for the recipient\n          to understand in order to use the message.\n\nThe use of the verb \"grants\" here implies some sort of specific\naction on the part of the server, which of course is not the case.\nIn fact, the second sentence does a better job of specifying the\nconditions under which an exception can be made ... the phrase\n\"this is only possible if\" is a good tip-off.\n\nAlso, the use of the term \"server\" instead of \"origin server\"\nimproperly (I think) allows an intervening proxy to \"grant\"\nthe right to drop metadata that the actual origin server might\nconsider \"mandatory.\"\n\nI would suggest using something more like:\n\n       b) The server is the origin server for the response, the\n          trailer fields consist entirely of optional metadata,\n  and the recipient could use the message (in a manner\n  acceptable to the origin server) without receiving this\n  metadata.  In other words, the origin server is willing\n  to accept the possibility that the trailer fields might\n  be silently discarded along the path to the client.\n\nAlso, two minor nits about the explanatory paragraph that follows:\n\n    The above requirement exists to avoid an interoperabilty paradox\n    when the message is being received by an HTTP/1.1 (or later) proxy\n    and forwarded to an HTTP/1.0 recipient.  It avoids a situation\n    where compliance with the protocol would have necessitated an\n    infinite buffer on the proxy.\n\nThere's a spelling error, and the use of \"paradox\" here doesn't\nmatch any of the definitions in my dictionary.  I'd suggest:\n\n    This requirement prevents an interoperability failure\n    when the message is being received by an HTTP/1.1 (or later) proxy\n    and forwarded to an HTTP/1.0 recipient.  It avoids a situation\n    where compliance with the protocol would have necessitated an\n    infinite buffer on the proxy.\n\n-Jeff\n\n\n\n", "id": "lists-012-5444299"}, {"subject": "Re: CHUNKEDTRAILERS, attempted fix ", "content": "Just to add a real data point to the discussion:\n\nIn handling Digest, my server decides whether to send qop=\"auth\" or\nqop=\"auth,auth-int\" in a challenge based on whether it knows that the\nclient can handle chunked+trailers, since I send Authentication-Info as\na chunked trailer always.  If you think it through, you see the\nmessage sequence is\n\n1) C->S\n    Get resource R\n2) S->C\n    401 Authentication Required\n    send qop=\"auth\" or qop=\"auth,auth-int\" based on client's ability to\n    handle chunked+trailers\n3) C->S\n    Get resource R\n    WWW-Authenticate ... qop=auth or qop=auth-int\n4) S->C\n    200 OK\n    send R, possibly chunked and with trailers for qop=auth-int\n\nTherefore, if the server can decide at step (2) whether it's safe\nto send trailers, there shouldn't be a problem.\n\nI can live with Roy's Via proposal or the TE: trailers proposal.  Both\nshould let me tell whether it's safe to send trailers.\n\nDave Kristol\n\n\n\n", "id": "lists-012-5454114"}, {"subject": "Re: CHUNKEDTRAILERS, attempted fix ", "content": "This solution works for me.\n(I _am_ paying attention even though I could not make it to Chicago...)\n\n\n\n", "id": "lists-012-5462665"}, {"subject": "new test Digest serve", "content": "After a bit of a rush-job, I've changed my test server to support the\nhttp-authentication-02 version of Digest.  Specifically, the calculation\nof Authentication-Info is different, and is incompatible with the\nprevious draft and previous version of my server.\n\nI haven't tested this version of the server as much as I would like, but\nI wanted to make it available prior to my going on vacation.\n\n<http://portal.research.bell-labs.com:8000>\n\nDave Kristol\n\n\n\n", "id": "lists-012-5470441"}, {"subject": "Minutes for HTTP working group meetin", "content": "Minutes for the HTTP working group; Larry Masinter, Chair\n42nd IETF.  Monday August 25, 1998\nReported by Ted Hardie (hardie@nasa.gov) and Larry Masinter;\nplease send any corrections or additions ASAP.\n\nThis (really!) was the last meeting of the HTTP working group\nat IETF. The intent is to have final drafts submitted to \ninternet-drafts, and forward to the ADs for Last Call, by \nthe end of this week.\n\nAgenda:\n-  Implementation reports\n-  Main draft: report on final issues\n-  Authentication draft: report on final issues\n-  closing the working group\n\nImplementation Reports: \n\nJim Gettys led a review of the status of implementation reports on the\nbase specification. To be fully satisfied, he believes the group\nshould put forward two interoperable implementations each for client\nand server and should ideally put forward two proxies as well except\nfor the case where proxy behavior is no different than other parties.\n\nCurrent implementation reports lack adequate testing of entity tags,\ntransport encoding, and trailers. One set of reports from a firewall\nproxy have been removed from those indicate because it functioned\nlargely as a tunnel; we now need one additional report from a proxy\nimplementing DELETE.\n\nFor digest we have one fully tested client and one fully tested\nserver. Some of the newer features related to 3rd party authentication\nare not yet available in any tested server. No proxy implementations\nare yet available. The latest draft changes one of the hashes, so\ncurrent implementations will need to fix that and re-test.\n\nAt present the group believes that proxy authentication using Digest\nrequires no special handling, so that we will try to move forward\nto IESG with the implementation reports that we currently have.\n\nIndependent of the \"implementation reports\", the chair raised the\nquestion of whether there might still be a face to face event,\nto insure interoperability in the context of complex interactions.\nYaron Goland of Microsoft suggested (and Josh Cohen later confirmed)\nthat the offer to host such an event still stood. A co-event with\nApacheCON or some similar event was also suggested.  Coordination\non such an event is continuing with; some of the participation can\nbe remote, although proxy testing seems to require more interaction.\nPlease contact Josh Cohen <joshco@microsoft.com> for arrangements.\n\n-------------\nMain HTTP Draft: Jim Gettys\n\nJim listed a number of editorial changes related to RFC2119 use of\nMUST/MAY/SHOULD; the new draft will move all normative language out of\nnotes, for the sake of clarity. He also reviewed and the working group\naccepted language around the NO-TRANSFORM flag.\n\nAfter discussion of the merits of allowing Content-MD5 and Digest\nAuthentication headers to appear in trailers when a message is\nchunked, the group decided to adopt Roy Fielding's proposal, which\nstates that Servers should not send essential headers in the trailer\nwhen the Via headers indicate a 1.0 proxy may be part of the chain;\nthe group will also include language from Paul Leach which indicates\nways in which an upstream 1.1 proxy might assist a 1.0 proxy in\nhandling very large chunked responses using these trailers.\n\nThe working group discussed a proposal to create a method registry for\nHTTP methods, but decided that related work implied a larger scope\nthan could be adopted by the group at this time. The group working on\nHTTP-EXT has a current proposal on this topic, and it will be the\nbasis for further discussion and that group will be the forum for\nfurther discussion.\n\nThe working group reconsidered the 416 header briefly; there is an\nexisting implementation of 2068's advice on out-of-bound ranges which\nconflicts with the advice in 416. After discussion which reiterated\nprevious positions, the chair ruled that the group had come to rough\nconsensus on this matter and the issue would remain closed.\n\nIn response to a question on the use of Upgrade by Carl-Uno Manros,\nthe group noted that it does not see any need for changes to the\ncurrent specification to allow UPGRADE to work with TLS; it does\nrequire a specification of a token for this use. Rohit Khare has\nwritten a draft to this end and it will be passed by the TLS working\ngroup before being sent to the IESG as an individual submission.\n\nJim Gettys offered to try to finish the main draft by 8/27, so that\nit could be completed before the end of this IETF meeting.\n\nAuthentication draft: Scott Lawrence\n\nA small editorial change is needed to the current draft to improve the\nlanguage related to Digest-URI. The URI is a component of the\nauthentication hash, but because of proxy transformation, the received\nrequest-URI and the Digest-URI may differ. It is the server's\nresponsibility to ensure that the two refer to the same resource, and\nthe language will be tightened to make that clear.\n\nAn editorial change related to the scope of damage related to snooped\npasswords is needed; the changes will be based on the language sent to\nthe list.\n\nScott Lawrence and Paul Leach offered to submit a revised draft\nof the Authentication draft by 8/27, so that it could be submitted\nbefore the end of this IETF.\n\n- Closing the working group\n\nThe chair went through the steps needed to close the working group:\n\n1)  Submit final drafts\n2)  Submit implementation reports\n3)  Send document to A-D for review\n4)  IETF Last Call\n5)  IESG review\n6)  RFC Editor publication\n\nThese elements might be pipelined to finish more quickly; the\nmost serious element for quick conclusion are the implementation\nreports, which, of course, depend on implementation testing\nof the few remaining insufficiently tested features; if this can\nhappen in the next week, we can send the documents to the A-D\nfor review and IETF Last Call, which will likely be simultaneous.\nGiven the wide review that has already occurred in the working\ngroup, it is likely that the IESG review will not be lengthy.\n\nThe working group will close after (5), which is likely as\nsoon as 2-3 months from now, and certainly before the next IETF\nmeeting.\n\nThe working group mailing list will remain open after the close\nof the working group.\n\nAt the close of the meeting, Keith Moore solicited feedback on the\nIESG draft describing layering other protocols on top of HTTP,\ndraft-iesg-using-http-00.txt.\n\nFeedback should go to the IESG or discuss@apps.ietf.org.\n\n\n\n", "id": "lists-012-5477078"}, {"subject": "Re: CHUNKEDTRAILERS, attempted fix ", "content": "Jeff's suggested tweaks are very good -- let's go with that.\n\n.....Roy\n\n\n\n", "id": "lists-012-5490773"}, {"subject": "report on HTTPW", "content": "This was (really!) the last meeting of the HTTP working group.\nWe have nearly completed interoperability testing, and have\ncompleted it sufficiently to move the HTTP specifications toward\nlast call. The final drafts of both documents (HTTP/1.1 and\nAuthentication) will be ready this week or next. After they\nappear, we will ask for A-D review and IETF last call. There\nhave been (as always) a few minor last-minute issues, but\nwe're done.\n\nAs was noted in the Application WG Chairs meeting, there may\nyet remain an issue over whether these documents 'recycle at\nProposed' or move forward to draft standard. The changes we\nmade were in response to problems found during Proposed Standard\ntesting, but in many ways it does not matter: what's important\nat this point is that these documents be published and move\nalong standards track quickly.\n\nLarry\n\n\n\n", "id": "lists-012-5497907"}, {"subject": "GET with offset ", "content": "Has anyone ever considered that it would be useful to be able\nto add an optionnal 'offset' parameter to the GET request ?\n\nThis would, for example, allow the implentation of very simple\nvideo and audio servers using just the http protocol (i.e.\nthe media server would just be the http server).\n\nCurrently the only feature missing to do that is the inability to\n'seek' in an object (i.e. requesting the object starting at\nan offset which is not always 0).\n\nBy the way, the same comments applies to the FTP protocol, too.\n\nIf you have comments on that, please send Cc to me\n(tristan@mpegtv.com).\n\nThanks.\n\n-- \nRegards, -- Tristan Savatier (President, MpegTV LLC)\n\nMpegTV:   http://www.mpegtv.com\n\n\n\n", "id": "lists-012-5505688"}, {"subject": "Re: GET with offset ", "content": "> \n> Has anyone ever considered that it would be useful to be able\n> to add an optionnal 'offset' parameter to the GET request ?\n\nThis feature already exists in HTTP/1.1 \n\nSee  http://www.w3.org/Protocols/rfc2068/rfc2068\n\n14.36.2 Range Retrieval Requests\n\nhttp://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v11-spec-rev-04.txt\n\n14.35.1 Byte Ranges\n\nor more generally, search for the word \"range\" in the text\nto find related subjects.\n-- \n-\n-\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-012-5513039"}, {"subject": "Re: GET with offset ", "content": "Thanks. Now I see how to do a GET with range request.\n\nNow, another question:\n\nIs there a way to specify the offset or range with the http: URL\nsyntax ?\n\ni.e. something like http://host/dir/file@range=0,500\n\n\n-t\n\nAlbert Lunde wrote:\n> \n> >\n> > Has anyone ever considered that it would be useful to be able\n> > to add an optionnal 'offset' parameter to the GET request ?\n> \n> This feature already exists in HTTP/1.1\n> \n> See  http://www.w3.org/Protocols/rfc2068/rfc2068\n> \n> 14.36.2 Range Retrieval Requests\n> \n> http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v11-spec-rev-04.txt\n> \n> 14.35.1 Byte Ranges\n> \n> or more generally, search for the word \"range\" in the text\n> to find related subjects.\n> --\n> -\n> -\n>     Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-012-5520918"}, {"subject": "Re: GET with offset ", "content": "On Sun, 30 Aug 1998, Tristan Savatier wrote:\n\n> Thanks. Now I see how to do a GET with range request.\n> \n> Now, another question:\n> \n> Is there a way to specify the offset or range with the http: URL\n> syntax ?\n> \n> i.e. something like http://host/dir/file@range=0,500\n> \n\nThere is no provision in HTTP to do this.  However, some servers\nhave the built-in capability and you could do it using a CGI script\nwith any server.\n\nAn early version of the the byterange specification did not use\na header and put the range request in the URL.  There are pros\nand cons for URL vs. header.  In the end it was deemed that a\nheader was best.  However, one of the cons is that you loose\na lot of functionality by not allowing a range specification\nin the URL.  Of course any server is free to do it in a non-standard\nway and, as mentioned above it can be done with CGI.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-5529841"}, {"subject": "Cachecontrol and Authenticatio", "content": "I'm attempting to get my head around the newest draft of 1.1, and was\nwondering if someone could clarify this for me:\n\nLet's say a server has content that clients access through a 1.1-capable\ncache (this is internal, so it can be controlled). There is a section of\nthe content that requires basic authentication, but the content does not\nchange based upon that authentication; any user-specific changes\ncontrolled by the path, query and parameters.\n\nWhat is the correct way to allow caches to keep, and satisfy requests\nfrom, a local copy, while still forcing the request to be revalidated\n(In this instance, so that the different users are indeed authenticated,\nas well as maintaining freshness, which is critical in this\napplication)?\n\n From reading section 14.9 as well as Squid docs, I'm lead to believe\nthat this can be done by server response headers that include\n\nCache-control: no-cache Authorization\n\n From my reading, this should cause and IMS to be issued WITH the new\nauthentication header, which will enable the cache to serve the local\nrequest IF the user is authenticated, and IF the object has not changed.\nIs this correct? Or would this be situation be covered by \n\nCache-control: public\n\n\nThanks,\n\n\n\nMark Nottingham\nInternet Project Manager\nMerrill Lynch - Melbourne, Australia\n\n\n\n", "id": "lists-012-5715221"}, {"subject": "Re: Cachecontrol and Authenticatio", "content": "Nottingham, Mark (Australia) wrote:\n\n> Let's say a server has content that clients access through a 1.1-capable\n> cache (this is internal, so it can be controlled). There is a section of\n> the content that requires basic authentication, but the content does not\n> change based upon that authentication; any user-specific changes\n> controlled by the path, query and parameters.\n> \n> What is the correct way to allow caches to keep, and satisfy requests\n> from, a local copy, while still forcing the request to be revalidated\n\nI believe that the correct way to do this is:\n\nCache-Control: must-revalidate\n\nIn addition to being controlled, I would also make it checked - look for a\n1.0 revision in the Via header (so that you know whether or not you've got a\n1.1 client or downstream proxy), and add 'Pragma: no-cache' header to\nprevent 1.0 caches from holding it just in case.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-5723598"}, {"subject": "RE: authentication02: threat of snooped passwor", "content": "How about this:\n\nThis is the preceding paragraph (for context):\n\nA common use of Basic authentication is for identification purposes --\nrequiring the user to provide a user name and password as a means of\nidentification, for example, for purposes of gathering accurate usage\nstatistics on a server. When used in this way it is tempting to think that\nthere is no danger in its use if illicit access to the protected documents\nis not a major concern. This is only correct if the server issues both user\nname and password to the users and in particular does not allow the user to\nchoose his or her own password. The danger arises because naive users\nfrequently reuse a single password to avoid the task of maintaining multiple\npasswords.\n\nThis is the proposed replacement for the paragraph in question:\n\nIf a server permits users to select their own passwords, then the threat is\nnot only illicit access to documents on the server but also illicit access\nto any other resources on other systems that the user protects with the same\npassword. Furthermore, in the server's password database, many of the\npasswords may also be users' passwords for other sites. The owner or\nadministrator of such a system could conceivably incur liability if this\ninformation is not maintained in a secure fashion.\n\n> -----Original Message-----\n> From: Dave Kristol [mailto:dmk@research.bell-labs.com]\n> Sent: Thursday, August 20, 1998 12:26 PM\n> To: http-wg@hplb.hpl.hp.com\n> Subject: authentication-02: threat of snooped password\n> \n> \n>     If a server permits users to select their own passwords, \n> then the threat\n>     is not only illicit access to documents on the server but \n> also illicit\n>     access to the accounts of all users who have chosen to \n> use their account\n>     password. If users are allowed to choose their own \n> password that also\n>     means the server must maintain files containing the (presumably\n>     encrypted) passwords. Many of these may be the account \n> passwords of\n>     users perhaps at distant sites. The owner or \n> administrator of such a\n>     system could conceivably incur liability if this \n> information is not\n>     maintained in a secure fashion.\n> \n> This paragraph surprises me a little.  It seems to me that if I choose\n> as a password some kind of account password, then the threat \n> is only to\n> me and all the accounts that share the password.  I don't see how this\n> allows \"illicit access to the accounts of all users who have chosen to\n> use their account password.\"  If an adversary grabs my password, how\n> does that open a risk to other users?\n> \n> I think what was meant here is said better and more succinctly in\n> Section 4.4:\n> \n>     The greatest threat to the type of transactions for which these\n>     protocols are used is network snooping. This kind of transaction\n>     might involve, for example, online access to a database whose use\n>     is restricted to paying subscribers. With Basic authentication an\n>     eavesdropper can obtain the password of the user. This not only\n>     permits him to access anything in the database, but, often worse,\n>     will permit access to anything else the user protects with the\n>     same password.\n> \n> Dave Kristol\n> \n\n\n\n", "id": "lists-012-5731873"}, {"subject": "Re: authentication02: threat of snooped passwor", "content": "I think that your proposed text is fine, and much better than what is in the\n02 draft except for the last sentence:\n\n> The owner or\n> administrator of such a system could conceivably incur liability if this\n> information is not maintained in a secure fashion.\n\nWe are engineers, not lawyers - I'm not really comfortable with expressing\nsomething that looks like a legal opinion.  It works well without it.\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-5743321"}, {"subject": "RE: authentication02: threat of snooped passwor", "content": "Paul Leach <paulle@microsoft.com> wrote:\n  > [...]\n  > This is the proposed replacement for the paragraph in question:\n  > \n  > If a server permits users to select their own passwords, then the threat is\n  > not only illicit access to documents on the server but also illicit access\n  > to any other resources on other systems that the user protects with the same\n  > password. Furthermore, in the server's password database, many of the\n  > passwords may also be users' passwords for other sites. The owner or\n  > administrator of such a system could conceivably incur liability if this\n  > information is not maintained in a secure fashion.\n\nJust a (what else?) nit:  the word \"illicit\" makes me uncomfortable.\nHow about \"unauthorized\"?\n\nI'm also inclined to agree with Scott's remarks about \"liability\".\nPerhaps the last sentence should read:\n\n    The owner or administrator of such a system could therefore expose\n    all users of the system to the risk of unauthorized access of all\n    those accounts if this information is not maintained in a secure\n    fashion.\n\nDave Kristol\n\n\n\n", "id": "lists-012-5751784"}, {"subject": "Re: Fwd: draft-ietf-http-v11-spec-rev04 comment", "content": "> Sender: bennettp@bt-sys.bt.co.uk\n> From: Paul Bennett <p.bennett@bt-sys.bt.co.uk>\n> Date: Wed, 12 Aug 1998 08:03:03 +0100\n> To: jg@w3.org\n> Subject: draft-ietf-http-v11-spec-rev-04 comments\n> -----\n> Jim,\n> \n> I've just completed reviewing the '-04' revision of the HTTP/1.1\n> spec, and have some comments for you.\n> \n> Most of these comments are simple editorial matters.  There are a\n> few genuine errors I noticed; I've listed things in decreasing order\n> of severity (I'd understand if you never get to the end of this\n> list, but I'd rather you dealt with the first couple).\n> \n> The spec's looking in good shape; I bet you'll be pleased to see it\n> reach Draft Standard.\n> \n\nSure will.  It will have a lot fewer mistakes courtesy of your read.\n\n> \n> Paul.\n> \n> \n> Out and out mistakes\n> \n>  * Page 2, last sentence: \"See section 18 for the full copyright\n>    notice.\" - that should be section *20*.  (I believe Henrik has\n>    already notified you of this one.)\n\nThere were some problems generating the plaintext; it is correct in the\npostscript.  I'll make sure the next draft's text is cleaner.\n\n> \n>  * Section 2.1 \"Augmented BNF\", p15, last sentence of first\n>    paragraph (\"implied *LWS\" rule): '(for the definition of\n>    \"token\" above)' - you mean, *below* (the def's in section 2.2).\n> \n\nFixed.\n\n>  * Section 5.1.2 \"Request-URI\", p33, second para talks about\n>    replacing 'a null abs_path with \"*\"' - surely it's replaced\n>    with a \"/\".\n> \nI think the spec is right as is.\n\n\n>  * Section 8.1.3 \"Proxy Servers\", first paragraph: \"... as specified\n>    in section 14.2.1.\" - no such section exists; maybe you mean\n>    section 14.10?\nfixed.\n> \n>  * Section 13.2.4 \"Expiration Calculations\", end of second paragraph:\n>    \"(see section 14.10.\" - missing right bracket, and the section\n>    number's wrong - should be 14.9.3.\n> \nfixed.\n\n>  * Section 13.5.2 \"Non-modifiable Headers\", page 79, last sentence\n>    on page refers to \"Warning 114 (Transformation Applied)\".  That\n>    should be \"214\", according to section 14.46, page 126.  This\n>    sentence also includes repetition: \"..., if not already present,\n>    ... if one does not already appear in the message.\".\n> \nfixed\n>  * Section 14.24 \"If-None-Match\", third para on p112, last sentence:\n>    \"... then the server MUST not return a 304 ...\"; the \"not\" should\n>    be capitalised: \"... the server MUST NOT return a 304 ...\".\n> \nfixed\n>  * Section 14.31 \"Max-Forwards\" refers to section 14.31 for the\n>    TRACE method; this should be section 9.8.\n> \nfixed\n>  * Section 19.4.2 \"Conversion to Canonical Form\" refers to \"Appendix\n>    G of RFC 2045\" - such an appendix doesn't exist!  The closest I\n>    can find is section 6.6... or maybe you mean something else.\n> \nfixed; the MIME documents were massively reorgized before they\nwent to draft standard, and I didn't catch all the reorganization\n>  * Section 13.5.1 \"End-to-end and Hop-by-hop Headers\" refers to the\n>    Keep-Alive field, which doesn't exist any more.\n> \nYes, it exists, but not in this spec (and it has not be fully \"standardized\");\nI fixed the reference.\n\n> \n> Clarifications\n> \n>  * Section 2.2 \"Basic Rules\", \"TEXT\" rule: \"<any OCTET except CTLs,\n>    but including LWS>\".  This excludes the optional \"CRLF\" of \"LWS\"\n>    by virtue of those two octets being CTLs, but it took a second\n>    reading to see this; perhaps \"<any OCTET except CTLs, but\n>    including HT and SP>\" would be clearer.\n> \n\nI've sent this to Roy Fielding and Dave Kristol to look at...  I don't\nbreath BNF the way they do.\n\n>  * Section 2.2 \"Basic Rules\", \"Note:\" in \"quoted-pair\" description:\n>    I *think* I get what the example's on about: it's the\n>    significance of an HT and/or SP before \"a\" and \"quoted-string\",\n>    right?  But the indentation of the example doesn't make this\n>    clear; perhaps indent those two lines more than the first of\n>    the example, or add more description to the text.\n> \nI've sent this to Roy Fielding and Dave Kristol to look at...  I don't\nbreath BNF the way they do.\n\n>  * Sections 9.3 \"GET\", 9.4 \"HEAD\" and 9.7 \"DELETE\" don't explicitly\n>    rule out the presence of an entity-body, as section 9.8 \"TRACE\"\n>    does.\n\nNo, and we're not sure that the spec should do so.  Not clear what\ncurrent implementations actually do, either.\n\n> \n>  * Section 10.2.2 \"201 Created\" says nothing about the format of the\n>    response entity.  (section 10.3.1 \"300 Multiple Choices\" in a\n>    similar position is explicitly non-committal.\n\nInteresting question; but it had to be defined carefully in 300 Multiple\nChoices to allow a future content negotiation standard, and I'm not sure\nit needs to be so well defined here.\n\n> \n>  * Section 10.3.6 \"305 Use Proxy\": \"The Location field gives the URI\n>    of the proxy.\"  But nowhere can I find a specification for this\n>    URI form; presumably it's an HTTP URI without an abs_path part.\n> \n\nNo, it can be any URI, per the URI spec.  This is necessary to allow\ngatewaying into other URI schemes.\n\n> \n> Typos\n> \n>  * Section 3.6 \"Transfer Codings\", third paragraph: \"section\n>    14.4114.41\" clearly should read \"section 14.41\".\n> \nfixed\n>  * Section 3.7.2 \"Multipart Types\", third paragraph is a repetition\n>    of the penultimate sentence of the second paragraph (but maybe you\n>    intended the repetition to emphasise the point!).\n> \nfixed\n>  * Section 10.1 \"Informational 1xx\": \"There are no required headers\n>    for this class of status codes.\"  Should be \"code\" singular, I\n>    think.\n> \nfixed\n>  * Section 10.2.7 \"206 Partial Content\", first bullet point: \"If a\n>    Content-Length header field is present in the response MUST match\n>    ...\" words missing!  Probably want \"... in the response, its value\n>    MUST match ...\".\n> \nfixed\n>  * Section 13.2.4 \"Expiration Calculations\", page 72, second para:\n>    weird additional whitespace in first line of this paragraph.\n>    Also, the last sentence includes repetition: \"If the value is\n>    greater than 24 hours, ... whose age is more than 24 hours ...\".\n> \nfixed\n>  * Section 13.3.3 \"Weak and Strong Validators\", first two sentences\n>    should be a single one (the first one doesn't make sense alone;\n>    perhaps replace \"... different entities.  One normally ...\" with\n>    \"... different entities, one normally ...\".\n> \nfixed\n>  * Section 13.5.3 \"Combining Headers\", second paragraph starts \"In\n>    the status code is 304\" - that should be \"If the status code ...\".\n>    The last sentence of this paragraph also repeats \"see 13.5.4\".\n> \nfixed\n>  * Section 13.5.4 \"Combining Byte Ranges\", last paragraph: \"If either\n>    requirement is not meant ...\" should be \"not met\", presumbaly.\n> \nfixed\n>  * Section 13.13 \"History Lists\", fourth paragraph: \"This is not be\n>    construed to prohibit\" - should be \"This is not construed ...\"\n> \nfixed\n>  * Section 14.8 \"Authorization\", last sentence of first para on p91:\n>    \"(assuming that the authentication schemed ...\" should be\n>    \"scheme\" (spurious 'd').\n> \nfixed\n>  * Section 14.21 \"Expires\", last paragraph \"... an response ...\"\n>    should be \"... a response ...\"\n> \nfixed\n>  * Section 14.25 \"If-None-Match\", end of first sentence: \"... used\n>    with a method to make the method conditional.\" should be \"... used\n>    with a method to make it conditional.\".  This brings the wording\n>    in line with that of the other If-* headers, as well as saving\n>    bytes :-)\n> \nfixed\n>  * Section 14.35.2 \"Range Retrieval Requests\", second para on p118:\n>    \"intermediate caches ought tosupport\" - space missing between \"to\"\n>    and \"support\".\n> \nfixed\n>  * Section 14.45 \"Via\", first para: spurious whitespace at start\n>    of fifth line.\n> \ntext problems; shouldn't be in future versions.\n>  * Section 14.46 \"Warning\", first sentence: \"transformation of a\n>    message whichmight\" - missing space.  Also spurious extra space\n>    at the start of the fourth para on p125: \" Warning headers can\n>    ...\".\n> \nfixed\n>  * Section 19.6.3 \"Changes from RFC 2068\", p146, third from last\n>    paragraph: extra right bracket: \"A new error code (416))\".\n> \nfixed\n> \n> Things you've probably debated to death\n> \n>  * Throughout: \"inbound\" vs. \"upstream\" - these terms seem to refer\n>    to the same concept, but are not defined or used consistently.\n>      \"inbound\":   13.11, 14.31, 14.35.2, 19.6.2\n>      \"outbound\":  13.2.3, 14.34\n>      \"upstream\":  10.5.3, 10.5.5, 14.45, 19.6.3,\n>      \"downstram\": 14.33\n> \nfixed\n>  * Section 3.6.1 \"Chunked Transfter Coding\", \"chunk-data\"\n>    production: \"chunk-size(OCTET)\" at a casual glance *might* imply\n>    \"1*HEX OCTET\", when presumably you mean \"n*n OCTET ; where n is\n>    the value of chunk-size\".\n> \nLeft as is.  various blood was sweated on this in the first place,\nand I expect/hope it is correct as is.\n>  * Section 15.1.2 \"Transfer of Sensitive Information\": given recent\n>    well-publicised security holes in specific user agents, perhaps\n>    \"User-agent\" should be added to the list of sensitive fields.\n> \n> \nadded comment\n\n\"The User-Agent header field (section 14.43) can sometimes be used to \ndetermine that a specific client has a particular security hole which \nmight be exploited. Unfortunately, this same information is often used \nfor other valuable purposes for which HTTP currently has no better \nmechanism. \" \n\n> Cross-references\n> \n>  * Throughout: I'd like to see more cross-referencing.  Particularly,\n>    against most occurrances of method names, field names, status and\n>    warning codes.\n> \nI'm out of energy on this topic.  I added some.\n\n>  * Section 1.2 \"Requirements\": the key words \"SHALL\", \"REQUIRED\",\n>    \"MAY\" and \"OPTIONAL\" are mentioned, but requirements for\n>    (un)conditionally compliant implementations with respect to\n>    these words are not laid down.  Suggest: replace \"the MUST\n>    requirements\" with \"the MUST, REQUIRED and SHALL requirements\".\n> \nMade minor clarifications here.  Your suggestion doesn't work,\nas it gets cumbersome.  I added \"MUST level\", \"SHOULD level\" requirements\ninstead to clarify this.\n\n>  * Section 3.2.2 \"http URL\", second paragraph: \"... and the\n>    Request-URI ...\" a forward reference to section 5.1.2 (for\n>    Request-URI) would be helpful.\n> \ndone.\n>  * Section 3.3.1 \"Full Date\": needs forward-reference to section\n>    19.3 (concerning the \"year 2000 problem\").\n> \ndone.\n>  * Section 8.2.1 \"Persistent Connections and Flow Control\":\n>    reference to section 3.6 (\"chunked encoding\") should more\n>    accurately be a reference to section 3.6.1.\n> \ndone\n>  * Section 13.6 \"Caching Negotiated Responses\", first sentence refers\n>    to section 12; 12.1 would be more accurate.\n> \ndone.\n>  * Section 14.4 \"Accapt-Language\" should refer to section 3.10.\n> \ndone.\n>  * Section 14.5 \"Accept-Range\" should refer to section 3.12.\n> \ndone.\n>  * Section 14.15 \"Content-Encoding\", second para on p100 refers to\n>    14.15 itself!\n> \nCouldn't find this one.\n>  * Section 14.15 \"Content-Range\" should refer to section 3.12.\n> \ndone.\n>  * Section 14.21 \"Expires\", third paragraph refers to section 3.3;\n>    3.3.1 would be more accurate.\n> \ndone.\n>  * Section 14.24 \"If-Match\" should refer so section 3.11.\n> \ndone.\n> \n> Really petty stuff\n> \n>  * Section 4.5 \"General Header Fields\": the BNF is *almost* in\n>    alphabetical order (shift \"Trailer\" up two places and you're\n>    there).  Also: comment part of \"Warning\" isn't quite aligned\n>    with the other comments.\n> \ndone\n>  * Section 5.3, \"Request Header Fields\": more alphabetical BNF:\n>    \"If-Match\" should appear before \"If-Modified-Since\".\n>\ndone.\n>  * Section 10.2.5 \"204 No Content\" and section 10.2.6 \"205 Reset\n>    Content\", last sentence of each, differ in their terminology:\n>    204 says \"The 204 response MUST NOT include a message-body\";\n>    205 says \"The response MUST NOT include an entity\".\n> \nNo.  you gotta be very careful here.  These are not interchangable.\n>  * Section 10.4.15 \"414 Requestr URI Too Long\": spurious extra\n>    space before the full-stop at the end of the first sentence.\n> \nfixed.\n> * Section 13.1.4 \"Explicit User Agent Warnings\": spurious extra\n>    full-stop at the end of the first paragraph.\n> \nfixed\n>  * Section 19.6.3 \"Changes from RFC 2068\", top of p146: spurious\n>    extra space at start of first line: \" Content-Base was\n>    deleted ...\".  Also, spurious extra whitespace at the start\n>    of points six and sever (page 147).\n> \nfixed\n> \n> Things you know about, but I'll mention anyway\n> \n>  * Throughout: The [jg] footnotes\n> \nWord artifact; I'll see the plaintext is clean the next time.\n>  * Throughout: \"HTTP Authentication: Basic and Digest Access\n>    Authentication\"; presumably you're waiting for an RFC number\n>    (but you might also flag these as [43]).  This appears at least\n>    in these sections: 10.4.2, 10.4.8, 11, 14.8, 14.33, 14.34, 14.47\n> \nFlagged by [ref]\n>  * Section 14.16 \"Content-Range\", second para on p104, \"See appendix\n>    Error! Reference source not found\".\n> \nfixed.\n>  * Section 19.1 \"Internet Media Type message/http and\n>    application/http\": screwy line-wrapping in the Encoding\n>    considerations section of application/http.\n> \nfixed.\n>  * Section 19.6 \"Compatibility with Previous Versions\", last\n>    sentence: \"Error! Reference source not found\" - presumably this\n>    should instead refer to RFC2068 since Keep-Alive has now gone.\n> \nfixed.\n>  * Section 21 \"Index\": the index is broken - at least, a good\n>    percentage of the referenced pages are irrelevant, which is\n>    a real shame because the index looks great and is *vital* in\n>    a document this size :-(\n> \n\nThe Word and Postscript versions should be fine in this area; I'll try \n to make sure the text index is clean in the next version. But Word has \n driven me to distraction with a bug I keep running into that causes it \n to crash.\n\n\n\n", "id": "lists-012-5760411"}, {"subject": "new BNFTEXT issue: use of CRLF in TEX", "content": "During a discussion with Jim and Dave about the note at the end of\nsection 2.2, I realized that an important part of the field parsing\nalgorithm is only stated indirectly in the HTTP/1.1 spec.\n\nSection 2.2:\n\n   HTTP/1.1 headers can be folded onto multiple lines if the continuation\n   line begins with a space or horizontal tab. All linear white space,\n   including folding, has the same semantics as SP.\n\n       LWS            = [CRLF] 1*( SP | HT )\n\n   The TEXT rule is only used for descriptive field contents and values\n   that are not intended to be interpreted by the message parser. Words of\n   *TEXT MAY contain characters from character sets other than ISO 8859-1\n   [22] only when encoded according to the rules of RFC 2047 [14].\n\n       TEXT           = <any OCTET except CTLs,\n                        but including LWS>\n\nwhere it should say\n\n   HTTP/1.1 header field values can be folded onto multiple lines if the\n   continuation line begins with a space or horizontal tab.  All linear\n   white space, including folding, has the same semantics as SP.\n   A recipient MAY replace any linear white space with a single SP before\n   interpreting the field value or forwarding the message downstream.\n\n       LWS            = [CRLF] 1*( SP | HT )\n\n   The TEXT rule is only used for descriptive field contents and values\n   that are not intended to be interpreted by the message parser. Words of\n   *TEXT MAY contain characters from character sets other than ISO 8859-1\n   [22] only when encoded according to the rules of RFC 2047 [14].\n\n       TEXT           = <any OCTET except CTLs,\n                        but including LWS>\n\n   A CRLF is allowed in the definition of TEXT only as part of a\n   header field continuation.  It is expected that the folding LWS will\n   be replaced with a single SP before interpretation of the TEXT value.\n\nAnd the note at the end of section 2.2:\n\n   Note: CRLF in a quoted string is legal, but only in a strange way:\n   as part of a header continuation, as in\n\n      \"part of\n      a\n      quoted-string\".\n\n   This is strange, and CRLF's ought to be allowed, but backward\n   compatibility constraints mean that they are not allowed in\n   general.\n\nneeds to be deleted because it is wrong.\n\nAlso, for good measure, the following should be added to section 4.2\njust after the BNF definition of field-content.\n\n   The field-content does not include any leading or trailing LWS:\n   linear white space occurring before the first non-whitespace\n   character of the field-value or after the last non-whitespace\n   character of the field-value.  Such leading or trailing LWS\n   MAY be removed without changing the semantics of the field value.\n   Any LWS that occurs between field-content MAY be replaced with\n   a single SP before interpreting the field value or forwarding\n   the message downstream.\n   \n....Roy\n\n\n\n", "id": "lists-012-5783894"}, {"subject": "Re: Clarification requests from Paul Bennett..", "content": "> From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n> Date: Fri, 04 Sep 1998 00:42:53 -0700\n> To: Jim Gettys <jg@pa.dec.com>\n> Cc: mogul@pa.dec.com, paulle@microsoft.com, frystyk@w3.org, dmk@bell-labs.com\n> Subject: Re: Clarification requests from Paul Bennett...\n> -----\nmaterial elided \n> > * Section 10.2.2 \"201 Created\" says nothing about the format of the\n> >   response entity.  (section 10.3.1 \"300 Multiple Choices\" in a\n> >   similar position is explicitly non-committal.\n> >*** Do we want to crib the text from 300? ***\n> \n> 300 is a redirect, which implies the client isn't done yet.  I guess the\n> text from the second para. of 300 is okay without the last two sentences\n> having to do with automatic redirection.\n> \n\nI added:\n\"The response SHOULD include an entity containing a list of resource \ncharacteristics and location(s) from which the user or user agent can \nchoose the one most appropriate. The entity format is specified by the \nmedia type given in the Content-Type header field. \"\n\nTo clarify this issue.\n- Jim\n\n\nattached mail follows:\nIn message <9809031526.AA21910@pachyderm.pa.dec.com>, Jim Gettys writes:\n>My opinions in *** ***\n>Comments, anyone?\n>- Jim\n>\n>Extracted from Paul's mail.\n>Clarifications:\n>(material elided)...\n>\n> * Sections 9.3 \"GET\", 9.4 \"HEAD\" and 9.7 \"DELETE\" don't explicitly\n>   rule out the presence of an entity-body, as section 9.8 \"TRACE\"\n>   does.\n>** at this late date, I'm uncomfortable introducing changes here, and in\n>   fact, it may be useful to allow (though exisiting software may not work).\n>   first, do no harm...***\n\nAgreed.\n\n> * Section 10.2.2 \"201 Created\" says nothing about the format of the\n>   response entity.  (section 10.3.1 \"300 Multiple Choices\" in a\n>   similar position is explicitly non-committal.\n>*** Do we want to crib the text from 300? ***\n\n300 is a redirect, which implies the client isn't done yet.  I guess the\ntext from the second para. of 300 is okay without the last two sentences\nhaving to do with automatic redirection.\n\n> * Section 10.3.6 \"305 Use Proxy\": \"The Location field gives the URI\n>   of the proxy.\"  But nowhere can I find a specification for this\n>   URI form; presumably it's an HTTP URI without an abs_path part.\n>** No, I don't think so; would inhibit evolution of the web to\n>future URI forms; you ought to be able to redirect to other\n>schemes.  I think that 3.2.1 defines well enough the\n>reference to generic URI's (finally draft standard!)***\n\nRight.\n\n....Roy\n\n\n\n", "id": "lists-012-5793720"}, {"subject": "Small typo in rev 0", "content": "In section 19.6.3 there is a typo:\n\nThis specification has now been carefully audited to to correct and\n\nMultiple \"to\" instances.\n\nEd\n\n\nrrect internal reference:\n\n Transfer-Encoding header field (section 14.4114.41).\n\nShould probably be 14.41, right?\n\nEd\n\n\n\n", "id": "lists-012-5804542"}, {"subject": "Referer [sic] Redirect questio", "content": "How should the user agent populate the referrer field when issuing a\nrequest after receiving a 3xx redirect response?  Is the original\nreferer maintained or is it now the URI that resulted in the redirect\nresponse?  There is no discussion of this scenario in the referer or\nredirect sections of the draft.\n\nReason for the question:\nAd Servers sometimes delegate remnant ad impression inventory to\n\"fire-sale\" ad networks using re-direct.  Usually an ad server will use\nthe referer field to determine on what page the ad will be appear.\n\n\n\n", "id": "lists-012-5810879"}, {"subject": "Fwd: draft-ietf-http-v11-spec-rev04 comment", "content": "Some loose ends from previous topics.\n\n\n> From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n> Date: Wed, 09 Sep 1998 00:41:41 -0700\n> To: Jim Gettys <jg@pa.dec.com>\n> Subject: Re: Fwd: draft-ietf-http-v11-spec-rev-04 comments\n> -----\n> It looks like the second-to-last paragraph in section 5.1:\n> \n>    In requests that they forward, transparent proxies MUST NOT rewrite the\n>    \"abs_path\" part of a Request-URI in any way except as noted above to\n>    replace a null abs_path with \"*\", no matter what the proxy does in its\n>    internal implementation.\n> \n> should be replaced by\n> \n>    A transparent proxy MUST NOT rewrite the \"abs_path\" part of the\n>    received Request-URI when forwarding it to the next inbound server,\n>    except as noted above to replace a null abs_path with \"/\".\n> \n\nI've changed all references to inbound to upstream and outbound to downstream\nin the document.\n\n> I checked and all the other leftovers with the \"*\" were removed.\n> \n> On a slightly unrelated note, I just noticed that the frequently used\n> terms inbound, outbound, upstream, and downstream are never defined.\n> Can they be assumed, or should they be added to the terminology section?\n> \n\nGotta stop polishing this apple; I think it is pretty obvious from the\ncontext what upstream and downstream mean.\n\n> Another continuing problem is that the spec only allows the\n> syntax used by the CONNECT method by accident (basically, because\n> the host:port syntax that CONNECT uses for a Request-URI also happens\n> to be a legitimate scheme:scheme-specific absoluteURI).  My suggestion\n> of replacing Request-URI with Request-Target was turned down due to\n> the editing cost.  We could bypass it by adding\n> \n>        Request-URI    = \"*\" | absoluteURI | abs_path | authority\n> \n> with a paragraph saying that the authority form is only used with CONNECT.\n> Speaking of which, the following\n> \n>    9.9 CONNECT\n> \n>    This specification reserves the method name CONNECT for use by SSL\n>    tunneling. [44]\n> \n> should not mention SSL by name.  It should just be \"for use with a\n> proxy that can dynamically switch to being a tunnel.\"  Fortunately,\n> I defined what \"tunnel\" means long before the CONNECT trick came out.\n> \n\nOK. I'll fix.\n\nThanks for the final set of nits.  I'm now starting what I hope to be\nrevision 5 production.\n\n- Jim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFielding, et al                                                 [Page 2]\n                           Table of Contents\n\n\n\n   HYPERTEXT TRANSFER PROTOCOL -- HTTP/1.1.............................1\n\n   Status of this Memo.................................................1\n\n   Abstract............................................................1\n\n   Copyright Notice....................................................2\n\n   Table of Contents...................................................3\n\n   1    Introduction ..................................................8\n    1.1   Purpose......................................................8\n    1.2   Requirements.................................................9\n    1.3   Terminology..................................................9\n    1.4   Overall Operation...........................................12\n\n   2    Notational Conventions and Generic Grammar ...................14\n    2.1   Augmented BNF...............................................14\n    2.2   Basic Rules.................................................15\n\n   3    Protocol Parameters ..........................................17\n    3.1     HTTP Version .............................................17\n    3.2     Uniform Resource Identifiers .............................18\n     3.2.1     General Syntax.........................................18\n     3.2.2     http URL...............................................19\n     3.2.3     URI Comparison.........................................19\n    3.3     Date/Time Formats ........................................20\n     3.3.1     Full Date..............................................20\n     3.3.2     Delta Seconds..........................................21\n    3.4     Character Sets ...........................................21\n     3.4.1     Missing Charset........................................22\n    3.5     Content Codings ..........................................22\n    3.6     Transfer Codings .........................................23\n     3.6.1     Chunked Transfer Coding................................24\n    3.7     Media Types ..............................................25\n     3.7.1     Canonicalization and Text Defaults.....................26\n     3.7.2     Multipart Types........................................27\n    3.8     Product Tokens ...........................................27\n    3.9     Quality Values ...........................................28\n    3.10    Language Tags ............................................28\n    3.11    Entity Tags ..............................................29\n    3.12    Range Units ..............................................29\n\n   4    HTTP Message .................................................30\n    4.1     Message Types ............................................30\n    4.2     Message Headers ..........................................30\n    4.3     Message Body .............................................31\n    4.4     Message Length ...........................................32\n    4.5     General Header Fields ....................................33\n\n   5    Request ......................................................34\n    5.1     Request-Line .............................................34\n     5.1.1     Method.................................................34\n\nFielding, et al                                                 [Page 3]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     5.1.2     Request-URI............................................35\n    5.2     The Resource Identified by a Request .....................36\n    5.3     Request Header Fields ....................................36\n\n   6    Response .....................................................37\n    6.1     Status-Line ..............................................37\n     6.1.1     Status Code and Reason Phrase..........................37\n    6.2     Response Header Fields ...................................39\n\n   7    Entity .......................................................40\n    7.1     Entity Header Fields .....................................40\n    7.2     Entity Body ..............................................40\n     7.2.1     Type...................................................41\n     7.2.2     Entity Length..........................................41\n\n   8    Connections ..................................................41\n    8.1     Persistent Connections ...................................41\n     8.1.1     Purpose................................................41\n     8.1.2     Overall Operation......................................42\n     8.1.3     Proxy Servers..........................................43\n     8.1.4     Practical Considerations...............................43\n    8.2     Message Transmission Requirements ........................44\n     8.2.1     Persistent Connections and Flow Control................44\n     8.2.2     Monitoring Connections for Error Status Messages.......45\n     8.2.3     Automatic Retrying of Requests.........................45\n     8.2.4     Use of the 100 (Continue) Status.......................45\n     8.2.5     Client Behavior if Server Prematurely Closes Connection47\n\n   9    Method Definitions ...........................................48\n    9.1     Safe and Idempotent Methods ..............................48\n     9.1.1     Safe Methods...........................................48\n     9.1.2     Idempotent Methods.....................................48\n    9.2     OPTIONS ..................................................49\n    9.3     GET ......................................................50\n    9.4     HEAD .....................................................50\n    9.5     POST .....................................................50\n    9.6     PUT ......................................................51\n    9.7     DELETE ...................................................52\n    9.8     TRACE ....................................................53\n    9.9     CONNECT ..................................................53\n\n   10   Status Code Definitions ......................................53\n    10.1    Informational 1xx ........................................53\n     10.1.1    100 Continue...........................................54\n     10.1.2    101 Switching Protocols................................54\n    10.2    Successful 2xx ...........................................54\n     10.2.1    200 OK.................................................54\n     10.2.2    201 Created............................................55\n     10.2.3    202 Accepted...........................................55\n     10.2.4    203 Non-Authoritative Information......................55\n     10.2.5    204 No Content.........................................56\n     10.2.6    205 Reset Content......................................56\n     10.2.7    206 Partial Content....................................56\n    10.3    Redirection 3xx ..........................................57\n     10.3.1    300 Multiple Choices...................................57\n\nFielding, et al                                                 [Page 4]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     10.3.2    301 Moved Permanently..................................58\n     10.3.3    302 Found..............................................58\n     10.3.4    303 See Other..........................................59\n     10.3.5    304 Not Modified.......................................59\n     10.3.6    305 Use Proxy..........................................60\n     10.3.7    306 (Unused)...........................................60\n     10.3.8    307 Temporary Redirect.................................60\n    10.4    Client Error 4xx .........................................60\n     10.4.1    400 Bad Request........................................61\n     10.4.2    401 Unauthorized.......................................61\n     10.4.3    402 Payment Required...................................61\n     10.4.4    403 Forbidden..........................................61\n     10.4.5    404 Not Found..........................................61\n     10.4.6    405 Method Not Allowed.................................62\n     10.4.7    406 Not Acceptable.....................................62\n     10.4.8    407 Proxy Authentication Required......................62\n     10.4.9    408 Request Timeout....................................63\n     10.4.10   409 Conflict...........................................63\n     10.4.11   410 Gone...............................................63\n     10.4.12   411 Length Required....................................63\n     10.4.13   412 Precondition Failed................................64\n     10.4.14   413 Request Entity Too Large...........................64\n     10.4.15   414 Request-URI Too Long...............................64\n     10.4.16   415 Unsupported Media Type.............................64\n     10.4.17   416 Requested Range Not Satisfiable....................64\n     10.4.18   417 Expectation Failed.................................65\n    10.5    Server Error 5xx .........................................65\n     10.5.1    500 Internal Server Error..............................65\n     10.5.2    501 Not Implemented....................................65\n     10.5.3    502 Bad Gateway........................................65\n     10.5.4    503 Service Unavailable................................65\n     10.5.5    504 Gateway Timeout....................................66\n     10.5.6    505 HTTP Version Not Supported.........................66\n\n   11   Access Authentication ........................................66\n\n   12   Content Negotiation ..........................................66\n    12.1    Server-driven Negotiation ................................67\n    12.2    Agent-driven Negotiation .................................68\n    12.3    Transparent Negotiation ..................................68\n\n   13   Caching in HTTP ..............................................69\n     13.1.1    Cache Correctness......................................70\n     13.1.2    Warnings...............................................71\n     13.1.3    Cache-control Mechanisms...............................72\n     13.1.4    Explicit User Agent Warnings...........................72\n     13.1.5    Exceptions to the Rules and Warnings...................72\n     13.1.6    Client-controlled Behavior.............................73\n    13.2    Expiration Model .........................................73\n     13.2.1    Server-Specified Expiration............................73\n     13.2.2    Heuristic Expiration...................................74\n     13.2.3    Age Calculations.......................................74\n     13.2.4    Expiration Calculations................................76\n     13.2.5    Disambiguating Expiration Values.......................77\n     13.2.6    Disambiguating Multiple Responses......................77\n\nFielding, et al                                                 [Page 5]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n    13.3    Validation Model .........................................78\n     13.3.1    Last-modified Dates....................................79\n     13.3.2    Entity Tag Cache Validators............................79\n     13.3.3    Weak and Strong Validators.............................79\n     13.3.4    When to Use Entity Tags and Last-modified Dates........81\n     13.3.5    Non-validating Conditionals............................83\n    13.4    Response Cachability .....................................83\n    13.5    Constructing Responses From Caches .......................84\n     13.5.1    End-to-end and Hop-by-hop Headers......................84\n     13.5.2    Non-modifiable Headers.................................85\n     13.5.3    Combining Headers......................................85\n     13.5.4    Combining Byte Ranges..................................86\n    13.6    Caching Negotiated Responses .............................87\n    13.7    Shared and Non-Shared Caches .............................88\n    13.8    Errors or Incomplete Response Cache Behavior .............88\n    13.9    Side Effects of GET and HEAD .............................89\n    13.10   Invalidation After Updates or Deletions ..................89\n    13.11   Write-Through Mandatory ..................................90\n    13.12   Cache Replacement ........................................90\n    13.13   History Lists ............................................90\n\n   14     Header Field Definitions....................................91\n    14.1    Accept ...................................................91\n    14.2    Accept-Charset ...........................................93\n    14.3    Accept-Encoding ..........................................93\n    14.4    Accept-Language ..........................................95\n    14.5    Accept-Ranges ............................................96\n    14.6    Age ......................................................96\n    14.7    Allow ....................................................97\n    14.8    Authorization ............................................97\n    14.9    Cache-Control ............................................98\n     14.9.1    What is Cachable.......................................99\n     14.9.2    What May be Stored by Caches..........................100\n     14.9.3    Modifications of the Basic Expiration Mechanism.......101\n     14.9.4    Cache Revalidation and Reload Controls................102\n     14.9.5    No-Transform Directive................................105\n     14.9.6    Cache Control Extensions..............................105\n    14.10   Connection ..............................................106\n    14.11   Content-Encoding ........................................107\n    14.12   Content-Language ........................................107\n    14.13   Content-Length ..........................................108\n    14.14   Content-Location ........................................109\n    14.15   Content-MD5 .............................................109\n    14.16   Content-Range ...........................................111\n    14.17   Content-Type ............................................112\n    14.18   Date ....................................................113\n     14.18.1   Clockless Origin Server Operation.....................114\n    14.19   ETag ....................................................114\n    14.20   Expect ..................................................114\n     14.20.1   Expect 100-continue...................................115\n    14.21   Expires .................................................115\n    14.22   From ....................................................116\n    14.23   Host ....................................................117\n    14.24   If-Match ................................................118\n    14.25   If-Modified-Since .......................................119\n\nFielding, et al                                                 [Page 6]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n    14.26   If-None-Match ...........................................120\n    14.27   If-Range ................................................121\n    14.28   If-Unmodified-Since .....................................122\n    14.29   Last-Modified ...........................................122\n    14.30   Location ................................................123\n    14.31   Max-Forwards ............................................123\n    14.32   Pragma ..................................................124\n    14.33   Proxy-Authenticate ......................................124\n    14.34   Proxy-Authorization .....................................125\n    14.35   Range ...................................................125\n     14.35.1   Byte Ranges...........................................125\n     14.35.2   Range Retrieval Requests..............................127\n    14.36   Referer .................................................127\n    14.37   Retry-After .............................................128\n    14.38   Server ..................................................128\n    14.39   TE ......................................................129\n    14.40   Trailer .................................................130\n    14.41   Transfer-Encoding .......................................130\n    14.42   Upgrade .................................................131\n    14.43   User-Agent ..............................................132\n    14.44   Vary ....................................................132\n    14.45   Via .....................................................133\n    14.46   Warning .................................................134\n    14.47   WWW-Authenticate ........................................137\n\n   15     Security Considerations....................................137\n    15.1    Personal Information ....................................137\n     15.1.1    Abuse of Server Log Information.......................138\n     15.1.2    Transfer of Sensitive Information.....................138\n     15.1.3    Encoding Sensitive Information in URI's...............139\n     15.1.4    Privacy Issues Connected to Accept Headers............139\n    15.2    Attacks Based On File and Path Names ....................140\n    15.3    DNS Spoofing ............................................140\n    15.4    Location Headers and Spoofing ...........................141\n    15.5    Content-Disposition Issues ..............................141\n    15.6    Authentication Credentials and Idle Clients .............141\n    15.7    Proxies and Caching .....................................141\n     15.7.1    Denial of Service Attacks on Proxies..................142\n\n   16     Acknowledgments............................................142\n\n   17     References.................................................144\n\n   18     Authors' Addresses.........................................148\n\n   19     Appendices.................................................149\n    19.1    Internet Media Type message/http and application/http ...149\n    19.2    Internet Media Type multipart/byteranges ................149\n    19.3    Tolerant Applications ...................................151\n    19.4    Differences Between HTTP Entities and RFC 2045 Entities .151\n     19.4.1    MIME-Version..........................................152\n     19.4.2    Conversion to Canonical Form..........................152\n     19.4.3    Conversion of Date Formats............................152\n     19.4.4    Introduction of Content-Encoding......................153\n     19.4.5    No Content-Transfer-Encoding..........................153\n\nFielding, et al                                                 [Page 7]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     19.4.6    Introduction of Transfer-Encoding.....................153\n     19.4.7    MHTML and Line Length Limitations.....................154\n    19.5    Additional Features .....................................154\n     19.5.1    Content-Disposition...................................154\n    19.6    Compatibility with Previous Versions ....................155\n     19.6.1    Changes from HTTP/1.0.................................155\n     19.6.2    Compatibility with HTTP/1.0 Persistent Connections....156\n     19.6.3    Changes from RFC 2068.................................157\n    19.7    Notes to the RFC Editor and IANA ........................159\n     19.7.1    Transfer-coding Values................................159\n     19.7.2    Definition of application/http........................159\n     19.7.3    Addition of \"identity\" to content-coding Registry.....159\n\n   20      Full Copyright Statement .................................160\n\n   21      Index ....................................................161\n\n\n\n1 Introduction\n\n\n1.1 Purpose\n\n   The Hypertext Transfer Protocol (HTTP) is an application-level\n   protocol for distributed, collaborative, hypermedia information\n   systems. HTTP has been in use by the World-Wide Web global\n   information initiative since 1990. The first version of HTTP,\n   referred to as HTTP/0.9, was a simple protocol for raw data transfer\n   across the Internet. HTTP/1.0, as defined by RFC 1945 [6], improved\n   the protocol by allowing messages to be in the format of MIME-like\n   messages, containing metainformation about the data transferred and\n   modifiers on the request/response semantics. However, HTTP/1.0 does\n   not sufficiently take into consideration the effects of hierarchical\n   proxies, caching, the need for persistent connections, or virtual\n   hosts. In addition, the proliferation of incompletely-implemented\n   applications calling themselves \"HTTP/1.0\" has necessitated a\n   protocol version change in order for two communicating applications\n   to determine each other's true capabilities.\n\n   This specification defines the protocol referred to as \"HTTP/1.1\".\n   This protocol includes more stringent requirements than HTTP/1.0 in\n   order to ensure reliable implementation of its features.\n\n   Practical information systems require more functionality than simple\n   retrieval, including search, front-end update, and annotation. HTTP\n   allows an open-ended set of methods and headers that indicate the\n   purpose of a request [47]. It builds on the discipline of reference\n   provided by the Uniform Resource Identifier (URI) [3], as a location\n   (URL) [4] or name (URN) [20], for indicating the resource to which a\n   method is to be applied. Messages are passed in a format similar to\n   that used by Internet mail [9] as defined by the Multipurpose\n   Internet Mail Extensions (MIME) [7].\n\n\n\nFielding, et al                                                 [Page 8]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   HTTP is also used as a generic protocol for communication between\n   user agents and proxies/gateways to other Internet systems, including\n   those supported by the SMTP [16], NNTP [13], FTP [18], Gopher [2],\n   and WAIS [10] protocols. In this way, HTTP allows basic hypermedia\n   access to resources available from diverse applications.\n\n\n1.2 Requirements\n\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described in RFC 2119 [34].\n\n   An implementation is not compliant if it fails to satisfy one or more\n   of the MUST level requirements for the protocols it implements. An\n   implementation that satisfies all the MUST level and all the SHOULD\n   level requirements for its protocols is said to be \"unconditionally\n   compliant\"; one that satisfies all the MUST level requirements but\n   not all the SHOULD level requirements for its protocols is said to be\n   \"conditionally compliant.\"\n\n\n1.3 Terminology\n\n   This specification uses a number of terms to refer to the roles\n   played by participants in, and objects of, the HTTP communication.\n\n   connection\n     A transport layer virtual circuit established between two programs\n     for the purpose of communication.\n\n   message\n     The basic unit of HTTP communication, consisting of a structured\n     sequence of octets matching the syntax defined in section 4 and\n     transmitted via the connection.\n\n   request\n     An HTTP request message, as defined in section 5.\n\n   response\n     An HTTP response message, as defined in section 6.\n\n   resource\n     A network data object or service that can be identified by a URI,\n     as defined in section 3.2. Resources may be available in multiple\n     representations (e.g. multiple languages, data formats, size, and\n     resolutions) or vary in other ways.\n\n   entity\n     The information transferred as the payload of a request or\n     response. An entity consists of metainformation in the form of\n     entity-header fields and content in the form of an entity-body, as\n     described in section 7.\n\n\n\nFielding, et al                                                 [Page 9]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   representation\n     An entity included with a response that is subject to content\n     negotiation, as described in section 12. There may exist multiple\n     representations associated with a particular response status.\n\n   content negotiation\n     The mechanism for selecting the appropriate representation when\n     servicing a request, as described in section 12. The representation\n     of entities in any response can be negotiated (including error\n     responses).\n\n   variant\n     A resource may have one, or more than one, representation(s)\n     associated with it at any given instant. Each of these\n     representations is termed a `variant.' Use of the term `variant'\n     does not necessarily imply that the resource is subject to content\n     negotiation.\n\n   client\n     A program that establishes connections for the purpose of sending\n     requests.\n\n   user agent\n     The client which initiates a request. These are often browsers,\n     editors, spiders (web-traversing robots), or other end user tools.\n\n   server\n     An application program that accepts connections in order to service\n     requests by sending back responses. Any given program may be\n     capable of being both a client and a server; our use of these terms\n     refers only to the role being performed by the program for a\n     particular connection, rather than to the program's capabilities in\n     general. Likewise, any server may act as an origin server, proxy,\n     gateway, or tunnel, switching behavior based on the nature of each\n     request.\n\n   origin server\n     The server on which a given resource resides or is to be created.\n\n   proxy\n     An intermediary program which acts as both a server and a client\n     for the purpose of making requests on behalf of other clients.\n     Requests are serviced internally or by passing them on, with\n     possible translation, to other servers. A proxy MUST implement both\n     the client and server requirements of this specification. A\n     \"transparent proxy\" is a proxy that does not modify the request or\n     response beyond what is required for proxy authentication and\n     identification. A \"non-transparent proxy\" is a proxy that modifies\n     the request or response in order to provide some added service to\n     the user agent, such as group annotation services, media type\n     transformation, protocol reduction, or anonymity filtering. Except\n     where either transparent or non-transparent behavior is explicitly\n     stated, the HTTP proxy requirements apply to both types of proxies.\n\n\n\nFielding, et al                                                [Page 10]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   gateway\n     A server which acts as an intermediary for some other server.\n     Unlike a proxy, a gateway receives requests as if it were the\n     origin server for the requested resource; the requesting client may\n     not be aware that it is communicating with a gateway.\n\n   tunnel\n     An intermediary program which is acting as a blind relay between\n     two connections. Once active, a tunnel is not considered a party to\n     the HTTP communication, though the tunnel may have been initiated\n     by an HTTP request. The tunnel ceases to exist when both ends of\n     the relayed connections are closed.\n\n   cache\n     A program's local store of response messages and the subsystem that\n     controls its message storage, retrieval, and deletion. A cache\n     stores cachable responses in order to reduce the response time and\n     network bandwidth consumption on future, equivalent requests. Any\n     client or server may include a cache, though a cache cannot be used\n     by a server that is acting as a tunnel.\n\n   cachable\n     A response is cachable if a cache is allowed to store a copy of the\n     response message for use in answering subsequent requests. The\n     rules for determining the cachability of HTTP responses are defined\n     in section 13. Even if a resource is cachable, there may be\n     additional constraints on whether a cache can use the cached copy\n     for a particular request.\n\n   first-hand\n     A response is first-hand if it comes directly and without\n     unnecessary delay from the origin server, perhaps via one or more\n     proxies. A response is also first-hand if its validity has just\n     been checked directly with the origin server.\n\n   explicit expiration time\n     The time at which the origin server intends that an entity should\n     no longer be returned by a cache without further validation.\n\n   heuristic expiration time\n     An expiration time assigned by a cache when no explicit expiration\n     time is available.\n\n   age\n     The age of a response is the time since it was sent by, or\n     successfully validated with, the origin server.\n\n   freshness lifetime\n     The length of time between the generation of a response and its\n     expiration time.\n\n   fresh\n     A response is fresh if its age has not yet exceeded its freshness\n     lifetime.\n\n\nFielding, et al                                                [Page 11]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   stale\n     A response is stale if its age has passed its freshness lifetime.\n\n   semantically transparent\n     A cache behaves in a \"semantically transparent\" manner, with\n     respect to a particular response, when its use affects neither the\n     requesting client nor the origin server, except to improve\n     performance. When a cache is semantically transparent, the client\n     receives exactly the same response (except for hop-by-hop headers)\n     that it would have received had its request been handled directly\n     by the origin server.\n\n   validator\n     A protocol element (e.g., an entity tag or a Last-Modified time)\n     that is used to find out whether a cache entry is an equivalent\n     copy of an entity.\n\n   upstream/downstream\n     Upstream and downstream describe the flow of a message: all\n     messages flow from upstream to downstream.\n\n   inbound/outbound\n     Inbound and outbound refer to the request and response paths for\n     messages: \"inbound\" means \"travelling toward the origin server\",\n     and \"outbound\" means \"travelling toward the user agent.\"\n\n\n1.4 Overall Operation\n\n   The HTTP protocol is a request/response protocol. A client sends a\n   request to the server in the form of a request method, URI, and\n   protocol version, followed by a MIME-like message containing request\n   modifiers, client information, and possible body content over a\n   connection with a server. The server responds with a status line,\n   including the message's protocol version and a success or error code,\n   followed by a MIME-like message containing server information, entity\n   metainformation, and possible entity-body content. The relationship\n   between HTTP and MIME is described in appendix 19.4.\n\n   Most HTTP communication is initiated by a user agent and consists of\n   a request to be applied to a resource on some origin server. In the\n   simplest case, this may be accomplished via a single connection (v)\n   between the user agent (UA) and the origin server (O).\n\n             request chain ------------------------>\n          UA -------------------v------------------- O\n             <----------------------- response chain\n\n   A more complicated situation occurs when one or more intermediaries\n   are present in the request/response chain. There are three common\n   forms of intermediary: proxy, gateway, and tunnel. A proxy is a\n   forwarding agent, receiving requests for a URI in its absolute form,\n   rewriting all or part of the message, and forwarding the reformatted\n   request toward the server identified by the URI. A gateway is a\n   receiving agent, acting as a layer above some other server(s) and, if\n\nFielding, et al                                                [Page 12]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   necessary, translating the requests to the underlying server's\n   protocol. A tunnel acts as a relay point between two connections\n   without changing the messages; tunnels are used when the\n   communication needs to pass through an intermediary (such as a\n   firewall) even when the intermediary cannot understand the contents\n   of the messages.\n\n             request chain -------------------------------------->\n          UA -----v----- A -----v----- B -----v----- C -----v----- O\n             <------------------------------------- response chain\n\n   The figure above shows three intermediaries (A, B, and C) between the\n   user agent and origin server. A request or response message that\n   travels the whole chain will pass through four separate connections.\n   This distinction is important because some HTTP communication options\n   may apply only to the connection with the nearest, non-tunnel\n   neighbor, only to the end-points of the chain, or to all connections\n   along the chain. Although the diagram is linear, each participant may\n   be engaged in multiple, simultaneous communications. For example, B\n   may be receiving requests from many clients other than A, and/or\n   forwarding requests to servers other than C, at the same time that it\n   is handling A's request.\n\n   Any party to the communication which is not acting as a tunnel may\n   employ an internal cache for handling requests. The effect of a cache\n   is that the request/response chain is shortened if one of the\n   participants along the chain has a cached response applicable to that\n   request. The following illustrates the resulting chain if B has a\n   cached copy of an earlier response from O (via C) for a request which\n   has not been cached by UA or A.\n\n             request chain ---------->\n          UA -----v----- A -----v----- B - - - - - - C - - - - - - O\n             <--------- response chain\n\n   Not all responses are usefully cachable, and some requests may\n   contain modifiers which place special requirements on cache behavior.\n   HTTP requirements for cache behavior and cachable responses are\n   defined in section 13.\n\n   In fact, there are a wide variety of architectures and configurations\n   of caches and proxies currently being experimented with or deployed\n   across the World Wide Web. These systems include national hierarchies\n   of proxy caches to save transoceanic bandwidth, systems that\n   broadcast or multicast cache entries, organizations that distribute\n   subsets of cached data via CD-ROM, and so on. HTTP systems are used\n   in corporate intranets over high-bandwidth links, and for access via\n   PDAs with low-power radio links and intermittent connectivity. The\n   goal of HTTP/1.1 is to support the wide diversity of configurations\n   already deployed while introducing protocol constructs that meet the\n   needs of those who build web applications that require high\n   reliability and, failing that, at least reliable indications of\n   failure.\n\n\n\nFielding, et al                                                [Page 13]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   HTTP communication usually takes place over TCP/IP connections. The\n   default port is TCP 80 [19], but other ports can be used. This does\n   not preclude HTTP from being implemented on top of any other protocol\n   on the Internet, or on other networks. HTTP only presumes a reliable\n   transport; any protocol that provides such guarantees can be used;\n   the mapping of the HTTP/1.1 request and response structures onto the\n   transport data units of the protocol in question is outside the scope\n   of this specification.\n\n   In HTTP/1.0, most implementations used a new connection for each\n   request/response exchange. In HTTP/1.1, a connection may be used for\n   one or more request/response exchanges, although connections may be\n   closed for a variety of reasons (see section 8.1).\n\n\n2 Notational Conventions and Generic Grammar\n\n\n2.1 Augmented BNF\n\n   All of the mechanisms specified in this document are described in\n   both prose and an augmented Backus-Naur Form (BNF) similar to that\n   used by RFC 822 [9]. Implementors will need to be familiar with the\n   notation in order to understand this specification. The augmented BNF\n   includes the following constructs:\n\n  name = definition\n     The name of a rule is simply the name itself (without any enclosing\n     \"<\" and \">\") and is separated from its definition by the equal \"=\"\n     character. White space is only significant in that indentation of\n     continuation lines is used to indicate a rule definition that spans\n     more than one line. Certain basic rules are in uppercase, such as\n     SP, LWS, HT, CRLF, DIGIT, ALPHA, etc. Angle brackets are used\n     within definitions whenever their presence will facilitate\n     discerning the use of rule names.\n\n  \"literal\"\n     Quotation marks surround literal text. Unless stated otherwise, the\n     text is case-insensitive.\n\n  rule1 | rule2\n     Elements separated by a bar (\"|\") are alternatives, e.g., \"yes |\n     no\" will accept yes or no.\n\n  (rule1 rule2)\n     Elements enclosed in parentheses are treated as a single element.\n     Thus, \"(elem (foo | bar) elem)\" allows the token sequences\n     \"elem foo elem\" and \"elem bar elem\".\n\n  *rule\n     The character \"*\" preceding an element indicates repetition. The\n     full form is \"<n>*<m>element\" indicating at least <n> and at most\n     <m> occurrences of element. Default values are 0 and infinity so\n     that \"*(element)\" allows any number, including zero; \"1*element\"\n     requires at least one; and \"1*2element\" allows one or two.\n\nFielding, et al                                                [Page 14]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n  [rule]\n     Square brackets enclose optional elements; \"[foo bar]\" is\n     equivalent to \"*1(foo bar)\".\n\n  N rule\n     Specific repetition: \"<n>(element)\" is equivalent to\n     \"<n>*<n>(element)\"; that is, exactly <n> occurrences of (element).\n     Thus 2DIGIT is a 2-digit number, and 3ALPHA is a string of three\n     alphabetic characters.\n\n  #rule\n     A construct \"#\" is defined, similar to \"*\", for defining lists of\n     elements. The full form is \"<n>#<m>element\" indicating at least <n>\n     and at most <m> elements, each separated by one or more commas\n     (\",\") and OPTIONAL linear white space (LWS). This makes the usual\n     form of lists very easy; a rule such as \n        ( *LWS element *( *LWS \",\" *LWS element ))\n     can be shown as\n        1#element\n     Wherever this construct is used, null elements are allowed, but do\n     not contribute to the count of elements present. That is,\n     \"(element), , (element) \" is permitted, but counts as only two\n     elements. Therefore, where at least one element is required, at\n     least one non-null element MUST be present. Default values are 0\n     and infinity so that \"#element\" allows any number, including zero;\n     \"1#element\" requires at least one; and \"1#2element\" allows one or\n     two.\n\n  ; comment\n     A semi-colon, set off some distance to the right of rule text,\n     starts a comment that continues to the end of line. This is a\n     simple way of including useful notes in parallel with the\n     specifications.\n\n  implied *LWS\n     The grammar described by this specification is word-based. Except\n     where noted otherwise, linear white space (LWS) can be included\n     between any two adjacent words (token or quoted-string), and\n     between adjacent tokens and separators, without changing the\n     interpretation of a field. At least one delimiter (LWS and/or\n     separators[jg13]) MUST exist between any two tokens (for the\n     definition of \"token\" below), since they would otherwise be\n     interpreted as a single token.\n\n\n2.2 Basic Rules\n\n   The following rules are used throughout this specification to\n   describe basic parsing constructs. The US-ASCII coded character set\n   is defined by ANSI X3.4-1986 [21].\n\n          OCTET          = <any 8-bit sequence of data>\n\n          CHAR           = <any US-ASCII character (octets 0 - 127)>\n\n\nFielding, et al                                                [Page 15]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n          UPALPHA        = <any US-ASCII uppercase letter \"A\"..\"Z\">\n\n          LOALPHA        = <any US-ASCII lowercase letter \"a\"..\"z\">\n\n          ALPHA          = UPALPHA | LOALPHA\n\n          DIGIT          = <any US-ASCII digit \"0\"..\"9\">\n\n          CTL            = <any US-ASCII control character\n                           (octets 0 - 31) and DEL (127)>\n\n          CR             = <US-ASCII CR, carriage return (13)>\n\n          LF             = <US-ASCII LF, linefeed (10)>\n\n          SP             = <US-ASCII SP, space (32)>\n\n          HT             = <US-ASCII HT, horizontal-tab (9)>\n\n          <\">            = <US-ASCII double-quote mark (34)>\n\n   HTTP/1.1 defines the sequence CR LF as the end-of-line marker for all\n   protocol elements except the entity-body (see appendix 19.3 for\n   tolerant applications). The end-of-line marker within an entity-body\n   is defined by its associated media type, as described in section 3.7.\n\n          CRLF           = CR LF\n\n   HTTP/1.1 header field values can be folded onto multiple lines if the\n   continuation line begins with a space or horizontal tab. All linear\n   white space, including folding, has the same semantics as SP. A\n   recipient MAY replace any linear white space with a single SP before\n   interpreting the field value or forwarding the message downstream.\n\n          LWS            = [CRLF] 1*( SP | HT )\n\n   The TEXT rule is only used for descriptive field contents and values\n   that are not intended to be interpreted by the message parser. Words\n   of *TEXT MAY contain characters from character sets other than ISO-\n   8859-1 [22] only when encoded according to the rules of RFC 2047\n   [14].\n\n          TEXT           = <any OCTET except CTLs,\n                           but including LWS>\n\n   A CRLF is allowed in the definition of TEXT only as part of a header\n   field continuation. It is expected that the folding LWS will be\n   replaced with a single SP before interpretation of the TEXT value.\n\n   Hexadecimal numeric characters are used in several protocol elements.\n\n          HEX            = \"A\" | \"B\" | \"C\" | \"D\" | \"E\" | \"F\"\n                         | \"a\" | \"b\" | \"c\" | \"d\" | \"e\" | \"f\" | DIGIT\n\n\n\nFielding, et al                                                [Page 16]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Many HTTP/1.1 header field values consist of words separated by LWS\n   or special characters. These special characters MUST be in a quoted\n   string to be used within a parameter value.\n\n          token          = 1*<any CHAR except CTLs or separators>\n\n          separators     = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n                         | \",\" | \";\" | \":\" | \"\\\" | <\">\n                         | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n                         | \"{\" | \"}\" | SP | HT\n\n   Comments can be included in some HTTP header fields by surrounding\n   the comment text with parentheses. Comments are only allowed in\n   fields containing \"comment\" as part of their field value definition.\n   In all other fields, parentheses are considered part of the field\n   value.\n\n          comment        = \"(\" *( ctext | quoted-pair | comment ) \")\"\n\n          ctext          = <any TEXT excluding \"(\" and \")\">\n\n   A string of text is parsed as a single word if it is quoted using\n   double-quote marks.\n\n          quoted-string  = ( <\"> *(qdtext | quoted-pair ) <\"> )\n\n          qdtext         = <any TEXT except <\">>\n\n   The backslash character (\"\\\") MAY be used as a single-character\n   quoting mechanism only within quoted-string and comment constructs.\n\n          quoted-pair    = \"\\\" CHAR\n\n\n3  Protocol Parameters\n\n\n3.1 HTTP Version\n\n   HTTP uses a \"<major>.<minor>\" numbering scheme to indicate versions\n   of the protocol. The protocol versioning policy is intended to allow\n   the sender to indicate the format of a message and its capacity for\n   understanding further HTTP communication, rather than the features\n   obtained via that communication. No change is made to the version\n   number for the addition of message components which do not affect\n   communication behavior or which only add to extensible field values.\n   The <minor> number is incremented when the changes made to the\n   protocol add features which do not change the general message parsing\n   algorithm, but which may add to the message semantics and imply\n   additional capabilities of the sender. The <major> number is\n   incremented when the format of a message within the protocol is\n   changed. See RFC 2145 [36] for a fuller explanation.\n\n   The version of an HTTP message is indicated by an HTTP-Version field\n   in the first line of the message.\n\nFielding, et al                                                [Page 17]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n          HTTP-Version   = \"HTTP\" \"/\" 1*DIGIT \".\" 1*DIGIT\n\n   Note that the major and minor numbers MUST be treated as separate\n   integers and that each MAY be incremented higher than a single digit.\n   Thus, HTTP/2.4 is a lower version than HTTP/2.13, which in turn is\n   lower than HTTP/12.3. Leading zeros MUST be ignored by recipients and\n   MUST NOT be sent.\n\n   An application that sends a request or response message that includes\n   HTTP-Version of \"HTTP/1.1\" MUST be at least conditionally compliant\n   with this specification. Applications that are at least conditionally\n   compliant with this specification SHOULD use an HTTP-Version of\n   \"HTTP/1.1\" in their messages, and MUST do so for any message that is\n   not compatible with HTTP/1.0. For more details on when to send\n   specific HTTP-Version values, see RFC 2145 [36].\n\n   The HTTP version of an application is the highest HTTP version for\n   which the application is at least conditionally compliant.\n\n   Proxy and gateway applications need to be careful when forwarding\n   messages in protocol versions different from that of the application.\n   Since the protocol version indicates the protocol capability of the\n   sender, a proxy/gateway MUST NOT send a message with a version\n   indicator which is greater than its actual version. If a higher\n   version request is received, the proxy/gateway MUST either downgrade\n   the request version, or respond with an error, or switch to tunnel\n   behavior.\n\n   Due to interoperability problems with HTTP/1.0 proxies discovered\n   since the publication of RFC 2068[33], caching proxies MUST, gateways\n   MAY, and tunnels MUST NOT upgrade the request to the highest version\n   they support. The proxy/gateway's response to that request MUST be in\n   the same major version as the request.\n\n     Note: Converting between versions of HTTP may involve\n     modification of header fields required or forbidden by the\n     versions involved.\n\n\n3.2 Uniform Resource Identifiers\n\n   URIs have been known by many names: WWW addresses, Universal Document\n   Identifiers, Universal Resource Identifiers [3], and finally the\n   combination of Uniform Resource Locators (URL) [4] and Names (URN)\n   [20]. As far as HTTP is concerned, Uniform Resource Identifiers are\n   simply formatted strings which identify--via name, location, or any\n   other characteristic--a resource.\n\n\n3.2.1 General Syntax\n\n   URIs in HTTP can be represented in absolute form or relative to some\n   known base URI [11], depending upon the context of their use. The two\n   forms are differentiated by the fact that absolute URIs always begin\n   with a scheme name followed by a colon. For definitive information on\n\nFielding, et al                                                [Page 18]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   URL syntax and semantics, see \"Uniform Resource Identifiers (URI):\n   Generic Syntax and Semantics,\" RFC 2396 [42] (which replaces RFCs\n   1738 [4] and RFC 1808 [11]). This specification adopts the\n   definitions of \"URI-reference\", \"absoluteURI\", \"relativeURI\", \"port\",\n   \"host\",\"abs_path\", \"rel_path\", and \"authority\" from that\n   specification.\n\n   The HTTP protocol does not place any a priori limit on the length of\n   a URI. Servers MUST be able to handle the URI of any resource they\n   serve, and SHOULD be able to handle URIs of unbounded length if they\n   provide GET-based forms that could generate such URIs. A server\n   SHOULD return 414 (Request-URI Too Long) status if a URI is longer\n   than the server can handle (see section 10.4.15).\n\n     Note: Servers ought to be cautious about depending on URI\n     lengths above 255 bytes, because some older client or proxy\n     implementations might not properly support these lengths.\n\n\n3.2.2 http URL\n\n   The \"http\" scheme is used to locate network resources via the HTTP\n   protocol. This section defines the scheme-specific syntax and\n   semantics for http URLs.\n\n          http_URL       = \"http:\" \"//\" host [ \":\" port ] [ abs_path ]\n\n   If the port is empty or not given, port 80 is assumed. The semantics\n   are that the identified resource is located at the server listening\n   for TCP connections on that port of that host, and the Request-URI\n   for the resource is abs_path (section 5.1.2). The use of IP addresses\n   in URLs SHOULD be avoided whenever possible (see RFC 1900 [24]). If\n   the abs_path is not present in the URL, it MUST be given as \"/\" when\n   used as a Request-URI for a resource (section 5.1.2). If a proxy\n   receives a host name which is not a fully qualified domain name, it\n   MAY add its domain to the host name it received. If a proxy receives\n   a fully qualified domain name, the proxy MUST NOT change the host\n   name.\n\n\n3.2.3 URI Comparison\n\n   When comparing two URIs to decide if they match or not, a client\n   SHOULD use a case-sensitive octet-by-octet comparison of the entire\n   URIs, with these exceptions:\n\n        . A port that is empty or not given is equivalent to the\n          default port for that URI-reference;\n        . Comparisons of host names MUST be case-insensitive;\n        . Comparisons of scheme names MUST be case-insensitive;\n        . An empty abs_path is equivalent to an abs_path of \"/\".\n   Characters other than those in the \"reserved\" and \"unsafe\" sets (see\n   section 3.2) are equivalent to their \"\"%\" HEX HEX\" encoding.\n\n   For example, the following three URIs are equivalent:\n\nFielding, et al                                                [Page 19]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n         http://abc.com:80/~smith/home.html\n         http://ABC.com/%7Esmith/home.html\n         http://ABC.com:/%7esmith/home.html\n\n3.3 Date/Time Formats\n\n\n3.3.1 Full Date\n\n   HTTP applications have historically allowed three different formats\n   for the representation of date/time stamps:\n\n       Sun, 06 Nov 1994 08:49:37 GMT  ; RFC 822, updated by RFC 1123\n       Sunday, 06-Nov-94 08:49:37 GMT ; RFC 850, obsoleted by RFC 1036\n       Sun Nov  6 08:49:37 1994       ; ANSI C's asctime() format\n\n   The first format is preferred as an Internet standard and represents\n   a fixed-length subset of that defined by RFC 1123 [8] (an update to\n   RFC 822 [9]). The second format is in common use, but is based on the\n   obsolete RFC 850 [12] date format and lacks a four-digit year.\n   HTTP/1.1 clients and servers that parse the date value MUST accept\n   all three formats (for compatibility with HTTP/1.0), though they MUST\n   only generate the RFC 1123 format for representing HTTP-date values\n   in header fields. See section 19.3 for further information.\n\n     Note: Recipients of date values are encouraged to be robust in\n     accepting date values that may have been sent by non-HTTP\n     applications, as is sometimes the case when retrieving or\n     posting messages via proxies/gateways to SMTP or NNTP.\n\n   All HTTP date/time stamps MUST be represented in Greenwich Mean Time\n   (GMT), without exception. For the purposes of HTTP, GMT is exactly\n   equal to UTC (Coordinated Universal Time). This is indicated in the\n   first two formats by the inclusion of \"GMT\" as the three-letter\n   abbreviation for time zone, and MUST be assumed when reading the\n   asctime format. HTTP-date is case sensitive and MUST NOT include\n   additional LWS beyond that specifically included as SP in the\n   grammar.\n\n          HTTP-date    = rfc1123-date | rfc850-date | asctime-date\n\n          rfc1123-date = wkday \",\" SP date1 SP time SP \"GMT\"\n\n          rfc850-date  = weekday \",\" SP date2 SP time SP \"GMT\"\n\n          asctime-date = wkday SP date3 SP time SP 4DIGIT\n\n          date1        = 2DIGIT SP month SP 4DIGIT\n                         ; day month year (e.g., 02 Jun 1982)\n\n          date2        = 2DIGIT \"-\" month \"-\" 2DIGIT\n                         ; day-month-year (e.g., 02-Jun-82)\n\n          date3        = month SP ( 2DIGIT | ( SP 1DIGIT ))\n                         ; month day (e.g., Jun  2)\n\nFielding, et al                                                [Page 20]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n\n          time         = 2DIGIT \":\" 2DIGIT \":\" 2DIGIT\n                         ; 00:00:00 - 23:59:59\n\n          wkday        = \"Mon\" | \"Tue\" | \"Wed\"\n                       | \"Thu\" | \"Fri\" | \"Sat\" | \"Sun\"\n\n          weekday      = \"Monday\" | \"Tuesday\" | \"Wednesday\"\n                       | \"Thursday\" | \"Friday\" | \"Saturday\" | \"Sunday\"\n\n          month        = \"Jan\" | \"Feb\" | \"Mar\" | \"Apr\"\n                       | \"May\" | \"Jun\" | \"Jul\" | \"Aug\"\n                       | \"Sep\" | \"Oct\" | \"Nov\" | \"Dec\"\n\n     Note: HTTP requirements for the date/time stamp format apply\n     only to their usage within the protocol stream. Clients and\n     servers are not required to use these formats for user\n     presentation, request logging, etc.\n\n\n3.3.2 Delta Seconds\n\n   Some HTTP header fields allow a time value to be specified as an\n   integer number of seconds, represented in decimal, after the time\n   that the message was received.\n\n          delta-seconds  = 1*DIGIT\n\n3.4 Character Sets\n\n   HTTP uses the same definition of the term \"character set\" as that\n   described for MIME:\n\n   The term \"character set\" is used in this document to refer to a\n   method used with one or more tables to convert a sequence of octets\n   into a sequence of characters. Note that unconditional conversion in\n   the other direction is not required, in that not all characters may\n   be available in a given character set and a character set may provide\n   more than one sequence of octets to represent a particular character.\n   This definition is intended to allow various kinds of character\n   encodings, from simple single-table mappings such as US-ASCII to\n   complex table switching methods such as those that use ISO-2022's\n   techniques. However, the definition associated with a MIME character\n   set name MUST fully specify the mapping to be performed from octets\n   to characters. In particular, use of external profiling information\n   to determine the exact mapping is not permitted.\n\n     Note: This use of the term \"character set\" is more commonly\n     referred to as a \"character encoding.\" However, since HTTP and\n     MIME share the same registry, it is important that the\n     terminology also be shared.\n\n   HTTP character sets are identified by case-insensitive tokens. The\n   complete set of tokens is defined by the IANA Character Set registry\n   [19].\n\nFielding, et al                                                [Page 21]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n          charset = token\n\n   Although HTTP allows an arbitrary token to be used as a charset\n   value, any token that has a predefined value within the IANA\n   Character Set registry [19] MUST represent the character set defined\n   by that registry. Applications SHOULD limit their use of character\n   sets to those defined by the IANA registry.\n\n   Implementors should be aware of IETF character set requirements [38]\n   [41].\n\n\n3.4.1 Missing Charset\n\n   Some HTTP/1.0 software has interpreted a Content-Type header without\n   charset parameter incorrectly to mean \"recipient should guess.\"\n   Senders wishing to defeat this behavior MAY include a charset\n   parameter even when the charset is ISO-8859-1 and SHOULD do so when\n   it is known that it will not confuse the recipient.\n\n   Unfortunately, some older HTTP/1.0 clients did not deal properly with\n   an explicit charset parameter. HTTP/1.1 recipients MUST respect the\n   charset label provided by the sender; and those user agents that have\n   a provision to \"guess\" a charset MUST use the charset from the\n   content-type field if they support that charset, rather than the\n   recipient's preference, when initially displaying a document. See\n   section 3.7.1.\n\n\n3.5 Content Codings\n\n   Content coding values indicate an encoding transformation that has\n   been or can be applied to an entity. Content codings are primarily\n   used to allow a document to be compressed or otherwise usefully\n   transformed without losing the identity of its underlying media type\n   and without loss of information. Frequently, the entity is stored in\n   coded form, transmitted directly, and only decoded by the recipient.\n\n          content-coding   = token\n\n   All content-coding values are case-insensitive. HTTP/1.1 uses\n   content-coding values in the Accept-Encoding (section 14.3) and\n   Content-Encoding (section 14.11) header fields. Although the value\n   describes the content-coding, what is more important is that it\n   indicates what decoding mechanism will be required to remove the\n   encoding.\n   The Internet Assigned Numbers Authority (IANA) acts as a registry for\n   content-coding value tokens. Initially, the registry contains the\n   following tokens:\n\n\n   gzip An encoding format produced by the file compression program\n     \"gzip\" (GNU zip) as described in RFC 1952 [25]. This format is a\n     Lempel-Ziv coding (LZ77) with a 32 bit CRC.\n\n\nFielding, et al                                                [Page 22]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   compress\n     The encoding format produced by the common UNIX file compression\n     program \"compress\". This format is an adaptive Lempel-Ziv-Welch\n     coding (LZW).\n\n     Use of program names for the identification of encoding formats is\n     not desirable and is discouraged for future encodings. Their use\n     here is representative of historical practice, not good design. For\n     compatibility with previous implementations of HTTP, applications\n     SHOULD consider \"x-gzip\" and \"x-compress\" to be equivalent to\n     \"gzip\" and \"compress\" respectively.\n\n   deflate\n     The \"zlib\" format defined in RFC 1950 [31] in combination with the\n     \"deflate\" compression mechanism described in RFC 1951 [29].\n\n   identity\n     The default (identity) encoding; the use of no transformation\n     whatsoever. This content-coding is used only in the Accept-Encoding\n     header, and SHOULD NOT be used in the Content-Encoding header.\n\n   New content-coding value tokens SHOULD be registered; to allow\n   interoperability between clients and servers, specifications of the\n   content coding algorithms needed to implement a new value SHOULD be\n   publicly available and adequate for independent implementation, and\n   conform to the purpose of content coding defined in this section.\n\n\n3.6 Transfer Codings\n\n   Transfer-coding values are used to indicate an encoding\n   transformation that has been, can be, or may need to be applied to an\n   entity-body in order to ensure \"safe transport\" through the network.\n   This differs from a content coding in that the transfer-coding is a\n   property of the message, not of the original entity.\n\n          transfer-coding         = \"chunked\" | transfer-extension\n\n          transfer-extension      = token *( \";\" parameter )\n\n   Parameters are in  the form of attribute/value pairs.\n\n          parameter               = attribute \"=\" value\n\n          attribute               = token\n\n          value                   = token | quoted-string\n\n   All transfer-coding values are case-insensitive. HTTP/1.1 uses\n   transfer-coding values in the TE header field (section 14.39) and in\n   the Transfer-Encoding header field (section 14.41).\n\n   Whenever a transfer-coding is applied to a message-body, the set of\n   transfer-codings MUST include \"chunked\", unless the message is\n   terminated by closing the connection. When the \"chunked\" transfer-\n\nFielding, et al                                                [Page 23]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   coding is used, it MUST be the last transfer-coding applied to the\n   message-body. The \"chunked\" transfer-coding MUST NOT be applied more\n   than once to a message-body. These rules allow the recipient to\n   determine the transfer-length of the message (section 4.4).\n\n   Transfer-codings are analogous to the Content-Transfer-Encoding\n   values of MIME [7], which were designed to enable safe transport of\n   binary data over a 7-bit transport service. However, safe transport\n   has a different focus for an 8bit-clean transfer protocol. In HTTP,\n   the only unsafe characteristic of message-bodies is the difficulty in\n   determining the exact body length (section 7.2.2), or the desire to\n   encrypt data over a shared transport.\n\n   The Internet Assigned Numbers Authority (IANA) acts as a registry for\n   transfer-coding value tokens. Initially, the registry contains the\n   following tokens: \"chunked\" (section 3.6.1), \"identity\" (section\n   3.6.2), \"gzip\" (section 3.5), \"compress\" (section 3.5), and \"deflate\"\n   (section 3.5).\n\n   New transfer-coding value tokens SHOULD be registered in the same way\n   as new content-coding value tokens (section 3.5).\n\n   A server which receives an entity-body with a transfer-coding it does\n   not understand SHOULD return 501 (Unimplemented), and close the\n   connection. A server MUST NOT send transfer-codings to an HTTP/1.0\n   client.\n\n\n3.6.1 Chunked Transfer Coding\n\n   The chunked encoding modifies the body of a message in order to\n   transfer it as a series of chunks, each with its own size indicator,\n   followed by an OPTIONAL trailer containing entity-header fields. This\n   allows dynamically produced content to be transferred along with the\n   information necessary for the recipient to verify that it has\n   received the full message.\n\n          Chunked-Body   = *chunk\n                           last-chunk\n                           trailer\n                           CRLF\n\n          chunk          = chunk-size [ chunk-extension ] CRLF\n                           chunk-data CRLF\n\n          chunk-size     = 1*HEX\n\n          last-chunk     = 1*(\"0\") [ chunk-extension ] CRLF\n\n          chunk-extension= *( \";\" chunk-ext-name [ \"=\" chunk-ext-val ] )\n\n          chunk-ext-name = token\n\n          chunk-ext-val  = token | quoted-string\n\n\nFielding, et al                                                [Page 24]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n          chunk-data     = chunk-size(OCTET)\n\n          trailer        = *entity-header\n\n   The chunk-size field is a string of hex digits indicating the size of\n   the chunk. The chunked encoding is ended by any chunk whose size is\n   zero, followed by the trailer, which is terminated by an empty line.\n\n   The trailer allows the sender to include additional HTTP header\n   fields at the end of the message. The Trailer header field can be\n   used to indicate which header fields are included in a trailer (see\n   section 14.40).\n\n   A server using chunked transfer-coding in a response MUST NOT use the\n   trailer for any header fields unless at least one of the following is\n   true:\n\n   a) the request included a TE header field that indicates \"trailers\"\n     is acceptable in the transfer-coding of the  response, as\n     described in section 14.39; or,\n\n   b)the server is the origin server for the response, the trailer\n     fields consist entirely of optional metadata, and the recipient\n     could use the message (in a manner acceptable to the origin\n     server) without receiving this metadata.  In other words, the\n     origin server is willing to accept the possibility that the\n     trailer fields might be silently discarded along the path to the\n     client.\n\n   This requirement prevents an interoperability failure when the\n   message is being received by an HTTP/1.1 (or later) proxy and\n   forwarded to an HTTP/1.0 recipient. It avoids a situation where\n   compliance with the protocol would have necessitated a possibly\n   infinite buffer on the proxy.\n\n   An example process for decoding a Chunked-Body is presented in\n   appendix 19.4.6.\n\n   All HTTP/1.1 applications MUST be able to receive and decode the\n   \"chunked\" transfer-coding, and MUST ignore chunk-extension extensions\n   they do not understand.\n\n\n3.7 Media Types\n\n   HTTP uses Internet Media Types [17] in the Content-Type (section\n   14.17) and Accept (section 14.1) header fields in order to provide\n   open and extensible data typing and type negotiation.\n\n          media-type     = type \"/\" subtype *( \";\" parameter )\n\n          type           = token\n\n          subtype        = token\n\n\nFielding, et al                                                [Page 25]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Parameters MAY follow the type/subtype in the form of attribute/value\n   pairs (as defined in section 3.6).\n\n   The type, subtype, and parameter attribute names are case-\n   insensitive. Parameter values might or might not be case-sensitive,\n   depending on the semantics of the parameter name. Linear white space\n   (LWS) MUST NOT be used between the type and subtype, nor between an\n   attribute and its value. The presence or absence of a parameter might\n   be significant to the processing of a media-type, depending on its\n   definition within the media type registry.\n\n   Note that some older HTTP applications do not recognize media type\n   parameters. When sending data to older HTTP applications,\n   implementations SHOULD only use media type parameters when they are\n   required by that type/subtype definition.\n\n   Media-type values are registered with the Internet Assigned Number\n   Authority (IANA [19]). The media type registration process is\n   outlined in RFC 1590 [17]. Use of non-registered media types is\n   discouraged.\n\n\n3.7.1 Canonicalization and Text Defaults\n\n   Internet media types are registered with a canonical form. An entity-\n   body transferred via HTTP messages MUST be represented in the\n   appropriate canonical form prior to its transmission except for\n   \"text\" types, as defined in the next paragraph.\n\n   When in canonical form, media subtypes of the \"text\" type use CRLF as\n   the text line break. HTTP relaxes this requirement and allows the\n   transport of text media with plain CR or LF alone representing a line\n   break when it is done consistently for an entire entity-body. HTTP\n   applications MUST accept CRLF, bare CR, and bare LF as being\n   representative of a line break in text media received via HTTP. In\n   addition, if the text is represented in a character set that does not\n   use octets 13 and 10 for CR and LF respectively, as is the case for\n   some multi-byte character sets, HTTP allows the use of whatever octet\n   sequences are defined by that character set to represent the\n   equivalent of CR and LF for line breaks. This flexibility regarding\n   line breaks applies only to text media in the entity-body; a bare CR\n   or LF MUST NOT be substituted for CRLF within any of the HTTP control\n   structures (such as header fields and multipart boundaries).\n\n   If an entity-body is encoded with a content-coding, the underlying\n   data MUST be in a form defined above prior to being encoded.\n\n   The \"charset\" parameter is used with some media types to define the\n   character set (section 3.4) of the data. When no explicit charset\n   parameter is provided by the sender, media subtypes of the \"text\"\n   type are defined to have a default charset value of \"ISO-8859-1\" when\n   received via HTTP. Data in character sets other than \"ISO-8859-1\" or\n   its subsets MUST be labeled with an appropriate charset value. See\n   section 3.4.1 for compatibility problems.\n\n\nFielding, et al                                                [Page 26]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n3.7.2 Multipart Types\n\n   MIME provides for a number of \"multipart\" types -- encapsulations of\n   one or more entities within a single message-body. All multipart\n   types share a common syntax, as defined in section 5.1.1 of RFC 2046\n   [40], and MUST include a boundary parameter as part of the media type\n   value. The message body is itself a protocol element and MUST\n   therefore use only CRLF to represent line breaks between body-parts.\n   Unlike in RFC 2046, the epilogue of any multipart message MUST be\n   empty; HTTP applications MUST NOT transmit the epilogue (even if the\n   original multipart contains an epilogue). These restrictions exist in\n   order to preserve the self-delimiting nature of a multipart message-\n   body, wherein the \"end\" of the message-body is indicated by the\n   ending multipart boundary.\n\n   In general, HTTP treats a multipart message-body no differently than\n   any other media type: strictly as payload. The one exception is the\n   \"multipart/byteranges\" type (appendix 19.2) when it appears in a 206\n   (Partial Content) response, which will be interpreted by some HTTP\n   caching mechanisms as described in sections 13.5.4 and 14.16. In all\n   other cases, an HTTP user agent SHOULD follow the same or similar\n   behavior as a MIME user agent would upon receipt of a multipart type.\n   The MIME header fields within each body-part of a multipart message-\n   body do not have any significance to HTTP beyond that defined by\n   their MIME semantics.\n\n   In general, an HTTP user agent SHOULD follow the same or similar\n   behavior as a MIME user agent would upon receipt of a multipart type.\n   If an application receives an unrecognized multipart subtype, the\n   application MUST treat it as being equivalent to \"multipart/mixed\".\n\n     Note: The \"multipart/form-data\" type has been specifically\n     defined for carrying form data suitable for processing via the\n     POST request method, as described in RFC 1867 [15].\n\n\n3.8 Product Tokens\n\n   Product tokens are used to allow communicating applications to\n   identify themselves by software name and version. Most fields using\n   product tokens also allow sub-products which form a significant part\n   of the application to be listed, separated by white space. By\n   convention, the products are listed in order of their significance\n   for identifying the application.\n\n          product         = token [\"/\" product-version]\n\n          product-version = token\n\n   Examples:\n\n          User-Agent: CERN-LineMode/2.15 libwww/2.17b3\n          Server: Apache/0.8.4\n\n\n\nFielding, et al                                                [Page 27]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Product tokens SHOULD be short and to the point. They MUST NOT be\n   used for advertising or other non-essential information. Although any\n   token character MAY appear in a product-version, this token SHOULD\n   only be used for a version identifier (i.e., successive versions of\n   the same product SHOULD only differ in the product-version portion of\n   the product value).\n\n\n3.9 Quality Values\n\n   HTTP content negotiation (section 12) uses short \"floating point\"\n   numbers to indicate the relative importance (\"weight\") of various\n   negotiable parameters.  A weight is normalized to a real number in\n   the range 0 through 1, where 0 is the minimum and 1 the maximum\n   value. If a parameter has a quality value of 0, then content with\n   this parameter is `not acceptable' for the client. HTTP/1.1\n   applications MUST NOT generate more than three digits after the\n   decimal point. User configuration of these values SHOULD also be\n   limited in this fashion.\n\n          qvalue         = ( \"0\" [ \".\" 0*3DIGIT ] )\n                         | ( \"1\" [ \".\" 0*3(\"0\") ] )\n\n   \"Quality values\" is a misnomer, since these values merely represent\n   relative degradation in desired quality.\n\n\n3.10 Language Tags\n\n   A language tag identifies a natural language spoken, written, or\n   otherwise conveyed by human beings for communication of information\n   to other human beings. Computer languages are explicitly excluded.\n   HTTP uses language tags within the Accept-Language and Content-\n   Language fields.\n\n   The syntax and registry of HTTP language tags is the same as that\n   defined by RFC 1766 [1]. In summary, a language tag is composed of 1\n   or more parts: A primary language tag and a possibly empty series of\n   subtags:\n\n           language-tag  = primary-tag *( \"-\" subtag )\n\n           primary-tag   = 1*8ALPHA\n\n           subtag        = 1*8ALPHA\n\n   White space is not allowed within the tag and all tags are case-\n   insensitive. The name space of language tags is administered by the\n   IANA. Example tags include:\n\n          en, en-US, en-cockney, i-cherokee, x-pig-latin\n\n   where any two-letter primary-tag is an ISO-639 language abbreviation\n   and any two-letter initial subtag is an ISO-3166 country code. (The\n\n\nFielding, et al                                                [Page 28]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   last three tags above are not registered tags; all but the last are\n   examples of tags which could be registered in future.)\n\n\n3.11 Entity Tags\n\n   Entity tags are used for comparing two or more entities from the same\n   requested resource. HTTP/1.1 uses entity tags in the ETag (section\n   14.19), If-Match (section 14.24), If-None-Match (section 14.26), and\n   If-Range (section 14.27) header fields. The definition of how they\n   are used and compared as cache validators is in section 13.3.3. An\n   entity tag consists of an opaque quoted string, possibly prefixed by\n   a weakness indicator.\n\n         entity-tag = [ weak ] opaque-tag\n\n         weak       = \"W/\"\n\n         opaque-tag = quoted-string\n\n   A \"strong entity tag\" MAY be shared by two entities of a resource\n   only if they are equivalent by octet equality.\n\n   A \"weak entity tag,\" indicated by the \"W/\" prefix, MAY be shared by\n   two entities of a resource only if the entities are equivalent and\n   could be substituted for each other with no significant change in\n   semantics. A weak entity tag can only be used for weak comparison.\n\n   An entity tag MUST be unique across all versions of all entities\n   associated with a particular resource. A given entity tag value MAY\n   be used for entities obtained by requests on different URIs without\n   implying anything about the equivalence of those entities.\n\n\n3.12 Range Units\n\n   HTTP/1.1 allows a client to request that only part (a range of) the\n   response entity be included within the response. HTTP/1.1 uses range\n   units in the Range (section 14.35) and Content-Range (section 14.16)\n   header fields. An entity can be broken down into subranges according\n   to various structural units.\n\n         range-unit       = bytes-unit | other-range-unit\n\n         bytes-unit       = \"bytes\"\n\n         other-range-unit = token\n\n   The only range unit defined by HTTP/1.1 is \"bytes\". HTTP/1.1\n   implementations MAY ignore ranges specified using other units.\n   HTTP/1.1 has been designed to allow implementations of applications\n   that do not depend on knowledge of ranges.\n\n\n\n\nFielding, et al                                                [Page 29]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n4 HTTP Message\n\n\n4.1 Message Types\n\n   HTTP messages consist of requests from client to server and responses\n   from server to client.\n\n          HTTP-message   = Request | Response     ; HTTP/1.1 messages\n\n   Request (section 5) and Response (section 6) messages use the generic\n   message format of RFC 822 [9] for transferring entities (the payload\n   of the message). Both types of message consist of a start-line, zero\n   or more header fields (also known as \"headers\"), an empty line (i.e.,\n   a line with nothing preceding the CRLF) indicating the end of the\n   header fields, and possibly a message-body.\n\n           generic-message = start-line\n                             *message-header\n                             CRLF\n                             [ message-body ]\n\n           start-line      = Request-Line | Status-Line\n\n   In the interest of robustness, servers SHOULD ignore any empty\n   line(s) received where a Request-Line is expected. In other words, if\n   the server is reading the protocol stream at the beginning of a\n   message and receives a CRLF first, it SHOULD ignore the CRLF.\n\n   Certain buggy HTTP/1.0 client implementations generate extra CRLF's\n   after a POST request. To restate what is explicitly forbidden by the\n   BNF, an HTTP/1.1 client MUST NOT preface or follow a request with an\n   extra CRLF.\n\n\n4.2 Message Headers\n\n   HTTP header fields, which include general-header (section 4.5),\n   request-header (section 5.3), response-header (section 6.2), and\n   entity-header (section 7.1) fields, follow the same generic format as\n   that given in Section 3.1 of RFC 822 [9]. Each header field consists\n   of a name followed by a colon (\":\") and the field value. Field names\n   are case-insensitive. The field value MAY be preceded by any amount\n   of LWS, though a single SP is preferred. Header fields can be\n   extended over multiple lines by preceding each extra line with at\n   least one SP or HT. Applications SHOULD follow \"common form\", where\n   one is known or indicated, when generating HTTP constructs, since\n   there might exist some implementations that fail to accept anything\n   beyond the common forms.\n\n          message-header = field-name \":\" [ field-value ] CRLF\n\n          field-name     = token\n\n          field-value    = *( field-content | LWS )\n\nFielding, et al                                                [Page 30]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n\n          field-content  = <the OCTETs making up the field-value\n                       and consisting of either *TEXT or combinations\n                       of token, separators, and quoted-string>\n\n   The field-content does not include any leading or trailing LWS:\n   linear white space occurring before the first non-whitespace\n   character of the field-value or after the last non-whitespace\n   character of the field-value. Such leading or trailing LWS MAY be\n   removed without changing the semantics of the field value. Any LWS\n   that occurs between field-content MAY be replaced with a single SP\n   before interpreting the field value or forwarding the message\n   downstream.\n\n   The order in which header fields with differing field names are\n   received is not significant. However, it is \"good practice\" to send\n   general-header fields first, followed by request-header or response-\n   header fields, and ending with the entity-header fields.\n\n   Multiple message-header fields with the same field-name MAY be\n   present in a message if and only if the entire field-value for that\n   header field is defined as a comma-separated list [i.e., #(values)].\n   It MUST be possible to combine the multiple header fields into one\n   \"field-name: field-value\" pair, without changing the semantics of the\n   message, by appending each subsequent field-value to the first, each\n   separated by a comma. The order in which header fields with the same\n   field-name are received is therefore significant to the\n   interpretation of the combined field value, and thus a proxy MUST NOT\n   change the order of these field values when a message is forwarded.\n\n\n4.3 Message Body\n\n   The message-body (if any) of an HTTP message is used to carry the\n   entity-body associated with the request or response. The message-body\n   differs from the entity-body only when a transfer-coding has been\n   applied, as indicated by the Transfer-Encoding header field (section\n   14.41).\n\n          message-body = entity-body\n                       | <entity-body encoded as per Transfer-Encoding>\n\n   Transfer-Encoding MUST be used to indicate any transfer-codings\n   applied by an application to ensure safe and proper transfer of the\n   message. Transfer-Encoding is a property of the message, not of the\n   entity, and thus MAY be added or removed by any application along the\n   request/response chain. (However, section 3.6 places restrictions on\n   when certain transfer-codings may be used.)\n\n   The rules for when a message-body is allowed in a message differ for\n   requests and responses.\n\n   The presence of a message-body in a request is signaled by the\n   inclusion of a Content-Length or Transfer-Encoding header field in\n   the request's message-headers. A message-body MUST NOT be included in\n\nFielding, et al                                                [Page 31]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   a request if the specification of the request method (section 5.1.1)\n   does not allow sending an entity-body in requests. A server SHOULD\n   read and forward a message-body on any request; if the request method\n   does not include defined semantics for an entity-body, then the\n   message-body SHOULD be ignored when handling the request.\n\n   For response messages, whether or not a message-body is included with\n   a message is dependent on both the request method and the response\n   status code (section 6.1.1). All responses to the HEAD request method\n   MUST NOT include a message-body, even though the presence of entity-\n   header fields might lead one to believe they do. All 1xx\n   (informational), 204 (no content), and 304 (not modified) responses\n   MUST NOT include a message-body. All other responses do include a\n   message-body, although it MAY be of zero length.\n\n\n4.4 Message Length\n\n   The transfer-length of a message is the length of the message-body as\n   it appears in the message; that is, after any transfer-codings have\n   been applied. When a message-body is included with a message, the\n   transfer-length of that body is determined by one of the following\n   (in order of precedence):\n\n  1. Any response message which MUST NOT include a message-body (such as\n     the 1xx, 204, and 304 responses and any response to a HEAD request)\n     is always terminated by the first empty line after the header\n     fields, regardless of the entity-header fields present in the\n     message.\n\n  2. If a Transfer-Encoding header field (section 14.41) is present and\n     has any value other than \"identity\", then the transfer-length is\n     defined by use of the \"chunked\" transfer-coding (section 3.6),\n     unless the message is terminated by closing the connection.\n\n  3. If a Content-Length header field (section 14.13) is present, its\n     decimal value in OCTETs represents both the entity-length and the\n     transfer-length. The Content-Length header field MUST NOT be used\n     if these two lengths are different (i.e., if a Transfer-Encoding\n     header field is present).\n\n  4.      If the message uses the media type \"multipart/byteranges\", and the\n     transfer-length is not otherwise specified, then this self-\n     delimiting media type defines the transfer-length. This media type\n     MUST NOT be used unless the sender knows that the recipient can\n     parse it; the presence in a request of a Range header with multiple\n     byte-range specifiers from a 1.1 client implies that the client can\n     parse multipart/byteranges responses.\n\n     Note: A range header may be forwarded by a 1.0 proxy that does\n     not understand multipart/byteranges; in this case the server\n     must delimit the message using methods defined in items 1,3 or 5\n     of this section.\n\n\n\nFielding, et al                                                [Page 32]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n  5. By the server closing the connection. (Closing the connection\n     cannot be used to indicate the end of a request body, since that\n     would leave no possibility for the server to send back a response.)\n\n   For compatibility with HTTP/1.0 applications, HTTP/1.1 requests\n   containing a message-body MUST include a valid Content-Length header\n   field unless the server is known to be HTTP/1.1 compliant. If a\n   request contains a message-body and a Content-Length is not given,\n   the server SHOULD respond with 400 (bad request) if it cannot\n   determine the length of the message, or with 411 (length required) if\n   it wishes to insist on receiving a valid Content-Length.\n\n   All HTTP/1.1 applications that receive entities MUST accept the\n   \"chunked\" transfer-coding (section 3.6), thus allowing this mechanism\n   to be used for messages when the message length cannot be determined\n   in advance.\n\n   Messages MUST NOT include both a Content-Length header field and a\n   non-identity transfer-coding. If the message does include a non-\n   identity transfer-coding, the Content-Length MUST be ignored.\n\n   When a Content-Length is given in a message where a message-body is\n   allowed, its field value MUST exactly match the number of OCTETs in\n   the message-body. HTTP/1.1 user agents MUST notify the user when an\n   invalid length is received and detected.\n\n\n4.5 General Header Fields\n\n   There are a few header fields which have general applicability for\n   both request and response messages, but which do not apply to the\n   entity being transferred. These header fields apply only to the\n   message being transmitted.\n\n          general-header = Cache-Control            ; Section 14.9\n                         | Connection               ; Section 14.10\n                         | Date                     ; Section 14.18\n                         | Pragma                   ; Section 14.32\n                         | Trailer                  ; Section 14.40\n                         | Transfer-Encoding        ; Section 14.41\n                         | Upgrade                  ; Section 14.42\n                         | Via                      ; Section 14.45\n                         | Warning                  ; Section 14.46\n\n   General-header field names can be extended reliably only in\n   combination with a change in the protocol version. However, new or\n   experimental header fields may be given the semantics of general\n   header fields if all parties in the communication recognize them to\n   be general-header fields. Unrecognized header fields are treated as\n   entity-header fields.\n\n\n\n\n\n\nFielding, et al                                                [Page 33]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n5 Request\n\n   A request message from a client to a server includes, within the\n   first line of that message, the method to be applied to the resource,\n   the identifier of the resource, and the protocol version in use.\n\n           Request       = Request-Line              ; Section 5.1\n                           *( general-header         ; Section 4.5\n                            | request-header         ; Section 5.3\n                            | entity-header )        ; Section 7.1\n                           CRLF\n                           [ message-body ]          ; Section 4.3\n\n5.1 Request-Line\n\n   The Request-Line begins with a method token, followed by the Request-\n   URI and the protocol version, and ending with CRLF. The elements are\n   separated by SP characters. No CR or LF is allowed except in the\n   final CRLF sequence.\n\n          Request-Line   = Method SP Request-URI SP HTTP-Version CRLF\n\n5.1.1 Method\n\n   The Method  token indicates the method to be performed on the\n   resource identified by the Request-URI. The method is case-sensitive.\n\n          Method         = \"OPTIONS\"                ; Section 9.2\n                         | \"GET\"                    ; Section 9.3\n                         | \"HEAD\"                   ; Section 9.4\n                         | \"POST\"                   ; Section 9.5\n                         | \"PUT\"                    ; Section 9.6\n                         | \"DELETE\"                 ; Section 9.7\n                         | \"TRACE\"                  ; Section 9.8\n                         | \"CONNECT\"                ; Section 9.9\n                         | extension-method\n\n          extension-method = token\n\n   The list of methods allowed by a resource can be specified in an\n   Allow header field (section 14.7). The return code of the response\n   always notifies the client whether a method is currently allowed on a\n   resource, since the set of allowed methods can change dynamically.\n   Servers SHOULD return the status code 405 (Method Not Allowed) if the\n   method is known by the server but not allowed for the requested\n   resource, and 501 (Not Implemented) if the method is unrecognized or\n   not implemented by the server. The methods GET and HEAD MUST be\n   supported by all general-purpose servers. All other methods are\n   OPTIONAL; however, if the above methods are implemented, they MUST be\n   implemented with the same semantics as those specified in section 9.\n\n\n\n\n\n\nFielding, et al                                                [Page 34]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n5.1.2 Request-URI\n\n   The Request-URI is a Uniform Resource Identifier (section 3.2) and\n   identifies the resource upon which to apply the request.\n\n          Request-URI    = \"*\" | absoluteURI | abs_path | authority\n\n   The three options for Request-URI are dependent on the nature of the\n   request. The asterisk \"*\" means that the request does not apply to a\n   particular resource, but to the server itself, and is only allowed\n   when the method used does not necessarily apply to a resource. One\n   example would be\n\n          OPTIONS * HTTP/1.1\n\n   The absoluteURI form is REQUIRED when the request is being made to a\n   proxy. The proxy is requested to forward the request or service it\n   from a valid cache, and return the response. Note that the proxy MAY\n   forward the request on to another proxy or directly to the server\n   specified by the absoluteURI. In order to avoid request loops, a\n   proxy MUST be able to recognize all of its server names, including\n   any aliases, local variations, and the numeric IP address. An example\n   Request-Line would be:\n\n          GET http://www.w3.org/pub/WWW/TheProject.html HTTP/1.1\n\n   To allow for transition to absoluteURIs in all requests in future\n   versions of HTTP, all HTTP/1.1 servers MUST accept the absoluteURI\n   form in requests, even though HTTP/1.1 clients will only generate\n   them in requests to proxies.\n\n   The authority form is only used by the CONNECT method (section 9.9).\n\n   The most common form of Request-URI is that used to identify a\n   resource on an origin server or gateway. In this case the absolute\n   path of the URI MUST be transmitted (see section 3.2.1, abs_path) as\n   the Request-URI, and the network location of the URI (authority) MUST\n   be transmitted in a Host header field. For example, a client wishing\n   to retrieve the resource above directly from the origin server would\n   create a TCP connection to port 80 of the host \"www.w3.org\" and send\n   the lines:\n\n          GET /pub/WWW/TheProject.html HTTP/1.1\n          Host: www.w3.org\n\n   followed by the remainder of the Request. Note that the absolute path\n   cannot be empty; if none is present in the original URI, it MUST be\n   given as \"/\" (the server root).\n\n   The Request-URI is transmitted in the format specified in section\n   3.2.1. If the Request-URI is encoded using the \"% HEX HEX\" encoding\n   [42], the origin server MUST decode the Request-URI in order to\n   properly interpret the request. Servers SHOULD respond to invalid\n   Request-URIs with an appropriate status code.\n\n\nFielding, et al                                                [Page 35]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   A transparent proxy MUST NOT rewrite the \"abs_path\" part of the\n   received Request-URI when forwarding it to the next inbound server,\n   except as noted above to replace a null abs_path with \"/\".\n\n     Note: The \"no rewrite\" rule prevents the proxy from changing the\n     meaning of the request when the origin server is improperly\n     using a non-reserved URI character for a reserved purpose.\n     Implementors should be aware that some pre-HTTP/1.1 proxies have\n     been known to rewrite the Request-URI.\n\n\n5.2 The Resource Identified by a Request\n\n   The exact resource identified by an Internet request is determined by\n   examining both the Request-URI and the Host header field.\n\n   An origin server that does not allow resources to differ by the\n   requested host MAY ignore the Host header field value when\n   determining the resource identified by an HTTP/1.1 request. (But see\n   section 19.6.1.1 for other requirements on Host support in HTTP/1.1.)\n\n   An origin server that does differentiate resources based on the host\n   requested (sometimes referred to as virtual hosts or vanity\n   hostnames) MUST use the following rules for determining the requested\n   resource on an HTTP/1.1 request:\n\n     1. If Request-URI is an absoluteURI, the host is part of the\n        Request-URI. Any Host header field value in the request MUST be\n        ignored.\n\n     2. If the Request-URI is not an absoluteURI, and the request\n        includes a Host header field, the host is determined by the Host\n        header field value.\n\n     3. If the host as determined by rule 1 or 2 is not a valid host on\n        the server, the response MUST be a 400 (Bad Request) error\n        message.\n\n   Recipients of an HTTP/1.0 request that lacks a Host header field MAY\n   attempt to use heuristics (e.g., examination of the URI path for\n   something unique to a particular host) in order to determine what\n   exact resource is being requested.\n\n\n5.3 Request Header Fields\n\n   The request-header fields allow the client to pass additional\n   information about the request, and about the client itself, to the\n   server. These fields act as request modifiers, with semantics\n   equivalent to the parameters on a programming language method\n   invocation.\n\n          request-header = Accept                   ; Section 14.1\n                         | Accept-Charset           ; Section 14.2\n                         | Accept-Encoding          ; Section 14.3\n\nFielding, et al                                                [Page 36]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n                         | Accept-Language          ; Section 14.4\n                         | Authorization            ; Section 14.8\n                         | Expect                   ; Section 14.20\n                         | From                     ; Section 14.22\n                         | Host                     ; Section 14.23\n                         | If-Match                 ; Section 14.24\n                         | If-Modified-Since        ; Section 14.25\n                         | If-None-Match            ; Section 14.26\n                         | If-Range                 ; Section 14.27\n                         | If-Unmodified-Since      ; Section 14.28\n                         | Max-Forwards             ; Section 14.31\n                         | Proxy-Authorization      ; Section 14.34\n                         | Range                    ; Section 14.35\n                         | Referer                  ; Section 14.36\n                         | TE                       ; Section 14.39\n                         | User-Agent               ; Section 14.43\n\n   Request-header field names can be extended reliably only in\n   combination with a change in the protocol version. However, new or\n   experimental header fields MAY be given the semantics of request-\n   header fields if all parties in the communication recognize them to\n   be request-header fields. Unrecognized header fields are treated as\n   entity-header fields.\n\n\n6 Response\n\n   After receiving and interpreting a request message, a server responds\n   with an HTTP response message.\n\n          Response      = Status-Line               ; Section 6.1\n                          *( general-header         ; Section 4.5\n                           | response-header        ; Section 6.2\n                           | entity-header )        ; Section 7.1\n                          CRLF\n                          [ message-body ]          ; Section 7.2\n\n6.1 Status-Line\n\n   The first line of a Response message is the Status-Line, consisting\n   of the protocol version followed by a numeric status code and its\n   associated textual phrase, with each element separated by SP\n   characters. No CR or LF is allowed except in the final CRLF sequence.\n\n          Status-Line = HTTP-Version SP Status-Code SP Reason-Phrase\n   CRLF\n\n6.1.1 Status Code and Reason Phrase\n\n   The Status-Code element is a 3-digit integer result code of the\n   attempt to understand and satisfy the request. These codes are fully\n   defined in section 10. The Reason-Phrase is intended to give a short\n   textual description of the Status-Code. The Status-Code is intended\n   for use by automata and the Reason-Phrase is intended for the human\n\n\nFielding, et al                                                [Page 37]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   user. The client is not required to examine or display the Reason-\n   Phrase.\n\n   The first digit of the Status-Code defines the class of response. The\n   last two digits do not have any categorization role. There are 5\n   values for the first digit:\n\n\n     .  1xx: Informational - Request received, continuing process\n\n     .  2xx: Success - The action was successfully received, understood,\n        and accepted\n\n     .  3xx: Redirection - Further action must be taken in order to\n        complete the request\n\n     .  4xx: Client Error - The request contains bad syntax or cannot be\n        fulfilled\n\n     .  5xx: Server Error - The server failed to fulfill an apparently\n        valid request\n\n\n   The individual values of the numeric status codes defined for\n   HTTP/1.1, and an example set of corresponding Reason-Phrase's, are\n   presented below. The reason phrases listed here are only\n   recommendations -- they MAY be replaced by local equivalents without\n   affecting the protocol.\n\n      Status-Code    =\n               \"100\"  ; Section 10.1.1: Continue\n             | \"101\"  ; Section 10.1.2: Switching Protocols\n             | \"200\"  ; Section 10.2.1: OK\n             | \"201\"  ; Section 10.2.2: Created\n             | \"202\"  ; Section 10.2.3: Accepted\n             | \"203\"  ; Section 10.2.4: Non-Authoritative Information\n             | \"204\"  ; Section 10.2.5: No Content\n             | \"205\"  ; Section 10.2.6: Reset Content\n             | \"206\"  ; Section 10.2.7: Partial Content\n             | \"300\"  ; Section 10.3.1: Multiple Choices\n             | \"301\"  ; Section 10.3.2: Moved Permanently\n             | \"302\"  ; Section 10.3.3: Found\n             | \"303\"  ; Section 10.3.4: See Other\n             | \"304\"  ; Section 10.3.5: Not Modified\n             | \"305\"  ; Section 10.3.6: Use Proxy\n             | \"307\"  ; Section 10.3.8: Temporary Redirect\n             | \"400\"  ; Section 10.4.1: Bad Request\n             | \"401\"  ; Section 10.4.2: Unauthorized\n             | \"402\"  ; Section 10.4.3: Payment Required\n             | \"403\"  ; Section 10.4.4: Forbidden\n             | \"404\"  ; Section 10.4.5: Not Found\n             | \"405\"  ; Section 10.4.6: Method Not Allowed\n             | \"406\"  ; Section 10.4.7: Not Acceptable\n             | \"407\"  ; Section 10.4.8: Proxy Authentication Required\n             | \"408\"  ; Section 10.4.9: Request Time-out\n\nFielding, et al                                                [Page 38]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n             | \"409\"  ; Section 10.4.10: Conflict\n             | \"410\"  ; Section 10.4.11: Gone\n             | \"411\"  ; Section 10.4.12: Length Required\n             | \"412\"  ; Section 10.4.13: Precondition Failed\n             | \"413\"  ; Section 10.4.14: Request Entity Too Large\n             | \"414\"  ; Section 10.4.15: Request-URI Too Large\n             | \"415\"  ; Section 10.4.16: Unsupported Media Type\n             | \"416\"  ; Section 10.4.17: Requested range not satisfiable\n             | \"417\"  ; Section 10.4.18: Expectation Failed\n             | \"500\"  ; Section 10.5.1: Internal Server Error\n             | \"501\"  ; Section 10.5.2: Not Implemented\n             | \"502\"  ; Section 10.5.3: Bad Gateway\n             | \"503\"  ; Section 10.5.4: Service Unavailable\n             | \"504\"  ; Section 10.5.5: Gateway Time-out\n             | \"505\"  ; Section 10.5.6: HTTP Version not supported\n             | extension-code\n\n      extension-code = 3DIGIT\n\n      Reason-Phrase  = *<TEXT, excluding CR, LF>\n\n   HTTP status codes are extensible. HTTP applications are not required\n   to understand the meaning of all registered status codes, though such\n   understanding is obviously desirable. However, applications MUST\n   understand the class of any status code, as indicated by the first\n   digit, and treat any unrecognized response as being equivalent to the\n   x00 status code of that class, with the exception that an\n   unrecognized response MUST NOT be cached. For example, if an\n   unrecognized status code of 431 is received by the client, it can\n   safely assume that there was something wrong with its request and\n   treat the response as if it had received a 400 status code. In such\n   cases, user agents SHOULD present to the user the entity returned\n   with the response, since that entity is likely to include human-\n   readable information which will explain the unusual status.\n\n\n6.2 Response Header Fields\n\n   The response-header fields allow the server to pass additional\n   information about the response which cannot be placed in the Status-\n   Line. These header fields give information about the server and about\n   further access to the resource identified by the Request-URI.\n\n          response-header = Accept-Ranges           ; Section 14.5\n                          | Age                     ; Section 14.6\n                          | ETag                    ; Section 14.19\n                          | Location                ; Section 14.30\n                          | Proxy-Authenticate      ; Section 14.33\n                          | Retry-After             ; Section 14.37\n                          | Server                  ; Section 14.38\n                          | Vary                    ; Section 14.44\n                          | WWW-Authenticate        ; Section 14.47\n\n   Response-header field names can be extended reliably only in\n   combination with a change in the protocol version. However, new or\n\nFielding, et al                                                [Page 39]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   experimental header fields MAY be given the semantics of response-\n   header fields if all parties in the communication recognize them to\n   be response-header fields. Unrecognized header fields are treated as\n   entity-header fields.\n\n\n7 Entity\n\n   Request and Response messages MAY transfer an entity if not otherwise\n   restricted by the request method or response status code. An entity\n   consists of entity-header fields and an entity-body, although some\n   responses will only include the entity-headers.\n\n   In this section, both sender and recipient refer to either the client\n   or the server, depending on who sends and who receives the entity.\n\n\n7.1 Entity Header Fields\n\n   Entity-header fields define metainformation about the entity-body or,\n   if no body is present, about the resource identified by the request.\n   Some of this metainformation is OPTIONAL; some might be REQUIRED by\n   portions of this specification.\n\n          entity-header  = Allow                    ; Section 14.7\n                         | Content-Encoding         ; Section 14.11\n                         | Content-Language         ; Section 14.12\n                         | Content-Length           ; Section 14.13\n                         | Content-Location         ; Section 14.14\n                         | Content-MD5              ; Section 14.15\n                         | Content-Range            ; Section 14.16\n                         | Content-Type             ; Section 14.17\n                         | Expires                  ; Section 14.21\n                         | Last-Modified            ; Section 14.29\n                         | extension-header\n\n          extension-header = message-header\n\n   The extension-header mechanism allows additional entity-header fields\n   to be defined without changing the protocol, but these fields cannot\n   be assumed to be recognizable by the recipient. Unrecognized header\n   fields SHOULD be ignored by the recipient and MUST be forwarded by\n   transparent proxies.\n\n\n7.2 Entity Body\n\n   The entity-body (if any) sent with an HTTP request or response is in\n   a format and encoding defined by the entity-header fields.\n\n          entity-body    = *OCTET\n\n   An entity-body is only present in a message when a message-body is\n   present, as described in section 4.3. The entity-body is obtained\n\n\nFielding, et al                                                [Page 40]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   from the message-body by decoding any Transfer-Encoding that might\n   have been applied to ensure safe and proper transfer of the message.\n\n\n7.2.1 Type\n\n   When an entity-body is included with a message, the data type of that\n   body is determined via the header fields Content-Type and Content-\n   Encoding. These define a two-layer, ordered encoding model:\n\n          entity-body := Content-Encoding( Content-Type( data ) )\n\n   Content-Type specifies the media type of the underlying data.\n   Content-Encoding may be used to indicate any additional content\n   codings applied to the data, usually for the purpose of data\n   compression, that are a property of the requested resource. There is\n   no default encoding.\n\n   Any HTTP/1.1 message containing an entity-body SHOULD include a\n   Content-Type header field defining the media type of that body. If\n   and only if the media type is not given by a Content-Type field, the\n   recipient MAY attempt to guess the media type via inspection of its\n   content and/or the name extension(s) of the URI used to identify the\n   resource. If the media type remains unknown, the recipient SHOULD\n   treat it as type \"application/octet-stream\".\n\n\n7.2.2 Entity Length\n\n   The entity-length of a message is the length of the message-body\n   before any transfer-codings have been applied. Section 4.4 defines\n   how the transfer-length of a message-body is determined.\n\n\n8 Connections\n\n\n8.1 Persistent Connections\n\n\n8.1.1 Purpose\n\n   Prior to persistent connections, a separate TCP connection was\n   established to fetch each URL, increasing the load on HTTP servers\n   and causing congestion on the Internet. The use of inline images and\n   other associated data often require a client to make multiple\n   requests of the same server in a short amount of time. Analysis of\n   these performance problems and results from a prototype\n   implementation are available [26] [30]. Implementation experience and\n   measurements of actual HTTP/1.1 (RFC 2068) implementations show good\n   results [39]. Alternatives have also been explored, for example,\n   T/TCP [27].\n\n   Persistent HTTP connections have a number of advantages:\n\n\nFielding, et al                                                [Page 41]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     .  By opening and closing fewer TCP connections, CPU time is saved\n        in routers and hosts (clients, servers, proxies, gateways,\n        tunnels, or caches), and memory used for TCP protocol control\n        blocks can be saved in hosts.\n     .  HTTP requests and responses can be pipelined on a connection.\n        Pipelining allows a client to make multiple requests without\n        waiting for each response, allowing a single TCP connection to\n        be used much more efficiently, with much lower elapsed time.\n     .  Network congestion is reduced by reducing the number of packets\n        caused by TCP opens, and by allowing TCP sufficient time to\n        determine the congestion state of the network.\n     .  Latency on subsequent requests is reduced since there is no time\n        spent in TCP's connection opening handshake.\n     .  HTTP can evolve more gracefully, since errors can be reported\n        without the penalty of closing the TCP connection. Clients using\n        future versions of HTTP might optimistically try a new feature,\n        but if communicating with an older server, retry with old\n        semantics after an error is reported.\n   HTTP implementations SHOULD implement persistent connections.\n\n\n8.1.2 Overall Operation\n\n   A significant difference between HTTP/1.1 and earlier versions of\n   HTTP is that persistent connections are the default behavior of any\n   HTTP connection. That is, unless otherwise indicated, the client\n   SHOULD assume that the server will maintain a persistent connection,\n   even after error responses from the server.\n\n   Persistent connections provide a mechanism by which a client and a\n   server can signal the close of a TCP connection. This signaling takes\n   place using the Connection header field (section 14.10). Once a close\n   has been signaled, the client MUST not send any more requests on that\n   connection.\n\n\n8.1.2.1 Negotiation\n\n   An HTTP/1.1 server MAY assume that a HTTP/1.1 client intends to\n   maintain a persistent connection unless a Connection header including\n   the connection-token \"close\" was sent in the request. If the server\n   chooses to close the connection immediately after sending the\n   response, it SHOULD send a Connection header including the\n   connection-token close.\n\n   An HTTP/1.1 client MAY expect a connection to remain open, but would\n   decide to keep it open based on whether the response from a server\n   contains a Connection header with the connection-token close. In case\n   the client does not want to maintain a connection for more than that\n   request, it SHOULD send a Connection header including the connection-\n   token close.\n\n   If either the client or the server sends the close token in the\n   Connection header, that request becomes the last one for the\n   connection.\n\nFielding, et al                                                [Page 42]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Clients and servers SHOULD NOT assume that a persistent connection is\n   maintained for HTTP versions less than 1.1 unless it is explicitly\n   signaled. See section 19.6.2 for more information on backward\n   compatibility with HTTP/1.0 clients.\n\n   In order to remain persistent, all messages on the connection MUST\n   have a self-defined message length (i.e., one not defined by closure\n   of the connection), as described in section 4.4.\n\n\n8.1.2.2 Pipelining\n\n   A client that supports persistent connections MAY \"pipeline\" its\n   requests (i.e., send multiple requests without waiting for each\n   response). A server MUST send its responses to those requests in the\n   same order that the requests were received.\n\n   Clients which assume persistent connections and pipeline immediately\n   after connection establishment SHOULD be prepared to retry their\n   connection if the first pipelined attempt fails. If a client does\n   such a retry, it MUST NOT pipeline before it knows the connection is\n   persistent. Clients MUST also be prepared to resend their requests if\n   the server closes the connection before sending all of the\n   corresponding responses.\n\n   Clients SHOULD NOT pipeline requests using non-idempotent methods or\n   non-idempotent sequences of methods (see section 9.1.2). Otherwise, a\n   premature termination of the transport connection could lead to\n   indeterminate results. A client wishing to send a non-idempotent\n   request SHOULD wait to send that request until it has received the\n   response status for the previous request.\n\n\n8.1.3 Proxy Servers\n\n   It is especially important that proxies correctly implement the\n   properties of the Connection header field as specified in14.10.\n\n   The proxy server MUST signal persistent connections separately with\n   its clients and the origin servers (or other proxy servers) that it\n   connects to. Each persistent connection applies to only one transport\n   link.\n\n   A proxy server MUST NOT establish a HTTP/1.1 persistent connection\n   with an HTTP/1.0 client (but see RFC 2068 [33] for information and\n   discussion of the problems with the Keep-Alive header implemented by\n   many HTTP/1.0 clients).\n\n\n8.1.4 Practical Considerations\n\n   Servers will usually have some time-out value beyond which they will\n   no longer maintain an inactive connection. Proxy servers might make\n   this a higher value since it is likely that the client will be making\n   more connections through the same server. The use of persistent\n\nFielding, et al                                                [Page 43]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   connections places no requirements on the length (or existence) of\n   this time-out for either the client or the server.\n\n   When a client or server wishes to time-out it SHOULD issue a graceful\n   close on the transport connection. Clients and servers SHOULD both\n   constantly watch for the other side of the transport close, and\n   respond to it as appropriate. If a client or server does not detect\n   the other side's close promptly it could cause unnecessary resource\n   drain on the network.\n\n   A client, server, or proxy MAY close the transport connection at any\n   time. For example, a client might have started to send a new request\n   at the same time that the server has decided to close the \"idle\"\n   connection. From the server's point of view, the connection is being\n   closed while it was idle, but from the client's point of view, a\n   request is in progress.\n\n   This means that clients, servers, and proxies MUST be able to recover\n   from asynchronous close events. Client software SHOULD reopen the\n   transport connection and retransmit the aborted sequence of requests\n   without user interaction so long as the request sequence is\n   idempotent (see section 9.1.2). Non-idempotent methods or sequences\n   MUST NOT be automatically retried, although user agents MAY offer a\n   human operator the choice of retrying the request(s). The automatic\n   retry SHOULD NOT be repeated if the second sequence of requests\n   fails.\n\n   Servers SHOULD always respond to at least one request per connection,\n   if at all possible. Servers SHOULD NOT close a connection in the\n   middle of transmitting a response, unless a network or client failure\n   is suspected.\n\n   Clients that use persistent connections SHOULD limit the number of\n   simultaneous connections that they maintain to a given server. A\n   single-user client SHOULD maintain AT MOST 2 connections with any\n   server or proxy. A proxy SHOULD use up to 2*N connections to another\n   server or proxy, where N is the number of simultaneously active\n   users. These guidelines are intended to improve HTTP response times\n   and avoid congestion.\n\n\n8.2 Message Transmission Requirements\n\n\n8.2.1 Persistent Connections and Flow Control\n\n   HTTP/1.1 servers SHOULD maintain persistent connections and use TCP's\n   flow control mechanisms to resolve temporary overloads, rather than\n   terminating connections with the expectation that clients will retry.\n   The latter technique can exacerbate network congestion.\n\n\n\n\n\n\nFielding, et al                                                [Page 44]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n8.2.2 Monitoring Connections for Error Status Messages\n\n   An HTTP/1.1 (or later) client sending a message-body SHOULD monitor\n   the network connection for an error status while it is transmitting\n   the request. If the client sees an error status, it SHOULD\n   immediately cease transmitting the body. If the body is being sent\n   using a \"chunked\" encoding (section 3.6), a zero length chunk and\n   empty trailer MAY be used to prematurely mark the end of the message.\n   If the body was preceded by a Content-Length header, the client MUST\n   close the connection.\n\n\n8.2.3 Automatic Retrying of Requests\n\n   If a user agent sees the transport connection close before it\n   receives all of the final response to its request or sequence of\n   requests, and if the requests or sequence are idempotent (see section\n   9.1.2), the user agent MAY retry the request or sequence without user\n   interaction. If the request method or sequence is not idempotent, the\n   user agent SHOULD NOT retry the request without user confirmation.\n   (Confirmation by user-agent software with semantic understanding of\n   the application MAY substitute for user confirmation.)\n\n\n8.2.4 Use of the 100 (Continue) Status\n\n   The purpose of the 100 (Continue) status (see section 10.1.1) is to\n   allow an end-client that is sending a request message with a request\n   body to determine if the origin server is willing to accept the\n   request (based on the request headers) before the client sends the\n   request body. In some cases, it might either be inappropriate or\n   highly inefficient for the client to send the body if the server will\n   reject the message without looking at the body.\n\n   Requirements for HTTP/1.1 clients:\n\n     .  If a client will wait for a 100 (Continue) response before\n        sending the request body, it MUST send an Expect request-header\n        field (section 14.20) with the \"100-continue\" expectation.\n\n     .  A client MUST NOT send an Expect request-header field (section\n        14.20) with the \"100-continue\" expectation if it does not intend\n        to send a request body.\n\n   Because of the presence of older implementations, the protocol allows\n   ambiguous situations in which a client may send \"Expect: 100-\n   continue\" without receiving either a 417 (Expectation Failed) status\n   or a 100 (Continue) status. Therefore, when a client sends this\n   header field to an origin server (possibly via a proxy) from which it\n   has never seen a 100 (Continue) status, the client SHOULD NOT wait\n   for an indefinite period before sending the request body.\n\n   Requirements for HTTP/1.1 origin servers:\n\n\n\nFielding, et al                                                [Page 45]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     .  Upon receiving a request which includes an Expect request-header\n        field with the \"100-continue\" expectation, an origin server MUST\n        either respond with 100 (Continue) status and continue to read\n        from the input stream, or respond with an error status. The\n        origin server MUST NOT wait for the request body before sending\n        the 100 (Continue) response. If it responds with an error\n        status, it MAY close the transport connection or it MAY continue\n        to read and discard the rest of the request. It MUST NOT perform\n        the requested method if it returns an error status.\n\n     .  An origin server SHOULD NOT send a 100 (Continue) response if\n        the request message does not include an Expect request-header\n        field with the \"100-continue\" expectation, and MUST NOT send a\n        100 (Continue) response if such a request comes from an HTTP/1.0\n        (or earlier) client. There is an exception to this rule: for\n        compatibility with RFC 2068, a server MAY send a 100 (Continue)\n        status in response to an HTTP/1.1 PUT or POST request that does\n        not include an Expect request-header field with the \"100-\n        continue\" expectation. This exception, the purpose of which is\n        to minimize any client processing delays associated with an\n        undeclared wait for 100 (Continue) status, applies only to\n        HTTP/1.1 requests, and not to requests with any other HTTP-\n        version value.\n\n     .  An origin server MAY omit a 100 (Continue) response if it has\n        already received some or all of the request body for the\n        corresponding request.\n\n     .  An origin server that sends a 100 (Continue) response MUST\n        ultimately send a final status code, once the request body is\n        received and processed, unless it terminates the transport\n        connection prematurely.\n\n     .  If an origin server receives a request that does not include an\n        Expect request-header field with the \"100-continue\" expectation,\n        the request includes a request body, and the server responds\n        with an error status before reading the entire request body from\n        the transport connection, then the server SHOULD NOT close the\n        transport connection until it has read the entire request, or\n        until the client closes the connection. Otherwise, the client\n        might not reliably receive the response message. However, this\n        requirement is not be construed as preventing a server from\n        defending itself against denial-of-service attacks, or from\n        badly broken client implementations.\n\n   Requirements for HTTP/1.1 proxies:\n\n     .  If a proxy receives a request that includes an Expect request-\n        header field with the \"100-continue\" expectation, and the proxy\n        either knows that the next-hop server complies with HTTP/1.1 or\n        higher, or does not know the HTTP version of the next-hop\n        server, it MUST forward the request, including the Expect header\n        field.\n\n\n\nFielding, et al                                                [Page 46]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     .  If the proxy knows that the version of the next-hop server is\n        HTTP/1.0 or lower, it MUST NOT forward the request, and it MUST\n        respond with a 417 (Expectation Failed) status.\n\n     .  Proxies SHOULD maintain a cache recording the HTTP version\n        numbers received from recently-referenced next-hop servers.\n\n     .  A proxy MUST NOT forward a 100 (Continue) response if the\n        request message was received from an HTTP/1.0 (or earlier)\n        client and did not include an Expect request-header field with\n        the \"100-continue\" expectation. This requirement overrides the\n        general rule for forwarding of 1xx responses (see section 10.1).\n\n\n8.2.5 Client Behavior if Server Prematurely Closes Connection\n\n   If an HTTP/1.1 client sends a request which includes a request body,\n   but which does not include an Expect request-header field with the\n   \"100-continue\" expectation, and if the client is not directly\n   connected to an HTTP/1.1 origin server, and if the client sees the\n   connection close before receiving any status from the server, the\n   client SHOULD retry the request, subject to the restrictions in\n   section 8.2.3. If the client does retry this request, it MAY use the\n   following \"binary exponential backoff\" algorithm to be assured of\n   obtaining a reliable response:\n\n     1. Initiate a new connection to the server\n\n     2. Transmit the request-headers\n\n     3. Initialize a variable R to the estimated round-trip time to the\n        server (e.g., based on the time it took to establish the\n        connection), or to a constant value of 5 seconds if the round-\n        trip time is not available.\n\n     4. Compute T = R * (2**N), where N is the number of previous\n        retries of this request.\n\n     5. Wait either for an error response from the server, or for T\n        seconds (whichever comes first)\n\n     6. If no error response is received, after T seconds transmit the\n        body of the request.\n\n     7. If client sees that the connection is closed prematurely, repeat\n        from step 1 until the request is accepted, an error response is\n        received, or the user becomes impatient and terminates the retry\n        process.\n\n   If at any point an error status is received, the client\n\n     .  SHOULD NOT continue and\n\n     .  SHOULD close the connection if it has not completed sending the\n        request message.\n\nFielding, et al                                                [Page 47]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n9 Method Definitions\n\n   The set of common methods for HTTP/1.1 is defined below. Although\n   this set can be expanded, additional methods cannot be assumed to\n   share the same semantics for separately extended clients and servers.\n\n   The Host request-header field (section 14.23) MUST accompany all\n   HTTP/1.1 requests.\n\n\n9.1 Safe and Idempotent Methods\n\n\n9.1.1 Safe Methods\n\n   Implementors should be aware that the software represents the user in\n   their interactions over the Internet, and should be careful to allow\n   the user to be aware of any actions they might take which may have an\n   unexpected significance to themselves or others.\n\n   In particular, the convention has been established that the GET and\n   HEAD methods SHOULD NOT have the significance of taking an action\n   other than retrieval. These methods ought to be considered \"safe.\"\n   This allows user agents to represent other methods, such as POST, PUT\n   and DELETE, in a special way, so that the user is made aware of the\n   fact that a possibly unsafe action is being requested.\n\n   Naturally, it is not possible to ensure that the server does not\n   generate side-effects as a result of performing a GET request; in\n   fact, some dynamic resources consider that a feature. The important\n   distinction here is that the user did not request the side-effects,\n   so therefore cannot be held accountable for them.\n\n\n9.1.2 Idempotent Methods\n\n   Methods can also have the property of \"idempotence\" in that (aside\n   from error or expiration issues) the side-effects of N > 0 identical\n   requests is the same as for a single request. The methods GET, HEAD,\n   PUT and DELETE share this property. Also, the methods OPTIONS and\n   TRACE SHOULD NOT have side effects, and so are inherently idempotent.\n\n   However, it is possible that a sequence of several requests is non-\n   idempotent, even if all of the methods executed in that sequence are\n   idempotent. (A sequence is idempotent if a single execution of the\n   entire sequence always yields a result that is not changed by a\n   reexecution of all, or part, of that sequence.) For example, a\n   sequence is non-idempotent if its result depends on a value that is\n   later modified in the same sequence.\n\n   A sequence that never has side effects is idempotent, by definition\n   (provided that no concurrent operations are being executed on the\n   same set of resources).\n\n\n\nFielding, et al                                                [Page 48]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n9.2 OPTIONS\n\n   The OPTIONS method represents a request for information about the\n   communication options available on the request/response chain\n   identified by the Request-URI. This method allows the client to\n   determine the options and/or requirements associated with a resource,\n   or the capabilities of a server, without implying a resource action\n   or initiating a resource retrieval.\n\n   Responses to this method are not cachable.\n\n   If the OPTIONS request includes an entity-body (as indicated by the\n   presence of Content-Length or Transfer-Encoding), then the media type\n   MUST be indicated by a Content-Type field. Although this\n   specification does not define any use for such a body, future\n   extensions to HTTP might use the OPTIONS body to make more detailed\n   queries on the server. A server that does not support such an\n   extension MAY discard the request body.\n\n   If the Request-URI is an asterisk (\"*\"), the OPTIONS request is\n   intended to apply to the server in general rather than to a specific\n   resource. Since a server's communication options typically depend on\n   the resource, the \"*\" request is only useful as a \"ping\" or \"no-op\"\n   type of method; it does nothing beyond allowing the client to test\n   the capabilities of the server. For example, this can be used to test\n   a proxy for HTTP/1.1 compliance (or lack thereof).\n\n   If the Request-URI is not an asterisk, the OPTIONS request applies\n   only to the options that are available when communicating with that\n   resource.\n\n   A 200 response SHOULD include any header fields that indicate\n   optional features implemented by the server and applicable to that\n   resource (e.g., Allow), possibly including extensions not defined by\n   this specification. The response body, if any, SHOULD also include\n   information about the communication options. The format for such a\n   body is not defined by this specification, but might be defined by\n   future extensions to HTTP. Content negotiation MAY be used to select\n   the appropriate response format. If no response body is included, the\n   response MUST include a Content-Length field with a field-value of\n   \"0\".\n\n   The Max-Forwards request-header field MAY be used to target a\n   specific proxy in the request chain. When a proxy receives an OPTIONS\n   request on an absoluteURI for which request forwarding is permitted,\n   the proxy MUST check for a Max-Forwards field. If the Max-Forwards\n   field-value is zero (\"0\"), the proxy MUST NOT forward the message;\n   instead, the proxy SHOULD respond with its own communication options.\n   If the Max-Forwards field-value is an integer greater than zero, the\n   proxy MUST decrement the field-value when it forwards the request. If\n   no Max-Forwards field is present in the request, then the forwarded\n   request MUST NOT include a Max-Forwards field.\n\n\n\n\nFielding, et al                                                [Page 49]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n9.3 GET\n\n   The GET method means retrieve whatever information (in the form of an\n   entity) is identified by the Request-URI. If the Request-URI refers\n   to a data-producing process, it is the produced data which shall be\n   returned as the entity in the response and not the source text of the\n   process, unless that text happens to be the output of the process.\n\n   The semantics of the GET method change to a \"conditional GET\" if the\n   request message includes an If-Modified-Since, If-Unmodified-Since,\n   If-Match, If-None-Match, or If-Range header field. A conditional GET\n   method requests that the entity be transferred only under the\n   circumstances described by the conditional header field(s). The\n   conditional GET method is intended to reduce unnecessary network\n   usage by allowing cached entities to be refreshed without requiring\n   multiple requests or transferring data already held by the client.\n\n   The semantics of the GET method change to a \"partial GET\" if the\n   request message includes a Range header field. A partial GET requests\n   that only part of the entity be transferred, as described in section\n   14.35. The partial GET method is intended to reduce unnecessary\n   network usage by allowing partially-retrieved entities to be\n   completed without transferring data already held by the client.\n\n   The response to a GET request is cachable if and only if it meets the\n   requirements for HTTP caching described in section 13.\n\n   See section 15.1.3 for security considerations when used for forms.\n\n\n9.4 HEAD\n\n   The HEAD method is identical to GET except that the server MUST NOT\n   return a message-body in the response. The metainformation contained\n   in the HTTP headers in response to a HEAD request SHOULD be identical\n   to the information sent in response to a GET request. This method can\n   be used for obtaining metainformation about the entity implied by the\n   request without transferring the entity-body itself. This method is\n   often used for testing hypertext links for validity, accessibility,\n   and recent modification.\n\n   The response to a HEAD request MAY be cachable in the sense that the\n   information contained in the response MAY be used to update a\n   previously cached entity from that resource. If the new field values\n   indicate that the cached entity differs from the current entity (as\n   would be indicated by a change in Content-Length, Content-MD5, ETag\n   or Last-Modified), then the cache MUST treat the cache entry as\n   stale.\n\n\n9.5 POST\n\n   The POST method is used to request that the origin server accept the\n   entity enclosed in the request as a new subordinate of the resource\n\n\nFielding, et al                                                [Page 50]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   identified by the Request-URI in the Request-Line. POST is designed\n   to allow a uniform method to cover the following functions:\n\n\n     .  Annotation of existing resources;\n\n     .  Posting a message to a bulletin board, newsgroup, mailing list,\n        or similar group of articles;\n\n     .  Providing a block of data, such as the result of submitting a\n        form, to a data-handling process;\n\n     .  Extending a database through an append operation.\n   The actual function performed by the POST method is determined by the\n   server and is usually dependent on the Request-URI. The posted entity\n   is subordinate to that URI in the same way that a file is subordinate\n   to a directory containing it, a news article is subordinate to a\n   newsgroup to which it is posted, or a record is subordinate to a\n   database.\n\n   The action performed by the POST method might not result in a\n   resource that can be identified by a URI. In this case, either 200\n   (OK) or 204 (No Content) is the appropriate response status,\n   depending on whether or not the response includes an entity that\n   describes the result.\n\n   If a resource has been created on the origin server, the response\n   SHOULD be 201 (Created) and contain an entity which describes the\n   status of the request and refers to the new resource, and a Location\n   header (see section 14.30).\n\n   Responses to this method are not cachable, unless the response\n   includes appropriate Cache-Control or Expires header fields. However,\n   the 303 (See Other) response can be used to direct the user agent to\n   retrieve a cachable resource.\n\n   POST requests MUST obey the message transmission requirements set out\n   in section 8.2.\n\n   See section 15.1.3 for security considerations.\n\n\n9.6 PUT\n\n   The PUT method requests that the enclosed entity be stored under the\n   supplied Request-URI. If the Request-URI refers to an already\n   existing resource, the enclosed entity SHOULD be considered as a\n   modified version of the one residing on the origin server. If the\n   Request-URI does not point to an existing resource, and that URI is\n   capable of being defined as a new resource by the requesting user\n   agent, the origin server can create the resource with that URI. If a\n   new resource is created, the origin server MUST inform the user agent\n   via the 201 (Created) response. If an existing resource is modified,\n   either the 200 (OK) or 204 (No Content) response codes SHOULD be sent\n   to indicate successful completion of the request. If the resource\n\nFielding, et al                                                [Page 51]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   could not be created or modified with the Request-URI, an appropriate\n   error response SHOULD be given that reflects the nature of the\n   problem. The recipient of the entity MUST NOT ignore any Content-*\n   (e.g. Content-Range) headers that it does not understand or implement\n   and MUST return a 501 (Not Implemented) response in such cases.\n\n   If the request passes through a cache and the Request-URI identifies\n   one or more currently cached entities, those entries SHOULD be\n   treated as stale. Responses to this method are not cachable.\n\n   The fundamental difference between the POST and PUT requests is\n   reflected in the different meaning of the Request-URI. The URI in a\n   POST request identifies the resource that will handle the enclosed\n   entity. That resource might be a data-accepting process, a gateway to\n   some other protocol, or a separate entity that accepts annotations.\n   In contrast, the URI in a PUT request identifies the entity enclosed\n   with the request -- the user agent knows what URI is intended and the\n   server MUST NOT attempt to apply the request to some other resource.\n   If the server desires that the request be applied to a different URI,\n   it MUST send a 301 (Moved Permanently) response; the user agent MAY\n   then make its own decision regarding whether or not to redirect the\n   request.\n\n   A single resource MAY be identified by many different URIs. For\n   example, an article might have a URI for identifying \"the current\n   version\" which is separate from the URI identifying each particular\n   version. In this case, a PUT request on a general URI might result in\n   several other URIs being defined by the origin server.\n\n   HTTP/1.1 does not define how a PUT method affects the state of an\n   origin server.\n\n   PUT requests MUST obey the message transmission requirements set out\n   in section 8.2.\n\n   Unless otherwise specified for a particular entity-header, the\n   entity-headers in the PUT request SHOULD be applied to the resource\n   created or modified by the PUT.\n\n\n9.7 DELETE\n\n   The DELETE method requests that the origin server delete the resource\n   identified by the Request-URI. This method MAY be overridden by human\n   intervention (or other means) on the origin server. The client cannot\n   be guaranteed that the operation has been carried out, even if the\n   status code returned from the origin server indicates that the action\n   has been completed successfully. However, the server SHOULD NOT\n   indicate success unless, at the time the response is given, it\n   intends to delete the resource or move it to an inaccessible\n   location.\n\n   A successful response SHOULD be 200 (OK) if the response includes an\n   entity describing the status, 202 (Accepted) if the action has not\n\n\nFielding, et al                                                [Page 52]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   yet been enacted, or 204 (No Content) if the action has been enacted\n   but the response does not include an entity.\n\n   If the request passes through a cache and the Request-URI identifies\n   one or more currently cached entities, those entries SHOULD be\n   treated as stale. Responses to this method are not cachable.\n\n\n9.8 TRACE\n\n   The TRACE method is used to invoke a remote, application-layer loop-\n   back of the request message. The final recipient of the request\n   SHOULD reflect the message received back to the client as the entity-\n   body of a 200 (OK) response. The final recipient is either the origin\n   server or the first proxy or gateway to receive a Max-Forwards value\n   of zero (0) in the request (see section 14.31). A TRACE request MUST\n   NOT include an entity.\n\n   TRACE allows the client to see what is being received at the other\n   end of the request chain and use that data for testing or diagnostic\n   information. The value of the Via header field (section 14.45) is of\n   particular interest, since it acts as a trace of the request chain.\n   Use of the Max-Forwards header field allows the client to limit the\n   length of the request chain, which is useful for testing a chain of\n   proxies forwarding messages in an infinite loop.\n\n   If the request is valid, the response SHOULD contain the entire\n   request message in the entity-body, with a Content-Type of\n   \"message/http\". Responses to this method MUST NOT be cached.\n\n\n9.9 CONNECT\n\n   This specification reserves the method name CONNECT for use for use\n   with a proxy that can dynamically switch to being a tunnel (e.g. SSL\n   tunneling [44]).\n\n\n10 Status Code Definitions\n\n   Each Status-Code is described below, including a description of which\n   method(s) it can follow and any metainformation required in the\n   response.\n\n\n10.1 Informational 1xx\n\n   This class of status code indicates a provisional response,\n   consisting only of the Status-Line and optional headers, and is\n   terminated by an empty line. There are no required headers for this\n   class of status code. Since HTTP/1.0 did not define any 1xx status\n   codes, servers MUST NOT send a 1xx response to an HTTP/1.0 client\n   except under experimental conditions.\n\n\n\nFielding, et al                                                [Page 53]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   A client MUST be prepared to accept one or more 1xx status responses\n   prior to a regular response, even if the client does not expect a 100\n   (Continue) status message. Unexpected 1xx status responses MAY be\n   ignored by a user agent.\n\n   Proxies MUST forward 1xx responses, unless the connection between the\n   proxy and its client has been closed, or unless the proxy itself\n   requested the generation of the 1xx response. (For example, if a\n   proxy adds a \"Expect: 100-continue\" field when it forwards a request,\n   then it need not forward the corresponding 100 (Continue)\n   response(s).)\n\n\n10.1.1 100 Continue\n\n   The client SHOULD continue with its request. This interim response is\n   used to inform the client that the initial part of the request has\n   been received and has not yet been rejected by the server. The client\n   SHOULD continue by sending the remainder of the request or, if the\n   request has already been completed, ignore this response. The server\n   MUST send a final response after the request has been completed. See\n   section 8.2.4 for detailed discussion of the use and handling of this\n   status code.\n\n\n10.1.2 101 Switching Protocols\n\n   The server understands and is willing to comply with the client's\n   request, via the Upgrade message header field (section 14.42), for a\n   change in the application protocol being used on this connection. The\n   server will switch protocols to those defined by the response's\n   Upgrade header field immediately after the empty line which\n   terminates the 101 response.\n\n   The protocol SHOULD be switched only when it is advantageous to do\n   so. For example, switching to a newer version of HTTP is advantageous\n   over older versions, and switching to a real-time, synchronous\n   protocol might be advantageous when delivering resources that use\n   such features.\n\n\n10.2 Successful 2xx\n\n   This class of status code indicates that the client's request was\n   successfully received, understood, and accepted.\n\n\n10.2.1 200 OK\n\n   The request has succeeded. The information returned with the response\n   is dependent on the method used in the request, for example:\n\n      GET  an entity corresponding to the requested resource is sent in\n           the response;\n\n\nFielding, et al                                                [Page 54]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n      HEAD the entity-header fields corresponding to the requested\n           resource are sent in the response without any message-body;\n\n      POST an entity describing or containing the result of the action;\n\n      TRACE an entity containing the request message as received by the\n           end server.\n\n\n10.2.2 201 Created\n\n   The request has been fulfilled and resulted in a new resource being\n   created. The newly created resource can be referenced by the URI(s)\n   returned in the entity of the response, with the most specific URI\n   for the resource given by a Location header field. The response\n   SHOULD include an entity containing a list of resource\n   characteristics and location(s) from which the user or user agent can\n   choose the one most appropriate. The entity format is specified by\n   the media type given in the Content-Type header field. The origin\n   server MUST create the resource before returning the 201 status code.\n   If the action cannot be carried out immediately, the server SHOULD\n   respond with 202 (Accepted) response instead.\n\n   A 201 response MAY contain an ETag response header field indicating\n   the current value of the entity tag for the requested variant just\n   created, see section 14.19.\n\n\n10.2.3 202 Accepted\n\n   The request has been accepted for processing, but the processing has\n   not been completed.  The request might or might not eventually be\n   acted upon, as it might be disallowed when processing actually takes\n   place. There is no facility for re-sending a status code from an\n   asynchronous operation such as this.\n\n   The 202 response is intentionally non-committal. Its purpose is to\n   allow a server to accept a request for some other process (perhaps a\n   batch-oriented process that is only run once per day) without\n   requiring that the user agent's connection to the server persist\n   until the process is completed. The entity returned with this\n   response SHOULD include an indication of the request's current status\n   and either a pointer to a status monitor or some estimate of when the\n   user can expect the request to be fulfilled.\n\n\n10.2.4 203 Non-Authoritative Information\n\n   The returned metainformation in the entity-header is not the\n   definitive set as available from the origin server, but is gathered\n   from a local or a third-party copy. The set presented MAY be a subset\n   or superset of the original version. For example, including local\n   annotation information about the resource might result in a superset\n   of the metainformation known by the origin server. Use of this\n\n\nFielding, et al                                                [Page 55]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   response code is not required and is only appropriate when the\n   response would otherwise be 200 (OK).\n\n\n10.2.5 204 No Content\n\n   The server has fulfilled the request but does not need to return an\n   entity-body, and might want to return updated metainformation. The\n   response MAY include new or updated metainformation in the form of\n   entity-headers, which if present SHOULD be associated with the\n   requested variant.\n\n   If the client is a user agent, it SHOULD NOT change its document view\n   from that which caused the request to be sent. This response is\n   primarily intended to allow input for actions to take place without\n   causing a change to the user agent's active document view, although\n   any new or updated metainformation SHOULD be applied to the document\n   currently in the user agent's active view.\n\n   The 204 response MUST NOT include a message-body, and thus is always\n   terminated by the first empty line after the header fields.\n\n\n10.2.6 205 Reset Content\n\n   The server has fulfilled the request and the user agent SHOULD reset\n   the document view which caused the request to be sent. This response\n   is primarily intended to allow input for actions to take place via\n   user input, followed by a clearing of the form in which the input is\n   given so that the user can easily initiate another input action. The\n   response MUST NOT include an entity.\n\n\n10.2.7 206 Partial Content\n\n   The server has fulfilled the partial GET request for the resource.\n   The request MUST have included a Range header field (section 14.35)\n   indicating the desired range, and MAY have included an If-Range\n   header field (section 14.27) to make the request conditional.\n\n   The response MUST include the following header fields:\n\n     .  Either a Content-Range header field (section 14.16) indicating\n        the range included with this response, or a multipart/byteranges\n        Content-Type including Content-Range fields for each part. If a\n        Content-Length header field is present in the response, its\n        value MUST match the actual number of OCTETs transmitted in the\n        message-body.\n\n     .  Date\n\n     .  ETag and/or Content-Location, if the header would have been sent\n        in a 200 response to the same request\n\n\n\nFielding, et al                                                [Page 56]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     .  Expires, Cache-Control, and/or Vary, if the field-value might\n        differ from that sent in any previous response for the same\n        variant\n\n   If the 206 response is the result of an If-Range request that used a\n   strong cache validator (see section 13.3.3), the response SHOULD NOT\n   include other entity-headers. If the response is the result of an If-\n   Range request that used a weak validator, the response MUST NOT\n   include other entity-headers; this prevents inconsistencies between\n   cached entity-bodies and updated headers. Otherwise, the response\n   MUST include all of the entity-headers that would have been returned\n   with a 200 (OK) response to the same request.\n\n   A cache MUST NOT combine a 206 response with other previously cached\n   content if the ETag or Last-Modified headers do not match exactly,\n   see 13.5.4.\n\n   A cache that does not support the Range and Content-Range headers\n   MUST NOT cache 206 (Partial) responses.\n\n\n10.3 Redirection 3xx\n\n   This class of status code indicates that further action needs to be\n   taken by the user agent in order to fulfill the request.  The action\n   required MAY be carried out by the user agent without interaction\n   with the user if and only if the method used in the second request is\n   GET or HEAD. A client SHOULD detect infinite redirection loops, since\n   such loops generate network traffic for each redirection.\n\n     Note: previous versions of this specification recommended a\n     maximum of five redirections. Content developers should be aware\n     that there might be clients that implement such a fixed\n     limitation.\n\n\n10.3.1 300 Multiple Choices\n\n   The requested resource corresponds to any one of a set of\n   representations, each with its own specific location, and agent-\n   driven negotiation information (section 12) is being provided so that\n   the user (or user agent) can select a preferred representation and\n   redirect its request to that location.\n\n   Unless it was a HEAD request, the response SHOULD include an entity\n   containing a list of resource characteristics and location(s) from\n   which the user or user agent can choose the one most appropriate. The\n   entity format is specified by the media type given in the Content-\n   Type header field. Depending upon the format and the capabilities of\n   the user agent, selection of the most appropriate choice MAY be\n   performed automatically. However, this specification does not define\n   any standard for such automatic selection.\n\n   If the server has a preferred choice of representation, it SHOULD\n   include the specific URI for that representation in the Location\n\nFielding, et al                                                [Page 57]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   field; user agents MAY use the Location field value for automatic\n   redirection. This response is cachable unless indicated otherwise.\n\n\n10.3.2 301 Moved Permanently\n\n   The requested resource has been assigned a new permanent URI and any\n   future references to this resource SHOULD be done using one of the\n   returned URIs.  Clients with link editing capabilities SHOULD\n   automatically re-link references to the Request-URI to one or more of\n   the new references returned by the server, where possible. This\n   response is cachable unless indicated otherwise.\n\n   The new permanent URI SHOULD be given by the Location field in the\n   response. Unless the request method was HEAD, the entity of the\n   response SHOULD contain a short hypertext note with a hyperlink to\n   the new URI(s).\n\n   If the 301 status code is received in response to a request other\n   than GET or HEAD, the user agent MUST NOT automatically redirect the\n   request unless it can be confirmed by the user, since this might\n   change the conditions under which the request was issued.\n\n     Note: When automatically redirecting a POST request after\n     receiving a 301 status code, some existing HTTP/1.0 user agents\n     will erroneously change it into a GET request.\n\n\n10.3.3 302 Found\n\n   The requested resource resides temporarily under a different URI.\n   Since the redirection might be altered on occasion, the client SHOULD\n   continue to use the Request-URI for future requests.  This response\n   is only cachable if indicated by a Cache-Control or Expires header\n   field.\n\n   The temporary URI SHOULD be given by the Location field in the\n   response. Unless the request method was HEAD, the entity of the\n   response SHOULD contain a short hypertext note with a hyperlink to\n   the new URI(s).\n\n   If the 302 status code is received in response to a request other\n   than GET or HEAD, the user agent MUST NOT automatically redirect the\n   request unless it can be confirmed by the user, since this might\n   change the conditions under which the request was issued.\n\n     Note: RFC 1945 and RFC 2068 specify that the client is not\n     allowed to change the method on the redirected request. However,\n     most existing user agent implementations treat 302 as if it were\n     a 303 response, performing a GET on the Location field-value\n     regardless of the original request method. The status codes 303\n     and 307 have been added for servers that wish to make\n     unambiguously clear which kind of reaction is expected of the\n     client.\n\n\nFielding, et al                                                [Page 58]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n10.3.4 303 See Other\n\n   The response to the request can be found under a different URI and\n   SHOULD be retrieved using a GET method on that resource. This method\n   exists primarily to allow the output of a POST-activated script to\n   redirect the user agent to a selected resource. The new URI is not a\n   substitute reference for the originally requested resource. The 303\n   response MUST NOT be cached, but the response to the second\n   (redirected) request might be cachable.\n\n   The different URI SHOULD be given by the Location field in the\n   response. Unless the request method was HEAD, the entity of the\n   response SHOULD contain a short hypertext note with a hyperlink to\n   the new URI(s).\n\n     Note: Many pre-HTTP/1.1 user agents do not understand the 303\n     status. When interoperability with such clients is a concern,\n     the 302 status code may be used instead, since most user agents\n     react to a 302 response as described here for 303.\n\n\n10.3.5 304 Not Modified\n\n   If the client has performed a conditional GET request and access is\n   allowed, but the document has not been modified, the server SHOULD\n   respond with this status code. The response MUST NOT contain a\n   message-body.\n\n   The response MUST include the following header fields:\n\n      . Date, unless its omission is required by section 14.18.1. If a\n        clockless origin server obeys these rules, and proxies and\n        clients add their own Date to any response received without one\n        (as already specified by [RFC 2068], section 14.19), caches\n        will operate correctly.\n      . ETag and/or Content-Location, if the header would have been\n        sent in a 200 response to the same request\n      . Expires, Cache-Control, and/or Vary, if the field-value might\n        differ from that sent in any previous response for the same\n        variant\n   If the conditional GET used a strong cache validator (see section\n   13.3.3), the response SHOULD NOT include other entity-headers.\n   Otherwise (i.e., the conditional GET used a weak validator), the\n   response MUST NOT include other entity-headers; this prevents\n   inconsistencies between cached entity-bodies and updated headers.\n\n   If a 304 response indicates an entity not currently cached, then the\n   cache MUST disregard the response and repeat the request without the\n   conditional.\n\n   If a cache uses a received 304 response to update a cache entry, the\n   cache MUST update the entry to reflect any new field values given in\n   the response.\n\n\n\nFielding, et al                                                [Page 59]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   The 304 response MUST NOT include a message-body, and thus is always\n   terminated by the first empty line after the header fields.\n\n\n10.3.6 305 Use Proxy\n\n   The requested resource MUST be accessed through the proxy given by\n   the Location field. The Location field gives the URI of the proxy.\n   The recipient is expected to repeat this single request via the\n   proxy. 305 responses MUST only be generated by origin servers.\n\n     Note: RFC 2068 was not clear that 305 was intended to redirect a\n     single request, and to be generated by origin servers only. Not\n     observing these limitations has significant security\n     consequences.\n\n\n10.3.7 306 (Unused)\n\n   The 306 status code was used in a previous version of the\n   specification, and is no longer used, and the code is reserved.\n\n\n10.3.8 307 Temporary Redirect\n\n   The requested resource resides temporarily under a different URI.\n   Since the redirection MAY be altered on occasion, the client SHOULD\n   continue to use the Request-URI for future requests.  This response\n   is only cachable if indicated by a Cache-Control or Expires header\n   field.\n\n   The temporary URI SHOULD be given by the Location field in the\n   response. Unless the request method was HEAD, the entity of the\n   response SHOULD contain a short hypertext note with a hyperlink to\n   the new URI(s).\n\n   If the 307 status code is received in response to a request other\n   than GET or HEAD, the user agent MUST NOT automatically redirect the\n   request unless it can be confirmed by the user, since this might\n   change the conditions under which the request was issued.\n\n   Many pre-HTTP/1.1 user agents do not understand the 307 status;\n   therefore, an appropriate response entity SHOULD contain the\n   information necessary for a user to repeat the original request on\n   the new URI.\n\n\n10.4 Client Error 4xx\n\n   The 4xx class of status code is intended for cases in which the\n   client seems to have erred. Except when responding to a HEAD request,\n   the server SHOULD include an entity containing an explanation of the\n   error situation, and whether it is a temporary or permanent\n   condition. These status codes are applicable to any request method.\n   User agents SHOULD display any included entity to the user.\n\nFielding, et al                                                [Page 60]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   If the client is sending data, a server implementation using TCP\n   SHOULD be careful to ensure that the client acknowledges receipt of\n   the packet(s) containing the response, before the server closes the\n   input connection. If the client continues sending data to the server\n   after the close, the server's TCP stack will send a reset packet to\n   the client, which may erase the client's unacknowledged input buffers\n   before they can be read and interpreted by the HTTP application.\n\n\n10.4.1 400 Bad Request\n\n   The request could not be understood by the server due to malformed\n   syntax. The client SHOULD NOT repeat the request without\n   modifications.\n\n\n10.4.2 401 Unauthorized\n\n   The request requires user authentication. The response MUST include a\n   WWW-Authenticate header field (section 14.47) containing a challenge\n   applicable to the requested resource. The client MAY repeat the\n   request with a suitable Authorization header field (section 14.8). If\n   the request already included Authorization credentials, then the 401\n   response indicates that authorization has been refused for those\n   credentials. If the 401 response contains the same challenge as the\n   prior response, and the user agent has already attempted\n   authentication at least once, then the user SHOULD be presented the\n   entity that was given in the response, since that entity might\n   include relevant diagnostic information. HTTP access authentication\n   is explained in \"HTTP Authentication: Basic and Digest Access\n   Authentication\" [43].\n\n\n10.4.3 402 Payment Required\n\n   This code is reserved for future use.\n\n\n10.4.4 403 Forbidden\n\n   The server understood the request, but is refusing to fulfill it.\n   Authorization will not help and the request SHOULD NOT be repeated.\n   If the request method was not HEAD and the server wishes to make\n   public why the request has not been fulfilled, it SHOULD describe the\n   reason for the refusal in the entity.  If the server does not wish to\n   make this information available to the client, the status code 404\n   (Not Found) can be used instead.\n\n\n10.4.5 404 Not Found\n\n   The server has not found anything matching the Request-URI. No\n   indication is given of whether the condition is temporary or\n   permanent. The 410 (Gone) status code SHOULD be used if the server\n   knows, through some internally configurable mechanism, that an old\n\nFielding, et al                                                [Page 61]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   resource is permanently unavailable and has no forwarding address.\n   This status code is commonly used when the server does not wish to\n   reveal exactly why the request has been refused, or when no other\n   response is applicable.\n\n\n10.4.6 405 Method Not Allowed\n\n   The method specified in the Request-Line is not allowed for the\n   resource identified by the Request-URI. The response MUST include an\n   Allow header containing a list of valid methods for the requested\n   resource.\n\n\n10.4.7 406 Not Acceptable\n\n   The resource identified by the request is only capable of generating\n   response entities which have content characteristics not acceptable\n   according to the accept headers sent in the request.\n\n   Unless it was a HEAD request, the response SHOULD include an entity\n   containing a list of available entity characteristics and location(s)\n   from which the user or user agent can choose the one most\n   appropriate. The entity format is specified by the media type given\n   in the Content-Type header field. Depending upon the format and the\n   capabilities of the user agent, selection of the most appropriate\n   choice MAY be performed automatically. However, this specification\n   does not define any standard for such automatic selection.\n\n     Note: HTTP/1.1 servers are allowed to return responses which are\n     not acceptable according to the accept headers sent in the\n     request. In some cases, this may even be preferable to sending a\n     406 response. User agents are encouraged to inspect the headers\n     of an incoming response to determine if it is acceptable.\n\n   If the response could be unacceptable, a user agent SHOULD\n   temporarily stop receipt of more data and query the user for a\n   decision on further actions.\n\n\n10.4.8 407 Proxy Authentication Required\n\n   This code is similar to 401 (Unauthorized), but indicates that the\n   client MUST first authenticate itself with the proxy. The proxy MUST\n   return a Proxy-Authenticate header field (section 14.33) containing a\n   challenge applicable to the proxy for the requested resource. The\n   client MAY repeat the request with a suitable Proxy-Authorization\n   header field (section 14.34). HTTP access authentication is explained\n   in \"HTTP Authentication: Basic and Digest Access Authentication\"\n   [43].\n\n\n\n\n\n\nFielding, et al                                                [Page 62]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n10.4.9 408 Request Timeout\n\n   The client did not produce a request within the time that the server\n   was prepared to wait. The client MAY repeat the request without\n   modifications at any later time.\n\n\n10.4.10 409 Conflict\n\n   The request could not be completed due to a conflict with the current\n   state of the resource. This code is only allowed in situations where\n   it is expected that the user might be able to resolve the conflict\n   and resubmit the request. The response body SHOULD include enough\n   information for the user to recognize the source of the conflict.\n   Ideally, the response entity would include enough information for the\n   user or user agent to fix the problem; however, that might not be\n   possible and is not required.\n\n   Conflicts are most likely to occur in response to a PUT request. For\n   example, if versioning were being used and the entity being PUT\n   included changes to a resource which conflict with those made by an\n   earlier (third-party) request, the server might use the 409 response\n   to indicate that it can't complete the request. In this case, the\n   response entity would likely contain a list of the differences\n   between the two versions in a format defined by the response Content-\n   Type.\n\n\n10.4.11 410 Gone\n\n   The requested resource is no longer available at the server and no\n   forwarding address is known. This condition is expected to be\n   considered permanent. Clients with link editing capabilities SHOULD\n   delete references to the Request-URI after user approval. If the\n   server does not know, or has no facility to determine, whether or not\n   the condition is permanent, the status code 404 (Not Found) SHOULD be\n   used instead. This response is cachable unless indicated otherwise.\n\n   The 410 response is primarily intended to assist the task of web\n   maintenance by notifying the recipient that the resource is\n   intentionally unavailable and that the server owners desire that\n   remote links to that resource be removed. Such an event is common for\n   limited-time, promotional services and for resources belonging to\n   individuals no longer working at the server's site. It is not\n   necessary to mark all permanently unavailable resources as \"gone\" or\n   to keep the mark for any length of time -- that is left to the\n   discretion of the server owner.\n\n\n10.4.12 411 Length Required\n\n   The server refuses to accept the request without a defined Content-\n   Length. The client MAY repeat the request if it adds a valid Content-\n   Length header field containing the length of the message-body in the\n   request message.\n\nFielding, et al                                                [Page 63]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n10.4.13 412 Precondition Failed\n\n   The precondition given in one or more of the request-header fields\n   evaluated to false when it was tested on the server. This response\n   code allows the client to place preconditions on the current resource\n   metainformation (header field data) and thus prevent the requested\n   method from being applied to a resource other than the one intended.\n\n\n10.4.14 413 Request Entity Too Large\n\n   The server is refusing to process a request because the request\n   entity is larger than the server is willing or able to process. The\n   server MAY close the connection to prevent the client from continuing\n   the request.\n\n   If the condition is temporary, the server SHOULD include a Retry-\n   After header field to indicate that it is temporary and after what\n   time the client MAY try again.\n\n\n10.4.15 414 Request-URI Too Long\n\n   The server is refusing to service the request because the Request-URI\n   is longer than the server is willing to interpret. This rare\n   condition is only likely to occur when a client has improperly\n   converted a POST request to a GET request with long query\n   information, when the client has descended into a URI \"black hole\" of\n   redirection (e.g., a redirected URI prefix that points to a suffix of\n   itself), or when the server is under attack by a client attempting to\n   exploit security holes present in some servers using fixed-length\n   buffers for reading or manipulating the Request-URI.\n\n\n10.4.16 415 Unsupported Media Type\n\n   The server is refusing to service the request because the entity of\n   the request is in a format not supported by the requested resource\n   for the requested method.\n\n\n10.4.17 416 Requested Range Not Satisfiable\n\n   A server SHOULD return a response with this status code if a request\n   included a Range request-header field (section 14.35) , and none of\n   the range-specifier values in this field overlap the current extent\n   of the selected resource, and the request did not include an If-Range\n   request-header field. (For byte-ranges, this means that the first-\n   byte-pos of all of the byte-range-spec values were greater than the\n   current length of the selected resource.)\n\n   When this status code is returned for a byte-range request, the\n   response SHOULD include a Content-Range entity-header field\n   specifying the current length of the selected resource (see section\n\n\nFielding, et al                                                [Page 64]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   14.16). This response MUST NOT use the multipart/byteranges content-\n   type.\n\n\n10.4.18 417 Expectation Failed\n\n   The expectation given in an Expect request-header field (see section\n   14.20) could not be met by this server, or, if the server is a proxy,\n   the server has unambiguous evidence that the request could not be met\n   by the next-hop server.\n\n\n10.5 Server Error 5xx\n\n   Response status codes beginning with the digit \"5\" indicate cases in\n   which the server is aware that it has erred or is incapable of\n   performing the request. Except when responding to a HEAD request, the\n   server SHOULD include an entity containing an explanation of the\n   error situation, and whether it is a temporary or permanent\n   condition. User agents SHOULD display any included entity to the\n   user. These response codes are applicable to any request method.\n\n\n10.5.1 500 Internal Server Error\n\n   The server encountered an unexpected condition which prevented it\n   from fulfilling the request.\n\n\n10.5.2 501 Not Implemented\n\n   The server does not support the functionality required to fulfill the\n   request. This is the appropriate response when the server does not\n   recognize the request method and is not capable of supporting it for\n   any resource.\n\n\n10.5.3 502 Bad Gateway\n\n   The server, while acting as a gateway or proxy, received an invalid\n   response from the upstream server it accessed in attempting to\n   fulfill the request.\n\n\n10.5.4 503 Service Unavailable\n\n   The server is currently unable to handle the request due to a\n   temporary overloading or maintenance of the server. The implication\n   is that this is a temporary condition which will be alleviated after\n   some delay. If known, the length of the delay MAY be indicated in a\n   Retry-After header. If no Retry-After is given, the client SHOULD\n   handle the response as it would for a 500 response.\n\n\n\n\nFielding, et al                                                [Page 65]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     Note: The existence of the 503 status code does not imply that a\n     server must use it when becoming overloaded. Some servers may\n     wish to simply refuse the connection.\n\n\n10.5.5 504 Gateway Timeout\n\n   The server, while acting as a gateway or proxy, did not receive a\n   timely response from the upstream server specified by the URI (e.g.\n   HTTP, FTP, LDAP) or some other auxiliary server (e.g. DNS) it needed\n   to access in attempting to complete the request.\n\n     Note: Note to implementors: some deployed proxies are known to\n     return 400 or 500 when DNS lookups time out.\n\n\n10.5.6 505 HTTP Version Not Supported\n\n   The server does not support, or refuses to support, the HTTP protocol\n   version that was used in the request message. The server is\n   indicating that it is unable or unwilling to complete the request\n   using the same major version as the client, as described in section\n   3.1, other than with this error message. The response SHOULD contain\n   an entity describing why that version is not supported and what other\n   protocols are supported by that server.\n\n\n11 Access Authentication\n\n   HTTP provides several OPTIONAL challenge-response authentication\n   mechanisms which MAY be used by a server to challenge a client\n   request and by a client to provide authentication information. The\n   general framework for access authentication, and the specification of\n   \"basic\" and \"digest\" authentication, are specified in \"HTTP\n   Authentication: Basic and Digest Access Authentication\" [43]. This\n   specification adopts the definitions of \"challenge\" and \"credentials\"\n   from that specification.\n\n\n12 Content Negotiation\n\n   Most HTTP responses include an entity which contains information for\n   interpretation by a human user. Naturally, it is desirable to supply\n   the user with the \"best available\" entity corresponding to the\n   request. Unfortunately for servers and caches, not all users have the\n   same preferences for what is \"best,\" and not all user agents are\n   equally capable of rendering all entity types. For that reason, HTTP\n   has provisions for several mechanisms for \"content negotiation\" --\n   the process of selecting the best representation for a given response\n   when there are multiple representations available.\n\n     Note: This is not called \"format negotiation\" because the\n     alternate representations may be of the same media type, but use\n     different capabilities of that type, be in different languages,\n     etc.\n\nFielding, et al                                                [Page 66]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Any response containing an entity-body MAY be subject to negotiation,\n   including error responses.\n\n   There are two kinds of content negotiation which are possible in\n   HTTP: server-driven and agent-driven negotiation. These two kinds of\n   negotiation are orthogonal and thus may be used separately or in\n   combination. One method of combination, referred to as transparent\n   negotiation, occurs when a cache uses the agent-driven negotiation\n   information provided by the origin server in order to provide server-\n   driven negotiation for subsequent requests.\n\n\n12.1 Server-driven Negotiation\n\n   If the selection of the best representation for a response is made by\n   an algorithm located at the server, it is called server-driven\n   negotiation. Selection is based on the available representations of\n   the response (the dimensions over which it can vary; e.g. language,\n   content-coding, etc.) and the contents of particular header fields in\n   the request message or on other information pertaining to the request\n   (such as the network address of the client).\n\n   Server-driven negotiation is advantageous when the algorithm for\n   selecting from among the available representations is difficult to\n   describe to the user agent, or when the server desires to send its\n   \"best guess\" to the client along with the first response (hoping to\n   avoid the round-trip delay of a subsequent request if the \"best\n   guess\" is good enough for the user). In order to improve the server's\n   guess, the user agent MAY include request header fields (Accept,\n   Accept-Language, Accept-Encoding, etc.) which describe its\n   preferences for such a response.\n\n   Server-driven negotiation has disadvantages:\n\n     1. It is impossible for the server to accurately determine what\n        might be \"best\" for any given user, since that would require\n        complete knowledge of both the capabilities of the user agent\n        and the intended use for the response (e.g., does the user want\n        to view it on screen or print it on paper?).\n\n     2. Having the user agent describe its capabilities in every request\n        can be both very inefficient (given that only a small percentage\n        of responses have multiple representations) and a potential\n        violation of the user's privacy.\n\n     3. It complicates the implementation of an origin server and the\n        algorithms for generating responses to a request.\n\n     4. It may limit a public cache's ability to use the same response\n        for multiple user's requests.\n\n   HTTP/1.1 includes the following request-header fields for enabling\n   server-driven negotiation through description of user agent\n   capabilities and user preferences: Accept (section 14.1), Accept-\n   Charset (section 14.2), Accept-Encoding (section 14.3), Accept-\n\nFielding, et al                                                [Page 67]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Language (section 14.4), and User-Agent (section 14.43). However, an\n   origin server is not limited to these dimensions and MAY vary the\n   response based on any aspect of the request, including information\n   outside the request-header fields or within extension header fields\n   not defined by this specification.\n\n   The Vary  header field can be used to express the parameters the\n   server uses to select a representation that is subject to server-\n   driven negotiation. See section 13.6 for use of the Vary header field\n   by caches and section 14.44 for use of the Vary header field by\n   servers.\n\n\n12.2 Agent-driven Negotiation\n\n   With agent-driven negotiation, selection of the best representation\n   for a response is performed by the user agent after receiving an\n   initial response from the origin server. Selection is based on a list\n   of the available representations of the response included within the\n   header fields (this specification reserves the field-name Alternates)\n   or entity-body of the initial response, with each representation\n   identified by its own URI. Selection from among the representations\n   may be performed automatically (if the user agent is capable of doing\n   so) or manually by the user selecting from a generated (possibly\n   hypertext) menu.\n\n   Agent-driven negotiation is advantageous when the response would vary\n   over commonly-used dimensions (such as type, language, or encoding),\n   when the origin server is unable to determine a user agent's\n   capabilities from examining the request, and generally when public\n   caches are used to distribute server load and reduce network usage.\n\n   Agent-driven negotiation suffers from the disadvantage of needing a\n   second request to obtain the best alternate representation. This\n   second request is only efficient when caching is used. In addition,\n   this specification does not define any mechanism for supporting\n   automatic selection, though it also does not prevent any such\n   mechanism from being developed as an extension and used within\n   HTTP/1.1.\n\n   HTTP/1.1 defines the 300 (Multiple Choices) and 406 (Not Acceptable)\n   status codes for enabling agent-driven negotiation when the server is\n   unwilling or unable to provide a varying response using server-driven\n   negotiation.\n\n\n12.3 Transparent Negotiation\n\n   Transparent negotiation is a combination of both server-driven and\n   agent-driven negotiation. When a cache is supplied with a form of the\n   list of available representations of the response (as in agent-driven\n   negotiation) and the dimensions of variance are completely understood\n   by the cache, then the cache becomes capable of performing server-\n   driven negotiation on behalf of the origin server for subsequent\n   requests on that resource.\n\nFielding, et al                                                [Page 68]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Transparent negotiation has the advantage of distributing the\n   negotiation work that would otherwise be required of the origin\n   server and also removing the second request delay of agent-driven\n   negotiation when the cache is able to correctly guess the right\n   response.\n\n   This specification does not define any mechanism for transparent\n   negotiation, though it also does not prevent any such mechanism from\n   being developed as an extension that could be used within HTTP/1.1.\n\n\n13 Caching in HTTP\n\n   HTTP is typically used for distributed information systems, where\n   performance can be improved by the use of response caches. The\n   HTTP/1.1 protocol includes a number of elements intended to make\n   caching work as well as possible. Because these elements are\n   inextricable from other aspects of the protocol, and because they\n   interact with each other, it is useful to describe the basic caching\n   design of HTTP separately from the detailed descriptions of methods,\n   headers, response codes, etc.\n\n   Caching would be useless if it did not significantly improve\n   performance. The goal of caching in HTTP/1.1 is to eliminate the need\n   to send requests in many cases, and to eliminate the need to send\n   full responses in many other cases. The former reduces the number of\n   network round-trips required for many operations; we use an\n   \"expiration\" mechanism for this purpose (see section 13.2). The\n   latter reduces network bandwidth requirements; we use a \"validation\"\n   mechanism for this purpose (see section 13.3).\n\n   Requirements for performance, availability, and disconnected\n   operation require us to be able to relax the goal of semantic\n   transparency. The HTTP/1.1 protocol allows origin servers, caches,\n   and clients to explicitly reduce transparency when necessary.\n   However, because non-transparent operation may confuse non-expert\n   users, and might be incompatible with certain server applications\n   (such as those for ordering merchandise), the protocol requires that\n   transparency be relaxed\n\n     .  only by an explicit protocol-level request when relaxed by\n        client or origin server\n     .  only with an explicit warning to the end user when relaxed by\n        cache or client\n   Therefore, the HTTP/1.1 protocol provides these important elements:\n\n     1. Protocol features that provide full semantic transparency when\n        this is required by all parties.\n\n     2. Protocol features that allow an origin server or user agent to\n        explicitly request and control non-transparent operation.\n\n     3. Protocol features that allow a cache to attach warnings to\n        responses that do not preserve the requested approximation of\n        semantic transparency.\n\nFielding, et al                                                [Page 69]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   A basic principle is that it must be possible for the clients to\n   detect any potential relaxation of semantic transparency.\n\n     Note: The server, cache, or client implementor might be faced\n     with design decisions not explicitly discussed in this\n     specification. If a decision might affect semantic transparency,\n     the implementor ought to err on the side of maintaining\n     transparency unless a careful and complete analysis shows\n     significant benefits in breaking transparency.\n\n\n13.1.1 Cache Correctness\n\n   A correct cache MUST respond to a request with the most up-to-date\n   response held by the cache that is appropriate to the request (see\n   sections 13.2.5, 13.2.6, and 13.12) which meets one of the following\n   conditions:\n\n     1. It has been checked for equivalence with what the origin server\n        would have returned by revalidating the response with the origin\n        server (section 13.3);\n\n     2. It is \"fresh enough\" (see section 13.2). In the default case,\n        this means it meets the least restrictive freshness requirement\n        of the client, origin server, and cache (see section 14.9); if\n        the origin server so specifies, it is the freshness requirement\n        of the origin server alone.\n\n        If a stored response is not \"fresh enough\" by the most\n        restrictive freshness requirement of both the client and the\n        origin server, in carefully considered circumstances the cache\n        MAY still return the response with the appropriate Warning\n        header (see section 13.1.5 and 14.46), unless such a response is\n        prohibited (e.g., by a \"no-store\" cache-directive, or by a \"no-\n        cache\" cache-request-directive; see section 14.9).\n\n     3. It is an appropriate 304 (Not Modified), 305 (Proxy Redirect),\n        or error (4xx or 5xx) response message.\n\n   If the cache can not communicate with the origin server, then a\n   correct cache SHOULD respond as above if the response can be\n   correctly served from the cache; if not it MUST return an error or\n   warning indicating that there was a communication failure.\n\n   If a cache receives a response (either an entire response, or a 304\n   (Not Modified) response) that it would normally forward to the\n   requesting client, and the received response is no longer fresh, the\n   cache SHOULD forward it to the requesting client without adding a new\n   Warning (but without removing any existing Warning headers). A cache\n   SHOULD NOT attempt to revalidate a response simply because that\n   response became stale in transit; this might lead to an infinite\n   loop. A user agent that receives a stale response without a Warning\n   MAY display a warning indication to the user.\n\n\n\nFielding, et al                                                [Page 70]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n13.1.2 Warnings\n\n   Whenever a cache returns a response that is neither first-hand nor\n   \"fresh enough\" (in the sense of condition 2 in section 13.1.1), it\n   MUST attach a warning to that effect, using a Warning general-header.\n   This warning allows clients to take appropriate action.\n\n   Warnings MAY be used for other purposes, both cache-related and\n   otherwise. The use of a warning, rather than an error status code,\n   distinguish these responses from true failures.\n\n   Warnings come in two categories:\n\n     1. Those that describe the freshness or revalidation status of the\n        response, and so MUST be deleted after a successful revalidation\n        (see section 13.3 for a definition of revalidation).\n\n     2. Those that describe some aspect of the entity body or entity\n        headers that is not rectified by a revalidation, for example, a\n        lossy compression of the entity bodies. These warnings MUST NOT\n        be deleted after a successful revalidation.\n\n   Warnings are assigned 3-digit code numbers. The first digit indicates\n   whether the Warning MUST or MUST NOT be deleted from a cached\n   response after it is successfully revalidated. This specification\n   defines the code numbers and meanings of each currently assigned\n   warning, allowing a client or cache to take automated action in some\n   (but not all) cases.\n\n   HTTP/1.0 caches will cache all Warnings in responses, without\n   deleting the ones in the first category. Warnings in responses that\n   are passed to HTTP/1.0 caches carry an extra warning-date field,\n   which prevents a future HTTP/1.1 recipient from believing an\n   erroneously cached Warning.\n\n   Warnings also carry a warning text. The text MAY be in any\n   appropriate natural language (perhaps based on the client's Accept\n   headers), and include an OPTIONAL indication of what character set is\n   used.\n\n   Multiple warnings MAY be attached to a response (either by the origin\n   server or by a cache), including multiple warnings with the same code\n   number. For example, a server might provide the same warning with\n   texts in both English and Basque.\n\n   When multiple warnings are attached to a response, it might not be\n   practical or reasonable to display all of them to the user. This\n   version of HTTP does not specify strict priority rules for deciding\n   which warnings to display and in what order, but does suggest some\n   heuristics.\n\n   The Warning header and the currently defined warnings are described\n   in section 14.46.\n\n\n\nFielding, et al                                                [Page 71]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n13.1.3 Cache-control Mechanisms\n\n   The basic cache mechanisms in HTTP/1.1 (server-specified expiration\n   times and validators) are implicit directives to caches. In some\n   cases, a server or client might need to provide explicit directives\n   to the HTTP caches. We use the Cache-Control header for this purpose.\n\n   The Cache-Control header allows a client or server to transmit a\n   variety of directives in either requests or responses. These\n   directives typically override the default caching algorithms. As a\n   general rule, if there is any apparent conflict between header\n   values, the most restrictive interpretation is applied (that is, the\n   one that is most likely to preserve semantic transparency). However,\n   in some cases, cache-control directives are explicitly specified as\n   weakening the approximation of semantic transparency (for example,\n   \"max-stale\" or \"public\").\n\n   The cache-control directives are described in detail in section 14.9.\n\n\n13.1.4 Explicit User Agent Warnings\n\n   Many user agents make it possible for users to override the basic\n   caching mechanisms. For example, the user agent might allow the user\n   to specify that cached entities (even explicitly stale ones) are\n   never validated. Or the user agent might habitually add \"Cache-\n   Control: max-stale=3600\" to every request. The user agent SHOULD NOT\n   default to either non-transparent behavior, or behavior that results\n   in abnormally ineffective caching, but MAY be explicitly configured\n   to do so by an explicit action of the user.\n\n   If the user has overridden the basic caching mechanisms, the user\n   agent SHOULD explicitly indicate to the user whenever this results in\n   the display of information that might not meet the server's\n   transparency requirements (in particular, if the displayed entity is\n   known to be stale). Since the protocol normally allows the user agent\n   to determine if responses are stale or not, this indication need only\n   be displayed when this actually happens. The indication need not be a\n   dialog box; it could be an icon (for example, a picture of a rotting\n   fish) or some other indicator.\n\n   If the user has overridden the caching mechanisms in a way that would\n   abnormally reduce the effectiveness of caches, the user agent SHOULD\n   continually indicate this state to the user (for example, by a\n   display of a picture of currency in flames) so that the user does not\n   inadvertently consume excess resources or suffer from excessive\n   latency.\n\n\n13.1.5 Exceptions to the Rules and Warnings\n\n   In some cases, the operator of a cache MAY choose to configure it to\n   return stale responses even when not requested by clients. This\n   decision ought not be made lightly, but may be necessary for reasons\n   of availability or performance, especially when the cache is poorly\n\nFielding, et al                                                [Page 72]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   connected to the origin server. Whenever a cache returns a stale\n   response, it MUST mark it as such (using a Warning header). This\n   allows the client software to alert the user that there might be a\n   potential problem.\n\n   It also allows the user agent to take steps to obtain a first-hand or\n   fresh response. For this reason, a cache SHOULD NOT return a stale\n   response if the client explicitly requests a first-hand or fresh one,\n   unless it is impossible to comply for technical or policy reasons.\n\n\n13.1.6 Client-controlled Behavior\n\n   While the origin server (and to a lesser extent, intermediate caches,\n   by their contribution to the age of a response) are the primary\n   source of expiration information, in some cases the client might need\n   to control a cache's decision about whether to return a cached\n   response without validating it. Clients do this using several\n   directives of the Cache-Control header.\n\n   A client's request MAY specify the maximum age it is willing to\n   accept of an unvalidated response; specifying a value of zero forces\n   the cache(s) to revalidate all responses. A client MAY also specify\n   the minimum time remaining before a response expires. Both of these\n   options increase constraints on the behavior of caches, and so cannot\n   further relax the cache's approximation of semantic transparency.\n\n   A client MAY also specify that it will accept stale responses, up to\n   some maximum amount of staleness. This loosens the constraints on the\n   caches, and so might violate the origin server's specified\n   constraints on semantic transparency, but might be necessary to\n   support disconnected operation, or high availability in the face of\n   poor connectivity.\n\n\n13.2 Expiration Model\n\n\n13.2.1 Server-Specified Expiration\n\n   HTTP caching works best when caches can entirely avoid making\n   requests to the origin server. The primary mechanism for avoiding\n   requests is for an origin server to provide an explicit expiration\n   time in the future, indicating that a response MAY be used to satisfy\n   subsequent requests. In other words, a cache can return a fresh\n   response without first contacting the server.\n\n   Our expectation is that servers will assign future explicit\n   expiration times to responses in the belief that the entity is not\n   likely to change, in a semantically significant way, before the\n   expiration time is reached. This normally preserves semantic\n   transparency, as long as the server's expiration times are carefully\n   chosen.\n\n\n\nFielding, et al                                                [Page 73]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   The expiration mechanism applies only to responses taken from a cache\n   and not to first-hand responses forwarded immediately to the\n   requesting client.\n\n   If an origin server wishes to force a semantically transparent cache\n   to validate every request, it MAY assign an explicit expiration time\n   in the past. This means that the response is always stale, and so the\n   cache SHOULD validate it before using it for subsequent requests. See\n   section 14.9.4 for a more restrictive way to force revalidation.\n\n   If an origin server wishes to force any HTTP/1.1 cache, no matter how\n   it is configured, to validate every request, it SHOULD use the \"must-\n   revalidate\" cache-control directive (see section 14.9).\n\n   Servers specify explicit expiration times using either the Expires\n   header, or the max-age directive of the Cache-Control header.\n\n   An expiration time cannot be used to force a user agent to refresh\n   its display or reload a resource; its semantics apply only to caching\n   mechanisms, and such mechanisms need only check a resource's\n   expiration status when a new request for that resource is initiated.\n   See section 13.13 for an explanation of the difference between caches\n   and history mechanisms.\n\n\n13.2.2 Heuristic Expiration\n\n   Since origin servers do not always provide explicit expiration times,\n   HTTP caches typically assign heuristic expiration times, employing\n   algorithms that use other header values (such as the Last-Modified\n   time) to estimate a plausible expiration time. The HTTP/1.1\n   specification does not provide specific algorithms, but does impose\n   worst-case constraints on their results. Since heuristic expiration\n   times might compromise semantic transparency, they ought to used\n   cautiously, and we encourage origin servers to provide explicit\n   expiration times as much as possible.\n\n\n13.2.3 Age Calculations\n\n   In order to know if a cached entry is fresh, a cache needs to know if\n   its age exceeds its freshness lifetime. We discuss how to calculate\n   the latter in section 13.2.4; this section describes how to calculate\n   the age of a response or cache entry.\n\n   In this discussion, we use the term \"now\" to mean \"the current value\n   of the clock at the host performing the calculation.\" Hosts that use\n   HTTP, but especially hosts running origin servers and caches, SHOULD\n   use NTP [28] or some similar protocol to synchronize their clocks to\n   a globally accurate time standard.\n\n   HTTP/1.1 requires origin servers to send a Date header, if possible,\n   with every response, giving the time at which the response was\n   generated (see section 14.18). We use the term \"date_value\" to denote\n\n\nFielding, et al                                                [Page 74]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   the value of the Date header, in a form appropriate for arithmetic\n   operations.\n\n   HTTP/1.1 uses the Age response-header to convey the estimated age of\n   the response message when obtained from a cache. The Age field value\n   is the cache's estimate of the amount of time since the response was\n   generated or revalidated by the origin server.\n\n   In essence, the Age value is the sum of the time that the response\n   has been resident in each of the caches along the path from the\n   origin server, plus the amount of time it has been in transit along\n   network paths.\n\n   We use the term \"age_value\" to denote the value of the Age header, in\n   a form appropriate for arithmetic operations.\n\n   A response's age can be calculated in two entirely independent ways:\n\n     1. now minus date_value, if the local clock is reasonably well\n        synchronized to the origin server's clock. If the result is\n        negative, the result is replaced by zero.\n\n     2. age_value, if all of the caches along the response path\n        implement HTTP/1.1.\n\n   Given that we have two independent ways to compute the age of a\n   response when it is received, we can combine these as\n\n          corrected_received_age = max(now - date_value, age_value)\n\n   and as long as we have either nearly synchronized clocks or all-\n   HTTP/1.1 paths, one gets a reliable (conservative) result.\n\n   Because of network-imposed delays, some significant interval might\n   pass between the time that a server generates a response and the time\n   it is received at the next outbound cache or client. If uncorrected,\n   this delay could result in improperly low ages.\n\n   Because the request that resulted in the returned Age value must have\n   been initiated prior to that Age value's generation, we can correct\n   for delays imposed by the network by recording the time at which the\n   request was initiated. Then, when an Age value is received, it MUST\n   be interpreted relative to the time the request was initiated, not\n   the time that the response was received. This algorithm results in\n   conservative behavior no matter how much delay is experienced. So, we\n   compute:\n\n         corrected_initial_age = corrected_received_age\n                               + (now - request_time)\n\n   where \"request_time\" is the time (according to the local clock) when\n   the request that elicited this response was sent.\n\n   Summary of age calculation algorithm, when a cache receives a\n   response:\n\nFielding, et al                                                [Page 75]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n         /*\n          * age_value\n          *      is the value of Age: header received by the cache with\n          *              this response.\n          * date_value\n          *      is the value of the origin server's Date: header\n          * request_time\n          *      is the (local) time when the cache made the request\n          *              that resulted in this cached response\n          * response_time\n          *      is the (local) time when the cache received the\n          *              response\n          * now\n          *      is the current (local) time\n          */\n         apparent_age = max(0, response_time - date_value);\n\n         corrected_received_age = max(apparent_age, age_value);\n\n         response_delay = response_time - request_time;\n\n         corrected_initial_age = corrected_received_age\n                         + response_delay;\n\n         resident_time = now - response_time;\n\n         current_age   = corrected_initial_age + resident_time;\n\n   The current_age of a cache entry is calculated by adding the amount\n   of time (in seconds) since the cache entry was last validated by the\n   origin server to the corrected_initial_age. When a response is\n   generated from a cache entry, the server MUST include a single Age\n   header field in the response with a value equal to the cache entry's\n   current_age.\n\n   The presence of an Age header field in a response implies that a\n   response is not first-hand. However, the converse is not true, since\n   the lack of an Age header field in a response does not imply that the\n   response is first-hand unless all caches along the request path are\n   compliant with HTTP/1.1 (i.e., older HTTP caches did not implement\n   the Age header field).\n\n\n13.2.4 Expiration Calculations\n\n   In order to decide whether a response is fresh or stale, we need to\n   compare its freshness lifetime to its age. The age is calculated as\n   described in section 13.2.3; this section describes how to calculate\n   the freshness lifetime, and to determine if a response has expired.\n   In the discussion below, the values can be represented in any form\n   appropriate for arithmetic operations.\n\n   We use the term \"expires_value\" to denote the value of the Expires\n   header. We use the term \"max_age_value\" to denote an appropriate\n\n\nFielding, et al                                                [Page 76]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   value of the number of seconds carried by the \"max-age\" directive of\n   the Cache-Control header in a response (see section 14.9.3).\n\n   The max-age directive takes priority over Expires, so if max-age is\n   present in a response, the calculation is simply:\n\n         freshness_lifetime = max_age_value\n\n   Otherwise, if Expires is present in the response, the calculation is:\n\n         freshness_lifetime = expires_value - date_value\n\n   Note that neither of these calculations is vulnerable to clock skew,\n   since all of the information comes from the origin server.\n\n   If none of Expires, Cache-Control: max-age, or Cache-Control: s-\n   maxage (see section 14.9.3) appears in the response, and the response\n   does not include other restrictions on caching, the cache MAY compute\n   a freshness lifetime using a heuristic. The cache MUST attach Warning\n   113 to any response whose age is more than 24 hours if such warning\n   has not already been added.\n\n   Also, if the response does have a Last-Modified time, the heuristic\n   expiration value SHOULD be no more than some fraction of the interval\n   since that time. A typical setting of this fraction might be 10%.\n\n   The calculation to determine if a response has expired is quite\n   simple:\n\n         response_is_fresh = (freshness_lifetime > current_age)\n\n13.2.5 Disambiguating Expiration Values\n\n   Because expiration values are assigned optimistically, it is possible\n   for two caches to contain fresh values for the same resource that are\n   different.\n\n   If a client performing a retrieval receives a non-first-hand response\n   for a request that was already fresh in its own cache, and the Date\n   header in its existing cache entry is newer than the Date on the new\n   response, then the client MAY ignore the response. If so, it MAY\n   retry the request with a \"Cache-Control: max-age=0\" directive (see\n   section 14.9), to force a check with the origin server.\n\n   If a cache has two fresh responses for the same representation with\n   different validators, it MUST use the one with the more recent Date\n   header. This situation might arise because the cache is pooling\n   responses from other caches, or because a client has asked for a\n   reload or a revalidation of an apparently fresh cache entry.\n\n\n13.2.6 Disambiguating Multiple Responses\n\n   Because a client might be receiving responses via multiple paths, so\n   that some responses flow through one set of caches and other\n\nFielding, et al                                                [Page 77]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   responses flow through a different set of caches, a client might\n   receive responses in an order different from that in which the origin\n   server sent them. We would like the client to use the most recently\n   generated response, even if older responses are still apparently\n   fresh.\n\n   Neither the entity tag nor the expiration value can impose an\n   ordering on responses, since it is possible that a later response\n   intentionally carries an earlier expiration time. The Date values are\n   ordered to a granularity of one second.\n\n   When a client tries to revalidate a cache entry, and the response it\n   receives contains a Date header that appears to be older than the one\n   for the existing entry, then the client SHOULD repeat the request\n   unconditionally, and include\n\n          Cache-Control: max-age=0\n\n   to force any intermediate caches to validate their copies directly\n   with the origin server, or\n\n          Cache-Control: no-cache\n\n   to force any intermediate caches to obtain a new copy from the origin\n   server.\n\n   If the Date values are equal, then the client MAY use either response\n   (or MAY, if it is being extremely prudent, request a new response).\n   Servers MUST NOT depend on clients being able to choose\n   deterministically between responses generated during the same second,\n   if their expiration times overlap.\n\n\n13.3 Validation Model\n\n   When a cache has a stale entry that it would like to use as a\n   response to a client's request, it first has to check with the origin\n   server (or possibly an intermediate cache with a fresh response) to\n   see if its cached entry is still usable. We call this \"validating\"\n   the cache entry. Since we do not want to have to pay the overhead of\n   retransmitting the full response if the cached entry is good, and we\n   do not want to pay the overhead of an extra round trip if the cached\n   entry is invalid, the HTTP/1.1 protocol supports the use of\n   conditional methods.\n\n   The key protocol features for supporting conditional methods are\n   those concerned with \"cache validators.\" When an origin server\n   generates a full response, it attaches some sort of validator to it,\n   which is kept with the cache entry. When a client (user agent or\n   proxy cache) makes a conditional request for a resource for which it\n   has a cache entry, it includes the associated validator in the\n   request.\n\n   The server then checks that validator against the current validator\n   for the entity, and, if they match (see section 13.3.3), it responds\n\nFielding, et al                                                [Page 78]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   with a special status code (usually, 304 (Not Modified)) and no\n   entity-body. Otherwise, it returns a full response (including entity-\n   body). Thus, we avoid transmitting the full response if the validator\n   matches, and we avoid an extra round trip if it does not match.\n\n   In HTTP/1.1, a conditional request looks exactly the same as a normal\n   request for the same resource, except that it carries a special\n   header (which includes the validator) that implicitly turns the\n   method (usually, GET) into a conditional.\n\n   The protocol includes both positive and negative senses of cache-\n   validating conditions. That is, it is possible to request either that\n   a method be performed if and only if a validator matches or if and\n   only if no validators match.\n\n     Note: a response that lacks a validator may still be cached, and\n     served from cache until it expires, unless this is explicitly\n     prohibited by a cache-control directive. However, a cache cannot\n     do a conditional retrieval if it does not have a validator for\n     the entity, which means it will not be refreshable after it\n     expires.\n\n\n13.3.1 Last-modified Dates\n\n   The Last-Modified entity-header field value is often used as a cache\n   validator. In simple terms, a cache entry is considered to be valid\n   if the entity has not been modified since the Last-Modified value.\n\n\n13.3.2 Entity Tag Cache Validators\n\n   The ETag response-header field value, an entity tag, provides for an\n   \"opaque\" cache validator. This might allow more reliable validation\n   in situations where it is inconvenient to store modification dates,\n   where the one-second resolution of HTTP date values is not\n   sufficient, or where the origin server wishes to avoid certain\n   paradoxes that might arise from the use of modification dates.\n\n   Entity Tags are described in section 3.11. The headers used with\n   entity tags are described in sections 14.19, 14.24, 14.26 and 14.44.\n\n\n13.3.3 Weak and Strong Validators\n\n   Since both origin servers and caches will compare two validators to\n   decide if they represent the same or different entities, one normally\n   would expect that if the entity (the entity-body or any entity-\n   headers) changes in any way, then the associated validator would\n   change as well. If this is true, then we call this validator a\n   \"strong validator.\"\n\n   However, there might be cases when a server prefers to change the\n   validator only on semantically significant changes, and not when\n\n\nFielding, et al                                                [Page 79]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   insignificant aspects of the entity change. A validator that does not\n   always change when the resource changes is a \"weak validator.\"\n\n   Entity tags are normally \"strong validators,\" but the protocol\n   provides a mechanism to tag an entity tag as \"weak.\" One can think of\n   a strong validator as one that changes whenever the bits of an entity\n   changes, while a weak value changes whenever the meaning of an entity\n   changes. Alternatively, one can think of a strong validator as part\n   of an identifier for a specific entity, while a weak validator is\n   part of an identifier for a set of semantically equivalent entities.\n\n     Note: One example of a strong validator is an integer that is\n     incremented in stable storage every time an entity is changed.\n\n     An entity's modification time, if represented with one-second\n     resolution, could be a weak validator, since it is possible that\n     the resource might be modified twice during a single second.\n\n     Support for weak validators is optional. However, weak\n     validators allow for more efficient caching of equivalent\n     objects; for example, a hit counter on a site is probably good\n     enough if it is updated every few days or weeks, and any value\n     during that period is likely \"good enough\" to be equivalent.\n\n   A \"use\" of a validator is either when a client generates a request\n   and includes the validator in a validating header field, or when a\n   server compares two validators.\n\n   Strong validators are usable in any context. Weak validators are only\n   usable in contexts that do not depend on exact equality of an entity.\n   For example, either kind is usable for a conditional GET of a full\n   entity. However, only a strong validator is usable for a sub-range\n   retrieval, since otherwise the client might end up with an internally\n   inconsistent entity.\n\n   The only function that the HTTP/1.1 protocol defines on validators is\n   comparison. There are two validator comparison functions, depending\n   on whether the comparison context allows the use of weak validators\n   or not:\n\n     .  The strong comparison function: in order to be considered equal,\n        both validators MUST be identical in every way, and both MUST\n        NOT be weak.\n     .  The weak comparison function: in order to be considered equal,\n        both validators MUST be identical in every way, but either or\n        both of them MAY be tagged as \"weak\" without affecting the\n        result.\n   The weak comparison function MAY be used for simple (non-subrange)\n   GET requests. The strong comparison function MUST be used in all\n   other cases.\n\n   An entity tag is strong unless it is explicitly tagged as weak.\n   Section 3.11 gives the syntax for entity tags.\n\n\n\nFielding, et al                                                [Page 80]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   A Last-Modified time, when used as a validator in a request, is\n   implicitly weak unless it is possible to deduce that it is strong,\n   using the following rules:\n\n     .  The validator is being compared by an origin server to the\n        actual current validator for the entity and,\n     .  That origin server reliably knows that the associated entity did\n        not change twice during the second covered by the presented\n        validator.\n   or\n\n     .  The validator is about to be used by a client in an If-Modified-\n        Since or If-Unmodified-Since header, because the client has a\n        cache entry for the associated entity, and\n     .  That cache entry includes a Date value, which gives the time\n        when the origin server sent the original response, and\n     .  The presented Last-Modified time is at least 60 seconds before\n        the Date value.\n   or\n\n     .  The validator is being compared by an intermediate cache to the\n        validator stored in its cache entry for the entity, and\n     .  That cache entry includes a Date value, which gives the time\n        when the origin server sent the original response, and\n     .  The presented Last-Modified time is at least 60 seconds before\n        the Date value.\n   This method relies on the fact that if two different responses were\n   sent by the origin server during the same second, but both had the\n   same Last-Modified time, then at least one of those responses would\n   have a Date value equal to its Last-Modified time. The arbitrary 60-\n   second limit guards against the possibility that the Date and Last-\n   Modified values are generated from different clocks, or at somewhat\n   different times during the preparation of the response. An\n   implementation MAY use a value larger than 60 seconds, if it is\n   believed that 60 seconds is too short.\n\n   If a client wishes to perform a sub-range retrieval on a value for\n   which it has only a Last-Modified time and no opaque validator, it\n   MAY do this only if the Last-Modified time is strong in the sense\n   described here.\n\n   A cache or origin server receiving a conditional request, other than\n   a full-body GET request, MUST use the strong comparison function to\n   evaluate the condition.\n\n   These rules allow HTTP/1.1 caches and clients to safely perform sub-\n   range retrievals on values that have been obtained from HTTP/1.0\n   servers.\n\n\n13.3.4 Rules for When to Use Entity Tags and Last-modified Dates\n\n   We adopt a set of rules and recommendations for origin servers,\n   clients, and caches regarding when various validator types ought to\n   be used, and for what purposes.\n\nFielding, et al                                                [Page 81]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   HTTP/1.1 origin servers:\n\n     .  SHOULD send an entity tag validator unless it is not feasible to\n        generate one.\n     .  MAY send a weak entity tag instead of a strong entity tag, if\n        performance considerations support the use of weak entity tags,\n        or if it is unfeasible to send a strong entity tag.\n     .  SHOULD send a Last-Modified value if it is feasible to send one,\n        unless the risk of a breakdown in semantic transparency that\n        could result from using this date in an If-Modified-Since header\n        would lead to serious problems.\n   In other words, the preferred behavior for an HTTP/1.1 origin server\n   is to send both a strong entity tag and a Last-Modified value.\n\n   In order to be legal, a strong entity tag MUST change whenever the\n   associated entity value changes in any way. A weak entity tag SHOULD\n   change whenever the associated entity changes in a semantically\n   significant way.\n\n     Note: in order to provide semantically transparent caching, an\n     origin server must avoid reusing a specific strong entity tag\n     value for two different entities, or reusing a specific weak\n     entity tag value for two semantically different entities. Cache\n     entries might persist for arbitrarily long periods, regardless\n     of expiration times, so it might be inappropriate to expect that\n     a cache will never again attempt to validate an entry using a\n     validator that it obtained at some point in the past.\n\n   HTTP/1.1 clients:\n\n     .  If an entity tag has been provided by the origin server, MUST\n        use that entity tag in any cache-conditional request (using If-\n        Match or If-None-Match).\n     .  If only a Last-Modified value has been provided by the origin\n        server, SHOULD use that value in non-subrange cache-conditional\n        requests (using If-Modified-Since).\n     .  If only a Last-Modified value has been provided by an HTTP/1.0\n        origin server, MAY use that value in subrange cache-conditional\n        requests (using If-Unmodified-Since:). The user agent SHOULD\n        provide a way to disable this, in case of difficulty.\n     .  If both an entity tag and a Last-Modified value have been\n        provided by the origin server, SHOULD use both validators in\n        cache-conditional requests. This allows both HTTP/1.0 and\n        HTTP/1.1 caches to respond appropriately.\n   An HTTP/1.1 origin server, upon receiving a conditional request that\n   includes both a Last-modified date (e.g., in an If-Modified-Since or\n   If-Unmodified-Since header field) and one or more entity tags (e.g.,\n   in an If-Match, If-None-Match, or If-Range header field) as cache\n   validators, MUST NOT return a response status of 304 (Not Modified)\n   unless doing so is consistent with all of the conditional header\n   fields in the request.\n\n   An HTTP/1.1 caching proxy, upon receiving a conditional request that\n   includes both a Last-modified date and one or more entity tags as\n   cache validators, MUST NOT return a locally cached response to the\n\nFielding, et al                                                [Page 82]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   client unless that cached response is consistent with all of the\n   conditional header fields in the request.\n\n   A note on rationale: The general principle behind these rules is that\n   HTTP/1.1 servers and clients should transmit as much non-redundant\n   information as is available in their responses and requests. HTTP/1.1\n   systems receiving this information will make the most conservative\n   assumptions about the validators they receive.\n\n   HTTP/1.0 clients and caches will ignore entity tags. Generally, last-\n   modified values received or used by these systems will support\n   transparent and efficient caching, and so HTTP/1.1 origin servers\n   should provide Last-Modified values. In those rare cases where the\n   use of a Last-Modified value as a validator by an HTTP/1.0 system\n   could result in a serious problem, then HTTP/1.1 origin servers\n   should not provide one.\n\n\n13.3.5 Non-validating Conditionals\n\n   The principle behind entity tags is that only the service author\n   knows the semantics of a resource well enough to select an\n   appropriate cache validation mechanism, and the specification of any\n   validator comparison function more complex than byte-equality would\n   open up a can of worms. Thus, comparisons of any other headers\n   (except Last-Modified, for compatibility with HTTP/1.0) are never\n   used for purposes of validating a cache entry.\n\n\n13.4 Response Cachability\n\n   Unless specifically constrained by a cache-control (section 14.9)\n   directive, a caching system MAY always store a successful response\n   (see section 13.8) as a cache entry, MAY return it without validation\n   if it is fresh, and MAY return it after successful validation. If\n   there is neither a cache validator nor an explicit expiration time\n   associated with a response, we do not expect it to be cached, but\n   certain caches MAY violate this expectation (for example, when little\n   or no network connectivity is available). A client can usually detect\n   that such a response was taken from a cache by comparing the Date\n   header to the current time.\n\n     Note that some HTTP/1.0 caches are known to violate this\n     expectation without providing any Warning.\n\n   However, in some cases it might be inappropriate for a cache to\n   retain an entity, or to return it in response to a subsequent\n   request. This might be because absolute semantic transparency is\n   deemed necessary by the service author, or because of security or\n   privacy considerations. Certain cache-control directives are\n   therefore provided so that the server can indicate that certain\n   resource entities, or portions thereof, MUST NOT be cached regardless\n   of other considerations.\n\n\n\nFielding, et al                                                [Page 83]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Note that section 14.8 normally prevents a shared cache from saving\n   and returning a response to a previous request if that request\n   included an Authorization header.\n\n   A response received with a status code of 200, 203, 206, 300, 301 or\n   410 MAY be stored by a cache and used in reply to a subsequent\n   request, subject to the expiration mechanism, unless a cache-control\n   directive prohibits caching. However, a cache that does not support\n   the Range and Content-Range headers MUST NOT cache 206 (Partial\n   Content) responses.\n\n   A response received with any other status code MUST NOT be returned\n   in a reply to a subsequent request unless there are cache-control\n   directives or another header(s) that explicitly allow it. For\n   example, these include the following: an Expires header (section\n   14.21); a \"max-age\", \"s-maxage\",  \"must-revalidate\", \"proxy-\n   revalidate\", \"public\" or \"private\" cache-control directive (section\n   14.9).\n\n\n13.5 Constructing Responses From Caches\n\n   The purpose of an HTTP cache is to store information received in\n   response to requests, for use in responding to future requests. In\n   many cases, a cache simply returns the appropriate parts of a\n   response to the requester. However, if the cache holds a cache entry\n   based on a previous response, it might have to combine parts of a new\n   response with what is held in the cache entry.\n\n\n13.5.1 End-to-end and Hop-by-hop Headers\n\n   For the purpose of defining the behavior of caches and non-caching\n   proxies, we divide HTTP headers into two categories:\n\n     .  End-to-end headers, which MUST be transmitted to the ultimate\n        recipient of a request or response. End-to-end headers in\n        responses MUST be stored as part of a cache entry and\n        transmitted in any response formed from a cache entry.\n     .  Hop-by-hop headers, which are meaningful only for a single\n        transport-level connection, and are not stored by caches or\n        forwarded by proxies.\n   The following HTTP/1.1 headers are hop-by-hop headers:\n\n     .  Connection\n     .  Keep-Alive\n     .  Proxy-Authenticate\n     .  Proxy-Authorization\n     .  Transfer-Encoding\n     .  Upgrade\n   All other headers defined by HTTP/1.1 are end-to-end headers.\n\n   Hop-by-hop headers introduced in future versions of HTTP MUST be\n   listed in a Connection header, as described in section 14.10.\n\n\nFielding, et al                                                [Page 84]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n13.5.2 Non-modifiable Headers\n\n   Some features of the HTTP/1.1 protocol, such as Digest\n   Authentication, depend on the value of certain end-to-end headers. A\n   transparent proxy SHOULD NOT modify an end-to-end header unless the\n   definition of that header requires or specifically allows that.\n\n   A transparent proxy MUST NOT modify any of the following fields in a\n   request or response, and it MUST NOT add any of these fields if not\n   already present:\n\n     .  Content-Location\n     .  Content-MD5\n     .  ETag\n     .  Last-Modified\n\n\n   A transparent proxy MUST NOT modify any of the following fields in a\n   response:\n\n     .  Expires\n   but it MAY add any of these fields if not already present. If an\n   Expires header is added, it MUST be given a field-value identical to\n   that of the Date header in that response.\n\n   A proxy MUST NOT modify or add any of the following fields in message\n   that contains the no-transform cache-control directive, or in any\n   request:\n\n     .  Content-Encoding\n     .  Content-Range\n     .  Content-Type\n   A non-transparent proxy MAY modify or add these fields to a message\n   that does not include no-transform, but if it does so, if not already\n   present, it MUST add a Warning 214 (Transformation applied) if one\n   does not already appear in the message (see section 14.46).\n\n     Warning: unnecessary modification of end-to-end headers might\n     cause authentication failures if stronger authentication\n     mechanisms are introduced in later versions of HTTP. Such\n     authentication mechanisms MAY rely on the values of header\n     fields not listed here.\n\n   The Content-Length field of a request or response is added or deleted\n   according to the rules in section 4.4. A transparent proxy MUST\n   preserve the entity-length (section 7.2.2) of the entity-body,\n   although it MAY change the transfer-length (section 4.4).\n\n\n13.5.3 Combining Headers\n\n   When a cache makes a validating request to a server, and the server\n   provides a 304 (Not Modified) response or a 206 (Partial Content)\n   response, the cache then constructs a response to send to the\n   requesting client.\n\nFielding, et al                                                [Page 85]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   If the status code is 304 (Not Modified), the cache uses the entity-\n   body stored in the cache entry as the entity-body of this outgoing\n   response. If the status code is 206 (Partial Content) and the ETag or\n   Last-Modified headers match exactly, the cache MAY combine the\n   contents stored in the cache entry with the new contents received in\n   the response and use the result as the entity-body of this outgoing\n   response, (see 13.5.4).\n\n   The end-to-end headers stored in the cache entry are used for the\n   constructed response, except that\n\n     .  any stored Warning headers with warn-code 1xx (see section\n        14.46) MUST be deleted from the cache entry and the forwarded\n        response.\n     .  any stored Warning headers with warn-code 2xx MUST be retained\n        in the cache entry and the forwarded response.\n     .  any end-to-end headers provided in the 304 or 206 response MUST\n        replace the corresponding headers from the cache entry.\n\n\n   Unless the cache decides to remove the cache entry, it MUST also\n   replace the end-to-end headers stored with the cache entry with\n   corresponding headers received in the incoming response.\n\n   In other words, the set of end-to-end headers received in the\n   incoming response overrides all corresponding end-to-end headers\n   stored with the cache entry (except for stored Warning headers with\n   warn-code 1xx, which are deleted even if not overridden).\n\n   If a header field-name in the incoming response matches more than one\n   header in the cache entry, all such old headers are replaced.\n\n     Note: this rule allows an origin server to use a 304 (Not\n     Modified) or a 206 (Partial Content) response to update any\n     header associated with a previous response for the same entity\n     or sub-ranges thereof, although it might not always be\n     meaningful or correct to do so. This rule does not allow an\n     origin server to use a 304 (Not Modified) or a 206 (Partial\n     Content) response to entirely delete a header that it had\n     provided with a previous response.\n\n\n13.5.4 Combining Byte Ranges\n\n   A response might transfer only a subrange of the bytes of an entity-\n   body, either because the request included one or more Range\n   specifications, or because a connection was broken prematurely. After\n   several such transfers, a cache might have received several ranges of\n   the same entity-body.\n\n   If a cache has a stored non-empty set of subranges for an entity, and\n   an incoming response transfers another subrange, the cache MAY\n   combine the new subrange with the existing set if both the following\n   conditions are met:\n\n\nFielding, et al                                                [Page 86]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     .  Both the incoming response and the cache entry have a cache\n        validator.\n     .  The two cache validators match using the strong comparison\n        function (see section 13.3.3).\n\n   If either requirement is not met, the cache MUST use only the most\n   recent partial response (based on the Date values transmitted with\n   every response, and using the incoming response if these values are\n   equal or missing), and MUST discard the other partial information.\n\n\n13.6 Caching Negotiated Responses\n\n   Use of server-driven content negotiation (section 12.1), as indicated\n   by the presence of a Vary header field in a response, alters the\n   conditions and procedure by which a cache can use the response for\n   subsequent requests. See section 14.44 for use of the Vary header\n   field by servers.\n\n   A server SHOULD use the Vary header field to inform a cache of what\n   request-header fields were used to select among multiple\n   representations of a cachable response subject to server-driven\n   negotiation. The set of header fields named by the Vary field value\n   is known as the \"selecting\" request-headers.\n\n   When the cache receives a subsequent request whose Request-URI\n   specifies one or more cache entries including a Vary header field,\n   the cache MUST NOT use such a cache entry to construct a response to\n   the new request unless all of the selecting request-headers present\n   in the new request match the corresponding stored request-headers in\n   the original request.\n\n   The selecting request-headers from two requests are defined to match\n   if and only if the selecting request-headers in the first request can\n   be transformed to the selecting request-headers in the second request\n   by adding or removing linear white space (LWS) at places where this\n   is allowed by the corresponding BNF, and/or combining multiple\n   message-header fields with the same field name following the rules\n   about message headers in section 4.2.\n\n   A Vary header field-value of \"*\" always fails to match and subsequent\n   requests on that resource can only be properly interpreted by the\n   origin server.\n\n   If the selecting request header fields for the cached entry do not\n   match the selecting request header fields of the new request, then\n   the cache MUST NOT use a cached entry to satisfy the request unless\n   it first relays the new request to the origin server in a conditional\n   request and the server responds with 304 (Not Modified), including an\n   entity tag or Content-Location that indicates the entity to be used.\n\n   If an entity tag was assigned to a cached representation, the\n   forwarded request SHOULD be conditional and include the entity tags\n   in an If-None-Match header field from all its cache entries for the\n   resource. This conveys to the server the set of entities currently\n\nFielding, et al                                                [Page 87]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   held by the cache, so that if any one of these entities matches the\n   requested entity, the server can use the ETag header field in its 304\n   (Not Modified) response to tell the cache which entry is appropriate.\n   If the entity-tag of the new response matches that of an existing\n   entry, the new response SHOULD be used to update the header fields of\n   the existing entry, and the result MUST be returned to the client.\n\n   If any of the existing cache entries contains only partial content\n   for the associated entity, its entity-tag SHOULD NOT be included in\n   the If-None-Match header field unless the request is for a range that\n   would be fully satisfied by that entry.\n\n   If a cache receives a successful response whose Content-Location\n   field matches that of an existing cache entry for the same Request-\n   URI, whose entity-tag differs from that of the existing entry, and\n   whose Date is more recent than that of the existing entry, the\n   existing entry SHOULD NOT be returned in response to future requests\n   and SHOULD be deleted from the cache.\n\n\n13.7 Shared and Non-Shared Caches\n\n   For reasons of security and privacy, it is necessary to make a\n   distinction between \"shared\" and \"non-shared\" caches. A non-shared\n   cache is one that is accessible only to a single user. Accessibility\n   in this case SHOULD be enforced by appropriate security mechanisms.\n   All other caches are considered to be \"shared.\" Other sections of\n   this specification place certain constraints on the operation of\n   shared caches in order to prevent loss of privacy or failure of\n   access controls.\n\n\n13.8 Errors or Incomplete Response Cache Behavior\n\n   A cache that receives an incomplete response (for example, with fewer\n   bytes of data than specified in a Content-Length header) MAY store\n   the response. However, the cache MUST treat this as a partial\n   response. Partial responses MAY be combined as described in section\n   13.5.4; the result might be a full response or might still be\n   partial. A cache MUST NOT return a partial response to a client\n   without explicitly marking it as such, using the 206 (Partial\n   Content) status code. A cache MUST NOT return a partial response\n   using a status code of 200 (OK).\n\n   If a cache receives a 5xx response while attempting to revalidate an\n   entry, it MAY either forward this response to the requesting client,\n   or act as if the server failed to respond. In the latter case, it MAY\n   return a previously received response unless the cached entry\n   includes the \"must-revalidate\" cache-control directive (see section\n   14.9).\n\n\n\n\n\n\nFielding, et al                                                [Page 88]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n13.9 Side Effects of GET and HEAD\n\n   Unless the origin server explicitly prohibits the caching of their\n   responses, the application of GET and HEAD methods to any resources\n   SHOULD NOT have side effects that would lead to erroneous behavior if\n   these responses are taken from a cache. They MAY still have side\n   effects, but a cache is not required to consider such side effects in\n   its caching decisions. Caches are always expected to observe an\n   origin server's explicit restrictions on caching.\n\n   We note one exception to this rule: since some applications have\n   traditionally used GETs and HEADs with query URLs (those containing a\n   \"?\" in the rel_path part) to perform operations with significant side\n   effects, caches MUST NOT treat responses to such URIs as fresh unless\n   the server provides an explicit expiration time. This specifically\n   means that responses from HTTP/1.0 servers for such URIs SHOULD NOT\n   be taken from a cache. See section 9.1.1 for related information.\n\n\n13.10 Invalidation After Updates or Deletions\n\n   The effect of certain methods performed on a resource at the origin\n   server might cause one or more existing cache entries to become non-\n   transparently invalid. That is, although they might continue to be\n   \"fresh,\" they do not accurately reflect what the origin server would\n   return for a new request on that resource.\n\n   There is no way for the HTTP protocol to guarantee that all such\n   cache entries are marked invalid. For example, the request that\n   caused the change at the origin server might not have gone through\n   the proxy where a cache entry is stored. However, several rules help\n   reduce the likelihood of erroneous behavior.\n\n   In this section, the phrase \"invalidate an entity\" means that the\n   cache will either remove all instances of that entity from its\n   storage, or will mark these as \"invalid\" and in need of a mandatory\n   revalidation before they can be returned in response to a subsequent\n   request.\n\n   Some HTTP methods MUST cause a cache to invalidate an entity. This is\n   either the entity referred to by the Request-URI, or by the Location\n   or Content-Location headers (if present). These methods are:\n\n     .  PUT\n     .  DELETE\n     .  POST\n\n   In order to prevent denial of service attacks, an invalidation based\n   on the URI in a Location or Content-Location header MUST only be\n   performed if the host part is the same as in the Request-URI.\n\n   A cache that passes through requests for methods it does not\n   understand SHOULD invalidate any entities referred to by the Request-\n   URI.\n\n\nFielding, et al                                                [Page 89]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n13.11 Write-Through Mandatory\n\n   All methods that might be expected to cause modifications to the\n   origin server's resources MUST be written through to the origin\n   server. This currently includes all methods except for GET and HEAD.\n   A cache MUST NOT reply to such a request from a client before having\n   transmitted the request to the inbound server, and having received a\n   corresponding response from the inbound server. This does not prevent\n   a proxy cache from sending a 100 (Continue) response before the\n   inbound server has sent its final reply.\n\n   The alternative (known as \"write-back\" or \"copy-back\" caching) is not\n   allowed in HTTP/1.1, due to the difficulty of providing consistent\n   updates and the problems arising from server, cache, or network\n   failure prior to write-back.\n\n\n13.12 Cache Replacement\n\n   If a new cachable (see sections 14.9.2, 13.2.5, 13.2.6 and 13.8)\n   response is received from a resource while any existing responses for\n   the same resource are cached, the cache SHOULD use the new response\n   to reply to the current request. It MAY insert it into cache storage\n   and MAY, if it meets all other requirements, use it to respond to any\n   future requests that would previously have caused the old response to\n   be returned. If it inserts the new response into cache storage it\n   SHOULD follow the rules in section 13.5.3.\n\n     Note: a new response that has an older Date header value than\n     existing cached responses is not cachable.\n\n\n13.13 History Lists\n\n   User agents often have history mechanisms, such as \"Back\" buttons and\n   history lists, which can be used to redisplay an entity retrieved\n   earlier in a session.\n\n   History mechanisms and caches are different. In particular history\n   mechanisms SHOULD NOT try to show a semantically transparent view of\n   the current state of a resource. Rather, a history mechanism is meant\n   to show exactly what the user saw at the time when the resource was\n   retrieved.\n\n   By default, an expiration time does not apply to history mechanisms.\n   If the entity is still in storage, a history mechanism SHOULD display\n   it even if the entity has expired, unless the user has specifically\n   configured the agent to refresh expired history documents.\n\n   This is not to be construed to prohibit the history mechanism from\n   telling the user that a view might be stale.\n\n     Note: if history list mechanisms unnecessarily prevent users\n     from viewing stale resources, this will tend to force service\n     authors to avoid using HTTP expiration controls and cache\n\nFielding, et al                                                [Page 90]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     controls when they would otherwise like to. Service authors may\n     consider it important that users not be presented with error\n     messages or warning messages when they use navigation controls\n     (such as BACK) to view previously fetched resources. Even though\n     sometimes such resources ought not to cached, or ought to expire\n     quickly, user interface considerations may force service authors\n     to resort to other means of preventing caching (e.g. \"once-only\"\n     URLs) in order not to suffer the effects of improperly\n     functioning history mechanisms.\n\n\n14 Header Field Definitions\n\n   This section defines the syntax and semantics of all standard\n   HTTP/1.1 header fields. For entity-header fields, both sender and\n   recipient refer to either the client or the server, depending on who\n   sends and who receives the entity.\n\n\n14.1 Accept\n\n   The Accept request-header field can be used to specify certain media\n   types which are acceptable for the response. Accept headers can be\n   used to indicate that the request is specifically limited to a small\n   set of desired types, as in the case of a request for an in-line\n   image.\n\n          Accept         = \"Accept\" \":\"\n                           #( media-range [ accept-params ] )\n\n          media-range    = ( \"*/*\"\n                           | ( type \"/\" \"*\" )\n                           | ( type \"/\" subtype )\n                           ) *( \";\" parameter )\n\n          accept-params  = \";\" \"q\" \"=\" qvalue *( accept-extension )\n\n          accept-extension = \";\" token [ \"=\" ( token | quoted-string ) ]\n\n   The asterisk \"*\" character is used to group media types into ranges,\n   with \"*/*\" indicating all media types and \"type/*\" indicating all\n   subtypes of that type. The media-range MAY include media type\n   parameters that are applicable to that range.\n\n   Each media-range MAY be followed by one or more accept-params,\n   beginning with the \"q\" parameter for indicating a relative quality\n   factor. The first \"q\" parameter (if any) separates the media-range\n   parameter(s) from the accept-params. Quality factors allow the user\n   or user agent to indicate the relative degree of preference for that\n   media-range, using the qvalue scale from 0 to 1 (section 3.9). The\n   default value is q=1.\n\n     Note: Use of the \"q\" parameter name to separate media type\n     parameters from Accept extension parameters is due to historical\n     practice. Although this prevents any media type parameter named\n\nFielding, et al                                                [Page 91]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     \"q\" from being used with a media range, such an event is\n     believed to be unlikely given the lack of any \"q\" parameters in\n     the IANA media type registry and the rare usage of any media\n     type parameters in Accept. Future media types are discouraged\n     from registering any parameter named \"q\".\n\n   The example\n\n          Accept: audio/*; q=0.2, audio/basic\n\n   SHOULD be interpreted as \"I prefer audio/basic, but send me any audio\n   type if it is the best available after an 80% mark-down in quality.\"\n\n   If no Accept header field is present, then it is assumed that the\n   client accepts all media types. If an Accept header field is present,\n   and if the server cannot send a response which is acceptable\n   according to the combined Accept field value, then the server SHOULD\n   send a 406 (not acceptable) response.\n\n   A more elaborate example is\n\n          Accept: text/plain; q=0.5, text/html,\n                  text/x-dvi; q=0.8, text/x-c\n\n   Verbally, this would be interpreted as \"text/html and text/x-c are\n   the preferred media types, but if they do not exist, then send the\n   text/x-dvi entity, and if that does not exist, send the text/plain\n   entity.\"\n\n   Media ranges can be overridden by more specific media ranges or\n   specific media types. If more than one media range applies to a given\n   type, the most specific reference has precedence. For example,\n\n          Accept: text/*, text/html, text/html;level=1, */*\n\n   have the following precedence:\n\n          1) text/html;level=1\n          2) text/html\n          3) text/*\n          4) */*\n\n   The media type quality factor associated with a given type is\n   determined by finding the media range with the highest precedence\n   which matches that type. For example,\n\n          Accept: text/*;q=0.3, text/html;q=0.7, text/html;level=1,\n                  text/html;level=2;q=0.4, */*;q=0.5\n\n   would cause the following values to be associated:\n\n          text/html;level=1         = 1\n          text/html                 = 0.7\n          text/plain                = 0.3\n          image/jpeg                = 0.5\n\nFielding, et al                                                [Page 92]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n          text/html;level=2         = 0.4\n          text/html;level=3         = 0.7\n\n     Note: A user agent might be provided with a default set of\n     quality values for certain media ranges. However, unless the\n     user agent is a closed system which cannot interact with other\n     rendering agents, this default set ought to be configurable by\n     the user.\n\n\n14.2 Accept-Charset\n\n   The Accept-Charset request-header field can be used to indicate what\n   character sets are acceptable for the response. This field allows\n   clients capable of understanding more comprehensive or special-\n   purpose character sets to signal that capability to a server which is\n   capable of representing documents in those character sets.\n\n      Accept-Charset = \"Accept-Charset\" \":\"\n                 1#( ( charset | \"*\" )[ \";\" \"q\" \"=\" qvalue ] )\n\n   Character set values are described in section 3.4. Each charset MAY\n   be given an associated quality value which represents the user's\n   preference for that charset. The default value is q=1. An example is\n\n      Accept-Charset: iso-8859-5, unicode-1-1;q=0.8\n\n   The special value \"*\", if present in the Accept-Charset field,\n   matches every character set (including ISO-8859-1) which is not\n   mentioned elsewhere in the Accept-Charset field. If no \"*\" is present\n   in an Accept-Charset field, then all character sets not explicitly\n   mentioned get a quality value of 0, except for ISO-8859-1, which gets\n   a quality value of 1 if not explicitly mentioned.\n\n   If no Accept-Charset header is present, the default is that any\n   character set is acceptable. If an Accept-Charset header is present,\n   and if the server cannot send a response which is acceptable\n   according to the Accept-Charset header, then the server SHOULD send\n   an error response with the 406 (not acceptable) status code, though\n   the sending of an unacceptable response is also allowed.\n\n\n14.3 Accept-Encoding\n\n   The Accept-Encoding request-header field is similar to Accept, but\n   restricts the content-codings (section 3.5) that are acceptable in\n   the response.\n\n          Accept-Encoding  = \"Accept-Encoding\" \":\"\n                             1#( codings [ \";\" \"q\" \"=\" qvalue ] )\n\n          codings          = ( content-coding | \"*\" )\n\n   Examples of its use are:\n\n\nFielding, et al                                                [Page 93]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n          Accept-Encoding: compress, gzip\n          Accept-Encoding:\n          Accept-Encoding: *\n          Accept-Encoding: compress;q=0.5, gzip;q=1.0\n          Accept-Encoding: gzip;q=1.0, identity; q=0.5, *;q=0\n\n   A server tests whether a content-coding is acceptable, according to\n   an Accept-Encoding field, using these rules:\n\n     1. If the content-coding is one of the content-codings listed in\n     the Accept-Encoding field, then it is acceptable, unless it is\n     accompanied by a qvalue of 0. (As defined in section 3.9, a qvalue\n     of 0 means \"not acceptable.\")\n\n     2. The special \"*\" symbol in an Accept-Encoding field matches any\n     available content-coding not explicitly listed in the header field.\n\n     3. If multiple content-codings are acceptable, then the acceptable\n     content-coding with the highest non-zero qvalue is preferred.\n\n     4. The \"identity\" content-coding is always acceptable, unless\n     specifically refused because the Accept-Encoding field includes\n     \"identity;q=0\", or because the field includes \"*;q=0\" and does not\n     explicitly include the \"identity\" content-coding. If the Accept-\n     Encoding field-value is empty, then only the \"identity\" encoding is\n     acceptable.\n\n   If an Accept-Encoding field is present in a request, and if the\n   server cannot send a response which is acceptable according to the\n   Accept-Encoding header, then the server SHOULD send an error response\n   with the 406 (Not Acceptable) status code.\n\n   If no Accept-Encoding field is present in a request, the server MAY\n   assume that the client will accept any content coding. In this case,\n   if \"identity\" is one of the available content-codings, then the\n   server SHOULD use the \"identity\" content-coding, unless it has\n   additional information that a different content-coding is meaningful\n   to the client.\n\n     Note: If the request does not include an Accept-Encoding field,\n     and if the \"identity\" content-coding is unavailable, then\n     content-codings commonly understood by HTTP/1.0 clients (i.e.,\n     \"gzip\" and \"compress\") are preferred; some older clients\n     improperly display messages sent with other content-codings. The\n     server might also make this decision based on information about\n     the particular user-agent or client.\n\n     Note: Most HTTP/1.0 applications do not recognize or obey\n     qvalues associated with content-codings. This means that qvalues\n     will not work and are not permitted with x-gzip or x-compress.\n\n\n\n\n\n\nFielding, et al                                                [Page 94]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n14.4 Accept-Language\n\n   The Accept-Language request-header field is similar to Accept, but\n   restricts the set of natural languages that are preferred as a\n   response to the request. Language tags are defined in section 3.10.\n\n          Accept-Language = \"Accept-Language\" \":\"\n                            1#( language-range [ \";\" \"q\" \"=\" qvalue ] )\n\n          language-range  = ( ( 1*8ALPHA *( \"-\" 1*8ALPHA ) ) | \"*\" )\n\n   Each language-range MAY be given an associated quality value which\n   represents an estimate of the user's preference for the languages\n   specified by that range. The quality value defaults to \"q=1\". For\n   example,\n\n          Accept-Language: da, en-gb;q=0.8, en;q=0.7\n\n   would mean: \"I prefer Danish, but will accept British English and\n   other types of English.\" A language-range matches a language-tag if\n   it exactly equals the tag, or if it exactly equals a prefix of the\n   tag such that the first tag character following the prefix is \"-\".\n   The special range \"*\", if present in the Accept-Language field,\n   matches every tag not matched by any other range present in the\n   Accept-Language field.\n\n     Note: This use of a prefix matching rule does not imply that\n     language tags are assigned to languages in such a way that it is\n     always true that if a user understands a language with a certain\n     tag, then this user will also understand all languages with tags\n     for which this tag is a prefix. The prefix rule simply allows\n     the use of prefix tags if this is the case.\n\n   The language quality factor assigned to a language-tag by the Accept-\n   Language field is the quality value of the longest language-range in\n   the field that matches the language-tag. If no language-range in the\n   field matches the tag, the language quality factor assigned is 0. If\n   no Accept-Language header is present in the request, the server\n   SHOULD assume that all languages are equally acceptable. If an\n   Accept-Language header is present, then all languages which are\n   assigned a quality factor greater than 0 are acceptable.\n\n   It might be contrary to the privacy expectations of the user to send\n   an Accept-Language header with the complete linguistic preferences of\n   the user in every request. For a discussion of this issue, see\n   section 15.1.4.\n\n     Note: As intelligibility is highly dependent on the individual\n     user, it is recommended that client applications make the choice\n     of linguistic preference available to the user. If the choice is\n     not made available, then the Accept-Language header field MUST\n     NOT be given in the request.\n\n     Note: When making the choice of linguistic preference available\n     to the user, we remind implementors of  the fact that users are\n\nFielding, et al                                                [Page 95]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     not familiar with the details of language matching as described\n     above, and should provide appropriate guidance. As an example,\n     users might assume that on selecting \"en-gb\", they will be\n     served any kind of English document if British English is not\n     available. A user agent might suggest in such a case to add \"en\"\n     to get the best matching behavior.\n\n\n14.5 Accept-Ranges\n\n   The Accept-Ranges response-header field allows the server to indicate\n   its acceptance of range requests for a resource:\n\n          Accept-Ranges     = \"Accept-Ranges\" \":\" acceptable-ranges\n          acceptable-ranges = 1#range-unit | \"none\"\n\n   Origin servers that accept byte-range requests MAY send\n\n          Accept-Ranges: bytes\n\n   but are not required to do so. Clients MAY generate byte-range\n   requests without having received this header for the resource\n   involved. Range units are defined in section 3.12.\n\n   Servers that do not accept any kind of range request for a resource\n   MAY send\n\n          Accept-Ranges: none\n\n   to advise the client not to attempt a range request.\n\n\n14.6 Age\n\n   The Age response-header field conveys the sender's estimate of the\n   amount of time since the response (or its revalidation) was generated\n   at the origin server. A cached response is \"fresh\" if its age does\n   not exceed its freshness lifetime. Age values are calculated as\n   specified in section 13.2.3.\n\n           Age = \"Age\" \":\" age-value\n\n           age-value = delta-seconds\n\n   Age values are non-negative decimal integers, representing time in\n   seconds.\n\n   If a cache receives a value larger than the largest positive integer\n   it can represent, or if any of its age calculations overflows, it\n   MUST transmit an Age header with a value of 2147483648 (2^31). An\n   HTTP/1.1 server that includes a cache MUST include an Age header\n   field in every response generated from its own cache. Caches SHOULD\n   use an arithmetic type of at least 31 bits of range.\n\n\n\nFielding, et al                                                [Page 96]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n14.7 Allow\n\n   The Allow entity-header field lists the set of methods supported by\n   the resource identified by the Request-URI. The purpose of this field\n   is strictly to inform the recipient of valid methods associated with\n   the resource. An Allow header field MUST be present in a 405 (Method\n   Not Allowed) response.\n\n          Allow   = \"Allow\" \":\" #Method\n\n   Example of use:\n\n          Allow: GET, HEAD, PUT\n\n   This field cannot prevent a client from trying other methods.\n   However, the indications given by the Allow header field value SHOULD\n   be followed. The actual set of allowed methods is defined by the\n   origin server at the time of each request.\n\n   The Allow header field MAY be provided with a PUT request to\n   recommend the methods to be supported by the new or modified\n   resource. The server is not required to support these methods and\n   SHOULD include an Allow header in the response giving the actual\n   supported methods.\n\n   A proxy MUST NOT modify the Allow header field even if it does not\n   understand all the methods specified, since the user agent might have\n   other means of communicating with the origin server.\n\n\n14.8 Authorization\n\n   A user agent that wishes to authenticate itself with a server--\n   usually, but not necessarily, after receiving a 401 response--does so\n   by including an Authorization request-header field with the request.\n   The Authorization field value consists of credentials containing the\n   authentication information of the user agent for the realm of the\n   resource being requested.\n\n          Authorization  = \"Authorization\" \":\" credentials\n\n   HTTP access authentication is described in \"HTTP Authentication:\n   Basic and Digest Access Authentication\" [43]. If a request is\n   authenticated and a realm specified, the same credentials SHOULD be\n   valid for all other requests within this realm (assuming that the\n   authentication scheme itself does not require otherwise, such as\n   credentials that vary according to a challenge value or using\n   synchronized clocks).\n\n   When a shared cache (see section 13.7) receives a request containing\n   an Authorization field, it MUST NOT return the corresponding response\n   as a reply to any other request, unless one of the following specific\n   exceptions holds:\n\n\n\nFielding, et al                                                [Page 97]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     1. If the response includes the \"s-maxage\" cache-control directive,\n        the cache MAY use that response in replying to a subsequent\n        request. But (if the specified maximum age has passed) a proxy\n        cache MUST first revalidate it with the origin server, using the\n        request-headers from the new request to allow the origin server\n        to authenticate the new request. (This is the defined behavior\n        for s-maxage.) If the response includes \"s-maxage=0\", the proxy\n        MUST always revalidate it before re-using it.\n\n     2. If the response includes the \"must-revalidate\" cache-control\n        directive, the cache MAY use that response in replying to a\n        subsequent request. But if the response is stale, all caches\n        MUST first revalidate it with the origin server, using the\n        request-headers from the new request to allow the origin server\n        to authenticate the new request.\n\n     3. If the response includes the \"public\" cache-control directive,\n        it MAY be returned in reply to any subsequent request.\n\n\n14.9 Cache-Control\n\n   The Cache-Control general-header field is used to specify directives\n   that MUST be obeyed by all caching mechanisms along the\n   request/response chain. The directives specify behavior intended to\n   prevent caches from adversely interfering with the request or\n   response. These directives typically override the default caching\n   algorithms. Cache directives are unidirectional in that the presence\n   of a directive in a request does not imply that the same directive is\n   to be given in the response.\n\n     Note that HTTP/1.0 caches might not implement Cache-Control and\n     might only implement Pragma: no-cache (see section 14.32).\n\n   Cache directives MUST be passed through by a proxy or gateway\n   application, regardless of their significance to that application,\n   since the directives might be applicable to all recipients along the\n   request/response chain. It is not possible to specify a cache-\n   directive for a specific cache.\n\n    Cache-Control   = \"Cache-Control\" \":\" 1#cache-directive\n\n       cache-directive = cache-request-directive\n            | cache-response-directive\n\n       cache-request-directive =\n              \"no-cache\"                        ; Section 14.9.1\n            | \"no-store\"                        ; Section 14.9.2\n            | \"max-age\" \"=\" delta-seconds       ; Section 14.9.3, 14.9.4\n            | \"max-stale\" [ \"=\" delta-seconds ] ; Section 14.9.3\n            | \"min-fresh\" \"=\" delta-seconds     ; Section 14.9.3\n            | \"no-transform\"                    ; Section 14.9.5\n            | \"only-if-cached\"                  ; Section 14.9.4\n            | cache-extension                   ; Section 14.9.6\n\n\nFielding, et al                                                [Page 98]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n        cache-response-directive =\n              \"public\"                               ; Section 14.9.1\n            | \"private\" [ \"=\" <\"> 1#field-name <\"> ] ; Section 14.9.1\n            | \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]; Section 14.9.1\n            | \"no-store\"                             ; Section 14.9.2\n            | \"no-transform\"                         ; Section 14.9.5\n            | \"must-revalidate\"                      ; Section 14.9.4\n            | \"proxy-revalidate\"                     ; Section 14.9.4\n            | \"max-age\" \"=\" delta-seconds            ; Section 14.9.4\n            | \"s-maxage\" \"=\" delta-seconds           ; Section 14.9.3\n            | cache-extension                        ; Section 14.9.6\n\n       cache-extension = token [ \"=\" ( token | quoted-string ) ]\n\n   When a directive appears without any 1#field-name parameter, the\n   directive applies to the entire request or response. When such a\n   directive appears with a 1#field-name parameter, it applies only to\n   the named field or fields, and not to the rest of the request or\n   response. This mechanism supports extensibility; implementations of\n   future versions of the HTTP protocol might apply these directives to\n   header fields not defined in HTTP/1.1.\n\n   The cache-control directives can be broken down into these general\n   categories:\n\n     .  Restrictions on what are cachable; these may only be imposed by\n        the origin server.\n     .  Restrictions on what may be stored by a cache; these may be\n        imposed by either the origin server or the user agent.\n     .  Modifications of the basic expiration mechanism; these may be\n        imposed by either the origin server or the user agent.\n     .  Controls over cache revalidation and reload; these may only be\n        imposed by a user agent.\n     .  Control over transformation of entities.\n     .  Extensions to the caching system.\n\n14.9.1 What is Cachable\n\n   By default, a response is cachable if the requirements of the request\n   method, request header fields, and the response status indicate that\n   it is cachable. Section 13.4 summarizes these defaults for\n   cachability. The following Cache-Control response directives allow an\n   origin server to override the default cachability of a response:\n\n   public\n     Indicates that the response is cachable by any cache, even if it\n     would normally be non-cachable or cachable only within a non-\n     shared cache. (See also Authorization, section 14.8, for\n     additional details.)\n\n   private\n     Indicates that all or part of the response message is intended for\n     a single user and MUST NOT be cached by a shared cache. This\n     allows an origin server to state that the specified parts of the\n     response are intended for only one user and are not a valid\n\nFielding, et al                                                [Page 99]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     response for requests by other users. A private (non-shared) cache\n     MAY cache the response.\n\n     Note: This usage of the word private only controls where the\n     response may be cached, and cannot ensure the privacy of the\n     message content.\n\n   no-cache\n      If the no-cache directive does not specify a field-name, then a\n     cache MUST NOT use the response to satisfy a subsequent request\n     without successful revalidation with the origin server. This\n     allows an origin server to prevent caching even by caches that\n     have been configured to return stale responses to client requests.\n\n     If the no-cache directive does specify one or more field-names,\n     then a cache MAY use the response to satisfy a subsequent request,\n     subject to any other restrictions on caching. However, the\n     specified field-name(s) MUST NOT be sent in the response to a\n     subsequent request without successful revalidation with the origin\n     server. This allows an origin server to prevent the re-use of\n     certain header fields in a response, while still allowing caching\n     of the rest of the response.\n\n     Note: Most HTTP/1.0 caches will not recognize or obey this\n     directive.\n\n\n14.9.2 What May be Stored by Caches\n\n   no-store\n     The purpose of the no-store directive is to prevent the\n     inadvertent release or retention of sensitive information (for\n     example, on backup tapes). The no-store directive applies to the\n     entire message, and MAY be sent either in a response or in a\n     request. If sent in a request, a cache MUST NOT store any part of\n     either this request or any response to it. If sent in a response,\n     a cache MUST NOT store any part of either this response or the\n     request that elicited it. This directive applies to both non-\n     shared and shared caches. \"MUST NOT store\" in this context means\n     that the cache MUST NOT intentionally store the information in\n     non-volatile storage, and MUST make a best-effort attempt to\n     remove the information from volatile storage as promptly as\n     possible after forwarding it.\n\n     Even when this directive is associated with a response, users\n     might explicitly store such a response outside of the caching\n     system (e.g., with a \"Save As\" dialog). History buffers MAY store\n     such responses as part of their normal operation.\n\n     The purpose of this directive is to meet the stated requirements\n     of certain users and service authors who are concerned about\n     accidental releases of information via unanticipated accesses to\n     cache data structures. While the use of this directive might\n     improve privacy in some cases, we caution that it is NOT in any\n     way a reliable or sufficient mechanism for ensuring privacy. In\n\nFielding, et al                                               [Page 100]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     particular, malicious or compromised caches might not recognize or\n     obey this directive, and communications networks might be\n     vulnerable to eavesdropping.\n\n\n14.9.3 Modifications of the Basic Expiration Mechanism\n\n   The expiration time of an entity MAY be specified by the origin\n   server using the Expires header (see section 14.21). Alternatively,\n   it MAY be specified using the max-age directive in a response. When\n   the max-age cache-control directive is present in a cached response,\n   the response is stale if its current age is greater than the age\n   value given (in seconds) at the time of a new request for that\n   resource. The max-age directive on a response implies that the\n   response is cachable (i.e., \"public\") unless some other, more\n   restrictive cache directive is also present.\n\n   If a response includes both an Expires header and a max-age\n   directive, the max-age directive overrides the Expires header, even\n   if the Expires header is more restrictive. This rule allows an origin\n   server to provide, for a given response, a longer expiration time to\n   an HTTP/1.1 (or later) cache than to an HTTP/1.0 cache. This might be\n   useful if certain HTTP/1.0 caches improperly calculate ages or\n   expiration times, perhaps due to desynchronized clocks.\n\n   Many HTTP/1.0 cache implementations will treat an Expires value that\n   is less than or equal to the response Date value as being equivalent\n   to the Cache-Control response directive \"no-cache\". If an HTTP/1.1\n   cache receives such a response, and the response does not include a\n   Cache-Control header field, it SHOULD consider the response to be\n   non-cachable in order to retain compatibility with HTTP/1.0 servers.\n\n     Note: An origin server might wish to use a relatively new HTTP\n     cache control feature, such as the \"private\" directive, on a\n     network including older caches that do not understand that\n     feature. The origin server will need to combine the new feature\n     with an Expires field whose value is less than or equal to the\n     Date value. This will prevent older caches from improperly\n     caching the response.\n\n   s-maxage\n     If a response includes an s-maxage directive, then for a shared\n     cache (but not for a private cache), the maximum age specified by\n     this directive overrides the maximum age specified by either the\n     max-age directive or the Expires header. The s-maxage directive\n     also implies the semantics of the proxy-revalidate directive (see\n     section 14.9.4), i.e., that the shared cache MUST NOT use the\n     entry after it becomes stale to respond to a subsequent request\n     without first revalidating it with the origin server. The s-maxage\n     directive is always ignored by a private cache.\n\n   Note that most older caches, not compliant with this specification,\n   do not implement any cache-control directives. An origin server\n   wishing to use a cache-control directive that restricts, but does not\n   prevent, caching by an HTTP/1.1-compliant cache MAY exploit the\n\nFielding, et al                                               [Page 101]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   requirement that the max-age directive overrides the Expires header,\n   and the fact that pre-HTTP/1.1-compliant caches do not observe the\n   max-age directive.\n\n   Other directives allow a user agent to modify the basic expiration\n   mechanism. These directives MAY be specified on a request:\n\n   max-age\n     Indicates that the client is willing to accept a response whose\n     age is no greater than the specified time in seconds. Unless max-\n     stale directive is also included, the client is not willing to\n     accept a stale response.\n\n   min-fresh\n     Indicates that the client is willing to accept a response whose\n     freshness lifetime is no less than its current age plus the\n     specified time in seconds. That is, the client wants a response\n     that will still be fresh for at least the specified number of\n     seconds.\n\n   max-stale\n     Indicates that the client is willing to accept a response that has\n     exceeded its expiration time. If max-stale is assigned a value,\n     then the client is willing to accept a response that has exceeded\n     its expiration time by no more than the specified number of\n     seconds. If no value is assigned to max-stale, then the client is\n     willing to accept a stale response of any age.\n\n   If a cache returns a stale response, either because of a max-stale\n   directive on a request, or because the cache is configured to\n   override the expiration time of a response, the cache MUST attach a\n   Warning header to the stale response, using Warning 110 (Response is\n   stale).\n\n   A cache MAY be configured to return stale responses without\n   validation, but only if this does not conflict with any MUST-level\n   requirements concerning cache validation (e.g., a \"must-revalidate\"\n   cache-control directive).\n\n   If both the new request and the cached entry include \"max-age\"\n   directives, then the lesser of the two values is used for determining\n   the freshness of the cached entry for that request.\n\n\n14.9.4 Cache Revalidation and Reload Controls\n\n   Sometimes a user agent might want or need to insist that a cache\n   revalidate its cache entry with the origin server (and not just with\n   the next cache along the path to the origin server), or to reload its\n   cache entry from the origin server. End-to-end revalidation might be\n   necessary if either the cache or the origin server has overestimated\n   the expiration time of the cached response. End-to-end reload may be\n   necessary if the cache entry has become corrupted for some reason.\n\n\n\nFielding, et al                                               [Page 102]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   End-to-end revalidation may be requested either when the client does\n   not have its own local cached copy, in which case we call it\n   \"unspecified end-to-end revalidation\", or when the client does have a\n   local cached copy, in which case we call it \"specific end-to-end\n   revalidation.\"\n\n   The client can specify these three kinds of action using Cache-\n   Control request directives:\n\n   End-to-end reload\n     The request includes a \"no-cache\" cache-control directive or, for\n     compatibility with HTTP/1.0 clients, \"Pragma: no-cache\". Field\n     names MUST NOT be included with the no-cache directive in a\n     request. The server MUST NOT use a cached copy when responding to\n     such a request.[jg418]\n\n   Specific end-to-end revalidation\n     The request includes a \"max-age=0\" cache-control directive, which\n     forces each cache along the path to the origin server to\n     revalidate its own entry, if any, with the next cache or server.\n     The initial request includes a cache-validating conditional with\n     the client's current validator.\n\n   Unspecified end-to-end revalidation\n     The request includes \"max-age=0\" cache-control directive, which\n     forces each cache along the path to the origin server to\n     revalidate its own entry, if any, with the next cache or server.\n     The initial request does not include a cache-validating\n     conditional; the first cache along the path (if any) that holds a\n     cache entry for this resource includes a cache-validating\n     conditional with its current validator.\n\n   max-age\n     When an intermediate cache is forced, by means of a max-age=0\n     directive, to revalidate its own cache entry, and the client has\n     supplied its own validator in the request, the supplied validator\n     might differ from the validator currently stored with the cache\n     entry. In this case, the cache MAY use either validator in making\n     its own request without affecting semantic transparency.\n\n     However, the choice of validator might affect performance. The\n     best approach is for the intermediate cache to use its own\n     validator when making its request. If the server replies with 304\n     (Not Modified), then the cache can return its now validated copy\n     to the client with a 200 (OK) response. If the server replies with\n     a new entity and cache validator, however, the intermediate cache\n     can compare the returned validator with the one provided in the\n     client's request, using the strong comparison function. If the\n     client's validator is equal to the origin server's, then the\n     intermediate cache simply returns 304 (Not Modified). Otherwise,\n     it returns the new entity with a 200 (OK) response.\n\n     If a request includes the no-cache directive, it SHOULD NOT\n     include min-fresh, max-stale, or max-age.\n\n\nFielding, et al                                               [Page 103]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   only-if-cached\n     In some cases, such as times of extremely poor network\n     connectivity, a client may want a cache to return only those\n     responses that it currently has stored, and not to reload or\n     revalidate with the origin server. To do this, the client may\n     include the only-if-cached directive in a request. If it receives\n     this directive, a cache SHOULD either respond using a cached entry\n     that is consistent with the other constraints of the request, or\n     respond with a 504 (Gateway Timeout) status. However, if a group\n     of caches is being operated as a unified system with good internal\n     connectivity, such a request MAY be forwarded within that group of\n     caches.\n\n   must-revalidate\n     Because a cache MAY be configured to ignore a server's specified\n     expiration time, and because a client request MAY include a max-\n     stale directive (which has a similar effect), the protocol also\n     includes a mechanism for the origin server to require revalidation\n     of a cache entry on any subsequent use. When the must-revalidate\n     directive is present in a response received by a cache, that cache\n     MUST NOT use the entry after it becomes stale to respond to a\n     subsequent request without first revalidating it with the origin\n     server. (I.e., the cache MUST do an end-to-end revalidation every\n     time, if, based solely on the origin server's Expires or max-age\n     value, the cached response is stale.)\n\n     The must-revalidate directive is necessary to support reliable\n     operation for certain protocol features. In all circumstances an\n     HTTP/1.1 cache MUST obey the must-revalidate directive; in\n     particular, if the cache cannot reach the origin server for any\n     reason, it MUST generate a 504 (Gateway Timeout) response.\n\n     Servers SHOULD send the must-revalidate directive if and only if\n     failure to revalidate a request on the entity could result in\n     incorrect operation, such as a silently unexecuted financial\n     transaction. Recipients MUST NOT take any automated action that\n     violates this directive, and MUST NOT automatically provide an\n     unvalidated copy of the entity if revalidation fails.\n\n     Although this is not recommended, user agents operating under\n     severe connectivity constraints MAY violate this directive but, if\n     so, MUST explicitly warn the user that an unvalidated response has\n     been provided. The warning MUST be provided on each unvalidated\n     access, and SHOULD require explicit user confirmation.\n\n   proxy-revalidate\n     The proxy-revalidate directive has the same meaning as the must-\n     revalidate directive, except that it does not apply to non-shared\n     user agent caches. It can be used on a response to an\n     authenticated request to permit the user's cache to store and\n     later return the response without needing to revalidate it (since\n     it has already been authenticated once by that user), while still\n     requiring proxies that service many users to revalidate each time\n     (in order to make sure that each user has been authenticated).\n\n\nFielding, et al                                               [Page 104]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     Note that such authenticated responses also need the public cache\n     control directive in order to allow them to be cached at all.\n\n\n14.9.5 No-Transform Directive\n\n   no-transform\n     Implementors of intermediate caches (proxies) have found it useful\n     to convert the media type of certain entity bodies. A non-\n     transparent proxy might, for example, convert between image\n     formats in order to save cache space or to reduce the amount of\n     traffic on a slow link.\n\n     Serious operational problems occur, however, when these\n     transformations are applied to entity bodies intended for certain\n     kinds of applications. For example, applications for medical\n     imaging, scientific data analysis and those using end-to-end\n     authentication, all depend on receiving an entity body that is bit\n     for bit identical to the original entity-body.\n\n     Therefore, if a message includes the no-transform directive, an\n     intermediate cache or proxy MUST NOT change those headers that are\n     listed in section 13.5.2 as being subject to the no-transform\n     directive. This implies that the cache or proxy MUST NOT change\n     any aspect of the entity-body that is specified by these headers,\n     including the value of the entity-body itself.\n\n\n14.9.6 Cache Control Extensions\n\n   The Cache-Control header field can be extended through the use of one\n   or more cache-extension tokens, each with an optional assigned value.\n   Informational extensions (those which do not require a change in\n   cache behavior) MAY be added without changing the semantics of other\n   directives. Behavioral extensions are designed to work by acting as\n   modifiers to the existing base of cache directives. Both the new\n   directive and the standard directive are supplied, such that\n   applications which do not understand the new directive will default\n   to the behavior specified by the standard directive, and those that\n   understand the new directive will recognize it as modifying the\n   requirements associated with the standard directive. In this way,\n   extensions to the cache-control directives can be made without\n   requiring changes to the base protocol.\n\n   This extension mechanism depends on an HTTP cache obeying all of the\n   cache-control directives defined for its native HTTP-version, obeying\n   certain extensions, and ignoring all directives that it does not\n   understand.\n\n   For example, consider a hypothetical new response directive called\n   community which acts as a modifier to the private directive. We\n   define this new directive to mean that, in addition to any non-shared\n   cache, any cache which is shared only by members of the community\n   named within its value may cache the response. An origin server\n\n\nFielding, et al                                               [Page 105]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   wishing to allow the UCI community to use an otherwise private\n   response in their shared cache(s) could do so by including\n\n          Cache-Control: private, community=\"UCI\"\n\n   A cache seeing this header field will act correctly even if the cache\n   does not understand the community cache-extension, since it will also\n   see and understand the private directive and thus default to the safe\n   behavior.\n\n   Unrecognized cache-directives MUST be ignored; it is assumed that any\n   cache-directive likely to be unrecognized by an HTTP/1.1 cache will\n   be combined with standard directives (or the response's default\n   cachability) such that the cache behavior will remain minimally\n   correct even if the cache does not understand the extension(s).\n\n\n14.10 Connection\n\n   The Connection general-header field allows the sender to specify\n   options that are desired for that particular connection and MUST NOT\n   be communicated by proxies over further connections.\n\n   The Connection header has the following grammar:\n\n          Connection = \"Connection\" \":\" 1#(connection-token)\n\n          connection-token  = token\n\n   HTTP/1.1 proxies MUST parse the Connection header field before a\n   message is forwarded and, for each connection-token in this field,\n   remove any header field(s) from the message with the same name as the\n   connection-token. Connection options are signaled by the presence of\n   a connection-token in the Connection header field, not by any\n   corresponding additional header field(s), since the additional header\n   field may not be sent if there are no parameters associated with that\n   connection option.\n\n   Message headers listed in the Connection header MUST NOT include end-\n   to-end headers, such as Cache-Control.\n\n   HTTP/1.1 defines the \"close\" connection option for the sender to\n   signal that the connection will be closed after completion of the\n   response. For example,\n\n          Connection: close\n\n   in either the request or the response header fields indicates that\n   the connection SHOULD NOT be considered `persistent' (section 8.1)\n   after the current request/response is complete.\n\n   HTTP/1.1 applications that do not support persistent connections MUST\n   include the \"close\" connection option in every message.\n\n\n\nFielding, et al                                               [Page 106]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   A system receiving an HTTP/1.0 (or lower-version) message that\n   includes a Connection header MUST, for each connection-token in this\n   field, remove and ignore any header field(s) from the message with\n   the same name as the connection-token. This protects against mistaken\n   forwarding of such header fields by pre-HTTP/1.1 proxies. See section\n   19.6.2.\n\n\n14.11 Content-Encoding\n\n   The Content-Encoding entity-header field is used as a modifier to the\n   media-type. When present, its value indicates what additional content\n   codings have been applied to the entity-body, and thus what decoding\n   mechanisms MUST be applied in order to obtain the media-type\n   referenced by the Content-Type header field. Content-Encoding is\n   primarily used to allow a document to be compressed without losing\n   the identity of its underlying media type.\n\n          Content-Encoding  = \"Content-Encoding\" \":\" 1#content-coding\n\n   Content codings are defined in section 3.5. An example of its use is\n\n          Content-Encoding: gzip\n\n   The content-coding is a characteristic of the entity identified by\n   the Request-URI. Typically, the entity-body is stored with this\n   encoding and is only decoded before rendering or analogous usage.\n   However, a non-transparent proxy MAY modify the content-coding if the\n   new coding is known to be acceptable to the recipient, unless the\n   \"no-transform\" cache-control directive is present in the message.\n\n   If the content-coding of an entity is not \"identity\", then the\n   response MUST including a Content-Encoding entity-header (section\n   14.11) that lists the non-identity content-coding(s) used.\n\n   If the content-coding of an entity in a request message is not\n   acceptable to the origin server, the server SHOULD respond with a\n   status code of 415 (Unsupported Media Type).\n\n   If multiple encodings have been applied to an entity, the content\n   codings MUST be listed in the order in which they were applied.\n   Additional information about the encoding parameters MAY be provided\n   by other entity-header fields not defined by this specification.\n\n\n14.12 Content-Language\n\n   The Content-Language entity-header field describes the natural\n   language(s) of the intended audience for the enclosed entity. Note\n   that this might not be equivalent to all the languages used within\n   the entity-body.\n\n          Content-Language  = \"Content-Language\" \":\" 1#language-tag\n\n\n\nFielding, et al                                               [Page 107]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Language tags are defined in section 3.10. The primary purpose of\n   Content-Language is to allow a user to identify and differentiate\n   entities according to the user's own preferred language. Thus, if the\n   body content is intended only for a Danish-literate audience, the\n   appropriate field is\n\n          Content-Language: da\n\n   If no Content-Language is specified, the default is that the content\n   is intended for all language audiences. This might mean that the\n   sender does not consider it to be specific to any natural language,\n   or that the sender does not know for which language it is intended.\n\n   Multiple languages MAY be listed for content that is intended for\n   multiple audiences. For example, a rendition of the \"Treaty of\n   Waitangi,\" presented simultaneously in the original Maori and English\n   versions, would call for\n\n          Content-Language: mi, en\n\n   However, just because multiple languages are present within an entity\n   does not mean that it is intended for multiple linguistic audiences.\n   An example would be a beginner's language primer, such as \"A First\n   Lesson in Latin,\" which is clearly intended to be used by an English-\n   literate audience. In this case, the Content-Language would properly\n   only include \"en\".\n\n   Content-Language MAY be applied to any media type -- it is not\n   limited to textual documents.\n\n\n14.13 Content-Length\n\n   The Content-Length entity-header field indicates the size of the\n   entity-body, in decimal number of OCTETs, sent to the recipient or,\n   in the case of the HEAD method, the size of the entity-body that\n   would have been sent had the request been a GET.\n\n          Content-Length    = \"Content-Length\" \":\" 1*DIGIT\n\n   An example is\n\n          Content-Length: 3495\n\n   Applications SHOULD use this field to indicate the transfer-length of\n   the message-body, unless this is prohibited by the rules in section\n   4.4.\n\n   Any Content-Length greater than or equal to zero is a valid value.\n   Section 4.4 describes how to determine the length of a message-body\n   if a Content-Length is not given.\n\n   Note that the meaning of this field is significantly different from\n   the corresponding definition in MIME, where it is an optional field\n   used within the \"message/external-body\" content-type. In HTTP, it\n\nFielding, et al                                               [Page 108]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   SHOULD be sent whenever the message's length can be determined prior\n   to being transferred, unless this is prohibited by the rules in\n   section 4.4.\n\n\n14.14 Content-Location\n\n   The Content-Location entity-header field MAY be used to supply the\n   resource location for the entity enclosed in the message when that\n   entity is accessible from a location separate from the requested\n   resource's URI. A server SHOULD provide a Content-Location for the\n   variant corresponding to the response entity; especially in the case\n   where a resource has multiple entities associated with it, and those\n   entities actually have separate locations by which they might be\n   individually accessed, the server SHOULD provide a Content-Location\n   for the particular variant which is returned.\n\n          Content-Location = \"Content-Location\" \":\"\n                            ( absoluteURI | relativeURI )\n\n   The value of Content-Location also defines the base URI for the\n   entity.\n\n   The Content-Location value is not a replacement for the original\n   requested URI; it is only a statement of the location of the resource\n   corresponding to this particular entity at the time of the request.\n   Future requests MAY specify the Content-Location URI as the request-\n   URI if the desire is to identify the source of that particular\n   entity.\n\n   A cache cannot assume that an entity with a Content-Location\n   different from the URI used to retrieve it can be used to respond to\n   later requests on that Content-Location URI. However, the Content-\n   Location can be used to differentiate between multiple entities\n   retrieved from a single requested resource, as described in section\n   13.6.\n\n   If the Content-Location is a relative URI, the relative URI is\n   interpreted relative to the Request-URI.\n\n   The meaning of the Content-Location header in PUT or POST requests is\n   undefined; servers are free to ignore it in those cases.\n\n\n14.15 Content-MD5\n\n   The Content-MD5 entity-header field, as defined in RFC 1864 [23], is\n   an MD5 digest of the entity-body for the purpose of providing an end-\n   to-end message integrity check (MIC) of the entity-body. (Note: a MIC\n   is good for detecting accidental modification of the entity-body in\n   transit, but is not proof against malicious attacks.)\n\n           Content-MD5   = \"Content-MD5\" \":\" md5-digest\n\n           md5-digest   = <base64 of 128 bit MD5 digest as per RFC 1864>\n\nFielding, et al                                               [Page 109]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n\n   The Content-MD5 header field MAY be generated by an origin server or\n   client to function as an integrity check of the entity-body. Only\n   origin servers or clients may generate the Content-MD5 header field;\n   proxies and gateways MUST NOT generate it, as this would defeat its\n   value as an end-to-end integrity check. Any recipient of the entity-\n   body, including gateways and proxies, MAY check that the digest value\n   in this header field matches that of the entity-body as received.\n\n   The MD5 digest is computed based on the content of the entity-body,\n   including any content-coding that has been applied, but not including\n   any transfer-encoding applied to the message-body. If the message is\n   received with a transfer-encoding, that encoding MUST be removed\n   prior to checking the Content-MD5 value against the received entity.\n\n   This has the result that the digest is computed on the octets of the\n   entity-body exactly as, and in the order that, they would be sent if\n   no transfer-encoding were being applied.\n\n   HTTP extends RFC 1864 to permit the digest to be computed for MIME\n   composite media-types (e.g., multipart/* and message/rfc822), but\n   this does not change how the digest is computed as defined in the\n   preceding paragraph.\n\n   There are several consequences of this. The entity-body for composite\n   types MAY contain many body-parts, each with its own MIME and HTTP\n   headers (including Content-MD5, Content-Transfer-Encoding, and\n   Content-Encoding headers). If a body-part has a Content-Transfer-\n   Encoding or Content-Encoding header, it is assumed that the content\n   of the body-part has had the encoding applied, and the body-part is\n   included in the Content-MD5 digest as is -- i.e., after the\n   application. The Transfer-Encoding header field is not allowed within\n   body-parts.\n\n   Conversion of all line breaks to CRLF MUST NOT be done before\n   computing or checking the digest: the line break convention used in\n   the text actually transmitted MUST be left unaltered when computing\n   the digest.\n\n     Note: while the definition of Content-MD5 is exactly the same\n     for HTTP as in RFC 1864 for MIME entity-bodies, there are\n     several ways in which the application of Content-MD5 to HTTP\n     entity-bodies differs from its application to MIME entity-\n     bodies. One is that HTTP, unlike MIME, does not use Content-\n     Transfer-Encoding, and does use Transfer-Encoding and Content-\n     Encoding. Another is that HTTP more frequently uses binary\n     content types than MIME, so it is worth noting that, in such\n     cases, the byte order used to compute the digest is the\n     transmission byte order defined for the type. Lastly, HTTP\n     allows transmission of text types with any of several line break\n     conventions and not just the canonical form using CRLF.\n\n\n\n\n\nFielding, et al                                               [Page 110]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n14.16 Content-Range\n\n   The Content-Range entity-header is sent with a partial entity-body to\n   specify where in the full entity-body the partial body should be\n   applied. It SHOULD indicate the total length of the full entity-body,\n   unless this length is unknown or difficult to determine. Range units\n   are defined in section 3.12.\n\n          Content-Range = \"Content-Range\" \":\" content-range-spec\n\n          content-range-spec      = byte-content-range-spec\n\n          byte-content-range-spec = bytes-unit SP\n                                    byte-range-resp-spec \"/\"\n                                    ( instance-length | \"*\" )\n\n          byte-range-resp-spec = (first-byte-pos \"-\" last-byte-pos)\n                                         | \"*\"\n\n          instance-length           = 1*DIGIT\n\n   The asterisk \"*\" character means that the instance-length is unknown\n   at the time when the response was generated.\n\n   Unlike byte-ranges-specifier values, a byte-range-resp-spec MUST only\n   specify one range, and MUST contain absolute byte positions for both\n   the first and last byte of the range.\n\n   A byte-content-range-spec with a byte-range-resp-spec whose last-\n   byte-pos value is less than its first-byte-pos value, or whose\n   instance-length value is less than or equal to its last-byte-pos\n   value, is invalid. The recipient of an invalid byte-content-range-\n   spec MUST ignore it and any content transferred along with it.\n\n   A server sending a response with status code 416 (Requested range not\n   satisfiable) SHOULD include a Content-Range field with a byte-range-\n   resp-spec of \"*\". The instance-length specifies the current length of\n   the selected resource. A response with status code 206 (Partial\n   Content) MUST NOT include a Content-Range field with a content-range-\n   spec of \"*\".\n\n   Examples of byte-content-range-spec values, assuming that the entity\n   contains a total of 1234 bytes:\n\n      . The first 500 bytes:\n             bytes 0-499/1234\n      . The second 500 bytes:\n             bytes 500-999/1234\n      . All except for the first 500 bytes:\n             bytes 500-1233/1234\n      . The last 500 bytes:\n             bytes 734-1233/1234\n\n\n\n\nFielding, et al                                               [Page 111]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   When an HTTP message includes the content of a single range (for\n   example, a response to a request for a single range, or to a request\n   for a set of ranges that overlap without any holes), this content is\n   transmitted with a Content-Range header, and a Content-Length header\n   showing the number of bytes actually transferred. For example,\n\n          HTTP/1.1 206 Partial content\n          Date: Wed, 15 Nov 1995 06:25:24 GMT\n          Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n          Content-Range: bytes 21010-47021/47022\n          Content-Length: 26012\n          Content-Type: image/gif\n\n   When an HTTP message includes the content of multiple ranges (for\n   example, a response to a request for multiple non-overlapping\n   ranges), these are transmitted as a multipart message. The multipart\n   media type used for this purpose is \"multipart/byteranges\" as defined\n   in appendix 19.2. See appendix 19.6.3for a compatibility issue.\n\n   A response to a request for a single range MUST NOT be sent using the\n   multipart/byteranges media type.  A response to a request for\n   multiple ranges, whose result is a single range, MAY be sent as a\n   multipart/byteranges media type with one part. A client that cannot\n   decode a multipart/byteranges message MUST NOT ask for multiple byte-\n   ranges in a single request.\n\n   When a client requests multiple byte-ranges in one request, the\n   server SHOULD return them in the order that they appeared in the\n   request.\n\n   If the server ignores a byte-range-spec because it is syntactically\n   invalid, the server SHOULD treat the request as if the invalid Range\n   header field did not exist. (Normally, this means return a 200\n   response containing the full entity).\n\n   If the server receives a request (other than one including an If-\n   Range request-header field) with an unsatisfiable Range request-\n   header field (that is, all of whose byte-range-spec values have a\n   first-byte-pos value greater than the current length of the selected\n   resource), it SHOULD return a response code of 416 (Requested range\n   not satisfiable) (section 10.4.17).\n\n     Note: clients cannot depend on servers to send a 416 (Requested\n     range not satisfiable) response instead of a 200 (OK) response\n     for an unsatisfiable Range request-header, since not all servers\n     implement this request-header.\n\n\n14.17 Content-Type\n\n   The Content-Type entity-header field indicates the media type of the\n   entity-body sent to the recipient or, in the case of the HEAD method,\n   the media type that would have been sent had the request been a GET.\n\n          Content-Type   = \"Content-Type\" \":\" media-type\n\nFielding, et al                                               [Page 112]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n\n   Media types are defined in section 3.7. An example of the field is\n\n          Content-Type: text/html; charset=ISO-8859-4\n\n   Further discussion of methods for identifying the media type of an\n   entity is provided in section 7.2.1.\n\n\n14.18 Date\n\n   The Date general-header field represents the date and time at which\n   the message was originated, having the same semantics as orig-date in\n   RFC 822. The field value is an HTTP-date, as described in section\n   3.3.1; it MUST be sent in RFC 1123 [8]-date format.\n\n          Date  = \"Date\" \":\" HTTP-date\n\n   An example is\n\n          Date: Tue, 15 Nov 1994 08:12:31 GMT\n\n   Origin servers MUST include a Date header field in all responses,\n   except in these cases:\n\n     1. If the response status code is 100 (Continue) or 101 (Switching\n        Protocols), the response MAY include a Date header field, at the\n        server's option.\n\n     2. If the response status code conveys a server error, e.g. 500\n        (Internal Server Error) or 503 (Service Unavailable), and it is\n        inconvenient or impossible to generate a valid Date.\n\n     3. If the server does not have a clock that can provide a\n        reasonable approximation of the current time, its responses MUST\n        NOT include a Date header field. In this case, the rules in\n        section 14.18.1 MUST be followed.\n\n   A received message that does not have a Date header field MUST be\n   assigned one by the recipient if the message will be cached by that\n   recipient or gatewayed via a protocol which requires a Date. An HTTP\n   implementation without a clock MUST NOT cache responses without\n   revalidating them on every use. An HTTP cache, especially a shared\n   cache, SHOULD use a mechanism, such as NTP [28], to synchronize its\n   clock with a reliable external standard.\n\n   Clients SHOULD only send a Date header field in messages that include\n   an entity-body, as in the case of the PUT and POST requests, and even\n   then it is optional. A client without a clock MUST NOT send a Date\n   header field in a request.\n\n   The HTTP-date sent in a Date header SHOULD NOT represent a date and\n   time subsequent to the generation of the message. It SHOULD represent\n   the best available approximation of the date and time of message\n   generation, unless the implementation has no means of generating a\n\nFielding, et al                                               [Page 113]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   reasonably accurate date and time. In theory, the date ought to\n   represent the moment just before the entity is generated. In\n   practice, the date can be generated at any time during the message\n   origination without affecting its semantic value.\n\n\n14.18.1 Clockless Origin Server Operation\n\n   Some origin server implementations might not have a clock available.\n   An origin server without a clock MUST NOT assign Expires or Last-\n   Modified values to a response, unless these values were associated\n   with the resource by a system or user with a reliable clock. It MAY\n   assign an Expires value that is known, at or before server\n   configuration time, to be in the past (this allows \"pre-expiration\"\n   of responses without storing separate Expires values for each\n   resource).\n\n\n14.19 ETag\n\n   The ETag response-header field provides the current value of the\n   entity tag for the requested variant. The headers used with entity\n   tags are described in sections 14.24, 14.26 and 14.44. The entity tag\n   MAY be used for comparison with other entities from the same resource\n   (see section 13.3.3).\n\n         ETag = \"ETag\" \":\" entity-tag\n\n   Examples:\n\n         ETag: \"xyzzy\"\n         ETag: W/\"xyzzy\"\n         ETag: \"\"\n\n14.20 Expect\n\n   The Expect request-header field is used to indicate that particular\n   server behaviors are required by the client. A server that does not\n   understand or is unable to comply with any of the expectation values\n   in the Expect field of a request MUST respond with appropriate error\n   status.\n\n         Expect       =  \"Expect\" \":\" 1#expectation\n\n         expectation  =  \"100-continue\" | expectation-extension\n\n         expectation-extension =  token [ \"=\" ( token | quoted-string )\n                                  *expect-params ]\n\n         expect-params =  \";\" token [ \"=\" ( token | quoted-string ) ]\n\n   The server MUST respond with a 417 (Expectation Failed) status if any\n   of the expectations cannot be met or, if there are other problems\n   with the request, some other 4xx status.\n\n\nFielding, et al                                               [Page 114]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   This header field is defined with extensible syntax to allow for\n   future extensions. If a server receives a request containing an\n   Expect field that includes an expectation-extension that it does not\n   support, it MUST respond with a 417 (Expectation Failed) status.\n\n   Comparison of expectation values is case-insensitive for unquoted\n   tokens (including the 100-continue token), and is case-sensitive for\n   quoted-string expectation-extensions.\n\n   The Expect mechanism is hop-by-hop: that is, an HTTP/1.1 proxy MUST\n   return a 417 (Expectation Failed) status if it receives a request\n   with an expectation that it cannot meet. However, the Expect request-\n   header itself is end-to-end; it MUST be forwarded if the request is\n   forwarded.\n\n   Many older HTTP/1.0 and HTTP/1.1 applications do not understand the\n   Expect header.\n\n\n14.20.1 Expect 100-continue\n\n   When the \"100-continue\" expectation is present on a request that\n   includes a body, the requesting client will wait after sending the\n   request headers before sending the content-body. In this case, the\n   server MUST conform to the requirements of section 8.2.4: it MUST\n   either send a 100 (Continue) status, or an error status, after\n   receiving the \"Expect: 100-continue\" request header.\n\n   If a proxy receives a request with the \"100-continue\" expectation,\n   and the proxy either knows that the next-hop server complies with\n   HTTP/1.1 or higher, or does not know the HTTP version of the next-hop\n   server, it MUST forward the request, including the Expect header\n   field. If the proxy knows that the version of the next-hop server is\n   HTTP/1.0 or lower, it MUST NOT forward the request, and it MUST\n   respond with a 417 (Expectation Failed) status. Proxies SHOULD\n   maintain a cache recording the HTTP version numbers received from\n   recently-referenced next-hop servers.\n\n   Because of the presence of older implementations that do not\n   implement Expect, the protocol allows ambiguous situations in which a\n   client MAY send \"Expect: 100-continue\" without receiving either a 417\n   (Expectation Failed) status or a 100 (Continue) status. Therefore,\n   when a client sends this header field to an origin server (possibly\n   via a proxy) from which it has never seen a 100 (Continue) status,\n   the client need not wait for an indefinite or lengthy period before\n   sending the request body.\n\n\n14.21 Expires\n\n   The Expires entity-header field gives the date/time after which the\n   response is considered stale. A stale cache entry may not normally be\n   returned by a cache (either a proxy cache or a user agent cache)\n   unless it is first validated with the origin server (or with an\n\n\nFielding, et al                                               [Page 115]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   intermediate cache that has a fresh copy of the entity). See section\n   13.2 for further discussion of the expiration model.\n\n   The presence of an Expires field does not imply that the original\n   resource will change or cease to exist at, before, or after that\n   time.\n\n   The format is an absolute date and time as defined by HTTP-date in\n   section 3.3.1; it MUST be in RFC 1123 date format:\n\n         Expires = \"Expires\" \":\" HTTP-date\n\n   An example of its use is\n\n         Expires: Thu, 01 Dec 1994 16:00:00 GMT\n\n     Note: if a response includes a Cache-Control field with the max-\n     age directive, that directive overrides the Expires field.\n\n   HTTP/1.1 clients and caches MUST treat other invalid date formats,\n   especially including the value \"0\", as in the past (i.e., \"already\n   expired\").\n\n   To mark a response as \"already expired,\" an origin server sends an\n   Expires date that is equal to the Date header value. (See the rules\n   for expiration calculations in section 13.2.4.)\n\n   To mark a response as \"never expires,\" an origin server sends an\n   Expires date approximately one year from the time the response is\n   sent. HTTP/1.1 servers SHOULD NOT send Expires dates more than one\n   year in the future.\n\n   The presence of an Expires header field with a date value of some\n   time in the future on a response that otherwise would by default be\n   non-cachable indicates that the response is cachable, unless\n   indicated otherwise by a Cache-Control header field (section 14.9).\n\n\n14.22 From\n\n   The From request-header field, if given, SHOULD contain an Internet\n   e-mail address for the human user who controls the requesting user\n   agent. The address SHOULD be machine-usable, as defined by \"mailbox\"\n   in RFC 822 [9] (as updated by RFC 1123 [8]):\n\n          From   = \"From\" \":\" mailbox\n\n   An example is:\n\n          From: webmaster@w3.org\n\n   This header field MAY be used for logging purposes and as a means for\n   identifying the source of invalid or unwanted requests. It SHOULD NOT\n   be used as an insecure form of access protection. The interpretation\n   of this field is that the request is being performed on behalf of the\n\nFielding, et al                                               [Page 116]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   person given, who accepts responsibility for the method performed. In\n   particular, robot agents SHOULD include this header so that the\n   person responsible for running the robot can be contacted if problems\n   occur on the receiving end.\n\n   The Internet e-mail address in this field MAY be separate from the\n   Internet host which issued the request. For example, when a request\n   is passed through a proxy the original issuer's address SHOULD be\n   used.\n\n     Note: The client SHOULD NOT send the From header field without\n     the user's approval, as it might conflict with the user's\n     privacy interests or their site's security policy. It is\n     strongly recommended that the user be able to disable, enable,\n     and modify the value of this field at any time prior to a\n     request.\n\n\n14.23 Host\n\n   The Host request-header field specifies the Internet host and port\n   number of the resource being requested, as obtained from the original\n   URI given by the user or referring resource (generally an HTTP URL,\n   as described in section 3.2.2). The Host field value MUST represent\n   the naming authority of the origin server or gateway given by the\n   original URL. This allows the origin server or gateway to\n   differentiate between internally-ambiguous URLs, such as the root \"/\"\n   URL of a server for multiple host names on a single IP address.\n\n          Host = \"Host\" \":\" host [ \":\" port ] ; Section 3.2.2\n\n   A \"host\" without any trailing port information implies the default\n   port for the service requested (e.g., \"80\" for an HTTP URL). For\n   example, a request on the origin server for\n   <http://www.w3.org/pub/WWW/> would properly include:\n\n          GET /pub/WWW/ HTTP/1.1\n          Host: www.w3.org\n\n   A client MUST include a Host header field in all HTTP/1.1 request\n   messages on the Internet (i.e., on any message corresponding to a\n   request for a URL which includes an Internet host address for the\n   service being requested). If the Host field is not already present,\n   an HTTP/1.1 proxy MUST add a Host field to the request message prior\n   to forwarding it on the Internet. All Internet-based HTTP/1.1 servers\n   MUST respond with a 400 (Bad Request) status code to any HTTP/1.1\n   request message which lacks a Host header field.\n\n   See sections 5.2 and 19.6.1.1 for other requirements relating to\n   Host.\n\n\n\n\n\n\nFielding, et al                                               [Page 117]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n14.24 If-Match\n\n   The If-Match request-header field is used with a method to make it\n   conditional. A client that has one or more entities previously\n   obtained from the resource can verify that one of those entities is\n   current by including a list of their associated entity tags in the\n   If-Match header field. Entity tags are defined in section 3.11. The\n   purpose of this feature is to allow efficient updates of cached\n   information with a minimum amount of transaction overhead. It is also\n   used, on updating requests, to prevent inadvertent modification of\n   the wrong version of a resource. As a special case, the value \"*\"\n   matches any current entity of the resource.\n\n          If-Match = \"If-Match\" \":\" ( \"*\" | 1#entity-tag )\n\n   If any of the entity tags match the entity tag of the entity that\n   would have been returned in the response to a similar GET request\n   (without the If-Match header) on that resource, or if \"*\" is given\n   and any current entity exists for that resource, then the server MAY\n   perform the requested method as if the If-Match header field did not\n   exist.\n\n   A server MUST use the strong comparison function (see section 13.3.3)\n   to compare the entity tags in If-Match.\n\n   If none of the entity tags match, or if \"*\" is given and no current\n   entity exists, the server MUST NOT perform the requested method, and\n   MUST return a 412 (Precondition Failed) response. This behavior is\n   most useful when the client wants to prevent an updating method, such\n   as PUT, from modifying a resource that has changed since the client\n   last retrieved it.\n\n   If the request would, without the If-Match header field, result in\n   anything other than a 2xx or 412 status, then the If-Match header\n   MUST be ignored.\n\n   The meaning of \"If-Match: *\" is that the method SHOULD be performed\n   if the representation selected by the origin server (or by a cache,\n   possibly using the Vary mechanism, see section 14.44) exists, and\n   MUST NOT be performed if the representation does not exist.\n\n   A request intended to update a resource (e.g., a PUT) MAY include an\n   If-Match header field to signal that the request method MUST NOT be\n   applied if the entity corresponding to the If-Match value (a single\n   entity tag) is no longer a representation of that resource. This\n   allows the user to indicate that they do not wish the request to be\n   successful if the resource has been changed without their knowledge.\n   Examples:\n\n          If-Match: \"xyzzy\"\n          If-Match: \"xyzzy\", \"r2d2xxxx\", \"c3piozzzz\"\n          If-Match: *\n\n\n\n\nFielding, et al                                               [Page 118]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   The result of a request having both an If-Match header field and\n   either an If-None-Match or an If-Modified-Since header fields is\n   undefined by this specification.\n\n\n14.25 If-Modified-Since\n\n   The If-Modified-Since request-header field is used with a method to\n   make it conditional: if the requested variant has not been modified\n   since the time specified in this field, an entity will not be\n   returned from the server; instead, a 304 (not modified) response will\n   be returned without any message-body.\n\n          If-Modified-Since = \"If-Modified-Since\" \":\" HTTP-date\n\n   An example of the field is:\n\n          If-Modified-Since: Sat, 29 Oct 1994 19:43:31 GMT\n\n   A GET method with an If-Modified-Since header and no Range header\n   requests that the identified entity be transferred only if it has\n   been modified since the date given by the If-Modified-Since header.\n   The algorithm for determining this includes the following cases:\n\n\n     a) If the request would normally result in anything other than a\n        200 (OK) status, or if the passed If-Modified-Since date is\n        invalid, the response is exactly the same as for a normal GET. A\n        date which is later than the server's current time is invalid.\n\n\n     b) If the variant has been modified since the If-Modified-Since\n        date, the response is exactly the same as for a normal GET.\n\n\n     c) If the variant has not been modified since a valid If-Modified-\n        Since date, the server SHOULD return a 304 (Not Modified)\n        response.\n\n   The purpose of this feature is to allow efficient updates of cached\n   information with a minimum amount of transaction overhead.\n\n     Note that the Range request-header field modifies the meaning of\n     If-Modified-Since; see section 14.35 for full details.\n\n     Note that If-Modified-Since times are interpreted by the server,\n     whose clock might not be synchronized with the client.\n\n     Note: When handling an If-Modified-Since header field, some\n     servers will use an exact date comparison function, rather than\n     a less-than function, for deciding whether to send a 304 (Not\n     Modified) response. To get best results when sending an If-\n     Modified-Since header field for cache validation, clients are\n     advised to use the exact date string received in a previous\n     Last-Modified header field whenever possible.\n\nFielding, et al                                               [Page 119]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Note that if a client uses an arbitrary date in the If-Modified-Since\n   header instead of a date taken from the Last-Modified header for the\n   same request, the client should be aware of the fact that this date\n   is interpreted in the server's understanding of time. The client\n   should consider unsynchronized clocks and rounding problems due to\n   the different encodings of time between the client and server. This\n   includes the possibility of race conditions if the document has\n   changed between the time it was first requested and the If-Modified-\n   Since date of a subsequent request, and the possibility of clock-\n   skew-related problems if the If-Modified-Since date is derived from\n   the client's clock without correction to the server's clock.\n   Corrections for different time bases between client and server are at\n   best approximate due to network latency.\n\n   The result of a request having both an If-Modified-Since header field\n   and either an If-Match or an If-Unmodified-Since header fields is\n   undefined by this specification.\n\n\n14.26 If-None-Match\n\n   The If-None-Match request-header field is used with a method to make\n   it conditional. A client that has one or more entities previously\n   obtained from the resource can verify that none of those entities is\n   current by including a list of their associated entity tags in the\n   If-None-Match header field. The purpose of this feature is to allow\n   efficient updates of cached information with a minimum amount of\n   transaction overhead. It is also used to prevent a method (e.g. PUT)\n   from inadvertently modifying an existing resource when the client\n   believes that the resource does not exist.\n\n   As a special case, the value \"*\" matches any current entity of the\n   resource.\n\n       If-None-Match = \"If-None-Match\" \":\" ( \"*\" | 1#entity-tag )\n\n   If any of the entity tags match the entity tag of the entity that\n   would have been returned in the response to a similar GET request\n   (without the If-None-Match header) on that resource, or if \"*\" is\n   given and any current entity exists for that resource, then the\n   server MUST NOT perform the requested method, unless required to do\n   so because the resource's modification date fails to match that\n   supplied in an If-Modified-Since header field in the request.\n   Instead, if the request method was GET or HEAD, the server SHOULD\n   respond with a 304 (Not Modified) response, including the cache-\n   related header fields (particularly ETag) of one of the entities that\n   matched. For all other request methods, the server MUST respond with\n   a status of 412 (Precondition Failed).\n\n   See section 13.3.3 for rules on how to determine if two entities tags\n   match. The weak comparison function can only be used with GET or HEAD\n   requests.\n\n   If none of the entity tags match, then the server MAY perform the\n   requested method as if the If-None-Match header field did not exist,\n\nFielding, et al                                               [Page 120]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   but MUST also ignore any If-Modified-Since header field(s) in the\n   request. That is, if no entity tags match, then the server MUST NOT\n   return a 304 (Not Modified) response.\n\n   If \"*\" is given and no current entity exists, then the server MAY\n   perform the requested method as if the If-None-Match header field did\n   not exist.\n\n   If the request would, without the If-None-Match header field, result\n   in anything other than a 2xx or 304 status, then the If-None-Match\n   header MUST be ignored. (See section 13.3.4 for a discussion of\n   server behavior when both If-Modified-Since and If-None-Match appear\n   in the same request.)\n\n   The meaning of \"If-None-Match: *\" is that the method MUST NOT be\n   performed if the representation selected by the origin server (or by\n   a cache, possibly using the Vary mechanism, see section 14.44)\n   exists, and SHOULD be performed if the representation does not exist.\n   This feature is intended to be useful in preventing races between PUT\n   operations.\n\n   Examples:\n\n          If-None-Match: \"xyzzy\"\n          If-None-Match: W/\"xyzzy\"\n          If-None-Match: \"xyzzy\", \"r2d2xxxx\", \"c3piozzzz\"\n          If-None-Match: W/\"xyzzy\", W/\"r2d2xxxx\", W/\"c3piozzzz\"\n          If-None-Match: *\n\n   The result of a request having both an If-None-Match header field and\n   either an If-Match or an If-Unmodified-Since header fields is\n   undefined by this specification.\n\n\n14.27 If-Range\n\n   If a client has a partial copy of an entity in its cache, and wishes\n   to have an up-to-date copy of the entire entity in its cache, it\n   could use the Range request-header with a conditional GET (using\n   either or both of If-Unmodified-Since and If-Match.) However, if the\n   condition fails because the entity has been modified, the client\n   would then have to make a second request to obtain the entire current\n   entity-body.\n\n   The If-Range header allows a client to \"short-circuit\" the second\n   request. Informally, its meaning is `if the entity is unchanged, send\n   me the part(s) that I am missing; otherwise, send me the entire new\n   entity.'\n\n           If-Range = \"If-Range\" \":\" ( entity-tag | HTTP-date )\n\n   If the client has no entity tag for an entity, but does have a Last-\n   Modified date, it MAY use that date in an If-Range header. (The\n   server can distinguish between a valid HTTP-date and any form of\n   entity-tag by examining no more than two characters.) The If-Range\n\nFielding, et al                                               [Page 121]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   header SHOULD only be used together with a Range header, and MUST be\n   ignored if the request does not include a Range header, or if the\n   server does not support the sub-range operation.\n\n   If the entity tag given in the If-Range header matches the current\n   entity tag for the entity, then the server SHOULD provide the\n   specified sub-range of the entity using a 206 (Partial content)\n   response. If the entity tag does not match, then the server SHOULD\n   return the entire entity using a 200 (OK) response.\n\n\n14.28 If-Unmodified-Since\n\n   The If-Unmodified-Since request-header field is used with a method to\n   make it conditional. If the requested resource has not been modified\n   since the time specified in this field, the server SHOULD perform the\n   requested operation as if the If-Unmodified-Since header were not\n   present.\n\n   If the requested variant has been modified since the specified time,\n   the server MUST NOT perform the requested operation, and MUST return\n   a 412 (Precondition Failed).\n\n         If-Unmodified-Since = \"If-Unmodified-Since\" \":\" HTTP-date\n\n   An example of the field is:\n\n          If-Unmodified-Since: Sat, 29 Oct 1994 19:43:31 GMT\n\n   If the request normally (i.e., without the If-Unmodified-Since\n   header) would result in anything other than a 2xx or 412 status, the\n   If-Unmodified-Since header SHOULD be ignored.\n\n   If the specified date is invalid, the header is ignored.\n\n   The result of a request having both an If-Unmodified-Since header\n   field and either an If-None-Match or an If-Modified-Since header\n   fields is undefined by this specification.\n\n\n14.29 Last-Modified\n\n   The Last-Modified entity-header field indicates the date and time at\n   which the origin server believes the variant was last modified.\n\n          Last-Modified  = \"Last-Modified\" \":\" HTTP-date\n\n   An example of its use is\n\n          Last-Modified: Tue, 15 Nov 1994 12:45:26 GMT\n\n   The exact meaning of this header field depends on the implementation\n   of the origin server and the nature of the original resource. For\n   files, it may be just the file system last-modified time. For\n   entities with dynamically included parts, it may be the most recent\n\nFielding, et al                                               [Page 122]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   of the set of last-modify times for its component parts. For database\n   gateways, it may be the last-update time stamp of the record. For\n   virtual objects, it may be the last time the internal state changed.\n\n   An origin server MUST NOT send a Last-Modified date which is later\n   than the server's time of message origination. In such cases, where\n   the resource's last modification would indicate some time in the\n   future, the server MUST replace that date with the message\n   origination date.\n\n   An origin server SHOULD obtain the Last-Modified value of the entity\n   as close as possible to the time that it generates the Date value of\n   its response. This allows a recipient to make an accurate assessment\n   of the entity's modification time, especially if the entity changes\n   near the time that the response is generated.\n\n   HTTP/1.1 servers SHOULD send Last-Modified whenever feasible.\n\n\n14.30 Location\n\n   The Location response-header field is used to redirect the recipient\n   to a location other than the Request-URI for completion of the\n   request or identification of a new resource. For 201 (Created)\n   responses, the Location is that of the new resource which was created\n   by the request. For 3xx responses, the location SHOULD indicate the\n   server's preferred URI for automatic redirection to the resource. The\n   field value consists of a single absolute URI.\n\n          Location       = \"Location\" \":\" absoluteURI\n\n   An example is:\n\n          Location: http://www.w3.org/pub/WWW/People.html\n\n     Note: The Content-Location header field (section 14.14) differs\n     from Location in that the Content-Location identifies the\n     original location of the entity enclosed in the request. It is\n     therefore possible for a response to contain header fields for\n     both Location and Content-Location. Also see section 13.10 for\n     cache requirements of some methods.\n\n\n14.31 Max-Forwards\n\n   The Max-Forwards request-header field provides a mechanism with the\n   TRACE (section 9.8) and OPTIONS (section 9.2) methods to limit the\n   number of proxies or gateways that can forward the request to the\n   next inbound server. This can be useful when the client is attempting\n   to trace a request chain which appears to be failing or looping in\n   mid-chain.\n\n          Max-Forwards   = \"Max-Forwards\" \":\" 1*DIGIT\n\n\n\nFielding, et al                                               [Page 123]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   The Max-Forwards value is a decimal integer indicating the remaining\n   number of times this request message may be forwarded.\n\n   Each proxy or gateway recipient of a TRACE or OPTIONS request\n   containing a Max-Forwards header field MUST check and update its\n   value prior to forwarding the request. If the received value is zero\n   (0), the recipient MUST NOT forward the request; instead, it MUST\n   respond as the final recipient. If the received Max-Forwards value is\n   greater than zero, then the forwarded message MUST contain an updated\n   Max-Forwards field with a value decremented by one (1).\n\n   The Max-Forwards header field MAY be ignored for all other methods\n   defined by this specification and for any extension methods for which\n   it is not explicitly referred to as part of that method definition.\n\n\n14.32 Pragma\n\n   The Pragma general-header field is used to include implementation-\n   specific directives that might apply to any recipient along the\n   request/response chain. All pragma directives specify optional\n   behavior from the viewpoint of the protocol; however, some systems\n   MAY require that behavior be consistent with the directives.\n\n          Pragma            = \"Pragma\" \":\" 1#pragma-directive\n\n          pragma-directive  = \"no-cache\" | extension-pragma\n\n          extension-pragma  = token [ \"=\" ( token | quoted-string ) ]\n\n   When the no-cache directive is present in a request message, an\n   application SHOULD forward the request toward the origin server even\n   if it has a cached copy of what is being requested. This pragma\n   directive has the same semantics as the no-cache cache-directive (see\n   section 14.9) and is defined here for backward compatibility with\n   HTTP/1.0. Clients SHOULD include both header fields when a no-cache\n   request is sent to a server not known to be HTTP/1.1 compliant.\n\n   Pragma directives MUST be passed through by a proxy or gateway\n   application, regardless of their significance to that application,\n   since the directives might be applicable to all recipients along the\n   request/response chain. It is not possible to specify a pragma for a\n   specific recipient; however, any pragma directive not relevant to a\n   recipient SHOULD be ignored by that recipient.\n\n   HTTP/1.1 caches SHOULD treat \"Pragma: no-cache\" as if the client had\n   sent \"Cache-Control: no-cache\". No new Pragma directives will be\n   defined in HTTP.\n\n\n14.33 Proxy-Authenticate\n\n   The Proxy-Authenticate response-header field MUST be included as part\n   of a 407 (Proxy Authentication Required) response. The field value\n\n\nFielding, et al                                               [Page 124]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   consists of a challenge that indicates the authentication scheme and\n   parameters applicable to the proxy for this Request-URI.\n\n          Proxy-Authenticate  = \"Proxy-Authenticate\" \":\" challenge\n\n   The HTTP access authentication process is described in \"HTTP\n   Authentication: Basic and Digest Access Authentication\" [43]. Unlike\n   WWW-Authenticate, the Proxy-Authenticate header field applies only to\n   the current connection and SHOULD NOT be passed on to downstream\n   clients. However, an intermediate proxy might need to obtain its own\n   credentials by requesting them from the downstream client, which in\n   some circumstances will appear as if the proxy is forwarding the\n   Proxy-Authenticate header field.\n\n\n14.34 Proxy-Authorization\n\n   The Proxy-Authorization request-header field allows the client to\n   identify itself (or its user) to a proxy which requires\n   authentication. The Proxy-Authorization field value consists of\n   credentials containing the authentication information of the user\n   agent for the proxy and/or realm of the resource being requested.\n\n          Proxy-Authorization  = \"Proxy-Authorization\" \":\" Credentials\n\n   The HTTP access authentication process is described in \"HTTP\n   Authentication: Basic and Digest Access Authentication\" [43] . Unlike\n   Authorization, the Proxy-Authorization header field applies only to\n   the next outbound proxy that demanded authentication using the Proxy-\n   Authenticate field. When multiple proxies are used in a chain, the\n   Proxy-Authorization header field is consumed by the first outbound\n   proxy that was expecting to receive credentials. A proxy MAY relay\n   the credentials from the client request to the next proxy if that is\n   the mechanism by which the proxies cooperatively authenticate a given\n   request.\n\n\n14.35 Range\n\n\n14.35.1 Byte Ranges\n\n   Since all HTTP entities are represented in HTTP messages as sequences\n   of bytes, the concept of a byte range is meaningful for any HTTP\n   entity. (However, not all clients and servers need to support byte-\n   range operations.)\n\n   Byte range specifications in HTTP apply to the sequence of bytes in\n   the entity-body (not necessarily the same as the message-body).\n\n   A byte range operation MAY specify a single range of bytes, or a set\n   of ranges within a single entity.\n\n          ranges-specifier = byte-ranges-specifier\n\n\nFielding, et al                                               [Page 125]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n          byte-ranges-specifier = bytes-unit \"=\" byte-range-set\n\n          byte-range-set = 1#( byte-range-spec |suffix-byte-range-spec )\n\n          byte-range-spec = first-byte-pos \"-\" [last-byte-pos]\n\n          first-byte-pos  = 1*DIGIT\n\n          last-byte-pos   = 1*DIGIT\n\n   The first-byte-pos value in a byte-range-spec gives the byte-offset\n   of the first byte in a range. The last-byte-pos value gives the byte-\n   offset of the last byte in the range; that is, the byte positions\n   specified are inclusive. Byte offsets start at zero.\n\n   If the last-byte-pos value is present, it MUST be greater than or\n   equal to the first-byte-pos in that byte-range-spec, or the byte-\n   range-spec is syntactically invalid. The recipient of a byte-range-\n   set that includes one or more syntactically invalid byte-range-spec\n   values MUST ignore the header field that includes that byte-range-\n   set.\n\n   If the last-byte-pos value is absent, or if the value is greater than\n   or equal to the current length of the entity-body, last-byte-pos is\n   taken to be equal to one less than the current length of the entity-\n   body in bytes.\n\n   By its choice of last-byte-pos, a client can limit the number of\n   bytes retrieved without knowing the size of the entity.\n\n          suffix-byte-range-spec = \"-\" suffix-length\n\n          suffix-length = 1*DIGIT\n\n   A suffix-byte-range-spec is used to specify the suffix of the entity-\n   body, of a length given by the suffix-length value. (That is, this\n   form specifies the last N bytes of an entity-body.) If the entity is\n   shorter than the specified suffix-length, the entire entity-body is\n   used.\n\n   If a syntactically valid byte-range-set includes at least one byte-\n   range-spec whose first-byte-pos is less than the current length of\n   the entity-body, or at least one suffix-byte-range-spec with a non-\n   zero suffix-length, then the byte-range-set is satisfiable.\n   Otherwise, the byte-range-set is unsatisfiable. If the byte-range-set\n   is unsatisfiable, the server SHOULD return a response with a status\n   of 416 (Requested range not satisfiable). Otherwise, the server\n   SHOULD return a response with a status of 206 (Partial Content)\n   containing the satisfiable ranges of the entity-body.\n\n   Examples of byte-ranges-specifier values (assuming an entity-body of\n   length 10000):\n\n     . The first 500 bytes (byte offsets 0-499, inclusive):\n            bytes=0-499\n\nFielding, et al                                               [Page 126]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n     . The second 500 bytes (byte offsets 500-999, inclusive):\n            bytes=500-999\n     . The final 500 bytes (byte offsets 9500-9999, inclusive):\n            bytes=-500\n     . Or\n            bytes=9500-\n     . The first and last bytes only (bytes 0 and 9999):\n            bytes=0-0,-1\n     . Several legal but not canonical specifications of the second 500\n       bytes (byte offsets 500-999, inclusive):\n            bytes=500-600,601-999\n            bytes=500-700,601-999\n\n14.35.2 Range Retrieval Requests\n\n   HTTP retrieval requests using conditional or unconditional GET\n   methods MAY request one or more sub-ranges of the entity, instead of\n   the entire entity, using the Range request header, which applies to\n   the entity returned as the result of the request:\n\n         Range = \"Range\" \":\" ranges-specifier\n\n   A server MAY ignore the Range header. However, HTTP/1.1 origin\n   servers and intermediate caches ought to support byte ranges when\n   possible, since Range supports efficient recovery from partially\n   failed transfers, and supports efficient partial retrieval of large\n   entities.\n\n   If the server supports the Range header and the specified range or\n   ranges are appropriate for the entity:\n\n     .  The presence of a Range header in an unconditional GET modifies\n        what is returned if the GET is otherwise successful. In other\n        words, the response carries a status code of 206 (Partial\n        Content) instead of 200 (OK).\n     .  The presence of a Range header in a conditional GET (a request\n        using one or both of If-Modified-Since and If-None-Match, or one\n        or both of If-Unmodified-Since and If-Match) modifies what is\n        returned if the GET is otherwise successful and the condition is\n        true. It does not affect the 304 (Not Modified) response\n        returned if the conditional is false.\n   In some cases, it might be more appropriate to use the If-Range\n   header (see section 14.27) in addition to the Range header.\n\n   If a proxy that supports ranges receives a Range request, forwards\n   the request to an inbound server, and receives an entire entity in\n   reply, it SHOULD only return the requested range to its client. It\n   SHOULD store the entire received response in its cache, if that is\n   consistent with its cache allocation policies.\n\n\n14.36 Referer\n\n   The Referer[sic] request-header field allows the client to specify,\n   for the server's benefit, the address (URI) of the resource from\n\nFielding, et al                                               [Page 127]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   which the Request-URI was obtained (the \"referrer\", although the\n   header field is misspelled.) The Referer request-header allows a\n   server to generate lists of back-links to resources for interest,\n   logging, optimized caching, etc. It also allows obsolete or mistyped\n   links to be traced for maintenance. The Referer field MUST NOT be\n   sent if the Request-URI was obtained from a source that does not have\n   its own URI, such as input from the user keyboard.\n\n          Referer        = \"Referer\" \":\" ( absoluteURI | relativeURI )\n\n   Example:\n\n          Referer: http://www.w3.org/hypertext/DataSources/Overview.html\n\n   If the field value is a partial URI, it SHOULD be interpreted\n   relative to the Request-URI. The URI MUST NOT include a fragment. See\n   section 15.1.3 for security considerations.\n\n\n14.37 Retry-After\n\n   The Retry-After response-header field can be used with a 503 (Service\n   Unavailable) response to indicate how long the service is expected to\n   be unavailable to the requesting client. This field MAY also be used\n   with any 3xx (Redirection) response to indicate the minimum time the\n   user-agent is asked wait before issuing the redirected request. The\n   value of this field can be either an HTTP-date or an integer number\n   of seconds (in decimal) after the time of the response.\n\n          Retry-After  = \"Retry-After\" \":\" ( HTTP-date | delta-seconds )\n\n   Two examples of its use are\n\n          Retry-After: Fri, 31 Dec 1999 23:59:59 GMT\n          Retry-After: 120\n\n   In the latter example, the delay is 2 minutes.\n\n\n14.38 Server\n\n   The Server response-header field contains information about the\n   software used by the origin server to handle the request. The field\n   can contain multiple product tokens (section 3.8) and comments\n   identifying the server and any significant subproducts. The product\n   tokens are listed in order of their significance for identifying the\n   application.\n\n          Server         = \"Server\" \":\" 1*( product | comment )\n\n   Example:\n\n          Server: CERN/3.0 libwww/2.17\n\n\n\nFielding, et al                                               [Page 128]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   If the response is being forwarded through a proxy, the proxy\n   application MUST NOT modify the Server response-header. Instead, it\n   SHOULD include a Via field (as described in section 14.45).\n\n     Note: Revealing the specific software version of the server\n     might allow the server machine to become more vulnerable to\n     attacks against software that is known to contain security\n     holes. Server implementors are encouraged to make this field a\n     configurable option.\n\n\n14.39 TE\n\n   The TE request-header field indicates what extension transfer-codings\n   it is willing to accept in the response and whether or not it is\n   willing to accept trailer fields in a chunked transfer-coding. Its\n   value may consist of the keyword \"trailers\" and/or a comma-separated\n   list of extension transfer-coding names with optional accept\n   parameters (as described in section 3.9).\n\n          TE           = \"TE\" \":\" #( t-codings )\n\n          t-codings   = \"trailers\"\n                      | ( transfer-extension [ accept-params ] )\n\n   The presence of the keyword \"trailers\" indicates that the client is\n   willing to accept trailer fields in a chunked transfer-coding, as\n   defined in section 3.6.1. This keyword is reserved for use with\n   transfer-coding values even though it does not itself represent a\n   transfer-coding.\n\n   Examples of its use are:\n\n          TE: deflate\n          TE:\n          TE: trailers, deflate;q=0.5\n\n   The TE header field only applies to the immediate connection.\n   Therefore, the keyword MUST be supplied within a Connection header\n   field (section 14.10) whenever TE is present in an HTTP/1.1 message.\n\n   A server tests whether a transfer-coding is acceptable, according to\n   a TE field, using these rules:\n\n     1.         The \"chunked\" transfer-coding is always acceptable. If the\n        keyword \"trailers\" is listed, the client indicates that it is\n        willing to accept trailer fields in the chunked response on\n        behalf of itself and any downstream clients. The implication is\n        that, if given, the client is stating that either all downstream\n        clients are willing to accept trailer fields in the forwarded\n        response, or that it will attempt to buffer the response on\n        behalf of downstream recipients.\n\n        Note: HTTP/1.1 does not define any means to limit the size of a\n\n\nFielding, et al                                               [Page 129]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n        chunked response such that a client can be assured of buffering\n        the entire response.\n\n     2. If the transfer-coding being tested is one of the transfer-\n        codings listed in the TE field, then it is acceptable unless it\n        is accompanied by a qvalue of 0. (As defined in section 3.9, a\n        qvalue of 0 means \"not acceptable.\")\n\n     3. If multiple transfer-codings are acceptable, then the acceptable\n        transfer-coding with the highest non-zero qvalue is preferred.\n        The \"chunked\" transfer-coding always has a qvalue of 1.\n\n     4. If the TE field-value is empty or if no TE field is present, the\n        only transfer-coding  is \"chunked\". A message with no transfer-\n        coding is always acceptable.\n\n\n\n\n14.40 Trailer\n\n   The Trailer general field value indicates that the given set of\n   header fields is present in the trailer of a message encoded with\n   chunked transfer-coding.\n\n          Trailer  = \"Trailer\" \":\" 1#field-name\n\n   An HTTP/1.1 sender SHOULD include a Trailer header field in a message\n   using chunked transfer-coding with a non-empty trailer. Doing so\n   allows the recipient to know which header fields to expect in the\n   trailer.\n\n   If no Trailer header field is present, the trailer SHOULD NOT include\n   any header fields. See section 3.6.1 for restrictions on the use of\n   trailer fields in a \"chunked\" transfer-coding.\n\n   A server MUST NOT include any header fields unless the \"chunked\"\n   transfer-coding is present in the request as an accepted transfer-\n   coding in the TE field.\n\n   Message header fields listed in the Trailer header field MUST NOT\n   include the following header fields:\n\n     . Transfer-Encoding\n\n     . Content-Length\n\n     . Trailer\n\n\n14.41 Transfer-Encoding\n\n   The Transfer-Encoding general-header field indicates what (if any)\n   type of transformation has been applied to the message body in order\n   to safely transfer it between the sender and the recipient. This\n\nFielding, et al                                               [Page 130]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   differs from the content-coding in that the transfer-coding is a\n   property of the message, not of the entity.\n\n          Transfer-Encoding = \"Transfer-Encoding\" \":\" 1#transfer-coding\n\n   Transfer-codings are defined in section 3.6. An example is:\n\n          Transfer-Encoding: chunked\n\n   If multiple encodings have been applied to an entity, the transfer-\n   codings MUST be listed in the order in which they were applied.\n   Additional information about the encoding parameters MAY be provided\n   by other entity-header fields not defined by this specification.\n\n   Many older HTTP/1.0 applications do not understand the Transfer-\n   Encoding header.\n\n\n14.42 Upgrade\n\n   The Upgrade general-header allows the client to specify what\n   additional communication protocols it supports and would like to use\n   if the server finds it appropriate to switch protocols. The server\n   MUST use the Upgrade header field within a 101 (Switching Protocols)\n   response to indicate which protocol(s) are being switched.\n\n          Upgrade        = \"Upgrade\" \":\" 1#product\n\n   For example,\n\n          Upgrade: HTTP/2.0, SHTTP/1.3, IRC/6.9, RTA/x11\n\n   The Upgrade header field is intended to provide a simple mechanism\n   for transition from HTTP/1.1 to some other, incompatible protocol. It\n   does so by allowing the client to advertise its desire to use another\n   protocol, such as a later version of HTTP with a higher major version\n   number, even though the current request has been made using HTTP/1.1.\n   This eases the difficult transition between incompatible protocols by\n   allowing the client to initiate a request in the more commonly\n   supported protocol while indicating to the server that it would like\n   to use a \"better\" protocol if available (where \"better\" is determined\n   by the server, possibly according to the nature of the method and/or\n   resource being requested).\n\n   The Upgrade header field only applies to switching application-layer\n   protocols upon the existing transport-layer connection. Upgrade\n   cannot be used to insist on a protocol change; its acceptance and use\n   by the server is optional. The capabilities and nature of the\n   application-layer communication after the protocol change is entirely\n   dependent upon the new protocol chosen, although the first action\n   after changing the protocol MUST be a response to the initial HTTP\n   request containing the Upgrade header field.\n\n   The Upgrade header field only applies to the immediate connection.\n   Therefore, the upgrade keyword MUST be supplied within a Connection\n\nFielding, et al                                               [Page 131]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   header field (section 14.10) whenever Upgrade is present in an\n   HTTP/1.1 message.\n\n   The Upgrade header field cannot be used to indicate a switch to a\n   protocol on a different connection. For that purpose, it is more\n   appropriate to use a 301, 302, 303, or 305 redirection response.\n\n   This specification only defines the protocol name \"HTTP\" for use by\n   the family of Hypertext Transfer Protocols, as defined by the HTTP\n   version rules of section 3.1 and future updates to this\n   specification. Any token can be used as a protocol name; however, it\n   will only be useful if both the client and server associate the name\n   with the same protocol.\n\n\n14.43 User-Agent\n\n   The User-Agent request-header field contains information about the\n   user agent originating the request. This is for statistical purposes,\n   the tracing of protocol violations, and automated recognition of user\n   agents for the sake of tailoring responses to avoid particular user\n   agent limitations. User agents SHOULD include this field with\n   requests. The field can contain multiple product tokens (section 3.8)\n   and comments identifying the agent and any subproducts which form a\n   significant part of the user agent. By convention, the product tokens\n   are listed in order of their significance for identifying the\n   application.\n\n          User-Agent     = \"User-Agent\" \":\" 1*( product | comment )\n\n   Example:\n\n          User-Agent: CERN-LineMode/2.15 libwww/2.17b3\n\n14.44 Vary\n\n   The Vary field value indicates the set of request-header fields that\n   fully determines, while the response is fresh, whether a cache is\n   permitted to use the response to reply to a subsequent request\n   without revalidation. For uncachable or stale responses, the Vary\n   field value advises the user agent about the criteria that were used\n   to select the representation. A Vary field value of \"*\" implies that\n   a cache cannot determine from the request headers of a subsequent\n   request whether this response is the appropriate representation. See\n   section 13.6 for use of the Vary header field by caches.\n\n          Vary  = \"Vary\" \":\" ( \"*\" | 1#field-name )\n\n   An HTTP/1.1 server SHOULD include a Vary header field with any\n   cachable response that is subject to server-driven negotiation. Doing\n   so allows a cache to properly interpret future requests on that\n   resource and informs the user agent about the presence of negotiation\n   on that resource. A server MAY include a Vary header field with a\n   non-cachable response that is subject to server-driven negotiation,\n   since this might provide the user agent with useful information about\n\nFielding, et al                                               [Page 132]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   the dimensions over which the response varies at the time of the\n   response.\n\n   A Vary field value consisting of a list of field-names signals that\n   the representation selected for the response is based on a selection\n   algorithm which considers ONLY the listed request-header field values\n   in selecting the most appropriate representation. A cache MAY assume\n   that the same selection will be made for future requests with the\n   same values for the listed field names, for the duration of time for\n   which the response is fresh.\n\n   The field-names given are not limited to the set of standard request-\n   header fields defined by this specification. Field names are case-\n   insensitive.\n\n   A Vary field value of \"*\" signals that unspecified parameters not\n   limited to the request-headers (e.g., the network address of the\n   client), play a role in the selection of the response representation.\n   The \"*\" value MUST NOT be generated by a proxy server; it may only be\n   generated by an origin server.\n\n\n14.45  Via\n\n   The Via general-header field MUST be used by gateways and proxies to\n   indicate the intermediate protocols and recipients between the user\n   agent and the server on requests, and between the origin server and\n   the client on responses. It is analogous to the \"Received\" field of\n   RFC 822 [9]                                         and is intended to be used for tracking message forwards,\n   avoiding request loops, and identifying the protocol capabilities of\n   all senders along the request/response chain.\n\n         Via = \"Via\" \":\" 1#( received-protocol received-by [ comment ] )\n\n         received-protocol = [ protocol-name \"/\" ] protocol-version\n\n         protocol-name     = token\n\n         protocol-version  = token\n\n         received-by       = ( host [ \":\" port ] ) | pseudonym\n\n         pseudonym         = token\n\n   The received-protocol indicates the protocol version of the message\n   received by the server or client along each segment of the\n   request/response chain. The received-protocol version is appended to\n   the Via field value when the message is forwarded so that information\n   about the protocol capabilities of upstream applications remains\n   visible to all recipients.\n\n   The protocol-name is optional if and only if it would be \"HTTP\". The\n   received-by field is normally the host and optional port number of a\n   recipient server or client that subsequently forwarded the message.\n   However, if the real host is considered to be sensitive information,\n\nFielding, et al                                               [Page 133]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   it MAY be replaced by a pseudonym. If the port is not given, it MAY\n   be assumed to be the default port of the received-protocol.\n\n   Multiple Via field values represents each proxy or gateway that has\n   forwarded the message. Each recipient MUST append its information\n   such that the end result is ordered according to the sequence of\n   forwarding applications.\n\n   Comments MAY be used in the Via header field to identify the software\n   of the recipient proxy or gateway, analogous to the User-Agent and\n   Server header fields. However, all comments in the Via field are\n   optional and MAY be removed by any recipient prior to forwarding the\n   message.\n\n   For example, a request message could be sent from an HTTP/1.0 user\n   agent to an internal proxy code-named \"fred\", which uses HTTP/1.1 to\n   forward the request to a public proxy at nowhere.com, which completes\n   the request by forwarding it to the origin server at www.ics.uci.edu.\n   The request received by www.ics.uci.edu would then have the following\n   Via header field:\n\n          Via: 1.0 fred, 1.1 nowhere.com (Apache/1.1)\n   Proxies and gateways used as a portal through a network firewall\n   SHOULD NOT, by default, forward the names and ports of hosts within\n   the firewall region. This information SHOULD only be propagated if\n   explicitly enabled. If not enabled, the received-by host of any host\n   behind the firewall SHOULD be replaced by an appropriate pseudonym\n   for that host.\n\n   For organizations that have strong privacy requirements for hiding\n   internal structures, a proxy MAY combine an ordered subsequence of\n   Via header field entries with identical received-protocol values into\n   a single such entry. For example,\n\n          Via: 1.0 ricky, 1.1 ethel, 1.1 fred, 1.0 lucy\n\n        could be collapsed to\n\n          Via: 1.0 ricky, 1.1 mertz, 1.0 lucy\n\n   Applications SHOULD NOT combine multiple entries unless they are all\n   under the same organizational control and the hosts have already been\n   replaced by pseudonyms. Applications MUST NOT combine entries which\n   have different received-protocol values.\n\n\n14.46 Warning\n\n   The Warning general-header field is used to carry additional\n   information about the status or transformation of a message which\n   might not be reflected in the message. This information is typically\n   used to warn about a possible lack of semantic transparency from\n   caching operations or transformations applied to the entity body of\n   the message.\n\n\nFielding, et al                                               [Page 134]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Warning headers are sent with responses using:\n\n          Warning    = \"Warning\" \":\" 1#warning-value\n\n          warning-value = warn-code SP warn-agent SP warn-text\n                                                [SP warn-date]\n\n          warn-code  = 3DIGIT\n\n          warn-agent = ( host [ \":\" port ] ) | pseudonym\n                          ; the name or pseudonym of the server adding\n                          ; the Warning header, for use in debugging\n\n          warn-text  = quoted-string\n\n          warn-date  = <\"> HTTP-date <\">\n\n   A response MAY carry more than one Warning header.\n\n   The warn-text SHOULD be in a natural language and character set that\n   is most likely to be intelligible to the human user receiving the\n   response. This decision MAY be based on any available knowledge, such\n   as the location of the cache or user, the Accept-Language field in a\n   request, the Content-Language field in a response, etc. The default\n   language is English and the default character set is ISO-8859-1.\n\n   If a character set other than ISO-8859-1 is used, it MUST be encoded\n   in the warn-text using the method described in RFC 2047 [14].\n\n    Warning headers can in general be applied to any message, however\n   some specific warn-codes are specific to caches and can only be\n   applied to response messages. New Warning headers SHOULD be added\n   after any existing Warning headers. A cache MUST NOT delete any\n   Warning header that it received with a message. However, if a cache\n   successfully validates a cache entry, it SHOULD remove any Warning\n   headers previously attached to that entry except as specified for\n   specific Warning codes. It MUST then add any Warning headers received\n   in the validating response. In other words, Warning headers are those\n   that would be attached to the most recent relevant response.\n\n   When multiple Warning headers are attached to a response, the user\n   agent ought to inform the user of as many of them as possible, in the\n   order that they appear in the response. If it is not possible to\n   inform the user of all of the warnings, the user agent SHOULD follow\n   these heuristics:\n\n     .  Warnings that appear early in the response take priority over\n        those appearing later in the response.\n     .  Warnings in the user's preferred character set take priority\n        over warnings in other character sets but with identical warn-\n        codes and warn-agents.\n   Systems that generate multiple Warning headers SHOULD order them with\n   this user agent behavior in mind.\n\n\n\nFielding, et al                                               [Page 135]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   The warn-code consists of three digits. The first digit indicates\n   whether the Warning MUST or MUST NOT be deleted from a stored cache\n   entry after a successful revalidation:\n\n      1xx  Warnings that describe the freshness or revalidation status\n           of the response, and so MUST be deleted after a successful\n           revalidation. 1XX warn-codes MAY be generated by a cache\n           only when validating a cached entry. It MUST NOT be\n           generated by clients.\n\n      2xx  Warnings that describe some aspect of the entity body or\n           entity headers that is not rectified by a revalidation, and\n           which MUST NOT be deleted after a successful revalidation.\n\n   This is a list of the currently-defined warn-codes, each with a\n   recommended warn-text in English, and a description of its meaning.\n\n      110 Response is stale\n          MUST be included whenever the returned response is stale.\n\n      111 Revalidation failed\n          MUST be included if a cache returns a stale response because\n          an attempt to revalidate the response failed, due to an\n          inability to reach the server.\n\n      112 Disconnected operation\n          SHOULD be included if the cache is intentionally disconnected\n          from the rest of the network for a period of time.\n\n      113 Heuristic expiration\n          MUST be included if the cache heuristically chose a freshness\n          lifetime greater than 24 hours and the response's age is\n          greater than 24 hours.\n\n      199 Miscellaneous warning\n          The warning text MAY include arbitrary information to be\n          presented to a human user, or logged. A system receiving this\n          warning MUST NOT take any automated action, besides presenting\n          the warning to the user.\n\n      214 Transformation applied\n          MUST be added by an intermediate cache or proxy if it applies\n          any transformation changing the content-coding (as specified\n          in the Content-Encoding header) or media-type (as specified in\n          the Content-Type header) of the response, or the entity-body\n          of the response, unless this Warning code already appears in\n          the response.\n\n      299 Miscellaneous persistent warning\n          The warning text MAY include arbitrary information to be\n          presented to a human user, or logged. A system receiving this\n          warning MUST NOT take any automated action.\n\n\n\n\nFielding, et al                                               [Page 136]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   If an implementation sends a message with one or more Warning headers\n   whose version is HTTP/1.0 or lower, then the sender MUST include in\n   each warning-value a warn-date that matches the date in the response.\n\n   If an implementation receives a message with a warning-value that\n   includes a warn-date, and that warn-date is different from the Date\n   value in the response, then that warning-value MUST be deleted from\n   the message before storing, forwarding, or using it. (This prevents\n   bad consequences of naive caching of Warning header fields.) If all\n   of the warning-values are deleted for this reason, the Warning header\n   MUST be deleted as well.\n\n\n14.47 WWW-Authenticate\n\n   The WWW-Authenticate response-header field MUST be included in 401\n   (Unauthorized) response messages. The field value consists of at\n   least one challenge that indicates the authentication scheme(s) and\n   parameters applicable to the Request-URI.\n\n          WWW-Authenticate  = \"WWW-Authenticate\" \":\" 1#challenge\n\n   The HTTP access authentication process is described in \"HTTP\n   Authentication: Basic and Digest Access Authentication\" [43]. User\n   agents are advised to take special care in parsing the WWW-\n   Authenticate field value as it might contain more than one challenge,\n   or if more than one WWW-Authenticate header field is provided, the\n   contents of a challenge itself can contain a comma-separated list of\n   authentication parameters.\n\n\n15 Security Considerations\n\n   This section is meant to inform application developers, information\n   providers, and users of the security limitations in HTTP/1.1 as\n   described by this document. The discussion does not include\n   definitive solutions to the problems revealed, though it does make\n   some suggestions for reducing security risks.\n\n\n15.1 Personal Information\n\n   HTTP clients are often privy to large amounts of personal information\n   (e.g. the user's name, location, mail address, passwords, encryption\n   keys, etc.), and SHOULD be very careful to prevent unintentional\n   leakage of this information via the HTTP protocol to other sources.\n   We very strongly recommend that a convenient interface be provided\n   for the user to control dissemination of such information, and that\n   designers and implementors be particularly careful in this area.\n   History shows that errors in this area often create serious security\n   and/or privacy problems and generate highly adverse publicity for the\n   implementor's company.\n\n\n\n\nFielding, et al                                               [Page 137]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n15.1.1 Abuse of Server Log Information\n\n   A server is in the position to save personal data about a user's\n   requests which might identify their reading patterns or subjects of\n   interest. This information is clearly confidential in nature and its\n   handling can be constrained by law in certain countries. People using\n   the HTTP protocol to provide data are responsible for ensuring that\n   such material is not distributed without the permission of any\n   individuals that are identifiable by the published results.\n\n\n15.1.2 Transfer of Sensitive Information\n\n   Like any generic data transfer protocol, HTTP cannot regulate the\n   content of the data that is transferred, nor is there any a priori\n   method of determining the sensitivity of any particular piece of\n   information within the context of any given request. Therefore,\n   applications SHOULD supply as much control over this information as\n   possible to the provider of that information. Four header fields are\n   worth special mention in this context: Server, Via, Referer and From.\n\n   Revealing the specific software version of the server might allow the\n   server machine to become more vulnerable to attacks against software\n   that is known to contain security holes. Implementors SHOULD make the\n   Server header field a configurable option.\n\n   Proxies which serve as a portal through a network firewall SHOULD\n   take special precautions regarding the transfer of header information\n   that identifies the hosts behind the firewall. In particular, they\n   SHOULD remove, or replace with sanitized versions, any Via fields\n   generated behind the firewall.\n\n   The Referer header allows reading patterns to be studied and reverse\n   links drawn. Although it can be very useful, its power can be abused\n   if user details are not separated from the information contained in\n   the Referer. Even when the personal information has been removed, the\n   Referer header might indicate a private document's URI whose\n   publication would be inappropriate.\n\n   The information sent in the From field might conflict with the user's\n   privacy interests or their site's security policy, and hence it\n   SHOULD NOT be transmitted without the user being able to disable,\n   enable, and modify the contents of the field. The user MUST be able\n   to set the contents of this field within a user preference or\n   application defaults configuration.\n\n   We suggest, though do not require, that a convenient toggle interface\n   be provided for the user to enable or disable the sending of From and\n   Referer information.\n\n   The User-Agent (section 14.43) or Server (section 14.38) header\n   fields can sometimes be used to determine that a specific client or\n   server have a particular security hole which might be exploited.\n   Unfortunately, this same information is often used for other valuable\n   purposes for which HTTP currently has no better mechanism.\n\nFielding, et al                                               [Page 138]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n15.1.3 Encoding Sensitive Information in URI's\n\n   Because the source of a link might be private information or might\n   reveal an otherwise private information source, it is strongly\n   recommended that the user be able to select whether or not the\n   Referer field is sent. For example, a browser client could have a\n   toggle switch for browsing openly/anonymously, which would\n   respectively enable/disable the sending of Referer and From\n   information.\n\n   Clients SHOULD NOT include a Referer header field in a (non-secure)\n   HTTP request if the referring page was transferred with a secure\n   protocol.\n\n   Authors of services which use the HTTP protocol SHOULD NOT use GET\n   based forms for the submission of sensitive data, because this will\n   cause this data to be encoded in the Request-URI. Many existing\n   servers, proxies, and user agents will log the request URI in some\n   place where it might be visible to third parties. Servers can use\n   POST-based form submission instead\n\n\n15.1.4 Privacy Issues Connected to Accept Headers\n\n   Accept request-headers can reveal information about the user to all\n   servers which are accessed. The Accept-Language header in particular\n   can reveal information the user would consider to be of a private\n   nature, because the understanding of particular languages is often\n   strongly correlated to the membership of a particular ethnic group.\n   User agents which offer the option to configure the contents of an\n   Accept-Language header to be sent in every request are strongly\n   encouraged to let the configuration process include a message which\n   makes the user aware of the loss of privacy involved.\n\n   An approach that limits the loss of privacy would be for a user agent\n   to omit the sending of Accept-Language headers by default, and to ask\n   the user whether or not to start sending Accept-Language headers to a\n   server if it detects, by looking for any Vary response-header fields\n   generated by the server, that such sending could improve the quality\n   of service.\n\n   Elaborate user-customized accept header fields sent in every request,\n   in particular if these include quality values, can be used by servers\n   as relatively reliable and long-lived user identifiers. Such user\n   identifiers would allow content providers to do click-trail tracking,\n   and would allow collaborating content providers to match cross-server\n   click-trails or form submissions of individual users. Note that for\n   many users not behind a proxy, the network address of the host\n   running the user agent will also serve as a long-lived user\n   identifier. In environments where proxies are used to enhance\n   privacy, user agents ought to be conservative in offering accept\n   header configuration options to end users. As an extreme privacy\n   measure, proxies could filter the accept headers in relayed requests.\n   General purpose user agents which provide a high degree of header\n\n\nFielding, et al                                               [Page 139]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   configurability SHOULD warn users about the loss of privacy which can\n   be involved.\n\n\n15.2 Attacks Based On File and Path Names\n\n   Implementations of HTTP origin servers SHOULD be careful to restrict\n   the documents returned by HTTP requests to be only those that were\n   intended by the server administrators. If an HTTP server translates\n   HTTP URIs directly into file system calls, the server MUST take\n   special care not to serve files that were not intended to be\n   delivered to HTTP clients. For example, UNIX, Microsoft Windows, and\n   other operating systems use \"..\" as a path component to indicate a\n   directory level above the current one. On such a system, an HTTP\n   server MUST disallow any such construct in the Request-URI if it\n   would otherwise allow access to a resource outside those intended to\n   be accessible via the HTTP server. Similarly, files intended for\n   reference only internally to the server (such as access control\n   files, configuration files, and script code) MUST be protected from\n   inappropriate retrieval, since they might contain sensitive\n   information. Experience has shown that minor bugs in such HTTP server\n   implementations have turned into security risks.\n\n\n15.3 DNS Spoofing\n\n   Clients using HTTP rely heavily on the Domain Name Service, and are\n   thus generally prone to security attacks based on the deliberate mis-\n   association of IP addresses and DNS names. Clients need to be\n   cautious in assuming the continuing validity of an IP number/DNS name\n   association.\n\n   In particular, HTTP clients SHOULD rely on their name resolver for\n   confirmation of an IP number/DNS name association, rather than\n   caching the result of previous host name lookups. Many platforms\n   already can cache host name lookups locally when appropriate, and\n   they SHOULD be configured to do so. It is proper for these lookups to\n   be cached, however, only when the TTL (Time To Live) information\n   reported by the name server makes it likely that the cached\n   information will remain useful.\n\n   If HTTP clients cache the results of host name lookups in order to\n   achieve a performance improvement, they MUST observe the TTL\n   information reported by DNS.\n\n   If HTTP clients do not observe this rule, they could be spoofed when\n   a previously-accessed server's IP address changes. As network\n   renumbering is expected to become increasingly common [24], the\n   possibility of this form of attack will grow. Observing this\n   requirement thus reduces this potential security vulnerability.\n\n   This requirement also improves the load-balancing behavior of clients\n   for replicated servers using the same DNS name and reduces the\n   likelihood of a user's experiencing failure in accessing sites which\n   use that strategy.\n\nFielding, et al                                               [Page 140]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n15.4 Location Headers and Spoofing\n\n   If a single server supports multiple organizations that do not trust\n   one another, then it MUST check the values of Location and Content-\n   Location headers in responses that are generated under control of\n   said organizations to make sure that they do not attempt to\n   invalidate resources over which they have no authority.\n\n\n15.5 Content-Disposition Issues\n\n   RFC 1806, from which the often implemented Content-Disposition (see\n   section 19.5.1) header in HTTP is derived, has a number of very\n   serious security considerations. Content-Disposition is not part of\n   the HTTP standard, but since it is widely implemented, we are\n   documenting its use and risks for implementors. See RFC 1806 [35] for\n   details.\n\n\n15.6 Authentication Credentials and Idle Clients\n\n   Existing HTTP clients and user agents typically retain authentication\n   information indefinitely. HTTP/1.1. does not provide a method for a\n   server to direct clients to discard these cached credentials. This is\n   a significant defect that requires further extensions to HTTP.\n   Circumstances under which credential caching can interfere with the\n   application's security model include but are not limited to:\n\n     .  Clients which have been idle for an extended period following\n        which the server might wish to cause the client to reprompt the\n        user for credentials.\n\n     .  Applications which include a session termination indication\n        (such as a `logout' or `commit' button on a page) after which\n        the server side of the application `knows' that there is no\n        further reason for the client to retain the credentials.\n\n   This is currently under separate study. There are a number of work-\n   arounds to parts of this problem, and we encourage the use of\n   password protection in screen savers, idle time-outs, and other\n   methods which mitigate the security problems inherent in this\n   problem. In particular, user agents which cache credentials are\n   encouraged to provide a readily accessible mechanism for discarding\n   cached credentials under user control.\n\n\n15.7 Proxies and Caching\n\n   By their very nature, HTTP proxies are men-in-the-middle, and\n   represent an opportunity for man-in-the-middle attacks. Compromise of\n   the systems on which the proxies run can result in serious security\n   and privacy problems. Proxies have access to security-related\n   information, personal information about individual users and\n   organizations, and proprietary information belonging to users and\n   content providers. A compromised proxy, or a proxy implemented or\n\nFielding, et al                                               [Page 141]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   configured without regard to security and privacy considerations,\n   might be used in the commission of a wide range of potential attacks.\n\n   Proxy operators should protect the systems on which proxies run as\n   they would protect any system that contains or transports sensitive\n   information. In particular, log information gathered at proxies often\n   contains highly sensitive personal information, and/or information\n   about organizations. Log information should be carefully guarded, and\n   appropriate guidelines for use developed and followed. (Section\n   15.1.1).\n\n   Caching proxies provide additional potential vulnerabilities, since\n   the contents of the cache represent an attractive target for\n   malicious exploitation. Because cache contents persist after an HTTP\n   request is complete, an attack on the cache can reveal information\n   long after a user believes that the information has been removed from\n   the network. Therefore, cache contents should be protected as\n   sensitive information.\n\n   Proxy implementors should consider the privacy and security\n   implications of their design and coding decisions, and of the\n   configuration options they provide to proxy operators (especially the\n   default configuration).\n\n   Users of a proxy need to be aware that they are no more trustworthy\n   than the people who run the proxy; HTTP itself cannot solve this\n   problem.\n\n   The judicious use of cryptography, when appropriate, may suffice to\n   protect against a broad range of security and privacy attacks. Such\n   cryptography is beyond the scope of the HTTP/1.1 specification.\n\n\n15.7.1 Denial of Service Attacks on Proxies\n\n   They exist. They are hard to defend against. Research continues.\n   Beware.\n\n\n16 Acknowledgments\n\n   This specification makes heavy use of the augmented BNF and generic\n   constructs defined by David H. Crocker for RFC 822 [9]. Similarly, it\n   reuses many of the definitions provided by Nathaniel Borenstein and\n   Ned Freed for MIME [7]. We hope that their inclusion in this\n   specification will help reduce past confusion over the relationship\n   between HTTP and Internet mail message formats.\n\n   The HTTP protocol has evolved considerably over the years. It has\n   benefited from a large and active developer community--the many\n   people who have participated on the www-talk mailing list--and it is\n   that community which has been most responsible for the success of\n   HTTP and of the World-Wide Web in general. Marc Andreessen, Robert\n   Cailliau, Daniel W. Connolly, Bob Denny, John Franks, Jean-Francois\n   Groff, Phillip M. Hallam-Baker, Hakon W. Lie, Ari Luotonen, Rob\n\nFielding, et al                                               [Page 142]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   McCool, Lou Montulli, Dave Raggett, Tony Sanders, and Marc\n   VanHeyningen deserve special recognition for their efforts in\n   defining early aspects of the protocol.\n\n   This document has benefited greatly from the comments of all those\n   participating in the HTTP-WG. In addition to those already mentioned,\n   the following individuals have contributed to this specification:\n\n          Gary Adams                  Albert Lunde\n          Harald Tveit Alvestrand     John C. Mallery\n          Keith Ball                  Jean-Philippe Martin-Flatin\n          Brian Behlendorf            Larry Masinter\n          Paul Burchard               Mitra\n          Maurizio Codogno            David Morris\n          Mike Cowlishaw              Gavin Nicol\n          Roman Czyborra              Bill Perry\n          Michael A. Dolan            Jeffrey Perry\n          David J. Fiander            Scott Powers\n          Alan Freier                 Owen Rees\n          Marc Hedlund                Luigi Rizzo\n          Greg Herlihy                David Robinson\n          Koen Holtman                Marc Salomon\n          Alex Hopmann                Rich Salz\n          Bob Jernigan                Allan M. Schiffman\n          Shel Kaphan                 Jim Seidman\n          Rohit Khare                 Chuck Shotton\n          John Klensin                Eric W. Sink\n          Martijn Koster              Simon E. Spero\n          Alexei Kosut                Richard N. Taylor\n          David M. Kristol            Robert S. Thau\n          Daniel LaLiberte            Bill (BearHeart) Weinman\n          Ben Laurie                  Francois Yergeau\n          Paul J. Leach               Mary Ellen Zurko\n          Daniel DuBois               Josh Cohen\n          Ross Patterson\n\n   Much of the content and presentation of the caching design is due to\n   suggestions and comments from individuals including: Shel Kaphan,\n   Paul Leach, Koen Holtman, David Morris, and Larry Masinter.\n\n   Most of the specification of ranges is based on work originally done\n   by Ari Luotonen and John Franks, with additional input from Steve\n   Zilles.\n\n   Thanks to the \"cave men\" of Palo Alto. You know who you are.\n\n   Jim Gettys (the current editor of this document) wishes particularly\n   to thank Roy Fielding, the previous editor of this document, along\n   with John Klensin, Jeff Mogul, Paul Leach, Dave Kristol, Koen\n   Holtman, John Franks, Josh Cohen, Alex Hopmann, Scott Lawrence, and\n   Larry Masinter for their help. And thanks go particularly to Jeff\n   Mogul and Scott Lawrence for performing the \"MUST/MAY/SHOULD\" audit.\n\n   The Apache Group, Anselm Baird-Smith, author of Jigsaw, and Henrik\n   Frystyk implemented RFC 2068 early, and we wish to thank them for the\n\nFielding, et al                                               [Page 143]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   discovery of many of the problems that this document attempts to\n   rectify.\n\n\n17 References\n\n\n   [1]  Alvestrand, H., \"Tags for the Identification of Languages\" RFC\n        1766, UNINETT, March 1995.\n\n\n   [2]  Anklesaria, F., McCahill, M., Lindner, P., Johnson, D., Torrey,\n        D., and B. Alberti. \"The Internet Gopher Protocol (a\n        distributed document search and retrieval protocol)\", RFC 1436,\n        University of Minnesota, March 1993.\n\n\n   [3]  Berners-Lee, T., \"Universal Resource Identifiers in WWW,\" RFC\n        1630, CERN, June 1994.\n\n\n   [4]  Berners-Lee, T., Masinter, L., and M. McCahill. \"Uniform\n        Resource Locators (URL),\" RFC 1738, CERN, Xerox PARC,\n        University of Minnesota, December 1994.\n\n\n   [5]  Berners-Lee, T. and D. Connolly. \"Hypertext Markup Language -\n        2.0,\" RFC 1866, MIT/LCS, November 1995.\n\n\n   [6]  Berners-Lee, T., Fielding, R. and H. Frystyk. \"Hypertext\n        Transfer Protocol -- HTTP/1.0,\" RFC 1945, MIT/LCS, UC Irvine,\n        May 1996.\n\n\n   [7]  Freed, N., and N. Borenstein. \"Multipurpose Internet Mail\n        Extensions (MIME) Part One: Format of Internet Message Bodies.\"\n        RFC 2045, Innosoft, First Virtual, November 1996.\n\n\n   [8]  Braden, R., \"Requirements for Internet Hosts -- Communication\n        Layers,\" STD 3, RFC 1123, IETF, October 1989.\n\n\n   [9]  D. H. Crocker, \"Standard for The Format of ARPA Internet Text\n        Messages,\" STD 11, RFC 822, UDEL, August 1982.\n\n\n   [10] Davis, F., Kahle, B., Morris, H., Salem, J., Shen, T., Wang,\n        R., Sui, J., and M. Grinbaum, \"WAIS Interface Protocol\n        Prototype Functional Specification.\" (v1.5), Thinking Machines\n        Corporation, April 1990.\n\n\n\n\nFielding, et al                                               [Page 144]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   [11] Fielding, R., \"Relative Uniform Resource Locators,\" RFC 1808,\n        UC Irvine, June 1995.\n\n\n   [12] Horton, M., and R. Adams. \"Standard for Interchange of USENET\n        Messages,\" RFC 1036 (Obsoletes RFC 850), AT&T Bell\n        Laboratories, Center for Seismic Studies, December 1987.\n\n\n   [13] Kantor, B. and P. Lapsley. \"Network News Transfer Protocol,\"\n        RFC 977, UC San Diego, UC Berkeley, February 1986.\n\n   [14] Moore, K., \"MIME (Multipurpose Internet Mail Extensions) Part\n        Three: Message Header Extensions for Non-ASCII Text\", RFC 2047,\n        University of Tennessee, November 1996.\n\n\n   [15] Nebel, E., and L. Masinter. \"Form-based File Upload in HTML,\"\n        RFC 1867, Xerox Corporation, November 1995.\n\n\n   [16] Postel, J., \"Simple Mail Transfer Protocol,\" STD 10, RFC 821,\n        USC/ISI, August 1982.\n\n\n   [17] Postel, J., \"Media Type Registration Procedure,\" RFC 1590,\n        USC/ISI, November 1996.\n\n\n   [18] Postel, J. and J. Reynolds. \"File Transfer Protocol,\" STD 9,\n        RFC 959, USC/ISI, October 1985.\n\n\n   [19] Reynolds, J. and J. Postel. \"Assigned Numbers,\" STD 2, RFC\n        1700, USC/ISI, October 1994.\n\n\n   [20] Sollins, K. and L. Masinter. \"Functional Requirements for\n        Uniform Resource Names,\" RFC 1737, MIT/LCS, Xerox Corporation,\n        December 1994.\n\n\n   [21] US-ASCII. Coded Character Set - 7-Bit American Standard Code\n        for Information Interchange. Standard ANSI X3.4-1986, ANSI,\n        1986.\n\n\n   [22] ISO-8859. International Standard -- Information Processing --\n        8-bit Single-Byte Coded Graphic Character Sets --\n        Part 1: Latin alphabet No. 1, ISO-8859-1:1987.\n        Part 2: Latin alphabet No. 2, ISO-8859-2, 1987.\n        Part 3: Latin alphabet No. 3, ISO-8859-3, 1988.\n        Part 4: Latin alphabet No. 4, ISO-8859-4, 1988.\n        Part 5: Latin/Cyrillic alphabet, ISO-8859-5, 1988.\n        Part 6: Latin/Arabic alphabet, ISO-8859-6, 1987.\n\nFielding, et al                                               [Page 145]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n        Part 7: Latin/Greek alphabet, ISO-8859-7, 1987.\n        Part 8: Latin/Hebrew alphabet, ISO-8859-8, 1988.\n        Part 9: Latin alphabet No. 5, ISO-8859-9, 1990.\n\n\n   [23] Meyers, J., and M. Rose. \"The Content-MD5 Header Field,\" RFC\n        1864, Carnegie Mellon, Dover Beach Consulting, October, 1995.\n\n\n   [24] Carpenter, B. and Y. Rekhter. \"Renumbering Needs Work,\" RFC\n        1900, IAB, February 1996.\n\n\n   [25] Deutsch, P., \"GZIP file format specification version 4.3,.\" RFC\n        1952, Aladdin Enterprises, May, 1996.\n\n\n   [26] Venkata N. Padmanabhan, and Jeffrey C. Mogul. \"Improving HTTP\n        Latency\", Computer Networks and ISDN Systems, v. 28, pp. 25-35,\n        Dec. 1995. Slightly revised version of paper in Proc. 2nd\n        International WWW Conference '94: Mosaic and the Web, Oct.\n        1994, which is available at\n        http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLa\n        tency.html.\n\n\n   [27] Joe Touch, John Heidemann, and Katia Obraczka. \"Analysis of\n        HTTP Performance\", <URL: http://www.isi.edu/touch/pubs/http-\n        perf96/>, ISI Research Report ISI/RR-98-463, (original report\n        dated Aug. 1996), USC/Information Sciences Institute, August\n        1998.\n\n\n   [28] Mills, D., \"Network Time Protocol (Version 3) Specification,\n        Implementation and Analysis.\" RFC 1305, University of Delaware,\n        March, 1992.\n\n\n   [29] Deutsch, P., \"DEFLATE Compressed Data Format Specification\n        version 1.3.\" RFC 1951, Aladdin Enterprises, May 1996.\n\n   [30] S. Spero, \"Analysis of HTTP Performance Problems,\"\n        http://sunsite.unc.edu/mdma-release/http-prob.html.\n\n\n   [31] Deutsch, P. and J-L. Gailly. \"ZLIB Compressed Data Format\n        Specification version 3.3,\" RFC 1950, Aladdin Enterprises,\n        Info-ZIP, May 1996.\n\n\n   [32] Franks, J., Hallam-Baker, P., Hostetler, J., Leach, P.,\n        Luotonen, A., Sink, E., and L. Stewart. \"An Extension to HTTP:\n        Digest Access Authentication,\" RFC 2069, January 1997.\n\n\n\nFielding, et al                                               [Page 146]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   [33] Fielding, R., Gettys, J., Mogul, J., Frystyk, H., Berners-Lee,\n        T., \"Hypertext Transfer Protocol -- HTTP/1.1\", RFC 2068, UC\n        Irvine, Digital Equipment Corporation, M.I.T., January, 1997.\n\n\n   [34] Bradner, S., \"Key words for use in RFCs to Indicate Requirement\n        Levels,\" RFC 2119, Harvard University, March 1997.\n\n   [35] Troost, R., and Dorner, S., \"Communicating Presentation\n        Information in Internet Messages: The Content-Disposition\n        Header,\" RFC 1806, New Century Systems, QUALCOMM, Inc., June\n        1995.\n\n   [36] Mogul, J.C., Fielding, R., Gettys, J, Frystyk, H., \"Use and\n        Interpretation of HTTP Version Numbers\", RFC 2145, Digital\n        Equipment Corporation, U.C. Irvine, M.I.T., May 1997.[jg605]\n\n   [37] Palme, J, \"Common Internet Message Headers,\" RFC 2076,\n        Stockholm University, KTH, February, 1997[jg606].\n\n   [38] Yergeau, F., \"UTF-8, a transformation format of Unicode and\n        ISO-10646,\" RFC 2279 (obsoleted RFC 2044), Alis Technologies,\n        January 1998. [jg607]\n\n   [39] Nielsen, H.F., Gettys, J., Baird-Smith, A., Prud'hommeaux, E.,\n        Lie, H., and C. Lilley. \"Network Performance Effects of\n        HTTP/1.1, CSS1, and PNG,\" Proceedings of ACM SIGCOMM '97,\n        Cannes France, September 1997.[jg608]\n\n   [40] Freed, N., and N. Borenstein. \"Multipurpose Internet Mail\n        Extensions (MIME) Part Two: Media Types.\" RFC 2046, Innosoft,\n        First Virtual, November 1996. [jg609]\n\n   [41] Alvestrand, H. T., \"IETF Policy on Character Sets and\n        Languages,\" RFC 2277, BCP 18, UNINETT, January, 1998. [jg610]\n\n   [42] Berners-Lee, T., Fielding, R., Masinter, L., \"Uniform Resource\n        Identifiers (URI): Generic Syntax and Semantics,\" RFC 2396,\n        August, 1998.[jg611]\n\n   [43] Franks, J., Hallam-Baker, P., Hostetler, J., Lawrence, S.,\n        Leach, P., Luotonen, A., Sink, E., Stewart, L., \"HTTP\n        Authentication: Basic and Digest Access Authentication,\" Work\n        in Progress, September, 1998.[jg612]\n\n   [44] Luotonen, A., \"Tunneling TCP based protocols through Web proxy\n        servers,\" Work in Progress, February, 1998.[jg613]\n\n   [45] Palme, J., Hopmann, A., \"MIME E-mail Encapsulation of Aggregate\n        Documents, such as HTML (MHTML),\" RFC 2110, March 1997\n\n   [46] Bradner, S., \"The Internet Standards Process -- Revision 3,\"\n        BCP 9, RFC 2026, Harvard University, October, 1996.\n\n\n\nFielding, et al                                               [Page 147]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   [47] Masinter, L., \"Hyper Text Coffee Pot Control Protocol\n        (HTCPCP/1.0),\" RFC 2324, April, 1998.\n\n   [48] Freed, N., Borenstein, N., \"Multipurpose Internet Mail\n        Extensions (MIME) Part Five: Conformance Criteria and\n        Examples,\" RFC 2049, November, 1998.\n\n\n18 Authors' Addresses\n\n   Roy T. Fielding\n   Department of Information and Computer Science\n   University of California\n   Irvine, CA 92697-3425, USA\n   Fax: +1 (714) 824-1715\n   Email: fielding@ics.uci.edu\n\n   James Gettys\n   Compaq Computer Corporation/World Wide Web Consortium\n   MIT Laboratory for Computer Science\n   545 Technology Square\n   Cambridge, MA 02139, USA\n   Fax: +1 (617) 258 8682\n   Email: jg@w3.org\n\n   Jeffrey C. Mogul\n   Western Research Laboratory\n   Compaq Computer Corporation\n   250 University Avenue\n   Palo Alto, California, 94305, USA\n   Email: mogul@wrl.dec.com\n\n   Henrik Frystyk Nielsen\n   World Wide Web Consortium\n   MIT Laboratory for Computer Science\n   545 Technology Square\n   Cambridge, MA 02139, USA\n   Fax: +1 (617) 258 8682\n   Email: frystyk@w3.org\n\n   Larry Masinter\n   Xerox PARC\n   3333 Coyote Hill Road\n   Palo Alto, CA 94034, USA\n   Fax:+1 (415) 812-4333\n   Email: masinter@parc.xerox.com\n\n   Paul J. Leach\n   Microsoft Corporation\n   1 Microsoft Way\n   Redmond, WA 98052, USA\n   Email: paulle@microsoft.com\n\n   Tim Berners-Lee\n   Director, World Wide Web Consortium\n\nFielding, et al                                               [Page 148]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   MIT Laboratory for Computer Science\n   545 Technology Square\n   Cambridge, MA 02139, USA\n   Fax: +1 (617) 258 8682\n   Email: timbl@w3.org\n\n\n19 Appendices\n\n\n19.1 Internet Media Type message/http and application/http\n\n   In addition to defining the HTTP/1.1 protocol, this document serves\n   as the specification for the Internet media type \"message/http\" and\n   \"application/http\". The message/http type can be used to enclose a\n   single HTTP request or response message, provided that it obeys the\n   MIME restrictions for all \"message\" types regarding line length and\n   encodings. The application/http type can be used to enclose a\n   pipeline of one or more HTTP request or response messages (not\n   intermixed). The following is to be registered with IANA [17].\n\n          Media Type name:         message\n          Media subtype name:      http\n          Required parameters:     none\n          Optional parameters:     version, msgtype\n           version: The HTTP-Version number of the enclosed message\n                    (e.g., \"1.1\"). If not present, the version can be\n                    determined from the first line of the body.\n           msgtype: The message type -- \"request\" or \"response\". If not\n                    present, the type can be determined from the first\n                    line of the body.\n          Encoding considerations: only \"7bit\", \"8bit\", or \"binary\" are\n                                   permitted\n          Security considerations: none\n\n          Media Type name:         application\n          Media subtype name:      http\n          Required parameters:     none\n          Optional parameters:     version, msgtype\n           version: The HTTP-Version number of the enclosed messages\n                    (e.g., \"1.1\"). If not present, the version can be\n                    determined from the first line of the body.\n           msgtype: The message type -- \"request\" or \"response\". If not\n                    present, the type can be determined from the first\n                    line of the body.\n          Encoding considerations: HTTP messages enclosed by this type\n                    are in \"binary\" format; use of an appropriate\n                    Content-Transfer-Encoding is required when\n                    transmitted via E-mail.\n          Security considerations: none\n\n19.2 Internet Media Type multipart/byteranges\n\n   When an HTTP 206 (Partial Content) response message includes the\n   content of multiple ranges (a response to a request for multiple non-\n\nFielding, et al                                               [Page 149]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   overlapping ranges), these are transmitted as a multipart message-\n   body. The media type for this purpose is called\n   \"multipart/byteranges\".\n\n   The multipart/byteranges media type includes two or more parts, each\n   with its own Content-Type and Content-Range fields. The required\n   boundary parameter specifies the boundary string used to separate\n   each body-part.\n\n          Media Type name:         multipart\n          Media subtype name:      byteranges\n          Required parameters:     boundary\n          Optional parameters:     none\n          Encoding considerations: only \"7bit\", \"8bit\", or \"binary\" are\n                                   permitted\n          Security considerations: none\n\n\n   For example:\n\n      HTTP/1.1 206 Partial Content\n      Date: Wed, 15 Nov 1995 06:25:24 GMT\n      Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n      Content-type: multipart/byteranges; boundary=THIS_STRING_SEPARATES\n\n      --THIS_STRING_SEPARATES\n      Content-type: application/pdf\n      Content-range: bytes 500-999/8000\n\n      ...the first range...\n      --THIS_STRING_SEPARATES\n      Content-type: application/pdf\n      Content-range: bytes 7000-7999/8000\n\n      ...the second range\n      --THIS_STRING_SEPARATES--\n\n\n     Notes:\n\n      1) Additional CRLFs may precede the first boundary string in\n         the entity.\n\n      2) Although RFC 2046 [40] permits the boundary string to be\n         quoted, some existing implementations handle a quoted\n         boundary string incorrectly.\n\n      3) A number of browsers and servers were coded to an early\n         draft of the byteranges specification to use a media type of\n         multipart/x-byteranges, which is almost, but not quite\n         compatible with the version documented in HTTP/1.1.\n\n\n\n\n\nFielding, et al                                               [Page 150]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n19.3 Tolerant Applications\n\n   Although this document specifies the requirements for the generation\n   of HTTP/1.1 messages, not all applications will be correct in their\n   implementation. We therefore recommend that operational applications\n   be tolerant of deviations whenever those deviations can be\n   interpreted unambiguously.\n\n   Clients SHOULD be tolerant in parsing the Status-Line and servers\n   tolerant when parsing the Request-Line. In particular, they SHOULD\n   accept any amount of SP or HT characters between fields, even though\n   only a single SP is required.\n\n   The line terminator for message-header fields is the sequence CRLF.\n   However, we recommend that applications, when parsing such headers,\n   recognize a single LF as a line terminator and ignore the leading CR.\n\n   The character set of an entity-body SHOULD be labeled as the lowest\n   common denominator of the character codes used within that body, with\n   the exception that not labeling the entity is preferred over labeling\n   the entity with the labels US-ASCII or ISO-8859-1. See section 3.7.1\n   and 3.4.1.\n\n   Additional rules for requirements on parsing and encoding of dates\n   and other potential problems with date encodings include:\n\n     .  HTTP/1.1 clients and caches SHOULD assume that an RFC-850 date\n        which appears to be more than 50 years in the future is in fact\n        in the past (this helps solve the \"year 2000\" problem).\n     .  An HTTP/1.1 implementation MAY internally represent a parsed\n        Expires date as earlier than the proper value, but MUST NOT\n        internally represent a parsed Expires date as later than the\n        proper value.\n     .  All expiration-related calculations MUST be done in GMT. The\n        local time zone MUST NOT influence the calculation or comparison\n        of an age or expiration time.\n     .  If an HTTP header incorrectly carries a date value with a time\n        zone other than GMT, it MUST be converted into GMT using the\n        most conservative possible conversion.\n\n19.4 Differences Between HTTP Entities and RFC 2045 Entities\n\n   HTTP/1.1 uses many of the constructs defined for Internet Mail (RFC\n   822 [9]) and the Multipurpose Internet Mail Extensions (MIME [7]) to\n   allow entities to be transmitted in an open variety of\n   representations and with extensible mechanisms. However, RFC 2045\n   discusses mail, and HTTP has a few features that are different from\n   those described in RFC 2045. These differences were carefully chosen\n   to optimize performance over binary connections, to allow greater\n   freedom in the use of new media types, to make date comparisons\n   easier, and to acknowledge the practice of some early HTTP servers\n   and clients.\n\n   This appendix describes specific areas where HTTP differs from RFC\n   2045. Proxies and gateways to strict MIME environments SHOULD be\n\nFielding, et al                                               [Page 151]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   aware of these differences and provide the appropriate conversions\n   where necessary. Proxies and gateways from MIME environments to HTTP\n   also need to be aware of the differences because some conversions\n   might be required.\n\n\n19.4.1 MIME-Version\n\n   HTTP is not a MIME-compliant protocol (see appendix 19.4). However,\n   HTTP/1.1 messages MAY include a single MIME-Version general-header\n   field to indicate what version of the MIME protocol was used to\n   construct the message. Use of the MIME-Version header field indicates\n   that the message is in full compliance with the MIME protocol (as\n   defined in RFC 2045[7]). Proxies/gateways are responsible for\n   ensuring full compliance (where possible) when exporting HTTP\n   messages to strict MIME environments.\n\n          MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT\n\n   MIME version \"1.0\" is the default for use in HTTP/1.1. However,\n   HTTP/1.1 message parsing and semantics are defined by this document\n   and not the MIME specification.\n\n\n19.4.2 Conversion to Canonical Form\n\n   RFC 2045 [7] requires that an Internet mail entity be converted to\n   canonical form prior to being transferred, as described in section 4\n   of RFC 2049 [48]. Section 3.7.1 of this document describes the forms\n   allowed for subtypes of the \"text\" media type when transmitted over\n   HTTP. RFC 2046 requires that content with a type of \"text\" represent\n   line breaks as CRLF and forbids the use of CR or LF outside of line\n   break sequences. HTTP allows CRLF, bare CR, and bare LF to indicate a\n   line break within text content when a message is transmitted over\n   HTTP.\n\n   Where it is possible, a proxy or gateway from HTTP to a strict MIME\n   environment SHOULD translate all line breaks within the text media\n   types described in section 3.7.1 of this document to the RFC 2049\n   canonical form of CRLF. Note, however, that this might be complicated\n   by the presence of a Content-Encoding and by the fact that HTTP\n   allows the use of some character sets which do not use octets 13 and\n   10 to represent CR and LF, as is the case for some multi-byte\n   character sets.\n\n   Implementors should note that conversion will break any cryptographic\n   checksums applied to the original content unless the original content\n   is already in canonical form. Therefore, the canonical form is\n   recommended for any content that uses such checksums in HTTP.\n\n\n19.4.3 Conversion of Date Formats\n\n   HTTP/1.1 uses a restricted set of date formats (section 3.3.1) to\n   simplify the process of date comparison. Proxies and gateways from\n\nFielding, et al                                               [Page 152]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   other protocols SHOULD ensure that any Date header field present in a\n   message conforms to one of the HTTP/1.1 formats and rewrite the date\n   if necessary.\n\n\n19.4.4 Introduction of Content-Encoding\n\n   RFC 2045 does not include any concept equivalent to HTTP/1.1's\n   Content-Encoding header field. Since this acts as a modifier on the\n   media type, proxies and gateways from HTTP to MIME-compliant\n   protocols MUST either change the value of the Content-Type header\n   field or decode the entity-body before forwarding the message. (Some\n   experimental applications of Content-Type for Internet mail have used\n   a media-type parameter of \";conversions=<content-coding>\" to perform\n   a function equivalent to Content-Encoding. However, this parameter is\n   not part of RFC 2045.)\n\n\n19.4.5 No Content-Transfer-Encoding\n\n   HTTP does not use the Content-Transfer-Encoding (CTE) field of RFC\n   2045. Proxies and gateways from MIME-compliant protocols to HTTP MUST\n   remove any non-identity CTE (\"quoted-printable\" or \"base64\") encoding\n   prior to delivering the response message to an HTTP client.\n\n   Proxies and gateways from HTTP to MIME-compliant protocols are\n   responsible for ensuring that the message is in the correct format\n   and encoding for safe transport on that protocol, where \"safe\n   transport\" is defined by the limitations of the protocol being used.\n   Such a proxy or gateway SHOULD label the data with an appropriate\n   Content-Transfer-Encoding if doing so will improve the likelihood of\n   safe transport over the destination protocol.\n\n\n19.4.6 Introduction of Transfer-Encoding\n\n   HTTP/1.1 introduces the Transfer-Encoding header field (section\n   14.41). Proxies/gateways MUST remove any transfer-coding prior to\n   forwarding a message via a MIME-compliant protocol.\n\n   A process for decoding the \"chunked\" transfer-coding (section 3.6)\n   can be represented in pseudo-code as:\n\n          length := 0\n          read chunk-size, chunk-extension (if any) and CRLF\n          while (chunk-size > 0) {\n             read chunk-data and CRLF\n             append chunk-data to entity-body\n             length := length + chunk-size\n             read chunk-size and CRLF\n          }\n          read entity-header\n          while (entity-header not empty) {\n             append entity-header to existing header fields\n             read entity-header\n\nFielding, et al                                               [Page 153]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n          }\n          Content-Length := length\n          Remove \"chunked\" from Transfer-Encoding\n\n19.4.7 MHTML and Line Length Limitations\n\n   HTTP implementations which share code with MHTML [45] implementations\n   need to be aware of MIME line length limitations. Since HTTP does not\n   have this limitation, HTTP does not fold long lines. MHTML messages\n   being transported by HTTP follow all conventions of MHTML, including\n   line length limitations and folding, canonicalization, etc., since\n   HTTP transports all message-bodies as payload (see section 3.7.2) and\n   does not interpret the content or any MIME header lines that might be\n   contained therein.\n\n\n19.5 Additional Features\n\n   RFC 1945 and RFC 2068 document protocol elements used by some\n   existing HTTP implementations, but not consistently and correctly\n   across most HTTP/1.1 applications. Implementors are advised to be\n   aware of these features, but cannot rely upon their presence in, or\n   interoperability with, other HTTP/1.1 applications. Some of these\n   describe proposed experimental features, and some describe features\n   that experimental deployment found lacking that are now addressed in\n   the base HTTP/1.1 specification.\n\n   A number of other headers, such as Content-Disposition and Title,\n   from SMTP and MIME are also often implemented (see RFC 2076 [37]).\n\n\n19.5.1 Content-Disposition\n\n   The Content-Disposition response-header field has been proposed as a\n   means for the origin server to suggest a default filename if the user\n   requests that the content is saved to a file. This usage is derived\n   from the definition of Content-Disposition in RFC 1806 [35].\n\n           content-disposition = \"Content-Disposition\" \":\"\n                          disposition-type *( \";\" disposition-parm )\n\n           disposition-type = \"attachment\" | disp-extension-token\n\n           disposition-parm = filename-parm | disp-extension-parm\n\n           filename-parm = \"filename\" \"=\" quoted-string\n\n           disp-extension-token = token\n\n           disp-extension-parm = token \"=\" ( token | quoted-string )\n\n   An example is\n\n           Content-Disposition: attachment; filename=\"fname.ext\"\n\n\nFielding, et al                                               [Page 154]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   The receiving user agent SHOULD NOT respect any directory path\n   information present in the filename-parm parameter, which is the only\n   parameter believed to apply to HTTP implementations at this time. The\n   filename SHOULD be treated as a terminal component only.\n\n   If this header is used in a response with the application/octet-\n   stream content-type, the implied suggestion is that the user agent\n   should not display the response, but directly enter a `save response\n   as...' dialog.\n\n   See section 15.5 for Content-Disposition security issues.\n\n\n19.6 Compatibility with Previous Versions\n\n   It is beyond the scope of a protocol specification to mandate\n   compliance with previous versions. HTTP/1.1 was deliberately\n   designed, however, to make supporting previous versions easy. It is\n   worth noting that, at the time of composing this specification\n   (1996), we would expect commercial HTTP/1.1 servers to:\n\n\n     .  recognize the format of the Request-Line for HTTP/0.9, 1.0, and\n        1.1 requests;\n\n     .  understand any valid request in the format of HTTP/0.9, 1.0, or\n        1.1;\n\n     .  respond appropriately with a message in the same major version\n        used by the client.\n   And we would expect HTTP/1.1 clients to:\n\n\n     .  recognize the format of the Status-Line for HTTP/1.0 and 1.1\n        responses;\n\n     .  understand any valid response in the format of HTTP/0.9, 1.0, or\n        1.1.\n   For most implementations of HTTP/1.0, each connection is established\n   by the client prior to the request and closed by the server after\n   sending the response. Some implementations implement the Keep-Alive\n   version of persistent connections described in section 19.7.1 of RFC\n   2068 [33].\n\n\n19.6.1 Changes from HTTP/1.0\n\n   This section summarizes major differences between versions HTTP/1.0\n   and HTTP/1.1.\n\n\n19.6.1.1 Changes to Simplify Multi-homed Web Servers and Conserve IP\nAddresses\n\n\n\nFielding, et al                                               [Page 155]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   The requirements that clients and servers support the Host request-\n   header, report an error if the Host request-header (section 14.23) is\n   missing from an HTTP/1.1 request, and accept absolute URIs (section\n   5.1.2) are among the most important changes defined by this\n   specification.\n\n   Older HTTP/1.0 clients assumed a one-to-one relationship of IP\n   addresses and servers; there was no other established mechanism for\n   distinguishing the intended server of a request than the IP address\n   to which that request was directed. The changes outlined above will\n   allow the Internet, once older HTTP clients are no longer common, to\n   support multiple Web sites from a single IP address, greatly\n   simplifying large operational Web servers, where allocation of many\n   IP addresses to a single host has created serious problems. The\n   Internet will also be able to recover the IP addresses that have been\n   allocated for the sole purpose of allowing special-purpose domain\n   names to be used in root-level HTTP URLs. Given the rate of growth of\n   the Web, and the number of servers already deployed, it is extremely\n   important that all implementations of HTTP (including updates to\n   existing HTTP/1.0 applications) correctly implement these\n   requirements:\n\n\n     .  Both clients and servers MUST support the Host request-header.\n\n     .  A client that sends an HTTP/1.1 request MUST send a Host header.\n\n     .  Servers MUST report a 400 (Bad Request) error if an HTTP/1.1\n        request does not include a Host request-header.\n\n     .  Servers MUST accept absolute URIs.\n\n19.6.2 Compatibility with HTTP/1.0 Persistent Connections\n\n   Some clients and servers might wish to be compatible with some\n   previous implementations of persistent connections in HTTP/1.0\n   clients and servers. Persistent connections in HTTP/1.0 are\n   explicitly negotiated as they are not the default behavior. HTTP/1.0\n   experimental implementations of persistent connections are faulty,\n   and the new facilities in HTTP/1.1 are designed to rectify these\n   problems. The problem was that some existing 1.0 clients may be\n   sending Keep-Alive to a proxy server that doesn't understand\n   Connection, which would then erroneously forward it to the next\n   inbound server, which would establish the Keep-Alive connection and\n   result in a hung HTTP/1.0 proxy waiting for the close on the\n   response. The result is that HTTP/1.0 clients must be prevented from\n   using Keep-Alive when talking to proxies.\n\n   However, talking to proxies is the most important use of persistent\n   connections, so that prohibition is clearly unacceptable. Therefore,\n   we need some other mechanism for indicating a persistent connection\n   is desired, which is safe to use even when talking to an old proxy\n   that ignores Connection. Persistent connections are the default for\n   HTTP/1.1 messages; we introduce a new keyword (Connection: close) for\n   declaring non-persistence. See section 14.10.\n\nFielding, et al                                               [Page 156]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   The original HTTP/1.0 form of persistent connections (the Connection:\n   Keep-Alive and Keep-Alive header) is documented in RFC 2068. [33]\n\n\n19.6.3 Changes from RFC 2068\n\n   This specification has been carefully audited to correct and\n   disambiguate key word usage; RFC 2068 had many problems in respect to\n   the conventions laid out in RFC 2119 [34].\n\n   Clarified which error code should be used for inbound server failures\n   (e.g. DNS failures). (Section 10.5.5)\n\n   CREATE had a race that required an Etag be sent when a resource is\n   first created. (Section 10.2.2).\n\n   Content-Base was deleted from the specification: it was not\n   implemented widely, and there is no simple, safe way to introduce it\n   without a robust extension mechanism. In addition, it is used in a\n   similar, but not identical fashion in MHTML [45].\n\n   Transfer-coding and message lengths all interact in ways that\n   required fixing exactly when chunked encoding is used (to allow for\n   transfer encoding that may not be self delimiting); it was important\n   to straighten out exactly how message lengths are computed. (Sections\n   3.6, 4.4, 7.2.2, 13.5.2, 14.13, 14.16)\n\n   A content-coding of \"identity\" was introduced, to solve problems\n   discovered in caching. (section 3.5)\n\n   Quality Values of zero should indicate that \"I don't want something\"\n   to allow clients to refuse a representation. (Section 3.9)\n\n   The use and interpretation of HTTP version numbers has been clarified\n   by RFC 2145. Require proxies to upgrade requests to highest protocol\n   version they support to deal with problems discovered in HTTP/1.0\n   implementations (Section 3.1)\n\n   Charset wildcarding is introduced to avoid explosion of character set\n   names in accept headers. (Section 14.2)\n\n   A case was missed in the Cache-Control model of HTTP/1.1; s-maxage\n   was introduced to add this missing case. (Sections 13.4, 14.8, 14.9,\n   14.9.3)\n\n   The Cache-Control: max-age directive was not properly defined for\n   responses. (Section 14.9.3)\n\n   There are situations where a server (especially a proxy) does not\n   know the full length of a response but is capable of serving a\n   byterange request. We therefore need a mechanism to allow byteranges\n   with a content-range not indicating the full length of the message.\n   (Section 14.16)\n\n\n\nFielding, et al                                               [Page 157]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Range request responses would become very verbose if all meta-data\n   were always returned; by allowing the server to only send needed\n   headers in a 206 response, this problem can be avoided. (Section\n   10.2.7, 13.5.3, and 14.27)\n\n   Fix problem with unsatisfiable range requests; there are two cases:\n   syntactic problems, and range doesn't exist in the document. The 416\n   status code was needed to resolve this ambiguity needed to indicate\n   an error for a byte range request that falls outside of the actual\n   contents of a document. (Section 10.4.17, 14.16)\n\n   Rewrite of message transmission requirements to make it much harder\n   for implementors to get it wrong, as the consequences of errors here\n   can have significant impact on the Internet, and to deal with the\n   following problems:\n\n     1. Changing \"HTTP/1.1 or later\" to \"HTTP/1.1\", in contexts where\n        this was incorrectly placing a requirement on the behavior of an\n        implementation of a future version of HTTP/1.x\n\n     2. Made it clear that user-agents should retry requests, not\n        \"clients\" in general.\n\n     3. Converted requirements for clients to ignore unexpected 100\n        (Continue) responses, and for proxies to forward 100 responses,\n        into a general requirement for 1xx responses.\n\n     4. Modified some TCP-specific language, to make it clearer that\n        non-TCP transports are possible for HTTP.\n\n     5. Require that the origin server MUST NOT wait for the request\n        body before it sends a required 100 (Continue) response.\n\n     6.         Allow, rather than require, a server to omit 100 (Continue) if\n        it has already seen some of the request body.\n\n     7.         Allow servers to defend against denial-of-service attacks and\n        broken clients.\n\n   This change adds the Expect header and 417 status code. The message\n   transmission requirements fixes are in sections 8.2, 10.4.18,\n   8.1.2.2, 13.11, and 14.20.\n\n   Proxies should be able to add Content-Length when appropriate.\n   (Section 13.5.2)\n\n   Clean up confusion between 403 and 404 responses. (Section 10.4.4,\n   10.4.5, and 10.4.11)\n\n   Warnings could be cached incorrectly, or not updated appropriately.\n   (Section 13.1.2, 13.2.4, 13.5.2, 13.5.3, 14.9.3, and 14.46) Warning\n   also needed to be a general header, as PUT or other methods may have\n   need for it in requests.\n\n\n\nFielding, et al                                               [Page 158]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n   Transfer-coding had significant problems, particularly with\n   interactions with chunked encoding. The solution is that transfer-\n   codings become as full fledged as content-codings. This involves\n   adding an IANA registry for transfer-codings (separate from content\n   codings), a new header field (TE) and enabling trailer headers in the\n   future. Transfer encoding is a major performance benefit, so it was\n   worth fixing [39]. TE also solves another, obscure, downward\n   interoperability problem that could have occured due to interactions\n   between authentication trailers, chunked encoding and HTTP/1.0\n   clients.(Section 3.6, 3.6.1, and 14.39)\n\n   The PATCH, LINK, UNLINK methods were defined but not commonly\n   implemented in previous versions of this specification. See RFC 2068\n   [33].\n\n   The Alternates, Content-Version, Derived-From, Link, URI, Public and\n   Content-Base header fields were defined in previous versions of this\n   specification, but not commonly implemented. See RFC 2068 [33].\n\n19.7 Notes to the RFC Editor and IANA\n\n   This section of the document should be DELETED! It calls for the RFC\n   editor and IANA to take some actions before the draft becomes a Draft\n   Standard. After those actions are taken, please delete this section\n   of the specification.\n\n\n19.7.1 Transfer-coding Values\n\n   This document defines a new class of registry for its transfer-coding\n   values as part of the solution to solve problems discovered in RFC\n   2068 with the caching of transfer encoded documents. Initially, the\n   registry should contain the following tokens: \"chunked\" (section\n   3.6.1),  \"gzip\" (section 3.5), \"compress\" (section 3.5), and\n   \"deflate\" (section 3.5) and the special keyword \"trailers\" (section\n   14.39). The registry should note that \"specifications of the\n   transfer-coding algorithms needed to implement a new value should be\n   publicly available and adequate for independent implementation, and\n   conform to the purpose of content coding defined RFC XXXX.\" where RFC\n   XXXX is the number assigned to this document.\n\n\n19.7.2 Definition of application/http\n\n   Appendix 19.1 defines Internet Media Type application/http in\n   addition to the Internet Media Type message/http defined by RFC 2068.\n\n\n19.7.3 Addition of \"identity\" content-coding to content-coding Registry\n\n   The \"identity\" content coding is added to the content-coding registry\n   by this document (section 3.5) to solve a problem discovered in RFC\n   2068.\n\n\n\nFielding, et al                                               [Page 159]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n20 Full Copyright Statement\n\n   Copyright (C) The Internet Society (1998). All Rights Reserved.\n\n   This document and translations of it may be copied and furnished to\n   others, and derivative works that comment on or otherwise explain it\n   or assist in its implementation may be prepared, copied, published\n   and distributed, in whole or in part, without restriction of any\n   kind, provided that the above copyright notice and this paragraph are\n   included on all such copies and derivative works. However, this\n   document itself may not be modified in any way, such as by removing\n   the copyright notice or references to the Internet Society or other\n   Internet organizations, except as needed for the purpose of\n   developing Internet standards in which case the procedures for\n   copyrights defined in the Internet Standards process must be\n   followed, or as required to translate it into languages other than\n   English.\n\n   The limited permissions granted above are perpetual and will not be\n   revoked by the Internet Society or its successors or assigns.\n\n   This document and the information contained herein is provided on an\n   \"AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFielding, et al                                               [Page 160]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n\n21 Index\n\n   While some care was taken producing this index, there is no guarantee\n   that all occurrences of an index term have been entered into the\n   index.\n\n\n\"literal\", 14                         411, 33, 39, 63\n#rule, 15                             412, 39, 64, 118, 120, 122\n(rule1 rule2), 14                     413, 39, 64\n*rule, 14                             414, 19, 39, 64\n; comment, 15                         415, 39, 64, 107\n[rule], 15                            416, 39, 64, 111, 112, 126, 158 \n<\">, 16                               417, 39, 65, 114, 115, 158\n100, 38, 45, 46, 47, 54, 90, 113,     4xx Client Error Status Codes, 60\n 115                                  500, 39, 65, 113\n101, 38, 54, 113, 131                 501, 24, 34, 39, 52, 65\n1xx Informational Status Codes,       502, 39, 65\n 53                                   503, 39, 65, 66, 113, 128\n200, 38, 49, 51, 52, 53, 54, 56,      504, 39, 66, 104\n 57, 59, 84, 88, 103, 112, 119,       505, 39, 66\n 122, 127                             5xx Server Error Status Codes, 65\n201, 38, 51, 55, 123                  abs_path, 19, 35\n202, 38, 52, 55                       absoluteURI, 19, 35, 36, 109,\n203, 38, 55, 84                        123, 128\n204, 32, 38, 51, 53, 56               Accept, 25, 36, 67, 71, 91, 92,\n205, 38, 56                            93, 95, 139\n206, 38, 56, 57, 84, 85, 86, 88,      acceptable-ranges, 96\n 111, 112, 122, 126, 127, 150,        Accept-Charset, 36, 67, 93\n 158                                  Accept-Encoding, 22, 23, 36, 67,\n2xx, 121                               93, 94\n2xx Successful Status Codes, 54       accept-extension, 91\n300, 38, 57, 68, 84                   Accept-Language, 28, 37, 67, 68,\n301, 38, 52, 58, 84, 132               95, 135, 139\n302, 38, 58, 60, 132                  accept-params, 91, 129\n303, 38, 51, 58, 59, 132              Accept-Ranges, 39, 96\n304, 32, 38, 59, 60, 70, 79, 82,      Access Authentication, 66\n 85, 86, 87, 88, 103, 119, 120,        Basic and Digest. See [43]\n 121, 127                             Acknowledgements, 142\n305, 38, 60, 70, 132                  age, 11\n306, 60                               Age, 39, 74, 75, 76, 96\n307, 38, 58, 60                       age-value, 96\n3xx Redirection Status Codes, 57      Allow, 34, 40, 49, 62, 97\n400, 33, 36, 38, 39, 61, 117, 156     ALPHA, 14, 16\n401, 38, 61, 62, 97, 137              Alternates. See RFC 2068\n402, 38, 61                           ANSI X3.4-1986, 15, 145\n403, 38, 61, 158                      asctime-date, 20\n404, 38, 61, 63, 158                  attribute, 23\n405, 34, 38, 62, 97                   authority, 19, 35\n406, 38, 62, 68, 92, 93, 94           Authorization, 37, 61, 84, 97,\n407, 38, 62, 124                       99, 125\n408, 38, 63                           Backus-Naur Form, 14\n409, 39, 63                           Basic Authentication. See [43]\n410, 39, 63, 84                       BCP 18, 147\n\nFielding, et al                                               [Page 161]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\nBCP 9, 1, 147                          max-age, 74, 77, 78, 84, 98, 99,\nbyte-content-range-spec, 111             101, 102, 103, 104, 116, 157\nbyte-range, 125                        max-stale, 72, 98, 102, 103, 104\nbyte-range-resp-spec, 111              min-fresh, 98, 102\nbyte-range-set, 126                    must-revalidate, 98, 99, 102,\nbyte-range-spec, 64, 112, 126            104\nbyte-ranges-specifier,                 no-cache, 70, 78, 98, 99, 100,\nbytes, 96                                         101, 103, 124\nbytes-unit, 29                         no-store, 70, 98, 99, 100\ncachable, 11                           no-transform, 98, 99, 105, 107\ncache, 11                              only-if-cached, 98, 104\nCache                                  private, 84, 99, 101, 105, 106\n cachability of responses, 83          proxy-revalidate, 84, 99, 104\n calculating the age of a              public, 72, 84, 98, 99, 101, 105\n  response, 74                         s-maxage, 77, 84, 98, 99, 101,\n combining byte ranges, 86               157\n combining headers, 85                cache-directive, 98, 106, 124\n combining negotiated responses,      cache-request-directive, 70, 98\n  87                                  Changes from HTTP/1.0. See RFC\n constructing responses, 84            1945 and RFC 2068\n correctness, 70                       Host requirement, 156\n disambiguating expiration            CHAR, 15\n  values, 77                          charset, 22, 93\n disambiguating multiple              chunk, 24\n  responses, 78                       chunk-data, 25\n entity tags used as cache            chunked, 129\n  validators, 79                      Chunked-Body, 24\n entry validation, 78                 chunk-extension, 24\n errors or incomplete responses,      chunk-ext-name, 24\n  88                                  chunk-ext-val, 24\n expiration calculation, 76           chunk-size, 24\n explicit expiration time, 73         client, 10\n GET and HEAD cannot affect           codings, 93\n  caching, 89                         comment, 17, 132, 133\n heuristic expiration, 74             Compatibility\n history list behavior, 90             missing charset, 22\n invalidation cannot be complete,      multipart/x-byteranges, 150\n  89                                  Compatibility with previous HTTP\n Last-Modified values used as          versions, 155\n  validators, 79                      CONNECT, 34. See [44].\n mechanisms, 72                       connection, 9\n replacement of cached responses,     Connection, 33, 42, 43, 84, 106,\n  90                                   107, 129, 131, 156\n shared and non-shared, 88             close, 42, 106, 156\n Warnings, 71                          Keep-Alive, 157. See RFC 2068\n weak and strong cache                connection-token, 106, 107\n  validators, 79                      Content Codings\n write-through mandatory, 90           compress, 23\nCache-Control, 33, 51, 57, 58,         deflate, 23\n 59, 60, 72, 73, 74, 77, 78, 79,       gzip, 22\n 83, 84, 85, 88, 98, 99, 101,          identity, 23\n 103, 106, 116, 124                   content negotiation, 10\n cache-extension, 98                  Content Negotiation, 66\n extensions, 105                      Content-Base, 157. See RFC 2068\n\n\nFielding, et al                                               [Page 162]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\ncontent-coding, 22, 23, 24, 26,       disposition-parm, 154\n 67, 93, 94, 107, 131, 136, 159       disposition-type, 154\n identity, 157                        DNS, 140, 157\n new tokens SHOULD be registered       HTTP applications MUST obey TTL\n  with IANA, 23                          information, 140\n qvalues used with, 94                downstream, 12\ncontent-disposition, 154              End-to-end headers, 84\nContent-Disposition, 141, 147,        entity, 9\n 154, 155                             Entity, 40\nContent-Encoding, 22, 23, 40, 41,     Entity body, 40\n 85, 107, 110, 136, 152, 153          Entity Tags, 29, 79\nContent-Language, 28, 40, 107,        entity-body, 40\n 108, 135                             entity-header, 34, 37, 40\nContent-Length, 31, 32, 33, 40,       Entity-header fields, 40\n 45, 49, 50, 56, 63, 85, 88, 108,     entity-length, 41, 85\n 112, 130, 154, 158                   entity-tag, 29, 120, 121\nContent-Location, 40, 56, 59, 85,     Etag, 157\n 87, 88, 89, 109, 123, 141            ETag, 29, 39, 50, 55, 56, 57, 59,\nContent-MD5, 40, 50, 85, 109,          79, 85, 86, 88, 114, 120\n 110, 146                             Expect, 37, 45, 46, 47, 54, 65,\nContent-Range, 56, 57, 84, 111         114, 115, 158\ncontent-range-spec, 111               expectation, 114\nContent-Transfer-Encoding, 24,        expectation-extension, 114, 115\n 110, 153                             expect-params, 114\nContent-Type, 22, 25, 40, 41, 49,     Expires, 40, 51, 57, 58, 59, 60,\n 53, 55, 56, 57, 62, 63, 85, 107,      74, 76, 77, 84, 85, 101, 102,\n 112, 136, 150, 153                    104, 114, 115, 116, 151\nContent-Version. See RFC 2068         explicit expiration time, 11\nCR, 16, 26, 34, 37, 39, 151, 152      extension-code, 39\nCRLF, 14, 16, 24, 26, 27, 30, 34,     extension-header, 40\n 37, 110, 151, 152                    extension-pragma, 124\nctext, 17                             field-content, 31\nCTL, 16                               field-name, 30, 31\nDate, 33, 56, 59, 74, 75, 77, 78,     field-value, 30\n 81, 83, 87, 88, 90, 101, 113,        filename-parm, 154\n 116, 123, 137, 153                   first-byte-pos, 64, 111, 112, 126\ndate1, 20                             first-hand, 11\ndate2, 20                             fresh, 11\ndate3, 20                             freshness lifetime, 11\nDELETE, 34, 48, 52, 89                freshness_lifetime, 77\ndelta-seconds, 21, 128                From, 37, 44, 116, 117, 138, 139\nDerived-From. See RFC 2068            gateway, 11\nDifferences between MIME and          General Header Fields, 33\n HTTP, 151                            general-header, 33, 34, 37\n canonical form, 152                  generic-message, 30\n Content-Encoding, 153                GET, 19, 34, 35, 48, 50, 54, 56,\n Content-Transfer-Encoding, 153        57, 58, 59, 60, 64, 79, 80, 81,\n date formats, 152                     89, 90, 97, 108, 112, 118, 119,\n MIME-Version, 152                     120, 121, 127, 139\n Transfer-Encoding, 153               HEAD, 32, 34, 48, 50, 55, 57, 58,\nDigest Authentication, 85. See         59, 60, 61, 62, 65, 89, 90, 97,\n [43]                                  108, 112, 120\nDIGIT, 14, 15, 16, 18, 20, 28,        Headers\n 123, 152                              end-to-end, 84, 85, 86, 106, 115\ndisp-extension-token, 154              hop-by-hop, 12, 84\n\nFielding, et al                                               [Page 163]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n non-modifiable headers, 85           Location, 39, 51, 55, 57, 58, 59,\nHenrik Frystyk Nielsen, 148            60, 89, 123, 141\nheuristic expiration time, 11         LWS, 14, 16, 31\nHEX, 16, 19, 24                       Max-Forwards, 37, 49, 53, 123,\nHop-by-hop headers, 84                 124\nhost, 19, 133, 135                    MAY, 9\nHost, 35, 36, 37, 48, 117, 156        media type, 16, 22, 26, 27, 32,\nHT, 14, 16, 17, 30, 151                 41, 55, 57, 62, 66, 91, 92, 105,\nhttp_URL, 19                           107, 108, 112, 113, 149, 150,\nHTTP-date, 20, 113, 114, 116,          152, 153\n 119, 121, 122, 128, 135              Media Types, 25\nHTTP-message, 30                      media-range, 91\nHTTP-Version, 18, 34, 37              media-type, 25, 26, 107, 110, 136\nIANA, 21, 22, 24, 26, 28, 92, 149     message, 9\nidentity, 23, 94, 107, 157            Message Body, 31\nIf-Match, 29, 37, 50, 82, 118,        Message Headers, 30\n 119, 120, 121, 127                   Message Length, 32\nIf-Modified-Since, 37, 50, 81,        Message Transmission\n 82, 119, 120, 121, 122, 127           Requirements, 44\nIf-None-Match, 29, 37, 50, 82,        Message Types, 30\n 87, 88, 119, 120, 121, 122, 127      message-body, 30, 31, 34, 37, 41\nIf-Range, 29, 37, 50, 56, 57, 64,     message-header, 30, 40\n 82, 112, 121, 122, 127               Method, 34, 97\nIf-Unmodified-Since, 37, 50, 81,      Method Definitions, 48\n 82, 120, 121, 122, 127               Methods\nIf-Unmodified-Since, 122               Idempotent, 48\nimplied *LWS, 15                       Safe and Idempotent, 48\ninbound, 12                           MIME, 8, 12, 21, 24, 27, 108,\ninstance-length, 111                   110, 142, 144, 145, 147, 151,\nISO-10646, 147                         152, 153, 154\nISO-2022, 21                           multipart, 27\nISO-3166, 28                          MIME-Version, 152\nISO-639, 28                           month, 21\nISO-8859, 145                         multipart/byteranges, 27, 32, 56,\nISO-8859-1, 16, 22, 26, 93, 135,       65, 112, 150\n 151                                  multipart/x-byteranges, 150\nJames Gettys, 148                     MUST, 9\nJeffrey C. Mogul, 148                 MUST NOT, 9\nKeep-Alive, 43, 84, 155, 156,         N rule, 15\n 157. See RFC 2068                    name, 14\nLanguage Tags, 28                     non-shared cache, 88, 99, 105\nlanguage-range, 95                    non-transparent proxy. See proxy:\nlanguage-tag, 28, 95                   non-transparent\nLarry Masinter, 148                   OCTET, 15, 40\nlast-byte-pos, 111, 126               opaque-tag, 29\nlast-chunk, 24                        OPTIONAL, 9\nLast-Modified, 12, 40, 50, 57,        OPTIONS, 34, 35, 48, 49, 123, 124\n 74, 77, 79, 81, 82, 83, 85, 86,      origin server, 10\n 114, 120, 121, 122, 123              other-range-unit, 29\nLF, 16, 26, 34, 37, 39, 151, 152      outbound, 12\nlifetime, 11, 12, 74, 76, 77, 96,     parameter, 23\n 102, 136                             PATCH. See RFC 2068\nLink. See RFC 2068                    Paul J. Leach, 148\nLINK. See RFC 2068                    Persistent Connections, 41\nLOALPHA, 16                            Overall Operation, 42\n\nFielding, et al                                               [Page 164]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\n Purpose, 41                          Request-Line, 30, 34, 35, 51, 62,\n Use of Connection Header, 42          151, 155\nPipelining, 43                        Request-URI, 19, 34,   , 36, 39,\nport,                                  49, 50, 51, 52, 53, 58, 60, 61,\nPOST, 27, 30, 34, 46, 48, 50, 51,      62, 63, 64, 87, 88, 89, 97, 107,\n 52, 55, 58, 59, 64, 89, 113, 139      109, 123, 125, 128, 137, 139,\nPragma, 33, 98, 103, 124               140\n no-cache, 70, 78, 98, 124            REQUIRED, 9\npragma-directive, 124                 Requirements\nprimary-tag, 28                        compliance, 9\nproduct, 27, 132                       key words, 9\nProduct tokens, 27                    resource, 9\nproduct-version, 27                   response, 9\nprotocol-name, 133                    Response, 37\nprotocol-version, 133                 Response Header Fields, 39\nproxy, 10                             response-header, 37, 39\n non-transparent, 10, 85, 105,        Retry-After, 39, 64, 65, 128\n  107                                 Revalidation\n transparent, 10, 40, 85               end-to-end, 102, 103\nProxy-Authenticate, 39, 62, 84,        end-to-end reload, 103\n 124, 125                              end-to-end specific\nProxy-Authorization, 37, 62, 84,         revalidation, 103\n 125                                   end-to-end unspecific\npseudonym, 133, 134, 135                 revalidation, 103\nPublic. See RFC 2068                  RFC 1036, 20, 145\npublic cache, 67, 68                  RFC 1123, 20, 113, 116, 144\nPUT, 34, 46, 48, 51, 52, 63, 89,      RFC 1305, 146\n 97, 113, 118, 121                    RFC 1436, 144\nqdtext, 17                            RFC 1590, 26, 145\nQuality Values, 28                    RFC 1630, 144\nquoted-pair, 17                       RFC 1700, 145\nquoted-string, 15, 17, 24, 29,        RFC 1737, 145\n 31, 91, 99, 114, 124, 135, 154       RFC 1738, 19, 144\nqvalue, 28, 91, 93                    RFC 1766, 28, 144\nRange, 29, 37, 40, 50, 52, 56,        RFC 1806, 141, 147, 154\n 57, 64, 84, 85, 86, 112, 119,        RFC 1808, 19, 145\n 121, 125, 127, 150                   RFC 1864, 109, 110, 146\nRange Units, 29                       RFC 1866, 144\nranges-specifier, 111, 125, 126,      RFC 1867, 27, 145\n 127                                  RFC 1900, 19, 146\nrange-unit, 29, 96                    RFC 1945, 8, 58, 144, 154\nReason-Phrase, 37, 38, 39             RFC 1950, 23, 146\nreceived-by, 133, 134                 RFC 1951, 23, 146\nreceived-protocol, 133, 134           RFC 1952, 146\nRECOMMENDED, 9                        RFC 2026, 147\nReferences, 144                       RFC 2044, 147\nReferer, 37, 127, 128, 138, 139       RFC 2045, 144, 151, 152, 153\nrel_path, 19, 89                      RFC 2046, 27, 147, 150, 152\nrelativeURI, 19, 109, 128             RFC 2047, 16, 135, 145\nrepresentation, 10                    RFC 2049, 148, 152\nrequest, 9                            RFC 2068, 2, 18, 41, 43, 46, 58,\nRequest, 34                            59, 60, 143, 147, 154, 155, 157\nRequest header fields, 36              changes from, 157\nrequest-header, 34, 36                RFC 2069, 146\n                                      RFC 2076, 147, 154\n\nFielding, et al                                               [Page 165]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\nRFC 2110, 147                         SHOULD NOT, 9\nRFC 2119, 9, 147, 157                 SP, 14, 16, 17, 20, 30, 31, 34,\nRFC 2145, 17, 147, 157                  37, 111, 135, 151\nRFC 2277, 147                         stale, 12\nRFC 2279, 147                         start-line, 30\nRFC 2324, 148                         Status Code Definitions, 53\nRFC 2396, 19, 147                     Status-Code, 37, 38, 53\nRFC 821,                              Status-Line, 30, 37, 39, 53, 151,\nRFC 822, 14, 20, 30, 113, 116,         155\n 133, 142, 144, 151                   strong entity tag, 29\nRFC 850, 20, 145                      strong validators, 80\nRFC 959, 145                          subtag, 28\nRFC 977, 145                          subtype, 25\nrfc1123-date, 20                      suffix-byte-range-spec, 126\nRFC-850, 151                          suffix-length, 126\nrfc850-date, 20                       T/TCP, 41\nRoy T. Fielding, 148                  t-codings, 129\nrule1 | rule2, 14                     TE, 25, 37, 129, 130, 159\nSafe and Idempotent Methods, 48       TEXT, 16\nSecurity Considerations, 137          Tim Berners-Lee, 148\n abuse of server logs, 138            time, 21\n Accept header, 139                   token, 14, 15, 17, 22, 23, 24,\n Accept headers can reveal ethnic      25, 27, 28, 29, 30, 31, 34, 91,\n  information, 139                     99, 106, 114, 124, 132, 133, 154\n attacks based on path names, 140     Tolerant Applications, 151\n Authentication Credentials and        bad dates, 151\n  Idle Clients, 141                    should tolerate whitespace in\n be careful about personal               request and status lines, 151\n  information, 137                     tolerate LF and ignore CR in\n Content-Disposition Header, 141         line terminators, 151\n Content-Location header, 141          use lowest common denominator of\n encoding information in URI's,          character set, 151\n  139                                 TRACE, 34, 48, 53, 55, 123, 124\n From header, 138, 139                trailer, 24, 25\n GET method, 139                      Trailer, 25, 33, 130\n Location header, 141                 Transfer Encoding\n Location headers and spoofing,        chunked, 23\n  141                                 transfer-coding\n Proxies and Caching, 141              chunked, 24\n Referer header, 138, 139              deflate, 24\n sensitive headers, 138                gzip, 24\n Server header, 138                    identity, 24\n Transfer of Sensitive                transfer-coding, 23, 24, 25, 31,\n  Information, 138                     32, 33, 41, 110, 129, 130, 131,\n Via header, 138                       153, 157, 159\nselecting request-headers, 87          chunked, 24, 32, 45, 129, 130,\nsemantically transparent, 12             153, 159\nseparators, 17                         chunked REQUIRED, 33\nserver, 10                             compress, 24, 159\nServer, 27, 39, 128, 129, 134,         identity, 32\n 138                                   trailers, 129\nSHALL, 9                              Transfer-Encoding, 23, 31, 32,\nSHALL NOT, 9                           33, 41, 49, 84, 130, 131, 153,\nshared caches, 88, 100                 154\nSHOULD, 9                             transfer-extension, 23, 129\n\nFielding, et al                                               [Page 166]\n\nINTERNET-DRAFT                  HTTP/1.1              September 11, 1998\n\ntransfer-length, 41, 85               warn-code, 86, 135, 136\ntransparent                           warn-date, 135, 137\n proxy, 85                            Warning, 33, 70, 71, 73, 77, 83, \ntransparent proxy. See proxy:          85, 86, 102, 134, 135, 136, 137,\n transparent                           158\ntunnel,                              Warnings\ntype, 25                               110 Response is stale, 136\nUNLINK. See RFC 2068                   111 Revalidation failed, 136\nUPALPHA, 16                            112 Disconnected operation, 136\nUpgrade, 33, 54, 84, 131, 132          113 Heuristic expiration, 136\nupstream, 12                           199 Miscellaneous warning, 136\nURI. See RFC 2068                      214 Transformation applied, 136\nURI-reference, 19                      299 Miscellaneous persistent\nUS-ASCII, 15, 21, 151                    warning, 136\nuser agent, 10                        warning-value, 135, 137\nUser-Agent, 27, 37, 68, 132, 134,     warn-text, 135, 136\n 138                                  weak, 29\nvalidators, 12, 29, 72, 77, 78,       weak entity tag, 29\n 79, 80, 82, 83, 87                   weak validators, 80\n rules on use of, 81                  weekday, 21\nvalue, 23                             wkday, 21\nvariant, 10                           WWW-Authenticate, 39, 61, 125,\nVary, 39, 57, 59, 68, 87, 118,         137\n 121, 132, 139                        x-compress, 94\nVia, 33, 53, 129, 133, 134, 138       x-gzip, 94\nwarn-agent, 135\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFielding, et al                                               [Page 167]\n\n--1--\n\n\n\n", "id": "lists-012-5818791"}, {"subject": "Re: Referer [sic] Redirect questio", "content": "Jaye, Dan:\n>\n>How should the user agent populate the referrer field when issuing a\n>request after receiving a 3xx redirect response?  Is the original\n>referer maintained or is it now the URI that resulted in the redirect\n>response?  There is no discussion of this scenario in the referer or\n>redirect sections of the draft.\n\nI think that the language \"allows the client to specify [...] the\naddress (URI) of the resource from which the Request-URI was obtained\"\nin the draft clearly implies that it should be the URI that gave the\n3xx response.  I would not be surprised however to see some 1.0 agents\ndo it the other way around.\n\n>Reason for the question:\n>Ad Servers sometimes delegate remnant ad impression inventory to\n>\"fire-sale\" ad networks using re-direct.  Usually an ad server will use\n>the referer field to determine on what page the ad will be appear.\n\nUsing referer in this case is a dangerous technique for the second ad\nserver.  It definitely breaks with browsers that do not send referer\nat all.  It is better to have a setup in which the first ad server\nencodes the necessary information in the request URI for the second ad\nserver.\n\nKoen.\n\n\n\n", "id": "lists-012-6283611"}, {"subject": "Administrivia: how to get off this lis", "content": "Folks,\n\nJust a reminder... if you wish to unsubscribe from the http-wg mailing list,\nsimply send a message to:\n\n  http-wg-request@hplb.hpl.hp.com\n\nwith a subject line of:\n\n  unsubscribe\n\nDon't forget the -request bit above.\n--\n(http-wg mailing list owner)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-012-6291816"}, {"subject": "TEXT, user names, and password", "content": "The Digest spec says that user names and passwords are TEXT. TEXT says that\nit is any octet, except CTLs, except LWS is allowed. It also says the only\ncharacters in it have to be ISO 8859-1 and or encoded according RFC 2047.\n\nAn issue which was raised to me is that user name and password have to be\ntyped by users. To quote the person (Chris Newman) who pointed this out to\nme:\n\nHere's a nasty one -- TEXT is defined as using either ISO 8859-1 or RFC\n2047 encoding for internationalization in [HTTP/1.1 spec].  RFC 2047\nencoding will\nnever work in this context since it has no reasonable canonical form.\nTherefore, in order to comply with RFC 2277 you either need to make this\nuse UTF-8, or make it use US-ASCII only, claiming the \"identifier\"\nexception (and we can make it UTF-8 later if we wish).\n\nI.e., the mapping from what you type to the octets of the password have to\nbe the same everywhere, otherwise the password won't compute the correct MD5\nhash values.\n\nI don't pretend to understand the I18N issues. Are there problems if we say\nthat the password is UTF-8 encoded (and change the BNF)? How about user\nname?\n\nPaul\n\n\n\n", "id": "lists-012-6298708"}, {"subject": "white space in WWWAut", "content": "For as long as I can remember, HTTP authentication has had this production\nin its grammar:\n\n\n\n", "id": "lists-012-6307096"}, {"subject": "Re: white space in WWWAut", "content": "Paul Leach wrote:\n> \n> For as long as I can remember, HTTP authentication has had this production\n> in its grammar:\n\nWell, you don't get any whiter than that! (sorry, couldn't resist).\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686| Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org/\nand Technical Director|Email: ben@algroup.co.uk |\nA.L. Digital Ltd,     |Apache-SSL author     http://www.apache-ssl.org/\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache/\n\nWE'RE RECRUITING! http://www.aldigital.co.uk/\n\n\n\n", "id": "lists-012-6313816"}, {"subject": "[HTTP] Request for Advancement http-v11-spec-rev-05 &amp; http-authentication0", "content": "The HTTP working group has reached rough consensus within the working group\nto advance the two documents:\n\n   draft-ietf-http-v11-spec-rev-05.txt, ps  9/15/1998\n   draft-ietf-http-authentication-03.txt    9/14/1998\n\nto Draft Standards status. This message is the official request for\nadvancement by the IESG.\n\nLarry Masinter, as HTTP working group chair.\n--\nhttp://www.parc.xerox.com/masinter\n \n\n\n\n", "id": "lists-012-6321588"}, {"subject": "Text, Postscript, and Word versions of HTTP/1.1 Rev05 now available", "content": "Off the issues list page, as usual.\n\nhttp://www.w3.org/Protocols/HTTP/Issues/\n- Jim\n\n\n\n", "id": "lists-012-6330107"}, {"subject": "Fwd: submission of draft-ietf-http-v11-spec-rev05.tx", "content": "Cynthia did not know of the new address...  Here is the formal IETF\nannouncement for those who do not follow IETF-Announce.\n- Jim\n\n\n\n\nattached mail follows:\n\nHello Jim,\n\nThank you for asking me directly.... it has been announced\non Sept 11th - I'm forwarding the copy of announcement far below.\n\nIn the same meantime, I wonder if you're already on the\nIETF Announcement list  ?  \n\nKind Regards,\n\nCynthia\n\n------- Forwarded Message\n\nReturn-Path: adm \nDelivery-Date: Tue, 15 Sep 1998 12:45:40 -0400\nReturn-Path: adm\nReceived: (from adm@localhost)\nby ietf.org (8.8.5/8.8.7a) id MAA27842\nfor ietf-123-outbound.10@ietf.org; Tue, 15 Sep 1998 12:35:02 -0400 (EDT)\nReceived: from CNRI.Reston.VA.US (localhost [127.0.0.1])\nby ietf.org (8.8.5/8.8.7a) with ESMTP id KAA21585;\nTue, 15 Sep 1998 10:25:03 -0400 (EDT)\nMessage-Id: <199809151425.KAA21585@ietf.org>\nMime-Version: 1.0\nContent-Type: Multipart/Mixed; Boundary=\"NextPart\"\nTo: IETF-Announce: ;\nCc: http-wg@cuckoo.hpl.hp.com\nFrom: Internet-Drafts@ietf.org\nReply-to: Internet-Drafts@ietf.org\nSubject: I-D ACTION:draft-ietf-http-v11-spec-rev-05.txt\nDate: Tue, 15 Sep 1998 10:25:02 -0400\nSender: cclark@ns.cnri.reston.va.us\n\n- --NextPart\n\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: Hypertext Transfer Protocol -- HTTP/1.1\nAuthor(s): R. Fielding, J. Gettys, J. Mogul, H. Frystyk,\n                          L. Masinter, P. Leach, T. Berners-Lee\nFilename: draft-ietf-http-v11-spec-rev-05.txt\nPages: 167\nDate: 11-Sep-98\n\n   The Hypertext Transfer Protocol (HTTP) is an application-level\n   protocol for distributed, collaborative, hypermedia information\n   systems. It is a generic, stateless, protocol which can be used for\n   many tasks beyond its use for hypertext, such as name servers and\n   distributed object management systems, through extension of its\n   request methods, error codes and headers [47]. A feature of HTTP is\n   the typing and negotiation of data representation, allowing systems\n   to be built independently of the data being transferred.\n \n   HTTP has been in use by the World-Wide Web global information\n   initiative since 1990. This specification defines the protocol\n   referred to as 'HTTP/1.1', and is an update to RFC 2068 [33].\n \n   At the time of submission, there were no known outstanding issues\n   with this document. The HTTP/1.1 issues list can be found\n   at http://www.w3.org/Protocols/HTTP/Issues/.  Linked from this page are\n   also Postscript and Microsoft Word versions (with versions with change\n   bars) of this document which are easier to review than this text\n   document.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-v11-spec-rev-05.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-v11-spec-rev-05.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-v11-spec-rev-05.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n- --NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n- --OtherAccess\nContent-Type: Message/External-body;\naccess-type=\"mail-server\";\nserver=\"mailserv@ietf.org\"\n\nContent-Type: text/plain\nContent-ID:<19980914111731.I-D@ietf.org>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-v11-spec-rev-05.txt\n\n- --OtherAccess\nContent-Type: Message/External-body;\nname=\"draft-ietf-http-v11-spec-rev-05.txt\";\nsite=\"ftp.ietf.org\";\naccess-type=\"anon-ftp\";\ndirectory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID:<19980914111731.I-D@ietf.org>\n\n- --OtherAccess--\n\n- --NextPart--\n\n\n\n------- End of Forwarded Message\n\n\n\n", "id": "lists-012-6336859"}, {"subject": "de facto standard of HTT", "content": "Hi, what is the current de factor standard version of HTTP used by most\ninternet web servers ?  I originally thought it's 1.1 but the recent\nIETF spec mail on 1.1 makes me wonder.  Perhaps it's 1.0 ?  What about\nHTTP-NG ?\n\nthanks,\nwendy\n\n\n\n", "id": "lists-012-6350711"}, {"subject": "Response Version  Clarificatio", "content": "I've been reading the spec for a while now and have read through the discussion\non \"Call for Closure - HTTP Response Version\", but I'm still a little unclear\nof the way a certain scenario should work.\n\nLet's say I had the following:\n\n1.0 Client -> 1.1 Proxy -> 1.0 Server\n\nFrom the spec, the proxy MUST upgrade the 1.0 request to 1.1 and send that on\nto the server. The server of course responds with a 1.0 response. The proxy can\nthen chose what response version to send to the client as long as it doesn't\nbreak the client (chunked data or something else like that).\n\nHowever, the spec also says that proxies should keep track of the HTTP version\nof all servers for various reasons. This is fine, but consider the following:\n\n1.0 Client -> 1.1 Proxy #1 -> 1.1 Proxy #2 -> 1.0 Server\n\nProxy #2 of course can know the true version of the server, but there's no way\nfor Proxy #1 to know the real version of the server if Proxy #2 decides to\nupgrade the 1.0 response to 1.1.\n\nIf this is covered by some other discussion, please point me to it. Otherwise\ncan someone tell me what's supposed to happen here?\n\nThanks,\nChris DiPierro\n\n\n\n", "id": "lists-012-6358466"}, {"subject": "Re: GET with offset ", "content": "Tristan Savatier <tristan@mpegtv.com> schrieb:\n> Thanks. Now I see how to do a GET with range request.\n> Now, another question:\n> Is there a way to specify the offset or range with the http: URL\n> syntax ?\n> i.e. something like http://host/dir/file@range=0,500\n\nNo, but media types can define how to access parts of a document.  \nExamples:\n\nhttp://host/foo.html#section2.1.1\nhttp://host/foo.pdf#page123\nhttp://host/foo.mpeg#2m03s04f   (hypothetical)\n\nWhat is missing is a method to only access the parts that are really  \nneeded, i.e. if the part referred to is #page123, the server determines  \nwhich byte ranges that are and returns them. However, this depends on  \nthe media type.\nFor some media types, you _can_ retrieve the header (of the file, not  \nthe http header!) after determining the media type and calculate the  \noffset of the part you want to retrieve.\n\nAn HTTP method by which the client can have the server/proxy parse the  \nmessage content and retrieve the parts' names with the corresponding  \nbyteranges could be useful:\n\n- RANGES /foo.html HTTP/1.1\n+ 300 HTTP/1.1 Ok.\n+ Content-Type: message/byterangeinfo\n+\n+ * 0-480\n+ #sec1 123-245\n+ #sec2 245-345\n+ #sec3 345-467\n\n--\nClaus Andre Faerber <http://www.muc.de/~cfaerber/> Fax: +49_8061_3361\nPGP: ID=1024/527CADCD FP=12 20 49 F3 E1 04 9E 9E  25 56 69 A5 C6 A0 C9 DC\n\n\n\n", "id": "lists-012-6366217"}, {"subject": "HTTP/1.1 Draft has inconsistent use of LastModified header fiel", "content": "The Last-modified header response field is inconsistently referenced in\nthe HTTP 1.1 spec as both Last-Modified and Last-modified.\n\nAccording to the definition in section 14.29 of\n<draft-ietf-http-v11-spec-rev-04> the Last-Modified entity-header field\nis in the form:  Last-Modified  = \"Last-Modified\" \":\" HTTP-date\n\nSuggestion that all occurrences of \"Last-modified\" be changed to\n\"Last-Modified\" to avoid confusion.\n\nThe following parts need changing:\n\nTable of Contents pages 6-7\n13.3.1 Last-modified Dates\n...............................................................................\n55\n13.3.4 Rules for When to Use Entity Tags and Last-modified Dates\n.................... 57\n\n[page 62]\n\nAn HTTP/1.1 caching proxy, upon receiving a conditional request that\nincludes both a Last-modified date and\n\n[page 74]\n\n13.3.1 Last-modified Dates\n\n[page 77]\n\nincludes both a Last-modified date and one or more entity tags as cache\n\nincludes both a Last-modified date (e.g., in an If-Modified-Since or\nIf-Unmodified-Since header field)\n\n[page 84]\n\nFor example,\n       HTTP/1.1 206 Partial content\n       Date: Wed, 15 Nov 1995 06:25:24 GMT\n       Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n\n[Page 138]\n\nFor example:\n\n   HTTP/1.1 206 Partial Content\n   Date: Wed, 15 Nov 1995 06:25:24 GMT\n   Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n\n--\nJason Mathews <mathews@mitre.org>\nThe MITRE Corporation <http://www.mitre.org/>\nInternet Technologies Dept., MS-B275\nBedford, MA 01730-1407\n\n\n\n", "id": "lists-012-6374677"}, {"subject": "ContentDispositio", "content": "I suggest the following change for Content-Disposition:\n\n| 19.5.1 Content-Disposition\n\n|     The receiving user agent SHOULD NOT respect any directory path\n!     information present in the filename-parm parameter. The\n|     filename SHOULD be treated as a terminal component only.\n+     The filename MAY be modified in a way that the system will\n+     correctly recognize the media type given in the Content-Type\n+     header, i.e. by adding an appropraite file extension.\n\n!            disposition-type = \"attachment\" | \"inline\" | disp-extension-token\n+            disposition-parm = filename-parm | disp-extension-parm\n+                      | creation-date-parm\n+                      | modification-date-parm\n+                      | read-date-parm\n+                      | size-parm\n                       | disp-extension-parameter\n\nThe new date fields of RFC 2183 could also be used in HTTP and are  \ncurrently missing from HTTP.\n\nI consider the fact that modification-data-parm is independent from Last- \nModified a feature: Last-Modified can be used to indicate when the  \nresource under this URL changed (i.e. date of upload to server), while  \nthe modification-date parameter explicitly refers to the modification  \ndate of the file offered for download (i.e. the file when the archive  \nwas compiled).\n\n-    If this header is used in a response with the application/octet-\n-    stream content-type, the implied suggestion is that the user agent\n-    should not display the response, but directly enter a `save response\n-    as...' dialog.\n\n+    The disposition \"inline\" suggests that the user agent should try to\n+    display the response directly. It may fallback to \"attachment\" if\n+    it is unable to display the response or display it using a generic\n+    viewing method (such as a hex viewer).\n+\n+    The disposition \"attachment\" suggests that the user agent should not\n+    display the response, but directly enter a `save response as...'\n+    dialog.\n+\n+    If no Content-Disposition is given, UAs usually makes a guess based\n+    on the Content-type.\n\nRationale: It is not a good idea to use the file type to determine  \nwhether to display it inline. A UA could still start a hex viewer for  \napplication/octet-stream to display it inline in it's browser window.\n\nOn the other hand, if is bad if I get type \"application/octet-stream\"  \nand filename \"foo.gif\" and don't know how to handle it, just because my  \nsystem does not determine file types by extension.\n\nI believe this is what the \"disposition\" information is actually for.\nThe assignments\n  inline = display directly and\n  attachment = save to disk\nmostly agree with the definitions in RFC 2183.\n\nThen, why not make it part of the official HTTP spec? If implemented  \ncorrectly, it can be safe and useful.\n\nAnd what about Content-Description?\n\n--\nClaus Andre Faerber <http://www.muc.de/~cfaerber/> Fax: +49_8061_3361\nPGP: ID=1024/527CADCD FP=12 20 49 F3 E1 04 9E 9E  25 56 69 A5 C6 A0 C9 DC\n\n\n\n", "id": "lists-012-6383710"}, {"subject": "IPP DL, FTP and Web services back u", "content": "All,\n\nThe IPP DL and partly also the FTP and Web services for the IPP WG has been\ndown or only sporadically working for the last few weeks, due to a change of\nserver and hosting organization. Everything is now back to full working\norder under the earlier names:\n\nIPP DL: mailto://ipp@pwg.org\nIPP Web: http://www.pwg.org/ipp\nFTP Arch: ftp://ftp.pwg.org/pub/pwg/ipp/\n\nIf you did send a message to the IPP DL during the last two weeks, it might\nhave ended up in the bit bucket and never reached the IPP DL members, please\nresend. Apologies for any inconveniences this might have caused you.\n\nCarl-Uno Manros\n\nChair of the IPP WG\n\n\n\n", "id": "lists-012-6393535"}, {"subject": "Re: GET with offset ", "content": "Tristan Savatier <tristan@mpegtv.com> schrieb:\n> Thanks. Now I see how to do a GET with range request.\n> Now, another question:\n> Is there a way to specify the offset or range with the http: URL\n> syntax ?\n> i.e. something like http://host/dir/file@range=0,500\n\nNo, but media types can define how to access parts of a document.  \nExamples:\n\nhttp://host/foo.html#section2.1.1\nhttp://host/foo.pdf#page123\nhttp://host/foo.mpeg#2m03s04f   (hypothetical)\n\nWhat is missing is a method to only access the parts that are really  \nneeded, i.e. if the part referred to is #page123, the server determines  \nwhich byte ranges that are and returns them. However, this depends on  \nthe media type.\nFor some media types, you _can_ retrieve the header (of the file, not  \nthe http header!) after determining the media type and calculate the  \noffset of the part you want to retrieve.\n\nAn HTTP method by which the client can have the server/proxy parse the  \nmessage content and retrieve the parts' names with the corresponding  \nbyteranges could be useful:\n\n- RANGES /foo.html HTTP/1.1\n+ 300 HTTP/1.1 Ok.\n+ Content-Type: message/byterangeinfo\n+\n+ * 0-480\n+ #sec1 123-245\n+ #sec2 245-345\n+ #sec3 345-467\n\n--\nClaus Andre Faerber <http://www.muc.de/~cfaerber/> Fax: +49_8061_3361\nPGP: ID=1024/527CADCD FP=12 20 49 F3 E1 04 9E 9E  25 56 69 A5 C6 A0 C9 DC\n\n\n\n", "id": "lists-012-6402335"}, {"subject": "ContentDispositio", "content": "I suggest the following change for Content-Disposition, which obviously  \nis still based on the now obsolete RFC 1806:\n\n| 19.5.1 Content-Disposition\n\n|     The receiving user agent SHOULD NOT respect any directory path\n!     information present in the filename-parm parameter. The\n|     filename SHOULD be treated as a terminal component only.\n+     The filename MAY be modified in a way that the system will\n+     correctly recognize the media type given in the Content-Type\n+     header, i.e. by adding an appropraite file extension.\n\n!            disposition-type = \"attachment\" | \"inline\" | disp-extension-token\n+            disposition-parm = filename-parm | disp-extension-parm\n+                      | creation-date-parm\n+                      | modification-date-parm\n+                      | read-date-parm\n+                      | size-parm\n                       | disp-extension-parameter\n\nThe new date fields of RFC 2183 could also be used in HTTP and are  \ncurrently missing from HTTP.\n\nI consider the fact that modification-data-parm is independent from Last- \nModified a feature: Last-Modified can be used to indicate when the  \nresource under this URL changed (i.e. date of upload to server), while  \nthe modification-date parameter explicitly refers to the modification  \ndate of the file offered for download (i.e. the file when the archive  \nwas compiled).\n\n-    If this header is used in a response with the application/octet-\n-    stream content-type, the implied suggestion is that the user agent\n-    should not display the response, but directly enter a `save response\n-    as...' dialog.\n\n+    The disposition \"inline\" suggests that the user agent should try to\n+    display the response directly. It may fallback to \"attachment\" if\n+    it is unable to display the response or display it using a generic\n+    viewing method (such as a hex viewer).\n+\n+    The disposition \"attachment\" suggests that the user agent should not\n+    display the response, but directly enter a `save response as...'\n+    dialog.\n+\n+    If no Content-Disposition is given, UAs usually makes a guess based\n+    on the Content-type.\n\nRationale: It is not a good idea to use the file type to determine  \nwhether to display it inline. A UA could still start a hex viewer for  \napplication/octet-stream to display it inline in it's browser window.\n\nOn the other hand, if is bad if I get type \"application/octet-stream\"  \nand filename \"foo.gif\" and don't know how to handle it, just because my  \nsystem does not determine file types by extension.\n\nI believe this is what the \"disposition\" information is actually for.\nThe assignments\n  inline = display directly and\n  attachment = save to disk\nmostly agree with the definitions in RFC 2183.\n\nAnd what about Content-Description?\n\n--\nClaus Andre Faerber <http://www.muc.de/~cfaerber/> Fax: +49_8061_3361\nPGP: ID=1024/527CADCD FP=12 20 49 F3 E1 04 9E 9E  25 56 69 A5 C6 A0 C9 DC\n\n\n\n", "id": "lists-012-6410554"}, {"subject": "Re: ContentDispositio", "content": "This is taken from the MIME useage of Content-Disposition.\n\nWe will not differ from current Web usage (without changing the name).\n\nWe will not add new material to HTTP/1.1 at this late date (we have\nfinished working group last call, and have asked the area directors\nto start the approval process.\n\nHowever, feel free (as a separate document) to propose improvements\nin this area.\n\nIs our description in variance with other MIME or current Web cleint usage?\n- Jim Gettys\n\n\n\n", "id": "lists-012-6419657"}, {"subject": "Re: de facto standard of HTT", "content": "wendy@cs.ualberta.ca wrote:\n> \n> Hi, what is the current de factor standard version of HTTP used by most\n> internet web servers ?  I originally thought it's 1.1 but the recent\n> IETF spec mail on 1.1 makes me wonder.  Perhaps it's 1.0 ?  What about\n> HTTP-NG ?\n\nSurveys say Apache runs more than 50% of the web. Apache uses HTTP/1.1.\nApache does not use HTTP-NG. Does that help?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686| Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org/\nand Technical Director|Email: ben@algroup.co.uk |\nA.L. Digital Ltd,     |Apache-SSL author     http://www.apache-ssl.org/\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache/\n\nWE'RE RECRUITING! http://www.aldigital.co.uk/\n\n\n\n", "id": "lists-012-6427186"}, {"subject": "http and range", "content": "> Thanks. Now I see how to do a GET with range request.\n> Now, another question:\n> Is there a way to specify the offset or range with the http: URL\n> syntax ?\n> i.e. something like http://host/dir/file@range=0,500\n\nThe real question is how such an addition (to some future http 1.2)\nwould be used. I can think of two general cases:\n\n1)Aas a means of obtaining a portion of a resource; such as a\nsingle chapter of a long document.\n\nBut  one could always just have separate files containing chapter 1,\nchapter 2,.. ( as complements to the \"entire document\").  Alternatively,  a\nscript (cgi-bin or whatever) that would parse the request line, looking\nfor a \"@range\",  and (if found) use the \"=0,500\" to selectively return a\nportion of the document, should be fairly easy to create.\n\n2) As a means of selectively updating a portion of an otherwise large\nresource. For example, acrobat can use a range: header to request\nselected pages of a long pdf file. \n\nBut this then means having client software (i.e.; browsers) that\nunderstand the request syntax being used.  But there already is a\nsyntax that will support such actions (that is, the aforementioned use of\nrange: request header).\n\nIn other words, although new http methods, etc (such as  adoption of a\nRANGE  method) might be useful, I suspect it's not worth the trouble.\n\n\n\n", "id": "lists-012-6435817"}, {"subject": "Re: de facto standard of HTT", "content": "Mike_Spreitzer.PARC@xerox.com wrote:\n> \n> > Apache does not use HTTP-NG.\n> \n> You might be interested to learn that we're using a modified version of Apache\n> in our HTTP-NG testbed.  But we haven't yet got as good an integration as we'd\n> like, and there are not yet any browsers that speak HTTP-NG, so this is just\n> FYI, not anything we can recommend to users right now.\n\n From what little I know of HTTP-NG, you may get better results with\nApache 2.0 (which is multithreaded). OTOH, we need to sort out the I/O\nlayer abstraction first. Input welcome.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686| Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org/\nand Technical Director|Email: ben@algroup.co.uk |\nA.L. Digital Ltd,     |Apache-SSL author     http://www.apache-ssl.org/\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache/\n\nWE'RE RECRUITING! http://www.aldigital.co.uk/\n\n\n\n", "id": "lists-012-6443682"}, {"subject": "Re: de facto standard of HTT", "content": "I have seen data that makes me believe over half the servers\non the public internet now claim to speak HTTP/1.1.\n\nClient and proxy support is currently considerably less than this.\n\nHTTP/NG is just at the formative stages, far from any deployment,\nif ever.\n- Jim\n\n\n\n", "id": "lists-012-6452532"}, {"subject": "Re: ContentDispositio", "content": "Jim Gettys:\n>\n>This is taken from the MIME useage of Content-Disposition.\n\nI initially wrote the Content-Disposition section, so let me add some\ndetails here.\n\nThe Content-Disposition section documents *only* what some popular\ncurrent web browsers do.  I was careful not to specify anything beyond\nwhat I could test to heve been implemented.  The section does not\ncontain any actual requirements for HTTP/1.1 software.  It was added\nto answer a frequently asked question.\n\nIf we had set out to specify an improved version of\nContent-Disposition, something like your proposal would have been in\nthe specification.  But we did not set out to specify any improvement,\nreasoning that others (e.g. the MIME community) could to this if\nneeded.\n\nThat being said, *if* another revision of the 1.1 spec is necessary,\nit would be nice to add a pointer to RFC 2183, which updates RFC 1806.\n\nKoen.\n\n\n\n", "id": "lists-012-6459781"}, {"subject": "white space in WWWAut", "content": "Try again....\n\nFor as long as I can remember, HTTP authentication has had this production\nin its grammar:\n\n challenge   = auth-scheme 1*SP 1#auth-param\n\nWhen the auth stuff was extracted and merged with the Digest spec, this was\nin the Digest section:\n\n     challenge        =  \"Digest\" digest-challenge\n\nI.e., between the \"Digest\" and \"digest-challenge\", instead of there being\n1*SP there is \"implied *LWS\" (see section 2.1 of the HTTP/1.1 spec).\n\nWhich is right? Does anyone depend on it being SP instead of LWS?\n\n\n\n", "id": "lists-012-6468229"}, {"subject": "Re: http and range", "content": "I think the trend in general is to move away from user entered URLs.\n\nAnother trend is away from complex GET URLs and toward use of POST as\nwell as either the Safe: approach or a new idempotent GETX method.\n\nAlso, adding new syntax to the URL has the possiblity of breaking\nexisting implementations because the parsing implementaton if often\nflakey at best.\n\nThere is no reason that the future should be defined by the limitations\nof current UIs and HTML attributes.\n\nI believe the range: approach was orginally conceived to allow a PDF\nviewer plugin to efficiently transfer portions of the file. Two other\nuses were quickly identified ... recovery of a user interrupted\ntransfer ... quickly getting image meta data for layout purposes w/o\nwaiting for the full transfer. \n\nThe whole range of ways and kinds of pieces users might want to \nrequest doesn't lend itself to user entry of byte level encoding.\nThere is no reason the current URL scheme can't retrieve whole documents\nas well as pieces with simple URLs ... outside of the scope of HTTP\nand URLs, the server implementation defines the interpretation of the\nURL. With a SINGLE data file and/or DB, the URLs \n    http://www.bookserve.com/clancy/HotBook\n    http://www.bookserve.com/clancy/HotBook/chapter1\n    http://www.bookserve.com/clancy/HotBook/chapter1/para6\ncould all produce variant levels of zoom into the document. This can\nbe achieved today on some servers by creative use of mapping to cgi\nprograms. Or an imperceptable alteration of the URL to:\n    http://www.bookserve.com/clancy/Hot.Book\ncould be mapped to an ISAPI DLL on IIS, etc.\n\nAlso, thre are some pretty straight forward improvments to HTML and\nUIs which would allow better access to the POST and possible GETX\nmethods.  Links which implement the POST method for example.\n\nDave Morris\n\nOn Fri, 18 Sep 1998, Daniel Hellerstein wrote:\n\n> > Thanks. Now I see how to do a GET with range request.\n> > Now, another question:\n> > Is there a way to specify the offset or range with the http: URL\n> > syntax ?\n> > i.e. something like http://host/dir/file@range=0,500\n> \n> The real question is how such an addition (to some future http 1.2)\n> would be used. I can think of two general cases:\n> \n> 1)Aas a means of obtaining a portion of a resource; such as a\n> single chapter of a long document.\n> \n> But  one could always just have separate files containing chapter 1,\n> chapter 2,.. ( as complements to the \"entire document\").  Alternatively,  a\n> script (cgi-bin or whatever) that would parse the request line, looking\n> for a \"@range\",  and (if found) use the \"=0,500\" to selectively return a\n> portion of the document, should be fairly easy to create.\n> \n> 2) As a means of selectively updating a portion of an otherwise large\n> resource. For example, acrobat can use a range: header to request\n> selected pages of a long pdf file. \n> \n> But this then means having client software (i.e.; browsers) that\n> understand the request syntax being used.  But there already is a\n> syntax that will support such actions (that is, the aforementioned use of\n> range: request header).\n> \n> In other words, although new http methods, etc (such as  adoption of a\n> RANGE  method) might be useful, I suspect it's not worth the trouble.\n> \n> \n> \n\n\n\n", "id": "lists-012-6475225"}, {"subject": "Re: Response Version  Clarificatio", "content": ">Let's say I had the following:\n>\n>1.0 Client -> 1.1 Proxy -> 1.0 Server\n>\nFrom the spec, the proxy MUST upgrade the 1.0 request to 1.1 and send that on\n>to the server. The server of course responds with a 1.0 response. The proxy can\n>then chose what response version to send to the client as long as it doesn't\n>break the client (chunked data or something else like that).\n>\n>However, the spec also says that proxies should keep track of the HTTP version\n>of all servers for various reasons. This is fine, but consider the following:\n>\n>1.0 Client -> 1.1 Proxy #1 -> 1.1 Proxy #2 -> 1.0 Server\n>\n>Proxy #2 of course can know the true version of the server, but there's no way\n>for Proxy #1 to know the real version of the server if Proxy #2 decides to\n>upgrade the 1.0 response to 1.1.\n\nThe \"should remember\" only talks about the server they connect to,\nnot the entire request path (all proxies are also servers).  Thus,\nit is only a performance advantage for Proxy #1 to remember the\nHTTP-version of Proxy #2.\n\nIn any case, Proxy #1 can determine the HTTP-version of the entire response\npath by looking at the Via header field.\n\n.....Roy\n\n\n\n", "id": "lists-012-6486318"}, {"subject": "Re: http and range", "content": "On Fri, 18 Sep 1998, David W. Morris wrote:\n\n> \n> I think the trend in general is to move away from user entered URLs.\n> \n...\n> \n> Also, adding new syntax to the URL has the possiblity of breaking\n> existing implementations because the parsing implementaton if often\n> flakey at best.\n> \n\nByte ranges (and indeed line ranges) in URLs have been implemented\nin the WN server since long before the Range: header.  They are\nextremely useful, but are never entered by the user.\n\n> \n> The whole range of ways and kinds of pieces users might want to \n> request doesn't lend itself to user entry of byte level encoding.\n> There is no reason the current URL scheme can't retrieve whole documents\n> as well as pieces with simple URLs ... outside of the scope of HTTP\n> and URLs, the server implementation defines the interpretation of the\n> URL. With a SINGLE data file and/or DB, the URLs \n>     http://www.bookserve.com/clancy/HotBook\n>     http://www.bookserve.com/clancy/HotBook/chapter1\n>     http://www.bookserve.com/clancy/HotBook/chapter1/para6\n> could all produce variant levels of zoom into the document. This can\n> be achieved today on some servers by creative use of mapping to cgi\n> programs. \n\nAs I said this has nothing to do with user entered URLs.  The \nfunctionality which is missing is the ability to make a link to\na subset of a document.  It can, of course, be done with CGIs,\nbut at great cost in complexity and efficiency.  \n\nExamples:\nIt is useful to be able to create links on server A to parts of \ndocuments on server B without server B having CGIs or a database.\nIt is useful to create a page of customized links to parts of\na single document (e.g. a mail archive) based on a query.\n\nThere are more.\n\nOn the other hand I agree that adding extra complexity to the\nURL syntax has serious drawbacks also.  At present much of the\nURL is opaque and a server can do with it what it wants.  This\nis a good thing and allows, for example, WN to implement byte\nand line ranges in the URL with its own syntax (which users\nnever enter because they don't know).\n\nThe way to have our cake and eat it too is an extension to the HTML\nsyntax which allows one to create a link to a URL *and* specify that a\nRange header be sent with the request.\n\n\n> \n> On Fri, 18 Sep 1998, Daniel Hellerstein wrote:\n> \n> > > Thanks. Now I see how to do a GET with range request.\n> > > Now, another question:\n> > > Is there a way to specify the offset or range with the http: URL\n> > > syntax ?\n> > > i.e. something like http://host/dir/file@range=0,500\n> > \n\nJust for your information the WN syntax is \n      http://host/dir/file;bytes=0,500 or\n      http://host/dir/file;lines=20,50\nYou can see more at http://hopf.math.nwu.edu/docs/range.html\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-6494300"}, {"subject": "Rev05: typ", "content": "There's a minor typo in section 9.9 of Rev-05: \"for use\" appears twice.\n\nJean-Philippe\n\n\n\n", "id": "lists-012-6505112"}, {"subject": "Re: de facto standard of HTT", "content": "At 00:31 15/09/1998 +0100, wendy@cs.ualberta.ca wrote:\n>Hi, what is the current de factor standard version of HTTP used by most\n>internet web servers ?  I originally thought it's 1.1 but the recent\n>IETF spec mail on 1.1 makes me wonder.  Perhaps it's 1.0 ?  What about\n>HTTP-NG ?\n\nHTTP/1.0 is declared dead\n\nHTTP/1.1 is de facto\n\nHTTP-NG isn't yet\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-6511580"}, {"subject": "Re: de facto standard of HTT", "content": "> Apache does not use HTTP-NG.\n\nYou might be interested to learn that we're using a modified version of Apache\nin our HTTP-NG testbed.  But we haven't yet got as good an integration as we'd\nlike, and there are not yet any browsers that speak HTTP-NG, so this is just\nFYI, not anything we can recommend to users right now.\n\n\n\n", "id": "lists-012-6519513"}, {"subject": "nonascii user name &amp; passwor", "content": "TEXT is inappropriate for user name and password, since:\n\n# The TEXT rule is only used for descriptive field contents and values\n# that are not intended to be interpreted by the message parser. \n\nWhether or not it's typed, it's still a string that has to be parsed\nand interpreted by the server.\n\nThe problem is that UTF-8 doesn't quite have a well-defined\n'canonical' form yet, either, although one is being developed, the\ncanonicalization algorithm won't be at \"draft standard\". So you might\nhave two browsers that would enter the same user name with different\nUTF-8 encodings, too.\n\nAnd we're not normally requiring clients to implement UTF-8\ntransformations of user type-in at all so this will be a big problem.\n\nOn the other hand, it seems inappropriate to restrict user *names* to\nUS-ASCII. I wonder if we could change the BNF and description text\nfrom \"user name\" and \"username\" to \"user id\", even if we leave\n\n    username         = \"username\" \"=\" user-id\n\nLarry\n\n\n\n", "id": "lists-012-6527585"}, {"subject": "Re: HTTP/1.1 Draft has inconsistent use of LastModified header fiel", "content": "Jason Mathews wrote:\n\n> The Last-modified header response field is inconsistently referenced in\n> the HTTP 1.1 spec as both Last-Modified and Last-modified.\n> \n> According to the definition in section 14.29 of\n> <draft-ietf-http-v11-spec-rev-04> the Last-Modified entity-header field\n> is in the form:  Last-Modified  = \"Last-Modified\" \":\" HTTP-date\n> \n> Suggestion that all occurrences of \"Last-modified\" be changed to\n> \"Last-Modified\" to avoid confusion.\n\nAll HTTP header field names are case insensitive, so those are equivalent\n(see 4.2).\n\n-- \nScott Lawrence           Consulting Engineer      <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-6536494"}, {"subject": "Re: ContentDispositio", "content": "Sounds like updating the spec to reference 2183 is in order,\nto me.\n\nAs Koen (and I) have pointed out, that non-normative section\nis there to document existing practice of existing browsers;\nanything more than this should be handled as a separate document.\n- Jim Gettys\n\n\n\n", "id": "lists-012-6545097"}, {"subject": "Re: ContentDispositio", "content": "Jim Gettys <jg@pa.dec.com> schrieb:\n> This is taken from the MIME useage of Content-Disposition.\n> We will not differ from current Web usage (without changing the name).\n\nIs it used at all on the Web? To my knowledge, it did first occur in  \nHTTP/1.1 rev. However, it certainly would not harm to update the  \nreference to RFC 1806, which is now obsolete, with a reference to RFC  \n2183.\n\n-- \nClaus Andre Faerber <http://www.muc.de/~cfaerber/> Fax: +49_8061_3361\nPGP: ID=1024/527CADCD FP=12 20 49 F3 E1 04 9E 9E  25 56 69 A5 C6 A0 C9 DC\n\n\n\n", "id": "lists-012-6552538"}, {"subject": "Re: de facto standard of HTTP (fwd", "content": "Thank you for all the responses on the following questions.\nI will sure acknowledge this mailing list in my thesis write up :)\n\nthanks,\nwendy\n\n> wendy@cs.ualberta.ca wrote:\n> > \n> > Hi, what is the current de factor standard version of HTTP used by most\n> > internet web servers ?  I originally thought it's 1.1 but the recent\n> > IETF spec mail on 1.1 makes me wonder.  Perhaps it's 1.0 ?  What about\n> > HTTP-NG ?\n> \n> Surveys say Apache runs more than 50% of the web. Apache uses HTTP/1.1.\n> Apache does not use HTTP-NG. Does that help?\n> \n> Cheers,\n> \n> Ben.\n> \n> -- \n> Ben Laurie            |Phone: +44 (181) 735 0686| Apache Group member\n> Freelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org/\n> and Technical Director|Email: ben@algroup.co.uk |\n> A.L. Digital Ltd,     |Apache-SSL author     http://www.apache-ssl.org/\n> London, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache/\n> \n> WE'RE RECRUITING! http://www.aldigital.co.uk/\n> \n\n\n\n", "id": "lists-012-6560163"}, {"subject": "Re: ContentDispositio", "content": "Content-Dispostion was implemented in Web browsers BEFORE it was added\nto the HTTP/1.1 spec, to document existing practice.  It is indeed\nquite useful, when people are downloading code, to be able to provide\na file name for \"save as\" to work.\n\nDunno how widespread it was among MIME implementations at the time.\n- Jim\n\n\n\n", "id": "lists-012-6568753"}, {"subject": "updates to Digest test sit", "content": "<http://portal.research.bell-labs.com:8000>\n\nI've changed the behavior of my Digest server test site to reflect the\nRev05 HTTP/1.1 specification.  In particular,\n\n1) The server will prompt with qop=\"auth,auth-int\" only when the request\nis HTTP/1.1 *and* it sees a TE: trailers header.  (The server *always*\nsends the auth-int Authentication-Info in a trailer.)\n\n2) The server will send Trailer: Authentication-Info when it sends\nAuthentication-Info in a trailer.\n\nDave Kristol\n\n\n\n", "id": "lists-012-6575746"}, {"subject": "RE: nonascii user name &amp; passwor", "content": "A careful re-reading of the digest spec shows that user-name is spec'd as\nquoted-string (not TEXT), and password is never interpreted by the message\nparser, just used to calculate the response from the challenge.\n\nSo, we can use TEXT for the password; that leaves the question of encoding.\n\n> -----Original Message-----\n> From: Larry Masinter [mailto:masinter@parc.xerox.com]\n> Sent: Monday, September 21, 1998 1:10 AM\n> To: Paul Leach\n> Cc: http-wg@hplb.hpl.hp.com; Chris.Newman@innosoft.com\n> Subject: non-ascii user name & password\n> \n> \n> TEXT is inappropriate for user name and password, since:\n> \n> # The TEXT rule is only used for descriptive field contents and values\n> # that are not intended to be interpreted by the message parser. \n> \n> Whether or not it's typed, it's still a string that has to be parsed\n> and interpreted by the server.\n> \n> The problem is that UTF-8 doesn't quite have a well-defined\n> 'canonical' form yet, either, although one is being developed, the\n> canonicalization algorithm won't be at \"draft standard\". So you might\n> have two browsers that would enter the same user name with different\n> UTF-8 encodings, too.\n> \n> And we're not normally requiring clients to implement UTF-8\n> transformations of user type-in at all so this will be a big problem.\n\nI don't think we need to require UTF-8, just say that USASCII is required,\nand UTF-8 is allowed. If you have a unicode password, then you need a UTF-8\ncapable browser; if not, you don't.\n\n> \n> On the other hand, it seems inappropriate to restrict user *names* to\n> US-ASCII. I wonder if we could change the BNF and description text\n> from \"user name\" and \"username\" to \"user id\", even if we leave\n> \n>     username         = \"username\" \"=\" user-id\n\nHow does this help?\n\nCan we say that user-id in USASCII must be supported and UTF-8 may be\nsupported if the user name contains characters not in the USASCII set? The\nUTF-8 encoding is well defined even if the canonical form isn't, isn't it?\nThen we don't reference the canonical form spec, and when that comes out\neveryone is cool.\n\nOr, I could say:\n------------------\npasswd   = *OCTET\n\nIt is the responsibility of the client implementation to make sure that the\nuser can generate any octet string when providing the password. A protocol\nfor setting and changing passwords (which is beyond the scope of this\ndocument) must specify how what the user provides maps to the actual octet\nstring \"on the wire\".\n-------------------\n \nPaul\n\n\n\n", "id": "lists-012-6582408"}, {"subject": "200 Level warnings in revalidatio", "content": "The spec says that all 200 level warnings must be retained after a successful\nrevalidation.\n\nHowever, it also says that all end-to-end headers (Warnings are end-to-end)\nmust be replaced after by the corresponding end-to-end header in a revalidation.\n\nSo if we had a cached copy of an entity that has a 200 level warning associated\nwith it and when we revalidate, we get a Warning header w/o the 200 level\nwarning, are we supposed to keep it or not?\n\nThanks,\nChris\n\n\n\n", "id": "lists-012-6593800"}, {"subject": "Full URL if Proxy ", "content": "HI, has anyone encounter the following problem:\n \nI run netscape as the client browser talking to an apache web server.\nIn between them there is a proxy server sitting on the client\nmachine.  The proxy serve simply do tunneling now.\n \n I request the first url, e.g. http://www.cs.ualbert.ca/~wendy, which\n is my home dir in my research acct.\n \n Then I request a link on the page, which in the html page is simply\n pointing to a file in a subdirectory, e.g. <a href=\"subdir/hello.html\">\n \n The request header received upon the second request is not what I read\n from the http 1.1 spec....\n \n Received:\n GET http://www.cs.ualberta.ca/subdir/hello.html HTTP/1.1\n \n Instead of what I expected to be \n GET http://www.cs.ualberta.ca/~wendy/subdir/hello.html HTTP/1.1\n (recall full path if thru proxy)\n \n Does anyone know why ?  This happens every so often.  Is this an apache\n server or netscape browser bug ???\n \n Thanks in advance for any responses....\n Wendy\n \n\n\n\n", "id": "lists-012-6600894"}, {"subject": "RE: nonascii user name &amp; passwor", "content": "> A careful re-reading of the digest spec shows that user-name is spec'd as\n> quoted-string (not TEXT), and password is never interpreted by the message\n> parser, just used to calculate the response from the challenge.\n\n> So, we can use TEXT for the password; that leaves the question of encoding.\n\nI think you're reading \"parsed\" too liberally. The problem is that\nTEXT is not useful for anything that depends on the interpretation\nof the string besides it being displayed to the user. That's because\nthe same user name can be represented in many different ways, each\nof which is a legal encoding:\n\n# The TEXT rule is only used for descriptive field contents and values\n# that are not intended to be interpreted by the message parser. Words of\n# *TEXT MAY contain characters from character sets other than ISO 8859-1\n# [22] only when encoded according to the rules of RFC 2047 [14].\n\nSo a Japanese user might expect to type a username and have it be\nrecognized, but two browsers might encode the username\nin Shift-JIS or EUC or UTF-8; the result, though, would be different\nstrings. A server wouldn't do well to try to match them all.\n\nIn addition, the spec selected ISO-8859-1 as the 'default' string\nrepresentation. It's built into the spec and I don't think we can\nretroactively change it to UTF-8.  There are probably many European\nusers who have ISO-8859-1 user names who already use the fact that\nusername & password are assumed to be ISO-8859-1. So I don't think\nwe can do the 'restrict to ASCII and migrate to UTF8 later' path.\n\nSo I'm back to restricting 'user-id' to be US-ASCII, and noting, as\ntactfully and apologetically as we can, that this does not actually\nallow userful user *names* for users whose name cannot be typed in\nASCII.\n\nLarry (do not reply to this email address)\n\n\n\n", "id": "lists-012-6609325"}, {"subject": "RE: nonascii user name &amp; passwor", "content": "On Mon, 21 Sep 1998, Paul Leach wrote:\n> Or, I could say:\n> ------------------\n> passwd   = *OCTET\n> \n> It is the responsibility of the client implementation to make sure that the\n> user can generate any octet string when providing the password. A protocol\n> for setting and changing passwords (which is beyond the scope of this\n> document) must specify how what the user provides maps to the actual octet\n> string \"on the wire\".\n> -------------------\n\nThat's the right formal syntax (although a recommendation to exclude NUL\nmight be wise).  However, most passwords are typed as characters, so an\ninteroperable spec (at least to get multi-client interoperability) has to\nsay how typed characters in a password are encoded.  The difference is\nvisible on the wire even though the server should not care.\n\nMy suggestion:\n\n  When a password is typed by a user, the characters are encoded in\n  US-ASCII.  Encoding of non-US-ASCII characters is not specified at this\n  time, but use of localized character sets such as ISO-8859-1 for this\n  purpose is forbidden.  Clients are encouraged to provide a facility for\n  entry of uninterpreted binary passwords.\n\nI think this is the best interoperability we can get at this time without \ncreating an implementation burden.\n\nThe user name is TEXT (in a quoted string) which is trickier to deal with\nsince it is in the base protocol which by context defaults to ISO-8859-1.\nThe problem is that no other protocol made the mistake of using ISO-8859-1\nfor user names, and they are likely to be amended to use UTF-8.  Since\nHTTP digest includes the username in the one-way-function used to store\nthe password on the server, this would mean that non-ASCII usernames\nwouldn't be able to share authentication backend databases between HTTP\ndigest and other protocols.  I think that would be a serious mistake. \n\nI'd say that any server which permits non-ASCII characters in the username\nfield SHOULD convert them to UTF-8 prior to including them in the \none-way-function.  The conversion from ISO-8859-1 to UTF-8 is trivial, so\nthat's not a significant burden in exchange for multi-protocol\ninteroperability of digest authentication.\n\nRFC 2047 encoding is forbidden in a quoted-string, so that can't be used\non the user name.  As it stands, user names in HTTP are Euro-centric\nunless RFC 2231 encoding were to be permitted at some future date.\n\n- Chris\n\n\n\n", "id": "lists-012-6618839"}, {"subject": "RE: Full URL if Proxy ", "content": "Sounds more like a quirk than a bug. Since \"~wendy\" is not followed by a\n\"/\", the browser thinks it has received the page \"~wendy\" in the root\ndirectory. That is why \"subdir/hello.html\" looks like it sits under the root\ndirectory, not your \"~wendy\" subdirectory.\n\nI've seen some servers respond to the request for \"~wendy\" with a 301\nredirecting to \"~wendy/\" just to make clear that you're getting the default\ndocument for that subdirectory. I don't see that as a requirement, but\nothers can correct that assumption.\n\n-Rob Polansky\n\n> -----Original Message-----\n> From: wendy@cs.ualberta.ca [mailto:wendy@cs.ualberta.ca]\n> Sent: Monday, September 21, 1998 6:53 PM\n> To: http-wg@hplb.hpl.hp.com\n> Cc: Wendy Liew\n> Subject: Full URL if Proxy ?\n>\n>\n> HI, has anyone encounter the following problem:\n>\n> I run netscape as the client browser talking to an apache web server.\n> In between them there is a proxy server sitting on the client\n> machine.  The proxy serve simply do tunneling now.\n>\n>  I request the first url, e.g. http://www.cs.ualbert.ca/~wendy, which\n>  is my home dir in my research acct.\n>\n>  Then I request a link on the page, which in the html page is simply\n>  pointing to a file in a subdirectory, e.g. <a href=\"subdir/hello.html\">\n>\n>  The request header received upon the second request is not what I read\n>  from the http 1.1 spec....\n>\n>  Received:\n>  GET http://www.cs.ualberta.ca/subdir/hello.html HTTP/1.1\n>\n>  Instead of what I expected to be\n>  GET http://www.cs.ualberta.ca/~wendy/subdir/hello.html HTTP/1.1\n>  (recall full path if thru proxy)\n>\n>  Does anyone know why ?  This happens every so often.  Is this an apache\n>  server or netscape browser bug ???\n>\n>  Thanks in advance for any responses....\n>  Wendy\n>\n>\n>\n\n\n\n", "id": "lists-012-6629047"}, {"subject": "Re: nonascii user name &amp; passwor", "content": ">My suggestion:\n>\n>  When a password is typed by a user, the characters are encoded in\n>  US-ASCII.  Encoding of non-US-ASCII characters is not specified at this\n>  time, but use of localized character sets such as ISO-8859-1 for this\n>  purpose is forbidden.  Clients are encouraged to provide a facility for\n>  entry of uninterpreted binary passwords.\n\nThat would invalidate almost all client implementations of HTTP.\nThere is no technical reason to define the encoding other than to\nsay it is a shared understanding between client and server that\nis outside the capacity of the protocol to determine, and that\ninteroperability problems may occur if non-US-ASCII characters\nare used.  Forbidding it just makes the specification worthless.\n\nChanging this in existing HTTP systems is not an option.  The only way\nto add a specific encoding to the username/password exchange is to\ndefine a new authentication method that requires it from the start.\n\n....Roy\n\n\n\n", "id": "lists-012-6638902"}, {"subject": "Re: nonascii user name &amp; passwor", "content": ">With your proposal, vendor A could build a compliant HTTP client which\n>only uses ISO-8859-1 for passwords and vendor B could build a compliant\n>HTTP client which only uses UTF-8 for passwords.  Client A and Client B\n>don't interoperate if the password contains non-ASCII characters. \n>Therefore the spec would fail the interoperability test and is certainly\n>not eligable for draft standard status (and probably not even proposed\n>standard status).\n\nNo, both would use octets for passwords and if either one does encoding\ntranslation then they may or may not interoperate, depending on how\nthe user created the password in the first place.\n\n>Forbidding this situation is necessary to make sure all compliant clients\n>interoperate.\n\nIf that were true, they wouldn't interoperate now.  The fact is that\neveryone either uses ASCII passwords or continues to use the\nsame charset for password entry that they used for password creation,\nwhich is not surprising.  None of the servers care about the encoding\nof the password characters.  None of the clients do encoding translation.\nThat is why it works, even if it is sub-optimal.\n\n>P.S. Will the average user realize he has to manually configure the\n>\"private agreement password charset\" in his browser before he can\n>authenticate if he uses non-ASCII characters?\n\nIt is, by its very nature, the default.  How do you think the average\nuser will feel about all of his current password-enabled services being\nbroken just to support a potential mismatch between system charsets?\n\n....Roy\n\n\n\n", "id": "lists-012-6647573"}, {"subject": "RE: Full URL if Proxy ", "content": "try adding a slash '/' after ~wendy..\nlike: http://server/~wendy/\nthe browser probably doesnt realize that it is within\na subdirectory.  This can be cause by the server not\nredirecting the original URL to a slash terminated\non or to the actual file..\nRedirect http://server/~wendy to:\neither\n1) http://server/~wendy/\n2) http://server/~wendy/index.html\n\n\n> -----Original Message-----\n> From: wendy@cs.ualberta.ca [mailto:wendy@cs.ualberta.ca]\n> Sent: Thursday, September 24, 1998 9:29 AM\n> To: http-wg@hplb.hpl.hp.com\n> Cc: wendy@cs.ualberta.ca\n> Subject: Full URL if Proxy ?\n> \n> \n> HI, has anyone encounter the following problem:\n>  \n> I run netscape as the client browser talking to an apache web server.\n> In between them there is a proxy server sitting on the client\n> machine.  The proxy serve simply do tunneling now.\n>  \n>  I request the first url, e.g. http://www.cs.ualbert.ca/~wendy, which\n>  is my home dir in my research acct.\n>  \n>  Then I request a link on the page, which in the html page is simply\n>  pointing to a file in a subdirectory, e.g. <a \n> href=\"subdir/hello.html\">\n>  \n>  The request header received upon the second request is not \n> what I read\n>  from the http 1.1 spec....\n>  \n>  Received:\n>  GET http://www.cs.ualberta.ca/subdir/hello.html HTTP/1.1\n>  \n>  Instead of what I expected to be \n>  GET http://www.cs.ualberta.ca/~wendy/subdir/hello.html HTTP/1.1\n>  (recall full path if thru proxy)\n>  \n>  Does anyone know why ?  This happens every so often.  Is \n> this an apache\n>  server or netscape browser bug ???\n>  \n>  Thanks in advance for any responses....\n>  Wendy\n>  \n> \n\n\n\n", "id": "lists-012-6656563"}, {"subject": "Re: nonascii user name &amp; passwor", "content": ">> How do you think the average\n>> user will feel about all of his current password-enabled services being\n>> broken just to support a potential mismatch between system charsets?\n>\n>Programs that rely on private agreements to interoperate deserve to break\n>(and occasionally do break in practice).\n\nYes, they do. That doesn't change the definition of the protocol.  \nThe username and password were defined as ISO-8859-1 when the \nauthentication fields were invented and deployed.  Except for the\nusual charset politics, that definition worked just fine.\n\n>I know in email protocols the IETF has held a hard line and never\n>permitted unlabeled 8-bit text in a standard.  Is there something about\n>http that justifies breaking this precedent?  What do other people think? \n>Is this a case where correct international interoperability has to be\n>sacrificed due to the localized private-agreement installed base?\n\nIn email protocols, specifications that contrast with reality have\ntraditionally been ignored by almost all developers and resulted in\ninteroperability failures when some poor sap actually attempted to comply\nwith the RFC.  HTTP does not allow that.  HTTP has a version number \nwhose minor number is supposed to change whenever compatible changes\nare introduced, and a major number that is supposed to change whenever\nincompatible changes are introduced.\n\nI have no problem with defining a new protocol in the HTTP family that\ncures the hundred-odd problems leftover from the installed base and\neventually progresses on the standards track.  I have a huge problem\nwith such a protocol masquerading as HTTP/1.x when we have carefully\ndesigned the protocol for forward compatibility.  The problem is that\nthe IETF standards-track process interferes with good protocol design\nby not allowing progress along delineated branches.\n\nIt is high time that the IETF started thinking in terms of protocol\nfamilies and planning for evolution rather than making standards\ndecrees and hoping the installed base gets sucked into the void.\n\n....Roy\n\n\n\n", "id": "lists-012-6666714"}, {"subject": "domain attribute in digest aut", "content": "I have some points concerning draft-ietf-http-authentication-03.\n\n1) Section 3.2.1:\n\n    domain\n      A quoted, space-separated list of URIs, as specified in RFC XURI [7],\n      that define the protection space.  If a URI is an abs_path, it is\n      relative to the canonical root URL (see section 1.2 above) of the\n      server being accessed. An absoluteURI in this list may refer to a\n      different server than the one being accessed. The client can use this\n      list to determine the set of URIs for which the same authentication\n      information may be sent: any URI that has a URI in this list as a\n      prefix (after both have been made absolute) may be assumed to be in\n      the same protection space. If this directive is omitted or its value\n      is empty, the client should assume that the protection space consists\n->    of all URIs on the responding server. This directive is not\n->    meaningful in Proxy-Authenticate headers, for which the protection\n->    space is always the entire proxy; if present it should be ignored.\n\n   I disagree with the last part. The domain directive could be used\n   usefully to share auth info between various proxies. So, while the\n   abs_path part of the URI is indeed not meaningful, I claim the host:port\n   part is. Therefore I suggest changing the last three lines to:\n\n      of all URIs on the responding server.\n\n      Since on proxies the protection space is always the entire proxy,\n      the abs_path part of the URIs is not meaningful in\n      Proxy-Authenticate headers and SHOULD be ignored.\n\n   (I think that should be normative SHOULD?)\n\n\n2) Same text:\n\n    domain\n      A quoted, space-separated list of URIs, as specified in RFC XURI [7],\n      that define the protection space.  If a URI is an abs_path, it is\n      relative to the canonical root URL (see section 1.2 above) of the\n      server being accessed. An absoluteURI in this list may refer to a\n      different server than the one being accessed. The client can use this\n      list to determine the set of URIs for which the same authentication\n      information may be sent: any URI that has a URI in this list as a\n      prefix (after both have been made absolute) may be assumed to be in\n->    the same protection space. If this directive is omitted or its value\n->    is empty, the client should assume that the protection space consists\n->    of all URIs on the responding server. This directive is not\n      meaningful in Proxy-Authenticate headers, for which the protection\n      space is always the entire proxy; if present it should be ignored.\n\n   Current practise for Basic auth and Cookies is that all URIs with the\n   same prefix as the requested URI are in the same protection space. I\n   think this is more intuitive and more closely follows the way sites are\n   set up, and therefore should be adopted here too. If somebody wants to\n   say that the whole server is in the same space they can always use\n   domain=\"/\". So, borrowing from the wording in the cookie spec I propose\n   changing the marked lines to read:\n\n      the same protection space. If this directive is omitted or its value\n      is empty it defaults to the request URI up to and including the\n      right-most /. This directive is not\n\n   Merging the above two changes would result in:\n\n    domain\n      A quoted, space-separated list of URIs, as specified in RFC XURI [7],\n      that define the protection space.  If a URI is an abs_path, it is\n      relative to the canonical root URL (see section 1.2 above) of the\n      server being accessed. An absoluteURI in this list may refer to a\n      different server than the one being accessed. The client can use this\n      list to determine the set of URIs for which the same authentication\n      information may be sent: any URI that has a URI in this list as a\n      prefix (after both have been made absolute) may be assumed to be in\n      the same protection space. If this directive is omitted or its value\n      is empty it defaults to the request URI up to and including the\n      right-most /.\n\n      Since on proxies the protection space is always the entire proxy,\n      the abs_path part of the URIs is not meaningful in\n      Proxy-Authenticate headers and SHOULD be ignored.\n\n\n3) While a careful reading of the spec does imply the following, it might\n   not hurt to add something like it (marked by new>) to the description\n   of the nonce-count in section 3.2.2:\n\n    nonce-count\n      This MUST be specified if a qop directive is sent (see above), and\n      MUST NOT be specified if the server did not send a qop directive in\n      the WWW-Authenticate header field.  The nc-value is the hexadecimal\n      count of the number of requests (including the current request) that\n      the client has sent with the nonce value in this request.  For\n      example, in the first request sent in response to a given nonce\n      value, the client sends \"nc=00000001\".  The purpose of this directive\n      is to allow the server to detect request replays by maintaining its\n      own copy of this count - if the same nc-value is seen twice, then the\n      request is a replay.   See the description below of the construction\n      of the request-digest value.\n\nnew>  Servers which check the nonce-count must be sure to increment their\nnew>  own copy of the count each time an Authorization header is received\nnew>  from the client, irrespective of the final outcome of the request\nnew>  (e.g. even if the request-URI doesn't require authentication, \nnew>  requires different authentication, or doesn't exist).\n\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-6676231"}, {"subject": "Editorial: &quot;chunked&quot; to &quot;trailers&quot; in T", "content": "It seems this one got overlooked in the switch to the \"trailers\" T-E in\ndraft-ietf-http-v11-spec-rev-05:\n\n  Section 14.40 TE:\n\n     A server MUST NOT include any header fields unless the \"chunked\"\n     transfer-coding is present in the request as an accepted transfer-\n     coding in the TE field.\n\n  Replace \"chunked\" by \"trailers\".\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-6690122"}, {"subject": "RE: domain attribute in digest aut", "content": "The second change you propose is incompatible with RFC 2069, for which\nimplementations exist. Furthermore, it reduces efficiency. For Basic, it had\nto be that way because it is very dangerous to send a cleartext password\noutside the protection space (inside, too, but that's a different story :-).\nFor Digest, it is relatively safe to send an Authorization header with a\nDigest response to any site that already has seen it, and only get a 401 if\nit doesn't work. So, the choice was by design. A server that wants the\n\"prefix\" behavior you propose can respond to \"GET http://host/path\" with\n\"domain=path\".\n\nThe first change is backwards compatible, so could probably be made at this\npoint if there were  concensus. I actually think that one could say that\nit's safe to consider all proxies in the same protection space, regardless\nof what \"domain\" says. One shouldn't configure one's browser to point at\nproxies to which one wouldn't be willing to send a Digest response. AS a\nresult, one could almost consider this an implementation issue: clients that\nwant to pre-authentication to all proxies should just do so.\n\nJim -- are there any implementations of Proxy-Auth with Digest in the\nimplementation reports?\n\n\n\n", "id": "lists-012-6697604"}, {"subject": "Re: domain attribute in digest aut", "content": "There are two Proxy-Authenticate Digest implementations reported:\nRonald's, and John Mallery's.\n- Jim\n\n\n\n", "id": "lists-012-6706992"}, {"subject": "RE: domain attribute in digest aut", "content": "Can you give a pointer to these implementations ? Are these proxies\navailable for use by netizens ?\n\nferoze\n\n-----Original Message-----\nFrom: jg@pa.dec.com [mailto:jg@pa.dec.com]\nSent: Tuesday, September 29, 1998 12:59 PM\nTo: Paul Leach\nCc: Ronald.Tschalaer@psi.ch; HTTP-WG@hplb.hpl.hp.com\nSubject: Re: domain attribute in digest auth\n\n\nThere are two Proxy-Authenticate Digest implementations reported:\nRonald's, and John Mallery's.\n- Jim\n\n\n\n", "id": "lists-012-6714437"}, {"subject": "Re: domain attribute in digest aut", "content": "Everything is off of the implementation reports page:\nhttp://www.w3.org/Protocols/HTTP/Forum/Reports/\n\nRonald's is a client implementation.\n\nI don't think John Mallery has updated his implementation to the current\ndraft; he claimed to me he was going to start hacking on it a few minutes\nago to update it (probably the base HTTP protocol first, before digest).\n\nThen again, John's been making such claims to me since before Chicago.\n(don't hold your breath)...  His proxy implementation isn't as up to\ndate as his origin server (which is used at the White House, so sees\nsome serious traffic.\n\nOn the other hand, if you poke at him, so he sees there is more\ninterest, it is more likely to get done.\n- Jim\n\n\n\n", "id": "lists-012-6724248"}, {"subject": "Re: 200 Level warnings in revalidatio", "content": "Chris DiPierro <cdipierr@us.ibm.com> writes:\n\n    The spec says that all 200 level warnings must be retained after a\n    successful revalidation.\n\n    However, it also says that all end-to-end headers (Warnings are\n    end-to-end) must be replaced after by the corresponding end-to-end\n    header in a revalidation.\n\n    So if we had a cached copy of an entity that has a 200 level\n    warning associated with it and when we revalidate, we get a Warning\n    header w/o the 200 level warning, are we supposed to keep it or\n    not?\n    \nI believe you are referring to section 13.5.3 (Combining Headers),\nand this particular passage:\n\n   The end-to-end headers stored in the cache entry are used for the\n   constructed response, except that\n\n     .  any stored Warning headers with warn-code 1xx (see section\n        14.46) MUST be deleted from the cache entry and the forwarded\n        response.\n     .  any stored Warning headers with warn-code 2xx MUST be retained\n        in the cache entry and the forwarded response.\n     .  any end-to-end headers provided in the 304 or 206 response MUST\n        replace the corresponding headers from the cache entry.\n\n   Unless the cache decides to remove the cache entry, it MUST also\n   replace the end-to-end headers stored with the cache entry with\n   corresponding headers received in the incoming response.\n\n   In other words, the set of end-to-end headers received in the\n   incoming response overrides all corresponding end-to-end headers\n   stored with the cache entry (except for stored Warning headers with\n   warn-code 1xx, which are deleted even if not overridden).\n\nI guess this might be somewhat confusing.  The second exception\nin the bulleted list:\n\n     .  any stored Warning headers with warn-code 2xx MUST be retained\n        in the cache entry and the forwarded response.\n\nappears to conflict with\n\n   Unless the cache decides to remove the cache entry, it MUST also\n   replace the end-to-end headers stored with the cache entry with\n   corresponding headers received in the incoming response.\n\nbut the obvious resolution is that special treatment for Warning\nheaders applies to this paragraph, as well as the one above the\nlist of exceptions.  I.e., the paragraph should read:\n\n   Unless the cache decides to remove the cache entry, it MUST also\n   replace the end-to-end headers stored with the cache entry with\n   corresponding headers received in the incoming response, except\n   for Warning headers as described immediately above.\n\n-Jeff\n\n\n\n", "id": "lists-012-6732940"}, {"subject": "question about implied LW", "content": "Section 2.2 on \"basic rules\" says:\n\nimplied *LWS\nThe grammar described by this specification is word-based. Except where\nnoted otherwise, linear white space (LWS) can be included between any two\nadjacent words (token or quoted-string), and between adjacent tokens and\nseparators, without changing the interpretation of a field. At least one\ndelimiter (LWS and/or separators) MUST exist between any two tokens (for the\ndefinition of \"token\" below), since they would otherwise be interpreted as a\nsingle token. \n\nThis seems to say that LWS is not allowed between adjacent quoted-strings.\nWas that intended? I assume not, but I could be wrong.\n\nIf not, I think it would be clearer to add the following production to the\nbasic rules\nword = token | quoted-string | separator\nand then change the section on implied LWS to say\n\nimplied *LWS\nThe grammar described by this specification is word-based. Except where\nnoted otherwise, linear white space (LWS) can be included between any two\nadjacent words (see below for the definition of \"word\") without changing the\ninterpretation of a field. At least one delimiter (LWS and/or separators)\nMUST exist between any two tokens (for the definition of \"token\" below),\nsince they would otherwise be interpreted as a single token.\n\nPaul\n\n\n\n", "id": "lists-012-6742959"}, {"subject": "Implementation report for HTTP/1.1 to Draft Standar", "content": "This implementation report is for http-v11-spec-rev-05 &\nhttp-authentication-03 to progress to Draft Standard.\n\nThis report is abbreviated; see\n    http://www.w3.org/Protocols/HTTP/Forum/Reports/\nfor more explanation and individual implementation reports. Many\nthanks to the contributors who took the time to report on their\nsystems, and to Jim Gettys for accumulating the results and\nencouraging sufficient testing and reporting.\n\nWe have surveyed implementations for whether each feature in these\nspecifications have been tested against independent interoperable\nimplementations. In the survey, nearly all features have had more than\ntwo clients, servers, and proxies tested. Since the survey was\nperformed, almost all of the _remaining_ features have been tested,\nand we feel confident in bringing forward the specifications as Draft\nStandard at this point.\n\nWe have implementation & testing reports from 20 implementations:\n\n* libwww-perl NG-alpha-0.11 from Aas Software: a Perl HTTP\n  client library (Gisle Aas)\n* HTTPClient V0.4-dev from Ronald Tschalaer: a client library in Java\n* libwww 5.1k from the World Wide Web Consortium: a client library in C,\n  with various tools built on top (Henrik Frystyk Nieslen)\n* Internet Explorer 4 from Microsoft Corporation: a full-feature web\n  browser (Yaron Goland)\n* Netscape Navagator and Communicator 5 from Netscape Communication\n  Corporation (Gagan Saksena): a full-feature web browser and editor\n* Netscape Enterprise Server 3.51 from Netscape Communication\n  Corporation: a HTTP/1.1 server (Mike Belshe)\n* Apache 1.3b6 from the Apache Group: a HTTP/1.1 Server and incomplete\n  caching proxy (Roy Fielding)\n* DMKHTD 1.06f from Dave Kristol, Bell Labs: a HTTP/1.1 Server\n* Microsoft IIS 4.0 from Microsoft Corporation: a HTTP/1.1 Server\n  (Henry Sanders)\n* WN 2.0.0 from John Franks, Northwestern University Math Department:\n  a HTTP/1.1 Server\n* SRE-http 1.3a from Daniel Hellerstein, USDA: a HTTP/1.1 Server  \n* HASS 1.00d.a from Applied Theory Communications: a Application\n  Server Suite (Patrick McManus)\n* Microsoft Proxy Server 2.0 from Microsoft Corporation a HTTP/1.1\n  Caching Proxy server (Lester Waters)\n* CL-HTTP 67.47 from John Mallery, MIT AI Lab: a combined sever and\n  caching proxy, including a client and Web Walker application\n* Jigsaw 2.0beta from the World Wide Web Consortium, a combined server\n  and caching proxy (Yves Lafon)\n* Raptor Firewall 5.1 from Axent Technologies, a HTTP/1.1 firewall\n  Proxy (no caching) (Robert Polansky)\n* GiambyNetGrabber 0.65 from GiambiSoft: a client Internet mirroring\n  tool (Giambattista Bloisi)\n* Millicent Proxy 1.0 (3 separate implementations) from Digital\n  Equipment Corporation: the MilliCent microcommerce system, including\n  Server, Proxy and Gateway (reverse proxy) (Steve Glassman).  The\n  separate implementations were by different people at different\n  geographical locations in different programming languages.\n\nTable of Results:\n\nEach implementation was surved to review the http-spec-v11-rev\ndocument and the http-authentication document to review each section\nfor whether all of the features in that section were implemented and\ntested for interoperability of that feature against a separate\nindependent implementation. The codes below indicate different\nresponses:\n\n't'   tested against another independent implementation\n'y'   implemented but not tested against independent implementation\n'n'   not implementated\n'-'   not applicable to this type of implementation\n\nClients   |Servers   | Proxies  |Feature\n11t 0y  3n|13t 1y  4n| 5t 0y  3n|H 8.1 Persistent Connections\n 7t 0y  5n| 3t 1y  4n| 3t 0y  4n|H 8.2.3 Automatic retrying\n 5t 3y  5n| 5t 6y  7n| 3t 2y  3n|H 8.2.4 100 (Continue) status\n 5t 1y  7n| 7t 3y  8n| 3t 1y  4n|H 9.2 OPTIONS\n14t 0y  0n|17t 1y  0n| 8t 0y  0n|H 9.3 GET\n13t 0y  1n|17t 1y  0n| 8t 0y  0n|H 9.4 HEAD\n13t 0y  0n|16t 2y  0n| 8t 0y  0n|H 9.5 POST\n 8t 1y  4n| 9t 3y  6n| 4t 0y  4n|H 9.6 PUT\n 5t 2y  6n| 5t 4y  9n| 2t 1y  5n|H 9.7 DELETE\n 5t 2y  5n| 6t 6y  5n| 2t 2y  4n|H 9.8 TRACE\n 6t 3y  4n| 4t 3y  7n| 3t 3y  2n|H 9.9 CONNECT\n 8t 2y  3n| 6t 7y  5n| 3t 2y  3n|H 10.1.1 100 Continue\n 2t 2y  9n| 2t 3y 13n| 2t 1y  5n|H 10.1.2 101 Switching Protocols\n14t 0y  0n|17t 1y  0n| 8t 0y  0n|H 10.2.1 200 OK\n 5t 2y  5n| 5t 4y  8n| 2t 1y  4n|H 10.2.2 201 Created\n 4t 1y  7n| 5t 1y 12n| 2t 0y  6n|H 10.2.3 202 Accepted\n 3t 1y  8n| 4t 1y 13n| 2t 0y  6n|H 10.2.4 203 Non-Authoritative Info\n 6t 3y  5n| 7t 2y  9n| 3t 1y  4n|H 10.2.5 204 No Content\n 3t 1y  8n| 4t 0y 14n| 2t 0y  6n|H 10.2.6 205 Reset Content\n 7t 1y  5n| 6t 5y  7n| 3t 1y  4n|H 10.2.7 206 Partial Content\n 4t 2y  7n| 4t 3y 11n| 2t 1y  5n|H 10.3.1 300 Multiple Choices\n11t 2y  1n|13t 2y  3n| 7t 0y  1n|H 10.3.2 301 Moved Permanently\n 7t 2y  5n|10t 3y  5n| 4t 0y  4n|H 10.3.3 302 Found\n 5t 4y  5n| 4t 3y 11n| 2t 2y  4n|H 10.3.4 303 See Other\n11t 1y  1n|16t 1y  1n| 8t 0y  0n|H 10.3.5 304 Not Modified\n 3t 5y  6n| 2t 3y 12n| 2t 2y  4n|H 10.3.6 305 Use Proxy\n 7t 5y  2n| 7t 1y 10n| 5t 1y  2n|H 10.3.7 307 Temporary Redirect\n12t 0y  1n|12t 5y  1n| 7t 0y  1n|H 10.4.1 400 Bad Request\n12t 2y  0n|16t 2y  0n| 8t 0y  0n|H 10.4.2 401 Unauthorized\n 3t 6y  3n| 3t 5y 10n| 3t 4y  1n|H 10.4.3 402 Payment Required\n10t 2y  1n|12t 5y  1n| 7t 1y  0n|H 10.4.4 403 Forbidden\n12t 0y  1n|15t 3y  0n| 8t 0y  0n|H 10.4.5 404 Not Found\n 7t 2y  4n| 7t 6y  5n| 4t 1y  3n|H 10.4.6 405 Method Not Allowed\n 6t 3y  4n| 5t 4y  9n| 3t 2y  3n|H 10.4.7 406 Not Acceptable\n11t 2y  1n| 8t 0y  7n| 7t 0y  1n|H 10.4.8 407 Proxy Auth Required\n 4t 3y  7n| 4t 3y 10n| 2t 0y  6n|H 10.4.9 408 Request Timeout\n 4t 3y  6n| 3t 4y 11n| 2t 1y  5n|H 10.4.10 409 Conflict\n 4t 2y  7n| 4t 0y 14n| 2t 0y  6n|H 10.4.11 410 Gone\n 4t 5y  5n| 4t 5y  9n| 2t 2y  4n|H 10.4.12 411 Length Required\n 4t 4y  5n| 5t 6y  7n| 2t 2y  4n|H 10.4.13 412 Precondition Failed\n 3t 4y  6n| 4t 2y 12n| 2t 1y  5n|H 10.4.14 413 Req Entity Too Large\n 5t 2y  6n| 3t 2y 13n| 2t 1y  5n|H 10.4.15 414 Request-URI Too Long\n 5t 2y  6n| 4t 2y 12n| 3t 0y  5n|H 10.4.16 415 Unsupported Media Type\n 4t 4y  5n| 3t 5y 10n| 3t 1y  4n|H 10.4.17 416 range not satisfiable\n 2t 4y  7n| 4t 3y 11n| 2t 1y  5n|H 10.4.18 417 Expectation|Failed\n 6t 2y  4n| 7t 7y  4n| 4t 1y  3n|H 10.5.1 500 Internal Server Error\n 6t 2y  4n| 7t 8y  3n| 4t 1y  3n|H 10.5.2 501 Not Implemented\n 4t 2y  6n| 3t 2y 10n| 2t 1y  5n|H 10.5.3 502 Bad Gateway\n 6t 3y  4n| 5t 3y 10n| 3t 2y  3n|H 10.5.4 503 Service Unavailable\n 5t 3y  4n| 4t 4y  7n| 3t 2y  3n|H 10.5.5 504 Gateway Timeout\n 4t 2y  6n| 4t 6y  8n| 2t 1y  5n|H 10.5.6 505 Version Not Supported\n 4t 4y  5n| 6t 6y  6n| 2t 2y  4n|H 13.3.3 Strong entity tags\n 1t 5y  7n| 3t 5y 10n| 1t 3y  4n|H 13.3.3 Weak entity tags\n12t 0y  1n|13t 2y  3n| 8t 0y  0n|H 14.1 Accept\n 8t 1y  4n| 9t 3y  6n| 5t 0y  3n|H 14.2 Accept-Charset\n 9t 1y  4n| 6t 6y  6n| 4t 1y  3n|H 14.3 Accept-Encoding\n 7t 2y  4n|10t 3y  5n| 4t 1y  3n|H 14.4 Accept-Language\n 6t 2y  5n| 6t 6y  6n| 2t 2y  4n|H 14.5 Accept-Ranges\n 7t 1y  5n| 5t 2y  4n| 4t 1y  3n|H 14.6 Age\n 3t 3y  6n| 7t 8y  3n| 2t 3y  3n|H 14.7 Allow\n12t 0y  2n|12t 5y  1n| 7t 0y  1n|H 14.8 Authorization\n10t 2y  1n|12t 5y  1n| 7t 1y  0n|H 14.9 Cache-Control\n14t 0y  0n|13t 4y  1n| 8t 0y  0n|H 14.10 Connection\n11t 2y  1n|10t 6y  2n| 6t 1y  1n|H 14.11 Content-Encoding\n 5t 3y  5n| 7t 6y  5n| 3t 2y  3n|H 14.12 Content-Language\n13t 0y  1n|14t 3y  1n| 8t 0y  0n|H 14.13 Content-Length\n 8t 1y  4n| 5t 5y  8n| 4t 1y  3n|H 14.14 Content-Location\n 3t 1y 10n| 4t 5y  9n| 2t 1y  5n|H 14.15 Content-MD5\n 8t 1y  4n| 6t 7y  5n| 4t 1y  3n|H 14.16 Content-Range\n12t 0y  1n|15t 3y  0n| 8t 0y  0n|H 14.17 Content-Type\n12t 0y  1n|15t 3y  0n| 8t 0y  0n|H 14.18 Date\n 6t 3y  4n| 7t 7y  4n| 3t 2y  3n|H 14.19 ETag\n 4t 1y  9n| 5t 4y  9n| 2t 1y  5n|H 14.20 Expect\n 8t 0y  5n|10t 4y  4n| 5t 0y  3n|H 14.21 Expires\n 6t 3y  4n| 4t 4y  8n| 2t 3y  3n|H 14.22 From\n14t 0y  0n|15t 3y  0n| 8t 0y  0n|H 14.23 Host\n 5t 3y  4n| 6t 7y  5n| 2t 3y  3n|H 14.24 If-Match\n12t 0y  1n|16t 2y  0n| 8t 0y  0n|H 14.25 If-Modified-Since\n 4t 2y  7n| 6t 6y  6n| 2t 2y  4n|H 14.26 If-None-Match\n 5t 1y  7n| 3t 6y  9n| 2t 1y  5n|H 14.27 If-Range\n 5t 2y  5n| 8t 6y  4n| 3t 2y  3n|H 14.28 If-Unmodified-Since\n 9t 0y  4n|13t 2y  3n| 5t 0y  3n|H 14.29 Last-Modified\n10t 1y  3n| 9t 5y  4n| 4t 1y  3n|H 14.30 Location\n 4t 2y  6n| 3t 2y 10n| 3t 2y  3n|H 14.31 Max-Forwards\n10t 2y  1n|12t 3y  3n| 7t 1y  0n|H 14.32 Pragma\n12t 1y  1n| 8t 0y  5n| 7t 0y  1n|H 14.33 Proxy-Authenticate\n12t 1y  1n| 8t 0y  5n| 7t 0y  1n|H 14.34 Proxy-Authorization\n 7t 2y  4n| 6t 6y  6n| 3t 2y  3n|H 14.35 Range\n 8t 1y  5n| 8t 3y  6n| 4t 0y  4n|H 14.36 Referer\n 3t 3y  8n| 4t 3y 11n| 2t 2y  4n|H 14.37 Retry-After\n 6t 2y  4n| 9t 6y  3n| 3t 2y  3n|H 14.38 Server\n 3t 3y  8n| 2t 2y 14n| 1t 1y  6n|H 14.39 TE\n 3t 2y  9n| 1t 2y 15n| 0t 2y  6n|H 14.40 Trailer\n11t 1y  2n|10t 6y  2n| 6t 1y  1n|H 14.41 Transfer-Encoding\n 2t 2y  9n| 2t 2y 14n| 2t 1y  5n|H 14.42 Upgrade\n12t 1y  1n|10t 2y  2n| 8t 0y  0n|H 14.43 User-Agent\n 5t 1y  7n| 7t 4y  7n| 4t 1y  3n|H 14.44 Vary\n 9t 0y  3n| 8t 0y  7n| 7t 0y  1n|H 14.45 Via\n 4t 3y  5n| 2t 3y 12n| 2t 3y  3n|H 14.46 Warning\n11t 1y  2n|14t 3y  1n| 7t 1y  0n|H 14.47 WWW-Authenticate\n13t 0y  1n|15t 2y  1n| 8t 0y  0n|A 2 Basic Authentication\n 2t 0y 12n| 4t 5y  9n| 1t 0y  7n|A 3.2.1 WWW-Authenticate Digest\n 1t 0y 13n| 2t 1y 15n| 0t 0y  8n|A 3.2.1 qop-options auth\n 1t 0y 13n| 1t 0y 17n| 0t 0y  8n|A 3.2.1 qop-options auth-int\n 2t 0y 12n| 4t 5y  9n| 1t 0y  7n|A 3.2.2 Authorization Digest\n 1t 0y 13n| 2t 1y 15n| 0t 0y  8n|A 3.2.2 request qop auth\n 1t 0y 13n| 1t 0y 17n| 0t 0y  8n|A 3.2.2 request qop auth-int\n 2t 0y 12n| 3t 2y 13n| 1t 0y  7n|A 3.2.3 Authentication-Info Digest\n 1t 0y 13n| 1t 2y 15n| 0t 0y  8n|A 3.2.3 response qop auth\n 1t 0y 13n| 1t 0y 17n| 0t 0y  8n|A 3.2.3 response qop auth-int\n11t 0y  3n| 7t 0y  7n| 6t 0y  2n|A 4.1 Proxy-Authenticate Basic\n 2t 0y 12n| 1t 1y 12n| 1t 0y  7n|A 4.2 Proxy-Authenticate Digest\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy qop-options auth\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy qop-options auth-int\n 2t 0y 12n| 1t 1y 12n| 1t 0y  7n|A 4.2 Proxy Auth Digest\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy request qop auth\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy request qop auth-int\n 1t 1y 12n| 1t 0y 13n| 1t 0y  7n|A 4.2 Proxy Auth-Info Digest\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy response qop auth\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy response qop auth-int\n\n\n\n", "id": "lists-012-6751967"}, {"subject": "Last Call: Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.1 to Draft Standar", "content": "The IESG has received a request from the HyperText Transfer Protocol\nWorking Group to consider publication of the following as Draft\nStandards:\n\n o Hypertext Transfer Protocol -- HTTP/1.1\n<draft-ietf-http-v11-spec-rev-05.txt>\n\n   This document replaces RFC2068, currently a Proposed Standard.\n\n\n o HTTP Authentication: Basic and Digest Access Authentication \n<draft-ietf-http-authentication-03.txt> \n\n   This document replaces RFC2069, currently a Proposed Standard.\n\n\nThe IESG plans to make a decision in the next few weeks, and solicits\nfinal comments on this action.  Please send any comments to the\niesg@ietf.org or ietf@ietf.org mailing lists by October 14, 1998.\n\nFiles can be obtained via:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-v11-spec-rev-05.txt\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-authentication-03.txt\n\nImplementation Report is included on the IETF Web Page:\n\nhttp://www.ietf.org/IESG/http1.1-implementations.txt\n\n\n\n", "id": "lists-012-6770007"}, {"subject": "Re: domain attribute in digest aut", "content": "> Can you give a pointer to these implementations ? Are these proxies\n> available for use by netizens ?\n\nI've just put up a new developement snapshot of my client at\nhttp://www.innovation.ch/java/HTTPClient/V0.4-dev/ . It's up to date with\nrespect to the current http and auth drafts.\n\nFor those who've never used it: the client itself is just a library, but\nthere is a simple test application at the above URL too.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-6778334"}, {"subject": "Re: Editorial: &quot;chunked&quot; to &quot;trailers&quot; in T", "content": ">It seems this one got overlooked in the switch to the \"trailers\" T-E in\n>draft-ietf-http-v11-spec-rev-05:\n>\n>  Section 14.40 TE:\n>\n>     A server MUST NOT include any header fields unless the \"chunked\"\n>     transfer-coding is present in the request as an accepted transfer-\n>     coding in the TE field.\n>\n>  Replace \"chunked\" by \"trailers\".\n\nActually, that sentence was supposed to be deleted -- it was intended\nto be replaced by the paragraph above it in rev-05.\n\n....Roy\n\n\n\n", "id": "lists-012-6785779"}, {"subject": "RE: Implementation report for HTTP/1.1 to Draft Standar", "content": "(The previous summary left out six reported implementations, please\nsubstitute this one)\n\nThis implementation report is for http-v11-spec-rev-05 &\nhttp-authentication-03 to progress to Draft Standard.\n\nThis report is abbreviated; see\n    http://www.w3.org/Protocols/HTTP/Forum/Reports/\nfor more explanation and individual implementation reports. Many\nthanks to the contributors who took the time to report on their\nsystems, and to Jim Gettys for accumulating the results and\nencouraging sufficient testing and reporting.\n\nWe have surveyed implementations for whether each feature in these\nspecifications have been tested against independent interoperable\nimplementations. In the survey, nearly all features have had more than\ntwo clients, servers, and proxies tested. Since the survey was\nperformed, almost all of the _remaining_ features have been tested,\nand we feel confident in bringing forward the specifications as Draft\nStandard at this point.\n\nWe have implementation & testing reports from 26 implementations:\n\n* libwww-perl NG-alpha-0.11 from Aas Software: a Perl HTTP\n  client library (Gisle Aas)\n* HTTPClient V0.4-dev from Ronald Tschalaer: a client library in Java\n* libwww 5.1k from the World Wide Web Consortium: a client library in C,\n  with various tools built on top (Henrik Frystyk Nieslen)\n* Internet Explorer 4 from Microsoft Corporation: a full-feature web\n  browser (Yaron Goland)\n* Netscape Navagator and Communicator 5 from Netscape Communication\n  Corporation (Gagan Saksena): a full-feature web browser and editor\n* Netscape Enterprise Server 3.51 from Netscape Communication\n  Corporation: a HTTP/1.1 server (Mike Belshe)\n* Apache 1.3b6 from the Apache Group: a HTTP/1.1 Server and incomplete\n  caching proxy (Roy Fielding)\n* DMKHTD 1.06f from Dave Kristol, Bell Labs: a HTTP/1.1 Server\n* Microsoft IIS 4.0 from Microsoft Corporation: a HTTP/1.1 Server\n  (Henry Sanders)\n* WN 2.0.0 from John Franks, Northwestern University Math Department:\n  a HTTP/1.1 Server\n* SRE-http 1.3a from Daniel Hellerstein, USDA: a HTTP/1.1 Server  \n* HASS 1.00d.a from Applied Theory Communications: a Application\n  Server Suite (Patrick McManus)\n* Microsoft Proxy Server 2.0 from Microsoft Corporation a HTTP/1.1\n  Caching Proxy server (Lester Waters)\n* CL-HTTP 67.47 from John Mallery, MIT AI Lab: a combined sever and\n  caching proxy, including a client and Web Walker application\n* Jigsaw 2.0beta from the World Wide Web Consortium, a combined server\n  and caching proxy (Yves Lafon)\n* Raptor Firewall 5.1 from Axent Technologies, a HTTP/1.1 firewall\n  Proxy (no caching) (Robert Polansky)\n* GiambyNetGrabber 0.65 from GiambiSoft: a client Internet mirroring\n  tool (Giambattista Bloisi)\n* Millicent Proxy 1.0 (3 separate implementations) from Digital\n  Equipment Corporation: the MilliCent microcommerce system, including\n  Server, Proxy and Gateway (reverse proxy) (Steve Glassman).  The\n  separate implementations were by different people at different\n  geographical locations in different programming languages.\n* EmWeb R3_04 from Agranat Systems: HTTP/1.1 server for embedded use\n  (Scott Lawrence)\n* WebSTAR 3.0 from StarNine Technologies, Inc.: MacOS Web server (Eric\n  Zelenka)\n* Spyglass MicroServer 2.0 from Spyglass, Inc.: Small footprint\n  HTTP/1.1 Server (Steve Wingard)\n* Java Web Server 1.1.1 from Sun Microsystems: Full Featured HTTP/1.1\n  Server (Rob Clark)\n* IBM Web Traffic Express 1.1 from IBM: Caching Proxy Server (Richard\n  Gray)\n* Traffic Server 1.0 from Inktomi Corporation: High-Performance Proxy\n  Server (Dr. Brian Totty)\n\nTable of Results:\n\nEach implementation was surved to review the http-spec-v11-rev\ndocument and the http-authentication document to review each section\nfor whether all of the features in that section were implemented and\ntested for interoperability of that feature against a separate\nindependent implementation. The codes below indicate different\nresponses:\n\n't'   tested against another independent implementation\n'y'   implemented but not tested against independent implementation\n'n'   not implementated\n'-'   not applicable to this type of implementation\n\nClients   |Servers   | Proxies  |Feature\n11t 0y  3n|13t 1y  4n| 5t 0y  3n|H 8.1 Persistent Connections\n 7t 0y  5n| 3t 1y  4n| 3t 0y  4n|H 8.2.3 Automatic retrying\n 5t 3y  5n| 5t 6y  7n| 3t 2y  3n|H 8.2.4 100 (Continue) status\n 5t 1y  7n| 7t 3y  8n| 3t 1y  4n|H 9.2 OPTIONS\n14t 0y  0n|17t 1y  0n| 8t 0y  0n|H 9.3 GET\n13t 0y  1n|17t 1y  0n| 8t 0y  0n|H 9.4 HEAD\n13t 0y  0n|16t 2y  0n| 8t 0y  0n|H 9.5 POST\n 8t 1y  4n| 9t 3y  6n| 4t 0y  4n|H 9.6 PUT\n 5t 2y  6n| 5t 4y  9n| 2t 1y  5n|H 9.7 DELETE\n 5t 2y  5n| 6t 6y  5n| 2t 2y  4n|H 9.8 TRACE\n 6t 3y  4n| 4t 3y  7n| 3t 3y  2n|H 9.9 CONNECT\n 8t 2y  3n| 6t 7y  5n| 3t 2y  3n|H 10.1.1 100 Continue\n 2t 2y  9n| 2t 3y 13n| 2t 1y  5n|H 10.1.2 101 Switching Protocols\n14t 0y  0n|17t 1y  0n| 8t 0y  0n|H 10.2.1 200 OK\n 5t 2y  5n| 5t 4y  8n| 2t 1y  4n|H 10.2.2 201 Created\n 4t 1y  7n| 5t 1y 12n| 2t 0y  6n|H 10.2.3 202 Accepted\n 3t 1y  8n| 4t 1y 13n| 2t 0y  6n|H 10.2.4 203 Non-Authoritative Info\n 6t 3y  5n| 7t 2y  9n| 3t 1y  4n|H 10.2.5 204 No Content\n 3t 1y  8n| 4t 0y 14n| 2t 0y  6n|H 10.2.6 205 Reset Content\n 7t 1y  5n| 6t 5y  7n| 3t 1y  4n|H 10.2.7 206 Partial Content\n 4t 2y  7n| 4t 3y 11n| 2t 1y  5n|H 10.3.1 300 Multiple Choices\n11t 2y  1n|13t 2y  3n| 7t 0y  1n|H 10.3.2 301 Moved Permanently\n 7t 2y  5n|10t 3y  5n| 4t 0y  4n|H 10.3.3 302 Found\n 5t 4y  5n| 4t 3y 11n| 2t 2y  4n|H 10.3.4 303 See Other\n11t 1y  1n|16t 1y  1n| 8t 0y  0n|H 10.3.5 304 Not Modified\n 3t 5y  6n| 2t 3y 12n| 2t 2y  4n|H 10.3.6 305 Use Proxy\n 7t 5y  2n| 7t 1y 10n| 5t 1y  2n|H 10.3.7 307 Temporary Redirect\n12t 0y  1n|12t 5y  1n| 7t 0y  1n|H 10.4.1 400 Bad Request\n12t 2y  0n|16t 2y  0n| 8t 0y  0n|H 10.4.2 401 Unauthorized\n 3t 6y  3n| 3t 5y 10n| 3t 4y  1n|H 10.4.3 402 Payment Required\n10t 2y  1n|12t 5y  1n| 7t 1y  0n|H 10.4.4 403 Forbidden\n12t 0y  1n|15t 3y  0n| 8t 0y  0n|H 10.4.5 404 Not Found\n 7t 2y  4n| 7t 6y  5n| 4t 1y  3n|H 10.4.6 405 Method Not Allowed\n 6t 3y  4n| 5t 4y  9n| 3t 2y  3n|H 10.4.7 406 Not Acceptable\n11t 2y  1n| 8t 0y  7n| 7t 0y  1n|H 10.4.8 407 Proxy Auth Required\n 4t 3y  7n| 4t 3y 10n| 2t 0y  6n|H 10.4.9 408 Request Timeout\n 4t 3y  6n| 3t 4y 11n| 2t 1y  5n|H 10.4.10 409 Conflict\n 4t 2y  7n| 4t 0y 14n| 2t 0y  6n|H 10.4.11 410 Gone\n 4t 5y  5n| 4t 5y  9n| 2t 2y  4n|H 10.4.12 411 Length Required\n 4t 4y  5n| 5t 6y  7n| 2t 2y  4n|H 10.4.13 412 Precondition Failed\n 3t 4y  6n| 4t 2y 12n| 2t 1y  5n|H 10.4.14 413 Req Entity Too Large\n 5t 2y  6n| 3t 2y 13n| 2t 1y  5n|H 10.4.15 414 Request-URI Too Long\n 5t 2y  6n| 4t 2y 12n| 3t 0y  5n|H 10.4.16 415 Unsupported Media Type\n 4t 4y  5n| 3t 5y 10n| 3t 1y  4n|H 10.4.17 416 range not satisfiable\n 2t 4y  7n| 4t 3y 11n| 2t 1y  5n|H 10.4.18 417 Expectation|Failed\n 6t 2y  4n| 7t 7y  4n| 4t 1y  3n|H 10.5.1 500 Internal Server Error\n 6t 2y  4n| 7t 8y  3n| 4t 1y  3n|H 10.5.2 501 Not Implemented\n 4t 2y  6n| 3t 2y 10n| 2t 1y  5n|H 10.5.3 502 Bad Gateway\n 6t 3y  4n| 5t 3y 10n| 3t 2y  3n|H 10.5.4 503 Service Unavailable\n 5t 3y  4n| 4t 4y  7n| 3t 2y  3n|H 10.5.5 504 Gateway Timeout\n 4t 2y  6n| 4t 6y  8n| 2t 1y  5n|H 10.5.6 505 Version Not Supported\n 4t 4y  5n| 6t 6y  6n| 2t 2y  4n|H 13.3.3 Strong entity tags\n 1t 5y  7n| 3t 5y 10n| 1t 3y  4n|H 13.3.3 Weak entity tags\n12t 0y  1n|13t 2y  3n| 8t 0y  0n|H 14.1 Accept\n 8t 1y  4n| 9t 3y  6n| 5t 0y  3n|H 14.2 Accept-Charset\n 9t 1y  4n| 6t 6y  6n| 4t 1y  3n|H 14.3 Accept-Encoding\n 7t 2y  4n|10t 3y  5n| 4t 1y  3n|H 14.4 Accept-Language\n 6t 2y  5n| 6t 6y  6n| 2t 2y  4n|H 14.5 Accept-Ranges\n 7t 1y  5n| 5t 2y  4n| 4t 1y  3n|H 14.6 Age\n 3t 3y  6n| 7t 8y  3n| 2t 3y  3n|H 14.7 Allow\n12t 0y  2n|12t 5y  1n| 7t 0y  1n|H 14.8 Authorization\n10t 2y  1n|12t 5y  1n| 7t 1y  0n|H 14.9 Cache-Control\n14t 0y  0n|13t 4y  1n| 8t 0y  0n|H 14.10 Connection\n11t 2y  1n|10t 6y  2n| 6t 1y  1n|H 14.11 Content-Encoding\n 5t 3y  5n| 7t 6y  5n| 3t 2y  3n|H 14.12 Content-Language\n13t 0y  1n|14t 3y  1n| 8t 0y  0n|H 14.13 Content-Length\n 8t 1y  4n| 5t 5y  8n| 4t 1y  3n|H 14.14 Content-Location\n 3t 1y 10n| 4t 5y  9n| 2t 1y  5n|H 14.15 Content-MD5\n 8t 1y  4n| 6t 7y  5n| 4t 1y  3n|H 14.16 Content-Range\n12t 0y  1n|15t 3y  0n| 8t 0y  0n|H 14.17 Content-Type\n12t 0y  1n|15t 3y  0n| 8t 0y  0n|H 14.18 Date\n 6t 3y  4n| 7t 7y  4n| 3t 2y  3n|H 14.19 ETag\n 4t 1y  9n| 5t 4y  9n| 2t 1y  5n|H 14.20 Expect\n 8t 0y  5n|10t 4y  4n| 5t 0y  3n|H 14.21 Expires\n 6t 3y  4n| 4t 4y  8n| 2t 3y  3n|H 14.22 From\n14t 0y  0n|15t 3y  0n| 8t 0y  0n|H 14.23 Host\n 5t 3y  4n| 6t 7y  5n| 2t 3y  3n|H 14.24 If-Match\n12t 0y  1n|16t 2y  0n| 8t 0y  0n|H 14.25 If-Modified-Since\n 4t 2y  7n| 6t 6y  6n| 2t 2y  4n|H 14.26 If-None-Match\n 5t 1y  7n| 3t 6y  9n| 2t 1y  5n|H 14.27 If-Range\n 5t 2y  5n| 8t 6y  4n| 3t 2y  3n|H 14.28 If-Unmodified-Since\n 9t 0y  4n|13t 2y  3n| 5t 0y  3n|H 14.29 Last-Modified\n10t 1y  3n| 9t 5y  4n| 4t 1y  3n|H 14.30 Location\n 4t 2y  6n| 3t 2y 10n| 3t 2y  3n|H 14.31 Max-Forwards\n10t 2y  1n|12t 3y  3n| 7t 1y  0n|H 14.32 Pragma\n12t 1y  1n| 8t 0y  5n| 7t 0y  1n|H 14.33 Proxy-Authenticate\n12t 1y  1n| 8t 0y  5n| 7t 0y  1n|H 14.34 Proxy-Authorization\n 7t 2y  4n| 6t 6y  6n| 3t 2y  3n|H 14.35 Range\n 8t 1y  5n| 8t 3y  6n| 4t 0y  4n|H 14.36 Referer\n 3t 3y  8n| 4t 3y 11n| 2t 2y  4n|H 14.37 Retry-After\n 6t 2y  4n| 9t 6y  3n| 3t 2y  3n|H 14.38 Server\n 3t 3y  8n| 2t 2y 14n| 1t 1y  6n|H 14.39 TE\n 3t 2y  9n| 1t 2y 15n| 0t 2y  6n|H 14.40 Trailer\n11t 1y  2n|10t 6y  2n| 6t 1y  1n|H 14.41 Transfer-Encoding\n 2t 2y  9n| 2t 2y 14n| 2t 1y  5n|H 14.42 Upgrade\n12t 1y  1n|10t 2y  2n| 8t 0y  0n|H 14.43 User-Agent\n 5t 1y  7n| 7t 4y  7n| 4t 1y  3n|H 14.44 Vary\n 9t 0y  3n| 8t 0y  7n| 7t 0y  1n|H 14.45 Via\n 4t 3y  5n| 2t 3y 12n| 2t 3y  3n|H 14.46 Warning\n11t 1y  2n|14t 3y  1n| 7t 1y  0n|H 14.47 WWW-Authenticate\n13t 0y  1n|15t 2y  1n| 8t 0y  0n|A 2 Basic Authentication\n 2t 0y 12n| 4t 5y  9n| 1t 0y  7n|A 3.2.1 WWW-Authenticate Digest\n 1t 0y 13n| 2t 1y 15n| 0t 0y  8n|A 3.2.1 qop-options auth\n 1t 0y 13n| 1t 0y 17n| 0t 0y  8n|A 3.2.1 qop-options auth-int\n 2t 0y 12n| 4t 5y  9n| 1t 0y  7n|A 3.2.2 Authorization Digest\n 1t 0y 13n| 2t 1y 15n| 0t 0y  8n|A 3.2.2 request qop auth\n 1t 0y 13n| 1t 0y 17n| 0t 0y  8n|A 3.2.2 request qop auth-int\n 2t 0y 12n| 3t 2y 13n| 1t 0y  7n|A 3.2.3 Authentication-Info Digest\n 1t 0y 13n| 1t 2y 15n| 0t 0y  8n|A 3.2.3 response qop auth\n 1t 0y 13n| 1t 0y 17n| 0t 0y  8n|A 3.2.3 response qop auth-int\n11t 0y  3n| 7t 0y  7n| 6t 0y  2n|A 4.1 Proxy-Authenticate Basic\n 2t 0y 12n| 1t 1y 12n| 1t 0y  7n|A 4.2 Proxy-Authenticate Digest\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy qop-options auth\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy qop-options auth-int\n 2t 0y 12n| 1t 1y 12n| 1t 0y  7n|A 4.2 Proxy Auth Digest\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy request qop auth\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy request qop auth-int\n 1t 1y 12n| 1t 0y 13n| 1t 0y  7n|A 4.2 Proxy Auth-Info Digest\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy response qop auth\n 0t 1y 13n| 0t 0y 14n| 0t 0y  8n|A 4.2 Proxy response qop auth-int\n\n\n\n", "id": "lists-012-6793726"}, {"subject": "Re: nonascii user name &amp; passwor", "content": "On Thu, 24 Sep 1998, Roy T. Fielding wrote:\n> That would invalidate almost all client implementations of HTTP.\n> There is no technical reason to define the encoding other than to\n> say it is a shared understanding between client and server that\n> is outside the capacity of the protocol to determine, and that\n> interoperability problems may occur if non-US-ASCII characters\n> are used.  Forbidding it just makes the specification worthless.\n\nI emphatically disagree.\n\nWith your proposal, vendor A could build a compliant HTTP client which\nonly uses ISO-8859-1 for passwords and vendor B could build a compliant\nHTTP client which only uses UTF-8 for passwords.  Client A and Client B\ndon't interoperate if the password contains non-ASCII characters. \nTherefore the spec would fail the interoperability test and is certainly\nnot eligable for draft standard status (and probably not even proposed\nstandard status).\n\nForbidding this situation is necessary to make sure all compliant clients\ninteroperate.\n\n- Chris\n\nP.S. Will the average user realize he has to manually configure the\n\"private agreement password charset\" in his browser before he can\nauthenticate if he uses non-ASCII characters?\n\n\n\n", "id": "lists-012-6812256"}, {"subject": "Re: nonascii user name &amp; passwor", "content": "On Mon, 21 Sep 1998, Larry Masinter wrote:\n> The problem is that UTF-8 doesn't quite have a well-defined\n> 'canonical' form yet, either, although one is being developed, the\n> canonicalization algorithm won't be at \"draft standard\". So you might\n> have two browsers that would enter the same user name with different\n> UTF-8 encodings, too.\n\nGood point.  In fact, the UTF-8 spec itself is still proposed, so the HTTP\nspec can't reference it normatively.\n\n> And we're not normally requiring clients to implement UTF-8\n> transformations of user type-in at all so this will be a big problem.\n\nFair enough.\n\n> On the other hand, it seems inappropriate to restrict user *names* to\n> US-ASCII. I wonder if we could change the BNF and description text\n> from \"user name\" and \"username\" to \"user id\", even if we leave\n> \n>     username         = \"username\" \"=\" user-id\n\nHow about we restrict user-ids and typed-in passwords to US-ASCII for now,\ndeclare encoding of non-ASCII characters in those fields undefined but\nexplicitly forbid use of localized charsets (e.g., ISO-8859-1)?  Then we\ncan amend it to use UTF-8 later with a spec that progresses separately\non the standards track.\n\n- Chris\n\n\n\n", "id": "lists-012-6821349"}, {"subject": "Re: nonascii user name &amp; passwor", "content": "On Thu, 24 Sep 1998, Roy T. Fielding wrote:\n> No, both would use octets for passwords and if either one does encoding\n> translation then they may or may not interoperate, depending on how\n> the user created the password in the first place.\n\nThe specific case is client A always encodes in 8859-1 and client B always\nencodes in UTF-8.  A password with non-ASCII characters that works in A\nwon't work in B and vice versa.  So both A and B are compliant but they\ndon't interoperate.\n\n> >Forbidding this situation is necessary to make sure all compliant clients\n> >interoperate.\n> \n> If that were true, they wouldn't interoperate now.  The fact is that\n> everyone either uses ASCII passwords or continues to use the\n> same charset for password entry that they used for password creation,\n> which is not surprising.  None of the servers care about the encoding\n> of the password characters.  None of the clients do encoding translation.\n> That is why it works, even if it is sub-optimal.\n\nServers (excluding account management tools) shouldn't care about the\nencoding of password characters.  That's why *OCTET is the right formal\nsyntax.  But the mapping from typed characters to *OCTET does matter.  If\nit differs between any two clients (or a client and a password\nadministrative tool), there is an interoperability problem today.  The\nvery name \"password\" implies that it's usually textual (and thus made up\nof characters) so a charset is needed.\n\n> >P.S. Will the average user realize he has to manually configure the\n> >\"private agreement password charset\" in his browser before he can\n> >authenticate if he uses non-ASCII characters?\n> \n> It is, by its very nature, the default.\n\nPerhaps it is more often than not since users tend to prefer clients using\nlocalized character sets.  But it's far from guaranteed.\n\n> How do you think the average\n> user will feel about all of his current password-enabled services being\n> broken just to support a potential mismatch between system charsets?\n\nPrograms that rely on private agreements to interoperate deserve to break\n(and occasionally do break in practice).\n\nI know in email protocols the IETF has held a hard line and never\npermitted unlabeled 8-bit text in a standard.  Is there something about\nhttp that justifies breaking this precedent?  What do other people think? \nIs this a case where correct international interoperability has to be\nsacrificed due to the localized private-agreement installed base?\n\nI'm thinking of writing an RFC on i18n of usernames and passwords in\ngeneral and appreciate debates of the issues.\n\n- Chris\n\n\n\n", "id": "lists-012-6829962"}, {"subject": "Re: (IPng 6517) Packet Loss (was Re:  Re: Host Fragmentatio", "content": "In a message dated 9/24/98 10:17:22 AM Eastern Daylight Time, kasten@argon.com\nwrites:\n\n >At 08:28 AM 9/24/98 EDT, Volsinians@aol.com wrote:\n\n >>If someones provide some real data on today's network and reasoning\n >>that does not depend on the assumption of old-fashioned, poorly\n >>designed or misdesigned hardware and very old very buggy software,\n >>there would be some reason to pay attention to his analysis.  Paying\n >>attention to the <A HREF=\"http://members.aol.com/Telford001/#ANALYZE\">Mogul\npaper</A> \n >>to design the protocol of the future is\n >>ludicrous.  \n\n >Packet loss in today's network is a not insignificant problem.\n \n >About a year ago I was doing some web browsing from home\n >looking at some pages that had several moderate to large\n >pictures on them. While I have a 28.8 modem, I was getting\n >only a couple hundred bytes/second aggregate goodput\n >downloading the pages, much less than the 2-3KB\n >I should see (doing simple arithmetic). At first I\n >thought it was just some odd property of the compression\n >in the modem. Soon I notived that the aggregate goodput\n >was much higher for pages with only one or two pictures\n >than, say, 5 or 6. I turned on a packet tracer, downloaded\n >a page with several pictures and discovered some truly odd\n >packet reception patterns. Lots of retransmissions, occasionally\n >I'd get the same packet (by TCP sequence number) several times\n >back to back, etc, etc. \n\n >To make a long story short, the problem was that I had multiple TCP\n >connections open (one per image) and they all were imploding onto the\n >same dialup server at my ISP. The server's queue was filling and\n >packets were being discarded left and right. Basically I was getting\n >randomly selected packets from the stream. The server TCPs were not\n >behaving well -- either they weren't doing good congestion avoidance\n >and backoff, or the traffic loss was so variable that the server TCP\n >just couldn't make sense out of the acks it was getting from me. (once\n >I saw what was happening, I reduced the number of simultaneous\n >connections to 2 and goodput went up to about 2.5KB).\n\n >So, if these packets were IP fragments rather than individual TCP\n >segments, the odds would be good that I would have received 0 complete\n >TCP segments.\n\nWhenever something is retransmitted on the internet, users assume that\nthe retransmission results from a lost packet. Under the assumption\nthat all delays are a result of packet loss, a large file could never\nbe sent across the network in less then a day:->\n\nThis example makes the argument for HTTP 1.1, not for path MTU\ndiscovery. I believe that a large piece of the problem seen in this\nexample is related to the queuing delay from the network to PC over\nthe 28k phone line. The observed problem is a result of limitation of\nthe the TCP congestion control algorithm.  This algorithm cannot deal\neffectively with multiple concurrent streams from or to a single host,\nfor it is only connection oriented and not host oriented.\n\nAs a result each connection queues at least one packet to the slow\ninterface.  Then, while the packets are waiting to be sent serially\nacross the 28K connection, the TCP retransmit timer fires and the\npacket is sent again. It is possible that reducing the size of the\npackets by using only IP fragmentation instead of including the extra\nTCP headers might have made getting 3 streams possible rather than\nonly 2. The observed phenomenon was certainly not a result of lost\npackets.\n\nThis example underscores why \n<A HREF=\"http://members.aol.com/Telford001/#SERVICES\">The Critical Review of\nInternet Technology and Intranet Computing (The\nC.R.I.T.I.C)</A>\nis useful and worthwhile for network administrators and designers as\nwell as for packet switch implementers, for it provides analysis and\ntools to help understand this sort of situation.\n\nJoachim Martillo\n<A HREF=\"http://members.aol.com/Telford001/\">Telford Tools, Inc.</A> \n\n\n\n", "id": "lists-012-6840530"}, {"subject": "Re: nonascii user name &amp; passwor", "content": "On Fri, 25 Sep 1998, Roy T. Fielding wrote:\n> Yes, they do. That doesn't change the definition of the protocol.  \n> The username and password were defined as ISO-8859-1 when the \n> authentication fields were invented and deployed.  Except for the\n> usual charset politics, that definition worked just fine.\n\nIn RFC 2068, username and password are defined as TEXT, which may be\neither ISO-8859-1 or something encoded according to RFC 1522 (the old\nversion of RFC 2047).  I suspect it's quite clear that nothing other than\nISO-8859-1 is going to work at all reliably in this context.  Does anyone\nactually implement RFC 2047 in this context?\n\nRFC 2069 is a bit different, as the username appears in a quoted-string\nwhich therefore forbids the use of RFC 2047.  So RFC 2069 requires\nISO-8859-1 for usernames.\n\nNow this may work just fine for western Europe and America, but it is not\ninternational, so it is broken.  When something is broken in a protocol,\nit should be fixed.  Then one has to choose whether compatibility must be\nretained.\n\nI suspect that compatibiliy with RFC 2047 encoding in usernames and\npasswords is not worth retaining, as it was probably never signficantly\ndeployed and even if it was, it probably doesn't interoperate.  RFC 2047\nis fine for most headers, but it was never designed for something which\nrequires a canonical form.  I have no opinion on whether compatibility\nwith ISO 8859-1 compatibility should be retained in this context, as I'm\nnot aware of the deployment patterns of that use -- that's a judgement\ncall for this working group. \n\nIf you want to make this international and retain ISO 8859-1\ncompatibility, then the right thing to do is use UTF-8 encoding, unless\nthe entire string is made up of 8859-1 characters in which case 8859-1\nencoding is used instead.  Now since a draft standard can't reference\nUTF-8, you'd want to leave the encoding for non 8859-1 characters\nundefined for now, and define it in an extension, but it'd probably be\nworth forbidding all encodings other than 8859-1 unless specified in a\nstandards track document -- that would reduce the problem. \n\nIf you don't care about retaining compatibility with ISO 8859-1 use and\nwant to make it international, then declare it US-ASCII for now and write\nan extension to make them UTF-8.\n\nThe username parameter in digest auth is stuck at ISO 8859-1 by reference,\nbut an encoding for UTF-8 could be added by an extension (e.g., RFC 2231\nencoding).\n\n> In email protocols, specifications that contrast with reality have\n> traditionally been ignored by almost all developers and resulted in\n> interoperability failures when some poor sap actually attempted to comply\n> with the RFC.\n\nI disagree.  Certainly the use of private agreement charsets is popular in\nemail as it was the only localization solution avaiable prior to MIME and\nit was widely deployed.\n\n> HTTP does not allow that.\n\nHTTP has no more power to enforce the specification than email does.\nI will admit that an interactive protocol is easier to extend and upgrade\nthan a store-and-forward protocol.\n\n>  HTTP has a version number \n> whose minor number is supposed to change whenever compatible changes\n> are introduced, and a major number that is supposed to change whenever\n> incompatible changes are introduced.\n\nA new port number provides equivalent functionality to a major version\nnumber.  Feature announcement provides superior functionality to minor\nversion numbers.  SMTP has been extensively modified without the need for\nversion numbers.\n\n> I have no problem with defining a new protocol in the HTTP family that\n> cures the hundred-odd problems leftover from the installed base and\n> eventually progresses on the standards track.  I have a huge problem\n> with such a protocol masquerading as HTTP/1.x when we have carefully\n> designed the protocol for forward compatibility.\n\nIf you want 100% compatibility with the interoperable portions of the\ninstalled base, that's fine.  But where the spec doesn't interoperate, it\nshould be fixed.  I suspect it doesn't interoperate for non-8859-1\ncharacters in usernames and passwords. \n\n> The problem is that\n> the IETF standards-track process interferes with good protocol design\n> by not allowing progress along delineated branches.\n\nQuite the contrary.  Version numbers prevent the development of branches.\nFeature announcement as ESMTP and IMAP use has been repeatedly successful\nin allowing multiple branches to develop simultaneously.\n\n> It is high time that the IETF started thinking in terms of protocol\n> families\n\nI see no problems in this area.  Extensions to standard protocols are\nflourishing in the IETF.\n\n> and planning for evolution rather than making standards\n> decrees and hoping the installed base gets sucked into the void.\n\nSometimes it is better to evolve.  Sometimes it is better to start over\nand create an incompatible version.  Sometimes the installed base sort of\nworks but doesn't really interoperate and is best ignored when creating a\nfully interoperable solution.  Which choice is better is an engineering\ndecision which needs to be evaluated carefully.\n\n- Chris\n\n\n\n", "id": "lists-012-6853443"}, {"subject": "Microsoft prox", "content": "I got this from wget maintainer. As far as I know, proxies aren't\nsupposed to append anything to the requested resource.\nHowever, I've been off-line for more than a year and I haven't\ndigested the most recent HTTP spec. So, could someone tell\nme if this is legal?\n\n----- Forwarded message from Hrvoje Niksic <hniksic@srce.hr> -----\n\nSender: hniksic@public.srce.hr\nTo: Drazen Kacar <dave@fly.cc.fer.hr>\nSubject: [Simon Munton <simonm@m4data.co.uk>] RE: wget 1.5.3 on Windows 98/NT, using MSVC 6\nX-Attribution: Hrvoje\nFrom: Hrvoje Niksic <hniksic@srce.hr>\nDate: 25 Sep 1998 13:58:06 +0200\nMessage-ID: <kigiuicz9o1.fsf@jagor.srce.hr>\nUser-Agent: Gnus/5.070033 (Pterodactyl Gnus v0.33) XEmacs/21.0 (Danish Landrace)\n\n------- Start of forwarded message -------\nMessage-ID: <c=GB%a=_%p=M4DATA%l=M4EXCH-980925091231Z-6978@m4exch.m4data.co.uk>\nFrom: Simon Munton <simonm@m4data.co.uk>\nTo: \"'Hrvoje Niksic'\" <hniksic@srce.hr>\nSubject: RE: wget 1.5.3 on Windows 98/NT, using MSVC 6\nDate: Fri, 25 Sep 1998 10:12:31 +0100\nMIME-Version: 1.0\nContent-Type: text/plain; charset=\"us-ascii\"\nContent-Transfer-Encoding: 7bit\n\nWhat seems to happen is this: part way through the download, there is an\nerror,\neg timeout, and it seems that in addition MS proxy server sends a text\nstring that gets put\non the end of the downloaded file. Something like:\n \"HTTP Proxy error: Connection timed out\".\n\nWhen wget retries, this error message gets left embedded in the\ndownloaded file.\nSo the final file looks like:\n\n<-------Good Data--------><---HTTP Proxy error message----><-------Good\ndata---->\n\nHope this makes sense. If you'd like me to try and find out more\nspecific details\nof what's going on, let me know. \n\n===========================================================\nSimon Muntonsimonm@m4data.co.uk\nM4 Data LtdTel: 44-1749-679222\nMendip court, Bath Rd, WellsFax: 44-1749-673928\nSomerset, BA5 3DG, England\n\n>\n>> I also use wget through Microsoft Proxy server, which has a habit of\n>> sending error messages which get appended to the files being\n>> downloaded, and when wget retries getting the file, the downloaded\n>> file ends up with these messages in the middle.  I added a couple of\n>> changes to ftp.c and http.c (diffs below), so that they 'back up' by\n>> 2048 bytes when retrying, thereby getting rid of the error message\n>> embedded in the file.  (2048 seems to work for me).\n>\n>I don't understand this part.  How does the server send these error\n>messages, and exactly how do they get \"appended to the files\"?\n>\n>-- \n>Hrvoje Niksic <hniksic@srce.hr> | Student at FER Zagreb, Croatia\n>--------------------------------+--------------------------------\n>Mix 2 table spoons sugar with 1 spoon salt.  Put it in a bottle\n>and stick a fuse into it.  Say \"Shit!\" when it doesn't detonate.\n>\n\n------- End of forwarded message -------\n\n----- End forwarded message -----\n\n-- \n .-.   .-.    Life is a sexually transmitted disease.\n(_  \\ /  _)\n     |        dave@fly.cc.fer.hr\n     |\n\n\n\n", "id": "lists-012-6866227"}, {"subject": "Re: Microsoft prox", "content": "From your description, it does sound like a bug in their proxy.\n - Jim\n\n\n\n", "id": "lists-012-6878659"}, {"subject": "Re: Microsoft prox", "content": "On Thu, 1 Oct 1998, Jim Gettys wrote:\n\n> >From your description, it does sound like a bug in their proxy.\n>  - Jim\n> \n> \n\nOn the other hand, the proxy detected an error and attempted to report \nit.  The question to me would hinge on whether the proxy then did\na connection reset or normal close. \n\nI don't believe that HTTP is well architected for recovering \npartial transfers or in some cases even detecting incomplete transfers\nexcept those initiated by the client. Without architected error recovery,\nits all up to the implementation.\n\nDave Morris\n\n\n\n", "id": "lists-012-6885707"}, {"subject": "Re: Microsoft prox", "content": "\"David W. Morris\" <dwm@XPASC.COM> wrote:\n  > On Thu, 1 Oct 1998, Jim Gettys wrote:\n  > \n  > > >From your description, it does sound like a bug in their proxy.\n  > >  - Jim\n  > > \n  > > \n  > \n  > On the other hand, the proxy detected an error and attempted to report \n  > it.  The question to me would hinge on whether the proxy then did\n  > a connection reset or normal close. \n  > \n  > I don't believe that HTTP is well architected for recovering \n  > partial transfers or in some cases even detecting incomplete transfers\n  > except those initiated by the client. Without architected error recovery,\n  > its all up to the implementation.\n\nI agree.  Consider the plight of an HTTP server running a gateway.\nSuppose the gateway (CGI) produces some of its output but doesn't\nfinish in a reasonable amount of time.  How can the server report an\nerror once it has begun its response?  I suspect that's more or less\nwhat happened to the MS proxy.\n\nDave Kristol\n\n\n\n", "id": "lists-012-6893299"}, {"subject": "Re: Microsoft prox", "content": ">On the other hand, the proxy detected an error and attempted to report \n>it.  The question to me would hinge on whether the proxy then did\n>a connection reset or normal close. \n\nThe proxy is broken.  The only thing it needs to do to \"report\"\na dropped connection is to close its own connection with the client.\nThat is why we have both content-length and the chunked encoding.\n\n>I don't believe that HTTP is well architected for recovering \n>partial transfers or in some cases even detecting incomplete transfers\n>except those initiated by the client. Without architected error recovery,\n>its all up to the implementation.\n\nIt is all up to the implementation in any case.  Partial transfers are\neasily and completely recoverable for HTTP/1.1.  The only thing it is\nmissing is some indication of the reason why the connection was dropped,\nbut that is hard to do without multiplexing or a separate control channel.\nThat was wisely left for a major protocol revision.\n\n....Roy\n\n\n\n", "id": "lists-012-6901533"}, {"subject": "domain attribute in digest auth/efficienc", "content": "> -----Original Message-----\n> From: Ronald.Tschalaer@psi.ch [mailto:Ronald.Tschalaer@psi.ch]\n> Sent: Thursday, October 01, 1998 12:45 AM\n> To: Paul Leach; HTTP-WG@hplb.hpl.hp.com\n> Subject: Re: domain attribute in digest auth\n> \n> \n> \n> > The second change you propose is incompatible with RFC \n> 2069, for which\n> > implementations exist. Furthermore, it reduces efficiency. \n> For Basic, it had\n> \n> I guess we disagree on the efficency issue (I know for the \n> apache module\n> I've been working on this'll mean usually sending extra bytes over the\n> wire at the same number of RTs).\n\nWhy? I think the typical case will be no domain= at all, and the protection\ndomain is the whole server, regardless of the initial URL that you fetched\nfrom the server.\n\n\n\n", "id": "lists-012-6909441"}, {"subject": "RE: domain attribute in digest aut", "content": "> -----Original Message-----\n> From: Ronald.Tschalaer@psi.ch [mailto:Ronald.Tschalaer@psi.ch]\n> Sent: Thursday, October 01, 1998 12:45 AM\n> \n> [snip]\n> > The first change is backwards compatible, so could probably \n> be made at this\n> > point if there were  concensus. I actually think that one \n> could say that\n> > it's safe to consider all proxies in the same protection \n> space, regardless\n> > of what \"domain\" says. One shouldn't configure one's \n> browser to point at\n> > proxies to which one wouldn't be willing to send a Digest \n> response. AS a\n> > result, one could almost consider this an implementation \n> issue: clients that\n> > want to pre-authentication to all proxies should just do so.\n> \n> The problem with considering all proxies in the same \n> protection space is\n> that the browser can then only usefully store a single set of \n> credentials\n> (if you get a 407 from a different proxy do the new \n> credentials from the\n> user replace the current credentials? Or should the new \n> credentials only\n> apply to the new proxy? Or the old credentials only to the \n> old proxy?).\n> And if you only distinguish by realm then you're making the \n> realm a global\n> namespace - the realm will have to be unique on all proxies \n> which might\n> take different auth info (which is doable inside a \n> corporation, I suppose,\n> but not on a larger scale). So it's not a question of trust, but a\n> question being able to (usefully) store multiple credentials \n> for multiple\n> proxies.\n\nI don't know of any scenario where I'd want to point my browser at multiple\nproxies that aren't in the same protection domain. I don't know how to even\nconfigure any browser to do that. Even so, if need be, realm name space can\nbe allocated from the DNS name space and hence be globally unique.\n\nPaul\n\n\n\n", "id": "lists-012-6918592"}, {"subject": "RE: domain attribute in digest aut", "content": "> -----Original Message-----\n> From: Ronald.Tschalaer@psi.ch [mailto:Ronald.Tschalaer@psi.ch]\n> Sent: Thursday, October 01, 1998 1:12 AM\n> To: HTTP-WG@hplb.hpl.hp.com\n> Subject: Re: domain attribute in digest auth\n> \n> \n> \n> One question concerning the domain attribute: suppose the \n> challenge from\n> http://foo.bar/blah contains the attribute \n> `domain=\"http://other.baz/\"'.\n> Is the browser to take that to mean that on foo.bar *only* /blah is in\n> the protection space?\n\nYes.\n\n If so, would sending \n> `domain=\"http://0.0.0.0/\"' be\n> valid way of saying that only a single URI on the server is \n> in the given\n> protection space?\n\nAssuming that 0.0.0.0 is an illegal or \"never used\" IP address, yes.\n\nPaul\n\n\n\n", "id": "lists-012-6928561"}, {"subject": "Re: domain attribute in digest auth/efficienc", "content": "> > > The second change you propose is incompatible with RFC 2069, for which\n> > > implementations exist. Furthermore, it reduces efficiency. For Basic,\n> > \n> > I guess we disagree on the efficency issue (I know for the apache module\n> > I've been working on this'll mean usually sending extra bytes over the\n> > wire at the same number of RTs).\n> \n> Why? I think the typical case will be no domain= at all, and the protection\n> domain is the whole server, regardless of the initial URL that you fetched\n> from the server.\n\nOnly if the server's pages are all in a single protection space (i.e.\nrealm) and none of them are unprotected (\"public\"). If any of these two\nconditions are not met you'll be sending more bytes over the wire witht\nthe current default.\n\nMost sites I've encountered which require authorization have some public\npages (i.e. no protection) and some private pages (i.e. protected). Now,\nif you don't send domain=... then the browser will send the\nAuthorization header on all requests, i.e. even for the public pages. \nBecause the Authorization header is typically quite large (> 200 bytes)\nyou therefore are better off sending the domain=... attribute (this\nassumes that the private pages all have a common path prefix, but that\nis also usually the case because of the way protection is configured).\n\nThe situation is similar for multiple realms on a server - the browser\nwill be sending the Authorization header unneccessarily each time it hits\na new realm.\n\nNow take a server like Apache where .htaccess files can (and do) control\nthe authorization stuff and can appear in any directory, and then you\nsee that in general you (the server) don't even have a clue of how many\ndifferent realms are on the server and which pages are protected and which\naren't (until you try to fetch them, that is). Therefore you want to send\nthe domain attribute where possible. (Just for the curious: this isn't\neasy to do because the protection is often defined in terms of files or\ndirectories, but the URI-to-filename translation is not always\nstraightforward (think mod-rewrite)).\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-6937836"}, {"subject": "Re: domain attribute in digest aut", "content": "> I don't know of any scenario where I'd want to point my browser at multiple\n> proxies that aren't in the same protection domain.\n\nHow about if authenticated proxies are used to allow and limit access to\nintranets from the outside? Or if a corporation wants to split their\nintranet into divisions or BUs and provide access to outsiders via\nauthenticated proxies? I have no idea if anybody is doing this or wants\nto do this, but then again I keep keep getting surprised at what kinds\nof applications and setups are being built. If all proxies are put in\nthe same protection space then you're cutting off these sorts of\npossibilities. By making proxy auth work similar to server auth you are\nleaving the possibility open.\n\n> I don't know how to even configure any browser to do that.\n\nNetscape at least allows for script based setting of proxies and if my\nmemory serves me right you can set different proxies for different URLs.\nDon't know if it handles multiple proxy-auth correctly, though.\n\n> Even so, if need be, realm name space can\n> be allocated from the DNS name space and hence be globally unique.\n\nTrue, but then you have to tell people to do so. Also, since the realm\nvalue is just about the only way a server can get some info of sorts in\nthe auth dialog box on browsers, it's used by a number of sites to to\nprovde additional info (I'm not saying this a great idea...). These\nfolks I believe would rather not have to litter the \"message\" with an\nextra DNS name just to guarantee uniqueness.\n\nThe question though is why go through all this? What are you trying to\ngain by ignoring the domain attribute? It doesn't simplfy the client any\n(since you can use basically the same code for both server and proxy\nauth), nor do I see it simplyfying the proxy much, nor does it simplify\nthe spec (you'd need to add words about the necessity of making the\nrealm globally unique).\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-6946911"}, {"subject": "PUT &amp; entity modificatio", "content": "I was wondering about this scenario:\n\nUser-agent issues a PUT request and transmits the entity to server.\nServer accepts it and performs some minor modifications on the\nentity, then stores it locally. Content type and/or length could\nbe changed and it would be nice if user-agen could be notified.\nContent-* headers issued with the response apply to the message body\nof the request and thus cannot be used.\n\nI was looking at the webdav drafts, but couldn't find anything\nthere. Is there a document that covers this situation?\n\n-- \n .-.   .-.    Life is a sexually transmitted disease.\n(_  \\ /  _)\n     |        dave@fly.cc.fer.hr\n     |\n\n\n\n", "id": "lists-012-6955934"}, {"subject": "RE: Microsoft prox", "content": "the behavior described below was exhibited in Microsoft Proxy Server 1.0.   \n\nProxy Server 2.0 'fixes' this by just closing the connection if we get a\ntimeout / reset from the server.\n\n\n\n> -----Original Message-----\n> From: Drazen Kacar [mailto:dave@fly.cc.fer.hr]\n> Sent: Thursday, October 01, 1998 6:14 AM\n> To: http-wg@hplb.hpl.hp.com\n> Subject: Microsoft proxy\n> \n> \n> I got this from wget maintainer. As far as I know, proxies aren't\n> supposed to append anything to the requested resource.\n> However, I've been off-line for more than a year and I haven't\n> digested the most recent HTTP spec. So, could someone tell\n> me if this is legal?\n> \n> ----- Forwarded message from Hrvoje Niksic <hniksic@srce.hr> -----\n> \n> Sender: hniksic@public.srce.hr\n> To: Drazen Kacar <dave@fly.cc.fer.hr>\n> Subject: [Simon Munton <simonm@m4data.co.uk>] RE: wget 1.5.3 \n> on Windows 98/NT, using MSVC 6\n> X-Attribution: Hrvoje\n> From: Hrvoje Niksic <hniksic@srce.hr>\n> Date: 25 Sep 1998 13:58:06 +0200\n> Message-ID: <kigiuicz9o1.fsf@jagor.srce.hr>\n> User-Agent: Gnus/5.070033 (Pterodactyl Gnus v0.33) \n> XEmacs/21.0 (Danish Landrace)\n> \n> ------- Start of forwarded message -------\n> Message-ID: \n> <c=GB%a=_%p=M4DATA%l=M4EXCH-980925091231Z-6978@m4exch.m4data.co.uk>\n> From: Simon Munton <simonm@m4data.co.uk>\n> To: \"'Hrvoje Niksic'\" <hniksic@srce.hr>\n> Subject: RE: wget 1.5.3 on Windows 98/NT, using MSVC 6\n> Date: Fri, 25 Sep 1998 10:12:31 +0100\n> MIME-Version: 1.0\n> Content-Type: text/plain; charset=\"us-ascii\"\n> Content-Transfer-Encoding: 7bit\n> \n> What seems to happen is this: part way through the download, \n> there is an\n> error,\n> eg timeout, and it seems that in addition MS proxy server sends a text\n> string that gets put\n> on the end of the downloaded file. Something like:\n>  \"HTTP Proxy error: Connection timed out\".\n> \n> When wget retries, this error message gets left embedded in the\n> downloaded file.\n> So the final file looks like:\n> \n> <-------Good Data--------><---HTTP Proxy error \n> message----><-------Good\n> data---->\n> \n> Hope this makes sense. If you'd like me to try and find out more\n> specific details\n> of what's going on, let me know. \n> \n> ===========================================================\n> Simon Muntonsimonm@m4data.co.uk\n> M4 Data LtdTel: 44-1749-679222\n> Mendip court, Bath Rd, WellsFax: 44-1749-673928\n> Somerset, BA5 3DG, England\n> \n> >\n> >> I also use wget through Microsoft Proxy server, which has \n> a habit of\n> >> sending error messages which get appended to the files being\n> >> downloaded, and when wget retries getting the file, the downloaded\n> >> file ends up with these messages in the middle.  I added a \n> couple of\n> >> changes to ftp.c and http.c (diffs below), so that they \n> 'back up' by\n> >> 2048 bytes when retrying, thereby getting rid of the error message\n> >> embedded in the file.  (2048 seems to work for me).\n> >\n> >I don't understand this part.  How does the server send these error\n> >messages, and exactly how do they get \"appended to the files\"?\n> >\n> >-- \n> >Hrvoje Niksic <hniksic@srce.hr> | Student at FER Zagreb, Croatia\n> >--------------------------------+--------------------------------\n> >Mix 2 table spoons sugar with 1 spoon salt.  Put it in a bottle\n> >and stick a fuse into it.  Say \"Shit!\" when it doesn't detonate.\n> >\n> \n> ------- End of forwarded message -------\n> \n> ----- End forwarded message -----\n> \n> -- \n>  .-.   .-.    Life is a sexually transmitted disease.\n> (_  \\ /  _)\n>      |        dave@fly.cc.fer.hr\n>      |\n> \n\n\n\n", "id": "lists-012-6963072"}, {"subject": "Re: question about implied LW", "content": "On Tue, 29 Sep 1998, Paul Leach wrote:\n> This seems to say that LWS is not allowed between adjacent quoted-strings.\n> Was that intended? I assume not, but I could be wrong.\n> \n> If not, I think it would be clearer to add the following production to the\n> basic rules\n> word = token | quoted-string | separator\n> and then change the section on implied LWS to say\n> \n> implied *LWS\n> The grammar described by this specification is word-based. Except where\n> noted otherwise, linear white space (LWS) can be included between any two\n> adjacent words (see below for the definition of \"word\") without changing the\n> interpretation of a field. At least one delimiter (LWS and/or separators)\n> MUST exist between any two tokens (for the definition of \"token\" below),\n> since they would otherwise be interpreted as a single token.\n\nI strongly recommend fixing this now.\n\nRFC 822 is vague about where embedded linear-white-space can be inserted. \nThere has been serious controversy in DRUMS about whether it's permitted\nbetween the header field name and the \":\", as well as if it's permitted\naround the \":\" in times or even between the digits.  In addition, many\nclients deal poorly with the implicit whitespace between tokens. \nTechnically, a media type of \"text(hi) / (there) plain\" is perfectly legal\nin email and has to be interpreted as \"text/plain\".  This often doesn't\nwork in practice.\n\nThe DRUMS WG chose to drop the concept of implicit linear-white-space and\nmake it all explicit in the grammar, as well as to deprecate the use of\ncomments and/or whitespace in stupid places so the \"generate\" grammer\nbetter matches reality.  The HTTP WG obviously doesn't have time to\nrewrite the BNF, so making sure it's as clear as possible could save pain\nand interoperability problems in the future.  Paul's suggestion is an\nexcellent step in the right direction.\n\n- Chris\n\n\n\n", "id": "lists-012-6977169"}, {"subject": "RE: domain attribute in digest aut", "content": "> -----Original Message-----\n> From: Ronald.Tschalaer@psi.ch [mailto:Ronald.Tschalaer@psi.ch]\n> Sent: Sunday, October 04, 1998 12:11 AM\n> To: Paul Leach; HTTP-WG@hplb.hpl.hp.com\n> Subject: Re: domain attribute in digest auth\n> \n> \n> \n> > I don't know of any scenario where I'd want to point my \n> browser at multiple\n> > proxies that aren't in the same protection domain.\n> \n> How about if authenticated proxies are used to allow and \n> limit access to\n> intranets from the outside? Or if a corporation wants to split their\n> intranet into divisions or BUs and provide access to outsiders via\n> authenticated proxies? I have no idea if anybody is doing \n> this or wants\n> to do this, but then again I keep keep getting surprised at what kinds\n> of applications and setups are being built. If all proxies are put in\n> the same protection space then you're cutting off these sorts of\n> possibilities. By making proxy auth work similar to server \n> auth you are\n> leaving the possibility open.\n\nFirst, I don't understand the scenarios: you haven't explained why the\nproxies are in different protection domains. \n\nBeing in the same protection domain just means that the browser will send an\nAuthorization header. If the credentials supplied in that Auth header fail,\nthen the proxy will return a 401, and different credentials can be supplied.\n\n> \n> > I don't know how to even configure any browser to do that.\n> \n> Netscape at least allows for script based setting of proxies and if my\n> memory serves me right you can set different proxies for \n> different URLs.\n> Don't know if it handles multiple proxy-auth correctly, though.\n\nSo does IE. It downloads the script from a \"base proxy\". I've only ever seen\nthe scripting capability used to implement a proxy farm.\n\n> \n> > Even so, if need be, realm name space can\n> > be allocated from the DNS name space and hence be globally unique.\n> \n> True, but then you have to tell people to do so. Also, since the realm\n> value is just about the only way a server can get some info \n> of sorts in\n> the auth dialog box on browsers, it's used by a number of sites to to\n> provde additional info (I'm not saying this a great idea...). These\n> folks I believe would rather not have to litter the \"message\" with an\n> extra DNS name just to guarantee uniqueness.\n\nThey only have to do it if they have wierd proxy server configurations with\nproxies in multiple realms.\n\n> \n> The question though is why go through all this? What are you trying to\n> gain by ignoring the domain attribute? It doesn't simplfy the \n> client any\n> (since you can use basically the same code for both server and proxy\n> auth), nor do I see it simplyfying the proxy much, nor does \n> it simplify\n> the spec (you'd need to add words about the necessity of making the\n> realm globally unique).\n\nIts way late to be making changes unless there's a compelling reason. It was\nnot obvious what domain= would be good for with Proxy-Auth, so I said that\nit should be ignored. The current spec at least is nicely interoperable. I\ndon't need to say anything about making the realm unique. I could say the\nProxy-Auth should ignore realm, if you wish.\n\nThe requirements here are unclear (to me at least). The most I want to do at\nthis point is to make sure that we don't get in the way of adding\nextensions. I'd be happy to say that domain= is ILLEGAL for Proxy-Auth. Then\nif we figure out what its good for and what its semantics are, we can write\nan RFC for an extension to Digest.\n\nPaul\n\nPaul \n\n\n\n", "id": "lists-012-6986637"}, {"subject": "Re: domain attribute in digest aut", "content": "> > > I don't know of any scenario where I'd want to point my browser\n> > > at multiple proxies that aren't in the same protection domain.\n> > \n> > How about if authenticated proxies are used to allow and limit\n> > access to intranets from the outside? Or if a corporation wants to\n> > split their intranet into divisions or BUs and provide access to\n> > outsiders via authenticated proxies? I have no idea if anybody is\n> > doing  this or wants to do this, but then again I keep keep getting\n> > surprised at what kinds of applications and setups are being built.\n> > If all proxies are put in the same protection space then you're\n> > cutting off these sorts of possibilities. By making proxy auth work\n> > similar to server  auth you are leaving the possibility open.\n> \n> First, I don't understand the scenarios: you haven't explained why the\n> proxies are in different protection domains.\n> \n> Being in the same protection domain just means that the browser will send an\n> Authorization header. If the credentials supplied in that Auth header fail,\n> then the proxy will return a 401, and different credentials can be supplied.\n\nYes, but then each time you go to a different proxy you will be prompted\nfor your username and password again. There are applications and setups\nout there (I'm not making this up, btw) which use multiple proxies and are\nconfigured with a list of proxies and a list of URIs for each proxy (i.e.\nwhich proxy to use for which URI). Now, if these proxies require\nauthentication then that means that the application has to keep changing\nthe (single) stored username/password before it access the URI. This also\nincurs unnecessary 407's, of course.\n\nOr back to the above example: you split the company net up along BUs (or\nwhatever they call them), providing access to a given BU's net via a\nproxy (which needs authentication), and configure the browsers to use the\nappropriate proxy for all the URIs on each BU's net. Furthermore, each\nproxy requires a different username/password because you want to be able\nto give users access only to a select few BU's info. Now, if a user\n(which has access to multiple BU's info) browses one BU's stuff (thereby\nusing that BU's proxy and entering the necessary username/password), then\nbrowses a second BU's stuff (again entering the corresponding credentials\nfor that BU's proxy), and then goes back to the first BU she will have to\nenter the credentials she already entered earlier just because we are not\nallowing different proxies to be in different protection spaces and\ntherefore not letting the browser store more than one set of credentials.\n\nBasically, proxies can not only be used to get out of an firewalled net\n(among other things), but also to get into one (in a controlled manner).\nTherefore a client may need to use different proxies for different URIs,\nand since access permission for one proxy does not imply access permission\nfor another it may need to supply different credentials for these proxies.\n\n> > Netscape at least allows for script based setting of proxies and if my\n> > memory serves me right you can set different proxies for different\n> > URLs. Don't know if it handles multiple proxy-auth correctly, though.\n> \n> So does IE. It downloads the script from a \"base proxy\". I've only ever\n> seen the scripting capability used to implement a proxy farm.\n\nHeck, I've never used it at all but that doesn't mean others don't use\nit for all sorts of stuff.\n\n> > The question though is why go through all this? What are you trying to\n> > gain by ignoring the domain attribute? It doesn't simplfy the client\n> > any (since you can use basically the same code for both server and\n> > proxy auth), nor do I see it simplyfying the proxy much, nor does it\n> > simplify the spec (you'd need to add words about the necessity of\n> > making the realm globally unique).\n> \n> Its way late to be making changes unless there's a compelling reason. It was\n> not obvious what domain= would be good for with Proxy-Auth, so I said that\n> it should be ignored. The current spec at least is nicely interoperable. I\n> don't need to say anything about making the realm unique. I could say the\n> Proxy-Auth should ignore realm, if you wish.\n\nMaking all proxies belong to the same protection space is changing the\nspec! If you leave the spec as it is now then Proxy-Auth is subject by\nthe same protection space definition as server auth ((host port realm) tuple)\nbut you have no way to extend that space over multiple proxies. I believe\nchanging the domain attribute wording as suggested is A) less of a change\nthan redefining the protection space of proxies, and B) more useful.\n\n> The requirements here are unclear (to me at least). The most I want to do\n> at this point is to make sure that we don't get in the way of adding\n> extensions. I'd be happy to say that domain= is ILLEGAL for Proxy-Auth.\n> Then if we figure out what its good for and what its semantics are, we\n> can write an RFC for an extension to Digest.\n\nI don't see why the semantics should be any different from those for\nserver auth - the (host port realm) tuple defines the protection space,\nwith the domain attribute as a means for extending that to a\n(list-of-hosts list-of-ports realm) (the host and port here just refer\nto the proxy's host/port instead of the server's host/port, that's all).\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-6998677"}, {"subject": "RE: domain attribute in digest aut", "content": "> -----Original Message-----\n> From: Ronald.Tschalaer@psi.ch [mailto:Ronald.Tschalaer@psi.ch]\n> Sent: Wednesday, October 07, 1998 11:48 PM\n>\n> \n> I don't see why the semantics should be any different from those for\n> server auth - the (host port realm) tuple defines the \n> protection space,\n> with the domain attribute as a means for extending that to a\n> (list-of-hosts list-of-ports realm) (the host and port here just refer\n> to the proxy's host/port instead of the server's host/port, \n> that's all).\n\nI think I finally see what you're talking about. I think you've got a\nfundamental misconception here. (Or maybe I do.) You're conflating proxy\nhost/port and origin server host/port. The protection space IS NOT defined\nby the host name of the proxy; it is defined by the host/port part of _the\nURI being accessed_. That's why I thought that \"domain\" had no meaning for\nproxies. What you are proposing is a new definition of _protection space_\nfor proxies, one which seems (on my first examination) to allow \"domain\" to\nmake sense for proxies.\n\nHowever, the spec is in last call to move forward to Draft Standard. I don't\nthink this comprises a bug, but an extension. You appear to have thought\nthat this was the way it worked before, but neither of the active authors,\nnor the people who asked us to clarify what \"domain\" meant for proxies,\nthought so. So, I will go back to my offer of making \"domain\" illegal for\nproxies (as an editorial change), so that the way would be clear for your\nextension to be implemented once it has been considered.\n\nPaul\n\n\n\n", "id": "lists-012-7011277"}, {"subject": "Re: domain attribute in digest aut", "content": "> > I don't see why the semantics should be any different from those for\n> > server auth - the (host port realm) tuple defines the \n> > protection space,\n> > with the domain attribute as a means for extending that to a\n> > (list-of-hosts list-of-ports realm) (the host and port here just refer\n> > to the proxy's host/port instead of the server's host/port, \n> > that's all).\n> \n> I think I finally see what you're talking about. I think you've got a\n> fundamental misconception here. (Or maybe I do.) You're conflating proxy\n> host/port and origin server host/port. The protection space IS NOT defined\n> by the host name of the proxy; it is defined by the host/port part of _the\n> URI being accessed_.\n\nBut, but, but, ... I just looked at the spec again and I don't see it\nsaying that (but I don't see it saying what exactly the protection\nspace for proxies is). Yes, my client works like I said above.\nInterestingly, I just tested Netscape (4.05) and it also works the way\nI understood it (btw., it will handle multiple credentials for multiple\nproxies, i.e. you are only prompted once for each proxy). I don't know\nof any other clients which handle multiple proxies except for IE - that\none somebody else would have to check.\n\n> That's why I thought that \"domain\" had no meaning for\n> proxies. What you are proposing is a new definition of _protection space_\n> for proxies, one which seems (on my first examination) to allow \"domain\" to\n> make sense for proxies.\n\nOk, I see where we were missing each other. I agree, if you use the\nserver's host/port then the domain attribute does not make sense (and I'm\nnot sure how to make it make sense).\n\n> However, the spec is in last call to move forward to Draft Standard. I don't\n> think this comprises a bug, but an extension. You appear to have thought\n> that this was the way it worked before, but neither of the active authors,\n> nor the people who asked us to clarify what \"domain\" meant for proxies,\n> thought so. So, I will go back to my offer of making \"domain\" illegal for\n> proxies (as an editorial change), so that the way would be clear for your\n> extension to be implemented once it has been considered.\n\nHowever, as noted above, there are at least two implementations which\nthink the host/port refers to that of the proxy. The problem is that I\nthink the two interpretations are mutually exclusive, i.e. if we make\nthe protection space always consist of the server's host/port then I\ndon't see how we can extend that in the future to work differently.\n\nI understand that the spec is in last call, but this problem must be\nsolved correctly - the protection space for a Proxy-Auth must defined much\nmore clearly. I think it makes more sense to use the proxy's host/port\ninstead of the server's for Proxy-Auth, but I'd like to hear from others\non the list. In any case, which ever solution is taken it can be\nconstrued as a significant change to the spec (depending on what your\ninterpretation was before) and therefore we should choose a solution\nbased on what makes sense, not on what the current spec (doesn't) say.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-7021063"}, {"subject": "HTTP 1.1 header incompatibilit", "content": "Section 4.2 \"Message Headers\" in RFC 1945 (HTTP 1.0) reads:\n\n   \"HTTP header fields, which include General-Header (Section 4.3),\n   Request-Header (Section 5.2), Response-Header (Section 6.2), and\n   Entity-Header (Section 7.1) fields, follow the same generic format\n   as that given in Section 3.1 of RFC 822 [7].  Each header field\n   consists of a name followed immediately by a colon (\":\"), a single\n   space (SP) character, and the field value.\"\n\nThe same section in the current HTTP 1.1 spec reads:\n\n   \"HTTP header fields, which include general-header (section 4.5),\n   request-header (section 5.3), response-header (section 6.2), and\n   entity-header (section 7.1) fields, follow the same generic format\n   as that given in Section 3.1 of RFC 822 [9].  Each header field\n   consists of a name followed by a colon (\":\") and the field value.\n   Field names are case-insensitive.  The field value MAY be preceded\n   by any amount of LWS, though a single SP is preferred.\"\n\nIn the process of liberalizing the syntax to allow LWS where SP was\npreviously required, we have made the whitespace optional altogether.  I\ndon't believe this was intentional, as such a basic syntactic change\nwould require incrementing the HTTP-Version major level (due to the\nincompatibility with HTTP 1.0).  I suggest rewording the middle sentence\nto say something like the following, and leaving the last sentence\nintact:\n\n   \"Each header field consists of a name followed by a colon (\":\"),\n   linear whitespace (LWS), and the field value.\"\n\nI know this sounds like a nit, but I have a customer citing this\nreference as allowing something like \"If-Modified-Since:Sat, 29 Oct 1994\n19:43:31 GMT\", and I'd rather thank him for helping us clean it up than\ntell him the HTTP 1.1 spec is in error.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7031060"}, {"subject": "HTTP 1.0 vs. 1.1 &ndash;&ndash; what's ne", "content": "hi,\n\ndave kristol, jeff mogul amd i are working on a document that catalogs the\nchanges between 1.0 and 1.1. am appending a list of changes -- please send\nme mail if i have missed anything. note that the taxonomy is somewhat \narbitrary and some things may fit in more than one place. \n\ncheers,\nbala\n\nbalachander krishnamurthy\nat&t labs--research\n\n################################################\n\nsection numbers in parens below refer to rev05 of http 1.1 found in\n  http://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-rev-05.txt\n\nstatus codes:\n1xx   reserved in .0 but never used\n100 Continue  \n101 Switching Protocols\n2xx   \n203 Non-Authoritative Information      \n205 Reset Content     \n206 Partial Content   \n3xx\n300 Multiple Choices  \n302 Found     (was Moved Temporarily, meaning changed slightly)\n303 See Other \n305 Use Proxy \n307 Temporary Redirect\n4xx\n402 Payment Required   \n405 Method Not Allowed\n406 Not Acceptable    \n407 Proxy Authentication Required     \n408 Request Timeout  \n409 Conflict  \n410 Gone     \n411 Length Required  \n412 Precondition Failed\n413 Request Entity Too Large \n414 Request-URI Too Long      \n415 Unsupported Media Type   \n416 Requested range not satisfiable  \n417 Expectation Failed \n5xx\n504 Gateway Timeout    \n505 HTTP Version Not Supported \n\ncaching:\nETag(14.19)\nStrong entity tags   (3.11)\nWeak entity tags     (3.11)\nAge (14.6)\nCache-control:(14.9)\n   request directives:  no-cache, no-store, max-age, max-stale, \nmin-fresh, no-transform, only-if-cached\n   response directives: public, private, no-cache, no-transform,\nno-store, must-revalidate, proxy-revalidate, max-age, s-maxage\nIf-Match    (14.24)\nIf-None-Match    (14.26)\nIf-Range(14.27)\nIf-Unmodified-Since(14.28)\nVary(14.44)\n\nbandwidth:\nExpect (14.20)\nAccept-Ranges (14.5)\nContent-Range (14.16)\nRange(14.35)\n\nnetwork/connection/protocol:  \nCONNECT (9.9)\nConnection (14.10)\nMax-Forwards   (14.31)\nUpgrade(14.42)\nVia(14.45)\n\nauthentication:\nProxy-Authenticate    (14.33)\nProxy-Authorization   (14.34)\n\ntransmission: \nChunked encoding\nContent-MD5 (14.15)\nTE(14.39)\nTrailer(14.40)\nTransfer-Encoding(14.41)\n\naddressing: \nHost (14.23)\nContent-Location (14.14)\n\nerror recovery/warning: \nWarning(14.46)\n\n\n\n", "id": "lists-012-7039250"}, {"subject": "HTTP 1.1 rev 5 issues under separate cove", "content": "As part of an evaluation of the impacts of HTTP 1.1 for my company's            \nVM:Webgateway product, I've been working on locating and summarizing all        \nthe MUST/MAY/SHOULD requirements in the current level of the spec               \n(draft-ietf-http-v11-spec-rev-05). In the process, I've found a number          \nof issues that I believe warrant consideration and possibly action.             \nSome are just editorial, but some aren't.  In my opinion none of them           \nshould stand in the way of standards advancement.                               \n                                                                                \nI'm sending each issue as a separate note following this one, as an aid         \nto folks like me who read mail by threads.                                      \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7048025"}, {"subject": "HTTP 1.1 issue 01: General note", "content": "There are 2 uses of \"REQUIRED\" and 6 of \"OPTIONAL\" that look like they          \nare intended as requirement statements, but don't use requirement terms         \n(MUST, SHOULD, etc.)                                                            \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7055514"}, {"subject": "HTTP 1.1 issue 02: 3.11 Entity Tag", "content": "In section 3.11 \"Entity Tags\", the statement                                    \n                                                                                \n   \"A given entity tag value MAY be used for entities obtained by               \n   requests on different URIs without implying anything about the               \n   equivalence of those entities.\"                                              \n                                                                                \nconfuses me to the point of being unable to parse it, let alone                 \nsummarize it.                                                                   \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7062368"}, {"subject": "HTTP 1.1 issue 03: 4.1 General Synta", "content": "In section 4.1 \"General Syntax\", the statements                                 \n                                                                                \n   \"In the interest of robustness, servers SHOULD ignore any empty              \n   line(s) received where a Request-Line is expected.\"                          \n                                                                                \nand                                                                             \n                                                                                \n   \"In other words, if the server is reading the protocol stream at             \n   the beginning of a message and receives a CRLF first, it SHOULD              \n   ignore the CRLF.\"                                                            \n                                                                                \nstate the same requirement in different forms.  Unless they're needed           \nfor other reasons, I suggest either deleting one of them or changing one        \nof the \"SHOULD\"s to \"should\".                                                   \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7069644"}, {"subject": "HTTP 1.1 issue 04: 4.2 Message Header", "content": "In section 4.2 \"Message Headers\", the statement                                 \n                                                                                \n   \"Applications SHOULD follow \"common form\", where one is known or             \n   indicated, when generating HTTP constructs, since there might                \n   exist some implementations that fail to accept anything beyond the           \n   common forms.\"                                                               \n                                                                                \nis so vague as to be impossible to measure, and should therefore not be         \nnormative and a requirement of compliance. I've brought this one up             \nbefore, so if the general opinion goes against me I won't complain.             \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7077965"}, {"subject": "HTTP 1.1 issue 05: 4.2 Message Header", "content": "In section 4.2 \"Message Headers\", the statement                                 \n                                                                                \n   \"It MUST be possible to combine the multiple header fields into              \n   one \"field-name: field-value\" pair, without changing the semantics           \n   of the message, by appending each subsequent field-value to the              \n   first, each separated by a comma.\"                                           \n                                                                                \nis a restatement of the preceding requirement,                                  \n                                                                                \n   \"Multiple message-header fields with the same field-name MAY be              \n   present in a message if and only if the entire field-value for               \n   that header field is defined as a comma-separated list [i.e.,                \n   #(values)].\"                                                                 \n                                                                                \nplacing a requirement upon future extensions to the protocol rather than        \non implementations of HTTP 1.1.  I think the \"MUST\" should be changed to        \na \"must\".                                                                       \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7085449"}, {"subject": "HTTP 1.1 issue 06: 4.4 Message Lengt", "content": "In section 4.4 \"Message Length\", the statement                                  \n                                                                                \n   \"Any response message which MUST NOT include a message-body (such            \n   as the 1xx, 204, and 304 responses and any response to a HEAD                \n   request) is always terminated by the first empty line after the              \n   header fields, regardless of the entity-header fields present in             \n   the message.\"                                                                \n                                                                                \nrefers to a requirement, it doesn't state one.  The \"MUST NOT\" should be        \nchanged to \"must not\".                                                          \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7094201"}, {"subject": "HTTP 1.1 issue 07: 4.4 Message Lengt", "content": "In section 4.4 \"Message Length\", does the statement                             \n                                                                                \n   \"If a Content-Length header field (section 14.13) is present, its            \n   decimal value in OCTETs represents both the entity-length and the            \n   transfer-length. The Content-Length header field MUST NOT be used            \n   if these two lengths are different (i.e., if a Transfer-Encoding             \n   header field is present).\"                                                   \n                                                                                \nmean that a receiver should ignore Content-Length, or a sender should           \nnot send it?                                                                    \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7101690"}, {"subject": "HTTP 1.1 issue 08: 8.1.2 Overall Operatio", "content": "In section 8.1.2 \"Overall Operation\", the statement                             \n                                                                                \n   \"Persistent connections provide a mechanism by which a client and            \n   a server can signal the close of a TCP connection. This signaling            \n   takes place using the Connection header field (section 14.10).               \n   Once a close has been signaled, the client MUST not send any more            \n   requests on that connection.\"                                                \n                                                                                \nshould probably read \"MUST NOT\" instead of \"MUST not\".                          \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n Inc.                                                         \n\n\n\n", "id": "lists-012-7109672"}, {"subject": "HTTP 1.1 issue 09: 8.1.2.1 Negotiatio", "content": "In section 8.1.2.1 \"Negotiation\", the statement                                 \n                                                                                \n   \"An HTTP/1.1 client MAY expect a connection to remain open, but              \n   would decide to keep it open based on whether the response from a            \n   server contains a Connection header with the connection-token                \n   close.\"                                                                      \n                                                                                \ndoes not state a requirement, the \"MAY\" should probably be \"may\".               \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7117732"}, {"subject": "HTTP 1.1 issue 10: 10.3.5 304 Not Modifie", "content": "In section 10.3.5 \"304 Not Modified\", the \"MUST NOT include a                   \nmessage-body\" requirement is stated twice in this section, at the top           \nand bottom.  One of them should be deleted.                                     \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7125612"}, {"subject": "HTTP 1.1 issue 11: 10.3.7 307 Temporary Redirec", "content": "In section 10.3.7 \"307 Temporary Redirect\", the \"SHOULD contain a short         \nhypertext note\" requirement is stated twice in this section, in the             \nsecond paragraph and at the bottom. One of them should be deleted.              \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7132585"}, {"subject": "HTTP 1.1 issue 14: 14.20.1 Expect 100continu", "content": "In section 14.20.1 \"Expect 100-continue\", the statement                         \n                                                                                \n   \"Proxies SHOULD maintain a cache recording the HTTP version                  \n   numbers received from recently-referenced next-hop servers.\"                 \n                                                                                \nduplicates the same requirement in section 8.2.4.                               \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7139610"}, {"subject": "HTTP 1.1 issue 17: 14.36 Refere", "content": "In section 14.36 \"Referer\", the statement                                       \n                                                                                \n   \"If the field value is a partial URI, it SHOULD be interpreted               \n   relative to the Request-URI.\"                                                \n                                                                                \nsounds more like a MUST than a SHOULD. An interoperability problem will         \nresult if two implementations interpret the referring URI differently.          \nIf the SHOULD is due to compatibility with earlier usage, that should be        \nnoted someplace.                                                                \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7146838"}, {"subject": "HTTP 1.1 issue 18: 14.46 Warnin", "content": "In section 14.46 \"Warning\", the statement                                       \n                                                                                \n   \"The warn-code consists of three digits. The first digit indicates           \n   whether the Warning MUST or MUST NOT be deleted from a stored                \n   cache entry after a successful revalidation:\"                                \n                                                                                \ndoes not specify a requirement.  The \"MUST or MUST NOT\" should be               \nchanged to \"must or must not\"                                                   \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7154185"}, {"subject": "HTTP 1.1 issue 20: 13.1.2 Warning", "content": "In section 13.1.2 \"Warnings\", the statement                                     \n                                                                                \n   \"The first digit indicates whether the Warning MUST or MUST NOT be           \n   deleted from a cached response after it is successfully                      \n   revalidated.                                                                 \n                                                                                \ndoes not specify a requirement.  The \"MUST or MUST NOT\" should be               \nchanged to \"must or must not\".                                                  \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7161446"}, {"subject": "HTTP 1.1 issue 19: 19.4.1 MIMEVersio", "content": "Section 19.4.1 \"MIME-Version\" contains the statement                            \n                                                                                \n   \"HTTP is not a MIME-compliant protocol (see appendix 19.4).\"                 \n                                                                                \nbut 19.4.1 *IS* in appendix 19.4.                                               \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7168727"}, {"subject": "HTTP 1.1 issue 22: 13.4 Response Cachabilit", "content": "In section 13.4 \"Response Cachability\", the statement                           \n                                                                                \n   \"Certain cache-control directives are therefore provided so that             \n   the server can indicate that certain resource entities, or                   \n   portions thereof, MUST NOT be cached regardless of other                     \n   considerations.\"                                                             \n                                                                                \ndoes not specify a requirement. The \"MUST or MUST NOT\" should be changed        \nto \"must or must not\".                                                          \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7175797"}, {"subject": "HTTP 1.1 issue 23: 13.5.2 Nonmodifiable Header", "content": "In section 13.5.2 \"Non-modifiable Headers\", the statement                       \n                                                                                \n   \"A non-transparent proxy MAY modify or add these fields to a                 \n   message that does not include no-transform, but if it does so, if            \n   not already present, it MUST add a Warning 214 (Transformation               \n   applied) if one does not already appear in the message (see                  \n   section 14.46).\"                                                             \n                                                                                \nappears to be in error.  It allows non-transparent proxies to make              \ncertain changes to messages that do not contain Cache-Control:                  \nno-transform.  The discussion of the no-transform directive in section          \n14.9.5 doesn't jive, and makes it look like this paragraph should read          \n\"... that contains no-transform ...\", just as the immediately prior             \nparagraph does.                                                                 \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7183284"}, {"subject": "HTTP 1.1 issue 21: 13.3.3 Weak and Strong Validator", "content": "In section 13.3.3 \"Weak and Strong Validators\", the statements                  \n                                                                                \n   \"The weak comparison function MAY be used for simple                         \n   (non-subrange) GET requests. The strong comparison function MUST             \n   be used in all other cases.\"                                                 \n                                                                                \nand                                                                             \n                                                                                \n   \"A cache or origin server receiving a conditional request, other             \n   than a full-body GET request, MUST use the strong comparison                 \n   function to evaluate the condition.\"                                         \n                                                                                \nstate the same requirement.                                                     \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7191218"}, {"subject": "HTTP 1.1 issue 24: 13.12 Cache Replacemen", "content": "In section 13.12 \"Cache Replacement\", the statement                             \n                                                                                \n\"If it inserts the new response into cache storage it                           \nSHOULD follow the rules in section 13.5.3.\"                                     \n                                                                                \ncould be construed as downgrading several MUSTs in 13.5.3 to SHOULDs.           \nThe \"SHOULD\" should either be replaced with \"should\" or this requirement        \nshould be upgraded to a \"MUST\".                                                 \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7199106"}, {"subject": "Th th th th that's all folks", "content": "Thanks for reading the foregoing pile of notes about the spec draft. If         \nyou're interested in the requirement table i've been working on, the            \ncurrent draft is available at                                                   \nhttp://WWW.GeoCities.Com/SiliconValley/Garage/3246/http11rq.txt for your        \nviewing pleasure.  Please remember that it is a work in progress, but           \nyour comments are very welcome.                                                 \n                                                                                \nRoss Patterson                                                                  \nVM Software Division                                                            \nSterling Software, Inc.                                                         \n\n\n\n", "id": "lists-012-7206496"}, {"subject": "Comments (Part 1): Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.1 to Dra ft Standar", "content": "Unfortunately, I have only had an opportunity to review this I-D for the\npast few\ndays. However, I have assembled an initial set of 51 comments. I expect\nto\nfollow this with additional comments over the next few days. Most of the\ncomments\npertain to form as opposed to technical substance. However, comments 6,\n10, 22,\n25, 30, 37, 38, and 41 are potentially substantive issues.\n\n1. Clause 1.2 fails to state that implementations that fail\nto satisfy statements marked as \"REQUIRIED\" would not qualify\nas comopliant.\n\n2. Clause 1.2 should indicate the status of these keywords in\n\"Notes\"; i.e., is the use of these keywords in notes normative?\n\n3. Clause 2.1, \"implied *LWS\", on pg. 15, contains what appears\nto be an editorial note \"[jg13]\".\n\n4. Clause 2.2, definition of \"CTL\", on pg. 16, fails to note that\nASCII (and ISO 646:IRV) consider SPACE (040) to be a control character\nof the same status as DEL (177).\n\n5. Clause 2.2, pg. 17, first paragraph, has a forward reference to\n\"parameter value\". Should add a cross reference to the section that\ndefines this non-terminal.\n\n6. Clause 3.4, pg. 21, specifies that \"the definition associated with\na MIME character set name MUST fully specify the mapping ...\". Should\nthis not be a requirement placed on the registrant of a MIME character\nset and not an HTTP implementation? Or, is this requirement really\nstating that any HTTP implementation must maintain a table of registered\ncharacter sets known to satisfy this requirement and MUST NOT use any\ncharacter set not present in this table? Overall, this seems an onerous\nrequirement for an HTTP implementation.\n\n7. Clause 3.6, pg. 24, 3rd para., states \"... (IANA) acts as a registry\nfor transfer-coding value tokens\" and goes on to list the initial set\nof registered tokens in which Content-Encoding tokens are included.\nShould\nthis not state \"acts as a registry for transfer and content coding value\ntokens\"?\n\n8. Clause 3.6, pg. 25, 5th para., uses the term \"optional metadata\"\nwithout\nproviding further definition of what such \"metadata\" might be.\n\n9. Clause 3.6, pg. 25, 6th para., discusses a \"situation\" regarding\ninteroperability failure. This \"situation\" should be described more\nfully\nor an example given to make clear what the problem was.\n\n10. Clause 3.7.1, pg. 26, 1st para., states \"An entity-body transferred\nvia HTTP messages MUST be represented in the appropriate canonical form\nprior to its transmission except for \"text\" types ...\". This requirement\nappears to be overly onerous for HTTP implementations. Is it actually\nthe\ncase that existing servers are validating canonical status of entity\nbodies?\n\n11. Clause 3.7.1, pg. 26, 2nd para., uses the phrases \"allows\" and\n\"allows\nthe use of\". Should these be rephrased using the \"MAY\" keyword? The same\ncomment applies elsewhere when the work \"allows\" or \"permitted\" is used.\n\n12. Clause 3.7.2, pg. 27, 2nd para., states \"In all other cases, an HTTP\nuser agent SHOULD follow the same or similar behavior as a MIME user\nagent\nwould ...\". This \"implied\" behavior needs to be made explicit. What is\nthe behavior of a MIME user agent in this context?\n\n13. Clause 3.7.2, pg. 27, 4th para., contains a note regarding\n\"multipart/\nform-data\". Why is this specific type given a special note? How about\n\"multipart/byte-ranges\"?\n\n14. Clause 3.8, pg. 28, 1st para., states \"Product tokens SHOULD be\nshort\nand to the point.\" and \"They MUST NOT be used for advertising or other\nnon-essential information.\" As an implementer, how can I interpret these\nrequirements? Either make them explicit or remove them.\n\n15. Clause 3.9 refers to \"short 'floating point' numbers\". I would\nsuggest\nreplacing this with \"real numbers\" since both \"short\" and \"floating\npoint\"\nseems to implementation specific.\n\n16. Clause 3.10 never actually says that RFC1766 language tags \"MUST\" be\nused. I'd suggest adding stronger language here.\n\n17. Clause 4.2, pg. 31, 4th para., states \"It MUST be possible ...\". I\nwould suggest replacing this with a statement that uses the converse and\n\"MUST NOT\"; e.g., \"Multiple header fields MUST NOT be combined into one\nheader\nunless ...\".\n\n18. Clause 4.3, pg. 31, 5th para., states \"The presence of a\nmessage-body\nin a request is signaled by the inclusion of Content-Length or Transfer-\nEncoding header field ...\".  However, \"multipart/byte-ranges\" may\ninclude\na message-body without either of these headers.\n\n19. Clause 4.4, pg. 32, 2nd para., has the relative clause \"... which\nMUST\nNOT ...\". This is not a requirement, so should not use these keywords.\nSuggest\nusing \"does not\".\n\n20. Clause 4.4, pg. 32, last para., the \"Note\" uses \"may\" and \"must\". If\nkeyword usage in notes is not normative, then it should be stated in\nclause 1.2.\n\n21. Clause 4.4, pg. 32, 1st para., uses the phrase \"cannot be\". Suggest\nrephrasing to use \"MUST NOT\".\n\n22. Clause 4.4, pg. 32, 5th para., states \"HTTP/1.1 user agents MUST\nnotify the user when an invalid length is received and detected.\" I have\nverified that the latest releases of Internet Explorer and Netscape\nCommunicator do not implement this requirement. If this standard is\nintended\nto capture current practice, then this is a broadening of current\npractice.\nI'd suggest using the keyword \"SHOULD\" or \"MAY\" instead.\n\n23. Clause 5.1.2, pg. 35, 3rd para., has \"three options\" when four\nare described.\n\n24. Clause 5.1.2, pg. 35, 5th para., uses the keyword \"REQUIRED\" instead\nof \"MUST\". It seems that \"MUST\" is given preference throughout this\ndocument. The same comment applies to the use of \"OPTIONAL\" vs. \"MAY\".\n\n25. Clause 7.2.1, pg. 41, 4th para., gives considerable flexibility to\na recipient regarding the heuristic guessing of an entity's content\ntype.\nIn particular, no default interpretation is dictated. In contrast, no\nflexibility is given in the hueristic guessing of a \"text\" content\ntype's\ncharacter set (cf. clause 3.4, where a default of ISO8859-1 is\ndictated).\nI wonder why the two quite different approaches are maintained. In\nparticular,\nI do know that the requirements of clause 3.4 will \"break\" many existing\nimplementations which assume that the default is applied as a default\nheuristic in the absence of an explicit CHARSET and not as an immediate\noverride to any heuristics. I fully expect our East Asian customers to\nrequire this feature of clause 3.4 to be permanently disabled to\naccommodate\nexisting practice.\n\n26. Clause 8.1.3, p. 43, 1st para., has the typo \"in14.10.\" Should\ninstead\nread \"in section 14.10.\".\n\n27. Clause 8.1.4, pg. 44, 6th para., has the phrase \"... SHOULD maintain\nAT MOST 2 connections ...\"; since \"AT MOST\" is not a keyword, suggest\nrephrasing his requirement using \"SHOULD NOT maintain more than 2\nconnections\".\n\n28. Clause 8.2.3, pg. 45, has the phrase \"(Confirmation by user-agent\nsoftware with semantic understanding of the application MAY substitute\nfor use confirmation.)\" This appears to controvert the stronger language\nin clause 8.1.4, para. 4, which does not have this parenthetical note.\n\n29. Clause 8.2.4, pg. 45, 1st para., uses the term \"end-client\". This\nterm seems to be nonstandard with other terminology regarding agents in\nthe HTTP context.\n\n30. Clause 9, pg. 48, 2nd para., appears to be partially redundant with\nclause 5.1.2, pg. 35, line 2078 (in file). Furthermore, does this\nrequirement\nactually hold for forms of Request-URI other than abs_path? For example,\ndoes an OPTIONS * HTTP/1.1 request require a Host header?\n\n31. Clause 9.2., pg. 49, 2nd para., states \"Response to this method are\nnot cachable.\" Should this be made strong with either MUST NOT or SHOULD\nNOT?\nThe same comment applies in a variety of other context regarding the\nsuitability or non-suitability of caching a response.\n\n32. Clause 9.3, pg. 50, 4th para., uses the expression \"if and only if\n...\".\nSuggest using \"MUST NOT unless\" instead.\n\n33. Clause 9.6, pg. 51, 1st para., uses the phrase \"the origin server\ncan \ncreate ...\". Suggest using MAY instead. Should review other uses of\n\"can\"\nin this document for similar substitution. Same comment applies to uses\nof\n\"cannot\" which should be substituted with \"MUST NOT\".\n\n34. Clause 9.6, pg. 52, 3rd para., uses the phrase \"server\" where\n\"origin\nserver\" is implied. Suggest reviewing uses of \"server\" for possible\nnarrower\nsemantics.\n\n35. Clause 9.8, pg. 53, 3rd para., note \"Responses to this method MUST\nNOT\nbe cached.\" while most other methods have \"Responses to this method are\nnot\ncachable.\" (cf. clause 9.6, 9.7).\n\n36. Clause 9.9 may wish to substitute its reference [44] with the new\nI-D\n<draft-luotonen-web-proxy-tunneling-01.txt>. However, I note that the\nargument to the CONNECT method prescribed by this I-D is not conformant\nwith the specification of \"Request URI\" in clause 5.1.2. Perhaps the\nreference to the tunneling draft should be removed altogether with this\nkeyword just stated as \"reserved\".\n\n37. Clause 10.2.5, pg. 56, 2nd para., states \"any new or updated\nmetainformation SHOULD be applied to the document currently in the user\nagent's active view.\" This conditional requirement seems to be placed on\nUA semantics outside the scope of HTTP proper. Suggest changing SHOULD\nto\nMAY.\n\n38. Clause 10.2.6 states \"the user agent SHOULD reset the document\nview\".\nThis conditional requirement seems to be placed on UA semantics outside\nthe scope of HTTP proper. Suggest changing SHOULD to MAY.\n\n39. Clause 10.2.7, pg. 56, 1st para., uses \"MUST\" in the past tense.\nSuggest rephrasing this to not use past tense.\n\n40. Clause 10.2.7, pg. 57, 2nd para., states \"the response MUST include\nall of the entity-headers that would have been returned ...\".  Which\nentity-headers are these precisely?\n\n41. Clause 10.3.2, pg. 58, 1st para., states \"The requested resource\nhas been assigned a new permanent URI and any future references to\nthis resource SHOULD be done using one of the returned URIs.\" This is\nan onerous requirement on UAs unless they happen to have link editing\ncapabilities. Should be qualified to not apply to UAs without such\ncapability; otherwise, no UA of this type will ever be unconditionally\ncompliant. Alternatively, change this requirement to MAY.\n\n42. Clause 10.3.2, pg. 58, 2nd para., states \"the entity of the response\nSHOULD contain a short hypertext note ...\". Suggest formalizing this to\nstate a specific content type or, alternatively, not use the term\nhypertext.\nThe same comment applies in a number of other clauses: search for \"short\nhypertext note\".\n\n43. Clause 10.3.3, pg. 58, 1st para., states \"This response is only\ncachable if indicated by a Cache-Control or Expires header field.\" Why\nthe conditionalization used here and not elsewhere regarding response\ncachability? Furthermore, these headers indicate non-suitability of\ncaching\nnot suitability.\n\n44. Clause 10.3.6 has a note describing \"significant security\nconsequences\".\nCould these consequences be detailed somewhere in this specification?\n\n45. Clause 10.3.7 has a typo. Change \"... specification, and is no\nlonger ...\"\nto \"... specification, is no longer ...\".\n\n46. Clause 10.4, pg. 61, 1st para., has a superfluous comma after \"the\nresponse\".\n\n47. Clause 10.4.8 has \"This code is similar to 401 (Unauthorized), but\nindicates that the client MUST first authenticate ...\" This doesn't seem\nto be a requirement but a statement of fact. Suggest changing to \"but\nindicates that the client did not first authenticate itself or its\ncredentials were not accepted ...\".\n\n48. Clause 10.4.10, pg. 63, 2nd para., has the phrase \"the server\nmight\".\nSuggest changing to \"the server MAY\". Should review other uses of\n\"might\"\nin this specification.\n\n49. Clause 10.4.10, pg. 63, 2nd para., has the phrase \"would likely\".\nSuggest\nrephrasing to use RECOMMENDED or MAY.\n\n50. Clause 10.4.11 has \"This response is cachable ...\". Suggest\nrephrasing\nas \"MAY be cached\". Is this the only 4XX response which is cachable?\n\n51. Claues 11 uses the term \"OPTIONAL\" as a keyword but this is not\na keyword context.\n\nGlenn Adams\nDirector, Software Architecture\nSpyglass, Inc.\nOne Cambridge Center\nCambridge, MA 02142\nTel: 617-679-4652, Fax: 617-621-9582\n\n\n\n", "id": "lists-012-7239207"}, {"subject": "Comments (Part 1) on HTTP ID Rev 0", "content": "I'm not certain which form is preferred, sending comments en masse or\nindividually. If the\nlatter is desired, let me know and I'll break these out. Of the\nfollowing, comments 6, 10, 22,\n25, 30, 37, 38, and 41 are potentially substantive issues. These\ncomments cover sections\n1-11; I intend to complete my comments later this week on the remaining\nsections.\n\n1. Section 1.2 fails to state that implementations that fail\nto satisfy statements marked as \"REQUIRIED\" would not qualify\nas compliant. Otherwise, suggest replacing REQUIRED with MUST or\nMUST NOT for the sake of consistency.\n\n2. Section 1.2 should indicate the status of these keywords in\n\"Notes\". Are the use of these keywords in notes normative?\n\n3. Section 2.1, pg. 15, \"implied *LWS\", contains what appears\nto be an editorial note \"[jg13]\".\n\n4. Section 2.2, pg. 16, definition of \"CTL\", fails to consider that\nASCII (and ISO646-1993) consider SPACE (040) to be a control character\nof the same status as DEL (177).\n\n5. Section 2.2, pg. 17, 1st para., has a forward reference to\n\"parameter value\". Should add a cross reference to the section that\ndefines this non-terminal.\n\n6. Section 3.4, pg. 21, specifies that \"the definition associated with\na MIME character set name MUST fully specify the mapping ...\". Should\nthis not be a requirement placed on the registrant of a MIME character\nset and not an HTTP implementation? Or, is this requirement really\nstating that any HTTP implementation must maintain a table of registered\ncharacter sets known to satisfy this requirement and MUST NOT use any\ncharacter set not present in this table? Overall, this seems an onerous\nrequirement for an HTTP implementation.\n\n7. Section 3.6, pg. 24, 3rd para., states \"... (IANA) acts as a registry\nfor transfer-coding value tokens\" and goes on to list the initial set\nof registered tokens in which Content-Encoding tokens are included.\nShould\nthis not state \"acts as a registry for transfer and content coding value\ntokens\"?\n\n8. Section 3.6, pg. 25, 5th para., uses the term \"optional metadata\"\nwithout\nproviding further definition of what such \"metadata\" might be. Suggest\nan\nexample here or clarification.\n\n9. Section 3.6, pg. 25, 6th para., discusses a \"situation\" regarding\ninteroperability failure. This \"situation\" should be described more\nfully\nor an example given to make clear what the problem is.\n\n10. Section 3.7.1, pg. 26, 1st para., states \"An entity-body transferred\nvia HTTP messages MUST be represented in the appropriate canonical form\nprior to its transmission except for \"text\" types ...\". Is it actually\nthe\ncase that servers are validating canonical status of entity bodies? This\ncontradicts the \"entity-body as payload\" philosophy.\n\n11. Section 3.7.1, pg. 26, 2nd para., uses the phrases \"allows\" and\n\"allows\nthe use of\". Should these be rephrased using the \"MAY\" keyword? The same\ncomment applies elsewhere when the work \"allows\" or \"permitted\" is used.\n\n12. Section 3.7.2, pg. 27, 2nd para., states \"In all other cases, an\nHTTP\nuser agent SHOULD follow the same or similar behavior as a MIME user\nagent\nwould ...\". This \"implied\" behavior needs to be made explicit. What is\nthe behavior of a MIME user agent in this context?\n\n13. Section 3.7.2, pg. 27, 4th para., contains a note regarding\n\"multipart/\nform-data\". Why is this specific type given a special note? How about\n\"multipart/byte-ranges\"?\n\n14. Section 3.8, pg. 28, 1st para., states \"Product tokens SHOULD be\nshort\nand to the point.\" and \"They MUST NOT be used for advertising or other\nnon-essential information.\" As an implementer, how can one interpret\nthese\nrequirements? Either make quantify them or remove them.\n\n15. Section 3.9 refers to \"short 'floating point' numbers\". I would\nsuggest\nreplacing this with \"real numbers\" since both \"short\" and \"floating\npoint\"\nseems to implementation specific.\n\n16. Section 3.10 never actually says that RFC1766 language tags \"MUST\"\nbe\nused. I'd suggest adding stronger language here.\n\n17. Section 4.2, pg. 31, 4th para., states \"It MUST be possible ...\". I\nwould suggest replacing this with a statement that uses the converse\nusing the\nform \"MUST NOT ... unless ...\"; e.g., \"Multiple header fields MUST NOT\nbe\ncombined into one header unless ...\".\n\n18. Section 4.3, pg. 31, 5th para., states \"The presence of a\nmessage-body\nin a request is signaled by the inclusion of Content-Length or Transfer-\nEncoding header field ...\".  However, \"multipart/byte-ranges\" may\ninclude\na message-body without either of these headers.\n\n19. Section 4.4, pg. 32, 2nd para., has the relative Section \"... which\nMUST\nNOT ...\". This is not a requirement, so should not use these keywords.\nSuggest\nusing \"does not\".\n\n20. Section 4.4, pg. 32, last para., the \"Note\" uses \"may\" and \"must\".\nIf\nkeyword usage in notes is not normative, then this should be stated in\nSection 1.2.\n\n21. Section 4.4, pg. 32, 1st para., uses the phrase \"cannot be\". Suggest\nrephrasing to use \"MUST NOT\".\n\n22. Section 4.4, pg. 32, 5th para., states \"HTTP/1.1 user agents MUST\nnotify the user when an invalid length is received and detected.\" This\ndoes\nnot seem to be reflected by current industry practice (cf. IE4 and\nNetscape\nCommunicator 4 behavior). If this standard is intended to capture\ncurrent\npractice, then this is a broadening of current practice. I'd suggest\nusing\nthe keyword \"MAY\" instead.\n\n23. Section 5.1.2, pg. 35, 3rd para., has \"three options\" when four\nare described.\n\n24. Section 5.1.2, pg. 35, 5th para., uses the keyword \"REQUIRED\"\ninstead\nof \"MUST\". It seems that \"MUST\" is given preference throughout this\ndocument. The same comment applies to the use of \"OPTIONAL\" vs. \"MAY\".\n\n25. Section 7.2.1, pg. 41, 4th para., gives considerable flexibility to\na recipient regarding the heuristic guessing of an entity's content\ntype.\nIn particular, no default interpretation is dictated. In contrast, no\nflexibility is given in the heuristic determination of a \"text\" content\ntype's\ncharacter set (cf. Section 3.4, where a default of ISO8859-1 is\ndictated).\nI wonder why the two quite different approaches are maintained. In\nparticular,\nI do know that the requirements of Section 3.4 will \"break\" many\nexisting\nimplementations which assume that the \"default\" is applied as a no more\nthan\na default heuristic in the absence of an explicit CHARSET and not as an\nimmediate override to any heuristics. I fully expect our East Asian\ncustomers\nto require this feature of Section 3.4 to be permanently disabled to\naccommodate\nexisting practice.\n\n26. Section 8.1.3, p. 43, 1st para., has the typo \"in14.10.\" Should\ninstead\nread \"in section 14.10.\".\n\n27. Section 8.1.4, pg. 44, 6th para., has the phrase \"... SHOULD\nmaintain\nAT MOST 2 connections ...\"; since \"AT MOST\" is not a keyword, suggest\nrephrasing his requirement using \"SHOULD NOT maintain more than 2\nconnections\".\n\n28. Section 8.2.3, pg. 45, has the phrase \"(Confirmation by user-agent\nsoftware with semantic understanding of the application MAY substitute\nfor use confirmation.)\" This appears to controvert the stronger language\nin Section 8.1.4, para. 4, which does not have this parenthetical note.\n\n29. Section 8.2.4, pg. 45, 1st para., uses the term \"end-client\". This\nterm seems to be nonstandard with other terminology regarding\ncommunicating\nparties in the HTTP context.\n\n30. Section 9, pg. 48, 2nd para., appears to be partially redundant with\nSection 5.1.2, pg. 35, line 2078 (in file). Furthermore, does this\nrequirement\nactually hold for forms of Request-URI other than abs_path? For example,\ndoes an OPTIONS * HTTP/1.1 request require a Host header?\n\n31. Section 9.2., pg. 49, 2nd para., states \"Response to this method are\nnot cachable.\" Should this be made stronger with either MUST NOT or\nSHOULD NOT?\nThe same comment applies in a variety of other context regarding the\nsuitability or non-suitability of caching a response.\n\n32. Section 9.3, pg. 50, 4th para., uses the expression \"if and only if\n...\".\nSuggest using \"MUST NOT unless\" instead.\n\n33. Section 9.6, pg. 51, 1st para., uses the phrase \"the origin server\ncan \ncreate ...\". Suggest using MAY instead. Should review other uses of\n\"can\"\nin this document for similar substitution. Same comment applies to uses\nof\n\"cannot\" which in most cases should be replaced with \"MUST NOT\".\n\n34. Section 9.6, pg. 52, 3rd para., uses the phrase \"server\" where\n\"origin\nserver\" appears to be implied. Suggest reviewing uses of \"server\" for\npossible\nnarrower semantics.\n\n35. Section 9.8, pg. 53, 3rd para., note \"Responses to this method MUST\nNOT\nbe cached.\" while most other methods have \"Responses to this method are\nnot\ncachable.\" (cf. Section 9.6, 9.7). Suggest making this language more\nconsistent.\n\n36. Section 9.9 may wish to substitute its reference [44] with the new\nI-D\n<draft-luotonen-web-proxy-tunneling-01.txt>. However, note that the\nargument to the CONNECT method prescribed by this I-D is not conformant\nwith the specification of \"Request URI\" in Section 5.1.2. Perhaps the\nreference to the tunneling draft should be removed altogether with this\nkeyword just stated as \"reserved\"?\n\n37. Section 10.2.5, pg. 56, 2nd para., states \"any new or updated\nmetainformation SHOULD be applied to the document currently in the user\nagent's active view.\" This conditional requirement seems to be place a\nconstraint on UA semantics outside the scope of HTTP proper. Suggest\nchanging SHOULD to MAY.\n\n38. Section 10.2.6 states \"the user agent SHOULD reset the document\nview\".\nThis conditional requirement seems to place a constraint on UA semantics\noutside the scope of HTTP proper. Suggest changing SHOULD to MAY.\n\n39. Section 10.2.7, pg. 56, 1st para., uses \"MUST\" in the past tense.\nSuggest rephrasing this to not use past tense.\n\n40. Section 10.2.7, pg. 57, 2nd para., states \"the response MUST include\nall of the entity-headers that would have been returned ...\".  Which\nentity-headers are these precisely?\n\n41. Section 10.3.2, pg. 58, 1st para., states \"The requested resource\nhas been assigned a new permanent URI and any future references to\nthis resource SHOULD be done using one of the returned URIs.\" This is\nan onerous requirement on UAs unless they happen to have link editing\ncapabilities. Should be qualified to not apply to UAs without such\ncapability; otherwise, no UA of this type will ever be unconditionally\ncompliant. Alternatively, change this requirement to MAY.\n\n42. Section 10.3.2, pg. 58, 2nd para., states \"the entity of the\nresponse\nSHOULD contain a short hypertext note ...\". Suggest formalizing this to\nstate a specific content type or, alternatively, not use the term\nhypertext.\nThe same comment applies in a number of other Sections: search for\n\"short\nhypertext note\".\n\n43. Section 10.3.3, pg. 58, 1st para., states \"This response is only\ncachable if indicated by a Cache-Control or Expires header field.\" In\ncontrast,\nother Sections (cf. 10.3.1, 10.3.2, etc.) have \"This response is\ncachable\nunless indicated otherwise.\" Suggest making these more consistent if\npossible\nor referring to Section 13.4.\n\n44. Section 10.3.6 has a note describing \"significant security\nconsequences\".\nCould these consequences be detailed somewhere in this specification?\n\n45. Section 10.3.7 has a typo. Change \"... specification, and is no\nlonger ...\"\nto \"... specification, is no longer ...\".\n\n46. Section 10.4, pg. 61, 1st para., has a superfluous comma after \"the\nresponse\".\n\n47. Section 10.4.8 has \"This code is similar to 401 (Unauthorized), but\nindicates that the client MUST first authenticate ...\" This doesn't seem\nto be a requirement but a statement of fact. Suggest changing to \"but\nindicates that the client did not first authenticate itself or its\ncredentials were not accepted ...\".\n\n48. Section 10.4.10, pg. 63, 2nd para., has the phrase \"the server\nmight\".\nSuggest changing to \"the server MAY\". Should review other uses of\n\"might\"\nin this specification.\n\n49. Section 10.4.10, pg. 63, 2nd para., has the phrase \"would likely\".\nSuggest\nrephrasing to use MAY or SHOULD instead.\n\n50. Section 10.4.11 has \"This response is cachable ...\". Suggest\nrephrasing\nas \"MAY be cached\". It may be useful here to point out that this is the\nonly\ncachable 4XX response (according to Section 13.4).\n\n51. Section 11 uses the term \"OPTIONAL\" as a keyword in a non-keyword\ncontext.\n\n\nGlenn Adams\nSpyglass, Inc., Cambridge, Mass.\n\n\n\n", "id": "lists-012-7260627"}, {"subject": "RE: Comments (Part 1): Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.1 to Dra ft Standar", "content": "Please ignore the message referenced by the above subject line and refer\ninstead\nto my second, revised message with subject line \"Comments (Part 1) on\nHTTP I-D Rev 05\".\n\n\n\n", "id": "lists-012-7282044"}, {"subject": "Re: HTTP 1.1 issue 04: 4.2 Message Header", "content": "jg@pa.dec.com (Jim Gettys) writes:\n\n>Empty message.  What was the complaint?\n\nHmm, that's two people who've reported that some of those notes came\nthrough empty.  Here's the original note for this one.\n\n>In section 4.2 \"Message Headers\", the statement\n>\n>   \"Applications SHOULD follow \"common form\", where one is known or\n>   indicated, when generating HTTP constructs, since there might\n>   exist some implementations that fail to accept anything beyond the\n>   common forms.\"\n>\n>is so vague as to be impossible to measure, and should therefore not be\n>normative and a requirement of compliance. I've brought this one up\n>before, so if the general opinion goes against me I won't complain.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7289609"}, {"subject": "Re: HTTP 1.1 issue 02: 3.11 Entity Tag", "content": "jg@pa.dec.com (Jim Gettys) writes:\n\n>The message was empty (by the time it got to me).\n>\n>What's the problem?\n\nHere's the original note:\n\n>In section 3.11 \"Entity Tags\", the statement\n>\n>   \"A given entity tag value MAY be used for entities obtained by\n>   requests on different URIs without implying anything about the\n>   equivalence of those entities.\"\n>\n>confuses me to the point of being unable to parse it, let alone\n>summarize it.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7297922"}, {"subject": "Re: HTTP 1.1 issue 02: 3.11 Entity Tag", "content": "Out of context, that seems to me to say that entity tag values are\nnot expected to be unique across different entities from the same\nserver, hence if the same value is sent by the server in response to\ndifferent request URIs, don't expect that the respones have any\nrelationship to each other.\n\nDave Morris\n\nOn Wed, 28 Oct 1998, Ross Patterson wrote:\n\n> \n> jg@pa.dec.com (Jim Gettys) writes:\n> \n> >The message was empty (by the time it got to me).\n> >\n> >What's the problem?\n> \n> Here's the original note:\n> \n> >In section 3.11 \"Entity Tags\", the statement\n> >\n> >   \"A given entity tag value MAY be used for entities obtained by\n> >   requests on different URIs without implying anything about the\n> >   equivalence of those entities.\"\n> >\n> >confuses me to the point of being unable to parse it, let alone\n> >summarize it.\n> \n> Ross Patterson\n> VM Software Division\n> Sterling Software, Inc.\n> \n> \n\n\n\n", "id": "lists-012-7305868"}, {"subject": "Re: HTTP 1.1 issue 05: 4.2 Message Header", "content": "> From: Ross Patterson <RossP@SS1.Reston.VMD.Sterling.Com>\n> Resent-From: Andy Norman <ange@hplb.hpl.hp.com>\n> Date: Tue, 20 Oct 1998 23:43:14 +0100 (BST)\n> To: http-wg@hplb.hpl.hp.com\n> Subject: HTTP 1.1 issue 05: 4.2 Message Headers                              \n>  \n>                                                                              \n>  \n> -----\n> In section 4.2 \"Message Headers\", the statement                              \n>  \n> \n>    \"It MUST be possible to combine the multiple header fields into           \n>  \n>    one \"field-name: field-value\" pair, without changing the semantics        \n>  \n>    of the message, by appending each subsequent field-value to the           \n>  \n>    first, each separated by a comma.\"                                        \n>  \n> \n> is a restatement of the preceding requirement,                               \n>  \n> \n>    \"Multiple message-header fields with the same field-name MAY be           \n>  \n>    present in a message if and only if the entire field-value for            \n>  \n>    that header field is defined as a comma-separated list [i.e.,             \n>  \n>    #(values)].\"                                                              \n>  \n> \n> placing a requirement upon future extensions to the protocol rather than     \n>  \n> on implementations of HTTP 1.1.  I think the \"MUST\" should be changed to     \n>  \n> a \"must\".                                                                    \n>  \n> \n> Ross Patterson                                                               \n>  \n> VM Software Division                                                         \n>  \n> Sterling Software, Inc.                                                      \n>  \n\n\n\n", "id": "lists-012-7314712"}, {"subject": "RFC: Web caching documentatio", "content": "This isn't precisely on-topic, but is IMHO of interest to the group.\nApologies if it is inappropriate.\n\nI've created a document that explains aspects of caching; what it is,\nwhy it is good, and how to go about it (with server-specific\ninstructions). The target audience of web designers, webmasters and the\nlike - anyone who creates content on the Web. \n\nI'd very much like to get feedback from the group as a first check to\nthe accuracy, usefulness and scope of the document. After that, it will\nbe presented to users to get feedback on its usability, readability,\netc., and then (hopefully) be distributed to the widest possible\naudience. \n\nIt most likely won't be distributed through traditional RFC channels, as\nIMHO it won't get read by the right people there.\n\nIt can be found at\nhttp://www.pobox.com/~mnot/cache_docs/\n[ a MS Word document; if this is a problem, please contact me for\nanother form ]\n\nPLEASE note that this is a preliminary draft, and is not intended for\ndistribution. It is imprecise, and almost certainly contains errors.\nPlease do not give it to anyone who can not contribute, as outlined\nabove. I am open to suggestions both about content, intent and ways to\nfacilitate distribution of the document, along with most anything else.\n\nThanks,\n\n\nMark Nottingham\nInternet Project Manager\nMerrill Lynch Australasia\n\n\n\n", "id": "lists-012-7324664"}, {"subject": "RE: Web caching documentatio", "content": "Now available in PDF form as well. Also, just to clarify -- this isn't\nintended (at least at this point) to be on track to a RFC; the subject\nwas chosen (perhaps unfortunately) with the literal meaning of RFC in\nmind.\n\nCheers,\n\n\n\n", "id": "lists-012-7333248"}, {"subject": "Fwd: Rev05 nit", "content": "Dave sent me some nits off line; as we are in the final throes, I want\nit in the record.\n- Jim\n\n\nattached mail follows:\nYou just *knew* you would hear from me, didn't you!? :-)\nSome nits for the next go-round.\n\nDave\n-------------\n\nHave you noticed there's no Page 2 in the PostScript versions?!\nAnother gift from Word.\n\n5.1.2 Request-URI\n\n   The Request-URI is a Uniform Resource Identifier (section 3.2) and\n   identifies the resource upon which to apply the request.\n\n          Request-URI    = \"*\" | absoluteURI | abs_path | authority\n\n   The three options for Request-URI are dependent on the nature of the\n       =====\n\nThere are evidently four now.\n\n14.16 Content Range\n   When an HTTP message includes the content of multiple ranges (for\n   example, a response to a request for multiple non-overlapping\n   ranges), these are transmitted as a multipart message. The multipart\n   media type used for this purpose is \"multipart/byteranges\" as defined\n   in appendix 19.2. See appendix 19.6.3for a compatibility issue.\n   ^-- insert space\n\n19.6.3 Changes from RFC 2068\n   ...\n   worth fixing [39]. TE also solves another, obscure, downward\n   interoperability problem that could have occured due to interactions\n       ======= -> occurred\n   between authentication trailers, chunked encoding and HTTP/1.0\n   clients.(Section 3.6, 3.6.1, and 14.39)\n\n\n\n", "id": "lists-012-7339992"}, {"subject": "Fwd: http issue &quot;AUTHVSPROXY&quot; not quite close", "content": "Nit by private mail; since we are so far down stream, I'd like it\nin the archives.\n- Jim\n\n\nattached mail follows:\n\n  Hi Jim,\n\nI sent you a mail about this a couple months back, but I think I got it\nslightly wrong. In the issue list \"AUTHVSPROXY\" actually has two things,\nthe second of which has been done. The first, however, still needs doing:\n\n[Gisle wrote:]\n> Is there a good reason why WWW-Authenticate can have multiple\n> challenges while Proxy-Authenticate can't?\n> \n> draft-ietf-http-v11-spec-rev-03:\n> \n> >         Proxy-Authenticate  = \"Proxy-Authenticate\" \":\" challenge\n> >         WWW-Authenticate  = \"WWW-Authenticate\" \":\" 1#challenge\n\n[Paul rsponded:]\n> none. Just forgot to update the one when I was updating the other.  I'll\n> fix it in the next draft.\n\nBut since it's in the http spec, not the auth spec, section 14.33 of\ndraft-ietf-http-v11-spec-rev-05 still has the old Proxy-Authenticate\nsyntax and needs to be changed to\n\n      Proxy-Authenticate  = \"Proxy-Authenticate\" \":\" 1#challenge\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-7347477"}, {"subject": "Fwd: Problem with PUT and redirections on Apach", "content": "Another one that should have gone to the list.\n\nThis one is a real protocol problem, however.\n- Jim\n\n\nattached mail follows:\nWe have a protocol problem.  Section 8.2.4 says that we MUST return\nan \"error status\" if we don't return a 100.  That shouldn't say what\nkind of other status the server might return, since some requests can\nbe completed successfully without reading the body (e.g., the OPTIONS\nbody can be discarded).  302 responses is another case where the\nserver does not want to return 100 even though it is not an error.\n\n....Roy\n\n\n\n", "id": "lists-012-7355022"}, {"subject": "Fwd: question about implied LW", "content": "This should have gone to the mailing list, but didn't.\n\nSeems like a simpler solution.\n- Jim\n\n> From: Paul Leach <paulle@microsoft.com>\n> Date: Mon, 5 Oct 1998 11:28:03 -0700\n> To: \"Roy Fielding (E-mail)\" <fielding@avron.ics.uci.edu>\n> Cc: \"Jim Gettys (E-mail)\" <jg@w3.org>\n> Subject: FW: FW: question about implied LWS\n> -----\n> Despite the fact that I didn't ask the question I intended to ask (whicy was\n> whether LWS is allowed between quoted-strings and separators) you answered\n> the question I intended to ask.\n> \n> Looking at my suggestion again, I think a simpler fix is to change the\n> underlined instance of \"tokens\" in the paragraph below to \"words\".\n> \n> implied *LWS\n> The grammar described by this specification is word-based. Except where\n> noted otherwise, linear white space (LWS) can be included between any two\n> adjacent words (token or quoted-string), and between adjacent tokens and\n>                                                               ======\n> separators, without changing the interpretation of a field. At least one\n> delimiter (LWS and/or separators) MUST exist between any two tokens (for the\n> definition of \"token\" below), since they would otherwise be interpreted as a\n> single token.\n> \n> Jim, can you mark this as an editorial issue?\n> \n> -----Original Message-----\n> From: Roy T. Fielding [mailto:fielding@kiwi.ics.uci.edu]\n> Sent: Saturday, October 03, 1998 11:11 PM\n> To: Paul Leach\n> Subject: Re: FW: question about implied LWS\n> \n> >Section 2.2 on \"basic rules\" says:\n> >\n> >implied *LWS\n> >The grammar described by this specification is word-based. Except where\n> >noted otherwise, linear white space (LWS) can be included between any two\n> >adjacent words (token or quoted-string), and between adjacent tokens and\n> >separators, without changing the interpretation of a field. At least one\n> >delimiter (LWS and/or separators) MUST exist between any two tokens (for\n> the\n> >definition of \"token\" below), since they would otherwise be interpreted as\n> a\n> >single token.\n> >\n> >This seems to say that LWS is not allowed between adjacent quoted-strings.\n> \n> Adjacent quoted-strings are adjacent words, so it doesn't say that.\n> \n> >Was that intended? I assume not, but I could be wrong.\n> >\n> >If not, I think it would be clearer to add the following production to the\n> >basic rules\n> >word = token | quoted-string | separator\n> >and then change the section on implied LWS to say\n> \n> That adds a rule saying you can put LWS between quoted strings and\n> separators.\n> AFAIK that is okay too, since everywhere that actually restricts LWS\n> should be specific about restricting it.\n> \n> >implied *LWS\n> >The grammar described by this specification is word-based. Except where\n> >noted otherwise, linear white space (LWS) can be included between any two\n> >adjacent words (see below for the definition of \"word\") without changing\n> the\n> >interpretation of a field. At least one delimiter (LWS and/or separators)\n> >MUST exist between any two tokens (for the definition of \"token\" below),\n> >since they would otherwise be interpreted as a single token.\n> \n> Fine with me.\n> \n> ....Roy\n\n\nattached mail follows:\nDespite the fact that I didn't ask the question I intended to ask (whicy was\nwhether LWS is allowed between quoted-strings and separators) you answered\nthe question I intended to ask.\n\nLooking at my suggestion again, I think a simpler fix is to change the\nunderlined instance of \"tokens\" in the paragraph below to \"words\".\n\nimplied *LWS\nThe grammar described by this specification is word-based. Except where\nnoted otherwise, linear white space (LWS) can be included between any two\nadjacent words (token or quoted-string), and between adjacent tokens and\n                                                              ======\nseparators, without changing the interpretation of a field. At least one\ndelimiter (LWS and/or separators) MUST exist between any two tokens (for the\ndefinition of \"token\" below), since they would otherwise be interpreted as a\nsingle token.\n\nJim, can you mark this as an editorial issue?\n\n-----Original Message-----\nFrom: Roy T. Fielding [mailto:fielding@kiwi.ics.uci.edu] \nSent: Saturday, October 03, 1998 11:11 PM\nTo: Paul Leach\nSubject: Re: FW: question about implied LWS \n\n\n>Section 2.2 on \"basic rules\" says:\n>\n>implied *LWS\n>The grammar described by this specification is word-based. Except where\n>noted otherwise, linear white space (LWS) can be included between any two\n>adjacent words (token or quoted-string), and between adjacent tokens and\n>separators, without changing the interpretation of a field. At least one\n>delimiter (LWS and/or separators) MUST exist between any two tokens (for\nthe\n>definition of \"token\" below), since they would otherwise be interpreted as\na\n>single token. \n>\n>This seems to say that LWS is not allowed between adjacent quoted-strings.\n\nAdjacent quoted-strings are adjacent words, so it doesn't say that.\n\n>Was that intended? I assume not, but I could be wrong.\n>\n>If not, I think it would be clearer to add the following production to the\n>basic rules\n>word = token | quoted-string | separator\n>and then change the section on implied LWS to say\n\nThat adds a rule saying you can put LWS between quoted strings and\nseparators.\nAFAIK that is okay too, since everywhere that actually restricts LWS\nshould be specific about restricting it.\n\n>implied *LWS\n>The grammar described by this specification is word-based. Except where\n>noted otherwise, linear white space (LWS) can be included between any two\n>adjacent words (see below for the definition of \"word\") without changing\nthe\n>interpretation of a field. At least one delimiter (LWS and/or separators)\n>MUST exist between any two tokens (for the definition of \"token\" below),\n>since they would otherwise be interpreted as a single token.\n\nFine with me.\n\n....Roy\n\n\n\n", "id": "lists-012-7362500"}, {"subject": "Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "Something interesting has come up that's one of: a bug or infelicity in\nmy server, or a bug or infelicity in Netscape 4.5\n\nNetscape 4.5 sends an HTTP/1.0 request with Accept-Encoding: gzip\nheader.  A web site has a paper.ps.Z file, i.e., Content-type:\napplication/postscript, Content-Encoding: compress.  When NS 4.5 tries\nto GET the paper, my server returns 406 Not Acceptable, because\n\"compress\" is not one of the accepted encodings.\n\nThere seem to be two (not mutually exclusive) conclusions to draw:\n\n1) Netscape 4.5 should send Accept-Encoding: gzip, compress, because\ngzip (well, the gzip program, anyway) understands the Unix compress\nformat.\n\n2) My server should not send 406, since it's only a SHOULD requirement\nanyway.  Or perhaps it should send 406 only for HTTP/1.1 requests.\n\nOpinions/comments?\n\nDave Kristol\n\n\n\n", "id": "lists-012-7376283"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "Dave,\nContent negotiation is hard, ain't it?  I think you\nhave run into an \"infelicity\" rather than a true bug, and\nthat the infelicty is shared between server and client.  It seems\nquite likely that the Netscape 4.5 client is using a gzip plugin\nor program capable of handling compress, so it would be better\nwere it configured to send an Accept-Encoding header which\nindicated that capability.  The server can't presume that, though,\nas someone might have implemented a library which did gzip\nencoding but not compress encoding (for licensing or religious reasons).\nThe infelicity on the server side is that it is sending\nout pre-compressed files in a single format.  In the best of all\npossible worlds it would always have the non-compressed format\navailable, or would be applying the compression on the fly based\non the accepted encodings, or even removing the compression on\nthe fly.  Since the Accept-Encoding header has a default inclusion\nof \"No Encoding\", that would guarantee that *something* was always\navailable.\nNot really a bug in either case, just an infelicity.\nregards,\nTed Hardie\n\n\nPS.  Today is my last day at NASA; mail will be forwarded, but communication\nafter November 11th should go to me as hardie@equinix.com\n\nOn Nov 3,  2:38pm, Dave Kristol wrote:\n> Subject: Netscape 4.5 and HTTP/1.1 Accept-Encoding\n> Something interesting has come up that's one of: a bug or infelicity in\n> my server, or a bug or infelicity in Netscape 4.5\n>\n> Netscape 4.5 sends an HTTP/1.0 request with Accept-Encoding: gzip\n> header.  A web site has a paper.ps.Z file, i.e., Content-type:\n> application/postscript, Content-Encoding: compress.  When NS 4.5 tries\n> to GET the paper, my server returns 406 Not Acceptable, because\n> \"compress\" is not one of the accepted encodings.\n>\n> There seem to be two (not mutually exclusive) conclusions to draw:\n>\n> 1) Netscape 4.5 should send Accept-Encoding: gzip, compress, because\n> gzip (well, the gzip program, anyway) understands the Unix compress\n> format.\n>\n> 2) My server should not send 406, since it's only a SHOULD requirement\n> anyway.  Or perhaps it should send 406 only for HTTP/1.1 requests.\n>\n> Opinions/comments?\n>\n> Dave Kristol\n>-- End of excerpt from Dave Kristol\n\n\n\n", "id": "lists-012-7384394"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "On Tue, 3 Nov 1998, Dave Kristol wrote:\n\n> Something interesting has come up that's one of: a bug or infelicity in\n> my server, or a bug or infelicity in Netscape 4.5\n> \n> Netscape 4.5 sends an HTTP/1.0 request with Accept-Encoding: gzip\n> header.  A web site has a paper.ps.Z file, i.e., Content-type:\n> application/postscript, Content-Encoding: compress.  When NS 4.5 tries\n> to GET the paper, my server returns 406 Not Acceptable, because\n> \"compress\" is not one of the accepted encodings.\n> \n> There seem to be two (not mutually exclusive) conclusions to draw:\n> \n> 1) Netscape 4.5 should send Accept-Encoding: gzip, compress, because\n> gzip (well, the gzip program, anyway) understands the Unix compress\n> format.\n\nI  think gzip and compress are different encodings.  The fact that\na program called \"gzip\" understands both is not relevant.\n\n> \n> 2) My server should not send 406, since it's only a SHOULD requirement\n> anyway.  Or perhaps it should send 406 only for HTTP/1.1 requests.\n> \n\nWhat does your server do if a client sends a request with *no* \nAccept-Encoding?  I think that's what it should do in this case.\n(E.g. send it as application/octet-stream).\n\nIt is quite likely that there will be clients which support some\ncontent encodings but not all the ones you support.  I don't think you\nwant your server to be in the situation where it completely refuses to\nserve a file to such clients.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-7394447"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "On Tue, 3 Nov 1998, John Franks wrote:\n\n> On Tue, 3 Nov 1998, Dave Kristol wrote:\n> \n> I  think gzip and compress are different encodings.  The fact that\n> a program called \"gzip\" understands both is not relevant.\n\nme too ...\n\n> \n> > \n> > 2) My server should not send 406, since it's only a SHOULD requirement\n> > anyway.  Or perhaps it should send 406 only for HTTP/1.1 requests.\n> > \n> \n> What does your server do if a client sends a request with *no* \n> Accept-Encoding?  I think that's what it should do in this case.\n> (E.g. send it as application/octet-stream).\n> \n> It is quite likely that there will be clients which support some\n> content encodings but not all the ones you support.  I don't think you\n> want your server to be in the situation where it completely refuses to\n> serve a file to such clients.\n\nBetter I think is that the server maintain two versions or in this case,\nuncompress on the fly ... or send a choice page that lets the user\nclick the link for the explicit file... my assumption being that the\nrequest was for the resource w/o the _Z ... if the _Z is requested,\nthen John's approach is reasonable ... one of the things missed in\nthis whole mess is that the user *MAY* simply want the file delivered\nunmolested and saved on the hard drive. It is quite inconvenient for\nme when something is unziped or uncompressed by browsers, servers, etc.\n\nDave Morris\n\n\n\n", "id": "lists-012-7404107"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "Dave Kristol:\n>\n>Something interesting has come up that's one of: a bug or infelicity in\n>my server, or a bug or infelicity in Netscape 4.5\n>\n>Netscape 4.5 sends an HTTP/1.0 request with Accept-Encoding: gzip\n>header.  A web site has a paper.ps.Z file, i.e., Content-type:\n>application/postscript, Content-Encoding: compress.  When NS 4.5 tries\n>to GET the paper, my server returns 406 Not Acceptable, because\n>\"compress\" is not one of the accepted encodings.\n>\n>There seem to be two (not mutually exclusive) conclusions to draw:\n>\n>1) Netscape 4.5 should send Accept-Encoding: gzip, compress, because\n>gzip (well, the gzip program, anyway) understands the Unix compress\n>format.\n\nNetscape would be allowed to include 'compress' in its Accept-Encoding\nline, but there is nothing in the spec which says that it should.\n\n>\n>2) My server should not send 406, since it's only a SHOULD requirement\n>anyway.  Or perhaps it should send 406 only for HTTP/1.1 requests.\n\nIf the above paper is available in ps.Z version only (so we don't have\ncontent negotiation in the sense of multiple variants) I would never\nsend a 406.  The 1.1 spec says:\n\n    Note: HTTP/1.1 servers are allowed to return responses which are\n    not acceptable according to the accept headers sent in the request.\n    In some cases, this may even be preferable to sending a 406\n    response.\n\nand your situation is one of the 'some cases'.  In your case I would\nreturn the document with \n\nContent-type: application/x-compress  \n      (MIME type from memory, I may be wrong)\n\nwhich will probably result in the end user having to handle the\ntop-level .ps format by hand, but at least it will prevent\ncompressed data from being fed into a postscript helper application.\n\nIn fact I would say that, given the current installed base and if you\nare aiming for maximum interoperability, if a server is in the\nsituation of only being able to send .ps.Z, it should always serve it\nwith the above content-type, and never with the\ncontent-type/content-encoding values you mention above.  If it is\ngoing to switch headers on a case by case basis, it should add the\nappropriate headers to disable proxy caching.\n\nYour scenario is really a case where a server has to choose between\nevils: really fixing the problem involves having the content author\nchange the .ps.Z into .ps, or a choice between .ps and .ps.gz.\n\n>Opinions/comments?\n>\n>Dave Kristol\n\nKoen.\n\n\n\n", "id": "lists-012-7413909"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "See section 3.5; I believe it is clear on the topic.\n\nThe tokens refer to algorithms; not programs.\n\ngzipAn encoding format produced by the file compression program ``gzip'' \n(GNU zip) as described in RFC 1952 [25]. This format is a Lempel-Ziv coding \n(LZ77) with a 32 bit CRC.\n\ncompress The encoding format produced by the common UNIX file compression \nprogram ``compress''. This format is an adaptive Lempel-Ziv-Welch coding \n(LZW).\n\n\nUse of program names for the identification of encoding formats is not \ndesirable and is discouraged for future encodings. Their use here is \nrepresentative of historical practice, not good design. For compatibility \nwith previous implementations of HTTP, applications SHOULD consider ``x-gzip'' \nand ``x-compress'' to be equivalent to ``gzip'' and ``compress'' respectively.\n\nThat a particular implementation chooses to use a gzip program that happens\nto also handle compress is an implementation issue; it should declare\nwhat algorithms it actually implements.\n\nNote the historical note.  We been here before, and done this folks.\n- Jim\n\n\n\n", "id": "lists-012-7423509"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "David W. Morris wrote:\n> \n> On Tue, 3 Nov 1998, John Franks wrote:\n> \n> > On Tue, 3 Nov 1998, Dave Kristol wrote:\n> >\n> > I  think gzip and compress are different encodings.  The fact that\n> > a program called \"gzip\" understands both is not relevant.\n\n[N.B.  I never received John Franks's message, either directly, or via\nthe mailing list.]\n\nYup, folks, I really did know they are different encodings.  My point\nwas that perhaps NS 4.5 was able to decode both \"gzip\" and \"compress\"\nencodings because the gzip *program* can decode both.  Just a hunch,\nalthough I was told privately that NS 4.5 probably cannot decode\n\"compress\".\n\nI think Koen's remarks were most appropriate about not sending 406 in\nthe absence of negotiation.  However, I don't agree with the idea of\nsending Content-type: application/x-compress, except in a nice\ntheoretical world.  Browsers that don't send Accept-Encoding work quite\nnicely with my server (assuming they understand x-compress).  So I'm\nmore inclined just to ignore Accept-Encoding.\n\nTo John Franks's (apparent) question, what do I send in the absence of\nAccept-Encoding:  I send Content-Type: application/postscript and\nContent-Encoding: x-compress.  I would not send\napplication/octet-stream.\n\nI disagree mildly with Dave Morris's unhappiness that browsers, etc.,\nmight uncompress something for him.  What *I* want varies with the\nsituation.  If I have a viewer for PostScript or PDF, for example, I\nwant the incoming compressed object to be uncompressed and passed to the\nviewer.  OTOH, if I'm saving a file, I *don't* want the object to be\nuncompressed.  (Maybe that's what he meant.)\n\nThanks for the many comments.\n\nDave Kristol\n\n\n\n", "id": "lists-012-7432690"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "On Tue, 3 Nov 1998, Jim Gettys wrote:\n> compress The encoding format produced by the common UNIX file compression \n> program ``compress''. This format is an adaptive Lempel-Ziv-Welch coding \n> (LZW).\n\nYikes!\n\nIf you reference LZW in the HTTP spec, then RFC 2026 section 10.3.2 comes\ninto effect. That means you have to document the Unisys patent in the spec\nand the IETF executive director will have to contact Unisys with a request\nfor openly specified, reasonable, non-discriminatory licensing terms.\n\n- Chris\n\n\n\n", "id": "lists-012-7441635"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "On Tue, 3 Nov 1998, Dave Kristol wrote:\n\n> David W. Morris wrote:\n> > \n> > On Tue, 3 Nov 1998, John Franks wrote:\n> > \n> > > On Tue, 3 Nov 1998, Dave Kristol wrote:\n> > >\n> > > I  think gzip and compress are different encodings.  The fact that\n> > > a program called \"gzip\" understands both is not relevant.\n> \n> [N.B.  I never received John Franks's message, either directly, or via\n> the mailing list.]\n> \n\n> Yup, folks, I really did know they are different encodings.  My point\n> was that perhaps NS 4.5 was able to decode both \"gzip\" and \"compress\"\n> encodings because the gzip *program* can decode both.  Just a hunch,\n> although I was told privately that NS 4.5 probably cannot decode\n> \"compress\".\n> \n\nI just tested and Netscape 4.5 for Unix seems to handle gzip and\ncompress just fine.  Here a file xxx.ps.Z was requested (presumably\nwith Accept-Encoding only containing gzip) and a compressed \nps document was returned with Content-Encoding x-compress and \nContent-type application/postscript.  NS properly uncompressed it\nand handed it off to the postscript viewer.\n\n> I think Koen's remarks were most appropriate about not sending 406 in\n> the absence of negotiation.  However, I don't agree with the idea of\n> sending Content-type: application/x-compress, except in a nice\n> theoretical world.  Browsers that don't send Accept-Encoding work quite\n> nicely with my server (assuming they understand x-compress).  So I'm\n> more inclined just to ignore Accept-Encoding.\n\nI think I agree with this.  It is current practice in HTTP/1.0 so it\nshould continue there, at least.  But I think Dave may be right and \nthis is good behavior in 1.1.  Any disagreement?\n\nGiven that this behavior does not fail ungracefully in current 1.0\nbrowsers, I would assume that 1.1 browsers would also do something\nreasonable (save to disk) when they can't handle a given content encoding.\n\n> \n> To John Franks's (apparent) question, what do I send in the absence of\n> Accept-Encoding:  I send Content-Type: application/postscript and\n> Content-Encoding: x-compress.  I would not send\n> application/octet-stream.\n> \n\nWell, on reflection, I think you are right.  And that's what my server\ndoes too.  :)\n\n\n> I disagree mildly with Dave Morris's unhappiness that browsers, etc.,\n> might uncompress something for him.  What *I* want varies with the\n> situation.  If I have a viewer for PostScript or PDF, for example, I\n> want the incoming compressed object to be uncompressed and passed to the\n> viewer.  OTOH, if I'm saving a file, I *don't* want the object to be\n> uncompressed.  (Maybe that's what he meant.)\n> \n\nAgain I agree with this behavior, but I am not sure the protocol has\nsomething to say about it.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-7450205"}, {"subject": "RE: Web caching documentation (now in PDF and HTML, v 0.6", "content": "Now available in Word, PDF and HTML. Have already incorporated some\nsuggestions (thanks), now at v 0.6.\n\nhttp://www.pobox.com/~mnot/cache_docs/\n\nIf you're interested in helping/discussing this, please send me an\ne-mail here or at <mnot@pobox.com> as it probably isn't within the WG's\nbest interest for me to keep on posting here. Thanks.\n\n\n[ I found it the tiniest bit ironic that so many of the WG wrote to ask\nfor HTML or PDF versions of the document, when the HTTP is not available\nfrom w3 as the same. Perhaps someone could address this? ]\n\n\n\n> -----Original Message-----\nThis isn't precisely on-topic, but is IMHO of interest to the group.\nApologies if it is inappropriate.\n\nI've created a document that explains aspects of caching; what it is,\nwhy it is good, and how to go about it (with server-specific\ninstructions). The target audience of web designers, webmasters and the\nlike - anyone who creates content on the Web. \n\nI'd very much like to get feedback from the group as a first check to\nthe accuracy, usefulness and scope of the document. After that, it will\nbe presented to users to get feedback on its usability, readability,\netc., and then (hopefully) be distributed to the widest possible\naudience. \n\nIt most likely won't be distributed through traditional RFC channels, as\nIMHO it won't get read by the right people there.\n\nIt can be found at\nhttp://www.pobox.com/~mnot/cache_docs/\n[ a MS Word document; if this is a problem, please contact me for\nanother form ]\n\nPLEASE note that this is a preliminary draft, and is not intended for\ndistribution. It is imprecise, and almost certainly contains errors.\nPlease do not give it to anyone who can not contribute, as outlined\nabove. I am open to suggestions both about content, intent and ways to\nfacilitate distribution of the document, along with most anything else.\n\nThanks,\n\n\n\n", "id": "lists-012-7461073"}, {"subject": "RE: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "> If you reference LZW in the HTTP spec, then RFC 2026 section 10.3.2 comes\n> into effect. That means you have to document the Unisys patent in the spec\n> and the IETF executive director will have to contact Unisys with a request\n> for openly specified, reasonable, non-discriminatory licensing terms.\n\nIf you want to bring this to the attention of the IESG, the IESG may decide\nto attach a note to the HTTP draft specification that the use of the\n\"compress\"\nencoding might be covered by patent claims, and I may need to ask the\nimplementations surveyed for implementation of content-encoding whether\nthey have \"taken adequate steps to comply with any such rights, or claimed\nrights\".\n\nHowever, the \"significant implementation and successful operational\nexperience\"\nwith the use of the \"compress\" encoding leads me to assert that there\nis no barrier to the use of \"compress\" in advancing to Draft Standard\nstatus.\n\nI'll review this with the area directors, though.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-7470240"}, {"subject": "Re: Netscape 4.5 and HTTP/1.1 AcceptEncodin", "content": "> From: Chris Newman <Chris.Newman@innosoft.com>\n> Date: Tue, 03 Nov 1998 14:44:39 -0800 (PST)\n> To: Jim Gettys <jg@pa.dec.com>\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: Netscape 4.5 and HTTP/1.1 Accept-Encoding\n> -----\n> On Tue, 3 Nov 1998, Jim Gettys wrote:\n> > compress The encoding format produced by the common UNIX file compression\n> > program ``compress''. This format is an adaptive Lempel-Ziv-Welch coding\n> > (LZW).\n> \n> Yikes!\n> \n> If you reference LZW in the HTTP spec, then RFC 2026 section 10.3.2 comes\n> into effect. That means you have to document the Unisys patent in the spec\n> and the IETF executive director will have to contact Unisys with a request\n> for openly specified, reasonable, non-discriminatory licensing terms.\n> \n>                 - Chris\n\nI don't think we have a problem here: compress is not required for \ninteroperability, is essentially deprecated (by deflate and gzip), and \nit is not a normative reference (beyond saying it is what a program does).\n\nAnd it is documenting existing practice (i.e. was in even RFC 1945, I think).\n- Jim\n\n\n\n", "id": "lists-012-7479028"}, {"subject": "Comments (Part 2) on HTTP ID Rev 0", "content": "For some reason, this has not appeared in the mailing list archive.\n\nIf at first you don't succeed, try, try again.\n- Jim\n\n\nattached mail follows:\n\nFollowing is my second set of comments, covering sections 12 through\n14.9.3. Part\nthree will follow presently.\n\n52. Section 12 uses the phrase \"agent-driven negotiation\"; suggest\nadding a note explaining that \"agent\" refers to \"user agent\".\n\n53. Section 12.2, pg. 68, 1st para., has \"(this specification reserves\nthe field-name Alternates)\", however this field name is not described\nin section 14 as a reserved field name nor otherwise elaborated\nelsewhere in this specification.\n\n54. Throughout the whole of section 13 it is often unclear as to\nwhether a requirement or statement is meant to apply only to a proxy\ncache, to a user agent cache, or to both. For example, in 13.1.1, the\n6th paragraph has \"If the cache can not [sic] communicate with the\norigin server ...\" which appears to apply to either a proxy or a user\nagent cache. On the other hand, the 7th paragraph has \"If a cache\nreceives a response (...) that it would normally forward to the\nrequesting client ...\" which appears to apply to a proxy cache only.\nSuggest editing the entirety of section 13 to clarify applicability to\nthese different caches.\n\n55. Section 13, pg. 69, 3rd para., has \"the protocol requires that\ntransparency be relaxed ... only ... only ...\"; suggest changing to use\nthe keyword phrase \"MUST NOT relax transparency unless\" to make clear\nthe requirement.\n\n56. Section 13.1.1, pg. 70, 1st para., has \"A correct cache MUST\nrespond with the most up-to-date response ...\"; since caching is always\noptional, this would read better as \"A cache MUST NOT respond with a\nresponse held by the cache that is appropriate to the request (...)\nunless it meets one of the following conditions:\".\n\n57. Section 13.1.1, pg. 70, numbered item (3) implies that 4XX and 5XX\nresponses are cachable. While this is true for 410, under what\ncircumstances should any 5XX response be cached?\n\n58. Section 13.1.2, 4th para., has \"whether the Warning MUST or MUST\nNOT be deleted ...\" which does not state a requirement per se: use\n\"is or is not to be deleted ...\".\n\n59. Section 13.1.2, 5th para., has \"Warnings in responses that are\npassed to HTTP/1.0 caches carry an extra warning-date field, which\nprevents a future HTTP/1.1 recipient from believing an erroneously\ncached Warning.\"  I can't interpret this statement based on information\nin this section.  Please explain it further and state as a requirement\nif indeed it is a requirement.\n\n60. Section 13.1.2, 7th para., has \"a server might provide the same\nwarning with texts in both English and Basque\". How would a UA\ndiscrimitate among different warnings using different languages unless\nthe language were explicitly marked? Unfortunately, RFC2047 does not\naddress this issue. I'd suggest permitting the extensions specified by\nRFC2231 (which updates RFC2047) to be used to provide explicit language\ntagging of quoted strings.\n\n61. Section 13.1.3, 2nd para., has \"if there is any apparent conflict\nbetween header values, the most restrictive interpretation is applied\n...\".  Change \"is applied\" to \"MUST\" or \"SHOULD\" \"be applied\".\n\n62. Section 13.1.4, 1st para., change \"the user agent might allow ...\"\nto \"the user agent MAY allow ...\".\n\n63. Section 13.1.4, 2nd para., has \"the user agent SHOULD explicitly\nindicate to the user ...\" while section 13.1.5, 1st para., has \"This\nallows the client software to alert the user\". This later statement\nappears to make the indication/alert optional rather than recommended\nas implied by the former.\n\n64. Section 13.2.3, 3rd para., has \"HTTP/1.1 requires origin servers to\nsend a Date header, if possible, with every response ...\" seems to be\nstating a conditional imperative. Rather than paraphrasing section\n14.18 and possibly confusing the requirements regarding Date header\ntransmission, I'd suggest rephrasing this to simply refer to a Date\nheader, if present, and to state what must be done in the case that a\nDate header is not present.\n\n65. Section 13.2.3, pg. 76, 1st para. after pseudo code block, has \"the\nserver MUST\"; suggest changing to \"the proxy server MUST\".\n\n66. Section 13.3, pg. 78, 1st para., suggest changing \"it first has to\ncheck\" with \"it will normally check\" to make this read more as a\nstatement of fact than a requirement.\n\n67. Section 13.3.3, pg. 81, next to last paragraph, has \"A cache or\norigin server ...\". Change to \"A caching proxy or origin server ...\".\n\n68. Section 13.3.4, pg. 83, 2nd para., starts \"A note on rationale:\n...\"  Suggest changing this to a standard note form, i.e., use \"Note:\"\nprefix with indented block paragraph.\n\n69. Section 13.4, pg. 83, 2nd para., starts \"Note that ...\". Suggest\nchanging this to a standard note form, i.e., use \"Note:\" prefix with\nindented block paragraph. Further, this note has \"some HTTP/1.0 caches\nare known to violate this expectation without providing any Warning.\"\nWhat warning should be provided in this case?\n\n70. Section 13.4, pg. 83, 3rd para., has \"so that the server can\nindicate that certain resource entities, or portions thereof, MUST NOT\nbe cached ...\" which does not appear to state a specific requirement\nper se. Suggest changing to \"... are not to be cached ...\".\n\n71. Section 13.4, pg. 84, 1st para., starts \"Note that ...\". Suggest\nchanging this to a standard note form.\n\n72. Section 13.4, pg. 84, 3rd para.: suggest giving status codes 302\nand 307 as examples of responses which are not cachable by default but\nwhich may be explicitly marked as cachable by using Expires or the\n\"public\" cache-control directive.\n\n73. Section 13.5, 1st para., remove comma in \"to requests, for use ...\".\n\n74. Section 13.5.1, pg. 84, 1st para, 1st bullet, has \"End-to-end\nheaders which MUST be ...\". The use of MUST and SHOULD keywords in\nrelative clauses is problematic and should be avoided since it does not\nstate a requirement per se. Most such instances can be replaced by some\nform of \"is\" to express simple fact; e.g., \"End-to-end headers which\nare ...\".\n\n75. Section 13.5.1, pg. 84, 4th para., has \"Hop-by-hop headers\nintroduced in future versions of HTTP MUST be listed in a Connection\nheader ...\"  stipulates a requirement on the authors and/or\nimplementors of future versions of HTTP, and not on implementors of\nHTTP/1.1. Suggest rephrasing this appropriately.\n\n76. Section 13.5.2, pg. 85, 4th para., has \" ... of the following\nfields in message that ...\" which needs an article \"a\" before \"message\".\n\n77. Section 13.5.2, pg. 85, 5th para., has \"if not already present, it\nMUST add a Warning 214 (...) if one does not already appear ...\" which\nuses redundant language about \"already present\"/\"already appear[ing]\".\n\n78. Section 13.5.3, pg. 86, 5th para., has \"all such old headers are\nreplaced.\" which sounds like a requirement: \"... MUST be replaced.\"\n\n79. Section 13.6, pg. 87, 3rd para., has \"When the cache receives a\nsubsequent request whose Request-URI specifies one or more cache\nentries including a Vary header field, ...\". Suggest changing \"one or\nmore cache entries including a Vary header field\" to \"one or more cache\nentries of previous responses which contained a Vary header field\".\nFurther, it appears that this paragraph implies a caching proxy\ncontext, but it is not clear that this would not also apply to a user\nagent cache. The next paragraph (end of pg. 87 and beginning of pg. 88)\nappears to be framed as applying only to a proxy. Again it isn't clear\nthat this does not apply to a UA cache.\n\n80. Section 13.8, pg. 88, 1st para., implies the context of a caching\nproxy, requiring a 206 response when forwarding a partial response. In\nthe case of a user agent cache that receives and wishes to use a\npartial response, it would seem that a Warning should be added by the\nUA cache to the response it generatese for internal UA consumption.\nHowever, there appears to be no Warning code that would serve this\npurpose.\n\n81. Section 13.11, 1st para., has \"All methods that might be expected\nto cause modifications to the origin server's resources MUST be written\nthrough to the origin server. This currently includes all methods\nexcept for GET and HEAD.\" It would be better here to specify the\nmethods that must be written through explicitly: PUT and DELETE. It\nisn't clear that OPTIONS, POST, or TRACE fall in this category; and\nthen there's CONNECT, which doesn't fit into either of the above\ngroups of methods.\n\n82. Section 14, pg. 91: suggest adding a sentence to each header\ndefined by this section that states whether the header is end-to-end or\nhop-by-hop and whether the header is cachable by default, cachable by\nexplicit cache directive, or never cachable.\n\n83. Section 14.1, pg. 92, 6th para., has \"the most specific reference\nhas precedence\"; suggest using \"SHOULD\" or \"MUST\" \"take precedence\".\n\n84. Section 14.2, pg. 93, 3rd para., is quite confusing: suggest\nrewriting without using the term \"mentioned\". Also, this para. seems to\nbe stating that if any \"iso-8859-1;q=1\" is always implied unless\notherwise explicitly present. This means that:\n\n    Accept-Charset: iso-8859-5, unicode-1-1;q=0.9\n\nreally means\n\n    Accept-Charset: iso-8859-1;q=1, iso-8859-5;q=1, unicode-1-1;q=0.9\n\n(in which case 8859-1 would be given equal billing with 8859-5). And\nthat consequently the only way to exclude 8859-1 is to specify\n\n    Accept-Charset: iso-8859-1;q=0, iso-8859-5, unicode-1-1;q=0.9\n\nIs this the intended usage? If so, I find this not only convoluted but\nseriously sub-optimal. This emphasis on 8859-1 as default really is too\nmuch. Why go so far overboard?\n\n84. Section 14.2, pg. 93, 4th para., has \"the server SHOULD send an\nerror response with the 406 (not acceptable) status code, though the\nsending of an unacceptable response is also allowed.\" The effect of the\nfinal clause of this statement is to downgrade SHOULD to MAY.  Either\nremove the final clause or change to MAY. [My preference is to remove\nthe final clause.]  Note that the semantics stated here are expressly\ndifferent from Accept and Accept-Encoding which do require 406\nresponses for unconditionally compliant implementations.  This\ninconsistency will make it difficult or impossible to implement\nagent-driven content negotiation based on Accept-Charset variants.\n\n85. Section 14.3, pg. 94, 1st para., has \"A server tests whether ...\";\nsuggest changing to \"A server MUST test ...\".\n\n86. Section 14.3, pg. 94, last para., has \"This means that qvalues will\nnot work and are not permitted with x-gzip or x-compress.\" This appears\nto be stating a compatibility requirement, in which case MUST or SHOULD\nis better (in which case this note would have to be made normative).\n\n87. Section 14.4, pg. 95, next to last para., has \"recommended\" and\n\"MUST NOT\" in a note. Either make this normative (not a note) and use\nSHOULD and MUST NOT consistently or use \"ought\" and \"ought not\",\nrespectively.\n\n88. Section 14.4 does not contain language as found in other Accept-*\nheaders that recommends a 406 response in the case the server cannot\nsatisfy the request based on its variant set for the specified URI.\nThis precludes implementing client-side content negotiation along this\nvariance axis. Suggest adding the required language or a note\nindicating why it is not present and what this means for client-side\nnegotiation.\n\n89. Section 14.5. I find \"Accept-Ranges\" to be inconsistent in its name\nand usage with other Accept-* headers. This really should have been\nhandled with Expect. Unless you can change this to use Expect (and I\ndoubt you can at this stage), I'd suggest adding a note to indicate\nthis inconsistency. I'd also urge adding a mechanism which does use\nExpect and deprecates the use of Accept-Ranges in a future version of\nHTTP. [If \"Allow-Ranges\" had been used instead, at least this would be\nconsistent with the \"Allow\" header usage.]\n\nRegarding the actual use of Accept-Ranges, which response would be\nappropriate for a server which sends \"Accept-Ranges: none\" in response\nto a Range request?  406? Some mention of the appropriate response\nshould be made here.\n\n90. Section 14.8, pg. 97-98, seems to imply a caching proxy when\nreferring to \"shared cache\"; however, this seems to apply as well to a\nshared cache on a user agent. Suggest making it clear which kinds of\ncaches are addressed by these paragraphs.\n\n91. Throughout the whole of section 14.9, it is often unclear as to\nwhether a requirement or statement is meant to apply only to a proxy\ncache, to a user agent cache, or to both.\n\n92. Section 14.9, pg. 98, 1st para., has \"that MUST be obeyed by all\ncaching mechanisms\" which does not specify a requirement per se (note\nuse of MUST in relative clause).\n\n93. Section 14.9, pg. 99, 1st para., has \"When a directive appears\nwithout any 1#field-name parameter, the directive applies to the entire\nrequest or response.\" At the present, no cache-request-directive\nemploys a 1#field-name parameter (see pg. 98); consequently all request\ndirectives apply to the entire request in all cases.\n\n94. Section 14.9.1, pg. 99, 2nd para., has \"Indicates that the response\nis cachable by any cache, ...\". Suggest changing \"is cachable\" to \"MAY\nbe cached\".\n\n95. Section 14.9.1, pg. 99, last para., has keyword MUST NOT in\nresultative clause \"Indicates that ...\"; suggest rephrasing as\nimperative, e.g., \"A response containing the cache directive 'private'\nMUST NOT be cached by a shared cache.\".\n\n96. Section 14.9.1, pg. 100, 2nd para. under \"no-cache\", has \"the\nspecified field-name(s) MUST NOT be sent in the response to a\nsubsequent request without successful revalidation with the origin\nserver.\" followed by \"This allows an origin server to prevent the\nre-use of certain header fields in a response, while still allowing\ncaching of the rest of the response.\" I find this rather confusing. My\nreading of this is that a cache can, indeed, retain and reuse a field\nspecified in a no-cache directive as long as it revalidates the entry\nwith the origin server.  Furthermore, it appears that \"no-cache\" with\nno field name is to be interpreted identically to must-revalidate. I\nhave always interpreted no-cache without a field-name to mean don't\ncache the response under any circumstances. Which is it?\n\n97. Section 14.9.2, pg. 100, 1st para., needs to do a better job of\ndefining \"store\" without using keywords in the term or definition.\nAlso, it would be better to place this definition at the beginning of\nthis section. I would suggest a rewrite as follows:\n\n\"The purpose of the no-store directive is to prevent the inadvertent\nrelease or retention of sensitive information. In the present context,\n'store' means to intentionally retain data in non-volatile storage.\nThe no-store directive applies to the entire message, and MAY be sent\neither in a response or in a request. If sent in a request, a cache\nMUST NOT store any part of the request or its response. If sent in a\nresponse, a cache MUST NOT store any part of either the response or the\nrequest that elicitied it. This directive applies to both shared and\nnon-shared caches.\"\n\nIn this rewrite, I've removed the statement about removing data from\nvolatile storage, since this doesn't seem to apply to the semantics of\nthis directive.  In particular, many caches employ a volatile and a\nnon-volatile component.  The described semantics appears to strictly\naddress use of the non-volatile component, and not the volatile\ncomponent. If this is not the case, then these semantics would appear\nto broaden the stated intention found in the last paragraph on pg.\n100. Furthermore, this directive applies equally to a user agent cache\nas well as a caching proxy, so language aimed at proxies (e.g., \"after\nforwarding it\") appears to be overly narrow in scope.\n\n98. Section 14.9.3, pg. 101, 2nd para., has \"the max-age directive\noverrides the Expires header\".  What is the imperative status of this\nstatement? Should this read \"MUST override\"?\n\n99. Section 14.9.3, pg. 101, 5th para. (s-maxage), has \"i.e., that the\nshared cache MUST NOT ...\" which doesn't state a requirement per se.\nRewrite to avoid using MUST NOT in an explanatory relative clause\n(particularly one using \"i.e.\").\n\n100. Section 14.9.3, pg. 101, 5th para. (s-maxage), has \"The s-maxage\ndirective is always ignored by a private cache\". Should this read \"A\nprivate cache MUST ignore the s-maxage directive.\"?\n\n101. Section 14.9.3, pg. 102, 7th para., has \"only if this does not\nconflict with any MUST-level requirements\"; suggest rephrasing as \"with\nany conditionally compliant requirements\" to avoid using MUST in this\ncontext which is not stating a requirement per se.\n\n\n\n", "id": "lists-012-7488192"}, {"subject": "Comments (Part 2) on HTTP ID Rev 0", "content": "Following is my second set of comments, covering sections 12 through\n14.9.3. Part\nthree will follow presently.\n\n52. Section 12 uses the phrase \"agent-driven negotiation\"; suggest\nadding a note explaining that \"agent\" refers to \"user agent\".\n\n53. Section 12.2, pg. 68, 1st para., has \"(this specification reserves\nthe field-name Alternates)\", however this field name is not described\nin section 14 as a reserved field name nor otherwise elaborated\nelsewhere in this specification.\n\n54. Throughout the whole of section 13 it is often unclear as to\nwhether a requirement or statement is meant to apply only to a proxy\ncache, to a user agent cache, or to both. For example, in 13.1.1, the\n6th paragraph has \"If the cache can not [sic] communicate with the\norigin server ...\" which appears to apply to either a proxy or a user\nagent cache. On the other hand, the 7th paragraph has \"If a cache\nreceives a response (...) that it would normally forward to the\nrequesting client ...\" which appears to apply to a proxy cache only.\nSuggest editing the entirety of section 13 to clarify applicability to\nthese different caches.\n\n55. Section 13, pg. 69, 3rd para., has \"the protocol requires that\ntransparency be relaxed ... only ... only ...\"; suggest changing to use\nthe keyword phrase \"MUST NOT relax transparency unless\" to make clear\nthe requirement.\n\n56. Section 13.1.1, pg. 70, 1st para., has \"A correct cache MUST\nrespond with the most up-to-date response ...\"; since caching is always\noptional, this would read better as \"A cache MUST NOT respond with a\nresponse held by the cache that is appropriate to the request (...)\nunless it meets one of the following conditions:\".\n\n57. Section 13.1.1, pg. 70, numbered item (3) implies that 4XX and 5XX\nresponses are cachable. While this is true for 410, under what\ncircumstances should any 5XX response be cached?\n\n58. Section 13.1.2, 4th para., has \"whether the Warning MUST or MUST\nNOT be deleted ...\" which does not state a requirement per se: use\n\"is or is not to be deleted ...\".\n\n59. Section 13.1.2, 5th para., has \"Warnings in responses that are\npassed to HTTP/1.0 caches carry an extra warning-date field, which\nprevents a future HTTP/1.1 recipient from believing an erroneously\ncached Warning.\"  I can't interpret this statement based on information\nin this section.  Please explain it further and state as a requirement\nif indeed it is a requirement.\n\n60. Section 13.1.2, 7th para., has \"a server might provide the same\nwarning with texts in both English and Basque\". How would a UA\ndiscrimitate among different warnings using different languages unless\nthe language were explicitly marked? Unfortunately, RFC2047 does not\naddress this issue. I'd suggest permitting the extensions specified by\nRFC2231 (which updates RFC2047) to be used to provide explicit language\ntagging of quoted strings.\n\n61. Section 13.1.3, 2nd para., has \"if there is any apparent conflict\nbetween header values, the most restrictive interpretation is applied\n...\".  Change \"is applied\" to \"MUST\" or \"SHOULD\" \"be applied\".\n\n62. Section 13.1.4, 1st para., change \"the user agent might allow ...\"\nto \"the user agent MAY allow ...\".\n\n63. Section 13.1.4, 2nd para., has \"the user agent SHOULD explicitly\nindicate to the user ...\" while section 13.1.5, 1st para., has \"This\nallows the client software to alert the user\". This later statement\nappears to make the indication/alert optional rather than recommended\nas implied by the former.\n\n64. Section 13.2.3, 3rd para., has \"HTTP/1.1 requires origin servers to\nsend a Date header, if possible, with every response ...\" seems to be\nstating a conditional imperative. Rather than paraphrasing section\n14.18 and possibly confusing the requirements regarding Date header\ntransmission, I'd suggest rephrasing this to simply refer to a Date\nheader, if present, and to state what must be done in the case that a\nDate header is not present.\n\n65. Section 13.2.3, pg. 76, 1st para. after pseudo code block, has \"the\nserver MUST\"; suggest changing to \"the proxy server MUST\".\n\n66. Section 13.3, pg. 78, 1st para., suggest changing \"it first has to\ncheck\" with \"it will normally check\" to make this read more as a\nstatement of fact than a requirement.\n\n67. Section 13.3.3, pg. 81, next to last paragraph, has \"A cache or\norigin server ...\". Change to \"A caching proxy or origin server ...\".\n\n68. Section 13.3.4, pg. 83, 2nd para., starts \"A note on rationale:\n...\"  Suggest changing this to a standard note form, i.e., use \"Note:\"\nprefix with indented block paragraph.\n\n69. Section 13.4, pg. 83, 2nd para., starts \"Note that ...\". Suggest\nchanging this to a standard note form, i.e., use \"Note:\" prefix with\nindented block paragraph. Further, this note has \"some HTTP/1.0 caches\nare known to violate this expectation without providing any Warning.\"\nWhat warning should be provided in this case?\n\n70. Section 13.4, pg. 83, 3rd para., has \"so that the server can\nindicate that certain resource entities, or portions thereof, MUST NOT\nbe cached ...\" which does not appear to state a specific requirement\nper se. Suggest changing to \"... are not to be cached ...\".\n\n71. Section 13.4, pg. 84, 1st para., starts \"Note that ...\". Suggest\nchanging this to a standard note form.\n\n72. Section 13.4, pg. 84, 3rd para.: suggest giving status codes 302\nand 307 as examples of responses which are not cachable by default but\nwhich may be explicitly marked as cachable by using Expires or the\n\"public\" cache-control directive.\n\n73. Section 13.5, 1st para., remove comma in \"to requests, for use ...\".\n\n74. Section 13.5.1, pg. 84, 1st para, 1st bullet, has \"End-to-end\nheaders which MUST be ...\". The use of MUST and SHOULD keywords in\nrelative clauses is problematic and should be avoided since it does not\nstate a requirement per se. Most such instances can be replaced by some\nform of \"is\" to express simple fact; e.g., \"End-to-end headers which\nare ...\".\n\n75. Section 13.5.1, pg. 84, 4th para., has \"Hop-by-hop headers\nintroduced in future versions of HTTP MUST be listed in a Connection\nheader ...\"  stipulates a requirement on the authors and/or\nimplementors of future versions of HTTP, and not on implementors of\nHTTP/1.1. Suggest rephrasing this appropriately.\n\n76. Section 13.5.2, pg. 85, 4th para., has \" ... of the following\nfields in message that ...\" which needs an article \"a\" before \"message\".\n\n77. Section 13.5.2, pg. 85, 5th para., has \"if not already present, it\nMUST add a Warning 214 (...) if one does not already appear ...\" which\nuses redundant language about \"already present\"/\"already appear[ing]\".\n\n78. Section 13.5.3, pg. 86, 5th para., has \"all such old headers are\nreplaced.\" which sounds like a requirement: \"... MUST be replaced.\"\n\n79. Section 13.6, pg. 87, 3rd para., has \"When the cache receives a\nsubsequent request whose Request-URI specifies one or more cache\nentries including a Vary header field, ...\". Suggest changing \"one or\nmore cache entries including a Vary header field\" to \"one or more cache\nentries of previous responses which contained a Vary header field\".\nFurther, it appears that this paragraph implies a caching proxy\ncontext, but it is not clear that this would not also apply to a user\nagent cache. The next paragraph (end of pg. 87 and beginning of pg. 88)\nappears to be framed as applying only to a proxy. Again it isn't clear\nthat this does not apply to a UA cache.\n\n80. Section 13.8, pg. 88, 1st para., implies the context of a caching\nproxy, requiring a 206 response when forwarding a partial response. In\nthe case of a user agent cache that receives and wishes to use a\npartial response, it would seem that a Warning should be added by the\nUA cache to the response it generatese for internal UA consumption.\nHowever, there appears to be no Warning code that would serve this\npurpose.\n\n81. Section 13.11, 1st para., has \"All methods that might be expected\nto cause modifications to the origin server's resources MUST be written\nthrough to the origin server. This currently includes all methods\nexcept for GET and HEAD.\" It would be better here to specify the\nmethods that must be written through explicitly: PUT and DELETE. It\nisn't clear that OPTIONS, POST, or TRACE fall in this category; and\nthen there's CONNECT, which doesn't fit into either of the above\ngroups of methods.\n\n82. Section 14, pg. 91: suggest adding a sentence to each header\ndefined by this section that states whether the header is end-to-end or\nhop-by-hop and whether the header is cachable by default, cachable by\nexplicit cache directive, or never cachable.\n\n83. Section 14.1, pg. 92, 6th para., has \"the most specific reference\nhas precedence\"; suggest using \"SHOULD\" or \"MUST\" \"take precedence\".\n\n84. Section 14.2, pg. 93, 3rd para., is quite confusing: suggest\nrewriting without using the term \"mentioned\". Also, this para. seems to\nbe stating that \"iso-8859-1;q=1\" is always implied unless\notherwise explicitly present. This means that:\n\n    Accept-Charset: iso-8859-5, unicode-1-1;q=0.9\n\nreally means\n\n    Accept-Charset: iso-8859-1;q=1, iso-8859-5;q=1, unicode-1-1;q=0.9\n\n(in which case 8859-1 would be given equal billing with 8859-5). And\nthat consequently the only way to exclude 8859-1 is to specify\n\n    Accept-Charset: iso-8859-1;q=0, iso-8859-5, unicode-1-1;q=0.9\n\nIs this the intended usage? If so, I find this not only convoluted but\nseriously sub-optimal. This emphasis on 8859-1 as default really is too\nmuch. Why go so far overboard?\n\n84. Section 14.2, pg. 93, 4th para., has \"the server SHOULD send an\nerror response with the 406 (not acceptable) status code, though the\nsending of an unacceptable response is also allowed.\" The effect of the\nfinal clause of this statement is to downgrade SHOULD to MAY.  Either\nremove the final clause or change to MAY. [My preference is to remove\nthe final clause.]  Note that the semantics stated here are expressly\ndifferent from Accept and Accept-Encoding which do require 406\nresponses for unconditionally compliant implementations.  This\ninconsistency will make it difficult or impossible to implement\nagent-driven content negotiation based on Accept-Charset variants.\n\n85. Section 14.3, pg. 94, 1st para., has \"A server tests whether ...\";\nsuggest changing to \"A server MUST test ...\".\n\n86. Section 14.3, pg. 94, last para., has \"This means that qvalues will\nnot work and are not permitted with x-gzip or x-compress.\" This appears\nto be stating a compatibility requirement, in which case MUST or SHOULD\nis better (in which case this note would have to be made normative).\n\n87. Section 14.4, pg. 95, next to last para., has \"recommended\" and\n\"MUST NOT\" in a note. Either make this normative (not a note) and use\nSHOULD and MUST NOT consistently or use \"ought\" and \"ought not\",\nrespectively.\n\n88. Section 14.4 does not contain language as found in other Accept-*\nheaders that recommends a 406 response in the case the server cannot\nsatisfy the request based on its variant set for the specified URI.\nThis precludes implementing client-side content negotiation along this\nvariance axis. Suggest adding the required language or a note\nindicating why it is not present and what this means for client-side\nnegotiation.\n\n89. Section 14.5. I find \"Accept-Ranges\" to be inconsistent in its name\nand usage with other Accept-* headers. This really should have been\nhandled with Expect. Unless you can change this to use Expect (and I\ndoubt you can at this stage), I'd suggest adding a note to indicate\nthis inconsistency. I'd also urge adding a mechanism which does use\nExpect and deprecates the use of Accept-Ranges in a future version of\nHTTP. [If \"Allow-Ranges\" had been used instead, at least this would be\nconsistent with the \"Allow\" header usage.]\n\nRegarding the actual use of Accept-Ranges, which response would be\nappropriate for a server which sends \"Accept-Ranges: none\" in response\nto a Range request?  406? Some mention of the appropriate response\nshould be made here.\n\n90. Section 14.8, pg. 97-98, seems to imply a caching proxy when\nreferring to \"shared cache\"; however, this seems to apply as well to a\nshared cache on a user agent. Suggest making it clear which kinds of\ncaches are addressed by these paragraphs.\n\n91. Throughout the whole of section 14.9, it is often unclear as to\nwhether a requirement or statement is meant to apply only to a proxy\ncache, to a user agent cache, or to both.\n\n92. Section 14.9, pg. 98, 1st para., has \"that MUST be obeyed by all\ncaching mechanisms\" which does not specify a requirement per se (note\nuse of MUST in relative clause).\n\n93. Section 14.9, pg. 99, 1st para., has \"When a directive appears\nwithout any 1#field-name parameter, the directive applies to the entire\nrequest or response.\" At the present, no cache-request-directive\nemploys a 1#field-name parameter (see pg. 98); consequently all request\ndirectives apply to the entire request in all cases.\n\n94. Section 14.9.1, pg. 99, 2nd para., has \"Indicates that the response\nis cachable by any cache, ...\". Suggest changing \"is cachable\" to \"MAY\nbe cached\".\n\n95. Section 14.9.1, pg. 99, last para., has keyword MUST NOT in\nresultative clause \"Indicates that ...\"; suggest rephrasing as\nimperative, e.g., \"A response containing the cache directive 'private'\nMUST NOT be cached by a shared cache.\".\n\n96. Section 14.9.1, pg. 100, 2nd para. under \"no-cache\", has \"the\nspecified field-name(s) MUST NOT be sent in the response to a\nsubsequent request without successful revalidation with the origin\nserver.\" followed by \"This allows an origin server to prevent the\nre-use of certain header fields in a response, while still allowing\ncaching of the rest of the response.\" I find this rather confusing. My\nreading of this is that a cache can, indeed, retain and reuse a field\nspecified in a no-cache directive as long as it revalidates the entry\nwith the origin server.  Furthermore, it appears that \"no-cache\" with\nno field name is to be interpreted identically to must-revalidate. I\nhave always interpreted no-cache without a field-name to mean don't\ncache the response under any circumstances. Which is it?\n\n97. Section 14.9.2, pg. 100, 1st para., needs to do a better job of\ndefining \"store\" without using keywords in the term or definition.\nAlso, it would be better to place this definition at the beginning of\nthis section. I would suggest a rewrite as follows:\n\n\"The purpose of the no-store directive is to prevent the inadvertent\nrelease or retention of sensitive information. In the present context,\n'store' means to intentionally retain data in non-volatile storage.\nThe no-store directive applies to the entire message, and MAY be sent\neither in a response or in a request. If sent in a request, a cache\nMUST NOT store any part of the request or its response. If sent in a\nresponse, a cache MUST NOT store any part of either the response or the\nrequest that elicitied it. This directive applies to both shared and\nnon-shared caches.\"\n\nIn this rewrite, I've removed the statement about removing data from\nvolatile storage, since this doesn't seem to apply to the semantics of\nthis directive.  In partice this doesn't seem to apply to the semantics of\nthis directive.  In particular, many caches employ a volatile and a\nnon-volatile component.  The described semantics appears to strictly\naddress use of the non-volatile component, and not the volatile\ncomponent. If this is not the case, then these semantics would appear\nto broaden the stated intention found in the last paragraph on pg.\n100. Furthermore, this directive applies equally to a user agent cache\nas well as a caching proxy, so language aimed at proxies (e.g., \"after\nforwarding it\") appears to be overly narrow in scope.\n\n98. Section 14.9.3, pg. 101, 2nd para., has \"the max-age directive\noverrides the Expires header\".  What is the imperative status of this\nstatement? Should this read \"MUST override\"?\n\n99. Section 14.9.3, pg. 101, 5th para. (s-maxage), has \"i.e., that the\nshared cache MUST NOT ...\" which doesn't state a requirement per se.\nRewrite to avoid using MUST NOT in an explanatory relative clause\n(particularly one using \"i.e.\").\n\n100. Section 14.9.3, pg. 101, 5th para. (s-maxage), has \"The s-maxage\ndirective is always ignored by a private cache\". Should this read \"A\nprivate cache MUST ignore the s-maxage directive.\"?\n\n101. Section 14.9.3, pg. 102, 7th para., has \"only if this does not\nconflict with any MUST-level requirements\"; suggest rephrasing as \"with\nany conditionally compliant requirements\" to avoid using MUST in this\ncontext which is not stating a requirement per se.\n\n\n\n", "id": "lists-012-7512779"}, {"subject": "HTTP 1.1 rev 5 issues under separate cove", "content": "My appologies to those of you who've seen a few of these before, but for some\nreason a bunch of them didn't make it to the list.  With the exception of\none that did and was answered (\"HTTP 1.1 issue 01: General notes\"), I'm\nabout to repost the whole mess.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n--------------------- Note Body ------------------\nFrom: Ross Patterson <RossP@SS1.Reston.VMD.Sterling.Com>\nTo: http-wg@hplb.hpl.hp.com\nDate: Tue, 20 Oct 98 18:38:00 EDT\nSubject: HTTP 1.1 rev 5 issues under separate cover\n\nAs part of an evaluation of the impacts of HTTP 1.1 for my company's\nVM:Webgateway product, I've been working on locating and summarizing all\nthe MUST/MAY/SHOULD requirements in the current level of the spec\n(draft-ietf-http-v11-spec-rev-05). In the process, I've found a number\nof issues that I believe warrant consideration and possibly action.\nSome are just editorial, but some aren't.  In my opinion none of them\nshould stand in the way of standards advancement.\n\nI'm sending each issue as a separate note following this one, as an aid\nto folks like me who read mail by threads.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n--------------------- Note End -------------------\n\n\n\n", "id": "lists-012-7537585"}, {"subject": "HTTP 1.1 issue 02: 3.11 Entity Tag", "content": "In section 3.11 \"Entity Tags\", the statement\n\n   \"A given entity tag value MAY be used for entities obtained by\n   requests on different URIs without implying anything about the\n   equivalence of those entities.\"\n\nconfuses me to the point of being unable to parse it, let alone\nsummarize it.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7545679"}, {"subject": "HTTP 1.1 issue 03: 4.1 General Synta", "content": "In section 4.1 \"General Syntax\", the statements\n\n   \"In the interest of robustness, servers SHOULD ignore any empty\n   line(s) received where a Request-Line is expected.\"\n\nand\n\n   \"In other words, if the server is reading the protocol stream at\n   the beginning of a message and receives a CRLF first, it SHOULD\n   ignore the CRLF.\"\n\nstate the same requirement in different forms.  Unless they're needed\nfor other reasons, I suggest either deleting one of them or changing one\nof the \"SHOULD\"s to \"should\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7553006"}, {"subject": "HTTP 1.1 issue 04: 4.2 Message Header", "content": "In section 4.2 \"Message Headers\", the statement\n\n   \"Applications SHOULD follow \"common form\", where one is known or\n   indicated, when generating HTTP constructs, since there might\n   exist some implementations that fail to accept anything beyond the\n   common forms.\"\n\nis so vague as to be impossible to measure, and should therefore not be\nnormative and a requirement of compliance. I've brought this one up\nbefore, so if the general opinion goes against me I won't complain.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7559915"}, {"subject": "HTTP 1.1 issue 05: 4.2 Message Header", "content": "In section 4.2 \"Message Headers\", the statement\n\n   \"It MUST be possible to combine the multiple header fields into\n   one \"field-name: field-value\" pair, without changing the semantics\n   of the message, by appending each subsequent field-value to the\n   first, each separated by a comma.\"\n\nis a restatement of the preceding requirement,\n\n   \"Multiple message-header fields with the same field-name MAY be\n   present in a message if and only if the entire field-value for\n   that header field is defined as a comma-separated list [i.e.,\n   #(values)].\"\n\nplacing a requirement upon future extensions to the protocol rather than\non implementations of HTTP 1.1.  I think the \"MUST\" should be changed to\na \"must\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7567314"}, {"subject": "HTTP 1.1 issue 06: 4.4 Message Lengt", "content": "In section 4.4 \"Message Length\", the statement\n\n   \"Any response message which MUST NOT include a message-body (such\n   as the 1xx, 204, and 304 responses and any response to a HEAD\n   request) is always terminated by the first empty line after the\n   header fields, regardless of the entity-header fields present in\n   the message.\"\n\nrefers to a requirement, it doesn't state one.  The \"MUST NOT\" should be\nchanged to \"must not\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7574448"}, {"subject": "HTTP 1.1 issue 10: 10.3.5 304 Not Modifie", "content": "In section 10.3.5 \"304 Not Modified\", the \"MUST NOT include a\nmessage-body\" requirement is stated twice in this section, at the top\nand bottom.  One of them should be deleted.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7581789"}, {"subject": "HTTP 1.1 issue 11: 10.3.7 307 Temporary Redirec", "content": "In section 10.3.7 \"307 Temporary Redirect\", the \"SHOULD contain a short\nhypertext note\" requirement is stated twice in this section, in the\nsecond paragraph and at the bottom. One of them should be deleted.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7588927"}, {"subject": "HTTP 1.1 issue 14: 14.20.1 Expect 100continu", "content": "In section 14.20.1 \"Expect 100-continue\", the statement\n\n   \"Proxies SHOULD maintain a cache recording the HTTP version\n   numbers received from recently-referenced next-hop servers.\"\n\nduplicates the same requirement in section 8.2.4.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7596197"}, {"subject": "HTTP 1.1 issue 19: 19.4.1 MIMEVersio", "content": "Section 19.4.1 \"MIME-Version\" contains the statement\n\n   \"HTTP is not a MIME-compliant protocol (see appendix 19.4).\"\n\nbut 19.4.1 *IS* in appendix 19.4.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7603433"}, {"subject": "HTTP 1.1 issue 12: 10.4.8 407 Proxy Authentication Require", "content": "In section 10.4.8 \"407 Proxy Authentication Required\", the statement\n\n   \"This code is similar to 401 (Unauthorized), but indicates that\n   the client MUST first authenticate itself with the proxy.\"\n\nis not a requirement, so the \"MUST\" should be changed to \"must\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7610530"}, {"subject": "HTTP 1.1 issue 09: 8.1.2.1 Negotiatio", "content": "In section 8.1.2.1 \"Negotiation\", the statement\n\n   \"An HTTP/1.1 client MAY expect a connection to remain open, but\n   would decide to keep it open based on whether the response from a\n   server contains a Connection header with the connection-token\n   close.\"\n\ndoes not state a requirement, the \"MAY\" should probably be \"may\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7617955"}, {"subject": "HTTP 1.1 issue 13: 11 Access Authenticatio", "content": "In section 11 \"Access Authentication\", the statement\n\n   \"HTTP provides several OPTIONAL challenge-response authentication\n   mechanisms which MAY be used by a server to challenge a client\n   request and by a client to provide authentication information.\"\n\nis not a requirement, so the \"MAY\" should be changed to \"may\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7624732"}, {"subject": "HTTP 1.1 issue 08: 8.1.2 Overall Operatio", "content": "In section 8.1.2 \"Overall Operation\", the statement\n\n   \"Persistent connections provide a mechanism by which a client and\n   a server can signal the close of a TCP connection. This signaling\n   takes place using the Connection header field (section 14.10).\n   Once a close has been signaled, the client MUST not send any more\n   requests on that connection.\"\n\nshould probably read \"MUST NOT\" instead of \"MUST not\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7632056"}, {"subject": "HTTP 1.1 issue 07: 4.4 Message Lengt", "content": "In section 4.4 \"Message Length\", does the statement\n\n   \"If a Content-Length header field (section 14.13) is present, its\n   decimal value in OCTETs represents both the entity-length and the\n   transfer-length. The Content-Length header field MUST NOT be used\n   if these two lengths are different (i.e., if a Transfer-Encoding\n   header field is present).\"\n\nmean that a receiver should ignore Content-Length, or a sender should\nnot send it?\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7638916"}, {"subject": "HTTP 1.1 issue 18: 14.46 Warnin", "content": "In section 14.46 \"Warning\", the statement\n\n   \"The warn-code consists of three digits. The first digit indicates\n   whether the Warning MUST or MUST NOT be deleted from a stored\n   cache entry after a successful revalidation:\"\n\ndoes not specify a requirement.  The \"MUST or MUST NOT\" should be\nchanged to \"must or must not\"\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7645731"}, {"subject": "HTTP 1.1 issue 20: 13.1.2 Warning", "content": "In section 13.1.2 \"Warnings\", the statement\n\n   \"The first digit indicates whether the Warning MUST or MUST NOT be\n   deleted from a cached response after it is successfully\n   revalidated.\n\ndoes not specify a requirement.  The \"MUST or MUST NOT\" should be\nchanged to \"must or must not\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7652884"}, {"subject": "HTTP 1.1 issue 17: 14.36 Refere", "content": "In section 14.36 \"Referer\", the statement\n\n   \"If the field value is a partial URI, it SHOULD be interpreted\n   relative to the Request-URI.\"\n\nsounds more like a MUST than a SHOULD. An interoperability problem will\nresult if two implementations interpret the referring URI differently.\nIf the SHOULD is due to compatibility with earlier usage, that should be\nnoted someplace.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7660004"}, {"subject": "HTTP 1.1 issue 24: 13.12 Cache Replacemen", "content": "In section 13.12 \"Cache Replacement\", the statement\n\n\"If it inserts the new response into cache storage it\nSHOULD follow the rules in section 13.5.3.\"\n\ncould be construed as downgrading several MUSTs in 13.5.3 to SHOULDs.\nThe \"SHOULD\" should either be replaced with \"should\" or this requirement\nshould be upgraded to a \"MUST\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7667199"}, {"subject": "HTTP 1.1 issue 22: 13.4 Response Cachabilit", "content": "In section 13.4 \"Response Cachability\", the statement\n\n   \"Certain cache-control directives are therefore provided so that\n   the server can indicate that certain resource entities, or\n   portions thereof, MUST NOT be cached regardless of other\n   considerations.\"\n\ndoes not specify a requirement. The \"MUST or MUST NOT\" should be changed\nto \"must or must not\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7674493"}, {"subject": "Th th th th that's all folks", "content": "Thanks for reading the foregoing pile of notes about the spec draft. If\nyou're interested in the requirement table i've been working on, the\ncurrent draft is available at\nhttp://WWW.GeoCities.Com/SiliconValley/Garage/3246/http11rq.txt for your\nviewing pleasure.  Please remember that it is a work in progress, but\nyour comments are very welcome.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7681824"}, {"subject": "HTTP 1.1 issue 21: 13.3.3 Weak and Strong Validator", "content": "In section 13.3.3 \"Weak and Strong Validators\", the statements\n\n   \"The weak comparison function MAY be used for simple\n   (non-subrange) GET requests. The strong comparison function MUST\n   be used in all other cases.\"\n\nand\n\n   \"A cache or origin server receiving a conditional request, other\n   than a full-body GET request, MUST use the strong comparison\n   function to evaluate the condition.\"\n\nstate the same requirement.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7688582"}, {"subject": "HTTP 1.1 issue 16: 14.26 If-NoneMatc", "content": "In section 14.26 \"If-None-Match\", the statements\n\n   \"If \"*\" is given and no current entity exists, then the server MAY\n   perform the requested method as if the If-None-Match header field\n   did not exist.\"\n\nand\n\n   \"The meaning of \"If-None-Match: *\" is that the method MUST NOT be\n   performed if the representation selected by the origin server (or\n   by a cache, possibly using the Vary mechanism, see section 14.44)\n   exists, and SHOULD be performed if the representation does not\n   exist.\"\n\nconflict on the topic of \"If-None-Match: *\". The former asserts that the\nserver MAY perform the request, the latter that it SHOULD.  I think the\nMAY is correct.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7696064"}, {"subject": "HTTP 1.1 issue 23: 13.5.2 Nonmodifiable Header", "content": "In section 13.5.2 \"Non-modifiable Headers\", the statement\n\n   \"A non-transparent proxy MAY modify or add these fields to a\n   message that does not include no-transform, but if it does so, if\n   not already present, it MUST add a Warning 214 (Transformation\n   applied) if one does not already appear in the message (see\n   section 14.46).\"\n\nappears to be in error.  It allows non-transparent proxies to make\ncertain changes to messages that do not contain Cache-Control:\nno-transform.  The discussion of the no-transform directive in section\n14.9.5 doesn't jive, and makes it look like this paragraph should read\n\"... that contains no-transform ...\", just as the immediately prior\nparagraph does.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7703706"}, {"subject": "HTTP 1.1 issue 15: 14.23 Hos", "content": "In section 14.23 \"Host\", the statements\n\n   \"A client MUST include a Host header field in all HTTP/1.1 request\n   messages on the Internet (i.e., on any message corresponding to a\n   request for a URL which includes an Internet host address for the\n   service being requested). If the Host field is not already\n   present, an HTTP/1.1 proxy MUST add a Host field to the request\n   message prior to forwarding it on the Internet. All Internet-based\n   HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code\n   to any HTTP/1.1 request message which lacks a Host header field.\"\n\ncan be interpreted to relax the requirement when requests are not\ntransiting the public Internet (e.g., between a client and server on a\ndepartmental LAN). I believe the intent is to make HOST a required\nrequest header whenever TCP/IP is the vehicle for the HTTP conversation.\nIf so, these statements should be clarified.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7711399"}, {"subject": "Re: Comments (Part 2) on HTTP ID Rev 0", "content": "\"Adams, Glenn\" <gadams@spyglass.com> writes:\n\n>54. Throughout the whole of section 13 it is often unclear as to\n>whether a requirement or statement is meant to apply only to a proxy\n>cache, to a user agent cache, or to both.\n\nThere's a third class to consider as well, although I don't know of any\nexamples to cite: the origin-server cache.  While a proxie cache has a\ncombined client and server, an origin-server cache would not have a\nclient component (or at least not in use in a given transaction).\n\n>64. Section 13.2.3, 3rd para., has \"HTTP/1.1 requires origin servers to\n>send a Date header, if possible, with every response ...\" seems to be\n>stating a conditional imperative. Rather than paraphrasing section\n>14.18 and possibly confusing the requirements regarding Date header\n>transmission, I'd suggest rephrasing this to simply refer to a Date\n>header, if present, and to state what must be done in the case that a\n>Date header is not present.\n\nGood point.\n\n>65. Section 13.2.3, pg. 76, 1st para. after pseudo code block, has \"the\n>server MUST\"; suggest changing to \"the proxy server MUST\".\n\nWhile this isn't my forte, I believe the text is correct, as it applies\nto all server-caches, not just proxy-caches.\n\n>74. Section 13.5.1, pg. 84, 1st para, 1st bullet, has \"End-to-end\n>headers which MUST be ...\". The use of MUST and SHOULD keywords in\n>relative clauses is problematic and should be avoided since it does not\n>state a requirement per se.\n\nI don't understand your meaning of \"relative clauses\", however this case\nin particular does state a requirement, that intermediaries forward\nend-to-end headers to the ultimate recipient.  This requirement isn't\nstated anywhere else in the document, and must be retained.\n\n>78. Section 13.5.3, pg. 86, 5th para., has \"all such old headers are\n>replaced.\" which sounds like a requirement: \"... MUST be replaced.\"\n\nThe requirement is already stated two paragraphs prior to the citation:\n\n   \"Unless the cache decides to remove the cache entry, it MUST also\n   replace the end-to-end headers stored with the cache entry with\n   corresponding headers received in the incoming response.\"\n\n>82. Section 14, pg. 91: suggest adding a sentence to each header\n>defined by this section that states whether the header is end-to-end or\n>hop-by-hop and whether the header is cachable by default, cachable by\n>explicit cache directive, or never cachable.\n\nWhile that would clear up some confusion, I believe the document already\ncontains too much duplication, and duplicated information always runs the\nrisk of becoming contradictory due to modification over time.\n\n>93. Section 14.9, pg. 99, 1st para., has \"When a directive appears\n>without any 1#field-name parameter, the directive applies to the entire\n>request or response.\" At the present, no cache-request-directive\n>employs a 1#field-name parameter (see pg. 98); consequently all request\n>directives apply to the entire request in all cases.\n\nTrue, however the text establishes a rule for all Cache-Control\ndirectives, now and in the future.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7719445"}, {"subject": "Re: Comments (Part 1) on HTTP ID Rev 0", "content": "Glenn Adams <gadams@spyglass.com> writes:\n\n>4. Section 2.2, pg. 16, definition of \"CTL\", fails to consider that\n>ASCII (and ISO646-1993) consider SPACE (040) to be a control character\n>of the same status as DEL (177).\n\nPerhaps, but HTTP and the Internet documents its BNF derive from have\nalways treated SP as a special case, differing from most of the CTLs in\nthat it has properties of its own (as do CR, HT, and LF).\n\n>15. Section 3.9 refers to \"short 'floating point' numbers\". I would\n>suggest this with \"real numbers\" since both \"short\" and \"floating\n>point\" seems to implementation specific.\n\nAnd as has been mentioned on this list before, these are actually\nfixed-point numbers.\n\n>17. Section 4.2, pg. 31, 4th para., states \"It MUST be possible ...\". I\n>would suggest replacing this with a statement that uses the converse using the\n>form \"MUST NOT ... unless ...\"; e.g., \"Multiple header fields MUST NOT be\n>combined into one header unless ...\".\n\nSorry, that changes the meaning entirely.  This paragraph expresses the\nheader-combination rule, which allows q receiving party to combine\nmultiple instances of the same header-name by appending the values of\nthose headers in order and separated by commas.  This behavior is\nactually required by the CGI 1.1 specification for passing headers to\nCGI programs as CGI variables.  Your rewrite places the restriction on\nthe receiver, not the sender (where it rightly belongs).  A \"MUST NOT\"\nrewrite should look more like this:\n\n   \"Header fields which are not defined in their entirety as a\n   comma-separated list [i.e., #(values)] MUST NOT be appear more\n   than once in a message\"\n\nStill, I find the existing text to be clearer and more to the point.\n\n>18. Section 4.3, pg. 31, 5th para., states \"The presence of a message-body\n>in a request is signaled by the inclusion of Content-Length or Transfer-\n>Encoding header field ...\".  However, \"multipart/byte-ranges\" may include\n>a message-body without either of these headers.\n\nThe document generally treats \"multipart/byteranges\" as appropriate only\nfor responses, however you are correct that if someone finds a use for\nit in requests there is a contradiction.\n\n>22. Section 4.4, pg. 32, 5th para., states \"HTTP/1.1 user agents MUST\n>notify the user when an invalid length is received and detected.\" This\n>does seem to be reflected by current industry practice (cf. IE4 and\n>Netscape Communicator 4 behavior). If this standard is intended to capture\n>current practice, then this is a broadening of current practice. I'd suggest\n>using the keyword \"MAY\" instead.\n\nRFC 1945 (HTTP 1.0) was a \"document current practices\" excercise.  HTTP\n1.1 places new requirements on implementations, and not all deployed\nHTTP 1.1 implementations are currently correct.\n\nPut differently, \"Mosaic and httpd defined HTTP 1.0, the IETF defined\nHTTP 1.1\". ;-)\n\n>30. Section 9, pg. 48, 2nd para., appears to be partially redundant with\n>Section 5.1.2, pg. 35, line 2078 (in file). Furthermore, does this requirement\n>actually hold for forms of Request-URI other than abs_path? For example,\n>does an OPTIONS * HTTP/1.1 request require a Host header?\n\nYes, Host is required on every HTTP 1.1 request, even for \"OPTIONS *\".\nFor example, this allows an origin server to respond differently\ndepending on whether the Host header refers to \"WWW.A.Com\" or \"WWW.B.Org\".\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7730198"}, {"subject": "Re: HTTP 1.1 issue 02: 3.11 Entity Tag", "content": "> In section 3.11 \"Entity Tags\", the statement\n> \n>    \"A given entity tag value MAY be used for entities obtained by\n>    requests on different URIs without implying anything about the\n>    equivalence of those entities.\"\n> \n> confuses me to the point of being unable to parse it, let alone\n> summarize it.\n>\n\nHow about rewriting this sentence to:\n\n   \"A given entity tag value MAY be used for entities obtained by\n   requests on different URIs and does not imply the\n   equivalence of those entities.\"\n\nThe point that is trying to be made is that an entity tag can only\nbe used relative to the same requested resource.  Personally, I wish\nEtags were stronger than this, but that is their definition.\n- Jim\n\n\n\n", "id": "lists-012-7741351"}, {"subject": "Re: HTTP 1.1 issue 03: 4.1 General Synta", "content": "> In section 4.1 \"General Syntax\", the statements                     \n>    \"In the interest of robustness, servers SHOULD ignore any empty           \n>    line(s) received where a Request-Line is expected.\"                       \n> and                                                                          \n>    \"In other words, if the server is reading the protocol stream at          \n>    the beginning of a message and receives a CRLF first, it SHOULD           \n>    ignore the CRLF.\"                                                         \n> state the same requirement in different forms.  Unless they're needed        \n> for other reasons, I suggest either deleting one of them or changing\n> one of the \"SHOULD\"s to \"should\".                                                \n>  \n\nA lower case should is appropriate in the second sentenc...  I don't want\nto delete the second sentence, as it isn't obvious what problem is\nbeing solved unless you are a BNF guru, and scratch your head for a while.\n- Jim\n\n\n\n", "id": "lists-012-7749705"}, {"subject": "RE: HTTP 1.1 issue 02: 3.11 Entity Tag", "content": "> The point that is trying to be made is that an entity tag can only\n> be used relative to the same requested resource.  Personally, I wish\n> Etags were stronger than this, but that is their definition.\n\nAt the time we discussed this, it was believed that we could at some\nlater date add other kinds of ETags with a broader scope (server\nunique or even globally unique).\n\nI think the possibility is still open to do this, in a separate\nspecification. Old clients/proxies would just see the new ETag as\nhaving a narrower scope of uniqueness.\n\nLarry\n\n\n\n", "id": "lists-012-7757847"}, {"subject": "Re: HTTP 1.1 issue 02: 3.11 Entity Tag", "content": "Larry Masinter writes:\n    > The point that is trying to be made is that an entity tag can only\n    > be used relative to the same requested resource.  Personally, I wish\n    > Etags were stronger than this, but that is their definition.\n    \n    At the time we discussed this, it was believed that we could at some\n    later date add other kinds of ETags with a broader scope (server\n    unique or even globally unique).\n    \n    I think the possibility is still open to do this, in a separate\n    specification. Old clients/proxies would just see the new ETag as\n    having a narrower scope of uniqueness.\n\nActually, there has a proposal on the table, since January:\n\n    http://www.ics.uci.edu/pub/ietf/http/draft-mogul-http-delta-00.txt\n\nwhich includes a careful definition of \"uniqueness scope\"\nand \"a means to extend the uniqueness scope to include\nmultiple resources,\" although that is not the primary purpose\nof this draft.  Constructive comments are encouraged.\n\n-Jeff\n\n\n\n", "id": "lists-012-7766470"}, {"subject": "Re: HTTP 1.1 issue 04: 4.2 Message Header", "content": "From: \"Ross Patterson\" <ROSSP@SS1.Reston.VMD.Sterling.COM>\nResent-From: http-wg@hplb.hpl.hp.com\nDate: Wed, 28 Oct 98 12:01:41 EST\nTo: http-wg@hplb.hpl.hp.com\nSubject: Re: HTTP 1.1 issue 04: 4.2 Message Headers\n-----\n\n\n>In section 4.2 \"Message Headers\", the statement\n>\n>   \"Applications SHOULD follow \"common form\", where one is known or\n>   indicated, when generating HTTP constructs, since there might\n>   exist some implementations that fail to accept anything beyond the\n>   common forms.\"\n>\n>is so vague as to be impossible to measure, and should therefore not be\n>normative and a requirement of compliance. I've brought this one up\n>before, so if the general opinion goes against me I won't complain.\n\nI guess I agree with you, particularly since \"common form\" is hard\nto define (and not used in other specifications much, as I read a\nquick AltaVista search).\n\nSo I'll replace \"SHOULD\" with \"ought to\".\n- Jim\n\n\n\n", "id": "lists-012-7774640"}, {"subject": "Re: HTTP 1.1 issue 06: 4.4 Message Lengt", "content": "> In section 4.4 \"Message Length\", the statement\n> \n\n> refers to a requirement, it doesn't state one.  The \"MUST NOT\" should be\n> changed to \"must not\".\n\nYup.   I see your point...\n\nI think adding quotes around \"MUST NOT\" makes it clear:\n\n    \"Any response message which \"MUST NOT\" include a message-body (such\n    as the 1xx, 204, and 304 responses and any response to a HEAD\n    request) is always terminated by the first empty line after the\n    header fields, regardless of the entity-header fields present in\n    the message.\"\n- Jim\n\n\n- Jim\n\n\n\n", "id": "lists-012-7783379"}, {"subject": "Re: HTTP 1.1 issue 05: 4.2 Message Headers (ROSS05", "content": "> In section 4.2 \"Message Headers\", the statement                              \n>  \n>    \"It MUST be possible to combine the multiple header fields into           \n>    one \"field-name: field-value\" pair, without changing the semantics        \n>    of the message, by appending each subsequent field-value to the           \n>    first, each separated by a comma.\"                                        \n>  \n> is a restatement of the preceding requirement,                               \n>  \n>    \"Multiple message-header fields with the same field-name MAY be           \n>    present in a message if and only if the entire field-value for            \n>    that header field is defined as a comma-separated list [i.e.,             \n>    #(values)].\"                                                              \n>  \n> placing a requirement upon future extensions to the protocol rather than     \n> on implementations of HTTP 1.1.  I think the \"MUST\" should be changed to     \n> a \"must\".                                                                    \n>  \n\nNo, in fact, it is a requirement placed on future versions of the HTTP/1.X\nfamily of protocols.  The issue is that there are deployed proxies that\ndo exactly this kind of header transformation, and the requirement is to\nprevent breaking them.  We might wish this isn't so, but it is, and\nfor this generation of protocol, we're stuck with it.\n- Jim\n\n\n\n", "id": "lists-012-7790962"}, {"subject": "RE: HTTP 1.1 issue 02: 3.11 Entity Tag", "content": ">  Constructive comments are encouraged.\n\nDo you think it would be useful to separate the proposal for\nallowing an entity tags with a greater uniqueness scope?\n\nFor other purposes, tying the entity tag uniqueness scope to\nthe delta coding header wouldn't be as useful.\n\n - Allowing naively simplistic implementations of delta encoding.\n\n  I suggest you remove \"naively\"; those who might value\n  simplicity of implementation over some of your\n  other goals may not be \"naive\" but just operating\n  under other constraints.\n\n# based on an entirely false analogy between MIME and HTTP...\n# It is too late to fix the terminological failure in the HTTP/1.1\n# specification\n\nI don't think that the dictionary definition of \"entity\"\nis any more relevant than the dictionary definition of \"protocol\".\nTerminology always fails to meet our expectations for it.\n\nI don't think that the relationship between MIME and HTTP is\nan 'analogy', nor is it 'entirely false'. Your grumpy complaint of\nthe word \"entity\" here doesn't really advance your cause.\nAnd it wasn't \"too late\" to fix a \"terminological failure\"\nin January 1998 (the date of your Internet Draft.)\nRather, there was rough consensus that it was not a \"failure\".\n\nI suggest that you continue to explain how the concepts of\n\"entity\" in HTTP and MIME differ, as it is a useful discussion,\nbut keep the introduction of \"instance\" neutral:\n\"to avoid confusion, we define a new term in this memo:\"\n\nNow that we're about to issue the HTTP/1.1 DS as an RFC, you might\nrevise the reference [9] and the tense of \"has been proposed\".\n\nDo you mean this to be 'on the table' as a proposed standard?\n\nLarry\n\n\n\n", "id": "lists-012-7800084"}, {"subject": "Re: HTTP 1.1 issue 07: 4.4 Message Lengt", "content": "> In section 4.4 \"Message Length\", does the statement                          \n>    \"If a Content-Length header field (section 14.13) is present, its         \n>    decimal value in OCTETs represents both the entity-length and the         \n>    transfer-length. The Content-Length header field MUST NOT be used         \n>    if these two lengths are different (i.e., if a Transfer-Encoding          \n>    header field is present).\"                                                \n> mean that a receiver should ignore Content-Length, or a sender should        \n> not send it?                                                                 \n \nIt means that the sender should not send it in the case of a transfer\nencoded message, since the transfer-length is otherwise encoded and would\nresult in a contradiction in the lenth.\n\nI don't see an obvious rewrite to make this more obvious.  Unless there\nis some concrete suggestion, I plan to leave this one alone.\n\nThis is one of the parts of HTTP that shows the problems with trying to\nmush message lengths into the same layer as the semantics of the\nmessage.  Ugh...  Always makes me sick to my stomach to look at this part\nof HTTP.\n- Jim\n\n- Jim\n\n\n\n", "id": "lists-012-7809365"}, {"subject": "Re: HTTP 1.1 issue 08: 8.1.2 Overall Operation (ROSS08", "content": "> In section 8.1.2 \"Overall Operation\", the statement                          \n>    \"Persistent connections provide a mechanism by which a client and         \n>    a server can signal the close of a TCP connection. This signaling         \n>    takes place using the Connection header field (section 14.10).            \n>    Once a close has been signaled, the client MUST not send any more         \n>    requests on that connection.\"                                             \n> should probably read \"MUST NOT\" instead of \"MUST not\".                       \n\nYes, you are right; I'll fix it.\n- Jim\n\n\n\n", "id": "lists-012-7818093"}, {"subject": "Re: HTTP 1.1 issue 05: 4.2 Message Headers (ROSS05", "content": "jg@pa.dec.com (Jim Gettys) writes:\n\n>                      The issue is that there are deployed proxies that\n>do exactly this kind of header transformation, and the requirement is to\n>prevent breaking them.\n\nSeen in that light you're obviously correct.\n\nThanks,\nRoss\n\n\n\n", "id": "lists-012-7825867"}, {"subject": "Re: HTTP 1.1 issue 10: 10.3.5 304 Not Modifie", "content": ">\n> In section 10.3.5 \"304 Not Modified\", the \"MUST NOT include a\n> message-body\" requirement is stated twice in this section, at the top\n> and bottom.  One of them should be deleted.\n> \n\nYup.  You are right...\n\nI will replace the sentence in the first paragraph with the last sentence\nof the section.\n- Jim\n\n\n\n", "id": "lists-012-7833460"}, {"subject": "Re: HTTP 1.1 issue 11: 10.3.7 307 Temporary Redirect (ROSS11", "content": "> In section 10.3.7 \"307 Temporary Redirect\", the \"SHOULD contain a short\n> hypertext note\" requirement is stated twice in this section, in the\n> second paragraph and at the bottom. One of them should be deleted.\n> \nI'll rewrite the second paragraph to be:\n\n\"The temporary URI SHOULD be given by the Location field in the response.\nUnless the request method was HEAD, the entity of the response SHOULD contain\na short hypertext note with a hyperlink to the new URI(s), since many\npre-HTTP/1.1 user agents do not understand the 307 status.  Therefore,\nthe note SHOULD contain the information necessary for a user to repeat\nthe original request on the new URI.\"\n\nAnd delete the last paragraph of the section.\n- Jim\n\n\n\n", "id": "lists-012-7840876"}, {"subject": "Re: HTTP 1.1 issue 12: 10.4.8 407 Proxy Authentication Require", "content": "> In section 10.4.8 \"407 Proxy Authentication Required\", the statement\n> \n>    \"This code is similar to 401 (Unauthorized), but indicates that\n>    the client MUST first authenticate itself with the proxy.\"\n> \n> is not a requirement, so the \"MUST\" should be changed to \"must\".\n> \nYou are right... I'll fix it.\n- Jim\n\n\n\n", "id": "lists-012-7848849"}, {"subject": "Re: HTTP 1.1 issue 13: 11 Access Authentication (ROSS13", "content": "> In section 11 \"Access Authentication\", the statement\n> \n>    \"HTTP provides several OPTIONAL challenge-response authentication\n>    mechanisms which MAY be used by a server to challenge a client\n>    request and by a client to provide authentication information.\"\n> \n> is not a requirement, so the \"MAY\" should be changed to \"may\".\n> \n\nI'll change the \"MAY\" to \"can\".\n- Jim\n\n\n\n", "id": "lists-012-7856499"}, {"subject": "Re: HTTP 1.1 issue 14: 14.20.1 Expect 100continu", "content": "> In section 14.20.1 \"Expect 100-continue\", the statement\n> \n>    \"Proxies SHOULD maintain a cache recording the HTTP version\n>    numbers received from recently-referenced next-hop servers.\"\n> \n> duplicates the same requirement in section 8.2.4.\n> \n\nIn fact, I think that all of section 14.20.1 can be removed, with\na single cross reference to 8.2.4 taking its place.  This looks like\nsomething that got left behind when some reorganization took place.\nOthers should check that my analysis is correct.\n\nI'll add the sentence: \n\n\"See section 8.2.4 for use of the 100 (continue) Status.\"\n\nin section 14.20.\n\nThanks for noticing; it will save a third of a page of redundent text.\n- Jim\n\n\n\n", "id": "lists-012-7864679"}, {"subject": "RE: HTTP 1.1 issue 13: 11 Access Authentication (ROSS13", "content": "> > In section 11 \"Access Authentication\", the statement\n> > \n> >    \"HTTP provides several OPTIONAL challenge-response authentication\n> >    mechanisms which MAY be used by a server to challenge a client\n> >    request and by a client to provide authentication information.\"\n> > \n> > is not a requirement, so the \"MAY\" should be changed to \"may\".\n> > \n> \n> I'll change the \"MAY\" to \"can\".\n\n\"MAY\" is not used to designate requirements, but goes\nalong with OPTIONAL; if something is OPTIONAL, then you\nMAY do it. It's not necessary to say both OPTIONAL and\nMAY, but there's nothing wrong with the text as it was.\n\nBut you MAY change it if you actually think it's better.\n\nLarry\n\n\n\n", "id": "lists-012-7872522"}, {"subject": "Re: HTTP 1.1 issue 15: 14.23 Hos", "content": ">In section 14.23 \"Host\", the statements\n>\n>   \"A client MUST include a Host header field in all HTTP/1.1 request\n>   messages on the Internet (i.e., on any message corresponding to a\n>   request for a URL which includes an Internet host address for the\n>   service being requested). If the Host field is not already\n>   present, an HTTP/1.1 proxy MUST add a Host field to the request\n>   message prior to forwarding it on the Internet. All Internet-based\n>   HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code\n>   to any HTTP/1.1 request message which lacks a Host header field.\"\n>\n>can be interpreted to relax the requirement when requests are not\n>transiting the public Internet (e.g., between a client and server on a\n>departmental LAN). I believe the intent is to make HOST a required\n>request header whenever TCP/IP is the vehicle for the HTTP conversation.\n>If so, these statements should be clarified.\n\nNo, the existing wording is better because the requirement still holds\neven if the use of HTTP is over UDP, T/TCP, or any other future transport\nprotocol.  The requirement relates to the meaning of identifier components\nand not to the transport protocol itself.  The reason for the implied\nrelaxation is because it is possible to use HTTP on a transport that\nhas no concept of hostnames (like between a PDA and its base station)\nand we don't want a separate specification of HTTP for every transport.\n\n....Roy\n\n\n\n", "id": "lists-012-7881108"}, {"subject": "Re: HTTP 1.1 issue 15: 14.23 Hos", "content": "> >In section 14.23 \"Host\", the statements\n> >\n> >   \"A client MUST include a Host header field in all HTTP/1.1 request\n> >   messages on the Internet (i.e., on any message corresponding to a\n> >   request for a URL which includes an Internet host address for the\n> >   service being requested). If the Host field is not already\n> >   present, an HTTP/1.1 proxy MUST add a Host field to the request\n> >   message prior to forwarding it on the Internet. All Internet-based\n> >   HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code\n> >   to any HTTP/1.1 request message which lacks a Host header field.\"\n> >\n> >can be interpreted to relax the requirement when requests are not\n> >transiting the public Internet (e.g., between a client and server on a\n> >departmental LAN). I believe the intent is to make HOST a required\n> >request header whenever TCP/IP is the vehicle for the HTTP conversation.\n> >If so, these statements should be clarified.\n> \n> No, the existing wording is better because the requirement still holds\n> even if the use of HTTP is over UDP, T/TCP, or any other future transport\n> protocol.  The requirement relates to the meaning of identifier components\n> and not to the transport protocol itself.  The reason for the implied\n> relaxation is because it is possible to use HTTP on a transport that\n> has no concept of hostnames (like between a PDA and its base station)\n> and we don't want a separate specification of HTTP for every transport.\n> \n\nActually, Roy, I see Ross's point here: it says \"on the Internet\". Some \nmight misinterpret this on a LAN or intranet.  I agree there should be \nno reference to TCP or other transport protocols in this paragraph, of \ncourse, for the reasons you give.\n\nSo I think striking the two phrases \"on the Internet\", and the word \n\"Internet-based\"  from the paragraph will reduce the wriggle room of \nimplementers to get it wrong.  The result would be:\n\n\n   \"A client MUST include a Host header field in all HTTP/1.1 request\n   messages (i.e., on any message corresponding to a\n   request for a URL which includes an Internet host address for the\n   service being requested). If the Host field is not already\n   present, an HTTP/1.1 proxy MUST add a Host field to the request\n   message prior to forwarding it. All\n   HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code\n   to any HTTP/1.1 request message which lacks a Host header field.\"\n\nI don't think that any arguments to allow the host header to be dropped\nfor PDA use are compelling enough to relax this requirement in this way.\n\nIf it is relaxed at all, it might be relaxed in the following way:\n\n  \"A client MUST include a Host header field in all HTTP/1.1 request messages \n   on any message corresponding to a request for a URI which includes \n   an Internet host for the naming authority. All HTTP/1.1 servers MUST \n   respond with a 400 (Bad Request) status code to any such HTTP/1.1 request \n   message which lacks a Host header field. If the Host field is not already \n   present, an HTTP/1.1 proxy MUST add a Host field to such a request message \n   prior to forwarding it.\"\n\nThis would bring it in line with the draft standard URI specification \nas well, and make it clear that for other URL schemes which might also \nhave a host name for the naming authority that we demand the host \ninformation, while making it clearer that for schemes that don't have \na host name for a naming authority that we don't need a Host header.\n\nWhat say you?\n- Jim\n\n\n\n", "id": "lists-012-7889988"}, {"subject": "Re: HTTP 1.1 issue 18: 14.46 Warnin", "content": "> From: \"Ross Patterson\" <ROSSP@SS1.Reston.VMD.Sterling.COM>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Mon, 09 Nov 98 10:46:56 EST\n> To: http-wg@hplb.hpl.hp.com\n> Subject: HTTP 1.1 issue 18: 14.46 Warning\n> -----\n> In section 14.46 \"Warning\", the statement\n> \n>    \"The warn-code consists of three digits. The first digit indicates\n>    whether the Warning MUST or MUST NOT be deleted from a stored\n>    cache entry after a successful revalidation:\"\n> \n> does not specify a requirement.  The \"MUST or MUST NOT\" should be\n> changed to \"must or must not\"\n> \n\nI don't think they hurt anything, but they don't help either given the\nnext two paragraphs are clear on the point, so I guess I'll lower casify\nthem.\n- Jim\n\n\n\n", "id": "lists-012-7901314"}, {"subject": "Re: HTTP 1.1 issue 19: 19.4.1 MIMEVersio", "content": "> Section 19.4.1 \"MIME-Version\" contains the statement\n> \n>    HTTP is not a MIME-compliant protocol (see appendix 19.4).\n> \n> but 19.4.1 *IS* in appendix 19.4.\n> \nWell, I think this is left over from a reoganization that pulled things\ntogether.  So I guess the cross reference should be eliminated at this\npoint, leaving the non-compliance statement.\n- Jim\n\n\n\n", "id": "lists-012-7909728"}, {"subject": "Re: HTTP 1.1 issue 15: 14.23 Hos", "content": "\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> writes:\n\n>>In section 14.23 \"Host\", the statements\n>>\n>>   \"A client MUST include a Host header field in all HTTP/1.1 request\n>>   messages on the Internet (i.e., on any message corresponding to a\n>>   request for a URL which includes an Internet host address for the\n>>   service being requested). If the Host field is not already\n>>   present, an HTTP/1.1 proxy MUST add a Host field to the request\n>>   message prior to forwarding it on the Internet. All Internet-based\n>>   HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code\n>>   to any HTTP/1.1 request message which lacks a Host header field.\"\n>>\n>>can be interpreted to relax the requirement when requests are not\n>>transiting the public Internet (e.g., between a client and server on a\n>>departmental LAN). I believe the intent is to make HOST a required\n>>request header whenever TCP/IP is the vehicle for the HTTP conversation.\n>>If so, these statements should be clarified.\n>\n>No, the existing wording is better because the requirement still holds\n>even if the use of HTTP is over UDP, T/TCP, or any other future transport\n>protocol.  The requirement relates to the meaning of identifier components\n>and not to the transport protocol itself.  The reason for the implied\n>relaxation is because it is possible to use HTTP on a transport that\n>has no concept of hostnames (like between a PDA and its base station)\n>and we don't want a separate specification of HTTP for every transport.\n\nThat's exactly where I was going, although using \"TCP/IP\" was a poor\nchoice.  (I meant something like \"any protocol that flows data over a\nnetwork where said network understands the concept of a 'hostname'\", but\nthat would have pegged the Obfusco-Meter).  In the case of\nHTTP-over-SNA, a Host header would look very different, if it could\nexist at all.  Certainly other transports could have similar issues.\n\nMy concern wasn't over the relaxation of the requirement when using\ntransports that don't have a \"hostname\" concept, but rather the way it\nwas written up.  I also wasn't sure if this was an intentional\nrelaxation - the statement in section 9 \"Method Definitions\" is\nunequivocal:\n\n   \"The Host request-header field (section 14.23) MUST accompany\n   all HTTP/1.1 requests.\"\n\nIf the relaxation is intentional, then section 9 is in error.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7917038"}, {"subject": "Re: HTTP 1.1 issue 15: 14.23 Hos", "content": "jg@pa.dec.com (Jim Gettys) writes:\n\n>Actually, Roy, I see Ross's point here: it says \"on the Internet\". Some\n>might misinterpret this on a LAN or intranet.  I agree there should be\n>no reference to TCP or other transport protocols in this paragraph, of\n>course, for the reasons you give.\n\nRight.\n\n>So I think striking the two phrases \"on the Internet\", and the word\n>\"Internet-based\"  from the paragraph will reduce the wriggle room of\n>implementers to get it wrong.  The result would be:\n>\n>\n>   \"A client MUST include a Host header field in all HTTP/1.1 request\n>   messages (i.e., on any message corresponding to a\n>   request for a URL which includes an Internet host address for the\n>   service being requested). If the Host field is not already\n>   present, an HTTP/1.1 proxy MUST add a Host field to the request\n>   message prior to forwarding it. All\n>   HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code\n>   to any HTTP/1.1 request message which lacks a Host header field.\"\n\nThat's clearer, and jibes with the MUST in section 9.\n\n>I don't think that any arguments to allow the host header to be dropped\n>for PDA use are compelling enough to relax this requirement in this way.\n\nIndeed, one could argue that a PDA if RF/infrared/whatever communication\nwith its base station should send Host headers if the URL includes a\nhost address for the base station (e.g., running IP-on-the-air ala\nAMPRNet).  By the same argument, if the URL requested by the PDA doesn't\ninclude a host address, no Host header is required (although that\nviolates section 9).\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-7927263"}, {"subject": "Re: HTTP 1.1 issue 20: 13.1.2 Warning", "content": "Has anyone ever told you that you'd have a great future as a copy editor?\n:-).\n\n> -----\n> In section 13.1.2 \"Warnings\", the statement\n> \n>    \"The first digit indicates whether the Warning MUST or MUST NOT be\n>    deleted from a cached response after it is successfully\n>    revalidated.\n> \n> does not specify a requirement.  The \"MUST or MUST NOT\" should be\n> changed to \"must or must not\".\n> \n\nThis, plus your issue 18 point out duplication in section 14.46\ntext with section 13.1.2...\n\nIn section 13.1.2 we find:\n\n   Warnings come in two categories:\n\n   1.Those that describe the freshness or revalidation status of the response, \n   and so MUST be deleted after a successful revalidation (see section 13.3 \n   for a definition of revalidation).\n\n   2.Those that describe some aspect of the entity body or entity headers \n   that is not rectified by a revalidation, for example, a lossy compression \n   of the entity bodies. These warnings MUST NOT be deleted after a successful \n   revalidation.\n\n   Warnings are assigned 3-digit code numbers. The first digit indicates \n   whether the Warning MUST or MUST NOT be deleted from a cached response \n   after it is successfully revalidated. This specification defines the code \n   numbers and meanings of each currently assigned warning, allowing a client \n   or cache to take automated action in some (but not all) cases.\n\n\nIn section 14.46 we find:\n\n  The warn-code consists of three digits. The first digit indicates whether \n  the Warning MUST or MUST NOT be deleted from a stored cache entry after \n  a successful revalidation:\n\n  1xxWarnings that describe the freshness or revalidation status of the \n  response, and so MUST be deleted after a successful revalidation. 1XX \n  warn-codes MAY be generated by a cache only when validating a cached entry. \n  It MUST NOT be generated by clients.\n\n  2xxWarnings that describe some aspect of the entity body or entity \n  headers that is not rectified by a revalidation, and which MUST NOT \n  be deleted after a successful revalidation.\n\nThese are close to identical, in slightly different ordering, but with\na few differences (e.g. the lossy compression reference, and the additional\nrequirements on 1xx codes in 14.45).\n\nI propose we clean this up (and shorten the specification) by replacing\nthe 13.1.2 text noted above with:\n\n  Warnings are assigned three digit warn-codes. The first digit indicates \n  whether the Warning MUST or MUST NOT be deleted from a stored cache \n  entry after a successful revalidation:\n\n  1xxWarnings that describe the freshness or revalidation status of the \n  response, and so MUST be deleted after a successful revalidation. 1XX \n  warn-codes MAY be generated by a cache only when validating a cached entry. \n  It MUST NOT be generated by clients.\n\n  2xxWarnings that describe some aspect of the entity body or entity headers \n  that is not rectified by a revalidation (for example, a lossy compression \n  of the entity bodies) and which MUST NOT be deleted \n  after a successful revalidation.\n\n  See section 14.46 for the definitions of the codes themselves.\n\nAnd replace the 14.46 text with:\n\n  The caching behavior of warnings is discussed in section 13.1.2.\n\nThis should reduce duplication and resulting confusion (and shorten the\ndocument further).\n- Jim\n\n\n\n", "id": "lists-012-7936266"}, {"subject": "Re: HTTP 1.1 issue 21: 13.3.3 Weak and Strong Validator", "content": "> In section 13.3.3 \"Weak and Strong Validators\", the statements\n> \n>    \"The weak comparison function MAY be used for simple\n>    (non-subrange) GET requests. The strong comparison function MUST\n>    be used in all other cases.\"\n> \n> and\n> \n>    \"A cache or origin server receiving a conditional request, other\n>    than a full-body GET request, MUST use the strong comparison\n>    function to evaluate the condition.\"\n> \n> state the same requirement.\n\nIt isn't quite the same thing.  One states requirements on the\nclient, and the other on origin servers or proxies.  It is a bit\nconfusing, and may be redundant.\n\nJeff, what is the right thing to do here?\n- Jim\n\n\n\n", "id": "lists-012-7947057"}, {"subject": "Re: HTTP 1.1 issue 22: 13.4 Response Cachabilit", "content": "> In section 13.4 \"Response Cachability\", the statement\n> \n>    \"Certain cache-control directives are therefore provided so that\n>    the server can indicate that certain resource entities, or\n>    portions thereof, MUST NOT be cached regardless of other\n>    considerations.\"\n> \n> does not specify a requirement. The \"MUST or MUST NOT\" should be changed\n> to \"must or must not\".\n> \n\nYup.  You are right; the will be lower cased...\n- Jim\n\n\n\n", "id": "lists-012-7955761"}, {"subject": "Re: HTTP 1.1 issue 23: 13.5.2 Nonmodifiable Header", "content": "> In section 13.5.2 \"Non-modifiable Headers\", the statement\n> \n>    \"A non-transparent proxy MAY modify or add these fields to a\n>    message that does not include no-transform, but if it does so, if\n>    not already present, it MUST add a Warning 214 (Transformation\n>    applied) if one does not already appear in the message (see\n>    section 14.46).\"\n> \n> appears to be in error.  It allows non-transparent proxies to make\n> certain changes to messages that do not contain Cache-Control:\n> no-transform.  The discussion of the no-transform directive in section\n> 14.9.5 doesn't jive, and makes it look like this paragraph should read\n> \"... that contains no-transform ...\", just as the immediately prior\n> paragraph does.\n> \n\nNo, I think it is correct: there are two cases: a message that\ncontains no-transform and one that does contain no-transform.\nYou are allowed to change the headers in the default case of\na message not marked \"no-transform\", so long as you mark\nthe message with the 214 (Transformation applied) warning.\n- Jim\n\n\n\n", "id": "lists-012-7963338"}, {"subject": "Re: HTTP 1.1 issue 24: 13.12 Cache Replacemen", "content": "> In section 13.12 \"Cache Replacement\", the statement\n> \n> \"If it inserts the new response into cache storage it\n> SHOULD follow the rules in section 13.5.3.\"\n> \n> could be construed as downgrading several MUSTs in 13.5.3 to SHOULDs.\n> The \"SHOULD\" should either be replaced with \"should\" or this requirement\n> should be upgraded to a \"MUST\".\n> \n\nI think the right solution is to rewrite this sentence to be:\n\n \"If it inserts the new response into cache storage the rules in section\n 13.5.3 apply.\"\n- Jim\n\n\n\n", "id": "lists-012-7971549"}, {"subject": "Re: HTTP/1.1 Draft has inconsistent use of LastModified header fiel", "content": "In theory, one might refer to the date itself as the \"last-modified\"\ndate, and the header as \"Last-Modified\", but I guess for simplicities\nsake I'll bow to the hob-goblin of consistency and call them\nall \"Last-Modified\" dates...\n- Jim\n\n> The Last-modified header response field is inconsistently referenced in\n> the HTTP 1.1 spec as both Last-Modified and Last-modified.\n> \n> According to the definition in section 14.29 of\n> <draft-ietf-http-v11-spec-rev-04> the Last-Modified entity-header field\n> is in the form:  Last-Modified  = \"Last-Modified\" \":\" HTTP-date\n> \n> Suggestion that all occurrences of \"Last-modified\" be changed to\n> \"Last-Modified\" to avoid confusion.\n> \n> The following parts need changing:\n> \n> Table of Contents pages 6-7\n\nHappens automatically if the headings are changed.\n\n> 13.3.1 Last-modified Date.\n\n> ..............................................................................\n> .\n> 55\n> 13.3.4 Rules for When to Use Entity Tags and Last-modified Dates\n> .................... 57\n> \n> [page 62]\n> \n> An HTTP/1.1 caching proxy, upon receiving a conditional request that\n> includes both a Last-modified date and\n> \n> [page 74]\n> \n> 13.3.1 Last-modified Dates\n> \n> [page 77]\n> \n> includes both a Last-modified date and one or more entity tags as cache\n> \n> includes both a Last-modified date (e.g., in an If-Modified-Since or\n> If-Unmodified-Since header field)\n> \n> [page 84]\n> \n> For example,\n>        HTTP/1.1 206 Partial content\n>        Date: Wed, 15 Nov 1995 06:25:24 GMT\n>        Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n> \n> [Page 138]\n> \n> For example:\n> \n>    HTTP/1.1 206 Partial Content\n>    Date: Wed, 15 Nov 1995 06:25:24 GMT\n>    Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n> \n\nTechnically, this example is OK, since headers are not case\nsensitive, but I'll fix it anyway...\n- Jim\n\n\n\n", "id": "lists-012-7979218"}, {"subject": "Re: Rev05 nit", "content": "> From: Dave Kristol <dmk@research.bell-labs.com>\n> Date: Thu, 17 Sep 1998 15:44:35 -0400 (EDT)\n> To: jg@w3.org\n> Subject: Rev05 nits\n> -----\n> You just *knew* you would hear from me, didn't you!? :-)\n> Some nits for the next go-round.\n> \n> Dave\n> -------------\n> \n> Have you noticed there's no Page 2 in the PostScript versions?!\n> Another gift from Word.\n> \n> 5.1.2 Request-URI\n> \n>    The Request-URI is a Uniform Resource Identifier (section 3.2) and\n>    identifies the resource upon which to apply the request.\n> \n>           Request-URI    = \"*\" | absoluteURI | abs_path | authority\n> \n>    The three options for Request-URI are dependent on the nature of the\n>        =====\n> \n> There are evidently four now.\\\n\nSo I can't count...\n\n> \n> 14.16 Content Range\n>    When an HTTP message includes the content of multiple ranges (for\n>    example, a response to a request for multiple non-overlapping\n>    ranges), these are transmitted as a multipart message. The multipart\n>    media type used for this purpose is \"multipart/byteranges\" as defined\n>    in appendix 19.2. See appendix 19.6.3for a compatibility issue.\n>    ^-- insert space\n\nYup.\n\n> \n> 19.6.3 Changes from RFC 2068\n>    ...\n>    worth fixing [39]. TE also solves another, obscure, downward\n>    interoperability problem that could have occured due to interactions\n>        ======= -> occurred\n>    between authentication trailers, chunked encoding and HTTP/1.0\n>    clients.(Section 3.6, 3.6.1, and 14.39)\n\nSo Word can't spell, either...  I guess I should do a final spelling\ncheck...\n- Jim\n\n\n\n", "id": "lists-012-7988461"}, {"subject": "Re: ContentDispositio", "content": "> From: koen@win.tue.nl (Koen Holtman)\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Fri, 18 Sep 1998 20:00:01 +0200 (MET DST)\n> To: jg@pa.dec.com (Jim Gettys)\n> Cc: claus@faerber.muc.de, http-wg@hplb.hpl.hp.com\n> Subject: Re: Content-Disposition\n> -----\n> Jim Gettys:\n> >\n> >This is taken from the MIME useage of Content-Disposition.\n> \n> I initially wrote the Content-Disposition section, so let me add some\n> details here.\n> \n> The Content-Disposition section documents *only* what some popular\n> current web browsers do.  I was careful not to specify anything beyond\n> what I could test to heve been implemented.  The section does not\n> contain any actual requirements for HTTP/1.1 software.  It was added\n> to answer a frequently asked question.\n> \n> If we had set out to specify an improved version of\n> Content-Disposition, something like your proposal would have been in\n> the specification.  But we did not set out to specify any improvement,\n> reasoning that others (e.g. the MIME community) could to this if\n> needed.\n> \n> That being said, *if* another revision of the 1.1 spec is necessary,\n> it would be nice to add a pointer to RFC 2183, which updates RFC 1806.\n> \n\nI'll add the pointer and bibliographic reference...\n- Jim\n\n\n\n", "id": "lists-012-7996989"}, {"subject": "http issue &quot;AUTHVSPROXY&quot; not quite close", "content": "> From: Ronald.Tschalaer@psi.ch (Life is hard... and then you die.)\n> Date: Mon, 28 Sep 1998 22:45:16 +0200\n> To: JG@pa.dec.com\n> Subject: http issue \"AUTHVSPROXY\" not quite closed\n> -----\n>   Hi Jim,\n> \n> I sent you a mail about this a couple months back, but I think I got it\n> slightly wrong. In the issue list \"AUTHVSPROXY\" actually has two things,\n> the second of which has been done. The first, however, still needs doing:\n> \n> [Gisle wrote:]\n> > Is there a good reason why WWW-Authenticate can have multiple\n> > challenges while Proxy-Authenticate can't?\n> >\n> > draft-ietf-http-v11-spec-rev-03:\n> >\n> > >         Proxy-Authenticate  = \"Proxy-Authenticate\" \":\" challenge\n> > >         WWW-Authenticate  = \"WWW-Authenticate\" \":\" 1#challenge\n> \n> [Paul rsponded:]\n> > none. Just forgot to update the one when I was updating the other.  I'll\n> > fix it in the next draft.\n> \n> But since it's in the http spec, not the auth spec, section 14.33 of\n> draft-ietf-http-v11-spec-rev-05 still has the old Proxy-Authenticate\n> syntax and needs to be changed to\n> \n>       Proxy-Authenticate  = \"Proxy-Authenticate\" \":\" 1#challenge\n> \n> \n>   Cheers,\n> \n>   Ronald\n\nI believe Ronald is correct, and that 14.33's BNF needs updating.\n\nDave, am I correct?\n- Jim\n\n\n\n", "id": "lists-012-8006500"}, {"subject": "Re: 200 Level warnings in revalidatio", "content": "> From: Jeffrey Mogul <mogul@pa.dec.com>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Tue, 29 Sep 98 15:29:53 MDT\n> To: Chris DiPierro <cdipierr@us.ibm.com>\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: 200 Level warnings in revalidation\n> -----\n> Chris DiPierro <cdipierr@us.ibm.com> writes:\n> \n>     The spec says that all 200 level warnings must be retained after a\n>     successful revalidation.\n> \n>     However, it also says that all end-to-end headers (Warnings are\n>     end-to-end) must be replaced after by the corresponding end-to-end\n>     header in a revalidation.\n> \n>     So if we had a cached copy of an entity that has a 200 level\n>     warning associated with it and when we revalidate, we get a Warning\n>     header w/o the 200 level warning, are we supposed to keep it or\n>     not?\n> \n> I believe you are referring to section 13.5.3 (Combining Headers),\n> and this particular passage:\n> \n>    The end-to-end headers stored in the cache entry are used for the\n>    constructed response, except that\n> \n>      .  any stored Warning headers with warn-code 1xx (see section\n>         14.46) MUST be deleted from the cache entry and the forwarded\n>         response.\n>      .  any stored Warning headers with warn-code 2xx MUST be retained\n>         in the cache entry and the forwarded response.\n>      .  any end-to-end headers provided in the 304 or 206 response MUST\n>         replace the corresponding headers from the cache entry.\n> \n>    Unless the cache decides to remove the cache entry, it MUST also\n>    replace the end-to-end headers stored with the cache entry with\n>    corresponding headers received in the incoming response.\n> \n>    In other words, the set of end-to-end headers received in the\n>    incoming response overrides all corresponding end-to-end headers\n>    stored with the cache entry (except for stored Warning headers with\n>    warn-code 1xx, which are deleted even if not overridden).\n> \n> I guess this might be somewhat confusing.  The second exception\n> in the bulleted list:\n> \n>      .  any stored Warning headers with warn-code 2xx MUST be retained\n>         in the cache entry and the forwarded response.\n> \n> appears to conflict with\n> \n>    Unless the cache decides to remove the cache entry, it MUST also\n>    replace the end-to-end headers stored with the cache entry with\n>    corresponding headers received in the incoming response.\n> \n> but the obvious resolution is that special treatment for Warning\n> headers applies to this paragraph, as well as the one above the\n> list of exceptions.  I.e., the paragraph should read:\n> \n>    Unless the cache decides to remove the cache entry, it MUST also\n>    replace the end-to-end headers stored with the cache entry with\n>    corresponding headers received in the incoming response, except\n>    for Warning headers as described immediately above.\n> \n> -Jeff\n\nI plan to adopt Jeff's version of the paragraph.\n- Jim\n\n\n\n", "id": "lists-012-8016144"}, {"subject": "Re: http issue &quot;AUTHVSPROXY&quot; not quite close", "content": "jg@pa.dec.com (Jim Gettys) wrote:\n  > > [...]\n  > > \n  > > But since it's in the http spec, not the auth spec, section 14.33 of\n  > > draft-ietf-http-v11-spec-rev-05 still has the old Proxy-Authenticate\n  > > syntax and needs to be changed to\n  > > \n  > >       Proxy-Authenticate  = \"Proxy-Authenticate\" \":\" 1#challenge\n  > > [...]\n  > \n  > I believe Ronald is correct, and that 14.33's BNF needs updating.\n  > \n  > Dave, am I correct?\n\nWho, me? :-)\n\nYes, that change looks correct.\n\nDave Kristol\n\n\n\n", "id": "lists-012-8027662"}, {"subject": "Re: HTTP 1.1 issue 17: 14.36 Refere", "content": "> From: \"Ross Patterson\" <ROSSP@SS1.Reston.VMD.Sterling.COM>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Mon, 09 Nov 98 10:46:42 EST\n> To: http-wg@hplb.hpl.hp.com\n> Subject: HTTP 1.1 issue 17: 14.36 Referer\n> -----\n> In section 14.36 \"Referer\", the statement\n> \n>    \"If the field value is a partial URI, it SHOULD be interpreted\n>    relative to the Request-URI.\"\n> \n> sounds more like a MUST than a SHOULD. An interoperability problem will\n> result if two implementations interpret the referring URI differently.\n> If the SHOULD is due to compatibility with earlier usage, that should be\n> noted someplace.\n> \n\nA referer (sic) cannot be an interoperability problem in the first place,\nas it is only provided to a server as a benefit (at some cost to the\nclient, I might add).\n\nSo it is hard to make a claim that this should be a MUST, as it isn't\nmandantory information in the first place, and is just suggesting how\nthe server might interpret the graciously supplied partial URI in the\nfirst place.\n\nI don't see any need for a change.\n- Jim\n\n\n\n", "id": "lists-012-8036143"}, {"subject": "Re: HTTP 1.1 issue 07: 4.4 Message Lengt", "content": "    > In section 4.4 \"Message Length\", does the statement\n    >    \"If a Content-Length header field (section 14.13) is present, its\n    >    decimal value in OCTETs represents both the entity-length and the\n    >    transfer-length. The Content-Length header field MUST NOT be used\n    >    if these two lengths are different (i.e., if a Transfer-Encoding\n    >    header field is present).\" \n    > mean that a receiver should ignore Content-Length, or a sender should\n    > not send it?\n     \n    It means that the sender should not send it in the case of a\n    transfer encoded message, since the transfer-length is otherwise\n    encoded and would result in a contradiction in the lenth.\n\n    I don't see an obvious rewrite to make this more obvious.  Unless\n    there is some concrete suggestion, I plan to leave this one alone.\n\n\"Use the Robustness Principle, Luke!\"\n\nIt means both: it means that the sender MUST NOT send it\nand that (in the event of a non-compliant sender) the receiver\nMUST ignore it.  I agree that the verb here (\"be used\") is\nfuzzy.\n\nSo how about:\n    If a Content-Length header field (section 14.13) is present, its\n    decimal value in OCTETs represents both the entity-length and the\n    transfer-length. The Content-Length header field MUST NOT be sent\n    if these two lengths are different (i.e., if a Transfer-Encoding\n    header field is present).  If a message is received with both a\n    Transfer-Encoding header field and a Content-Length header field,\n    the latter MUST be ignored.\n\nMaybe a little more rigid than formally necessary, but I think\nwe need to take a strong stand against non-compliance in this\narea (as we did with the Host header), or else we could end up\nin a bad mess.\n\n-Jeff\n\n\n\n", "id": "lists-012-8044922"}, {"subject": "Re: HTTP 1.1 issue 07: 4.4 Message Lengt", "content": "> From: Jeffrey Mogul <mogul>\n> Date: Wed, 11 Nov 98 13:36:22 PST\n> To: jg@pa.dec.com (Jim Gettys)\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: HTTP 1.1 issue 07: 4.4 Message Length\n> -----\n>     > In section 4.4 \"Message Length\", does the statement\n>     >    \"If a Content-Length header field (section 14.13) is present, its\n>     >    decimal value in OCTETs represents both the entity-length and the\n>     >    transfer-length. The Content-Length header field MUST NOT be used\n>     >    if these two lengths are different (i.e., if a Transfer-Encoding\n>     >    header field is present).\"\n>     > mean that a receiver should ignore Content-Length, or a sender should\n>     > not send it?\n> \n>     It means that the sender should not send it in the case of a\n>     transfer encoded message, since the transfer-length is otherwise\n>     encoded and would result in a contradiction in the lenth.\n> \n>     I don't see an obvious rewrite to make this more obvious.  Unless\n>     there is some concrete suggestion, I plan to leave this one alone.\n> \n> \"Use the Robustness Principle, Luke!\"\n> \n> It means both: it means that the sender MUST NOT send it\n> and that (in the event of a non-compliant sender) the receiver\n> MUST ignore it.  I agree that the verb here (\"be used\") is\n> fuzzy.\n> \n> So how about:\n>     If a Content-Length header field (section 14.13) is present, its\n>     decimal value in OCTETs represents both the entity-length and the\n>     transfer-length. The Content-Length header field MUST NOT be sent\n>     if these two lengths are different (i.e., if a Transfer-Encoding\n>     header field is present).  If a message is received with both a\n>     Transfer-Encoding header field and a Content-Length header field,\n>     the latter MUST be ignored.\n> \n> Maybe a little more rigid than formally necessary, but I think\n> we need to take a strong stand against non-compliance in this\n> area (as we did with the Host header), or else we could end up\n> in a bad mess.\n> \n\nConcrete suggestion gratefully accepted.  My light saber is getting limp\nat this date on this spec... :-)\n- Jim\n\n\n\n", "id": "lists-012-8054082"}, {"subject": "Re: HTTP 1.1 issue 02: 3.11 Entity Tag", "content": "    > In section 3.11 \"Entity Tags\", the statement\n    > \n    >    \"A given entity tag value MAY be used for entities obtained by\n    >    requests on different URIs without implying anything about the\n    >    equivalence of those entities.\"\n    > \n    > confuses me to the point of being unable to parse it, let alone\n    > summarize it.\n    >\n    \n    How about rewriting this sentence to:\n    \n       \"A given entity tag value MAY be used for entities obtained by\n       requests on different URIs and does not imply the\n       equivalence of those entities.\"\n    \nOn reflection, I don't think this rewrite is that much easier\nto parse, since it's not clear what the subject of \"does not imply\"\nis.\n\nHow about\n       A given entity tag value MAY be used for entities obtained by\n       requests on different URIs.  The use of the same entity tag\n       value in conjunction with entities obtained by requests on\n       different URIs does not imply the equivalence of those\n       entities.\n\n-Jeff\n\n\n\n", "id": "lists-012-8064050"}, {"subject": "Re: HTTP 1.1 issue 20: 13.1.2 Warning", "content": "Jim suggests:\n\n    [replace] the 14.46 text with:\n    \n      The caching behavior of warnings is discussed in section 13.1.2.\n    \nI think it might be better to say something like\n\n      Requirements for the behavior of caches with respect to\n      Warnings are stated in section 13.1.2.\n\nI think it's probably worth making it clear to implementors\nthat 13.1.2 contains *requirements*, not just *discussion*.\n\nThis is the original reason why similar requirements were\nstated in both places.  It's hard to understand the discussion\nwithout a statement of the requirements (so I'm glad that you\nkept these in 13.1.2), but it's also somewhat confusing\nto have the requirements w.r.t. the Warning header field\nspread over several non-adjacent sections.  However, if\nan explicit cross-reference to *requirements* is made,\nI'm satisfied.\n\n-Jeff\n\n\n\n", "id": "lists-012-8071966"}, {"subject": "Re: HTTP 1.1 issue 20: 13.1.2 Warning", "content": "> From: Jeffrey Mogul <mogul>\n> Date: Wed, 11 Nov 98 14:30:39 PST\n> To: jg@pa.dec.com (Jim Gettys)\n> Cc: http-wg@hplb.hpl.hp.com\n> Subject: Re: HTTP 1.1 issue 20: 13.1.2 Warnings\n> -----\n> Jim suggests:\n> \n>     [replace] the 14.46 text with:\n> \n>       The caching behavior of warnings is discussed in section 13.1.2.\n> \n> I think it might be better to say something like\n> \n>       Requirements for the behavior of caches with respect to\n>       Warnings are stated in section 13.1.2.\n> \n> I think it's probably worth making it clear to implementors\n> that 13.1.2 contains *requirements*, not just *discussion*.\n> \n> This is the original reason why similar requirements were\n> stated in both places.  It's hard to understand the discussion\n> without a statement of the requirements (so I'm glad that you\n> kept these in 13.1.2), but it's also somewhat confusing\n> to have the requirements w.r.t. the Warning header field\n> spread over several non-adjacent sections.  However, if\n> an explicit cross-reference to *requirements* is made,\n> I'm satisfied.\n> \n\nYou are right; I'll make the cross reference as you suggest.\n- Jim\n\n\n\n", "id": "lists-012-8080123"}, {"subject": "Re: HTTP 1.1 issue 15: 14.23 Hos", "content": ">If it is relaxed at all, it might be relaxed in the following way:\n>\n>  \"A client MUST include a Host header field in all HTTP/1.1 request messages \n>   on any message corresponding to a request for a URI which includes \n>   an Internet host for the naming authority. All HTTP/1.1 servers MUST \n>   respond with a 400 (Bad Request) status code to any such HTTP/1.1 request \n>   message which lacks a Host header field. If the Host field is not already \n>   present, an HTTP/1.1 proxy MUST add a Host field to such a request message \n>   prior to forwarding it.\"\n>\n>This would bring it in line with the draft standard URI specification \n>as well, and make it clear that for other URL schemes which might also \n>have a host name for the naming authority that we demand the host \n>information, while making it clearer that for schemes that don't have \n>a host name for a naming authority that we don't need a Host header.\n\nYes, either that or\n\n    A client MUST include a Host header field in all HTTP/1.1 request\n    messages.  If the requested URI does not include an Internet hostname\n    for the service being requested, then the Host header field MUST be\n    given with an empty value.  An HTTP/1.1 proxy MUST ensure that any\n    request message it forwards does contain an appropriate Host header\n    field that identifies the service being requested by the proxy.\n    All Internet-based HTTP/1.1 servers MUST respond with a 400 (Bad Request)\n    status code to any HTTP/1.1 request message which lacks a Host\n    header field.\n\nBoth will give people doing things like name-to-location resolution via\nHTTP some guidance.\n\n....Roy\n\n\n\n", "id": "lists-012-8088682"}, {"subject": "Re: HTTP 1.1 issue 21: 13.3.3 Weak and Strong Validator", "content": "    > In section 13.3.3 \"Weak and Strong Validators\", the statements\n    > \n    >    \"The weak comparison function MAY be used for simple\n    >    (non-subrange) GET requests. The strong comparison function MUST\n    >    be used in all other cases.\"\n    > \n    > and\n    > \n    >    \"A cache or origin server receiving a conditional request, other\n    >    than a full-body GET request, MUST use the strong comparison\n    >    function to evaluate the condition.\"\n    > \n    > state the same requirement.\n    \n    It isn't quite the same thing.  One states requirements on the\n    client, and the other on origin servers or proxies.  It is a bit\n    confusing, and may be redundant.\n    \n    Jeff, what is the right thing to do here?\n\nIt may be a bit confusing, but it's not really redundant.  The\nproblem is the passive voice (i.e., lack of explicit subject).\nOne might change the first statement to read:\n\n   Clients MAY use the weak comparison function for simple\n   (non-subrange) GET requests.  They MUST use the strong \n   comparison in all other cases.\n\nHowever, on reflection, it's a little more complicated than\nthis.  Since the comparison is normally done at the server,\nwhat we really want to do is to prevent the client from\nissuing a request that cannot be properly evaluated.\n\nSo, how about moving this paragraph up before the\nprevious one (begins with \"The only function\"), and changing\nit to read:\n\n   Clients MAY issue simple (non-subrange) GET requests\n   with either weak validators or strong validators.\n   Clients MUST NOT use weak validators in other forms\n   of request.\n\nWhich makes it basically a formal statement of the informal\ndescription in the next previous paragraph.\n\n-Jeff\n\n\n\n", "id": "lists-012-8097803"}, {"subject": "Re: HTTP 1.1 issue 16: 14.26 If-NoneMatc", "content": "Ross Patterson writes:\n\n    In section 14.26 \"If-None-Match\", the statements\n    \n       \"If \"*\" is given and no current entity exists, then the\n       server MAY perform the requested method as if the\n       If-None-Match header field did not exist.\"\n\n    and\n\n       \"The meaning of \"If-None-Match: *\" is that the method\n       MUST NOT be performed if the representation selected by\n       the origin server (or by a cache, possibly using the Vary\n       mechanism, see section 14.44) exists, and SHOULD be\n       performed if the representation does not exist.\"\n\n    conflict on the topic of \"If-None-Match: *\". The former\n    asserts that the server MAY perform the request, the latter\n    that it SHOULD.  I think the MAY is correct.\n\nHmm.  I confess that I no longer remember all of the\ndiscussion behind this, but I lean towards SHOULD.\n\nConsider the request:\n\nPUT /foo.data HTTP/1.1\nHost: whatever.com\nIf-None-Match: *\nContent-Length: 0\n\nThe basic intent is to prevent a race condition where the\norigin server performs this request and overwrites an existing\nresource when the client didn't realize it already exists.\nBut in the absence of that pre-existing resource, presumably\nwe want the effect to be the same as\n\nPUT /foo.data HTTP/1.1\nHost: whatever.com\nContent-Length: 0\n\nOf course, I don't think we ever state whether a server\nSHOULD or MAY perform any arbitrary (but valid) request\nthat it receives.  MUST would be too strong - there might\nbe resource constraints that the server needs to observe.\nBut I think it would be unfortunate if HTTP servers started\nrejecting requests for essentially random reasons, and\nnot what the average user expects, and so \"SHOULD\" seems\nlike the more appropriate choice.\n\nI'll avoid commenting on whether any existing implementations\ndo, in fact, reject requests for seemingly random reasons :-)\n\n-Jeff\n\n\n\n\n\n", "id": "lists-012-8106643"}, {"subject": "Re: HTTP 1.1 issue 16: 14.26 If-NoneMatc", "content": "At 14:58 11/11/98 PST, Jeffrey Mogul wrote:\n>Ross Patterson writes:\n>\n>    In section 14.26 \"If-None-Match\", the statements\n>    \n>       \"If \"*\" is given and no current entity exists, then the\n>       server MAY perform the requested method as if the\n>       If-None-Match header field did not exist.\"\n>\n>    and\n>\n>       \"The meaning of \"If-None-Match: *\" is that the method\n>       MUST NOT be performed if the representation selected by\n>       the origin server (or by a cache, possibly using the Vary\n>       mechanism, see section 14.44) exists, and SHOULD be\n>       performed if the representation does not exist.\"\n>\n>    conflict on the topic of \"If-None-Match: *\". The former\n>    asserts that the server MAY perform the request, the latter\n>    that it SHOULD.  I think the MAY is correct.\n>\n>Hmm.  I confess that I no longer remember all of the\n>discussion behind this, but I lean towards SHOULD.\n\nI think it actually is correct although the logic maybe more complex than\nneed be:\n\n*If* there is an \"If-None-Match: *\"  header *and* no representation exists\nthen the server can either a) discard the \"If-None-Match: *\" or b) use it\naccording to the algorithm outlined in the second paragraph above. Assuming\nthat your example below SHOULD work, then the result should be the same.\n\nHowever, I am not sure that a) in fact makes sense as presumably the server\nwould have used the existence of the \"If-None-Match: *\" header to check for\nexisting representations before finding out that it could have ignored it.\nThis is like saying: \"After you have used it you may find out that you\ndon't need it, but that's OK.\"\n\nTherefore, I would suggest that the first paragraph is simply removed. The\nsecond handles all cases as far as I can see.\n\n>Consider the request:\n>\n>PUT /foo.data HTTP/1.1\n>Host: whatever.com\n>Content-Length: 0\n\nHenrik\n\n\n\n", "id": "lists-012-8116112"}, {"subject": "ADAMS1, point 31. (cachability of methods)", "content": "Query to the list on something more than usual in the editorial\nprocess:\n\nGlenn asks: \n\n> \n> 31. Section 9.2., pg. 49, 2nd para., states \"Response to this method are\n> not cachable.\" Should this be made stronger with either MUST NOT or\n> SHOULD NOT?\n> The same comment applies in a variety of other context regarding the\n> suitability or non-suitability of caching a response.\n\nThis problem occurs in section 9.2, 9.5, 9.6, 9.7 (methods OPTIONS,\nPOST, PUT, DELETE.\n\nAt first blush, it would seem that Glenn's MUST NOT suggestion would\nbe best.\n\nBut...\n\nPOST says:\n\n\"Responses to this method are not cachable, unless the response includes\nappropriate Cache-Control or Expires header fields. \"\n\nHere's my question:\n===================\n\nShould the text for POST not in fact be a general statement for methods, \n(saying \"MUST NOT be cached\", rather than \"are not cachable\" to make it\ncrystal clear).  This would make things much more consistent, and allow\na origin server control over what is going on).\n\nThis would result in putting this sentence in secion 9:\n\n\"Responses to methods other than GET or HEAD MUST NOT be cached, unless \nthe response includes appropriate Cache-Control or Expires header fields\"\n\nAnd removing the statements about \"Response to this method are not cachable.\"\nin section 9.2, 9.5, 9.6, 9.7 (methods OPTIONS, POST, PUT, DELETE.)\n\nBesides the cleanlyness and simplification this change would make, the \nother big feature of this is that it makes caching behavior crystal clear \nfor extensions to HTTP - that caching is not allowed unless the server \nmarks the response cachable.\n\nIs there any danger in this I don't see?\n- Jim\n\n\n\n", "id": "lists-012-8125577"}, {"subject": "RE: ADAMS1, point 31. (cachability of methods)", "content": "In most cases, the 'cachable' constraint is not on whether\nyou store the value in a cache, but whether you USE the\ncached value. Now, it may be foolish to cache something that\nyou cannot use, but perhaps not; e.g., even if the results\naren't exactly cachable, could you use delta-coding on the\nnext request with an appropriate entity tag?\n\nI don't know whether it's best to say SHOULD NOT and let\nimplementors figure out the constraints, or to go ahead\nand use MUST NOT, but to carefully redefine 'cachable' or\n'cached' to mean that the cached value cannot be used,\nand thus probably shouldn't be stored.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n> > 31. Section 9.2., pg. 49, 2nd para., states \"Response to this method are\n> > not cachable.\" Should this be made stronger with either MUST NOT or\n> > SHOULD NOT?\n> > The same comment applies in a variety of other context regarding the\n> > suitability or non-suitability of caching a response.\n>\n> This problem occurs in section 9.2, 9.5, 9.6, 9.7 (methods OPTIONS,\n> POST, PUT, DELETE.\n>\n> At first blush, it would seem that Glenn's MUST NOT suggestion would\n> be best.\n>\n> But...\n>\n> POST says:\n>\n> \"Responses to this method are not cachable, unless the response includes\n> appropriate Cache-Control or Expires header fields. \"\n>\n> Here's my question:\n> ===================\n>\n> Should the text for POST not in fact be a general statement for methods,\n> (saying \"MUST NOT be cached\", rather than \"are not cachable\" to make it\n> crystal clear).  This would make things much more consistent, and allow\n> a origin server control over what is going on).\n>\n> This would result in putting this sentence in secion 9:\n>\n> \"Responses to methods other than GET or HEAD MUST NOT be cached, unless\n> the response includes appropriate Cache-Control or Expires header fields\"\n>\n> And removing the statements about \"Response to this method are\n> not cachable.\"\n> in section 9.2, 9.5, 9.6, 9.7 (methods OPTIONS, POST, PUT, DELETE.)\n>\n> Besides the cleanlyness and simplification this change would make, the\n> other big feature of this is that it makes caching behavior crystal clear\n> for extensions to HTTP - that caching is not allowed unless the server\n> marks the response cachable.\n>\n> Is there any danger in this I don't see?\n> - Jim\n\n\n\n", "id": "lists-012-8134315"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "Jim Gettys:\n>\n[...]\n>Here's my question:\n>===================\n>\n>Should the text for POST not in fact be a general statement for methods, \n>(saying \"MUST NOT be cached\", rather than \"are not cachable\" to make it\n>crystal clear).  This would make things much more consistent, and allow\n>a origin server control over what is going on).\n>\n>This would result in putting this sentence in secion 9:\n>\n>\"Responses to methods other than GET or HEAD MUST NOT be cached, unless \n>the response includes appropriate Cache-Control or Expires header fields\"\n\nI believe that the spec makes the above statement somewhere already,\nthough I just tried to find it and failed.  Jeff?\n\n>And removing the statements about \"Response to this method are not cachable.\"\n>in section 9.2, 9.5, 9.6, 9.7 (methods OPTIONS, POST, PUT, DELETE.)\n\nI have always interpreted these 'are not cachable' lines as useful\nreminders of the general rule I just failed to find.  Removing them\nwould decrease the readability of the document I think.\n\n>Besides the cleanlyness and simplification this change would make, the \n>other big feature of this is that it makes caching behavior crystal clear \n>for extensions to HTTP - that caching is not allowed unless the server \n>marks the response cachable.\n>\n>Is there any danger in this I don't see?\n\nAs far as I can see making these changes does not introduce some new\ndanger.\n\n>- Jim\n\nKoen.\n\n\n\n", "id": "lists-012-8144397"}, {"subject": "ADAMS1, point 28 (connection management", "content": "> 28. Section 8.2.3, pg. 45, has the phrase \"(Confirmation by user-agent\n> software with semantic understanding of the application MAY substitute\n> for use confirmation.)\" This appears to controvert the stronger language\n> in Section 8.1.4, para. 4, which does not have this parenthetical note.\n\nYes, it is a relaxation if the user agent understands what is going on\n(e.g. java or javascript applications).\n\nActually, section 8.2.3 looks almost entirely redundant with the 4th\nparagraph of 8.1.4 (with the exception of the parenthetical remark),\nthough slightly contradictory (a MUST NOT vs. a SHOULD NOT).\n\nSo I think deleting section 8.2.3, and moving the sentence about sofware \nwith semantic understanding to that paragraph of  8.1.4 results in:\n\n  \"This means that clients, servers, and proxies MUST be able to recover \n   from asynchronous close events. Client software SHOULD reopen the transport \n   connection and retransmit the aborted sequence of requests without user \n   interaction so long as the request sequence is idempotent (see section \n   9.1.2). Non-idempotent methods or sequences MUST NOT be automatically \n   retried, although user agents MAY offer a human operator the choice \n   of retrying the request(s). Confirmation by user-agent software with \n   semantic understanding of the application MAY substitute for user \n   confirmation. The automatic retry SHOULD NOT be repeated if the second \n   sequence of requests fails.\"\n\nwill solve this.\n\n- Jim\n\n\n\n", "id": "lists-012-8153005"}, {"subject": "Re: Comments (Part 1) on HTTP ID Rev 05 (ADAMS01 point 41)", "content": "> \n> 41. Section 10.3.2, pg. 58, 1st para., states \"The requested resource\n> has been assigned a new permanent URI and any future references to\n> this resource SHOULD be done using one of the returned URIs.\" This is\n> an onerous requirement on UAs unless they happen to have link editing\n> capabilities. Should be qualified to not apply to UAs without such\n> capability; otherwise, no UA of this type will ever be unconditionally\n> compliant. Alternatively, change this requirement to MAY.\n\nYes, I see the point, and this was clearly not our intent, as the\nfollowing sentence explicitly deals with the case of UA's with\nlink editing capability, where it recommends relinking.  I also\nthink that the SHOULD in the sentence about link editing capabilities\nis incorrect; it is weasle worded with \"where possible\", and I don't\nthink it was originally intended it should be SHOULD.\n\nSo I think a proper replacement for the paragraph is.\n\n \"The requested resource has been assigned a new permanent URI and the \n  reference to this resource SHOULD use one of the returned URIs.  \n  Clients with link editing capabilities ought to automatically re-link \n  references to the Request-URI to one or more of the new references returned \n  by the server, where possible. This response is cachable unless indicated \n  otherwise.\"\n- Jim\n\n\n\n", "id": "lists-012-8161824"}, {"subject": "Re: Comments (Part 1) on HTTP ID Rev 05 (ADAMS1", "content": "Here's what I percieve to be the less significant of Glenn's comments, with\nmy intended resolutions.  If anyone wants to take issue with any of these\nresolutions, please indicate the comment number in the subject\nline so we can keep track of them.\n- Jim\n\n> From:Adams, Glenn\n> Sent:Monday, October 26, 1998 11:13 AM\n> To:'http-wg@cuckoo.hpl.hp.com'\n> Subject:Comments (Part 1) on HTTP I-D Rev 05\n> \n> I'm not certain which form is preferred, sending comments en masse or\n> individually. If the\n> latter is desired, let me know and I'll break these out. Of the\n> following, comments 6, 10, 22,\n> 25, 30, 37, 38, and 41 are potentially substantive issues. These\n> comments cover sections\n> 1-11; I intend to complete my comments later this week on the remaining\n> sections.\n> \n> 1. Section 1.2 fails to state that implementations that fail\n> to satisfy statements marked as \"REQUIRIED\" would not qualify\n> as compliant. Otherwise, suggest replacing REQUIRED with MUST or\n> MUST NOT for the sake of consistency.\n\nOk, I'll say \"An implementation is not compliant if it fails to to satisfy\nMUST or REQUIRED level requirements for the protocols it implements.\"\n\n> \n> 2. Section 1.2 should indicate the status of these keywords in\n> \"Notes\". Are the use of these keywords in notes normative?\n\nI don't believe this is a problem in draft 05; we already took\nnormative text out of notes, and a quick scan of the document\nlooks like I succeeded.\n\n> \n> 3. Section 2.1, pg. 15, \"implied *LWS\", contains what appears\n> to be an editorial note \"[jg13]\".\n> \n\nMicrosoft Word droppings.  I'll fix.\n\n\n> 4. Section 2.2, pg. 16, definition of \"CTL\", fails to consider that\n> ASCII (and ISO646-1993) consider SPACE (040) to be a control character\n> of the same status as DEL (177).\n\nSorry, no, we handle space differently than CTL, and the BNF reflects this.\n\n> \n> 5. Section 2.2, pg. 17, 1st para., has a forward reference to\n> \"parameter value\". Should add a cross reference to the section that\n> defines this non-terminal.\n\nI'll add a cross reference to section 3.6, where parameter values are\ndefined first.\n\n> \n> 6. Section 3.4, pg. 21, specifies that \"the definition associated with\n> a MIME character set name MUST fully specify the mapping ...\". Should\n> this not be a requirement placed on the registrant of a MIME character\n> set and not an HTTP implementation? Or, is this requirement really\n> stating that any HTTP implementation must maintain a table of registered\n> character sets known to satisfy this requirement and MUST NOT use any\n> character set not present in this table? Overall, this seems an onerous\n> requirement for an HTTP implementation.\n> \n\nI'm not the MIME expert of the working group, but I take this to mean \nthat this is just a restriction on which character sets may be used, and \nimplies there are character sets that do not meet this requirement by \nhaving external profiling information.  Maybe a MIME expert can confirm \nthis one.\n\n\n> 7. Section 3.6, pg. 24, 3rd para., states \"... (IANA) acts as a registry\n> for transfer-coding value tokens\" and goes on to list the initial set\n> of registered tokens in which Content-Encoding tokens are included.\n> Should\n> this not state \"acts as a registry for transfer and content coding value\n> tokens\"?\n\nThere are two independent registries, and the document defines\nboth independently.  I don't think this is a help.\n\n> \n> 8. Section 3.6, pg. 25, 5th para., uses the term \"optional metadata\"\n> without\n> providing further definition of what such \"metadata\" might be. Suggest\n> an\n> example here or clarification.\n\nNo, I don't think an example will be helpful here; it might just be\nmisleading.\n\n> \n> 9. Section 3.6, pg. 25, 6th para., discusses a \"situation\" regarding\n> interoperability failure. This \"situation\" should be described more\n> fully\n> or an example given to make clear what the problem is.\n\nThis was discussed at length in the list, and is quite subtle, requiring\nquite a few conditions before the situation arises.\n\nIt would take more space to describe than is appropriate for a protocol\nspec, though we allude to the problem enough that someone might\nbe able to figure it out from first principles.  So since we don't\nhave annotation capabilities, I think it is best to keep things\nas it is now.\n\n> \n> 10. Section 3.7.1, pg. 26, 1st para., states \"An entity-body transferred\n> via HTTP messages MUST be represented in the appropriate canonical form\n> prior to its transmission except for \"text\" types ...\". Is it actually\n> the\n> case that servers are validating canonical status of entity bodies? This\n> contradicts the \"entity-body as payload\" philosophy.\n\nNo, entities are always payload.  The requirement is that you have to\nplay by MIME rules for that data type, but we acknowledge the UNIX usage\nof newline line terminators means that text document line terminators\ndon't play by MIME rules.  The Web has worked this way (just ship the\nbits) from day one, and any arguments that it should play by MIME rules\nfor text payload at this date are doomed to failure.\n\n> \n> 11. Section 3.7.1, pg. 26, 2nd para., uses the phrases \"allows\" and\n> \"allows\n> the use of\". Should these be rephrased using the \"MAY\" keyword? The same\n> comment applies elsewhere when the work \"allows\" or \"permitted\" is used.\n> \n\nMaybe they should be rephrased.  But not at this date. There are alot \nof instances of \"allows\" or \"permitted\", and the rephrasing is more than \njust a single word change, and often might result in more awkward text; \nthis should have been brought up earlier this year when we were doing \nthe general MUST/MAY/SHOULD audit.  At this stage, I want to lean toward \n\"first, do no harm\".\n\n> 12. Section 3.7.2, pg. 27, 2nd para., states \"In all other cases, an\n> HTTP\n> user agent SHOULD follow the same or similar behavior as a MIME user\n> agent\n> would ...\". This \"implied\" behavior needs to be made explicit. What is\n> the behavior of a MIME user agent in this context?\n\nI think you should go read the MIME specs to find out; HTTP incorporating\nrecommendations for what MIME should do here is a great way for specs\nto end up contradictory.\n\n> \n> 13. Section 3.7.2, pg. 27, 4th para., contains a note regarding\n> \"multipart/\n> form-data\". Why is this specific type given a special note? How about\n> \"multipart/byte-ranges\"?\n\nJust to get a reference in to another multipart type used in the Web.\nMultipart/byteranges are defined in this document.\n\n> \n> 14. Section 3.8, pg. 28, 1st para., states \"Product tokens SHOULD be\n> short\n> and to the point.\" and \"They MUST NOT be used for advertising or other\n> non-essential information.\" As an implementer, how can one interpret\n> these\n> requirements? Either make quantify them or remove them.\n\nI think common sense is in order here. Keep'em short.  Your customers will\nthank you (lower latency, fewer bytes).\n\nWe've seen people put the kitchen sink in them.\n\nUnless others complain, I plan to keep these as is.\n\n> \n> 15. Section 3.9 refers to \"short 'floating point' numbers\". I would\n> suggest\n> replacing this with \"real numbers\" since both \"short\" and \"floating\n> point\"\n> seems to implementation specific.\n\nThe BNF follows and is very specific.  I think it is fine as is.\n\n> \n> 16. Section 3.10 never actually says that RFC1766 language tags \"MUST\"\n> be\n> used. I'd suggest adding stronger language here.\n\nI think it is pretty clear as is.\n\n> \n> 17. Section 4.2, pg. 31, 4th para., states \"It MUST be possible ...\". I\n> would suggest replacing this with a statement that uses the converse\n> using the\n> form \"MUST NOT ... unless ...\"; e.g., \"Multiple header fields MUST NOT\n> be\n> combined into one header unless ...\".\n> \n\nNo, this is a requirement on the protocol, and for future HTTP protocols, \ndue to exisiting practice.  See my reply to issue ROSS05 for further \ndiscussion.\n\n> 18. Section 4.3, pg. 31, 5th para., states \"The presence of a\n> message-body\n> in a request is signaled by the inclusion of Content-Length or Transfer-\n> Encoding header field ...\".  However, \"multipart/byte-ranges\" may\n> include\n> a message-body without either of these headers.\n\nThe operative words are in the first sentence:\n\"The presense of a message-body *in a request*\".\n\n> \n> 19. Section 4.4, pg. 32, 2nd para., has the relative clause \"... which\n> MUST\n> NOT ...\". This is not a requirement, so should not use these keywords.\n> Suggest\n> using \"does not\".\n\nAdding quotes around \"MUST NOT\" is better than saying \"does not\",\nas a response might be empty on other types of responses, therefore be\nincorrect.\n\n> \n> 20. Section 4.4, pg. 32, last para., the \"Note\" uses \"may\" and \"must\".\n> If\n> keyword usage in notes is not normative, then this should be stated in\n> Section 1.2.\n\nProbably shouldn't be a note; I believe the\nrequirements are actually explicit elsewhere. The may should be \"might\"\nin any case (we are specifying HTTP/1.1 here, not what HTTP/1.0\nproxies might do), though the MUST arguably should be capitalized.\n\n> \n> 21. Section 4.4, pg. 32, 1st para., uses the phrase \"cannot be\". Suggest\n> rephrasing to use \"MUST NOT\".\n\nNo, it is an explanation of why a client can't take the same action\na server might take.\n\n> \n> 22. Section 4.4, pg. 32, 5th para., states \"HTTP/1.1 user agents MUST\n> notify the user when an invalid length is received and detected.\" This\n> does\n> not seem to be reflected by current industry practice (cf. IE4 and\n> Netscape\n> Communicator 4 behavior). If this standard is intended to capture\n> current\n> practice, then this is a broadening of current practice. I'd suggest\n> using\n> the keyword \"MAY\" instead.\n\nLook, this is a server being fundamentally broken.  We don't say\nthat a client can't attempt to continue in the face of this fundamental\nbrokenness, but we do want people to complain to the server operator\nabout it.  Having worked hard at getting persistent connections going,\nwe really need to get this to work properly, and having brokenness go\nundetected is a good way for systems to never get fixed or implemented\ncorrectly.  We don't constrain how the user gets notified, and industry\npractice is not an issue here: there are NO conforming HTTP/1.1 user\nagents at this time.  We are defining new practice, not old (a fundamental\ndifference between 2068/this document and RFC 1945).\n\n> \n> 23. Section 5.1.2, pg. 35, 3rd para., has \"three options\" when four\n> are described.\n> \n\nYup.  Dave Kristol reported this one too.\n\n> 24. Section 5.1.2, pg. 35, 5th para., uses the keyword \"REQUIRED\"\n> instead\n> of \"MUST\". It seems that \"MUST\" is given preference throughout this\n> document. The same comment applies to the use of \"OPTIONAL\" vs. \"MAY\".\n> \n\nSee section 1.2 and RFC 2119, as referenced in the document.\n\n> 25. Section 7.2.1, pg. 41, 4th para., gives considerable flexibility to\n> a recipient regarding the heuristic guessing of an entity's content\n> type.\n> In particular, no default interpretation is dictated. In contrast, no\n> flexibility is given in the heuristic determination of a \"text\" content\n> type's\n> character set (cf. Section 3.4, where a default of ISO8859-1 is\n> dictated).\n> I wonder why the two quite different approaches are maintained. In\n> particular,\n> I do know that the requirements of Section 3.4 will \"break\" many\n> existing\n> implementations which assume that the \"default\" is applied as a no more\n> than\n> a default heuristic in the absence of an explicit CHARSET and not as an\n> immediate override to any heuristics. I fully expect our East Asian\n> customers\n> to require this feature of Section 3.4 to be permanently disabled to\n> accommodate\n> existing practice.\n\nWe've already been through all this with our East Asian friends.  It\nreflects the reality of server's content (where Content-Type is often\nnot provided).  This is the mess of existing servers.\nThe best we can do with the current situation is\nrequire 1.1 to behave properly when told explicitly, and deal with exising\nbrokenness.\n\n> \n> 26. Section 8.1.3, p. 43, 1st para., has the typo \"in14.10.\" Should\n> instead\n> read \"in section 14.10.\".\n\nYup. I'll fix this.\n\n> \n> 27. Section 8.1.4, pg. 44, 6th para., has the phrase \"... SHOULD\n> maintain\n> AT MOST 2 connections ...\"; since \"AT MOST\" is not a keyword, suggest\n> rephrasing his requirement using \"SHOULD NOT maintain more than 2\n> connections\".\n> \n\nGood rewording.  Thanks.\n\n\n> \n> 29. Section 8.2.4, pg. 45, 1st para., uses the term \"end-client\". This\n> term seems to be nonstandard with other terminology regarding\n> communicating\n> parties in the HTTP context.\n\nYes, I think \"client\" should be sufficient here.\n\n> \n> 30. Section 9, pg. 48, 2nd para., appears to be partially redundant with\n> Section 5.1.2, pg. 35, line 2078 (in file). Furthermore, does this\n> requirement\n> actually hold for forms of Request-URI other than abs_path? For example,\n> does an OPTIONS * HTTP/1.1 request require a Host header?\n\nYes, that sentence is redundant, and exact wording elsewhere is already under\ndiscussion in ROSS15.\n\n> \n> 31. Section 9.2., pg. 49, 2nd para., states \"Response to this method are\n> not cachable.\" Should this be made stronger with either MUST NOT or\n> SHOULD NOT?\n> The same comment applies in a variety of other context regarding the\n> suitability or non-suitability of caching a response.\n> \n\nSee separate message on this topic.\n\n> 32. Section 9.3, pg. 50, 4th para., uses the expression \"if and only if\n> ...\".\n> Suggest using \"MUST NOT unless\" instead.\n\nNo, the actual requirements are in section 13.  Saying MUST NOT here\nis being repetatively redundant. :-)  If and only if is pointing\nout to readers that they really better follow the cross reference.\n\n> \n> 33. Section 9.6, pg. 51, 1st para., uses the phrase \"the origin server\n> can\n> create ...\". Suggest using MAY instead. Should review other uses of\n> \"can\"\n> in this document for similar substitution. Same comment applies to uses\n> of\n> \"cannot\" which in most cases should be replaced with \"MUST NOT\".\n\nI don't think this helps; and in fact, cannot is generally used in\nstating fact, not requirements on the protocol.\n\n> \n> 34. Section 9.6, pg. 52, 3rd para., uses the phrase \"server\" where\n> \"origin\n> server\" appears to be implied. Suggest reviewing uses of \"server\" for\n> possible\n> narrower semantics.\n\nServers can be origin servers or proxies.\n\nOrigin servers are places where documents originate.\n\nI think the terminology is correct as applied; we've tried to be very \ncareful here.  Please see the terminology section in 1.3 for definitions.  \nCertainly your specific complaint in 9.6 applies to servers in general, \nnot just to origin server.\n\nIf you have specific complaints, please make them.\n\n> \n> 35. Section 9.8, pg. 53, 3rd para., note \"Responses to this method MUST\n> NOT\n> be cached.\" while most other methods have \"Responses to this method are\n> not\n> cachable.\" (cf. Section 9.6, 9.7). Suggest making this language more\n> consistent.\n\nSee separate mail on this topic.\n\n> \n> 36. Section 9.9 may wish to substitute its reference [44] with the new\n> I-D\n> <draft-luotonen-web-proxy-tunneling-01.txt>. However, note that the\n> argument to the CONNECT method prescribed by this I-D is not conformant\n> with the specification of \"Request URI\" in Section 5.1.2. Perhaps the\n> reference to the tunneling draft should be removed altogether with this\n> keyword just stated as \"reserved\"?\n\nIf you look at the reference [44], you will see it is to Ari's draft.\n\nAll the HTTP spec does is reserve the name CONNECT; I can't control\nwhat Ari's draft says.\n\nNot having any reference seems braindead; people should be able to find\nout what the method name is used for.\n\n> \n> 37. Section 10.2.5, pg. 56, 2nd para., states \"any new or updated\n> metainformation SHOULD be applied to the document currently in the user\n> agent's active view.\" This conditional requirement seems to be place a\n> constraint on UA semantics outside the scope of HTTP proper. Suggest\n> changing SHOULD to MAY.\n\nWe don't constrain how or what an UA does with the metainformation, just \nthat if it is in view and they get a 204 No Content, whatever is viewed \ngets updated.  This seems reasonable to me.  A content provider should \nbe able to predict what happens at the end user viewpoint or what good \nwould this response be?  For example, even though the content hasn't changed, \nthe expiration date might have changed.  If a client is displaying that \nmeta-data, it had really better display the updated expiration date.  \nRemember, this is an optimization so that you don't have to send a whole \nentity just because something else has changed.  If it can't be relied \non to show the user what has changed, the only alternative a content provider \nwill have is to send the whole entity again, defeating the attempted \noptimization entirely.\n\n> \n> 38. Section 10.2.6 states \"the user agent SHOULD reset the document\n> view\".\n> This conditional requirement seems to place a constraint on UA semantics\n> outside the scope of HTTP proper. Suggest changing SHOULD to MAY.\n\nRelaxing the requirement would defeat the purpose entirely.  It is\nentirely proper for HTTP to constrain the UA semantics; it is inappropriate\nto demand how the semantic meaning be met (i.e. the UA syntactic\npresentation of the semantic intent).  If a content provider can't\nrely on the semantics of a message, what can they rely on?\n\n> \n> 39. Section 10.2.7, pg. 56, 1st para., uses \"MUST\" in the past tense.\n> Suggest rephrasing this to not use past tense.\n\nIt seems appropriate to me as is to use the past tense; it is referring\nto the request on a response.\n\n> \n> 40. Section 10.2.7, pg. 57, 2nd para., states \"the response MUST include\n> all of the entity-headers that would have been returned ...\".  Which\n> entity-headers are these precisely?\n\nWhatever headers you would have returned on a 200 response, as the\ntext indicates.  Since this is an arbitrary set you might generate,\nthere is no way to enumerate them; what you can do is to tell\npeople to \"do what they'd do in the normal case\".\n\nThe point of this code was extensively discussed on the mailing list;\nthe problem is that the headers can be very large, and having to\nalways send them on a range request would be a bummer.\n\n\n> \n> 42. Section 10.3.2, pg. 58, 2nd para., states \"the entity of the\n> response\n> SHOULD contain a short hypertext note ...\". Suggest formalizing this to\n> state a specific content type \n\nNo, you can't specify the content type; for example, with XML's deployment,\nit will be perfectly appropriate if the note is in XML (if sent to\nan XML capable user agent).\n\n> or, alternatively, not use the term\n> hypertext.\n> The same comment applies in a number of other Sections: search for\n> \"short\n> hypertext note\".\n\nCome now, this is HTTP after all \"HYPER-TEXT TRANSPORT PROTOCOL\", after\nall.  The notes are for the benefit of the deployed base of user\nagents that don't know about the new features, so that end users\ncan continue to use them (though more clumsily).\n\nThe UA's I'm familiar with all understand HTML...\n\n> \n> 43. Section 10.3.3, pg. 58, 1st para., states \"This response is only\n> cachable if indicated by a Cache-Control or Expires header field.\" In\n> contrast,\n> other Sections (cf. 10.3.1, 10.3.2, etc.) have \"This response is\n> cachable\n> unless indicated otherwise.\" Suggest making these more consistent if\n> possible\n> or referring to Section 13.4.\n\nI think things are already consistent; there are two cases here:\nthings that are cachable unless marked uncachable, and things that are\nnot cachable by default unless marked cachable.\n\nThese seem the same, but are not quite the same, as for historical \n(hysterical) reasons, HTTP has not had its act together on a consistent \ncaching model.  So we acknowledge existing caching practic, but allow \nimplementations to relax the current constraints if the server says it \nis OK.  This should enable a wider range of responses to be cachable.\n\nFor example, alot of POST responses are potentially cachable, but aren't\ncached today; in HTTP/1.1 a server can be clever and mark the response\ncachable if it is safe to do so.  A classic example is a search engine\nwhich updates its underlying database once per day; it should be\nable to mark all responses valid to the same query for at least the\ntime until the next database update.\n\n\n> \n> 44. Section 10.3.6 has a note describing \"significant security\n> consequences\".\n> Could these consequences be detailed somewhere in this specification?\n> \n\nIt could be, but as few people had implemented 305 before we understood \nthe problem and issued a previous draft with corrections, I don't think \nit is worth the space.  It had extensive discussion in the mailing list.\n\nThe reason it is limited to origin servers is that only the origin\nserver is authoritative for that resource.  A proxy might otherwise\nbe redirecting things over which it is not authoritative.\n\n\n>45. Section 10.3.7 has a typo. Change \"... specification, and is no\n> longer ...\"\n> to \"... specification, is no longer ...\".\n\nSure.  I'll fix.\n\n> \n> 46. Section 10.4, pg. 61, 1st para., has a superfluous comma after \"the\n> response\".\n\nIt is actually the second paragraph.  I'll fix.\n\n> \n> 47. Section 10.4.8 has \"This code is similar to 401 (Unauthorized), but\n> indicates that the client MUST first authenticate ...\" This doesn't seem\n> to be a requirement but a statement of fact. Suggest changing to \"but\n> indicates that the client did not first authenticate itself or its\n> credentials were not accepted ...\".\n\nSee issue ROSS12 for resolution.\n\n> \n> 48. Section 10.4.10, pg. 63, 2nd para., has the phrase \"the server\n> might\".\n> Suggest changing to \"the server MAY\". Should review other uses of\n> \"might\"\n> in this specification.\n\nAs this code is an extensibility hook for WEBDAV or similar extensions, \nit is a hook and the text is written hypothetically to show it intended \nuse, rather than a normative fashion.\n\n> \n> 49. Section 10.4.10, pg. 63, 2nd para., has the phrase \"would likely\".\n> Suggest\n> rephrasing to use MAY or SHOULD instead.\n\nSame as 48.\n\n> \n> 50. Section 10.4.11 has \"This response is cachable ...\". Suggest\n> rephrasing\n> as \"MAY be cached\". It may be useful here to point out that this is the\n> only\n> cachable 4XX response (according to Section 13.4).\n\nYes, it is cachable as you want dead links to be able to be detected without \na round trip. Previous file system experience is that this can be a very \nsignificant performance optimization.\n\nI guess I can't get very concerned about saying \"is cachable\" vs.\n\"MAY be cached\" to want to fix all the occurances of the \"is cachable\"\nphrase at this date, particularly since it is more awkward.\n\n> \n> 51. Section 11 uses the term \"OPTIONAL\" as a keyword in a non-keyword\n> context.\n> \n\nWe're just trying to make it clear that if all you want to do is build\na piece of software to get documents from a public web server, you\ndon't necessarily have to go to the trouble of doing authentication.\n- Jim\n\n\n\n", "id": "lists-012-8170667"}, {"subject": "Re: Comments (Part 2) on HTTP ID Rev 0", "content": "> From: \"Adams, Glenn\" <gadams@spyglass.com>\n> Date: Mon, 2 Nov 1998 19:42:17 -0600\n> To: \"'Masinter, Larry'\" <masinter@parc.xeroc.com>,\n>         \"'Getty, James'\"\n>          <jg@pa.dec.com>\n> Cc: \"'WG - HTTP'\" <http-wg@hplb.hpl.hp.com>\n> Subject: Comments (Part 2) on HTTP I-D Rev 05\n> -----\n> Alternative 1 --\n>   This text\n> \n> Following is my second set of comments, covering sections 12 through\n> 14.9.3. Part\n> three will follow presently.\n> \n> 52. Section 12 uses the phrase \"agent-driven negotiation\"; suggest\n> adding a note explaining that \"agent\" refers to \"user agent\".\n\nIt isn't necessarily a user agent.\n\n> \n> 53. Section 12.2, pg. 68, 1st para., has \"(this specification reserves\n> the field-name Alternates)\", however this field name is not described\n> in section 14 as a reserved field name nor otherwise elaborated\n> elsewhere in this specification.\n\nYes, this is left to the content negotiation people to define.\nIt is inappropriate to do more than reserve it, so that others\ndon't usurp the header name.  It should be \"header-name\" however.\n\n> \n> 54. Throughout the whole of section 13 it is often unclear as to\n> whether a requirement or statement is meant to apply only to a proxy\n> cache, to a user agent cache, or to both. For example, in 13.1.1, the\n> 6th paragraph has \"If the cache can not [sic] communicate with the\n> origin server ...\" which appears to apply to either a proxy or a user\n> agent cache. On the other hand, the 7th paragraph has \"If a cache\n> receives a response (...) that it would normally forward to the\n> requesting client ...\" which appears to apply to a proxy cache only.\n> Suggest editing the entirety of section 13 to clarify applicability to\n> these different caches.\n\n\"cache\" is the general term.  There can be client or proxy caches,\nand if it applies to both, the general term is used.  Saying\n\"proxy and client cache\" all the time would just get confusing,\nverbose and painful.  \n\nWe also don't want to get caught up in terminological problems that can \noccur if a proxy is on the client system.  Web caching systems also often \nget implemented as pretty separate parts of browsers, rather than all \nentangled, and so such requirements can apply to client caches (or caches \non clients) as well, to illustrate the slippery slope, potentially.\n\nIf you want to point out particular places you think there are problems,\nwe can go into things further, but I'm not going to entertain \n\"editing the entirety of section 13\" at this late date.\n\n> \n> 55. Section 13, pg. 69, 3rd para., has \"the protocol requires that\n> transparency be relaxed ... only ... only ...\"; suggest changing to use\n> the keyword phrase \"MUST NOT relax transparency unless\" to make clear\n> the requirement.\n> \n> 56. Section 13.1.1, pg. 70, 1st para., has \"A correct cache MUST\n> respond with the most up-to-date response ...\"; since caching is always\n> optional, this would read better as \"A cache MUST NOT respond with a\n> response held by the cache that is appropriate to the request (...)\n> unless it meets one of the following conditions:\".\n> \n> 57. Section 13.1.1, pg. 70, numbered item (3) implies that 4XX and 5XX\n> responses are cachable. While this is true for 410, under what\n> circumstances should any 5XX response be cached?\n> \n> 58. Section 13.1.2, 4th para., has \"whether the Warning MUST or MUST\n> NOT be deleted ...\" which does not state a requirement per se: use\n> \"is or is not to be deleted ...\".\n> \n> 59. Section 13.1.2, 5th para., has \"Warnings in responses that are\n> passed to HTTP/1.0 caches carry an extra warning-date field, which\n> prevents a future HTTP/1.1 recipient from believing an erroneously\n> cached Warning.\"  I can't interpret this statement based on information\n> in this section.  Please explain it further and state as a requirement\n> if indeed it is a requirement.\n> \n> 60. Section 13.1.2, 7th para., has \"a server might provide the same\n> warning with texts in both English and Basque\". How would a UA\n> discrimitate among different warnings using different languages unless\n> the language were explicitly marked? Unfortunately, RFC2047 does not\n> address this issue. I'd suggest permitting the extensions specified by\n> RFC2231 (which updates RFC2047) to be used to provide explicit language\n> tagging of quoted strings.\n> \n> 61. Section 13.1.3, 2nd para., has \"if there is any apparent conflict\n> between header values, the most restrictive interpretation is applied\n> ...\".  Change \"is applied\" to \"MUST\" or \"SHOULD\" \"be applied\".\n> \n> 62. Section 13.1.4, 1st para., change \"the user agent might allow ...\"\n> to \"the user agent MAY allow ...\".\n> \n> 63. Section 13.1.4, 2nd para., has \"the user agent SHOULD explicitly\n> indicate to the user ...\" while section 13.1.5, 1st para., has \"This\n> allows the client software to alert the user\". This later statement\n> appears to make the indication/alert optional rather than recommended\n> as implied by the former.\n> \n> 64. Section 13.2.3, 3rd para., has \"HTTP/1.1 requires origin servers to\n> send a Date header, if possible, with every response ...\" seems to be\n> stating a conditional imperative. Rather than paraphrasing section\n> 14.18 and possibly confusing the requirements regarding Date header\n> transmission, I'd suggest rephrasing this to simply refer to a Date\n> header, if present, and to state what must be done in the case that a\n> Date header is not present.\n> \n> 65. Section 13.2.3, pg. 76, 1st para. after pseudo code block, has \"the\n> server MUST\"; suggest changing to \"the proxy server MUST\".\n> \n> 66. Section 13.3, pg. 78, 1st para., suggest changing \"it first has to\n> check\" with \"it will normally check\" to make this read more as a\n> statement of fact than a requirement.\n> \n> 67. Section 13.3.3, pg. 81, next to last paragraph, has \"A cache or\n> origin server ...\". Change to \"A caching proxy or origin server ...\".\n> \n> 68. Section 13.3.4, pg. 83, 2nd para., starts \"A note on rationale:\n> ...\"  Suggest changing this to a standard note form, i.e., use \"Note:\"\n> prefix with indented block paragraph.\n> \n> 69. Section 13.4, pg. 83, 2nd para., starts \"Note that ...\". Suggest\n> changing this to a standard note form, i.e., use \"Note:\" prefix with\n> indented block paragraph. Further, this note has \"some HTTP/1.0 caches\n> are known to violate this expectation without providing any Warning.\"\n> What warning should be provided in this case?\n> \n> 70. Section 13.4, pg. 83, 3rd para., has \"so that the server can\n> indicate that certain resource entities, or portions thereof, MUST NOT\n> be cached ...\" which does not appear to state a specific requirement\n> per se. Suggest changing to \"... are not to be cached ...\".\n> \n> 71. Section 13.4, pg. 84, 1st para., starts \"Note that ...\". Suggest\n> changing this to a standard note form.\n> \n> 72. Section 13.4, pg. 84, 3rd para.: suggest giving status codes 302\n> and 307 as examples of responses which are not cachable by default but\n> which may be explicitly marked as cachable by using Expires or the\n> \"public\" cache-control directive.\n> \n> 73. Section 13.5, 1st para., remove comma in \"to requests, for use ...\".\n> \n> 74. Section 13.5.1, pg. 84, 1st para, 1st bullet, has \"End-to-end\n> headers which MUST be ...\". The use of MUST and SHOULD keywords in\n> relative clauses is problematic and should be avoided since it does not\n> state a requirement per se. Most such instances can be replaced by some\n> form of \"is\" to express simple fact; e.g., \"End-to-end headers which\n> are ...\".\n> \n> 75. Section 13.5.1, pg. 84, 4th para., has \"Hop-by-hop headers\n> introduced in future versions of HTTP MUST be listed in a Connection\n> header ...\"  stipulates a requirement on the authors and/or\n> implementors of future versions of HTTP, and not on implementors of\n> HTTP/1.1. Suggest rephrasing this appropriately.\n> \n> 76. Section 13.5.2, pg. 85, 4th para., has \" ... of the following\n> fields in message that ...\" which needs an article \"a\" before \"message\".\n> \n> 77. Section 13.5.2, pg. 85, 5th para., has \"if not already present, it\n> MUST add a Warning 214 (...) if one does not already appear ...\" which\n> uses redundant language about \"already present\"/\"already appear[ing]\".\n> \n> 78. Section 13.5.3, pg. 86, 5th para., has \"all such old headers are\n> replaced.\" which sounds like a requirement: \"... MUST be replaced.\"\n> \n> 79. Section 13.6, pg. 87, 3rd para., has \"When the cache receives a\n> subsequent request whose Request-URI specifies one or more cache\n> entries including a Vary header field, ...\". Suggest changing \"one or\n> more cache entries including a Vary header field\" to \"one or more cache\n> entries of previous responses which contained a Vary header field\".\n> Further, it appears that this paragraph implies a caching proxy\n> context, but it is not clear that this would not also apply to a user\n> agent cache. The next paragraph (end of pg. 87 and beginning of pg. 88)\n> appears to be framed as applying only to a proxy. Again it isn't clear\n> that this does not apply to a UA cache.\n> \n> 80. Section 13.8, pg. 88, 1st para., implies the context of a caching\n> proxy, requiring a 206 response when forwarding a partial response. In\n> the case of a user agent cache that receives and wishes to use a\n> partial response, it would seem that a Warning should be added by the\n> UA cache to the response it generatese for internal UA consumption.\n> However, there appears to be no Warning code that would serve this\n> purpose.\n> \n> 81. Section 13.11, 1st para., has \"All methods that might be expected\n> to cause modifications to the origin server's resources MUST be written\n> through to the origin server. This currently includes all methods\n> except for GET and HEAD.\" It would be better here to specify the\n> methods that must be written through explicitly: PUT and DELETE. It\n> isn't clear that OPTIONS, POST, or TRACE fall in this category; and\n> then there's CONNECT, which doesn't fit into either of the above\n> groups of methods.\n> \n> 82. Section 14, pg. 91: suggest adding a sentence to each header\n> defined by this section that states whether the header is end-to-end or\n> hop-by-hop and whether the header is cachable by default, cachable by\n> explicit cache directive, or never cachable.\n> \n> 83. Section 14.1, pg. 92, 6th para., has \"the most specific reference\n> has precedence\"; suggest using \"SHOULD\" or \"MUST\" \"take precedence\".\n> \n> 84. Section 14.2, pg. 93, 3rd para., is quite confusing: suggest\n> rewriting without using the term \"mentioned\". Also, this para. seems to\n> be stating that if any \"iso-8859-1;q=1\" is always implied unless\n> otherwise explicitly present. This means that:\n> \n>     Accept-Charset: iso-8859-5, unicode-1-1;q=0.9\n> \n> really means\n> \n>     Accept-Charset: iso-8859-1;q=1, iso-8859-5;q=1, unicode-1-1;q=0.9\n> \n> (in which case 8859-1 would be given equal billing with 8859-5). And\n> that consequently the only way to exclude 8859-1 is to specify\n> \n>     Accept-Charset: iso-8859-1;q=0, iso-8859-5, unicode-1-1;q=0.9\n> \n> Is this the intended usage? If so, I find this not only convoluted but\n> seriously sub-optimal. This emphasis on 8859-1 as default really is too\n> much. Why go so far overboard?\n> \n> 84. Section 14.2, pg. 93, 4th para., has \"the server SHOULD send an\n> error response with the 406 (not acceptable) status code, though the\n> sending of an unacceptable response is also allowed.\" The effect of the\n> final clause of this statement is to downgrade SHOULD to MAY.  Either\n> remove the final clause or change to MAY. [My preference is to remove\n> the final clause.]  Note that the semantics stated here are expressly\n> different from Accept and Accept-Encoding which do require 406\n> responses for unconditionally compliant implementations.  This\n> inconsistency will make it difficult or impossible to implement\n> agent-driven content negotiation based on Accept-Charset variants.\n> \n> 85. Section 14.3, pg. 94, 1st para., has \"A server tests whether ...\";\n> suggest changing to \"A server MUST test ...\".\n> \n> 86. Section 14.3, pg. 94, last para., has \"This means that qvalues will\n> not work and are not permitted with x-gzip or x-compress.\" This appears\n> to be stating a compatibility requirement, in which case MUST or SHOULD\n> is better (in which case this note would have to be made normative).\n> \n> 87. Section 14.4, pg. 95, next to last para., has \"recommended\" and\n> \"MUST NOT\" in a note. Either make this normative (not a note) and use\n> SHOULD and MUST NOT consistently or use \"ought\" and \"ought not\",\n> respectively.\n> \n> 88. Section 14.4 does not contain language as found in other Accept-*\n> headers that recommends a 406 response in the case the server cannot\n> satisfy the request based on its variant set for the specified URI.\n> This precludes implementing client-side content negotiation along this\n> variance axis. Suggest adding the required language or a note\n> indicating why it is not present and what this means for client-side\n> negotiation.\n> \n> 89. Section 14.5. I find \"Accept-Ranges\" to be inconsistent in its name\n> and usage with other Accept-* headers. This really should have been\n> handled with Expect. Unless you can change this to use Expect (and I\n> doubt you can at this stage), I'd suggest adding a note to indicate\n> this inconsistency. I'd also urge adding a mechanism which does use\n> Expect and deprecates the use of Accept-Ranges in a future version of\n> HTTP. [If \"Allow-Ranges\" had been used instead, at least this would be\n> consistent with the \"Allow\" header usage.]\n> \n> Regarding the actual use of Accept-Ranges, which response would be\n> appropriate for a server which sends \"Accept-Ranges: none\" in response\n> to a Range request?  406? Some mention of the appropriate response\n> should be made here.\n> \n> 90. Section 14.8, pg. 97-98, seems to imply a caching proxy when\n> referring to \"shared cache\"; however, this seems to apply as well to a\n> shared cache on a user agent. Suggest making it clear which kinds of\n> caches are addressed by these paragraphs.\n> \n> 91. Throughout the whole of section 14.9, it is often unclear as to\n> whether a requirement or statement is meant to apply only to a proxy\n> cache, to a user agent cache, or to both.\n> \n> 92. Section 14.9, pg. 98, 1st para., has \"that MUST be obeyed by all\n> caching mechanisms\" which does not specify a requirement per se (note\n> use of MUST in relative clause).\n> \n> 93. Section 14.9, pg. 99, 1st para., has \"When a directive appears\n> without any 1#field-name parameter, the directive applies to the entire\n> request or response.\" At the present, no cache-request-directive\n> employs a 1#field-name parameter (see pg. 98); consequently all request\n> directives apply to the entire request in all cases.\n> \n> 94. Section 14.9.1, pg. 99, 2nd para., has \"Indicates that the response\n> is cachable by any cache, ...\". Suggest changing \"is cachable\" to \"MAY\n> be cached\".\n> \n> 95. Section 14.9.1, pg. 99, last para., has keyword MUST NOT in\n> resultative clause \"Indicates that ...\"; suggest rephrasing as\n> imperative, e.g., \"A response containing the cache directive 'private'\n> MUST NOT be cached by a shared cache.\".\n> \n> 96. Section 14.9.1, pg. 100, 2nd para. under \"no-cache\", has \"the\n> specified field-name(s) MUST NOT be sent in the response to a\n> subsequent request without successful revalidation with the origin\n> server.\" followed by \"This allows an origin server to prevent the\n> re-use of certain header fields in a response, while still allowing\n> caching of the rest of the response.\" I find this rather confusing. My\n> reading of this is that a cache can, indeed, retain and reuse a field\n> specified in a no-cache directive as long as it revalidates the entry\n> with the origin server.  Furthermore, it appears that \"no-cache\" with\n> no field name is to be interpreted identically to must-revalidate. I\n> have always interpreted no-cache without a field-name to mean don't\n> cache the response under any circumstances. Which is it?\n> \n> 97. Section 14.9.2, pg. 100, 1st para., needs to do a better job of\n> defining \"store\" without using keywords in the term or definition.\n> Also, it would be better to place this definition at the beginning of\n> this section. I would suggest a rewrite as follows:\n> \n> \"The purpose of the no-store directive is to prevent the inadvertent\n> release or retention of sensitive information. In the present context,\n> 'store' means to intentionally retain data in non-volatile storage.\n> The no-store directive applies to the entire message, and MAY be sent\n> either in a response or in a request. If sent in a request, a cache\n> MUST NOT store any part of the request or its response. If sent in a\n> response, a cache MUST NOT store any part of either the response or the\n> request that elicitied it. This directive applies to both shared and\n> non-shared caches.\"\n> \n> In this rewrite, I've removed the statement about removing data from\n> volatile storage, since this doesn't seem to apply to the semantics of\n> this directive.  In particular, many caches employ a volatile and a\n> non-volatile component.  The described semantics appears to strictly\n> address use of the non-volatile component, and not the volatile\n> component. If this is not the case, then these semantics would appear\n> to broaden the stated intention found in the last paragraph on pg.\n> 100. Furthermore, this directive applies equally to a user agent cache\n> as well as a caching proxy, so language aimed at proxies (e.g., \"after\n> forwarding it\") appears to be overly narrow in scope.\n> \n> 98. Section 14.9.3, pg. 101, 2nd para., has \"the max-age directive\n> overrides the Expires header\".  What is the imperative status of this\n> statement? Should this read \"MUST override\"?\n> \n> 99. Section 14.9.3, pg. 101, 5th para. (s-maxage), has \"i.e., that the\n> shared cache MUST NOT ...\" which doesn't state a requirement per se.\n> Rewrite to avoid using MUST NOT in an explanatory relative clause\n> (particularly one using \"i.e.\").\n> \n> 100. Section 14.9.3, pg. 101, 5th para. (s-maxage), has \"The s-maxage\n> directive is always ignored by a private cache\". Should this read \"A\n> private cache MUST ignore the s-maxage directive.\"?\n> \n> 101. Section 14.9.3, pg. 102, 7th para., has \"only if this does not\n> conflict with any MUST-level requirements\"; suggest rephrasing as \"with\n> any conditionally compliant requirements\" to avoid using MUST in this\n> context which is not stating a requirement per se.\n> \n> \n> Alternative 2 --\n>   Enclosed HTML page: NoName.htm (30 KBytes)\n\n\n\n", "id": "lists-012-8203611"}, {"subject": "RE: Comments (Part 2) on HTTP ID Rev 0", "content": "-----Original Message-----\nFrom:Ross Patterson\n[mailto:ROSSP@ss1.reston.vmd.sterling.com]\nSent:Monday, November 09, 1998 12:07 PM\nTo:http-wg@hplb.hpl.hp.com\nCc:gadams@rafiki.spyglass.com\nSubject:Re: Comments (Part 2) on HTTP I-D Rev 05\n\n\n\"Adams, Glenn\" <gadams@spyglass.com> writes:\n\n\n>74. Section 13.5.1, pg. 84, 1st para, 1st bullet, has\n\"End-to-end\n>headers which MUST be ...\". The use of MUST and SHOULD\nkeywords in\n>relative clauses is problematic and should be avoided\nsince it does not\n>state a requirement per se.\n\nI don't understand your meaning of \"relative clauses\",\nhowever this case\nin particular does state a requirement, that\nintermediaries forward\nend-to-end headers to the ultimate recipient.  This\nrequirement isn't\nstated anywhere else in the document, and must be\nretained.\n\nI use \"relative clause\" in the sense of English syntax; i.e., a\nsubordinate clause\nintroduced by a relative pronoun. Since such clauses are used to comment\non or restrict the scope of interpretation of the preceding noun or noun\nphrase,\nit is not appropriate to embed an imperative in such a clause. It should\nbe\nrestated outside of a relative clause; e.g., \"End-to-end headers MUST be\n...\".\n\n\n\n", "id": "lists-012-8232407"}, {"subject": "RE: Comments (Part 1) on HTTP ID Rev 05 (ADAMS1", "content": "-----Original Message-----\nFrom:jg@pa.dec.com [mailto:jg@pa.dec.com]\nSent:Friday, November 13, 1998 2:38 PM\nTo:Adams, Glenn\nCc:http-wg@cuckoo.hpl.hp.com\nSubject:Re: Comments (Part 1) on HTTP I-D Rev 05\n(ADAMS1)\n\n\n> From:Adams, Glenn\n> Sent:Monday, October 26, 1998 11:13 AM\n> To:'http-wg@cuckoo.hpl.hp.com'\n> Subject:Comments (Part 1) on HTTP I-D Rev 05\n> \n\n> 4. Section 2.2, pg. 16, definition of \"CTL\", fails to\nconsider that\n> ASCII (and ISO646-1993) consider SPACE (040) to be a\ncontrol character\n> of the same status as DEL (177).\n\nSorry, no, we handle space differently than CTL, and the\nBNF reflects this.\n\nWhat I'm saying here is that you define CTL as:\n\nCTL = <any US-ASCII control character (octets 0-31) and DEL (127)>\n\nwhile, in fact, \"any US-ASCII control character\" includes octet 32 by\ndefinition\nas well as 0-31 and 127. If you want to exclude SP from this syntactic\ncategory,\nI'd suggest adding \"exluding SP (32)\" to this definition.\n\n> \n> 6. Section 3.4, pg. 21, specifies that \"the definition\nassociated with\n> a MIME character set name MUST fully specify the\nmapping ...\". Should\n> this not be a requirement placed on the registrant of\na MIME character\n> set and not an HTTP implementation? Or, is this\nrequirement really\n> stating that any HTTP implementation must maintain a\ntable of registered\n> character sets known to satisfy this requirement and\nMUST NOT use any\n> character set not present in this table? Overall, this\nseems an onerous\n> requirement for an HTTP implementation.\n> \n\nI'm not the MIME expert of the working group, but I take\nthis to mean \nthat this is just a restriction on which character sets\nmay be used, and \nimplies there are character sets that do not meet this\nrequirement by \nhaving external profiling information.  Maybe a MIME\nexpert can confirm \nthis one.\n\nI'm concerned about whether this use of MUST is stipulating a\nrequirement\nfor an HTTP implementation or the entity using HTTP. If it is a\nrequirement on\nthe implementation, then the implementation would have to maintain\ninternal\nknowledge about which \"character sets\" satisfy this requirement.\n\n> \n> 10. Section 3.7.1, pg. 26, 1st para., states \"An\nentity-body transferred\n> via HTTP messages MUST be represented in the\nappropriate canonical form\n> prior to its transmission except for \"text\" types\n...\". Is it actually\n> the\n> case that servers are validating canonical status of\nentity bodies? This\n> contradicts the \"entity-body as payload\" philosophy.\n\nNo, entities are always payload.  The requirement is\nthat you have to\nplay by MIME rules for that data type, but we\nacknowledge the UNIX usage\nof newline line terminators means that text document\nline terminators\ndon't play by MIME rules.  The Web has worked this way\n(just ship the\nbits) from day one, and any arguments that it should\nplay by MIME rules\nfor text payload at this date are doomed to failure.\n\nThe problem I have here with this is the use of \"MUST\". Which entity is\nresponsible for\nenforcing \"MUST\"? If it is true that entities are always payload, then\nit would clearly not\nbe a requirement on the server or client. In this case, the requirement\nis on the entity which\nis employing an HTTP implementation and not on the implementation\nitself, in which case\nthis should not be specified as a MUST requirement in this context.\n\n> 12. Section 3.7.2, pg. 27, 2nd para., states \"In all\nother cases, an\n> HTTP\n> user agent SHOULD follow the same or similar behavior\nas a MIME user\n> agent\n> would ...\". This \"implied\" behavior needs to be made\nexplicit. What is\n> the behavior of a MIME user agent in this context?\n\nI think you should go read the MIME specs to find out;\nHTTP incorporating\nrecommendations for what MIME should do here is a great\nway for specs\nto end up contradictory.\n\nI'm not suggesting incorporating parts of the MIME specs into this spec;\nrather, that\nthis be more explicit about which parts of the MIME specs are being\nreferenced.\nMIME specifies many behaviors.\n\n> \n> 14. Section 3.8, pg. 28, 1st para., states \"Product\ntokens SHOULD be\n> short\n> and to the point.\" and \"They MUST NOT be used for\nadvertising or other\n> non-essential information.\" As an implementer, how can\none interpret\n> these\n> requirements? Either make quantify them or remove\nthem.\n\nI think common sense is in order here. Keep'em short.\nYour customers will\nthank you (lower latency, fewer bytes).\n\nWe've seen people put the kitchen sink in them.\n\nUnless others complain, I plan to keep these as is.\n\nThe problem I have here is that common sense is a relative. Without\nspecifying\nany limits (e.g, no greater than 1024 characters in length), anyone can\ninterpret this\nas they see fit.\n\n\n\n\n\n\n", "id": "lists-012-8242571"}, {"subject": "Comments (Part 3) on HTTP ID Rev 0", "content": "Following is my third set of comments, covering sections 14.9.4 through\n14.40. Part\nfour will follow presently.\n\n102. Section 14.9.4, pg. 103, 3rd para., has an editors note \"[jg418]\"\nwhich should be removed.\n\n103. Section 14.9.4, pg. 103, 4th para., would read more clearly if\nthe last sentence were further elaborated, e.g., \"The initial request\n(from user agent to first proxy) includes cache-validating\nconditional(s) in the request, based on the validator(s) of the local\ncached copy.\"\n\n104. Section 14.9.4, pg. 103, 5th para., would read more clearly if\nthe last sentence were further elaborated, e.g., \"The initial request\n(from user agent to first proxy) does not include a cache-validating\nconditional; rather, the first caching proxy in the request path that\nholds a cache entry for this resource includes cache-validating\nconditional(s) in the request, based on the validator(s) of its cache\nentry.\"\n\n105. Section 14.9.4, pg. 104, 2nd para., suggest changing \"I.e., the\ncache MUST obey ...\" to \"The cache MUST obey ...\". Imperatives\nshouldn't be stated as parenthetical explanations.\n\n106. Section 14.10, pg. 106, 1st para., suggest changing \"MUST NOT be\ncommunicated by proxies over futher connections.\" to \"MUST NOT be\nforwarded by proxies.\"\n\n107. Section 14.10, pg. 106, 2nd para., the production used with\nConnection appears to be overly general as compared to the explanation\nin this section.  I would suggest rewriting as follows:\n\nConnection = \"Connection\" \":\" 1#connection-directive\n\nconnection-directive = \"close\" | field-name\n\nWhile it is the case that both \"close\" and field-name can be expressed\nas token, the semantics implied by field-name corresponds better to\nthe required usage, and, further, implies the additional semantics of\nsection 14 regarding field names defined by this specification.\n\nThe above expression also does a better job of discriminating the\nspecial directive \"close\" from the other possible values for this\nfield (i.e., field-names).\n\nIf I were designing this anew, I would have modeled this similarly to\nthe expression used with the no-cache cache-response-directive:\n\nconnection-directive = \"close \" | \"no-forward\" \"=\" <\"> 1#field-name <\">\n\nAny chance for revising this into a more consistent form?\n\n108. Section 14.11, pg. 107, 1st para., has \"MUST be\" in relative\nclause \"... and thus what decoding mechanisms MUST be ...\".\n\n109. Section 14.14, pg. 109, 2nd para., refers to \"base URI\". Is this\nconcept defined in this specification? Perhaps a reference to another\ndocument defining this would be useful.\n\n110. Section 14.14, pg. 109, 6th para., has \"The meaning of the\nContent-Location header in PUT or POST requests is undefined; servers\nare free to ignore it in those cases.\" Suggest changing \"servers are\nfree to ignore\" to \"servers MAY ignore\". Also, how about other methods\nsuch as DELETE, etc.\n\n111. Section 14.15, pg. 110, 1st para., suggest changing \"may\ngenerate\" to \"MAY generate\".\n\n112. Section 14.15, pg. 110, 1st para., has \"Any recipient ... MAY\ncheck that the digest value ... matches ...\". Is there a\nrecommendation for the case where it does not match? Or is this\nimplementation specific behavior?\n\n113. Section 14.15, pg. 110, it seems that the material discussing an\nextension to RFC 1864 (4th through 7th paragraphs) would best be moved\nto an appendix. I also found this material difficult to follow as\npresently worded.\n\n114. Section 14.16, pg. 111, 1st para., suggest changing \"It SHOULD\nindicate the total length ...\" to \"It SHOULD indicate by means of an\n'instance-length' the total length ...\". It may also read better if\nthis is placed after the syntactical description of Content-Range;\ni.e., before the paragraph starting \"The asterisk ...\".\n\n115. Section 14.16, pg. 111, 3rd para., suggest adding \"(see section\n14.35.1)\" after \"Unlike byte-range-specifier values ...\".\n\n116. Section 14.16, pg. 111, 5th para., has 'A response with status\ncode 206 (Partial Content) MUST NOT include a Content-Range field with\na content-range-spec of \"*\"'. It isn't clear how to interpret this\nsince it is not possible for the value of content-range-spec to be\nmerely \"*\".  Perhaps this means 'with a content-range-spec which uses\n\"*\" as the instance-length'? If this is the case, then perhaps it\nwould be better to change the definition of instance-length to:\n\ninstance-length = 1*DIGIT | \"*\"\n\nAnd then say '... MUST NOT include a Content-Range field with an\ninstance-length of \"*\"'.\n\n117. Section 14.16, pg. 112, 2nd para., needs a space in \"19.6.3for\".\n\n118. Section 14.18, pg. 113, 5th para., has \"Clients SHOULD only send\na Date header field in messages that include an entity body ...\" which\nwould read better as a prohibition: \"Client SHOULD NOT send a Date\nheader field in messages that do not include an entity body ...\".\n\n119. Section 14.20, pg. 114, would seem better to merge the last\nsentence of the 1st paragraph (\"A server that does not ...\") with the\n2nd para.  (\"The server MUST respond ...\"), since they appear to\nduplicate the requirement about responding with an error status.\n\n120. Section 14.20.1, pg. 115, appears to duplicate language from\n8.2.4.\n\n121. Section 14.21, pg. 116, 4th para.: suggest adding \"(see section\n14.9.3)\" after \"Note: if a response ...\".\n\n122. Section 14.22, pg. 116, 1st para.: suggest removing parentheses\naround \"as updated by RFC1123 [8]\" to make it clear that this update\nisn't optional.\n\n123. Section 14.22, pg. 117, 2nd para., suggest changing \"issuer's\naddress\" to \"requester's address\".\n\n124. Section 14.22, pg. 117, 3rd para., has \"SHOULD NOT\" in a note.\n\n125. Section 14.24, pg. 118, 6th para., suggest rewriting to not use\n\"SHOULD\" and \"MUST NOT\" in the context of defining a term \"If-Match:\"\n-- or use \"SHOULD\" and \"MUST NOT\" without placing in a definition.\n\n126. Section 14.24, pg. 118, last para., uses \"MUST NOT\" in a relative\nclause \"to signal that ...\"; suggest rewriting to make into an\nimperative related explicitly to server.\n\n127. Section 14.24, pg. 119, 1st para.: I find the fact that this\nsituation is defined as undefined to be rather troubling. Can't a\nspecific 4XX response (e.g., 400) be recommended instead? It has been\nstated that broken behavior should be strongly discouraged, rather\nthan merely ignored or arbitrarily interpreted.\n\n128. Section 14.25, pg. 119, 1st para., should \"an entity will not be\nreturned from the server\" be strengthened to \"A server MUST NOT return\nan entity ...\"?\n\n129. Section 14.25, pg. 119, 5th and 6th paras., starting \"Note that\nthe Range ...\" and \"Note that the If-Modified-Since ...\",\nrespectively, should be either indented at the same level as other\nparagraphs (e.g., see 1st para.  of pg. 120) or use standard \"Note:\n...\" form.\n\n130. Section 14.25, pg. 120, 1st para.: is this normative text, in\nwhich case \"The client should ...\" should be changed to \"The client\nSHOULD ...\", or informative text, in which case the standard \"Note:\n...\" form should be used?\n\n131. Section 14.26, pg. 121, 3rd para., suggest rewriting to not use\n\"MUST NOT\" in the context of defining a term \"If-None-Match:\" -- or\nuse \"MUST NOT\" without placing in a definition.\n\n132. Section 14.27, pg. 121, 1st para., suggest changing \"it could use\nthe Range ...\" to \"it MAY use the range ...\".\n\n133. Section 14.27, pg. 122, 1st para., uses the term \"sub-range\noperation\".  This term doesn't seem to be well defined elsewhere in\nthis specification.\n\n134. Section 14.28, pg. 122, 1st para., why have \"... the server\nSHOULD perform the requested operation as if the If-Unmodified-Since\nheader were not present.\" while section 14.24, 2nd para., has \"... the\nserver MAY perform the requested method if the If-Match header field\ndid not exist.\"? Suggest using consistent language in these sections\n\"SHOULD/MAY\", \"operation/method\", \"were not present/did not exist\".\n\n135. Section 14.28, pg. 122, 2nd para., should note the asymmetry in\nresponse codes with respect to \"If-Modified-Since\" (section 14.25).\nHere we have 412 (Precondition Failed) while in 14.25 we have 304 (Not\nModified). This asymmetry is odd since If-Match and If-Not-Match both\nuse 412.\n\n136. Section 14.29, pg. 123, 4th para., suggest removing \"whenever\nfeasible\" since this weakens this as an unconditionally compliant\nrequirement.\n\n137. Section 14.35.2, pg. 127, 2nd para. has \"origin servers and\nintermediate caches ought to support byte ranges when possible\" seems\nto be a quasi-requirement but doesn't use standard keywords. Should\nthis be \"SHOULD support byte ranges\" (recommended) or \"MAY support\nbyte ranges\" (optional)?\n\n138. Section 14.35.2, pg. 127, 3rd para., 2nd bullet, has \"It does not\naffect the 304 (Not Modified) response returned if the conditional is\nfalse.\" However, a 412 (Precondition Failed) response may apply as\nwell.\n\n139. Section 14.35.2, pg. 127, 5th para., suggest removing \",\" in \"...\nin its cache, if that is ...\".\n\n140. Section 14.36, pg. 127, rather than just using \"[sic]\" to excuse\nthe misspelling \"Referer\", suggest adding a note indicating that this\nis a historical error maintained for compatibility sake.\n\n141. Section 14.36, pg. 128, 3rd para., what does \"partial URI\" mean\nin \"If the field value is a partial URI, it SHOULD be ...\" Is this\nusage defined somewhere?\n\n142. Section 14.37, pg. 128, has \"This field MAY also be used with any\n3xx (Redirection) response to indicate the minimum time the user-agent\nis asked to wait before issuing the redirected request.\" Should there\nbe a concurrent requirement placed on the user-agent (or client),\ne.g., \"The user-agent SHOULD NOT issue a redirected request before\n...\"?\n\n143. Section 14.39, pg. 129, 1st para., has \"in section 3.9\" which\nshould read \"in section 3.6\".\n\n144. Section 14.39, isn't t-codings syntactically ambiguous since:\n\ntransfer-extension     = token *( \";\" parameter )\nparameter              = attribute \"=\" value\n\nand\n\naccept-params          = \";\" \"q\" \"=\" qvalue *( accept-extension )\n\nPerhaps it should read:\n\nt-codings              = \"trailers\" | transfer-extension\n\nand then add language indicating that accept parameters are included\nby means of the general transfer-extension parameter.\n\n145. Section 14.39, pg. 129, 4th para., has \"Therefore the keyword\nMUST be ...\": what does \"keyword\" mean in this context? Shouldn't this\nread \"connection-token\" (or, as I suggested in my point 107,\n\"field-name\").\n\n146. Section 14.39, pg. 129, 5th para., item 1, suggest changing to\nstart \"The presence of a TE header indicates that the \"chunked\"\ntransfer- coding is acceptable.\"\n\n147. Section 14.39, pg. 130, item 3, if an implied \"chunked\" always\ngets qvalue of 1, then it will always win over lesser qvalues.\nSpecifying \"chunked;q=0\" would permit overriding this default;\nhowever, the present syntax does not admit \"chunked\" (unless the\nsyntax of t-codings is changed to:\n\nt-codings = \"trailers\" | transfer-coding\n\n148. Section 14.40, pg. 130, 2nd para, has \"An HTTP/1.1 sender SHOULD\n...\": suggest changing to \"An HTTP/1.1 message SHOULD include a\nTrailer header field if a chunked transfer-coding is used with a\nnon-empty trailer.\"\n\n149. Section 14.40, pg. 130, 4th para., has \"A server MUST NOT include\nany header fields unless ...\". I believe this should read \"A server\nMUST NOT include any header fields in the trailer of a message unless\n...\".\n\n150. Section 14.40 uses language that implies \"Trailer\" is only used\nin response messages; however, Trailer is specified in section 4.5 as\na general header field. Under what circumstance(s) would Trailer be\nused in a request message? Should the language of 14.40 take this into\naccount?\n\n\n\n", "id": "lists-012-8256953"}, {"subject": "Re: Comments (Part 2) on HTTP I-D Rev 05 (Adams #84, AcceptCharset", "content": "Glenn Adams writes:\n\n> \n> 84. Section 14.2, pg. 93, 4th para., has \"the server SHOULD send an\n> error response with the 406 (not acceptable) status code, though the\n> sending of an unacceptable response is also allowed.\" The effect of the\n> final clause of this statement is to downgrade SHOULD to MAY.  Either\n> remove the final clause or change to MAY. [My preference is to remove\n> the final clause.]  Note that the semantics stated here are expressly\n> different from Accept and Accept-Encoding which do require 406\n> responses for unconditionally compliant implementations.  This\n> inconsistency will make it difficult or impossible to implement\n> agent-driven content negotiation based on Accept-Charset variants.\n\nWhat is the correct fix here?  Opinions solicited.\n- Jim\n\n\n\n", "id": "lists-012-8277859"}, {"subject": "Re: Comments (Part 2) on HTTP I-D Rev 05 (Adams #84, AcceptCharset", "content": "Jim Gettys wrote:\n> \n> Glenn Adams writes:\n> \n> >\n> > 84. Section 14.2, pg. 93, 4th para., has \"the server SHOULD send an\n> > error response with the 406 (not acceptable) status code, though the\n> > sending of an unacceptable response is also allowed.\" The effect of the\n> > final clause of this statement is to downgrade SHOULD to MAY.  Either\n> > remove the final clause or change to MAY. [My preference is to remove\n> > the final clause.]  Note that the semantics stated here are expressly\n> > different from Accept and Accept-Encoding which do require 406\n> > responses for unconditionally compliant implementations.  This\n> > inconsistency will make it difficult or impossible to implement\n> > agent-driven content negotiation based on Accept-Charset variants.\n> \n> What is the correct fix here?  Opinions solicited.\n\nIMO, no fix is required.  In my mind, SHOULD and MAY are very close. \nFurthermore, the wording looks to me like that for Accept and\nAccept-Encoding w.r.t. 406 as far as unconditional compliance is\nconcerned.  And for conditional compliance, servers have the option of\nsending an \"unacceptable response\" in place of an error response.  (We\njust went over this territory with my question about Accept-Encoding\n(<http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q4/0057.html>).\n\nDave Kristol\n\n\n\n", "id": "lists-012-8286251"}, {"subject": "Re: Comments (Part 2) on HTTP I-D Rev 05 (Adams #84, AcceptCharset", "content": "Glen Adams notes:\n\n> \n> 84. Section 14.2, pg. 93, 3rd para., is quite confusing: suggest\n> rewriting without using the term \"mentioned\". Also, this para. seems to\n> be stating that if any \"iso-8859-1;q=1\" is always implied unless\n> otherwise explicitly present. This means that:\n> \n>     Accept-Charset: iso-8859-5, unicode-1-1;q=0.9\n> \n> really means\n> \n>     Accept-Charset: iso-8859-1;q=1, iso-8859-5;q=1, unicode-1-1;q=0.9\n> \n> (in which case 8859-1 would be given equal billing with 8859-5). And\n> that consequently the only way to exclude 8859-1 is to specify\n> \n>     Accept-Charset: iso-8859-1;q=0, iso-8859-5, unicode-1-1;q=0.9\n> \n> Is this the intended usage? If so, I find this not only convoluted but\n> seriously sub-optimal. This emphasis on 8859-1 as default really is too\n> much. Why go so far overboard?\n\nNot being a charset wizard, I don't have a good feeling for whether\nany change is necessary.\n\nComments?\n- Jim\n\n\n\n", "id": "lists-012-8295663"}, {"subject": "Re: Comments (Part 2) on HTTP I-D Rev 05 (Adams #88, AcceptLanguage", "content": "Similar to Adams #84:\n\nGlenn Adams notes:\n> \n> 88. Section 14.4 does not contain language as found in other Accept-*\n> headers that recommends a 406 response in the case the server cannot\n> satisfy the request based on its variant set for the specified URI.\n> This precludes implementing client-side content negotiation along this\n> variance axis. Suggest adding the required language or a note\n> indicating why it is not present and what this means for client-side\n> negotiation.\n\nI believe this is an editorial bug from when 406 was introduced (or\nlater, when some edit was applied).\n\nAgain, guidance from content negotiation wizards soliticited.\n- Jim\n\n\n\n", "id": "lists-012-8304298"}, {"subject": "Re: Comments (Part 2) on HTTP ID Rev 0", "content": "Oops, I hit \"send\" long before I was done by mistake.\n\n> From: \"Adams, Glenn\" <gadams@spyglass.com>\n> Date: Mon, 2 Nov 1998 19:42:17 -0600\n> To: \"'Masinter, Larry'\" <masinter@parc.xeroc.com>,\n>         \"'Getty, James'\"\n>          <jg@pa.dec.com>\n> Cc: \"'WG - HTTP'\" <http-wg@hplb.hpl.hp.com>\n> Subject: Comments (Part 2) on HTTP I-D Rev 05\n> -----\n> Alternative 1 --\n>   This text\n> \n> Following is my second set of comments, covering sections 12 through\n> 14.9.3. Part\n> three will follow presently.\n> \n> 52. Section 12 uses the phrase \"agent-driven negotiation\"; suggest\n> adding a note explaining that \"agent\" refers to \"user agent\".\n\nIt isn't necessarily a user agent.\n\n> \n> 53. Section 12.2, pg. 68, 1st para., has \"(this specification reserves\n> the field-name Alternates)\", however this field name is not described\n> in section 14 as a reserved field name nor otherwise elaborated\n> elsewhere in this specification.\n\nYes, this is left to the content negotiation people to define.\nIt is inappropriate to do more than reserve it, so that others\ndon't usurp the header name.  It should be \"header-name\" however.\n\n> \n> 54. Throughout the whole of section 13 it is often unclear as to\n> whether a requirement or statement is meant to apply only to a proxy\n> cache, to a user agent cache, or to both. For example, in 13.1.1, the\n> 6th paragraph has \"If the cache can not [sic] communicate with the\n> origin server ...\" which appears to apply to either a proxy or a user\n> agent cache. On the other hand, the 7th paragraph has \"If a cache\n> receives a response (...) that it would normally forward to the\n> requesting client ...\" which appears to apply to a proxy cache only.\n> Suggest editing the entirety of section 13 to clarify applicability to\n> these different caches.\n\n\"cache\" is the general term.  There can be client or proxy caches,\nand if it applies to both, the general term is used.  Saying\n\"proxy and client cache\" all the time would just get confusing,\nverbose and painful.  And there can be caches at the server.\nAnd there are braindead network caches to boot.  The generic term\nis best when we are talking bout caching systems, or someone\nwill think a requirement doesn't apply to them, and get the wrong\nanswer.\n\nWe also don't want to get caught up in terminological problems that can \noccur if a proxy is on the client system.  Web caching systems also often \nget implemented as pretty separate parts of browsers, rather than all \nentangled, and so such requirements can apply to client caches (or caches \non clients) as well, to illustrate the slippery slope, potentially.\n\nIf you want to point out particular places you think there are problems,\nwe can go into things further, but I'm not going to entertain \n\"editing the entirety of section 13\" at this very, very, late date.\n\n> \n> 55. Section 13, pg. 69, 3rd para., has \"the protocol requires that\n> transparency be relaxed ... only ... only ...\"; suggest changing to use\n> the keyword phrase \"MUST NOT relax transparency unless\" to make clear\n> the requirement.\n> \n\nThis is explanatory text rather than the requirement itself\nand it isn't appropriate for there to be a normative reference.\n\n> 56. Section 13.1.1, pg. 70, 1st para., has \"A correct cache MUST\n> respond with the most up-to-date response ...\"; since caching is always\n> optional, this would read better as \"A cache MUST NOT respond with a\n> response held by the cache that is appropriate to the request (...)\n> unless it meets one of the following conditions:\".\n\nThis is an improvement?\n\n> \n> 57. Section 13.1.1, pg. 70, numbered item (3) implies that 4XX and 5XX\n> responses are cachable. While this is true for 410, under what\n> circumstances should any 5XX response be cached?\n\nIf they were explicitly marked as cachable.  I don't think\nthat is very likely given the class of error, but might happen.\n\n> \n> 58. Section 13.1.2, 4th para., has \"whether the Warning MUST or MUST\n> NOT be deleted ...\" which does not state a requirement per se: use\n> \"is or is not to be deleted ...\".\n\nSee the ROSS20 issue.\n\n> \n> 59. Section 13.1.2, 5th para., has \"Warnings in responses that are\n> passed to HTTP/1.0 caches carry an extra warning-date field, which\n> prevents a future HTTP/1.1 recipient from believing an erroneously\n> cached Warning.\"  I can't interpret this statement based on information\n> in this section.  Please explain it further and state as a requirement\n> if indeed it is a requirement.\n\nIt is explanation: the actual requirements and full explanation are stated \nin the last paragraph of 14.46.  I'll move the last sentence of 13.1.2 \n(the cross reference to 14.46) up toward the beginning of the section \nso that it is clearer that this discussion is to be read together with \n14.46.\n\n> \n> 60. Section 13.1.2, 7th para., has \"a server might provide the same\n> warning with texts in both English and Basque\". How would a UA\n> discrimitate among different warnings using different languages unless\n> the language were explicitly marked? Unfortunately, RFC2047 does not\n> address this issue. I'd suggest permitting the extensions specified by\n> RFC2231 (which updates RFC2047) to be used to provide explicit language\n> tagging of quoted strings.\n\n\n> \n> 61. Section 13.1.3, 2nd para., has \"if there is any apparent conflict\n> between header values, the most restrictive interpretation is applied\n> ...\".  Change \"is applied\" to \"MUST\" or \"SHOULD\" \"be applied\".\n\nNo, I think this can't be made normative...  It doesn't lay out what\nthe conflicts might be, what \"most restrictive\" might be, and is\na general rule; it serves as guidance.\n\n> \n> 62. Section 13.1.4, 1st para., change \"the user agent might allow ...\"\n> to \"the user agent MAY allow ...\".\n\nThese are all examples in explanatory text. Ergo the \"might\" and it\nis inappropriate to word them that way, as they are hypothetical cases,\nrather than explicit protocol elements.\n\n> \n> 63. Section 13.1.4, 2nd para., has \"the user agent SHOULD explicitly\n> indicate to the user ...\" while section 13.1.5, 1st para., has \"This\n> allows the client software to alert the user\". This later statement\n> appears to make the indication/alert optional rather than recommended\n> as implied by the former.\n> \n\nYup.  Thanks...\n\nI'll reword the last two sentences to be:\n\n  \"Whenever a cache returns a stale response, it MUST mark it as such (using\n  a Warning header), enabling the client software to alert the user\n  that there might be a potential problem.\"\n\n\n> 64. Section 13.2.3, 3rd para., has \"HTTP/1.1 requires origin servers to\n> send a Date header, if possible, with every response ...\" seems to be\n> stating a conditional imperative. Rather than paraphrasing section\n> 14.18 and possibly confusing the requirements regarding Date header\n> transmission, I'd suggest rephrasing this to simply refer to a Date\n> header, if present, and to state what must be done in the case that a\n> Date header is not present.\n\nI don't see that this hurts: the statement is true, with the wiggle room\nto allow for clockless servers, as laid out in 14.18.  And the cross reference\nis there.\n\n> \n> 65. Section 13.2.3, pg. 76, 1st para. after pseudo code block, has \"the\n> server MUST\"; suggest changing to \"the proxy server MUST\".\n\nActually, I think it should be \"the cache MUST\", as you might have\na client cache performing the computation, not explicitly a proxy\nserver.\n\nNote that one could in principle have a cache built in to the origin server \n(some people call these \"HTTP accelerators\") and the same rule should \napply to these caches.\n\n> \n> 66. Section 13.3, pg. 78, 1st para., suggest changing \"it first has to\n> check\" with \"it will normally check\" to make this read more as a\n> statement of fact than a requirement.\n\nNo, the requirements stated elsewhere are pretty strong requirements. \nThey are mandantory requirements, with a wiggle or two for island caches. \nYour rewording makes it sound like those requirements are not as strong \nas they are.\n\n> \n> 67. Section 13.3.3, pg. 81, next to last paragraph, has \"A cache or\n> origin server ...\". Change to \"A caching proxy or origin server ...\".\n\nNo, you can have a local cache which has to make the same computation.\nThe requirement is on them as well.\n\n> \n> 68. Section 13.3.4, pg. 83, 2nd para., starts \"A note on rationale:\n> ...\"  Suggest changing this to a standard note form, i.e., use \"Note:\"\n> prefix with indented block paragraph.\n> \n\nIt is already a block paragraph.  I suppose I can just say Note:.\n\n> 69. Section 13.4, pg. 83, 2nd para., starts \"Note that ...\". Suggest\n> changing this to a standard note form, i.e., use \"Note:\" prefix with\n> indented block paragraph. Further, this note has \"some HTTP/1.0 caches\n> are known to violate this expectation without providing any Warning.\"\n> What warning should be provided in this case?\n\nOk, Note: it is. \n\nThe expectation is the one in the first paragraph. A 1.1 cache (or a 1.0 \ncache implementing Warning) would give you a 1xx Warning, depending on \nthe circumstances (little or no network connectivity).  But many/most\n1.0 caches don't implement Warning, so you won't see a warning from them.\n\n> \n> 70. Section 13.4, pg. 83, 3rd para., has \"so that the server can\n> indicate that certain resource entities, or portions thereof, MUST NOT\n> be cached ...\" which does not appear to state a specific requirement\n> per se. Suggest changing to \"... are not to be cached ...\".\n> \n\nYup.  You are right.  I'll fix.\n\n> 71. Section 13.4, pg. 84, 1st para., starts \"Note that ...\". Suggest\n> changing this to a standard note form.\n\nOk, will do.\n\n> \n> 72. Section 13.4, pg. 84, 3rd para.: suggest giving status codes 302\n> and 307 as examples of responses which are not cachable by default but\n> which may be explicitly marked as cachable by using Expires or the\n> \"public\" cache-control directive.\n\nI'll add an: \"(e.g. status codes 302 and 307)\" to the sentence.\n\n> \n> 73. Section 13.5, 1st para., remove comma in \"to requests, for use ...\".\n\nYup.  I'll fix.\n\n> \n> 74. Section 13.5.1, pg. 84, 1st para, 1st bullet, has \"End-to-end\n> headers which MUST be ...\". The use of MUST and SHOULD keywords in\n> relative clauses is problematic and should be avoided since it does not\n> state a requirement per se. Most such instances can be replaced by some\n> form of \"is\" to express simple fact; e.g., \"End-to-end headers which\n> are ...\".\n\nOK, I'll delete the \", which\".  But the MUST is necessary in this case\nas it is a requirement that all end-to-end headers are transmitted.\n\n\n> \n> 75. Section 13.5.1, pg. 84, 4th para., has \"Hop-by-hop headers\n> introduced in future versions of HTTP MUST be listed in a Connection\n> header ...\"  stipulates a requirement on the authors and/or\n> implementors of future versions of HTTP, and not on implementors of\n> HTTP/1.1. Suggest rephrasing this appropriately.\n> \n\nYou can introduce a hop-by-hop header in HTTP/1.1 as well, by\nusing Connection.\n\nI'll rephrase to be:\n\n\"Other hop-by-hop headers MUST be listed in a Connection header (section \n14.10) to be introduced into HTTP/1.1 (or later).\n\n> 76. Section 13.5.2, pg. 85, 4th para., has \" ... of the following\n> fields in message that ...\" which needs an article \"a\" before \"message\".\n\nYup.\n\n> \n> 77. Section 13.5.2, pg. 85, 5th para., has \"if not already present, it\n> MUST add a Warning 214 (...) if one does not already appear ...\" which\n> uses redundant language about \"already present\"/\"already appear[ing]\".\n\nOh, another one for the department of redundancy bureau...\n\n> \n> 78. Section 13.5.3, pg. 86, 5th para., has \"all such old headers are\n> replaced.\" which sounds like a requirement: \"... MUST be replaced.\"\n\nI think the sentence you complains about should be pulled up to the\nparagraph starting \"Unless\".\n\n    \"Unless the cache decides to remove the cache entry, it MUST also\n    replace the end-to-end headers stored with the cache entry with\n    coresponding headers recived in the incoming response.  If a header\n    field-name in the incoming response matches more than one header in\n    the cache entry, all such old headers MUST be replaced.\n\nI also suspect that changing the \"are\" to a \"MUST be\" makes\nit clearer that this is a requirement.\n\n> \n> 79. Section 13.6, pg. 87, 3rd para., has \"When the cache receives a\n> subsequent request whose Request-URI specifies one or more cache\n> entries including a Vary header field, ...\". Suggest changing \"one or\n> more cache entries including a Vary header field\" to \"one or more cache\n> entries of previous responses which contained a Vary header field\".\n> Further, it appears that this paragraph implies a caching proxy\n> context, but it is not clear that this would not also apply to a user\n> agent cache. The next paragraph (end of pg. 87 and beginning of pg. 88)\n> appears to be framed as applying only to a proxy. Again it isn't clear\n> that this does not apply to a UA cache.\n\nIf the document just says \"cache\", it applies to all caches, not\njust proxies.  Vary certainly applies to client caches as well.\nWe're not going to make the text more complicated (complicated enough\nas it is) by saying \"proxy and user agent caches all over\".\n\n> \n> 80. Section 13.8, pg. 88, 1st para., implies the context of a caching\n> proxy, requiring a 206 response when forwarding a partial response. In\n> the case of a user agent cache that receives and wishes to use a\n> partial response, it would seem that a Warning should be added by the\n> UA cache to the response it generatese for internal UA consumption.\n> However, there appears to be no Warning code that would serve this\n> purpose.\n\nAgain, this applies to all caches.  There is no implication that\nit only applies to a proxy.\n\nYou can do what you want in the user agent, including passing back\nother information (and there are obvious hacks possible of internal\nwarnings you might generate).\n\n> \n> 81. Section 13.11, 1st para., has \"All methods that might be expected\n> to cause modifications to the origin server's resources MUST be written\n> through to the origin server. This currently includes all methods\n> except for GET and HEAD.\" It would be better here to specify the\n> methods that must be written through explicitly: PUT and DELETE. It\n> isn't clear that OPTIONS, POST, or TRACE fall in this category; and\n> then there's CONNECT, which doesn't fit into either of the above\n> groups of methods.\n\nNo, HTTP is extensible, so we can't enumerate methods here. I\ndon't know how it could be any more clear than 'all methods except\nGET and HEAD\".  We don't define the semantic meaning of CONNECT in\nour document.  Certainly, OPTIONS, POST and TRACE all have to be\nwritten through.\n\n> \n> 82. Section 14, pg. 91: suggest adding a sentence to each header\n> defined by this section that states whether the header is end-to-end or\n> hop-by-hop and whether the header is cachable by default, cachable by\n> explicit cache directive, or never cachable.\n\nNo.  Duplicates with exisiting text in section 13.  Responses are\ncachable, not headers, in any case.\n\n> \n> 83. Section 14.1, pg. 92, 6th para., has \"the most specific reference\n> has precedence\"; suggest using \"SHOULD\" or \"MUST\" \"take precedence\".\n\nNo.  There is already a SHOULD above that says what to do.\n\n> \n> 84. Section 14.2, pg. 93, 3rd para., is quite confusing: suggest\n> rewriting without using the term \"mentioned\". Also, this para. seems to\n> be stating that if any \"iso-8859-1;q=1\" is always implied unless\n> otherwise explicitly present. This means that:\n> \n>     Accept-Charset: iso-8859-5, unicode-1-1;q=0.9\n> \n> really means\n> \n>     Accept-Charset: iso-8859-1;q=1, iso-8859-5;q=1, unicode-1-1;q=0.9\n> \n> (in which case 8859-1 would be given equal billing with 8859-5). And\n> that consequently the only way to exclude 8859-1 is to specify\n> \n>     Accept-Charset: iso-8859-1;q=0, iso-8859-5, unicode-1-1;q=0.9\n> \n> Is this the intended usage? If so, I find this not only convoluted but\n> seriously sub-optimal. This emphasis on 8859-1 as default really is too\n> much. Why go so far overboard?\n\nIndependent thread of discussion started.\n\n> \n> 84. Section 14.2, pg. 93, 4th para., has \"the server SHOULD send an\n> error response with the 406 (not acceptable) status code, though the\n> sending of an unacceptable response is also allowed.\" The effect of the\n> final clause of this statement is to downgrade SHOULD to MAY.  Either\n> remove the final clause or change to MAY. [My preference is to remove\n> the final clause.]  Note that the semantics stated here are expressly\n> different from Accept and Accept-Encoding which do require 406\n> responses for unconditionally compliant implementations.  This\n> inconsistency will make it difficult or impossible to implement\n> agent-driven content negotiation based on Accept-Charset variants.\n\nIndependent thread of discussion started.\n\n> \n> 85. Section 14.3, pg. 94, 1st para., has \"A server tests whether ...\";\n> suggest changing to \"A server MUST test ...\".\n\nHart to say MUST, when the most that can be said is SHOULD on what\nthe server will do.  (MUST is reserved in IETF parlance for what is\nrequired for the protocol to function.  I like it better as is.\n\n> \n> 86. Section 14.3, pg. 94, last para., has \"This means that qvalues will\n> not work and are not permitted with x-gzip or x-compress.\" This appears\n> to be stating a compatibility requirement, in which case MUST or SHOULD\n> is better (in which case this note would have to be made normative).\n\nNo, the note is there to allow implementers to do things that\nmaximize interoperability with the installed base.  Identity was\nintroduced to solve a existing problem in that installed base,\nand so is not out there.  We're providing guidance for implementers to\nminimize problems with this installed base.  But there will be problems,\nno matter how hard we try.\n\nLife is hard, and then you die; HTTP was buggy in this area.\n\n> \n> 87. Section 14.4, pg. 95, next to last para., has \"recommended\" and\n> \"MUST NOT\" in a note. Either make this normative (not a note) and use\n> SHOULD and MUST NOT consistently or use \"ought\" and \"ought not\",\n> respectively.\n\nI'll put it inline just to reduce confusion.\n\n> \n> 88. Section 14.4 does not contain language as found in other Accept-*\n> headers that recommends a 406 response in the case the server cannot\n> satisfy the request based on its variant set for the specified URI.\n> This precludes implementing client-side content negotiation along this\n> variance axis. Suggest adding the required language or a note\n> indicating why it is not present and what this means for client-side\n> negotiation.\n\nIndependent thread of discussion started.\n\n> \n> 89. Section 14.5. I find \"Accept-Ranges\" to be inconsistent in its name\n> and usage with other Accept-* headers. This really should have been\n> handled with Expect. Unless you can change this to use Expect (and I\n> doubt you can at this stage), I'd suggest adding a note to indicate\n> this inconsistency. I'd also urge adding a mechanism which does use\n> Expect and deprecates the use of Accept-Ranges in a future version of\n> HTTP. [If \"Allow-Ranges\" had been used instead, at least this would be\n> consistent with the \"Allow\" header usage.]\n> \n\nWe all know the name is a bug.  But it is too late to change.  Too late\nto use expect for it.  And I don't think it is worth introducing into\nexpect in the future, since it is an optional field, mostly documented\nfor the sake of existing practice.\n\n\n> Regarding the actual use of Accept-Ranges, which response would be\n> appropriate for a server which sends \"Accept-Ranges: none\" in response\n> to a Range request?  406? Some mention of the appropriate response\n> should be made here.\n\nNo, it is advisory (a hint from the server to \"please don't bother to ask,\nas I can't).\n\n> \n> 90. Section 14.8, pg. 97-98, seems to imply a caching proxy when\n> referring to \"shared cache\"; however, this seems to apply as well to a\n> shared cache on a user agent. Suggest making it clear which kinds of\n> caches are addressed by these paragraphs.\n\nIt is a cache shared between users, as defined in 13.7, to which\nit has a correct cross reference.  And yes, it could be a shared\ncache in a client machine); if it shared between users, it is a shared\ncache.  If it is private to an individual user, it is a private cache.\n\nCaching proxies aren't necessarily shared. (though likely are).\n\nIf we didn't make appropriate distinctions, we'd get into real trouble.  \nYou have to keep the concepts separate, or you'll get into trouble on \nprivacy grounds, and/or return documents to the wrong people.\n\n> \n> 91. Throughout the whole of section 14.9, it is often unclear as to\n> whether a requirement or statement is meant to apply only to a proxy\n> cache, to a user agent cache, or to both.\n\nCaches are caches.  The requirements apply to caches.  If we say\ncache, you can presume a user agent cache is included...\n\n> \n> 92. Section 14.9, pg. 98, 1st para., has \"that MUST be obeyed by all\n> caching mechanisms\" which does not specify a requirement per se (note\n> use of MUST in relative clause).\n\nThis is saying that the Cache-Control header is mandantory to implement \nin HTTP/1.1, everywhere along the chain (server, proxy, client caches, \netc.).  Note that a simplistic cache implementation might choose to not \ncache under alot of circumstances that it can concievably cache, but we \nhave to demand it be implemented at least at the minimal level.  Otherwise,\nit won't be of any use.\n\nErgo, you MUST obey the directives.\n\n\n> \n> 93. Section 14.9, pg. 99, 1st para., has \"When a directive appears\n> without any 1#field-name parameter, the directive applies to the entire\n> request or response.\" At the present, no cache-request-directive\n> employs a 1#field-name parameter (see pg. 98); consequently all request\n> directives apply to the entire request in all cases.\n> \n\nThe text already makes it clear that this is an extensibility hook; see\n14.9.6 for an example.\n\n> 94. Section 14.9.1, pg. 99, 2nd para., has \"Indicates that the response\n> is cachable by any cache, ...\". Suggest changing \"is cachable\" to \"MAY\n> be cached\".\n> \nOk, I'll change to \"MAY be cached\".\n\n> 95. Section 14.9.1, pg. 99, last para., has keyword MUST NOT in\n> resultative clause \"Indicates that ...\"; suggest rephrasing as\n> imperative, e.g., \"A response containing the cache directive 'private'\n> MUST NOT be cached by a shared cache.\".\n\nI can't get excited about this rewording...  And it drops the intent phrase.\nNo.\n\n> \n> 96. Section 14.9.1, pg. 100, 2nd para. under \"no-cache\", has \"the\n> specified field-name(s) MUST NOT be sent in the response to a\n> subsequent request without successful revalidation with the origin\n> server.\" followed by \"This allows an origin server to prevent the\n> re-use of certain header fields in a response, while still allowing\n> caching of the rest of the response.\" I find this rather confusing. My\n> reading of this is that a cache can, indeed, retain and reuse a field\n> specified in a no-cache directive as long as it revalidates the entry\n> with the origin server.  Furthermore, it appears that \"no-cache\" with\n> no field name is to be interpreted identically to must-revalidate. I\n> have always interpreted no-cache without a field-name to mean don't\n> cache the response under any circumstances. Which is it?\n\nAgain, you are confused by the extensibility hook.  We define the default \nbehavior for the directive (not to cache), and allow extensions to relax \nthat behavior (possibly cache, depending upon these future extensions).\n\n> \n> 97. Section 14.9.2, pg. 100, 1st para., needs to do a better job of\n> defining \"store\" without using keywords in the term or definition.\n> Also, it would be better to place this definition at the beginning of\n> this section. I would suggest a rewrite as follows:\n> \n> \"The purpose of the no-store directive is to prevent the inadvertent\n> release or retention of sensitive information. In the present context,\n> 'store' means to intentionally retain data in non-volatile storage.\n> The no-store directive applies to the entire message, and MAY be sent\n> either in a response or in a request. If sent in a request, a cache\n> MUST NOT store any part of the request or its response. If sent in a\n> response, a cache MUST NOT store any part of either the response or the\n> request that elicitied it. This directive applies to both shared and\n> non-shared caches.\"\n> \n> In this rewrite, I've removed the statement about removing data from\n> volatile storage, since this doesn't seem to apply to the semantics of\n> this directive.  In particular, many caches employ a volatile and a\n> non-volatile component.  The described semantics appears to strictly\n> address use of the non-volatile component, and not the volatile\n> component. If this is not the case, then these semantics would appear\n> to broaden the stated intention found in the last paragraph on pg.\n> 100. Furthermore, this directive applies equally to a user agent cache\n> as well as a caching proxy, so language aimed at proxies (e.g., \"after\n> forwarding it\") appears to be overly narrow in scope.\n\nI don't like your rewrite; for example, it drops the requirement to\neliminate the copy in a volatile cache as soon as you've forwarded the\nmessage in a proxy.  Maybe you'd like your medical records in someone's\ncrash dump, but I don't.\n\nThis privacy issue is more slippery than it appears on the surface.  I'm\nnot about to try to rewrite it at this late date.\n\nAgain, caches are caches; if you are a user agent, and build a cache,\nyou are subject to these requirement.\n\n> \n> 98. Section 14.9.3, pg. 101, 2nd para., has \"the max-age directive\n> overrides the Expires header\".  What is the imperative status of this\n> statement? Should this read \"MUST override\"?\n\nI think it is fine as written.  It is just stating what to do when\nboth are present and you understand both (use max-age).\n\n> \n> 99. Section 14.9.3, pg. 101, 5th para. (s-maxage), has \"i.e., that the\n> shared cache MUST NOT ...\" which doesn't state a requirement per se.\n> Rewrite to avoid using MUST NOT in an explanatory relative clause\n> (particularly one using \"i.e.\").\n\nI guess you're right.  MUST NOT here should be must not.\n\n> \n> 100. Section 14.9.3, pg. 101, 5th para. (s-maxage), has \"The s-maxage\n> directive is always ignored by a private cache\". Should this read \"A\n> private cache MUST ignore the s-maxage directive.\"?\n> \n\nI think this is ok the way it is.\n\n> 101. Section 14.9.3, pg. 102, 7th para., has \"only if this does not\n> conflict with any MUST-level requirements\"; suggest rephrasing as \"with\n> any conditionally compliant requirements\" to avoid using MUST in this\n> context which is not stating a requirement per se.\n> \n\nI'll put the MUST in quotes.\n- Jim\n\n\n\n", "id": "lists-012-8312211"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "Sorry for the slow response - due to a mailing list malfunction,\nI wasn't getting HTTP-WG messages for a few days.\n\nLarry writes:\n\n    In most cases, the 'cachable' constraint is not on whether\n    you store the value in a cache, but whether you USE the\n    cached value. Now, it may be foolish to cache something that\n    you cannot use, but perhaps not; e.g., even if the results\n    aren't exactly cachable, could you use delta-coding on the\n    next request with an appropriate entity tag?\n    \nYes indeed.  This might be one of the biggest payoffs for\ndelta-encoding, since it allows the \"virtual transmission\"\nof a sequence of changing responses without actually sending\nall of the bits over and over again.\n\n    I don't know whether it's best to say SHOULD NOT and let\n    implementors figure out the constraints, or to go ahead\n    and use MUST NOT, but to carefully redefine 'cachable' or\n    'cached' to mean that the cached value cannot be used,\n    and thus probably shouldn't be stored.\n\nI agree that the term \"cachable\" may be a poor one to use in a\nstatement of formal requirements.  But we *do* have the right\nformal definition in section 1.3:\n\n cachable\n     A response is cachable if a cache is allowed to store a copy of\n     the response message for use in answering subsequent requests. The\n     rules for determining the cachability of HTTP responses are\n     defined in section 13. Even if a resource is cachable, there may\n     be additional constraints on whether a cache can use the cached\n     copy for a particular request.\n\nalthough I'm not sure I would trust every implementor to read and\ninternalize these definitions.  Fortunately, the folkloric definition\nof \"cachable\" (meaning: to store at all in a cache) is a stronger\nrequirement.  So an implementor who uses the folkloric definition,\nrather than the formal one, risks losing efficiency but not\ntransparency.\n\nHowever, Jim proposed:\n\n    Responses to methods other than GET or HEAD MUST NOT be cached,\n    unless the response includes appropriate Cache-Control or Expires\n    header fields\n\nand the verb here (\"be cached\") is not formally defined in the\nsense that Larry describes, as far as I know.  I'd suggest:\n\n    Responses to methods other than GET or HEAD MUST NOT be treated\n    as \"cachable\" (by the definition in section 1.3),\n    unless the response includes appropriate Cache-Control or Expires\n    header fields.\n\nKoen writes:\n    >\"Responses to methods other than GET or HEAD MUST NOT be cached,\n    >unless the response includes appropriate Cache-Control or Expires\n    >header fields\"\n\n    I believe that the spec makes the above statement somewhere already,\n    though I just tried to find it and failed.  Jeff?\n\nHmm.  Section 13.4 (Response Cachability) says something\nvaguely similar, although it might say something wrong: it\ndoesn't mention any methods, but says (in effect) that all\nstatus-200 responses are cachable unless otherwise marked.\nSince POST definitely allows a status-200 response and is\nby default not cachable (section 9.5), this is a contradiction.\n\nI did a quick search for a statement matching the one that Jim\nsuggests, and didn't find it (which doesn't mean that it isn't\nthere).\n\nMy suggested fix:\n\n(1) In 13.4 change:\n\n   Unless specifically constrained by a cache-control (section 14.9)\n   directive, a caching system MAY always store a successful response\n   (see section 13.8) as a cache entry, MAY return it without validation\n   if it is fresh, and MAY return it after successful validation.\n\nto:\n\n   Unless specifically constrained by a cache-control (section 14.9)\n   directive, a caching system MAY always store a successful response\n   (see section 13.8) to a GET or HEAD request\n   as a cache entry, MAY return it without validation\n   if it is fresh, and MAY return it after successful validation.\n   A caching system MUST NOT treat responses to other methods\n   as cachable (by the definition in section 1.3) unless the\n   response includes Cache-Control or Expires header fields\n   implying that the response is cachable.\n\n(2) In section 9, make a forward reference to section 13.4?\n\n-Jeff\n\n\n\n", "id": "lists-012-8349085"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "At 11:47 11/16/98 PST, Jeffrey Mogul wrote:\n>My suggested fix:\n>\n>(1) In 13.4 change:\n>\n>   Unless specifically constrained by a cache-control (section 14.9)\n>   directive, a caching system MAY always store a successful response\n>   (see section 13.8) as a cache entry, MAY return it without validation\n>   if it is fresh, and MAY return it after successful validation.\n>\n>to:\n>\n>   Unless specifically constrained by a cache-control (section 14.9)\n>   directive, a caching system MAY always store a successful response\n>   (see section 13.8) to a GET or HEAD request\n>   as a cache entry, MAY return it without validation\n>   if it is fresh, and MAY return it after successful validation.\n>   A caching system MUST NOT treat responses to other methods\n>   as cachable (by the definition in section 1.3) unless the\n>   response includes Cache-Control or Expires header fields\n>   implying that the response is cachable.\n\nThis doesn't work with the wording in section 13.11 which I am basing the \ncache interactions for the M- methods used by the HTTP Extensions\nFramework on:\n\nAll methods that might be expected to cause modifications to the origin \nserver's resources MUST be written through to the origin server. This \ncurrently includes all methods except for GET and HEAD. A cache MUST NOT \nreply to such a request from a client before having transmitted the request \nto the inbound server, and having received a corresponding response from the \ninbound server. This does not prevent a proxy cache from sending a 100 \n(Continue) response before the inbound server has sent its final reply.\n\nThe alternative (known as \"write-back\" or \"copy-back\" caching) is not \nallowed in HTTP/1.1, due to the difficulty of providing consistent updates \nand the problems arising from server, cache, or network failure prior to \nwrite-back.\n\nThe latest draft of the HTTP Extension Framework is available at\n\nhttp://info.internet.isi.edu:80/in-drafts/files/draft-frystyk-http-extensions-01.txt\n\nI don't mind that the request to the origin server is made conditional\nbut it must not be served without having been forwarded to the origin\nserver.\n\nHenrik\n\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-8360700"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "Henrik writes:\n>   A caching system MUST NOT treat responses to other methods\n>   as cachable (by the definition in section 1.3) unless the\n>   response includes Cache-Control or Expires header fields\n>   implying that the response is cachable.\n\n    This doesn't work with the wording in section 13.11 which I am\n    basing the cache interactions for the M- methods used by the HTTP\n    Extensions Framework on:\n\n    ...\n\n    I don't mind that the request to the origin server is made\n    conditional but it must not be served without having been forwarded\n    to the origin server.\n\nIf the origin server is using an extension that does require\nwrite-through, then it shouldn't be sending responses with\nsomething like:\nCache-control: max-age=12345\nwhich implies cachability.  In the case you're describing, the\norigin server has to send something like\nCache-control: max-age=12345, must-revalidate\nor\nCache-control: max-age=0, must-revalidate\nto get the right semantics.\n\nUnless your extension mechanism wants to support extensions\nwhere the origin-server is ignorant of the caching implications\n(which seems foolish), then I think we already have the necessary\nmechanisms in place, and we shouldn't be adding new restrictions.\n\n-Jeff\n\n\n\n", "id": "lists-012-8370824"}, {"subject": "Re: HTTP 1.1 issue 09: 8.1.2.1 Negotiation (ROSS09", "content": "> In section 8.1.2.1 \"Negotiation\", the statement                              \n>  \"An HTTP/1.1 client MAY expect a connection to remain open, but           \n>   would decide to keep it open based on whether the response from a         \n>   server contains a Connection header with the connection-token             \n>   close.\"                                                                   \n> does not state a requirement, the \"MAY\" should probably be \"may\".            \n \nI don't think I agree: we want the usual 1.1 client to use persistent\nconnections, though it is not mandantory.\n- Jim\n\n\n\n", "id": "lists-012-8379332"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "At 12:32 11/16/98 PST, Jeffrey Mogul wrote:\n\n>If the origin server is using an extension that does require\n>write-through, then it shouldn't be sending responses with\n>something like:\n>Cache-control: max-age=12345\n>which implies cachability.  In the case you're describing, the\n>origin server has to send something like\n>Cache-control: max-age=12345, must-revalidate\n>or\n>Cache-control: max-age=0, must-revalidate\n>to get the right semantics.\n\nThis is different from the cache semantics that I am after.\n\nThere is a large group of extensions which your proposed change will\nimpact:  the group of extensions describing under which terms a cached\nentry can be handed out based on payment, copyright, licensing, content\nfiltering, added service, etc.\n\nIf the cache knows about one of these mandatory extensions (using the M-\nprefix) then it should be able to serve the request without revalidating\nthe response. Imagine if 80% of requests are M- prefixed with some widely\nused copyright extension then the forced revalidations are potentially\nseriously impacting cache performance without reason.\n\nTrue, caches that don't understand the extension will have to validate but\non the other hand, if the extension is fully cachable by everyone then it\nhardly calls for messing with the method name but instead for an optional\nextension. There is no need to produce more than one GET method.\n\nI do not believe we at this point can change whether we consider caching or\nthe method to be the highest in the hierarchy.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-8387070"}, {"subject": "Re: Comments (Part 3) on HTTP ID Rev 05 (#'s 116, 134, 135, 138, 147 and 149", "content": "I'm running out of time, and may not be able to review all of your (Glenn's)\npoints (submitted Friday, LONG after the end of IETF last call)\nbefore having to start production of the draft for the ID draft\ndeadline.  \n\nYou (Glenn_ did ask in private mail that I look into points\n116, 134, 135, 138, 147 and 149 in particular.\n\nI'll look at your other points in this mail only if time permits. \n\nAgain, thanks for all your efforts; I just wish they had been\nsooner.\n- Jim\n\n\n\n> 116. Section 14.16, pg. 111, 5th para., has 'A response with status\n> code 206 (Partial Content) MUST NOT include a Content-Range field with\n> a content-range-spec of \"*\"'. It isn't clear how to interpret this\n> since it is not possible for the value of content-range-spec to be\n> merely \"*\".  Perhaps this means 'with a content-range-spec which uses\n> \"*\" as the instance-length'? If this is the case, then perhaps it\n> would be better to change the definition of instance-length to:\n> \n> instance-length = 1*DIGIT | \"*\"\n> \n> And then say '... MUST NOT include a Content-Range field with an\n> instance-length of \"*\"'.\n> \n\nIn out of band mail, Dave Krystol says:\n\n\"I believe this was meant:\n\n   A server sending a response with status code 416 (Requested range not\n   satisfiable) SHOULD include a Content-Range field with a byte-range-\n   resp-spec of \"*\". The instance-length specifies the current length of\n   the selected resource. A response with status code 206 (Partial\n   Content) MUST NOT include a Content-Range field with a byte-range-resp-\n   spec of \"*\".\n\nThis looks better to me as well.\n\n> 134. Section 14.28, pg. 122, 1st para., why have \"... the server\n> SHOULD perform the requested operation as if the If-Unmodified-Since\n> header were not present.\" while section 14.24, 2nd para., has \"... the\n> server MAY perform the requested method if the If-Match header field\n> did not exist.\"? Suggest using consistent language in these sections\n> \"SHOULD/MAY\", \"operation/method\", \"were not present/did not exist\".\n\nThe SHOULD/MAY difference is deliberate.  You need to understand the *\nmatching of If-Match.  I think things are ok as is.\n\n> \n> 135. Section 14.28, pg. 122, 2nd para., should note the asymmetry in\n> response codes with respect to \"If-Modified-Since\" (section 14.25).\n> Here we have 412 (Precondition Failed) while in 14.25 we have 304 (Not\n> Modified). This asymmetry is odd since If-Match and If-Not-Match both\n> use 412.\n> \n\nIf-Modified-Since and If-Unmodified-Since are independent of Etags,\nbeing date related.  If-Modified-Since has been the common cache validation\nmechanism (rather, the only one before etages.)\n\nPart of the asymmytry is due to If-Unmodified-Since is new in HTTP/1.1;\nit seemed best if If-Unmodified-Since fit into the general precondition\nfailed framework.  We can't go back and change If-Modified-Since at this\ndate, though.  I don't think it is worth a note; if we noted every\npiece of HTTP/1.0 strangeness, we'd be wading through too much extraneous\nmaterial.  \"Ours is not to wonder why, ours is to do and die.\"\n\n> 138. Section 14.35.2, pg. 127, 3rd para., 2nd bullet, has \"It does not\n> affect the 304 (Not Modified) response returned if the conditional is\n> false.\" However, a 412 (Precondition Failed) response may apply as\n> well.\n\nNo, Range is a modifier placed on other requests, specifying\nwhich bytes are to be returned if an entity is transferred.  So it\nis describing the case of when a Range header and conditional GET\nis being performed; it is the case that you'll still get a 304 (Not Modified)\nfor a IF-Modified-Since, for example.\n\n\n> 147. Section 14.39, pg. 130, item 3, if an implied \"chunked\" always\n> gets qvalue of 1, then it will always win over lesser qvalues.\n> Specifying \"chunked;q=0\" would permit overriding this default;\n> however, the present syntax does not admit \"chunked\" (unless the\n> syntax of t-codings is changed to:\n> \n> t-codings = \"trailers\" | transfer-coding\n\nI ran this one by Henrik.  He says:\n\n\"Nope, it's fine asis. Chunked can not be set to 0 - all the client can do\nis to say whether it understands chunked with or without trailers. Other\nencodings can use a value of q=1 (default) to be just as good as chunked.\n\nBasically, the q-values are used as binary values. Understanding deflate\nwith -.65 doesn't make a lot of sense to me.\"\n\n> \n> 149. Section 14.40, pg. 130, 4th para., has \"A server MUST NOT include\n> any header fields unless ...\". I believe this should read \"A server\n> MUST NOT include any header fields in the trailer of a message unless\n> ...\".\n\n\nHenrik points out that your complaint is due to an editorial mistake\nI made in the last draft; the entire sentence was meant to be struck.  See:\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q3/0135.html\nfor details.  It is mistakes like these that make me reluctant to\ndo more than absolutely necessary at this point.\n\nSo I'll fix it this time around.\n\n\n- Jim\n\n\n\n", "id": "lists-012-8396482"}, {"subject": "Re: Comments (Part 3) on HTTP ID Rev 05 (#'s 116, 134, 135, 138,    147 and 149", "content": "At 15:34 11/16/98 -0800, Jim Gettys wrote:\n\n>Basically, the q-values are used as binary values. Understanding deflate\n>with -.65 doesn't make a lot of sense to me.\"\n\nJust to clear out any confusion, the dash is not a minus - it's a typo.\n\nHenrik\n\n\n\n", "id": "lists-012-8410711"}, {"subject": "Fwd: Warning and I18N question (Adams #60)", "content": "I asked Jeff about this one.\n\nAs a result, I think we are best leaving things as they are, particularly\nsince it would be a new feature.\n- Jim\n\n\nattached mail follows:\n    Is it safe to update the reference in 14.46 to RFC 2231?\n    \n    > \n    > 60. Section 13.1.2, 7th para., has \"a server might provide the same\n    > warning with texts in both English and Basque\". How would a UA\n    > discrimitate among different warnings using different languages unless\n    > the language were explicitly marked? Unfortunately, RFC2047 does not\n    > address this issue. I'd suggest permitting the extensions specified by\n    > RFC2231 (which updates RFC2047) to be used to provide explicit language\n    > tagging of quoted strings.\n    \nI don't have any gripes about *allowing* the use of RFC2231.\nI'm not sure I would require it, though, since I don't know\nhow widely implemented this is.  (Maybe Larry knows.)\n\nHowever, my recollection of the history behind the Warning\ninternationalization discussion was that we assumed that either\nthe reader would understand the warning text or s/he wouldn't.\nThe expectation is that the warning-generator (probably a proxy)\nwould use the Accept-Language field in a request to guess at\nthe appropriate language for a Warning in the response.  If\nthe guess was hazy, it might give back several Warning texts,\ne.g.\n\nWarning: 113 proxy.foo.com \"Heuristic Expiration\"\nWarning: 113 proxy.foo.com \"Expiration Heuristique\"\nWarning: 113 proxy.foo.com \"Heuristischer Verfall\"\n\n(warning: translations courtesy of AltaVista) and the browser\nmight display this in a pop-up as\n\nWarning #113 from proxy.foo.com:\n\nHeuristic Expiration\nExpiration Heuristique\nHeuristischer Verfal\n\nand let the user figure it out.  (Of course, to most users,\nsuch a warning would be incomprehensible in any language.)\n\nAdding RFC2231-style tagging would let the browser avoid\ndisplaying Warnings not in the user's native language, I\nsuppose.  But I'm not sure that a proxy implementor should\ncount on this being available.\n\nAlso, it's a new \"feature\" that probably hasn't been tested\nfor our implementation report.\n\n-Jeff\n\n\n\n", "id": "lists-012-8420361"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "Henrik writes:\n    There is a large group of extensions which your proposed change\n    will impact:  the group of extensions describing under which terms\n    a cached entry can be handed out based on payment, copyright,\n    licensing, content filtering, added service, etc.\n\n    If the cache knows about one of these mandatory extensions (using\n    the M- prefix) then it should be able to serve the request without\n    revalidating the response. Imagine if 80% of requests are M-\n    prefixed with some widely used copyright extension then the forced\n    revalidations are potentially seriously impacting cache performance\n    without reason.\n\n    True, caches that don't understand the extension will have to\n    validate but on the other hand, if the extension is fully cachable\n    by everyone then it hardly calls for messing with the method name\n    but instead for an optional extension. There is no need to produce\n    more than one GET method.\n\nHenrik, please read section 14.9.6 (Cache Control Extensions).\nIt already handles the problem you are describing.  For example,\nan extension can define the meaning of a new cache-control\ndirective (let's call it \"henrik-mode\"), and send:\n\nCache-control: max-age=0, must-revalidate, henrik-mode=999\n\nAny cache that doesn't understand your extension must ignore\n\"henrik-mode\" and so will do the necessary revalidation.  If\nthe cache does understand your extension, 14.9.6 allows it\nto use the \"henrik-mode\" directive to modify the behavior\nof the other two (\"modify\" may mean \"ignore\").\n\n    I do not believe we at this point can change whether we consider\n    caching or the method to be the highest in the hierarchy.\n\nI don't think at this point we should be proposing changes to\nthe HTTP/1.1 protocol without first reading the existing\nspecification :-)\n\n-Jeff\n\n\n\n", "id": "lists-012-8429837"}, {"subject": "Re: Fwd: Problem with PUT and redirections on Apach", "content": ">We have a protocol problem.  Section 8.2.4 says that we MUST return\n>an \"error status\" if we don't return a 100.  That shouldn't say what\n>kind of other status the server might return, since some requests can\n>be completed successfully without reading the body (e.g., the OPTIONS\n>body can be discarded).  302 responses is another case where the\n>server does not want to return 100 even though it is not an error.\n\nI think this can be fixed by replacing \"error status\" with\n\"final status code\" throughout section 8.2.4.\n\n....Roy\n\n\n\n", "id": "lists-012-8438942"}, {"subject": "Preparation of draft 06 (probably the FINAL draft) has begun..", "content": "Before it isn't too late (i.e. today), please look over the responses\nI (and some others) have made to the issues raised since IETF last call.\n\nI have begun preparation of the draft, and it will be soon too late to\nmake any changes.  With some luck, this will be the draft that goes to\nDRAFT Standard; I'd hate to have made some mistake that would require\nanother draft, and it is up to you to make sure I don't (other than a\nmechanical editing mistake, for which I am alone responsible). I REALLY\nwant to get a draft in for the ID draft cutoff, and get this to bed.\n\nThere are a number of issues that still should have some final\ndiscussion.  These include:\n\nADAMS3 ADAMS31 ADAMS84 ADAMS84b ADAMS88\n\nPlease look at these in particular, and my previous responses, while there\nis time.  I don't know if I will get to all of the nits in ADAMS3 in time;\nI did look at the ones that Glenn said were significant and respond to\nthem.\n\nThank you all for your contributions to the HTTP working group.\nYou editor,\nJim Gettys\n\n\n\n", "id": "lists-012-8446630"}, {"subject": "My thoughts on issues ADAMS84b, ADAMS8", "content": "ADAMS84b (406 / Accept-Charset):\n\nTo me, the main issue is that the language in 14.2 implies an\n\"either/or\" choice when it means to suggest optional additional\nbehavior. \n\nThe description of the 406 status code (sec. 10.4.7) already says the\nresponse SHOULD also contain an explanatory entity (unless the response\nis to a HEAD request), *and* that the entity does not have to be\n\"acceptable\" to the user agent (according to the constellation of Accept\nheader fields the user agent has sent).\n\nSo I interpret the the clause: \n\n   \"...the server SHOULD send an error response with the 406 (not\nacceptable) status code, though the sending of an unacceptable response\nis also allowed.\"\n\nto mean \n\n   \"... the server SHOULD send an error response with the 406 (not\nacceptable) status code, though the character set of the response entity\nmay not be acceptable to the user agent based upon the Accept-Charset\ninformation it provided.\"\n\nI think the SHOULD needs to stay a SHOULD, but the second part might be\nredundant given the existing language in 10.4.7.\n\nAlong the same lines, I think the \"Note\" in 10.4.7 also implies the same\nkind of false choice between sending an \"unacceptable\" response and\nsending a 406 response.  To me, sending a 406 is always the right thing\nto do -- the choice is whether or not you attach an \"unacceptable\"\nentity to the response.\n\n\nADAMS84:\n\nI think the roots of the \"implicit\" use of iso-8859-1 goes way way back\nto the summer of '96 and a discussion which happened at the 36th IETF\nmeeting.  The main issues involved\n\n* backwards compatibility with HTTP/1.0\n* the existence of content mechanisms (CGI, for example) that often have\nnever indicated what character set they're generating\n* a vigorous discussion about what should be assumed for a \"text\" entity\nof \"unknown\" charset (and iso-8859-1 was winning based on the first\npoint). \n\nMaybe Larry and Roy can fill in some of the gaps....\n\n\n\n", "id": "lists-012-8454931"}, {"subject": "More background re: ADAMS8", "content": "I did a little digging in the archives on this one.\n\nTo find the history of the \"charset flap\", go to \n\nhttp://www.egroups.com/list/http-wg/\n\nand search for a thread that began on June 26, 1996 entitled \"DRAFT\nMinutes, HTTP working group\", and then mutated into \"charset flap\", and\nthen \"proposed HTTP changes for charset\".  This basically details the\nevolution of the choice that iso-8859-1 is the \"unstated default\ncharset\", and a user agent that sends no Accept-Charset at all is\nexpected to support iso-8859-1.   \n\n\n\n", "id": "lists-012-8465187"}, {"subject": "Re: Comments (Part 2) on HTTP I-D Rev 05 (Adams #84, AcceptCharset", "content": "Jim Gettys:\n>\n>\n>Glen Adams notes:\n>\n>> \n>> 84. Section 14.2, pg. 93, 3rd para., is quite confusing: suggest\n>> rewriting without using the term \"mentioned\". Also, this para. seems to\n>> be stating that if any \"iso-8859-1;q=1\" is always implied unless\n>> otherwise explicitly present. This means that:\n>> \n>>     Accept-Charset: iso-8859-5, unicode-1-1;q=0.9\n>> \n>> really means\n>> \n>>     Accept-Charset: iso-8859-1;q=1, iso-8859-5;q=1, unicode-1-1;q=0.9\n>> \n>> (in which case 8859-1 would be given equal billing with 8859-5). And\n>> that consequently the only way to exclude 8859-1 is to specify\n>> \n>>     Accept-Charset: iso-8859-1;q=0, iso-8859-5, unicode-1-1;q=0.9\n>> \n>> Is this the intended usage? If so, I find this not only convoluted but\n>> seriously sub-optimal. This emphasis on 8859-1 as default really is too\n>> much. Why go so far overboard?\n>\n>Not being a charset wizard, I don't have a good feeling for whether\n>any change is necessary.\n\nI am not charset wizzard, but I play one on the net.  The current\nlanguage is correct, no change should be made.  It is this convoluted\nfor historical/compatibility reasons.\n\n>\n>Comments?\n>- Jim\n\nKoen.\n\n\n\n", "id": "lists-012-8473620"}, {"subject": "Re: My thoughts on issues ADAMS84b, ADAMS8", "content": "Wingard, Steve:\n>\n>ADAMS84b (406 / Accept-Charset):\n>\n>To me, the main issue is that the language in 14.2 implies an\n>\"either/or\" choice when it means to suggest optional additional\n>behavior. \n>\n>The description of the 406 status code (sec. 10.4.7) already says the\n>response SHOULD also contain an explanatory entity (unless the response\n>is to a HEAD request), *and* that the entity does not have to be\n>\"acceptable\" to the user agent (according to the constellation of Accept\n>header fields the user agent has sent).\n>\n>So I interpret the the clause: \n>\n>   \"...the server SHOULD send an error response with the 406 (not\n>acceptable) status code, though the sending of an unacceptable response\n>is also allowed.\"\n>\n>to mean \n>\n>   \"... the server SHOULD send an error response with the 406 (not\n>acceptable) status code, though the character set of the response entity\n>may not be acceptable to the user agent based upon the Accept-Charset\n>information it provided.\"\n\nNo, that is probably not the intended meaning. I can't recall whether\nI wrote that particular sentence but I definately wrote sentences like\nit elsewhere.\n\nThe above probably refers to the usual problem of what to send as a\nresponse for un-negotiated resources only available in a single form.\nSending an response with a known-unacceptable entity is not that good,\nbut sending an error response is hardly better, so in this case the\nspec intends to allow the server author to freely choose between the\ntwo evils.\n\nNo edit is required in my opinion.\n\n>I think the SHOULD needs to stay a SHOULD, but the second part might be\n>redundant given the existing language in 10.4.7.\n\nTrue, one could see it as a restatement the language in 10.4.7.\n\nKoen.\n\n\n\n", "id": "lists-012-8482614"}, {"subject": "RE: My thoughts on issues ADAMS84b, ADAMS8", "content": "Koen Holtman: \n> Wingard, Steve:\n> >\n> >ADAMS84b (406 / Accept-Charset):\n> >\n> >To me, the main issue is that the language in 14.2 implies an\n> >\"either/or\" choice when it means to suggest optional additional\n> >behavior. \n> >\n> >The description of the 406 status code (sec. 10.4.7) already says the\n> >response SHOULD also contain an explanatory entity (unless \n> the response\n> >is to a HEAD request), *and* that the entity does not have to be\n> >\"acceptable\" to the user agent (according to the \n> constellation of Accept\n> >header fields the user agent has sent).\n> >\n> >So I interpret the the clause: \n> >\n> >   \"...the server SHOULD send an error response with the 406 (not\n> >acceptable) status code, though the sending of an \n> unacceptable response\n> >is also allowed.\"\n> >\n> >to mean \n> >\n> >   \"... the server SHOULD send an error response with the 406 (not\n> >acceptable) status code, though the character set of the \n> response entity\n> >may not be acceptable to the user agent based upon the Accept-Charset\n> >information it provided.\"\n> \n> No, that is probably not the intended meaning. I can't recall whether\n> I wrote that particular sentence but I definately wrote sentences like\n> it elsewhere.\n> \n> The above probably refers to the usual problem of what to send as a\n> response for un-negotiated resources only available in a single form.\n> Sending an response with a known-unacceptable entity is not that good,\n> but sending an error response is hardly better, so in this case the\n> spec intends to allow the server author to freely choose between the\n> two evils.\n> \n> No edit is required in my opinion.\n\nAh.  Now I understand.  Maybe I was just being dense in interpreting it\nto mean the entity sent with the 406, but perhaps section 10.4.7 could\ncapture your explanation above a little more explicitly to help clarify\nit.\n\n \n> >I think the SHOULD needs to stay a SHOULD, but the second \n> part might be\n> >redundant given the existing language in 10.4.7.\n> \n> True, one could see it as a restatement the language in 10.4.7.\n\nGiven that the other references (Accept, sec. 14.1; Accept-Encoding,\nsec. 14.3) do not include that same language, is it better to eliminate\nit from 14.2, OR are there more of these \"special cases\" (where it may\nbe preferable for the server to go ahead and return something\n\"unacceptable\") for Accept-Charset that led to have the alternative\nexplicitly spelled out here?\n\n\n\n", "id": "lists-012-8492167"}, {"subject": "Re: Comments (Part 2) on HTTP I-D Rev 05 (Adams #88, AcceptLanguage", "content": "Jim Gettys:\n>\n>\n>Similar to Adams #84:\n>\n>Glenn Adams notes:\n>> \n>> 88. Section 14.4 does not contain language as found in other Accept-*\n>> headers that recommends a 406 response in the case the server cannot\n>> satisfy the request based on its variant set for the specified URI.\n>> This precludes implementing client-side content negotiation along this\n>> variance axis. Suggest adding the required language or a note\n>> indicating why it is not present and what this means for client-side\n>> negotiation.\n>\n>I believe this is an editorial bug from when 406 was introduced (or\n>later, when some edit was applied).\n>\n>Again, guidance from content negotiation wizards soliticited.\n\nThis is not an editorial bug, it is intentional.  The idea behind it\nis that accept-language states a preference rather than a hard software\nrestriction like the other accept headers.  Sending a 406 in stead of\nsomething which cannot be by software is OK, sending a 406 in stead of\nsomehing not (explicitely) preferred is much less preferable.  This is\nreally a bit of a borderline case, one could also argue for a MAY send\n406 here in the Accept-Language section.\n\nThe absence of language about 406 in 14.4 should not be interpreted as\nblocking the implementation of client-side content negotiation on\nlanguage.  It does mean however that other mechanisms are needed on\ntop of HTTP (like TCN in rfc2295) if the client wants to express that\nsending responses with unacceptable languages is absolutely forbidden.\n\nOn a related note: not only is the wording in Accept-language weaker\nthan that in the other accept headers, note also that the 406 language\nin Accept: and Accept-Encoding: is stronger than that in\nAccept-Charset.  Again this is intentional, and supposed to reflect\nthe probability that a response which is unacceptable according to the\ngiven header can still be handled in some sub-optimal way.\n\nIn summary, HTTP/1.1 gives servers a lot of leeway in dealing with\nsituations where the accept headers cannot really be reconciled with\nthe available content.  This is intentional, and reflects the\ninaccurary of the accept headers sent by historical (and still\ncurrent) client implementations.\n\n>- Jim\n\n\n\n", "id": "lists-012-8503202"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "Jeffrey Mogul:\n>\n[...]\n>(1) In 13.4 change:\n>\n>   Unless specifically constrained by a cache-control (section 14.9)\n>   directive, a caching system MAY always store a successful response\n>   (see section 13.8) as a cache entry, MAY return it without validation\n>   if it is fresh, and MAY return it after successful validation.\n>\n>to:\n>\n>   Unless specifically constrained by a cache-control (section 14.9)\n>   directive, a caching system MAY always store a successful response\n>   (see section 13.8) to a GET or HEAD request\n>   as a cache entry, MAY return it without validation\n>   if it is fresh, and MAY return it after successful validation.\n>   A caching system MUST NOT treat responses to other methods\n>   as cachable (by the definition in section 1.3) unless the\n>   response includes Cache-Control or Expires header fields\n>   implying that the response is cachable.\n\nFor the record: I support this proposed change.  This is exactly what\nwe need.\n\nI don't really understand Henrik's objections, but I have the feeling\nthat they are based on the mistaken assumption that the above addition\nspecifies a top-level requirement which cannot be overridden.  The\naddition actually only specifies what the default is if there is no\nCache-Control header in the response.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-012-8512894"}, {"subject": "Re: Comments (Part 3) on HTTP ID Rev 05 (#'s 116, 134, 135", "content": "Henrik Frystyk Nielsen:\n>\n>At 15:34 11/16/98 -0800, Jim Gettys wrote:\n>\n>>Basically, the q-values are used as binary values. Understanding deflate\n>>with -.65 doesn't make a lot of sense to me.\"\n>\n>Just to clear out any confusion, the dash is not a minus - it's a typo.\n\nI believe the reason for allowing q values like 0.65 here is that it\nallows a client to express a preference for a particular hop-level\ncompression algorithm. E.g.\n\n TE: chunked, compress;q=0.65, gzip;q=0.8\n\nwould mean that gzip compression is preferred over compress-style\ncompression.\n\n>\n>Henrik\n\nKoen.\n\n\n\n", "id": "lists-012-8521449"}, {"subject": "Re: Comments (Part 1) on HTTP ID Rev 05 (ADAMS1", "content": "Glenn sent this to the old mailing list address...  He also needs\na new mailer that doesn't make such a hash out of indenting text...\n\nMy comments in ! below.\n- Jim\n\n\n\nFrom: \"Adams, Glenn\" <gadams@spyglass.com>\nDate: Fri, 13 Nov 1998 14:31:36 -0600\nTo: \"'jg@pa.dec.com'\" <jg@pa.dec.com>\nCc: http-wg@cuckoo.hpl.hp.com\nSubject: RE: Comments (Part 1) on HTTP I-D Rev 05 (ADAMS1)\n-----\n-----Original Message-----\n                From:jg@pa.dec.com [mailto:jg@pa.dec.com]\n                Sent:Friday, November 13, 1998 2:38 PM\n                To:Adams, Glenn\n                Cc:http-wg@cuckoo.hpl.hp.com\n                Subject:Re: Comments (Part 1) on HTTP I-D Rev 05\n(ADAMS1)\n\n\n                > From:Adams, Glenn\n                > Sent:Monday, October 26, 1998 11:13 AM\n                > To:'http-wg@cuckoo.hpl.hp.com'\n                > Subject:Comments (Part 1) on HTTP I-D Rev 05\n                >\n\n                > 4. Section 2.2, pg. 16, definition of \"CTL\", fails to\nconsider that\n                > ASCII (and ISO646-1993) consider SPACE (040) to be a\ncontrol character\n                > of the same status as DEL (177).\n\n                Sorry, no, we handle space differently than CTL, and the\nBNF reflects this.\n\nWhat I'm saying here is that you define CTL as:\n\nCTL = <any US-ASCII control character (octets 0-31) and DEL (127)>\n\nwhile, in fact, \"any US-ASCII control character\" includes octet 32 by\ndefinition\nas well as 0-31 and 127. If you want to exclude SP from this syntactic\ncategory,\nI'd suggest adding \"exluding SP (32)\" to this definition.\n\n! I'm not that familiar with the exact definition of US-ASCII control\n! character.  I think things are clear enough by the (octets 0-31) comment\n! unless you can give me a good pointer to US-ASCII's formal definition.\n!\n\n                >\n                > 6. Section 3.4, pg. 21, specifies that \"the definition\nassociated with\n                > a MIME character set name MUST fully specify the\nmapping ...\". Should\n                > this not be a requirement placed on the registrant of\na MIME character\n                > set and not an HTTP implementation? Or, is this\nrequirement really\n                > stating that any HTTP implementation must maintain a\ntable of registered\n                > character sets known to satisfy this requirement and\nMUST NOT use any\n                > character set not present in this table? Overall, this\nseems an onerous\n                > requirement for an HTTP implementation.\n                >\n\n                I'm not the MIME expert of the working group, but I take\n                this to mean that this is just a restriction on which character sets\nmay be used, and\n                implies there are character sets that do not meet this\nrequirement by\n                having external profiling information.  Maybe a MIME\nexpert can confirm\n                this one.\n\nI'm concerned about whether this use of MUST is stipulating a requirement\nfor an HTTP implementation or the entity using HTTP. If it is a requirement on\nthe implementation, then the implementation would have to maintain internal\nknowledge about which \"character sets\" satisfy this requirement.\n!\n! Could someone with better character set knowledge than I have please comment???\n!\n\n                >\n                > 10. Section 3.7.1, pg. 26, 1st para., states \"An\nentity-body transferred\n                > via HTTP messages MUST be represented in the\nappropriate canonical form\n                > prior to its transmission except for \"text\" types\n...\". Is it actually\n                > the\n                > case that servers are validating canonical status of\nentity bodies? This\n                > contradicts the \"entity-body as payload\" philosophy.\n\n                No, entities are always payload.  The requirement is\nthat you have to\n                play by MIME rules for that data type, but we\nacknowledge the UNIX usage\n                of newline line terminators means that text document\nline terminators\n                don't play by MIME rules.  The Web has worked this way\n(just ship the\n                bits) from day one, and any arguments that it should\nplay by MIME rules\n                for text payload at this date are doomed to failure.\n\nThe problem I have here with this is the use of \"MUST\". Which entity is\nresponsible for\nenforcing \"MUST\"? If it is true that entities are always payload, then\nit would clearly not\nbe a requirement on the server or client. In this case, the requirement\nis on the entity which\nis employing an HTTP implementation and not on the implementation\nitself, in which case\nthis should not be specified as a MUST requirement in this context.\n\n!\n! I see what you are saying; it is really a requirement on the content\n! provider to play by the rules. It is a requirement on the server.\n! On the other hand, I believe we can't\n! be wishy-washy here, that if a payload is typed with a MIME type,\n! we REALLY want that type's canonical form transmitted rather than some\n! random transformation of it.  Otherwise, there is not much use to\n! the type information anyway.  I agree it is unlikely that people will\n! bother to write too many servers that bother to check, but it is\n! quite possible that an authoring tool might do such checks (often,\n! by automatically figuring out the type from magic numbers stored in the\n! file.  Despite the fact that it is probably unenforcable, I think we\n! should stick by our guns and say MUST.\n!\n\n                > 12. Section 3.7.2, pg. 27, 2nd para., states \"In all\nother cases, an\n                > HTTP\n                > user agent SHOULD follow the same or similar behavior\nas a MIME user\n                > agent\n                > would ...\". This \"implied\" behavior needs to be made\nexplicit. What is\n                > the behavior of a MIME user agent in this context?\n\n                I think you should go read the MIME specs to find out;\nHTTP incorporating\n                recommendations for what MIME should do here is a great\nway for specs\n                to end up contradictory.\n\nI'm not suggesting incorporating parts of the MIME specs into this spec;\nrather, that\nthis be more explicit about which parts of the MIME specs are being\nreferenced.\nMIME specifies many behaviors.\n!\n! No, I have no concrete ideas of how things might be improved here, and\n! we've already bowed many times at the MIME altar.  At this date\n! we're not going to spend more time worrying about MIME.\n!\n\n                >\n                > 14. Section 3.8, pg. 28, 1st para., states \"Product\ntokens SHOULD be\n                > short\n                > and to the point.\" and \"They MUST NOT be used for\nadvertising or other\n                > non-essential information.\" As an implementer, how can\none interpret\n                > these\n                > requirements? Either make quantify them or remove\nthem.\n\n                I think common sense is in order here. Keep'em short.\nYour customers will\n                thank you (lower latency, fewer bytes).\n\n                We've seen people put the kitchen sink in them.\n\n                Unless others complain, I plan to keep these as is.\n\nThe problem I have here is that common sense is a relative. Without\nspecifying\nany limits (e.g, no greater than 1024 characters in length), anyone can\ninterpret this\nas they see fit.\n\n!\n! Look, they are shooting themselves in the foot.  We just wanted some\n! way for a user or customer to point to the spec and tell the stupid vendor\n! \"You aren't playing by the rules; fix it\".  Any number you set will just\n! encourage people to do dumb things here.  I believe things are fine the\n! way they are.\n!\n\n\n\n", "id": "lists-012-8530643"}, {"subject": "ADAMS84, ADAMS84b, ADAMS8", "content": "I've seen no consensus for any change in these, in fact, rough consensus\nthat none of them should change.  So I'm closing them out as no action\nneeded.\n\nThis leaves ADAMS31 open, at least unless/until Henrik makes a final plea\n(or gets unconfused).\n- Jim\n\n\n\n", "id": "lists-012-8547437"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "At 17:44 11/16/98 PST, Jeffrey Mogul wrote:\n\n>   Unless specifically constrained by a cache-control (section 14.9)\n>   directive, a caching system MAY always store a successful response\n>   (see section 13.8) to a GET or HEAD request\n>   as a cache entry, MAY return it without validation\n>   if it is fresh, and MAY return it after successful validation.\n>   A caching system MUST NOT treat responses to other methods\n>   as cachable (by the definition in section 1.3) unless the\n>   response includes Cache-Control or Expires header fields\n>   implying that the response is cachable.\n\nWhat is the cache allowed do in the following scenario:\n\nclient A to proxy:\n\nFOO / HTTP/1.1\nhost: some.host\n\nproxy to origin\n\nFOO / HTTP/1.1\nhost: some.host\nvia: ...\n\norigin to proxy\n\nHTTP/1.1 200 OK\nCache-control: max-age=3600\n...\n\nclient B to proxy\n\nBAR / HTTP/1.1\nhost: some.host\n\nCan it return the cached response? Do we need a vary on the method name?\nThe definition in 1.3 doesn't indicate this.\n\nRegardless, do you agree that the wording conflicts with 13.11:\n\nAll methods that might be expected to cause modifications to the origin\nserver's resources MUST be written through to the origin server. This\ncurrently includes all methods except for GET and HEAD. A cache MUST NOT\nreply to such a request from a client before having transmitted the request\nto the inbound server, and having received a corresponding response from\nthe inbound server. This does not prevent a proxy cache from sending a 100\n(Continue) response before the inbound server has sent its final reply.\n\nand also with 5.1.1\n\nServers SHOULD return the status code 405 (Method Not Allowed) if the\nmethod is known by the server but not allowed for the requested resource,\nand 501 (Not Implemented) if the method is unrecognized or not implemented\nby the server.\n\nwhere it in 1.3 it is stated how a server at any time can become a tunnel\n\nServer\n\n... Likewise, any server may act as an origin server, proxy, gateway, or\ntunnel, switching behavior based on the nature of each request....\n\nin which case it can't be a cache:\n\nCache\n\n...Any client or server may include a cache, \n\nAnyway, the reason why I don't think this is a good idea is that *if* a\nresponse is fully cachable regardless of whether the method is understood\nor not, then it smell, feels, and looks like a GET request. There is\nabsolutely no reason to have several GET-alike methods. GET is special\nbecause it is special because it is special.\n\nNote, that this has nothing to do with your cache control extensibility\nwhich I like (and know about) - it just shouldn't be applied to responses\nfrom operations by unknown methods.\n\nA confused? Henrik\n\n>Henrik, please read section 14.9.6 (Cache Control Extensions).\n>It already handles the problem you are describing.  For example,\n>an extension can define the meaning of a new cache-control\n>directive (let's call it \"henrik-mode\"), and send:\n>\n>Cache-control: max-age=0, must-revalidate, henrik-mode=999\n>\n>Any cache that doesn't understand your extension must ignore\n>\"henrik-mode\" and so will do the necessary revalidation.  If\n>the cache does understand your extension, 14.9.6 allows it\n>to use the \"henrik-mode\" directive to modify the behavior\n>of the other two (\"modify\" may mean \"ignore\").\n>\n>    I do not believe we at this point can change whether we consider\n>    caching or the method to be the highest in the hierarchy.\n>\n>I don't think at this point we should be proposing changes to\n>the HTTP/1.1 protocol without first reading the existing\n>specification :-)\n>\n>-Jeff\n>\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-8553854"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "Henrik writes:\n    At 17:44 11/16/98 PST, Jeffrey Mogul wrote:\n    \n    >   Unless specifically constrained by a cache-control (section 14.9)\n    >   directive, a caching system MAY always store a successful response\n    >   (see section 13.8) to a GET or HEAD request\n    >   as a cache entry, MAY return it without validation\n    >   if it is fresh, and MAY return it after successful validation.\n    >   A caching system MUST NOT treat responses to other methods\n    >   as cachable (by the definition in section 1.3) unless the\n    >   response includes Cache-Control or Expires header fields\n    >   implying that the response is cachable.\n    \n    What is the cache allowed do in the following scenario:\n    \n    client A to proxy:\n    \n    FOO / HTTP/1.1\n    host: some.host\n    \n    proxy to origin\n    \n    FOO / HTTP/1.1\n    host: some.host\n    via: ...\n    \n    origin to proxy\n    \n    HTTP/1.1 200 OK\n    Cache-control: max-age=3600\n    ...\n    \n    client B to proxy\n    \n    BAR / HTTP/1.1\n    host: some.host\n    \n    Can it return the cached response? Do we need a vary on the method\n    name?  The definition in 1.3 doesn't indicate this.\n    \nFirst of all, the *definition* of \"cachable\" in 1.3 has nothing\nto do with the *specification* of what the proxy is supposed to\ndo.  So I'll ignore that part of your question, rather than trying\nto guess what you really meant.\n\nSecond, I see what you mean about the need to match the method\nname, since otherwise I think we're in dangerously undefined\nterritory.  So I guess that sentence I added should become\n\n       A caching system MUST NOT treat responses to other methods\n       as cachable (by the definition in section 1.3) unless the\n       response includes Cache-Control or Expires header fields\n       implying that the response is cachable, and the subsequent\n       request uses the same method as the request that generated\n       the response.\n\nNote that the preceding sentence basically includes the usual\nexception to the method-matching rule (GET and HEAD \"match\")\nwith the implicit understanding that a cache can't take a HEAD\nresponse and give it back for a new GET request.  Do we need\nto make this explicit, too?\n\n    Regardless, do you agree that the wording conflicts with 13.11:\n    \n    All methods that might be expected to cause modifications to the\n    origin server's resources MUST be written through to the origin\n    server. This currently includes all methods except for GET and\n    HEAD. A cache MUST NOT reply to such a request from a client before\n    having transmitted the request to the inbound server, and having\n    received a corresponding response from the inbound server. This\n    does not prevent a proxy cache from sending a 100 (Continue)\n    response before the inbound server has sent its final reply.\n    \nWell, I could try to wriggle out of this one, but I admit that this is\na little flakey.  After all, the MUST applies with the precondition\n\"that might be expected to cause modifications\", and perhaps it's\nreasonable that if the cached response with the same method has\na non-zero max-age value, then the method shouldn't be expected\nto cause modifications.  But the second sentence does contradict this.\n\nSo I would change this to:\n\n    All requests that might be expected to cause modifications to the\n    origin server's resources MUST be written through to the origin\n    server. This currently includes all methods other than GET and\n    HEAD, unless the request (including its method) matches a cached\n    response that includes Cache-Control or Expires header fields\n    implying that the response is cachable.\n    A cache MUST NOT reply to such a request from a client before\n    having transmitted the request to the inbound server, and having\n    received a corresponding response from the inbound server. This\n    does not prevent a proxy cache from sending a 100 (Continue)\n    response before the inbound server has sent its final reply.\n\nI can guess that Jim is probably getting itchy.\n\n    and also with 5.1.1\n\n    Servers SHOULD return the status code 405 (Method Not Allowed) if\n    the method is known by the server but not allowed for the requested\n    resource, and 501 (Not Implemented) if the method is unrecognized\n    or not implemented by the server.\n\n    where it in 1.3 it is stated how a server at any time can become a\n    tunnel\n\n    Server\n\n    ... Likewise, any server may act as an origin server, proxy,\n    gateway, or tunnel, switching behavior based on the nature \n    of each request....\n\n    in which case it can't be a cache:\n\n    Cache\n\n    ...Any client or server may include a cache,\n\nIf you interpret SHOULD as MUST.  I don't.  Since I wasn't involved\nin writing 5.1.1, I'm not sure whether it really meant to say\nMUST (or if the word \"server\" here really includes \"proxy\").\n    \n    Anyway, the reason why I don't think this is a good idea is that\n    *if* a response is fully cachable regardless of whether the method\n    is understood or not, then it smell, feels, and looks like a GET\n    request. There is absolutely no reason to have several GET-alike\n    methods. GET is special because it is special because it is\n    special.\n\nI think the point is that if an origin server wants to make the\nresponse to a FOO cachable, why should we try to forbid it?\nAs long as we do it in a way that can't be misconstrued.  If\nyour origin server doesn't want a proxy to cache a response\nto a FOO method, then why on earth is it sending \"max-age=3600\"?\nJust leave that out, and nobody gets hurt.\n\nBut if this is all too much for a last-minute change, then I\npropose resolving ADAMS31 by not changing anything (relative\nto -rev-05).  I started down this path by complaining about\nJim's proposed change in\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q4/0142.html\nand trying to put his words in the right part of the document.\nI suspect we will survive if we just leave this part of -rev-05 alone.\n\n-Jeff\n\n\n\n", "id": "lists-012-8565492"}, {"subject": "Re: Comments (Part 3) on HTTP ID Rev 0", "content": "OK, I managed to wade through all the nits...  Here are my\nresolutions.\n- Jim\n\n\n> From: \"Adams, Glenn\" <gadams@spyglass.com>\n> Resent-From: Andy Norman <ange@hplb.hpl.hp.com>\n> Date: Fri, 13 Nov 1998 22:40:29 GMT\n> To: \"'Getty, James'\" <jg@pa.dec.com>,\n>         \"'Masinter, Larry'\" <masinter@parc.xerox.com>\n> Cc: \"'WG - HTTP'\" <http-wg@hplb.hpl.hp.com>\n> Subject: Comments (Part 3) on HTTP I-D Rev 05\n> -----\n> Following is my third set of comments, covering sections 14.9.4 through\n> 14.40. Part\n> four will follow presently.\n> \n> 102. Section 14.9.4, pg. 103, 3rd para., has an editors note \"[jg418]\"\n> which should be removed.\n\nI'll fix.  More Word droppings.\n\n> \n> 103. Section 14.9.4, pg. 103, 4th para., would read more clearly if\n> the last sentence were further elaborated, e.g., \"The initial request\n> (from user agent to first proxy) includes cache-validating\n> conditional(s) in the request, based on the validator(s) of the local\n> cached copy.\"\n\nI don't think this improves things.  It implies a cache when there\nmay just be the one document.\n\n> \n> 104. Section 14.9.4, pg. 103, 5th para., would read more clearly if\n> the last sentence were further elaborated, e.g., \"The initial request\n> (from user agent to first proxy) does not include a cache-validating\n> conditional; rather, the first caching proxy in the request path that\n> holds a cache entry for this resource includes cache-validating\n> conditional(s) in the request, based on the validator(s) of its cache\n> entry.\"\n> \n\nI don't think this improves things.  It implies a cache when there\nmay just be the one document.\n\n> 105. Section 14.9.4, pg. 104, 2nd para., suggest changing \"I.e., the\n> cache MUST obey ...\" to \"The cache MUST obey ...\". Imperatives\n> shouldn't be stated as parenthetical explanations.\n\nI can't find this one.\n\n> \n> 106. Section 14.10, pg. 106, 1st para., suggest changing \"MUST NOT be\n> communicated by proxies over futher connections.\" to \"MUST NOT be\n> forwarded by proxies.\"\n\nI think I like the current wording better, as it really applies\nto the TCP connection...  But maybe I'm just getting tired.\n\n> \n> 107. Section 14.10, pg. 106, 2nd para., the production used with\n> Connection appears to be overly general as compared to the explanation\n> in this section.  I would suggest rewriting as follows:\n> \n> Connection = \"Connection\" \":\" 1#connection-directive\n> \n> connection-directive = \"close\" | field-name\n> \n> While it is the case that both \"close\" and field-name can be expressed\n> as token, the semantics implied by field-name corresponds better to\n> the required usage, and, further, implies the additional semantics of\n> section 14 regarding field names defined by this specification.\n> \n> The above expression also does a better job of discriminating the\n> special directive \"close\" from the other possible values for this\n> field (i.e., field-names).\n> \n> If I were designing this anew, I would have modeled this similarly to\n> the expression used with the no-cache cache-response-directive:\n> \n> connection-directive = \"close \" | \"no-forward\" \"=\" <\"> 1#field-name <\">\n> \n> Any chance for revising this into a more consistent form?\n\nNo.  There were other tokens defined in previous drafts, and there\nneeds to be other tokens defined for connection management,\nso leaving things as is with a open-ended token is the right\nthing to do.\n\n> \n> 108. Section 14.11, pg. 107, 1st para., has \"MUST be\" in relative\n> clause \"... and thus what decoding mechanisms MUST be ...\".\n\nyes, MUST shouldn't be capitalized\n\n> \n> 109. Section 14.14, pg. 109, 2nd para., refers to \"base URI\". Is this\n> concept defined in this specification? Perhaps a reference to another\n> document defining this would be useful.\n> \n\nSee 3.2.1, where it is already cross referenced to the URI spec.\n\n> 110. Section 14.14, pg. 109, 6th para., has \"The meaning of the\n> Content-Location header in PUT or POST requests is undefined; servers\n> are free to ignore it in those cases.\" Suggest changing \"servers are\n> free to ignore\" to \"servers MAY ignore\". Also, how about other methods\n> such as DELETE, etc.\n> \n\nWe are silent on DELETE for good reason; it might be useful to\nhave a Content-Location on a DELETE, if the semantics were ever defined.\nSame goes for PUT right now.\n\n> 111. Section 14.15, pg. 110, 1st para., suggest changing \"may\n> generate\" to \"MAY generate\".\n\nSure.\n\n> \n> 112. Section 14.15, pg. 110, 1st para., has \"Any recipient ... MAY\n> check that the digest value ... matches ...\". Is there a\n> recommendation for the case where it does not match? Or is this\n> implementation specific behavior?\n\nIt isn't implementation specific, it is up to the application. You remember \nthese pesky applications that people build, don't you?  The reason for \nContent-MD5 is to get a strong end to end checksum of an entity.  Presumably \nthe application had some need to get this integrity assurance; often this \nis not a browser.\n\n> \n> 113. Section 14.15, pg. 110, it seems that the material discussing an\n> extension to RFC 1864 (4th through 7th paragraphs) would best be moved\n> to an appendix. I also found this material difficult to follow as\n> presently worded.\n\nNot going to be moved at this date.\n\nYes, MIME uglyness is rearing its ugly head again.  We spend lots\nof time explaining how HTTP != MIME.  Sgih...\n\n> \n> 114. Section 14.16, pg. 111, 1st para., suggest changing \"It SHOULD\n> indicate the total length ...\" to \"It SHOULD indicate by means of an\n> 'instance-length' the total length ...\". It may also read better if\n> this is placed after the syntactical description of Content-Range;\n> i.e., before the paragraph starting \"The asterisk ...\".\n\nYup.\n\n> \n> 115. Section 14.16, pg. 111, 3rd para., suggest adding \"(see section\n> 14.35.1)\" after \"Unlike byte-range-specifier values ...\".\n\nGood suggestion.\n\n> 116.\n\nHandled in other mail.\n\n> 117. Section 14.16, pg. 112, 2nd para., needs a space in \"19.6.3for\".\n\nI'll fix.\n\n> \n> 118. Section 14.18, pg. 113, 5th para., has \"Clients SHOULD only send\n> a Date header field in messages that include an entity body ...\" which\n> would read better as a prohibition: \"Client SHOULD NOT send a Date\n> header field in messages that do not include an entity body ...\".\n\nI prefer the positive statement.\n\n> \n> 119. Section 14.20, pg. 114, would seem better to merge the last\n> sentence of the 1st paragraph (\"A server that does not ...\") with the\n> 2nd para.  (\"The server MUST respond ...\"), since they appear to\n> duplicate the requirement about responding with an error status.\n> \n\nOK.  Will do.\n\n> 120. Section 14.20.1, pg. 115, appears to duplicate language from\n> 8.2.4.\n\nYes, dealt with already in ROSS14.\n\n> \n> 121. Section 14.21, pg. 116, 4th para.: suggest adding \"(see section\n> 14.9.3)\" after \"Note: if a response ...\".\n\nOK.\n\n> \n> 122. Section 14.22, pg. 116, 1st para.: suggest removing parentheses\n> around \"as updated by RFC1123 [8]\" to make it clear that this update\n> isn't optional.\n> \n\nOK.\n\n> 123. Section 14.22, pg. 117, 2nd para., suggest changing \"issuer's\n> address\" to \"requester's address\".\n\nNo; that would imply the system that made the request, and this is\nspecifically about allowing a different address.\n\n> \n> 124. Section 14.22, pg. 117, 3rd para., has \"SHOULD NOT\" in a note.\n\nOK. Fixed.\n\n> \n> 125. Section 14.24, pg. 118, 6th para., suggest rewriting to not use\n> \"SHOULD\" and \"MUST NOT\" in the context of defining a term \"If-Match:\"\n> -- or use \"SHOULD\" and \"MUST NOT\" without placing in a definition.\n\nNo.  Not worth it.\n\n> \n> 126. Section 14.24, pg. 118, last para., uses \"MUST NOT\" in a relative\n> clause \"to signal that ...\"; suggest rewriting to make into an\n> imperative related explicitly to server.\n\nNo.  Not worth it.\n\n> \n> 127. Section 14.24, pg. 119, 1st para.: I find the fact that this\n> situation is defined as undefined to be rather troubling. Can't a\n> specific 4XX response (e.g., 400) be recommended instead? It has been\n> stated that broken behavior should be strongly discouraged, rather\n> than merely ignored or arbitrarily interpreted.\n> \n\nWe discussed this at length before, and decided leaving it undefined\nwas the right solution.  We can't define all outcomes of permutations\nof headers in HTTP (a fundamental failing of HTTP is this mess of\nheaders that may or may not apply to a given method), and decided\nthat it was best in this case to make sure no one relied on the\nbehavior so that we would be able to define it in the future.\n\nSomeone (or you) can find this discussion in the mailing list archives.\n\n> 128. Section 14.25, pg. 119, 1st para., should \"an entity will not be\n> returned from the server\" be strengthened to \"A server MUST NOT return\n> an entity ...\"?\n\nI think it is written this way to allow servers that more or less\nalways blindly return an entity, and don't implement If-Modified-Since.\nSo it can't be normative.\n\n> \n> 129. Section 14.25, pg. 119, 5th and 6th paras., starting \"Note that\n> the Range ...\" and \"Note that the If-Modified-Since ...\",\n> respectively, should be either indented at the same level as other\n> paragraphs (e.g., see 1st para.  of pg. 120) or use standard \"Note:\n> ...\" form.\n\nOk.\n\n> \n> 130. Section 14.25, pg. 120, 1st para.: is this normative text, in\n> which case \"The client should ...\" should be changed to \"The client\n> SHOULD ...\", or informative text, in which case the standard \"Note:\n> ...\" form should be used?\n\nInformative.  I've indented the paragraph.\n\n> \n> 131. Section 14.26, pg. 121, 3rd para., suggest rewriting to not use\n> \"MUST NOT\" in the context of defining a term \"If-None-Match:\" -- or\n> use \"MUST NOT\" without placing in a definition.\n\nI can't get excited about this suggestion.\n\n> \n> 132. Section 14.27, pg. 121, 1st para., suggest changing \"it could use\n> the Range ...\" to \"it MAY use the range ...\".\n\nNo, this is a possibility, not a concrete situation.\n\n> \n> 133. Section 14.27, pg. 122, 1st para., uses the term \"sub-range\n> operation\".  This term doesn't seem to be well defined elsewhere in\n> this specification.\n> \n\nNo, it is pretty well defined.  Remember, some implementations\nmay ask for a bunch of ranges at once (if they support multi-part).\n\n\n> 134. Section 14.28, pg. 122, 1st para., why have \"... the server\n> SHOULD perform the requested operation as if the If-Unmodified-Since\n> header were not present.\" while section 14.24, 2nd para., has \"... the\n> server MAY perform the requested method if the If-Match header field\n> did not exist.\"? Suggest using consistent language in these sections\n> \"SHOULD/MAY\", \"operation/method\", \"were not present/did not exist\".\n\nHandled in separate message.\n\n> \n> 135. Section 14.28, pg. 122, 2nd para., should note the asymmetry in\n> response codes with respect to \"If-Modified-Since\" (section 14.25).\n> Here we have 412 (Precondition Failed) while in 14.25 we have 304 (Not\n> Modified). This asymmetry is odd since If-Match and If-Not-Match both\n> use 412.\n> \n\nHandled in separate message.\n\n> 136. Section 14.29, pg. 123, 4th para., suggest removing \"whenever\n> feasible\" since this weakens this as an unconditionally compliant\n> requirement.\n> \n\nWeakened deliberately for clock-less servers.\n\n> 137. Section 14.35.2, pg. 127, 2nd para. has \"origin servers and\n> intermediate caches ought to support byte ranges when possible\" seems\n> to be a quasi-requirement but doesn't use standard keywords. Should\n> this be \"SHOULD support byte ranges\" (recommended) or \"MAY support\n> byte ranges\" (optional)?\n\nNo, we are making a recommendation here; we believe it is worth it\nfor most implementations to implement ranges, but can't require it.  Implying\nthat it is entirely optional by a bare MAY doesn't get the intent across.\n\n> \n> 138. Section 14.35.2, pg. 127, 3rd para., 2nd bullet, has \"It does not\n> affect the 304 (Not Modified) response returned if the conditional is\n> false.\" However, a 412 (Precondition Failed) response may apply as\n> well.\n\nHandled in separate message.\n\n> \n> 139. Section 14.35.2, pg. 127, 5th para., suggest removing \",\" in \"...\n> in its cache, if that is ...\".\n\nOK.\n\n> \n> 140. Section 14.36, pg. 127, rather than just using \"[sic]\" to excuse\n> the misspelling \"Referer\", suggest adding a note indicating that this\n> is a historical error maintained for compatibility sake.\n\nNot worth the space.  In any case, this is an intellegence test,\nto see if you know what [sic] means...\n\n> \n> 141. Section 14.36, pg. 128, 3rd para., what does \"partial URI\" mean\n> in \"If the field value is a partial URI, it SHOULD be ...\" Is this\n> usage defined somewhere?\n\nNo, it should be \"relative URI\".\n\n> \n> 142. Section 14.37, pg. 128, has \"This field MAY also be used with any\n> 3xx (Redirection) response to indicate the minimum time the user-agent\n> is asked to wait before issuing the redirected request.\" Should there\n> be a concurrent requirement placed on the user-agent (or client),\n> e.g., \"The user-agent SHOULD NOT issue a redirected request before\n> ...\"?\n\nHard to enforce, with impatient users at the drivers seat...  I can't\nsee that we can be stronger here; as a browser implementer, I don't\nthink you'd want to try to implement a SHOULD NOT in this case,\nnow would you?\n\n> \n> 143. Section 14.39, pg. 129, 1st para., has \"in section 3.9\" which\n> should read \"in section 3.6\".\n\nOK. \n\n> \n> 144. Section 14.39, isn't t-codings syntactically ambiguous since:\n> \n> transfer-extension     = token *( \";\" parameter )\n> parameter              = attribute \"=\" value\n> \n> and\n> \n> accept-params          = \";\" \"q\" \"=\" qvalue *( accept-extension )\n> \n> Perhaps it should read:\n> \n> t-codings              = \"trailers\" | transfer-extension\n> \n> and then add language indicating that accept parameters are included\n> by means of the general transfer-extension parameter.\n\nDon't think so, but I'm not the best BNF wizard on the crew.\n\n> \n> 145. Section 14.39, pg. 129, 4th para., has \"Therefore the keyword\n> MUST be ...\": what does \"keyword\" mean in this context? Shouldn't this\n> read \"connection-token\" (or, as I suggested in my point 107,\n> \"field-name\").\n\nKeyword == \"trailers\" per the previous sentence.\n\n> \n> 146. Section 14.39, pg. 129, 5th para., item 1, suggest changing to\n> start \"The presence of a TE header indicates that the \"chunked\"\n> transfer- coding is acceptable.\"\n\nCan't find this one.\n\n> \n> 147. Section 14.39, pg. 130, item 3, if an implied \"chunked\" always\n> gets qvalue of 1, then it will always win over lesser qvalues.\n> Specifying \"chunked;q=0\" would permit overriding this default;\n> however, the present syntax does not admit \"chunked\" (unless the\n> syntax of t-codings is changed to:\n> \n> t-codings = \"trailers\" | transfer-coding\n\nHandled in separate message.\n\n> \n> 148. Section 14.40, pg. 130, 2nd para, has \"An HTTP/1.1 sender SHOULD\n> ...\": suggest changing to \"An HTTP/1.1 message SHOULD include a\n> Trailer header field if a chunked transfer-coding is used with a\n> non-empty trailer.\"\n\nOK.\n\n> \n> 149. Section 14.40, pg. 130, 4th para., has \"A server MUST NOT include\n> any header fields unless ...\". I believe this should read \"A server\n> MUST NOT include any header fields in the trailer of a message unless\n> ...\".\n\nHandled in separate message.\n\n> \n> 150. Section 14.40 uses language that implies \"Trailer\" is only used\n> in response messages; however, Trailer is specified in section 4.5 as\n> a general header field. Under what circumstance(s) would Trailer be\n> used in a request message? Should the language of 14.40 take this into\n> account?\n\nYou might use it in a PUT, for example, to add a signature.  It is correct\nas is.\n\n\n\n", "id": "lists-012-8578839"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "I'm fine with Jeff's updated wording, though I think worrying about\nthe cached entry corresponding to the new request method should be\npart of the definition of cachable rather than having to repeat the\nconditional in several places.  In any case, these are just clarifying\nwhat the implementer should be able to figure out on their own.\n\nWhat does bother me is\n\n>and also with 5.1.1\n>\n>Servers SHOULD return the status code 405 (Method Not Allowed) if the\n>method is known by the server but not allowed for the requested resource,\n>and 501 (Not Implemented) if the method is unrecognized or not implemented\n>by the server.\n\nwhich is a bug -- it should say\n\n   An origin server SHOULD return the status code 405 (Method Not Allowed)\n   if the method is known by the origin server but not allowed for the\n   requested resource, and 501 (Not Implemented) if the method is unrecognized\n   or not implemented by the origin server.\n\n....Roy\n\n\n\n", "id": "lists-012-8605082"}, {"subject": "Re: ADAMS1, point 31. (cachability of methods)", "content": "Jeff says:\n> \n> But if this is all too much for a last-minute change, then I\n> propose resolving ADAMS31 by not changing anything (relative\n> to -rev-05).  I started down this path by complaining about\n> Jim's proposed change in\n>    http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q4/0142.html\n> and trying to put his words in the right part of the document.\n> I suspect we will survive if we just leave this part of -rev-05 alone.\n> \n> -Jeff\n\nOK, we'll keep it as is.\n\n\n\n", "id": "lists-012-8613645"}, {"subject": "RE: Adams, #84 (urgent, as I need to get a draft out)", "content": "What we wrote means just what he thinks it does, and we meant it that\nway.\n\n\n> > 84. Section 14.2, pg. 93, 3rd para., is quite confusing: suggest\n> > rewriting without using the term \"mentioned\". Also, this para. seems to\n> > be stating that if any \"iso-8859-1;q=1\" is always implied unless\n> > otherwise explicitly present. This means that:\n> > \n> >     Accept-Charset: iso-8859-5, unicode-1-1;q=0.9\n> > \n> > really means\n> > \n> >     Accept-Charset: iso-8859-1;q=1, iso-8859-5;q=1, unicode-1-1;q=0.9\n> > \n> > (in which case 8859-1 would be given equal billing with 8859-5). And\n> > that consequently the only way to exclude 8859-1 is to specify\n> > \n> >     Accept-Charset: iso-8859-1;q=0, iso-8859-5, unicode-1-1;q=0.9\n> > \n> > Is this the intended usage? If so, I find this not only convoluted but\n> > seriously sub-optimal. This emphasis on 8859-1 as default really is too\n> > much. Why go so far overboard?\n\n\n\n", "id": "lists-012-8621548"}, {"subject": "RE: Comments (Part 2) on HTTP I-D Rev 05 (Adams #84, AcceptCharset", "content": "> 84. Section 14.2, pg. 93, 4th para., has \"the server SHOULD send an\n> error response with the 406 (not acceptable) status code, though the\n> sending of an unacceptable response is also allowed.\" The effect of the\n> final clause of this statement is to downgrade SHOULD to MAY.\n\n\"You SHOULD do A, but if you don't, you MAY do B.\"\n\ndoes not downgrade the first SHOULD.\n\nLarry\n\n\n\n", "id": "lists-012-8629815"}, {"subject": "RE: Warning and I18N question (Adams #60)", "content": "The purpose of the language identification in warning strings is not\nfor allowing the recipient to choose the appropriate one, but rather\nmerely to allow it to interpret the string correctly, if the display\naction might involve, for example, reading the string out loud.\n\n> > I'd suggest permitting the extensions specified by\n> > RFC2231 (which updates RFC2047) to be used to provide\n> > explicit language tagging of quoted strings.\n\nThere is no harm in updating the reference, but there's should\nbe no presumption that language tags are necessarily any more\nuseful for selection among appropriate warnings.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-8638517"}, {"subject": "Re: Preparation of draft 06 (probably the FINAL draft) has begun..", "content": "jg@pa.dec.com (Jim Gettys) writes:\n\n>Before it isn't too late (i.e. today), please look over the responses\n>I (and some others) have made to the issues raised since IETF last call.\n\nI'm very satisfied with the state of affairs as reflected in the issues\nlist today (Wednesday - sorry Jim!).  Nice work, as usual.\n\nThere is one question I'm aware of (because I raised it ;-) ) that\ndoesn't appear on the issues list and hasn't provoked any traffic on\nthis list.  It has to do with the general syntax of headers and a change\nbetween HTTP 1.0 and 1.1 that appears accidental.  See\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q4/0019.html for\nthe original message.\n\nIt's not by any means a show-stopper, but if it was accidentally missed\nI'd like to see it addressed.  If it was intentionally omitted, that's\nnot going to give me any heartburn.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-8646894"}, {"subject": "Re: Preparation of draft 06 (probably the FINAL draft) has begun..", "content": "ID draft deadline was at 5:00PM.  I made it...  To late....\n\nI'll issue an announcement of \"come and get it\" as soon as the files\nget installed in the Web.  Probably in the next day or two.\n- Jim\n\n\n\n", "id": "lists-012-8655785"}, {"subject": "HTTP/1.1 Rev 06 available...", "content": "I haven't seen an announcement yet, but I sent in Rev06 at about 1:30\nyesterday afternoon (the deadline was 5:00 yesterday, the 18th, for\nOrlando).  As there is a generally a flurry of final drafts, it often\ntakes a few days for the IETF secretariat to dig itself out.\n\nAs always, it is available in may forms off of the issues list page:\nhttp://www.w3.org/Protocols/HTTP/Issues/.\n\nNote that this draft deals with all issues raised during and since IETF \nlast call on the HTTP/1.1 specification for Draft Standard.  Please take \na final look at this draft to catch any mistakes before any IESG action (it \nis embarrassing to all concerned to find them only after the RFC issues).\nI don't know when the next IESG meeting is, but if my memory serves me,\nthere is a 2 week minimum from the issuance of a draft and any possible\nIESG action on that draft.\n\nIt is, of course, possible that the IESG could direct another draft, so\nit ain't over until the fat lady sings, as they say.\n\nIn any case, thank you all for your help.  Keep your fingers crossed\nthat this is really the last draft for draft standard, but as I said,\nI'd much prefer any mistakes to be caught before any RFC issues.\n\n\n- Jim Gettys\nHTTP/1.1 editor\n\nBTW, after the RFC issues, I'd really like to bother to get a good\n(compliant) version of the spec available in HTML.  Microsoft Word 97\nstill generates really terrible HTML; if anyone knows of a *good* path\nto get there while preserving cross references, hyper-links, table\nof contents, and the index, and something resembling HTML 4.0,\nI'd like to hear from them. \n\n\n\n", "id": "lists-012-8663289"}, {"subject": "Re: HTTP/1.1 Rev 06 available...", "content": "Jim Gettys wrote:\n\n> BTW, after the RFC issues, I'd really like to bother to get a good\n> (compliant) version of the spec available in HTML.  Microsoft Word 97\n> still generates really terrible HTML; if anyone knows of a *good* path\n> to get there while preserving cross references, hyper-links, table\n> of contents, and the index, and something resembling HTML 4.0,\n> I'd like to hear from them. \n\nI didn't try it, since I don't use MS products and have no need for this\nkind of conversion. A friend of mine has a good opinion about HTML Transit\nfrom InfoAccess <URL:http://www.infoaccess.com>. There is a free trial\nversion available (Windows only :-|). Let me know if it works good.\n\n-- \n .-.   .-.    Life is a sexually transmitted disease.\n(_  \\ /  _)\n     |        dave@fly.cc.fer.hr\n     |\n\n\n\n", "id": "lists-012-8672123"}, {"subject": "authentication issue", "content": "draft-ietf-http-authentication-03 does not specify whether the\n\"algorithm\" and \"qop\" attributes are case-sensitive or case-insensitive.\nI presume they are both meant to be case-insensitive (which is also\nsupported by the code in section 5) and therefore suggest the following\nchanges:\n\nSection 3.2.1, change \n\n  algorithm\n    A string indicating a pair of algorithms used to produce the digest\n    and a checksum. If this is not present it is assumed to be \"MD5\". If\n    the algorithm is not understood, the challenge should be ignored (and\n    a different one used, if there is more than one).\n\nto\n\n  algorithm\n    A string (case-insensitive) indicating a pair of algorithms used to\n    produce the digest and a checksum. If this is not present it is assumed\n    to be \"MD5\". If the algorithm is not understood, the challenge should be\n    ignored (and a different one used, if there is more than one).\n\nand change\n\n  qop-options\n    This directive is optional, but is made so only for backward\n    compatibility with RFC 2069 [6]; it SHOULD be used by all\n    implementations compliant with this version of the Digest scheme.\n    If present, it is a quoted string of one or more tokens indicating\n    the \"quality of protection\" values supported by the server.  The\n    value \"auth\" indicates authentication; the value \"auth-int\" indicates\n    authentication with integrity protection; see the descriptions below\n    for calculating the response directive value for the application of\n    this choice. Unrecognized options MUST be ignored.\n\nto \n\n  qop-options\n    This directive is optional, but is made so only for backward\n    compatibility with RFC 2069 [6]; it SHOULD be used by all\n    implementations compliant with this version of the Digest scheme.\n    If present, it is a quoted string of one or more case-insensitive\n    tokens indicating the \"quality of protection\" values supported by\n    the server.  The value \"auth\" indicates authentication; the value\n    \"auth-int\" indicates authentication with integrity protection; see\n    the descriptions below for calculating the response directive value\n    for the application of this choice. Unrecognized options MUST be\n    ignored.\n\nI believe the case-sensitivity of the other attributes can be safely\nimplied by virtue of them defined elsewhere (URIs) or by being opaque to\nthe client (nonce, nextnonce, and opaque).\n\nFinally, I have some nits with the code in Section 5:\n\n  void main(int argc, char ** argv) {\n\nis invalid ansi C, and should be\n\n  int main(int argc, char ** argv) {\n\n(and a \"return 0;\" should probably also be added to the end too). In\nCvtHex()\n\n    Hex[i*2] = (j + 'a' - 10);\n\nis not guaranteed to work correctly (i.e. ('b' == 'a'+1) need not be\ntrue). Under both ASCII and EBCDIC this will be correct, however. A\nnicer (IMHO) and more portable solution would be to use something like\nthe following:\n\nconst char *HexVal = \"0123456789abcdef\";\n...\n    for (i = 0; i < HASHLEN; i++) {\nHex[i*2]   = HexVal[(Bin[i] >> 4) & 0xf];\nHex[i*2+1] = HexVal[Bin[i] & 0xf];\n    }\n\nAnd lastly, since stricmp() is non-standard (neither ANSI C nor POSIX) it\nmight be worth mentioning that it is supposed to be case-insensitive\nversion of strcmp() .\n\n\n  Cheers,\n\n  Ronald\n\n\n\n", "id": "lists-012-8679999"}, {"subject": "KeepAlive", "content": "I have some questioins about Netscape 4.05 and its implementation.\n\nCondition: Fecth Web page directly from server.\n\nWhen I load a Web page, consisting of 7 GIFs and a source file, the Netscape\nopen as many TCP connections as GIFs.\n\nIf I add more GIFs in the Web page, it open only four TCP connections.\n\ncould anybody give me an explaination about it?\n\n\nThank you\n\nDabin\n\n\n\n", "id": "lists-012-8689382"}, {"subject": "[Fwd: I-D ACTION:draft-iesg-http-cookies00.txt", "content": "attached mail follows:\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the IESG.\n\nTitle: Applicability Statement for HTTP State Management\nAuthor(s): K. Moore\nFilename: draft-iesg-http-cookies-00.txt\nPages: 8\nDate: 23-Nov-98\n\nThe mechanisms described in 'HTTP State Management Mechanism' [RFC-\nXXXX]  and  its  predecessor  [RFC-2109], can be used for many different\npurposes.  Even though this protocol has been approved for the  Internet\nstandards track, some current and potential uses of the protocol are not\nwithin the scope of the standard approved by IESG.  This memo identifies\nspecific  uses  of  HTTP  State Management protocol which are either (a)\nnonstandard and thus  not  recommended  by  IETF,  or  (b)  nonstandard,\nbelieved  to  be  harmful,  and  discouraged.   This  memo  also details\nadditional privacy considerations which are  not  covered  by  the  HTTP\nState Management protocol specification.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-iesg-http-cookies-00.txt\".\nA URL for the Internet-Draft is:\nhttp://www.ietf.org/internet-drafts/draft-iesg-http-cookies-00.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nic.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-iesg-http-cookies-00.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-8696309"}, {"subject": "I-D ACTION:draft-ietf-http-v11-spec-rev06.txt,.p", "content": "Note: This revision reflects comments received during the last call period.\n\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: Hypertext Transfer Protocol -- HTTP/1.1\nAuthor(s): R. Fielding, J. Gettys, J. Mogul, \n                          H. Nielsen, L. Masinter, P. Leach, T. Berners-Lee\nFilename: draft-ietf-http-v11-spec-rev-06.txt,.ps\nPages: 166\nDate: 24-Nov-98\n\n   The Hypertext Transfer Protocol (HTTP) is an application-level\n   protocol for distributed, collaborative, hypermedia information\n   systems. It is a generic, stateless, protocol which can be used for^M\n   many tasks beyond its use for hypertext, such as name servers and\n   distributed object management systems, through extension of its\n   request methods, error codes and headers [47]. A feature of HTTP is^M\n   the typing and negotiation of data representation, allowing systems^M\n   to be built independently of the data being transferred.\n \n   HTTP has been in use by the World-Wide Web global information\n   initiative since 1990. This specification defines the protocol\n   referred to as 'HTTP/1.1', and is an update to RFC 2068 [33].\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-v11-spec-rev-06.txt\".\nA URL for the Internet-Draft is:\nhttp://www.ietf.org/internet-drafts/draft-ietf-http-v11-spec-rev-06.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nic.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ftp.ietf.org\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ietf.org.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-v11-spec-rev-06.txt\".\n\nNOTE:The mail server at ietf.org can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-012-8705545"}, {"subject": "Announcement of IETF HTTPNG WG BOF in Orland", "content": "The charter for the proposed IETF HTTP-NG working group [1] is now\navailable for discussion on the new HTTP-NG mailing list [2] - if you are\ninterested in HTTP-NG then please join this list!\n\nWe will be meeting at the HTTP-NG BOF [3] at the IETF meeting in Orlando,\nFl. [4] where we will discuss the charter and milestones. The tentative\nplan is Monday, 13.00-15.00 but please check IETF agenda [6] for details.\n\nWe submitted an Internet Draft [5] for Orlando which gives an overview of\nthe HTTP-NG, what it tries to solve and how. Comments are welcome, also on\nthe mailing list [2].\n\nYou can find more information at the HTTP-NG Overview [4].\n\nThanks,\n\nHenrik\n\n[1] http://www.w3.org/Protocols/HTTP-NG/\n[2] http://www.w3.org/Protocols/HTTP-NG/#Lists\n[3] http://www.w3.org/Protocols/HTTP-NG/1998/11/IETFBofAgenda-2.html\n[5]\nhttp://www.w3.org/Protocols/HTTP-NG/1998/11/draft-frystyk-httpng-overview-00\n[6] http://www.ietf.org/meetings/agenda.txt\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-8714922"}, {"subject": "CacheControl and Pragm", "content": "I can't find anything in the Draft that addresses the situation when\nboth Cache-Control and Pragma: no-cache headers are set, and they are in\nconflict. \nFor instance, if I use\n\nCache-Control: must-revalidate\nPragma: no-cache\n\non a HTTP response, it would be desireable to have it cached (and always\nrevalidated) by HTTP 1.1 caches, and not cached by HTTP 1.0 caches.\nHowever, I can see nothing in the Draft about this, and some\nimplementations, upon reading 14.32 Pragma, might never cache the\nobject. While the action is safe, it's not ideal.\n \nAm I missing something somewhere? Thanks.\n\n\n\nMark Nottingham\nInternet Project Manager\nMerrill Lynch - Melbourne, Australia\n\n\n\n", "id": "lists-012-8722865"}, {"subject": "RE: CacheControl and Pragm", "content": "Mark Nottingham wrote:\n\n> I can't find anything in the Draft that addresses the situation when\n> both Cache-Control and Pragma: no-cache headers are set, and they are in\n> conflict. \n> For instance, if I use\n> \n> Cache-Control: must-revalidate\n> Pragma: no-cache\n> \n> on a HTTP response, it would be desireable to have it cached (and always\n> revalidated) by HTTP 1.1 caches, and not cached by HTTP 1.0 caches.\n> However, I can see nothing in the Draft about this, and some\n> implementations, upon reading 14.32 Pragma, might never cache the\n> object. While the action is safe, it's not ideal.\n>  \n> Am I missing something somewhere? Thanks.\n\nThe interaction of headers whose interaction is not specified\nis not specified.\n\nAn origin server cannot depend on the precedence of\n\"Cache-Control: must-revalidate\" and \"Pragma: no-cache\",\nand a server (1.0 or 1.1) might ignore either or both.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-8730821"}, {"subject": "43rd IETF HTTP Working Group oneparagraph summar", "content": "The HTTP Working Group has completed its work. We have issued\nfinal drafts of the HTTP protocol specification (in two parts),\nand we are waiting for A-D review and IESG balloting. We're hoping\nto see the HTTP RFCs issue and the working group close soon.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-8739525"}, {"subject": "Re: CacheControl and Pragm", "content": "Larry Masinter:\n>\n>Mark Nottingham wrote:\n>\n>> I can't find anything in the Draft that addresses the situation when\n>> both Cache-Control and Pragma: no-cache headers are set, and they are in\n>> conflict. \n>> For instance, if I use\n>> \n>> Cache-Control: must-revalidate\n>> Pragma: no-cache\n>> \n>> on a HTTP response, it would be desireable to have it cached (and always\n>> revalidated) by HTTP 1.1 caches, and not cached by HTTP 1.0 caches.\n>> However, I can see nothing in the Draft about this, and some\n>> implementations, upon reading 14.32 Pragma, might never cache the\n>> object. While the action is safe, it's not ideal.\n>>  \n>> Am I missing something somewhere? Thanks.\n>\n>The interaction of headers whose interaction is not specified\n>is not specified.\n>\n>An origin server cannot depend on the precedence of\n>\"Cache-Control: must-revalidate\" and \"Pragma: no-cache\",\n>and a server (1.0 or 1.1) might ignore either or both.\n\nJust to add to this: the preferred method of making responses cachable\nby 1.1 caches and non-cachable by 1.0 caches is to combine\nCache-Control: something with Expires: <current time and date>.  The\ninteraction between these two _is_ specified.\n\n>\n>Larry\n\nKoen.\n\n\n\n", "id": "lists-012-8747192"}, {"subject": "Re: CacheControl and Pragm", "content": "    I can't find anything in the Draft that addresses the situation\n    when both Cache-Control and Pragma: no-cache headers are set, and\n    they are in conflict.\n\n    For instance, if I use\n    \n    Cache-Control: must-revalidate\n    Pragma: no-cache\n    \n    on a HTTP response, it would be desireable to have it cached (and\n    always revalidated) by HTTP 1.1 caches, and not cached by HTTP 1.0\n    caches.  However, I can see nothing in the Draft about this, and\n    some implementations, upon reading 14.32 Pragma, might never cache\n    the object. While the action is safe, it's not ideal.\n     \n    Am I missing something somewhere? Thanks.\n    \nLarry Masinter (who writes that this is \"not specified\") and Koen (who\npoints out that what you probably want to do is to send an Expires\nheader) are both right.\n\nHowever, there is a subtle point that you and they both missed: the\nformal specification for \"Pragma: no-cache\" applies ONLY to request\nmessages (in both HTTP/1.1 and in RFC1945); it has no formal definition\nfor response messages.\n\nWhile it is possible that some caches do look at \"Pragma\" headers in\nresponses, I don't know of any specific proxy software that does.  I\nchecked both Squid 1.1.20 and an ancient copy of the CERN server/proxy\ncode; both would ignore \"Pragma\" headers in responses.  So sending\n\"Pragma: no-cache\" in a response would probably be pointless,\nregardless of what the HTTP/1.1 specification says about its\ninteraction with other headers.\n\nIn retrospect, I think the wording of the specification of\nPragma should have included a Note to this effect, because\nlots of people seem to be confused by this (even me, at times).\nBut let's not delay the standardization process over this (non)issue!\n\n-Jeff\n\n\n\n", "id": "lists-012-8755982"}, {"subject": "RE: CacheControl and Pragm", "content": "There seems to be a lot of references to Pragma: no-cache response\nheaders out there, and they are in fairly common usage (I believe\nsomeone from Squid could give stats on what they've seen, probably). I\nbelieve that I was pointed to them by someone on this list a while back,\nso it might be worthwhile to (eventually) clear it up.\n\nBTW anawat, the behavior re: pragma and Cache-control that we were\nasking for will no longer be necessary ;-)\n\n\n> -----Original Message-----\n> From: chankhun@netapp.com [mailto:chankhun@netapp.com]\n> Sent: Friday, December 11, 1998 7:02 AM\n> To: mogul@pa.dec.com\n> Cc: mark_nottingham@exchange.au.ml.com; http-wg@hplb.hpl.hp.com\n> Subject: Re: Cache-Control and Pragma\n> \n> \n> > While it is possible that some caches do look at \"Pragma\" headers in\n> > responses, I don't know of any specific proxy software that does.  I\n> > checked both Squid 1.1.20 and an ancient copy of the CERN \n> server/proxy\n> \n> Netcache from netapp does.\n> \n> Anawat\n> \n\n\n\n", "id": "lists-012-8765067"}, {"subject": "RE: Cachecontrol vs Pragm", "content": "I used the book Web Proxy Servers by Ari Luotonen like a pratical guide\nto the HTTP spec.\n\nFrom that book, it mentioned that Cache-control is more in favour now.\nPragma might still be send by browser that support HTTP 1.0 only.\n\nWendy\n\n> \n> There seems to be a lot of references to Pragma: no-cache response\n> headers out there, and they are in fairly common usage (I believe\n> someone from Squid could give stats on what they've seen, probably). I\n> believe that I was pointed to them by someone on this list a while back,\n> so it might be worthwhile to (eventually) clear it up.\n> \n> BTW anawat, the behavior re: pragma and Cache-control that we were\n> asking for will no longer be necessary ;-)\n> \n> \n> > -----Original Message-----\n> > From: chankhun@netapp.com [mailto:chankhun@netapp.com]\n> > Sent: Friday, December 11, 1998 7:02 AM\n> > To: mogul@pa.dec.com\n> > Cc: mark_nottingham@exchange.au.ml.com; http-wg@hplb.hpl.hp.com\n> > Subject: Re: Cache-Control and Pragma\n> > \n> > \n> > > While it is possible that some caches do look at \"Pragma\" headers in\n> > > responses, I don't know of any specific proxy software that does.  I\n> > > checked both Squid 1.1.20 and an ancient copy of the CERN \n> > server/proxy\n> > \n> > Netcache from netapp does.\n> > \n> > Anawat\n> > \n> \n\n\n\n", "id": "lists-012-8774938"}, {"subject": "Re: CacheControl and Pragm", "content": "> While it is possible that some caches do look at \"Pragma\" headers in\n> responses, I don't know of any specific proxy software that does.  I\n> checked both Squid 1.1.20 and an ancient copy of the CERN server/proxy\n\nNetcache from netapp does.\n\nAnawat\n\n\n\n", "id": "lists-012-8784057"}, {"subject": "Fwd: Comments on the digest authentication draf", "content": "Dunno if there will be another authentication draft or not.\n\nIn any case, such comments go to the http working group mailing list;\nI'm forwarding them there.\n- Jim\n\n\n\n\nattached mail follows:\nHello Jim,\n\nHere are some of the notes I have been taking while implementating\nthis protocol inside libwww.\n\nIf the authors or editor find them interesting, it'd be nice to have\nan acknowledgment somewhere :)\n\nI'm also planning to make a formal analysis of the security protocol\nlater on. \n\nIf things work out correctly, we'll ship a first implementation of the\nprotocol next week, inside Amaya. At least, it'll be backward\ncompatible with previous message digest protocol and will work with the\ncode shipped inside the latest Apache. \n\nIf I can make some interoperability tests during this week, I'll be able to \nprovide more new features.\n\nCheers,\n  \n-Jose\n\n=========================================\nNotes on the Internet-Draft <draft-ietf-http-authentication-03>\n\nby: Jose Kahan (kahan@w3.org)\n\nWho am I?\n\nI'm currently implementing the digest authentication protocol inside libwww.\n\nGeneral impression:\n\nNice draft and easy to read. The C code for digest authentication\nworks right out of the box. \n\nI have some remarks concerning digest authentication.\n\n==========\npg. 9\n\nAbout domains... consider the following scenario:\n       * client request URL1\n       * server sends back a challenge, with a domain value (list of URLs)\n       * later on, client requests URL1\n       * server sends back a new challenge, with no domain valeu\n       * later on, client requests URL1\n       * server sends back a new challenge, with a different domain value\n\nShould the client keep a track of the \"root\" URL which gave access to a\ndomain and invalidate that domain whenever the challenge for the \"root\" URL\nchanges?\n\nAnother scenario\n\n       * client request URL\n       * server sends back a challenge, with a domain value (list of URLs)\n       * client requests one of the URLs in the domain, which happens to\n         be stored in server 2\n       * server 2 sends back a new challenge, with another domain value\n\nIs server 2 authoritative, in that it can tell the client that the same\npassword can be used with other domains, or it should only be the case with\nthe \"root\" URL?\n\n===========\npg. 11\n\nAlgorithm sounds confusing when combined with the word \"MD5\".\n\nMD5 is a message digest algorithm, indepent of the one used by the\nauthentication protocol.\n\nI'd redefine the two algorithms as \"MD5-base\", \"MD5-sess\". Better, I'd\nseparate this word into two pieces:\n\n  \"digest-algorithm\" = (MD4 | MD5 |  SHA | ...)\n\nand use anoter keyword to precise the type of digest:\n\n   \"digest-type\" = \"base\" | \"sess\"\n\n>From an implementor's points of view, this gives a better separation of\nfunctionality and allows to write a more generic code. For example, in\nmy libwww implementation, I have three levels:\n\n      - a high level part deals with the \"digest-type\",\n      - a middle level which gives a generic interface to the message digest\n        algorithms\n      - a low level part, where the digest algorithms are called.\n\nAnyway, I think that what  I call the \"digest-type\" is independent of\nthe \"digest-algorithm\".\n\n================\npg. 12:\n\ncnonce value is not quoted, but it is quoted in the example on pg. 18\n\n================\npg. 12:\n\nA suggestion or example on how to generate the cnonce would be useful.\n\n\n===============\npg. 16\n\nI suppose that the Authentication-Info header doesn't include a scheme\nidentifier (e.g., Digest) as the client already knows what scheme it used\nwhen contacting the server. Maybe a paragraph precising this would be useful.\n\n==============\npg. 18\n\nthe defy and response in the example don't include an algorithm string, but\n9 and pg. 12 say they should.\n\n\n------\n\nThat's it for the moment.\n\nI hope this is helpful,\n\n-Jose\n\n\n\n", "id": "lists-012-8791753"}, {"subject": "RE: IPP&gt; Chunked POS", "content": "> Many http server implementors seem to have interpreted the combination\n> of these requirements to imply that a POST request without a\n> Content-Length HTTP header cannot have a message-body. \n\nThis implication might have held for HTTP/1.0, but is wrong for HTTP/1.1.\n\n> Indeed, I have tried several commercial web servers and\n> in all cases, a servlet or CGI program gets end-of-file as soon as it\n> tries to read the message-body input stream for a POST request with\n> chunked transfer-coding.\n\nDid these servers purport to support HTTP/1.1 for CGI scripts?\nhttp://www.w3.org/Protocols/HTTP/Forum/Reports/ is a survey of\nimplementations, but we didn't ask implementations to distinguish\nwhether they supported 'chunked' transfer encoding specifically.\n\nIn any case, you left out the most obvious solution: don't use 'CGI'\nto implement IPP.\n\nHowever, it would be useful to update CGI for HTTP/1.1.\n\nLarry\n\n\n\n", "id": "lists-012-8803810"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "Larry Masinter wrote:\n> \n> > Many http server implementors seem to have interpreted the combination\n> > of these requirements to imply that a POST request without a\n> > Content-Length HTTP header cannot have a message-body.\n> \n> This implication might have held for HTTP/1.0, but is wrong for HTTP/1.1.\n> \n> > Indeed, I have tried several commercial web servers and\n> > in all cases, a servlet or CGI program gets end-of-file as soon as it\n> > tries to read the message-body input stream for a POST request with\n> > chunked transfer-coding.\n> \n> Did these servers purport to support HTTP/1.1 for CGI scripts?\n> http://www.w3.org/Protocols/HTTP/Forum/Reports/ is a survey of\n> implementations, but we didn't ask implementations to distinguish\n> whether they supported 'chunked' transfer encoding specifically.\n> \n> In any case, you left out the most obvious solution: don't use 'CGI'\n> to implement IPP.\n> \n> However, it would be useful to update CGI for HTTP/1.1.\n\nHmm. I'm pretty sure Apache works correctly in this case, but I doubt it\nhas been heavily tested. Does anyone know different?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686| Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org/\nand Technical Director|Email: ben@algroup.co.uk |\nA.L. Digital Ltd,     |Apache-SSL author     http://www.apache-ssl.org/\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache/\n\n\n\n", "id": "lists-012-8812509"}, {"subject": "RE: Comments on the digest authentication draf", "content": "It's time to start collecting 'errata', rather than consider\nrevising our drafts.\n\n\n\n", "id": "lists-012-8822368"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "kugler@us.ibm.com wrote:\n> \n> To wrap up this discussion, can we summarize the answer in RFC 2119\n> standard-speak?  What requirements must an http server meet in order to\n> claim compliance with HTTP/1.1 AND CGI/1.1?\n\nI can only speak with anything resembling authority on the\nCGI aspects.  I find your questions a bit contradictory,\nbut here's my take on them:\n\n> 1.  MAY a server discard the message body of a POST request with no\n> Content-Length?\n\n[Opinion] No.  The message-body is an integral part of\nthe request, and cannot be silently ignored.  The server\nshould return a 411 (Length Required) instead.\n\n> 2.  MAY a server discard the message body of a POST request with no\n> Content-Length when the destination resource is CGI?\n\n[Opinion] Same answer as for [1].\n\n> 3. MUST a server buffer the message body of a POST request with\n> Transfer-Encoding: chunked and generate a CONTENT_LENGTH when the\n> destination resource is CGI?\n\nYes, if it accepts the request at all.  The CONTENT_LENGTH\nmetavariable is a MUST for CGI compliance.\n\n> 4. SHOULD a server buffer the message body of a POST request with\n> Transfer-Encoding: chunked and generate a CONTENT_LENGTH when the\n> destination resource is CGI?\n\nNo, it's a MUST.\n\n> What if the destination resource is a servlet?\n\nI don't think this applies to CGI.  I don't know what\ninterface is used by servers to communicate with servlets.\nIf they use CGI, or a particular servlet implementation\ndoes, then they need to abide by the CGI requirements.\n\n> Should this information be written in a spec or informational something\n> somewhere?\n\nI think that specs that define how applications should juggle\ndifferent protocols are a waste of time (MHO).  This is more likely\na HOWTO or FAQ sort of thing, explaining how related aspects\nshould be handled by implementors.\n\nOf course, if you want to write something up for the CGI-NG\ndraft, feel free.. :-)\n-- \n#kenP-)}\n\nKen Coar                    <http://Web.Golux.Com/coar/>\nApache Group member         <http://www.apache.org/>\n\"Apache Server for Dummies\" <http://Web.Golux.Com/coar/ASFD/>\n\n\n\n", "id": "lists-012-8830599"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "In a previous episode Rodent of Unusual Size said...\n:: \n:: \n:: > 1.  MAY a server discard the message body of a POST request with no\n:: > Content-Length?\n:: \n:: [Opinion] No.  The message-body is an integral part of\n:: the request, and cannot be silently ignored.  The server\n:: should return a 411 (Length Required) instead.\n:: \n\nSection 4.4 (http rev6):\n\nAll HTTP/1.1 applications that receive entities MUST accept the\n   \"chunked\" transfer-coding (section 3.6), thus allowing this\n   mechanism to be used for messages when the message length cannot be\n   determined in advance.\n\nso, imho, it must take it and use it.\n\nhttp-wg: upon reflection, I actually can't think of any reason why a\nserver would ever send 411... About the best I can come up with is a\ncomplete request (sans body) that has been sent that does hold any of\nthe criteria set forth in 4.4.. but it's not really CL that's needed\nthere, it's something that meets the 4.4 criteria..\n\nour server actually does issue 411 for the above case, but in\nretrospect I'm not really sure that's right.\n\nThe archives didn't clear this up for me.. any recollections?\n\n-P\n\n\n\n", "id": "lists-012-8840734"}, {"subject": "Re: IPP&gt; Chunked POS", "content": "On Thu, 17 Dec 1998 mcmanus@appliedtheory.com wrote:\n\n> In a previous episode Rodent of Unusual Size said...\n> :: \n> :: \n> :: > 1.  MAY a server discard the message body of a POST request with no\n> :: > Content-Length?\n> :: \n> :: [Opinion] No.  The message-body is an integral part of\n> :: the request, and cannot be silently ignored.  The server\n> :: should return a 411 (Length Required) instead.\n> :: \n> \n> Section 4.4 (http rev6):\n> \n> All HTTP/1.1 applications that receive entities MUST accept the\n>    \"chunked\" transfer-coding (section 3.6), thus allowing this\n>    mechanism to be used for messages when the message length cannot be\n>    determined in advance.\n> \n> so, imho, it must take it and use it.\n> \n> http-wg: upon reflection, I actually can't think of any reason why a\n> server would ever send 411... About the best I can come up with is a\n> complete request (sans body) that has been sent that does hold any of\n> the criteria set forth in 4.4.. but it's not really CL that's needed\n> there, it's something that meets the 4.4 criteria..\n> \n> our server actually does issue 411 for the above case, but in\n> retrospect I'm not really sure that's right.\n> \n> The archives didn't clear this up for me.. any recollections?\n> \n\nIn my opinion, Ken Coar is correct in saying that for a server to\nbe *both* HTTP/1.1 compliant and CGI/1.1 compliant it MUST buffer \nchunked POST data and provide a Content-Length for the CGI script.\n\nMy recollection is that some servers chose not to be completely \nCGI/1.1 compliant to avoid the buffering.  The 411 header was a\nway to be HTTP/1.1 compliant and indicate their rejection of \nchunked POST data.\n\nFor example, I have heard that Apache rejects chunked POST data,\nbut I have not personally verified this.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-8849440"}, {"subject": "Re: IPP&gt; Chunked POS", "content": ">In my opinion, Ken Coar is correct in saying that for a server to\n>be *both* HTTP/1.1 compliant and CGI/1.1 compliant it MUST buffer \n>chunked POST data and provide a Content-Length for the CGI script.\n\nSending 411 is HTTP/1.1 compliant.  Failure to parse the chunked\nencoding (and puking) would be non-compliance, but requiring a\ncontent-length for a given resource is necessary for many reasons\n(DoS and legacy system protection).\n\n>My recollection is that some servers chose not to be completely \n>CGI/1.1 compliant to avoid the buffering.  The 411 header was a\n>way to be HTTP/1.1 compliant and indicate their rejection of \n>chunked POST data.\n>\n>For example, I have heard that Apache rejects chunked POST data,\n>but I have not personally verified this.\n\nRight, the default mod_cgi distributed with Apache will respond with\n411 because it assumes the CGI is dumb.  The core of Apache does support\nchunked reading, so one could always write a module that reads chunked,\nbut CGI requires a content-length before the script is execed.\nA module that does limited-size buffering before handing over to the CGI\nscript has been on my Apache to-do list for a long time (over a year).\n\n....Roy\n\n\n\n", "id": "lists-012-8859915"}, {"subject": "Redirection messages for proxy client", "content": "Hi everybody,\n\nI was wondering if it is possible to redirect a client (configured to use a\nproxy) to communicate directly with an origin server, thus, bypassing the\nproxy server. In other words, can we force this client to use a partial URL\ninstead of a full URL. I doubt that a JavaScript program or a META tag can\nhelp in this case. I am still not clear if a \"301 Moved Permanently\" or \"302\nMoved Temporarily\" message could do the trick.\nThanks in advance for considering my request.\nFrederic\n\nPS: apologies to those who have received that message already\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\"There is no excellent beauty that hath not some strangeness in the\nproportion.\"\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFrederic Delley - Software Engineer at NOMADIX\n\nemail: fdelley@nomadix.com\nTel.: (310) 664-0280\nFax.: (310) 396-7881\n\n\n\n", "id": "lists-012-9051087"}, {"subject": "Re: Redirection messages for proxy client", "content": "> I was wondering if it is possible to redirect a client (configured to use a\n> proxy) to communicate directly with an origin server, thus, bypassing the\n> proxy server. In other words, can we force this client to use a partial URL\n> instead of a full URL. I doubt that a JavaScript program or a META tag can\n> help in this case. I am still not clear if a \"301 Moved Permanently\" or \"302\n> Moved Temporarily\" message could do the trick.\n\nI think the answer in all cases is \"No\".  Usually the client's\nconfiguration determines whether a request uses a proxy.  Typically all\nrequests go through a proxy except for those in the \"No Proxy\" list.  I\nbelieve Netscape has a proxy auto-configuration mechanism that can make\nmore sophisticated choices, but a random web page can't change that\nconfiguration.  A Java applet might be able to connect directly to the\norigin server, but you wouldn't use that for a \"regular\" HTTP request.\n\nDave Kristol\n\n\n\n", "id": "lists-012-9058970"}, {"subject": "Re: IPP&gt; Chunked POST: SUMMAR", "content": "kugler@us.ibm.com writes:\n\n>Here is a summary of the responses that I received to my queries about\n>chunked POST in HTTP/1.1.\n>\n>...\n>5.  Origin servers MUST remove the Transfer-Encoding before passing a\n>request body to a plug-in, servlet, (Fast)CGI, or across any other gateway\n>boundary.\n\nThat's not clearly stated in either the HTTP 1.1 or CGI 1.1 specifications.\nI'll grant that it makes good sense though.  HTTP requires that gateways\nto MIME-compliant protocols remove transfer-codings (HTTP 1.1 rev 6 sec.\n19.4.6), but HTTP doesn't assume a separate HTTP-protocol engine and a\nprogram interacting with it according to the CGI protocol.  As far as\nHTTP is concerned, they might jointly comprise the \"gateway\", and the CGI\nprogram might be responsible for removing any transfer-codings.\n\n>Implications for IPP:\n>1.  The IPP layer doesn't have to deal with chunking.  In the context of\n>CGI scripts, the HTTP layer either rejects a chunked POST request with 411\n>or removes any chunking information in the received data and supplies\n>CONTENT_LENGTH.\n\nI'm not up on IPP, but I don't think you can blindly say that, given the\nabove.\n\n>2.  The HTTP/1.1 standard does not guarantee that an origin server will\n>accept chunked requests, regardless of the resource identified in the\n>request.\n\nHTTP actually doesn't guarantee anything about *acceptance* of requests.\nThere are lots of conditions that can provoke responses other than\n\"200 OK\".  It does, however, require that all HTTP 1.1 applications be\nable to receive and decode chunked requests (HTTP 1.1 rev 6, sec. 3.6.1).\nIf the resource in question is implemented via CGI 1.1, it might still\nbe rejected as you noted due to buffering concerns, but if it is\nimplemented via an interface that does not require a pre-determined\nlength, there's nothing wrong with sending chunked data.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-9066845"}, {"subject": "Re: IPP&gt; Chunked POST: SUMMAR", "content": "kugler@us.ibm.com writes:\n\n>Ross Patterson wrote:\n>>\n>>>...\n>>>5.  Origin servers MUST remove the Transfer-Encoding before passing a\n>>>request body to a plug-in, servlet, (Fast)CGI, or across any other\n>gateway\n>>>boundary.\n>>\n>>That's not clearly stated in either the HTTP 1.1 or CGI 1.1\n>specifications.\n>\n>An unfortunate omission, IMO.\n\nNot really.  The HTTP Working Group generally avoids discussion of any\npurely-internal interfaces, including CGI, servlets, NSAPI, etc.  And of\ncourse CGI 1.1, which predates any discussion of transfer-codings in\nHTTP, couldn't be expected to address the issue.\n\nThe CGI Working Group <CGI-WG at Golux.Com> is working on getting CGI 1.1\nfinally up to snuff for publication as an RFC, and then plans to\nundertake CGI 1.2, which is targeted at addressing HTTP 1.1 issues\n(among other things).  As often happens with protocol revisions, there\nis likely to be some debate about the scope of changes from CGI 1.1 to\n1.2, and now that you've raised this point about removing\ntransfer-codings I expect that will be discussed.\n\n>Section 4.4 says:\n>\n>> All HTTP/1.1 applications that receive entities MUST accept the\n>> \"chunked\" transfer-coding (section 3.6), thus allowing this mechanism\n>> to be used for messages when the message length cannot be determined\n>> in advance.\n>\n>Apparently that should be interpreted as \"MUST accept the 'chunked'\n>TRANSFER-CODING, but NEED NOT accept REQUESTs with that transfer-coding.\"\n\nCorrect - all HTTP 1.1 servers must be able to process requests encoded\nas chunked data, but they are still allowed to refuse the request for\nother reasons.  For example, the targeted resource might not accept the\nmethod you've specified (many servers refuse POST for static files with\n403 Forbidden or (in HTTP 1.1) 405 Method Not Allowed), or the request\nmight require authorization (401 Not Authorized), or as you've noted\nalready it might exceed the server's willingness to buffer data.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-9076556"}, {"subject": "GETs with EntityBody ?", "content": "I am wondering whether GET requests are allowed to have an Entity-Body?\nI have read through the HTTP RFC several times and have not been able to\nfind a specific prohibition of this, however several colleagues of mine\nfeel very strongly that GET requests are not permitted to provide an\nEntity-Body.\n\nThanks ...\n--ROb\n\n\n\n", "id": "lists-012-9086184"}, {"subject": "Re: GETs with EntityBody ?", "content": "Robert Long wrote:\n> \n> I am wondering whether GET requests are allowed to have an Entity-Body?\n\nI can't see any reason why they would not. I have written a small http\nproxy, and I check for the \"Content-Length\" header to see if there is\nany entity body, and not the METHOD (except for HEAD :).\n\n> I have read through the HTTP RFC several times and have not been able to\n> find a specific prohibition of this, however several colleagues of mine\n> feel very strongly that GET requests are not permitted to provide an\n> Entity-Body.\n> \n\nWell, it doesn't make any sense for the GET to have a body, since all\nthe information (both static and dynamic) are stored in the URL. So\nmaybe there is an implementation of a proxy and/or server that gets all\nmixed up when there is a Body in a GET request.\n\nI guess this might be one of those times where you are forced to find\nout what the real world implementations do, by doing some testing...;)\n\nIf you do find out, could you tell me, since I too would like to know,\nand am too lazy to work it out myself...;)\n\nCheers,\nKal.\n\n-- \n    .      Kalvinder Singh                     singh@ozy.dec.com\n _-_|\\     Software Engineering Australia\n/     \\<-- Compaq       \n\\_.-._/    Research Park, Bond University      Phone: +61 7 5575 0106\n     v     Gold Coast, Qld, 4229, Australia    Fax:   +61 7 5575 0100\n\n\n\n", "id": "lists-012-9093707"}, {"subject": "HTTP 1.1 rev 6 BNF erratu", "content": "I don't know if this rates high enough to include on an errata list, but\nthe BNF in section 3.6.1 \"Chunked Transfer Coding\" for trailer reads:\n\n   trailer        = *entity-header\n\nwhen it really ought to read:\n\n   trailer        = *(entity-header CRLF)\n\nThe rest of the BNF in this section is very explicit, and chunked encoding\nis new in HTTP 1.1, so this might be worth fixing.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-9102706"}, {"subject": "Re: GETs with EntityBody ?", "content": "Robert Long wrote:\n> \n> I am wondering whether GET requests are allowed to have an Entity-Body?\n\nKalvinder Singh wrote:\n> I can't see any reason why they would not. I have written a small http\n> proxy, and I check for the \"Content-Length\" header to see if there is\n> any entity body, and not the METHOD (except for HEAD :).\n> \n> Well, it doesn't make any sense for the GET to have a body, since all\n> the information (both static and dynamic) are stored in the URL. So\n> maybe there is an implementation of a proxy and/or server that gets all\n> mixed up when there is a Body in a GET request.\n\nThis is probably a good time to remind people of the Robustness\nPrinciple, first stated (although not under that name) in RFC791,\nwith respect to IP datagrams:\n\n  In general, an implementation must be conservative in its sending\n  behavior, and liberal in its receiving behavior.  That is, it must be\n  careful to send well-formed datagrams, but must accept any datagram\n  that it can interpret (e.g., not object to technical errors where the\n  meaning is still clear).\n\nApplying this to the current question: a client implementation\nshould not send an entity-body with a GET request, because it\nhas no specified meaning.  BUT: a server ought to accept such\na GET message (and ignore the entity-body), rather than reject\nsuch a message as erroneous.\n\nIt's perhaps harder to use this principle to decide whether a proxy\nshould forward the request intact (\"be liberal in its receiving\nbehavior\") or with the body deleted (\"be conservative in its sending\nbehavior\").  If the proxy is being used as part of a security\nfirewall, I'd probably vote for the latter, so as to avoid the\npossibility of a covert channel ... but this is debatable, I'm sure.\n\n-Jeff\n\n\n\n", "id": "lists-012-9109295"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "kugler@us.ibm.com wrote:\n\n> The IPP WG would really like clarification on this point:  Is the intent of\n> the HTTP/1.1 spec to say that an HTTP/1.1 server MAY reject any request\n> without a defined Content-Length?  This would imply that a conformant\n> HTTP/1.1 server MAY reject any request with the \"chunked\" transfer-coding.\n\nI don't know who can provide any sort of authoritative response - don't\ntake mine as being 'from the HTTP WG'; I'm just another HTTP server\nvendor.  \n\nFirst, I think that the note Harry Lewis sent titled \"IPP> Chunking\nExplanation\" [1] sums it up pretty well.  An HTTP server certainly has the\noption of using the \"Length Required\" code for whatever reason it wants\nto.  My own judgement would be that a printer design that did not allow for\nvery large inputs of indeterminate length would be a poor one, and as a\nresult I would not choose an HTTP layer implementation that restricted me\nto CGI.  \n\n[1] <872566FF.0013A85F.00@d53mta05h.boulder.ibm.com>\n    (Can't seem to find a web-accessible ipp list archive...)\n\n-- \nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-9118021"}, {"subject": "(no subject", "content": "Scott Lawrence  wrote:\nOriginal Article: http://www.egroups.com/list/http-wg/?start=8491\n> kugler@us.ibm.com wrote:\n>\n> > The IPP WG would really like clarification on this point:  Is the\nintent of\n> > the HTTP/1.1 spec to say that an HTTP/1.1 server MAY reject any request\n> > without a defined Content-Length?  This would imply that a conformant\n> > HTTP/1.1 server MAY reject any request with the \"chunked\"\ntransfer-coding.\n>\n> I don't know who can provide any sort of authoritative response - don't\n> take mine as being 'from the HTTP WG'; I'm just another HTTP server\n> vendor.\n\nThanks for your reply.  I realize there's probably no authoritative answer\navailable, but as an HTTP server vendor you probably know more about this\nthan I do as an printer vendor, so I appreciate your help.\n\n>\n> First, I think that the note Harry Lewis sent titled \"IPP> Chunking\n> Explanation\" [1] sums it up pretty well.  An HTTP server certainly has\nthe\n> option of using the \"Length Required\" code for whatever reason it wants\n> to.\nIf this is the correct interpretation, then I was misled for a long time by\nthe paragraph in section 4.4, \"Message Length\", that says \"All HTTP/1.1\napplications that receive entities MUST accept the ?chunked?\ntransfer-coding (section 3.6), thus allowing this mechanism to be used for\nmessages when the message length cannot be determined in advance. \"  I\nthink it would be very helpful to have a note or warning added to that\nparagraph;  perhaps:\n\nAll HTTP/1.1 applications that receive entities MUST accept the ?chunked?\ntransfer-coding (section 3.6), thus allowing this mechanism to be used for\nmessages when the message length cannot be determined in advance.  Note:\nthis does NOT mean that servers must accept HTTP/1.1 requests containing a\nmessage-body with the ?chunked? transfer-coding.\n\n> My own judgement would be that a printer design that did not allow for\n> very large inputs of indeterminate length would be a poor one, and as a\n> result I would not choose an HTTP layer implementation that restricted me\n> to CGI.\n>\nAgreed.\n\n> [1] <872566FF.0013A85F.00@d53mta05h.boulder.ibm.com>\n>     (Can't seem to find a web-accessible ipp list archive...)\n\nYou can find web-accessible ipp list archives at\n\nhttp://www.pwg.org/hypermail/ipp/\n\nand\n\nhttp://www.egroups.com/list/ipp/info.html\n\n(BTW, none of my messages to http-wg@cuckoo.hpl.hp.com seem to make it to\nthe archives at\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/\n\nor\n\nhttp://www.findmail.com/listsaver/http-wg/\n\nDoes one have to be subscribed in order to post messages?  I thought there\nwas some kind of IETF rule against that.)\n\n\nCarl Kugler\nIBM Printing Systems Co.\n\n\n\n", "id": "lists-012-9127205"}, {"subject": "number lost (HTTP 1.1 rev 6 ", "content": "Hello,\n\nI've just found (better: haven't found) a missing number in the \nHTTP 1.1 rev 6 (txt) document:\n\nSection 8.1.4 Practical Considerations, last para.:\n  A single-user client SHOULD NOT maintain more than connections with any\n  server or proxy.\nAccording to Andy Norman (Mon, 26 Oct 1998) and Jim Gettys' reply I expected\nto find a '2' after the word 'connections'.\n\nI'm afraid that this is a little bit to late to report, but ...\n\nBye,\n\nJacob\n-- \n                                             Jacob Schroeder\n                                             Dipl. Informatiker\n                                             eMail: jschroeder@becomsys.de\n\n\n\n", "id": "lists-012-9137855"}, {"subject": "Re: Unidentified subject", "content": "On Wed, 20 Jan 1999 kugler@us.ibm.com wrote:\n\n> \n> \n> Scott Lawrence  wrote:\n> Original Article: http://www.egroups.com/list/http-wg/?start=8491\n> > kugler@us.ibm.com wrote:\n> >\n> > > The IPP WG would really like clarification on this point:  Is the\n> intent of\n> > > the HTTP/1.1 spec to say that an HTTP/1.1 server MAY reject any request\n> > > without a defined Content-Length?  This would imply that a conformant\n> > > HTTP/1.1 server MAY reject any request with the \"chunked\"\n> transfer-coding.\n> >\n> > First, I think that the note Harry Lewis sent titled \"IPP> Chunking\n> > Explanation\" [1] sums it up pretty well.  An HTTP server certainly has\n> the\n> > option of using the \"Length Required\" code for whatever reason it wants\n> > to.\n> If this is the correct interpretation, then I was misled for a long time by\n> the paragraph in section 4.4, \"Message Length\", that says \"All HTTP/1.1\n> applications that receive entities MUST accept the ?chunked?\n> transfer-coding (section 3.6), thus allowing this mechanism to be used for\n> messages when the message length cannot be determined in advance. \"  I\n> think it would be very helpful to have a note or warning added to that\n> paragraph;  perhaps:\n> \n> All HTTP/1.1 applications that receive entities MUST accept the \"chunked\"\n> transfer-coding (section 3.6), thus allowing this mechanism to be used for\n> messages when the message length cannot be determined in advance.  Note:\n> this does NOT mean that servers must accept HTTP/1.1 requests containing a\n> message-body with the \"chunked\" transfer-coding.\n> \n> > My own judgement would be that a printer design that did not allow for\n> > very large inputs of indeterminate length would be a poor one, and as a\n> > result I would not choose an HTTP layer implementation that restricted me\n> > to CGI.\n> >\n> Agreed.\n> \n\nI believe that the history of this problem has to do with the\nlimitations of CGI/1.1 and the fact that it has been essentially\nfrozen.  For a chunked message-body to go to a CGI program it must be\nunchunked.  CGI has not advanced with HTTP/1.1 so the CGI spec will\nonly accept unchunked data of a known length.  This means that the\nonly possibility is for the server to unchunk, buffer and calculate\nthe length of the data.\n\nSome server implementations (most notably Apache) decided they were\nunwilling to do this -- presumably for efficiency reasons.  Hence\nwhile they are required to \"accept\" chunked message bodies they are\nnot required to buffer them so they can be used by CGI.  This may make\nthe notion of \"accept\" somewhat silly, but it is also silly to say\ndata of arbitrary length must be accepted.\n\nThis is unfortunate, because CGI, while not very efficient, is very\nflexible.  It provides a mechanism for existing installed servers to\nwork with new applications.  It may be easy to rewrite a server to\nhandle a new application or even create a new module and recompile,\nbut those changes are not likely to affect the large installed base of\nservers.\n\nI can sympathize with the Apache developers view that unchunking and\nbuffering message bodies on a site which gets millions of hits per day\ncould be very problematic.  On the other hand applications, like\nprinting will normally occur on a much smaller scale for which CGI is\nperfectly appropriate.  It is unfortunate that there seems to be no\nway out of this dilemma.\n\nSome servers do support unchunking and buffering for CGI, but the\nlarge majority do not.  There is no way to change this fact\nretroactively.  The bottom line is this is an implementation issue,\nnot a protocol issue.  If implementors choose not to support\nfunctionality you need, you may just be stuck.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-9144999"}, {"subject": "Chunked POST (was Re: Unidentified subject!", "content": " <pine.lnx.4.04.9901201238350.7238-10000-@hopf.math.nwu.edu> wrote:\nOriginal Article: http://www.egroups.com/list/http-wg/?start=8494\n> On Wed, 20 Jan 1999 kugler@us.ibm.com wrote:\n>\n> >\n> >\n> > Scott Lawrence  wrote:\n> > Original Article: http://www.egroups.com/list/http-wg/?start=8491\n> > > kugler@us.ibm.com wrote:\n> > >\n> > > > The IPP WG would really like clarification on this point:  Is the\n> > intent of\n> > > > the HTTP/1.1 spec to say that an HTTP/1.1 server MAY reject any\nrequest\n> > > > without a defined Content-Length?  This would imply that a\nconformant\n> > > > HTTP/1.1 server MAY reject any request with the \"chunked\"\n> > transfer-coding.\n> > >\n> > > First, I think that the note Harry Lewis sent titled \"IPP> Chunking\n> > > Explanation\" [1] sums it up pretty well.  An HTTP server certainly\nhas\n> > the\n> > > option of using the \"Length Required\" code for whatever reason it\nwants\n> > > to.\n> > If this is the correct interpretation, then I was misled for a long\ntime by\n> > the paragraph in section 4.4, \"Message Length\", that says \"All HTTP/1.1\n> > applications that receive entities MUST accept the \n\n\n?chunked\n\n\n?\n> > transfer-coding (section 3.6), thus allowing this mechanism to be used\nfor\n> > messages when the message length cannot be determined in advance. \"  I\n> > think it would be very helpful to have a note or warning added to that\n> > paragraph;  perhaps:\n> >\n> > All HTTP/1.1 applications that receive entities MUST accept the\n\"chunked\"\n> > transfer-coding (section 3.6), thus allowing this mechanism to be used\nfor\n> > messages when the message length cannot be determined in advance.\nNote:\n> > this does NOT mean that servers must accept HTTP/1.1 requests\ncontaining a\n> > message-body with the \"chunked\" transfer-coding.\n> >\n> > > My own judgement would be that a printer design that did not allow\nfor\n> > > very large inputs of indeterminate length would be a poor one, and as\na\n> > > result I would not choose an HTTP layer implementation that\nrestricted me\n> > > to CGI.\n> > >\n> > Agreed.\n> >\n>\n> I believe that the history of this problem has to do with the\n> limitations of CGI/1.1 and the fact that it has been essentially\n> frozen.  For a chunked message-body to go to a CGI program it must be\n> unchunked.  CGI has not advanced with HTTP/1.1 so the CGI spec will\n> only accept unchunked data of a known length.  This means that the\n> only possibility is for the server to unchunk, buffer and calculate\n> the length of the data.\n>\n> Some server implementations (most notably Apache) decided they were\n> unwilling to do this -- presumably for efficiency reasons.  Hence\n> while they are required to \"accept\" chunked message bodies they are\n> not required to buffer them so they can be used by CGI.  This may make\n> the notion of \"accept\" somewhat silly, but it is also silly to say\n> data of arbitrary length must be accepted.\n\nCould you clarify what the \"accepted\" notion of \"accept\" is?  To me, the\nworst possible behavior is for a server to accept a chunked POST request,\nrespond with 200 (OK), but pass a zero-length entity-body to the service\nlayer\n(CGI or servlet).  This silent acceptance gives the client nothing to go\non.  At least if the server \"rejects\" the request with 411 (Length\nRequired)\na smart client could buffer the request itself and retry with\nContent-Length.\n\n>\n> This is unfortunate, because CGI, while not very efficient, is very\n> flexible.  It provides a mechanism for existing installed servers to\n> work with new applications.  It may be easy to rewrite a server to\n> handle a new application or even create a new module and recompile,\n> but those changes are not likely to affect the large installed base of\n> servers.\n>\n> I can sympathize with the Apache developers view that unchunking and\n> buffering message bodies on a site which gets millions of hits per day\n> could be very problematic.  On the other hand applications, like\n> printing will normally occur on a much smaller scale for which CGI is\n> perfectly appropriate.  It is unfortunate that there seems to be no\n> way out of this dilemma.\n>\n> Some servers do support unchunking and buffering for CGI, but the\n> large majority do not.  There is no way to change this fact\n> retroactively.  The bottom line is this is an implementation issue,\n> not a protocol issue.  If implementors choose not to support\n> functionality you need, you may just be stuck.\n>\n> John Franks\n> john@math.nwu.edu\n>\n\nCarl Kugler\n\n\n\n", "id": "lists-012-9157025"}, {"subject": "RE: IPP&gt; Chunking Explanatio", "content": "Stefan-\n\nI agree with you, however, this started out as a direct quote from the spec\n:\n\ndraft-ietf-http-v11-spec-rev-06: 10.4.14      413 Request Entity Too Large\nThe server is refusing to process a request because the request entity is\nlarger than the server is willing or able to process. The server MAY close\nthe connection to prevent the client from continuing the request.\n\n\n\nIt seems the 413 is a special case of 4xx.\n\n\n\n     -Carl\n\n\n\n\n\nStefan Andersson <stefan.andersson@axis.com> on 01/21/99 05:34:01 AM\n\nPlease respond to Stefan Andersson <stefan.andersson@axis.com>\n\nTo:   \"Wagner,William\" <bwagner@digprod.com>\ncc:   Harry Lewis/Boulder/IBM, Carl Kugler/Boulder/IBM, ipp@pwg.org\nSubject:  RE: IPP> Chunking Explanation\n\n\n\n\n\n\nOn Wed, 20 Jan 1999, Wagner,William wrote:\n>\n> RFC2068 requires HTTP1.1 Servers to support both Content Length and\n> Chunking for all requests.  However, some IPP servers may exist that\n> do not support Chunking. In this case, the IPP server may return error\n> 411(Length Required) in response to the HTTP POST.\n>\n> Further,  some IPP servers may implement a filter-and-buffer approach to\n> determine CONTENT_LENGTH from a chunked encoding before passing the\ndecoded\n> request body to a CGI application.  If the buffered request grows too\nlarge,\n> the server may reject the request with status code 413 (Request Entity\nToo\n> Large). If this occurs, the IPP server may also close the connection to\n> prevent the client from continuing the request.\n>\nIf the server can't ensure that the client has acknowledged the response\nit's better to consume the rest of the data, this is to make sure that the\nclient reads the response.\n\ndraft-ietf-http-v11-spec-rev-06: 10.4 Client Error 4xx\n   If the client is sending data, a server implementation using TCP\n   SHOULD be careful to ensure that the client acknowledges receipt of\n   the packet(s) containing the response, before the server closes the\n   input connection. If the client continues sending data to the server\n   after the close, the server's TCP stack will send a reset packet to\n   the client, which may erase the client's unacknowledged input buffers\n   before they can be read and interpreted by the HTTP application.\n\n  /Stefan\n\n--\nStefan Andersson                         Software Engineer\nPrint Server Business Unit               Stefan.Andersson@axis.com\nAXIS Communications AB                   Phone: +46 46 270 19 85\nScheelev?gen 16                          Fax: +46 46 13 61 30\nS-223 70  LUND, SWEDEN                   http://www.axis.com\n\n\n\n", "id": "lists-012-9169650"}, {"subject": "Re: GETs with EntityBody ?", "content": "Jeffrey Mogul <mogul@pa.dec.com> writes:\n\n>Robert Long wrote:\n>>\n>> I am wondering whether GET requests are allowed to have an Entity-Body?\n>\n>This is probably a good time to remind people of the Robustness\n>Principle, first stated (although not under that name) in RFC791,\n>with respect to IP datagrams:\n\nJeff is right about the Robustness Principle, but in addition, HTTP 1.1\nrev 6 explicitly states the requirement in section 4.3 \"Message Body\":\n\n   \"The presence of a message-body in a request is signaled by the\n   inclusion of a Content-Length or Transfer-Encoding header field in\n   the request's message-headers.  A message-body MUST NOT be included\n   in a request if the specification of the request method (section\n   5.1.1) does not allow sending an entity-body in requests.  A server\n   SHOULD read and forward a message-body on any request;  if the\n   request method does not include defined semantics for an entity-body,\n   then the message-body SHOULD be ignored when handling the request.\"\n\nSection 5.1.1 \"Method\" doesn't state an opinion on the validity of\nentity-bodies for any methods, so they appear to always be allowed.\nSection 9.3 \"GET\" doesn't define any semantics for entity-bodies, so\nunconditionally-compliant proxy servers should always forward them and\nunconditionally-compliant origin servers should always ignore them.\nN.B. \"Ignore\" means read-and-discard if you want to keep a persistent\nconnection alive.\n\nRoss Patterson\nVM Software Division\nSterling Software, Inc.\n\n\n\n", "id": "lists-012-9181079"}, {"subject": "Test mai", "content": "Pl ignore.\n\n\n\n", "id": "lists-012-9189281"}, {"subject": "Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "To the HTTP WG from the IPP WG:\n\nI've tried to post this twice before, but it never makes it into the\narchives at\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/\n\nor\n\nhttp://www.findmail.com/listsaver/http-wg/\n\nThe IPP WG would really like clarification on this point:  Is the intent of\nthe HTTP/1.1 spec to say that an HTTP/1.1 server MAY reject any request\nwithout a defined Content-Length?  This would imply that a conformant\nHTTP/1.1 server MAY reject any request with the \"chunked\" transfer-coding.\n\n     -Carl Kugler\n\n\n---------------------- Forwarded by Carl Kugler/Boulder/IBM on 01/12/99\n04:33 PM ---------------------------\n\n\nCarl Kugler\n01/07/99 08:27 AM\n\nTo:   CGI-WG@Golux.Com\ncc:   ipp@pwg.org, http-wg@cuckoo.hpl.hp.com, SERVLET-INTEREST@JAVA.SUN.COM\nFrom: Carl Kugler/Boulder/IBM@IBMUS\nSubject:  Re: IPP> Chunked POST: SUMMARY  (Document link not converted)\n\nRoss Patterson wrote:\n\n>>> All HTTP/1.1 applications that receive entities MUST accept the\n>>> \"chunked\" transfer-coding (section 3.6), thus allowing this mechanism\n>>> to be used for messages when the message length cannot be determined\n>>> in advance.\n>>\n>>Apparently that should be interpreted as \"MUST accept the 'chunked'\n>>TRANSFER-CODING, but NEED NOT accept REQUESTs with that transfer-coding.\"\n\n>Correct - all HTTP 1.1 servers must be able to process requests encoded\n>as chunked data, but they are still allowed to refuse the request for\n>other reasons.\n\nTo me, the presence of the 411 status code means that an HTTP/1.1 server\nMAY refuse to accept a request for the specific reason that entity body is\nencoded with the \"chunked\" transfer-coding:\n\n411 Length Required\nThe server refuses to accept the request without a defined Content-Length.\nThe client MAY repeat the request if it adds a valid Content-Length header\nfield containing the length of the message-body in the request message.\n\n     -Carl\n\n\n\n", "id": "lists-012-9195421"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": ">The IPP WG would really like clarification on this point:  Is the intent of\n>the HTTP/1.1 spec to say that an HTTP/1.1 server MAY reject any request\n>without a defined Content-Length?  This would imply that a conformant\n>HTTP/1.1 server MAY reject any request with the \"chunked\" transfer-coding.\n\nYes.  A conformant HTTP/1.1 server MAY reject any request for any reason,\njust one of them being 411 Length Required.  There would be no reason to\ndefine 411 if it could never be used by a conformant server.  The wording\nin the spec is poor -- it should have said that an HTTP/1.1 application\nis required to understand the \"chunked\" transfer-coding, not accept it,\nsince it is referring to message parsing and not the response status.\n\nWhy is this necessary?  Because an Internet protocol cannot require a\nserver to accept denial of service attacks.\n\n....Roy\n\n\n\n", "id": "lists-012-9206966"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "Thanks, Roy.\n\nI'd like to propose that the wording be clarified in the spec.  I have\nencountered servers that \"accept\" a chunked POST with 200 (OK) and then\nsilently discard the message body, passing a zero-length entity-body to the\nservice layer (CGI or servlet), so I think some implementors are\nmisinterpreting the current wording.\n\nI propose adding something along these lines:\n\"If a server disallows message bodies encoded with the chunked\ntransfer-coding in requests to some resource, it MUST return an error code\nin the response to such requests.  If this is the primary reason for\nrejecting a request, the response MUST contain the 411 (Length Required)\nerror code.\"\n\nAlso, this sentence should be reworded:\n\nSection 4.4, Message Length:\nAll HTTP/1.1 applications that receive entities MUST accept the ?chunked?\ntransfer-coding (section 3.6), thus allowing this mechanism to be used for\nmessages when the message length cannot be determined in advance.\n\nbecomes something like:\n\nAll HTTP/1.1 applications that receive entities MUST understand the\n?chunked? transfer-coding (section 3.6), thus allowing this mechanism to be\nused for messages when the message length cannot be determined in advance.\nThis does NOT mean that servers must accept messages containing bodies\nencoded with the chunked transfer-coding.\n\n\n\n          -Carl\n\n\n\n\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> on 01/22/99 10:54:56 AM\n\nTo:   Carl Kugler/Boulder/IBM\ncc:   http-wg@cuckoo.hpl.hp.com, ipp@pwg.org\nSubject:  Re: Resend: Re: IPP> Chunked POST: SUMMARY\n\n\n\n\n\n>The IPP WG would really like clarification on this point:  Is the intent\nof\n>the HTTP/1.1 spec to say that an HTTP/1.1 server MAY reject any request\n>without a defined Content-Length?  This would imply that a conformant\n>HTTP/1.1 server MAY reject any request with the \"chunked\" transfer-coding.\n\nYes.  A conformant HTTP/1.1 server MAY reject any request for any reason,\njust one of them being 411 Length Required.  There would be no reason to\ndefine 411 if it could never be used by a conformant server.  The wording\nin the spec is poor -- it should have said that an HTTP/1.1 application\nis required to understand the \"chunked\" transfer-coding, not accept it,\nsince it is referring to message parsing and not the response status.\n\nWhy is this necessary?  Because an Internet protocol cannot require a\nserver to accept denial of service attacks.\n\n....Roy\n\n\n\n", "id": "lists-012-9215407"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "An IPP server is not going to be a general-purpose HTTP server,\nand implementing it as a CGI script would be braindead anyway,\nso why does the fact that a general-purpose HTTP server reject\nsome non-length-given POST requests matter to IPP?\n\nIn other words, if you are already assuming that the destination\nserver can handle an IPP message, why can't you assume that it can\nalso handle chunked input?  The fact that it isn't required by HTTP\nin general doesn't mean you can't require it for particular resources.\nOr even for any IPP gateway.  Apache, for instance, includes the\nability to parse and forward chunked input within the core server --\nit is only the mod_cgi module that won't accept it.\n\nThe denial of service case exists for any service.  Basically, the\nservice needs some mechanism of rejecting a request and giving a\nreason for that rejection, preferably providing a means for the client\nto workaround the limitation if it is legitimate.  The 411 and 413\nresponse codes describe two such reasons.\n\n....Roy\n\n\n\n", "id": "lists-012-9226713"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "Roy, would implementing it as a Java servlet be braindead?  Should every\napplication that uses HTTP as a transport build in its own HTTP\nimplementation, even if one is already available on the platform?  Even in\nthe future?\n\n     -Carl\n\n\n\nhttp-wg@hplb.hpl.hp.com on 01/22/99 02:52:23 PM\n\nTo:   \"Hastings, Tom N\" <hastings@cp10.es.xerox.com>\ncc:   Carl Kugler/Boulder/IBM, http-wg@cuckoo.hpl.hp.com, ipp@pwg.org\nSubject:  Re: Resend: Re: IPP> Chunked POST: SUMMARY\n\n\n\n\n\nAn IPP server is not going to be a general-purpose HTTP server,\nand implementing it as a CGI script would be braindead anyway,\nso why does the fact that a general-purpose HTTP server reject\nsome non-length-given POST requests matter to IPP?\n\nIn other words, if you are already assuming that the destination\nserver can handle an IPP message, why can't you assume that it can\nalso handle chunked input?  The fact that it isn't required by HTTP\nin general doesn't mean you can't require it for particular resources.\nOr even for any IPP gateway.  Apache, for instance, includes the\nability to parse and forward chunked input within the core server --\nit is only the mod_cgi module that won't accept it.\n\nThe denial of service case exists for any service.  Basically, the\nservice needs some mechanism of rejecting a request and giving a\nreason for that rejection, preferably providing a means for the client\nto workaround the limitation if it is legitimate.  The 411 and 413\nresponse codes describe two such reasons.\n\n....Roy\n\n\n\n", "id": "lists-012-9235630"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "On Fri, 22 Jan 1999 kugler@us.ibm.com wrote:\n\n> \n> \n> Roy, would implementing it as a Java servlet be braindead?  Should every\n> application that uses HTTP as a transport build in its own HTTP\n> implementation, even if one is already available on the platform?  Even in\n> the future?\n> \n\nThis discussion is getting off track.  There is a problem here and it\nmay well impinge on IPP.  But the problem is not with the HTTP spec.\n\nThe problem is that there is no way to use CGI with chunked message-bodies.\nThis is can be viewed as a limitation of CGI (CGI's problem) or a \nlimitation of the most popular implementations of HTTP (HTTP implementor's\nproblem).  But it is not a problem with the HTT Protocol.  HTTP does\nnot prevent using chunking with CGI and it is not hard to find servers\nwhich support this, even though the most popular ones do not.\n\nYou might also blame the CGI protocol for not permitting the use of\nchunked input.  This could only be changed in a new version of CGI.\n\nIt is not very helpful to tell people what they want to do is braindead.\nObviously, some uses of CGI are useful and appropriate and some are not.\nAlso this really has nothing to do with denial of service which can be\ndone in lots of ways more easily than using chunking.\n\nI am not sure what recourse people have at this point.  You could try\nto persuade Apache developers to implement this feature.  I am not\nsure if it would be possible to write an Apache module to do this.\nYou could also try to support a new version of the CGI spec which would\npermit CGI to take chunked input.  Neither of these would deal with the\nexisting base of installed servers, though.\n\nBut the one thing which does seem clear is that no change or\nclarification in the HTTP spec can can help.\n\nIn that regard, I would suggest that a server which rejects chunked\nmessage-body but returns a 200 status is not in compliance with the\nspec as it stands now.\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n", "id": "lists-012-9246682"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": ">>An IPP server is not going to be a general-purpose HTTP server,\n>>and implementing it as a CGI script would be braindead anyway,\n>\n>That sounds like a very considered opinion, can you elucidate?\n>\n>For the record, we've implemented IPP as CGI (well, a set of C++ functions\n>called by our general purpose embedded web server) and all seems to be\n\nThat is not CGI, nor is an embedded web server general-purpose.\nAny server where you have control over the nature of the resources\nback-end is not general-purpose.\n\nThe reason it doesn't make any sense to implement IPP via a CGI script\nis because the server is being installed specifically to listen to IPP\nrequests on the IPP port, and thus doesn't need to deal with the\nmultifaceted heterogenerity of resources on a general-purpose HTTP server.\nIt therefore doesn't make sense to have an IPP implementation that is\ngeneric among many servers (the ONLY reason to ever use CGI).\n\n>running fine. I can think of many reasons why you'd want to base it on an\n>existing web server, not the least of which is keeping the code size\n>reasonable. I would imagine that most printer OEMs would do a lot of soul\n>searching before deciding to use two separate instances of much the same\n>code in a printer...\n\nYour own implementation uses a server-specific API on a specific set\nof pre-allocated printer resources within an embedded system.  That is\nexactly what I would expect of an IPP implementation, and there is no\nreason why you can't support chunked requests.  My point was that it is\nsilly to limit IPP capabilities to the lowest common denominator of\nexisting HTTP origin servers, since IPP isn't an existing HTTP service.\n\n....Roy\n\n\n\n", "id": "lists-012-9257556"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": ">Roy, would implementing it as a Java servlet be braindead?\n\nThat depends on the servlet API.\n\n>Should every\n>application that uses HTTP as a transport build in its own HTTP\n>implementation, even if one is already available on the platform?\n\nNo application should use HTTP as a transport.  If it used HTTP as HTTP,\nthere wouldn't be any need for a separate implementation, but it would\nstill be desirable to base the service on the best capabilities of HTTP\nand not on the characteristics that might be true of some other resources\nthat are accessible via HTTP.\n\n....Roy\n\n\n\n", "id": "lists-012-9266806"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": ">This discussion is getting off track.  There is a problem here and it\n>may well impinge on IPP.  But the problem is not with the HTTP spec.\n>\n>The problem is that there is no way to use CGI with chunked message-bodies.\n>This is can be viewed as a limitation of CGI (CGI's problem) or a \n>limitation of the most popular implementations of HTTP (HTTP implementor's\n>problem).  But it is not a problem with the HTT Protocol.  HTTP does\n>not prevent using chunking with CGI and it is not hard to find servers\n>which support this, even though the most popular ones do not.\n\nAgreed.\n\n>Also this really has nothing to do with denial of service which can be\n>done in lots of ways more easily than using chunking.\n\nOne of many is still one.  It is the reason that this is hard to\nimplement correctly on a general-purpose server.  If it wasn't for that\nreason I would have implemented it last year for 1.2.\n\n>I am not sure what recourse people have at this point.  You could try\n>to persuade Apache developers to implement this feature.  I am not\n>sure if it would be possible to write an Apache module to do this.\n\nA configurably limited input buffer that redirects the request body\nbefore calling the script could either be implemented in the core\n(where dechunking is already being done) or within a mod_cgi replacement\n(duplicates effort, but certainly do-able).  OTOH, it is easier to just\nimplement IPP as a module.\n\n>You could also try to support a new version of the CGI spec which would\n>permit CGI to take chunked input.  Neither of these would deal with the\n>existing base of installed servers, though.\n\nJust replace mod_cgi with something that passes chunked to the script --\nit is only a one word change, but requires scripts that can parse chunked.\n\n>But the one thing which does seem clear is that no change or\n>clarification in the HTTP spec can can help.\n>\n>In that regard, I would suggest that a server which rejects chunked\n>message-body but returns a 200 status is not in compliance with the\n>spec as it stands now.\n\nYep, since the status shouldn't be OK if the action wasn't successfully\nperformed, and you can't perform a POST successfully without understanding\nthe request body.\n\n....Roy\n\n\n\n", "id": "lists-012-9275758"}, {"subject": "ProxyConnection header definition", "content": "Could someone tell me where is Proxy-Connection header defined? It doesn't\nappear in RFC 1945, nor in RFC 2068, nor in draft-ietf-http-v11-spec-rev-06.\n\n-- \n .-.   .-.    Life is a sexually transmitted disease.\n(_  \\ /  _)\n     |        dave@fly.cc.fer.hr\n     |\n\n\n\n", "id": "lists-012-9285849"}, {"subject": "RE: ProxyConnection header definition", "content": "AFAIK it isn't; squid uses it to no effect.\n\n\n> -----Original Message-----\n> From: Drazen Kacar [mailto:dave@fly.cc.fer.hr]\n> Sent: Monday, January 25, 1999 4:22 PM\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Proxy-Connection header definition?\n> \n> \n> Could someone tell me where is Proxy-Connection header \n> defined? It doesn't\n> appear in RFC 1945, nor in RFC 2068, nor in \n> draft-ietf-http-v11-spec-rev-06.\n\n\n\n", "id": "lists-012-9294002"}, {"subject": "Re: ProxyConnection header definition", "content": "????????????????\n\n????????????????????????????????????????????!\n\n:-) ??????????????????????????????????????????????????????????????????????\n????????????\n\n:-D ????????????????????\n\n:->:> ??-??-??????????????????????\n\n8-) ??????????????????????????????\n\n;-) ??????????????????\n\n:-0 ????????????????????????????????\n\n:-() ??????????????\n\n:-o ??????????????????????????????\n\n|-) ??????????????????????????????????????\n\n|-D ????????\n\n|-P ??????????\n\n:-< ????????????????\n\n8:-) ????????????????????????????????????????????????????????\n???????????????????????????? ????????????\n\n-( ????????????????????????????\n\n!-) ????????????????????????????????????????\n\n#-) ????????????????????????????\n\n#:-) ??????????????????????????????????\n\n:-9 ??????????????????\n\n%-) ??????????????????????????????????????????\n\n%-} ????????????????????????????\n\n:-{ ????????????????????????????\n\n':-) ????????????????????????????????????\n\n(-) ??????????????????????????????????\n\n&:-) ??????????????\n\n{:-) ????????????????????\n\n(-: ??????????????????\n\n(-_-) ????????????????????????????\n\n(:)-) ????????????????????????????????????????\n\n(:-* ????????????????????????\n\n(:-& ????????????????????\n\n(:-( ????????????????????????????????????????????\n\n(:-) ????????????????????????????????????\n\n(:-* Kiss????\n\n(:-???? ??????????\n\n(:-D) ??????????????????\n\n(:-|k- ????????????????????????????????????????????????\n\n:-|K- ????????????????????\n\n(:<) ??????????\n\n(:>>-< ??????????????????????\n\n(:I ????????\n\n(:^^( ??????????????????????????\n\n(0--< ????????????????????\n\n*-( ??????????????????????????????\n\no-) ??????????????????????????????\n\n@-) ??????????????????????\n\n*:** ????????????????????????????????????????????\n\n*:o) ??????\n\n*<|:-) ??????????\n\n+-(:-) ??????????????????\n\n+:-) ????????????????????????????????\n\n+<:-| ????????????????????\n\n,-) ??????????????????????????????????????????\n\n,-} ??????????????????????????PASS??\n\n.-| ????????????????????????\n\n..._... SOS??????\n\n0-) ????????????????????????????????????????????\n\n3:=9 ??????????????????\n\n8:-I ????????????????????????????????????\n\n8:] ??????????????????????????\n\n:{ ??????????????????????\n\n:%)% ??????????????\n\n:*) ??????????????????????????????????????\n\n:-! ????????????????????\n\n:-\" ??????????\n\n:-# ????????????????????????????????????????????????????????\n\n:-#| ??????????????????????????\n\n:-* ??????????Put your money where your mouth is.\n\n:-% ????????????????????????\n\n:-& ????????????????????????????\n\n:-') ????????\n\n:-'| ????????????????????????????\n\n:-( ??????????????????\n\n:-(*) ????????????\n\n:-(=) ????????????????????????????\n\n^^ ????????????????????????????????????????????\n\n^^-^^ ????????????????????????\n\n^^_< @_@ ??????????\n\n..~^^.^^~... ??????????????????????????\n\n...~*.*~... ??????????????????????????\n\n^^ ????????????\n\n=^^-^^= ??????????\n\n?_? ??????????????????????????\n\n~~~~>_<~~~~ ??????????????\n\n:-)' ????????????????\n\n:-)-{8 ??????????????????\n\n:-)8 ??????????????????????????????\n\n:-* ??????????????????????\n\n:-/ ????????????????\n\n:-0 ??????????????\n\n:-1 ??????????????\n\n:-6 ????????????????????\n\n:-7 ????????????????????????????????????\n\n:-8( ????????????????????????\n\n:-{ ) ????????????????\n\n:-}) ????????????????\n\n:-<) ??????????????????\n\n:-=) ??????????????????\n\n:-@ ??????????????\n\n:-$ ????????\n\n:-? ????????\n\n:-q ??????????\n\n:-e ????????????\n\n:-I Hmm????\n\n:-i ??????????????????\n\n:-j ????????????\n\n:-P ????????\n\n:-Q ??????????????\n\n:-x ????????????????\n\n:-X ??????????\n\n:-[ ??????????????????????????\n\n:-\\ ????????????\n\n:-] ??????\n\n:-` ??????????????????????????\n\n:-{#} ????????????????\n\n:-{~ ??????????????????????????????\n\n:-| ??????????????????????????\n\n:-} ????????????????????\n\n:<| ????????????????????????????\n\n:=) ????????????????????????\n\n:=| ??????????????????????\n\n:>) ??????????????????\n\n:~) ??????????????????????????\n\n;-\\ ????????????????????\n\n<:-)<<| ??????????????????????????????\n\n<:I ????????\n\n<<:>>== ??????????????????????????????\n\n<<<<(:-) ????????????????????????????????\n\n<I==I) ????????????????\n\n<{:-)} ????????????????????\n\n=:-) ??????????????\n\n=:-( ????????????????????????????\n\n=:-#} ??????????????????\n\n>-r ????????????????????\n\n>:-< ??????????????????????????????????????????\n\n>>-( ??????????????????????????????????????\n\n~~:-( ??????????????????????????\n\n?-( ??????????????????????\n\n@:-) ??????????????\n\n@= ????????????????????????????\n\n@>>--->--- ??????????????????????\n\nB-) ??????????????????\n\nE-:-I ??????????????????\n\ni-) ????????????????\n\n0-) ????????????\n\nx-< ????????????????????\n\n[:-) ??????????????????????????\n\n[:|] ????????\n\n{(:-) ??????????????????????????\n\n}(:-( ????????????????\n\n}:-) ????????????????????????\n\n|-( ????????????????\n\n<@_@> ????\n\n@%&$%& ????????\n\n\n\n", "id": "lists-012-9302618"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "  <9901240030.aa0531-@paris.ics.uci.edu> wrote:\nOriginal Article: http://www.egroups.com/list/http-wg/?start=8506\n> >Roy, would implementing it as a Java servlet be braindead?\n>\n> That depends on the servlet API.\nUnfortunately, the Servlet API spec is silent on the issues of\nContent-Length and chunking.\n\n>\n> >Should every\n> >application that uses HTTP as a transport build in its own HTTP\n> >implementation, even if one is already available on the platform?\n>\n> No application should use HTTP as a transport.\n\nI don't think everyone would agree with that statement.  See\n\nNetwork Working Group                                        Keith Moore\nInternet-Draft                                   University of Tennessee\n5 August 1998                                           Patrik Faltstrom\nExpires: 5 February 1999                                           Tele2\n         On the use of HTTP as a Substrate for Other Protocols\n                      draft-iesg-using-http-00.txt\n\n\n> If it used HTTP as HTTP,\n> there wouldn't be any need for a separate implementation, but it would\n> still be desirable to base the service on the best capabilities of HTTP\n> and not on the characteristics that might be true of some other resources\n> that are accessible via HTTP.\n>\n> ....Roy\n>\n\n\n\n", "id": "lists-012-9314649"}, {"subject": "World's smallest Web server...", "content": "Anyone top this one?  http://wearables.stanford.edu/\n\nAnd of course, it speaks HTTP/1.1...  \n\nThe Stanford gadget looks smaller\nthan Itsy (see: http://www.research.digital.com/wrl/itsy/index.html),\nwhich has a display, on the other hand.\nItsy runs off of triple A cells, at much lower power...,\nso Itsy is clearly the champ on hits/joule...\n\n\n- Jim\n\n\n\n", "id": "lists-012-9323783"}, {"subject": "Re: ProxyConnection header definition", "content": "AFAIK, it is just a Netscape hack.  Too bad it never worked properly and\nhas problems inherent in the very way it was defined.\n\nOn Mon, 25 Jan 1999, Drazen Kacar wrote:\n\n> Could someone tell me where is Proxy-Connection header defined? It doesn't\n> appear in RFC 1945, nor in RFC 2068, nor in draft-ietf-http-v11-spec-rev-06.\n\n\n\n", "id": "lists-012-9330900"}, {"subject": "RE: ProxyConnection header definition", "content": "I too am unaware of a formal definition but we implement it in IE 4.0 and IE\n5.0. I also believe we implement it in IE 3.0 but am too lazy to go back and\ncheck. It has worked out quite well for us allowing us to authenticate\nagainst HTTP/1.0 proxies without worrying about breaking against 1.0 proxies\nthat don't support the mechanism and thus always pass the header on.\n\nYaron\n\n> -----Original Message-----\n> From: Marc Slemko [mailto:marcs@znep.com]\n> Sent: Monday, January 25, 1999 9:19 PM\n> To: Drazen Kacar\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Proxy-Connection header definition?\n> \n> \n> AFAIK, it is just a Netscape hack.  Too bad it never worked \n> properly and\n> has problems inherent in the very way it was defined.\n> \n> On Mon, 25 Jan 1999, Drazen Kacar wrote:\n> \n> > Could someone tell me where is Proxy-Connection header \n> defined? It doesn't\n> > appear in RFC 1945, nor in RFC 2068, nor in \n> draft-ietf-http-v11-spec-rev-06.\n> \n\n\n\n", "id": "lists-012-9338621"}, {"subject": "RE: ProxyConnection header definition", "content": "On Tue, 26 Jan 1999, Yaron Goland wrote:\n\n> I too am unaware of a formal definition but we implement it in IE 4.0 and IE\n> 5.0. I also believe we implement it in IE 3.0 but am too lazy to go back and\n> check. It has worked out quite well for us allowing us to authenticate\n\nAuthenticate?\n\n> against HTTP/1.0 proxies without worrying about breaking against 1.0 proxies\n> that don't support the mechanism and thus always pass the header on.\n\nNo.  The whole problem is that it doesn't work with proxies that don't\nunderstand it if you have anything more complex than client to\nproxy to origin server.\n\nFrom what I know, Netscape wanted a quick hack to let them do\npersistent connections to proxy servers without having to have all\nclients and proxies and servers support it.  Unfortunately, their\nquick hack was too hacky and quickly falls apart when you have chains\nof proxies.\n\nC(a) -> P(b) -> P(c) -> O(d)\n\nC is client, P is proxy, O is origin server.\n\na sends a Proxy-Connection to b.  b doesn't understand the Proxy-Connection\nheader, so it passes it on.  c understands it, so it tries to do a \npersistent connection with b.  Things break.\n\nWhat the Proxy-Connection header has done is require that every\nproxy which can pass requests to an \"upstream\" proxy of some type\nknow about it or be \"broken\".  In reality, it is the concept of the\nProxy-Connection header that is broken, but given that clients implement\nit and other proxies implement it, the only thing this poor little \nstandards conforming proxy can do is deal with it as well. \n\nIt doesn't, of course, have to do anything other than drop it from \nforwarded requests, but that still involves dealing with it.\n\nIn fact, you even run into this problem sometimes in situations\nwhere the proxy thinks it is talking directly to the origin server,\ndue to hacks like broken \"transparent proxies\" (which, of course, by\ndefinition are broken), broken \"reverse proxies\" that think they are\nstill proxies (eg. www.novell.com used to be behind one), etc.\nThese cases are different though, since it is much more arguable\nthat the \"normal\" proxy isn't the thing that should be fixed.\n\n\n\n", "id": "lists-012-9347966"}, {"subject": "Re: World's smallest Web server...", "content": "This What about this one:\nhttp://ernst.informatik.uni-ulm.de/web332/\n\nIt is 2x2 in, 1.5x1.5 inch core logic without I/O pins\n\nIt connects directly to a modem. Contains PPP server, TCP/IP and HTTP.\n\n>Anyone top this one?  http://wearables.stanford.edu/\n>\n>And of course, it speaks HTTP/1.1...\n>\n>The Stanford gadget looks smaller\n>than Itsy (see: http://www.research.digital.com/wrl/itsy/index.html),\n>which has a display, on the other hand.\n>Itsy runs off of triple A cells, at much lower power...,\n>so Itsy is clearly the champ on hits/joule...\n>\n>\n>                - Jim\n\n----\nDistributed Systems Dept.                    voice: +49 731 502 4145\nUniversity of Ulm                        ethernet: 08:00:20:12:2a:01\n89069 Ulm                             Cobrow: http://www.cobrow.com/\nGermany           Live: http://www.cobrow.com/pages/people/wolf.html\n\n\n\n", "id": "lists-012-9357605"}, {"subject": "Re: World's smallest Web server...", "content": "Looks pretty small to me.  But we'll have to get you and Stanford to\ncompare notes on exactly who is smaller, as theirs is pretty tiny\nas well.\n\nSo far in the race we have:\nStanford and ULM for smallest size\nCompaq Itsy for slightly larger, but much faster.\n\nAnd we can now generate a bunch more bogus performance metrics, like\nhits/joule of energy, Hits/cubic CC/second, and other fun useless metrics\nof performance.\n\n- Jim\n\n\n\n", "id": "lists-012-9365832"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "On Sun, 24 Jan 1999, Roy T. Fielding wrote:\n> >Should every\n> >application that uses HTTP as a transport build in its own HTTP\n> >implementation, even if one is already available on the platform?\n> \n> No application should use HTTP as a transport.\n\nI _strongly_ agree.\n\n> If it used HTTP as HTTP,\n> there wouldn't be any need for a separate implementation, but it would\n> still be desirable to base the service on the best capabilities of HTTP\n> and not on the characteristics that might be true of some other resources\n> that are accessible via HTTP.\n\nWell said.\n\n- Chris\n\n\n\n", "id": "lists-012-9373072"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "On Tue, 26 Jan 1999, Chris Newman wrote:\n\n> On Sun, 24 Jan 1999, Roy T. Fielding wrote:\n> > >Should every\n> > >application that uses HTTP as a transport build in its own HTTP\n> > >implementation, even if one is already available on the platform?\n> > \n> > No application should use HTTP as a transport.\n> \n\nAs an absolute statement, that's absurd. Everything done with HTTP is\nrunning applications. It is myoptic to claim that a CGI program is\nanything other than an application which uses HTTP as the transport.\nThe CGI program in conjunction with its host HTTP server must follow\nthe HTTP protocol to transport requests to the server and responses\nback.\n\nThe issue is to define the characteristics of applications which are\nappropriate for implementation using HTTP.\n\nConversely, given the availability of a high quality HTTP implementation\nfor the application context, finding a need to replace that\nimplementation would make me wonder if HTTP is an appropriate transport\nchoice.  OTH, there a lot of parameters to optimize and some \napplications may be better served with a custom HTTP implementation.\nFor example, if the application is to serve web resources based on a\nRDBMS, one might make different implementation choices than if the OS file \nsystem is the data respository.\n\nDave Morris\n\n\n\n", "id": "lists-012-9381558"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": ">> > No application should use HTTP as a transport.\n>\n>As an absolute statement, that's absurd. Everything done with HTTP is\n>running applications. It is myoptic to claim that a CGI program is\n>anything other than an application which uses HTTP as the transport.\n\nThe Web uses HTTP as a transfer protocol, not a transport protocol.\nHTTP includes application semantics and any application that conforms\nto those semantics while using HTTP is also using it as a transfer\nprotocol.  Those that don't are using it as a transport protocol,\nwhich is just a waste of bytes.\n\nThis is a topic for some future IETF hallway conversation, not two\nstandards mailing list, so I apologize in advance for not continuing\nthis further.\n\n....Roy\n\n\n\n", "id": "lists-012-9391396"}, {"subject": "RE: ProxyConnection header definition", "content": "Sorry, a slip on the term authenticate. A persistent connection is needed\nfor some of our authentication algorithms to work properly.\n\nAs for the problems with the header, yes I am aware of the issues. I wasn't\nactually around when the feature was put in so I'm not sure what the\narguments were that lead us to adopt it. I suspect they went something like\n\"Netscape is doing it so we have to do it.\"\n\nYaron\n\n> -----Original Message-----\n> From: Marc Slemko [mailto:marcs@znep.com]\n> Sent: Tuesday, January 26, 1999 12:39 PM\n> To: Yaron Goland\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: RE: Proxy-Connection header definition?\n> \n> \n> On Tue, 26 Jan 1999, Yaron Goland wrote:\n> \n> > I too am unaware of a formal definition but we implement it \n> in IE 4.0 and IE\n> > 5.0. I also believe we implement it in IE 3.0 but am too \n> lazy to go back and\n> > check. It has worked out quite well for us allowing us to \n> authenticate\n> \n> Authenticate?\n> \n> > against HTTP/1.0 proxies without worrying about breaking \n> against 1.0 proxies\n> > that don't support the mechanism and thus always pass the header on.\n> \n> No.  The whole problem is that it doesn't work with proxies that don't\n> understand it if you have anything more complex than client to\n> proxy to origin server.\n> \n> From what I know, Netscape wanted a quick hack to let them do\n> persistent connections to proxy servers without having to have all\n> clients and proxies and servers support it.  Unfortunately, their\n> quick hack was too hacky and quickly falls apart when you have chains\n> of proxies.\n> \n> C(a) -> P(b) -> P(c) -> O(d)\n> \n> C is client, P is proxy, O is origin server.\n> \n> a sends a Proxy-Connection to b.  b doesn't understand the \n> Proxy-Connection\n> header, so it passes it on.  c understands it, so it tries to do a \n> persistent connection with b.  Things break.\n> \n> What the Proxy-Connection header has done is require that every\n> proxy which can pass requests to an \"upstream\" proxy of some type\n> know about it or be \"broken\".  In reality, it is the concept of the\n> Proxy-Connection header that is broken, but given that \n> clients implement\n> it and other proxies implement it, the only thing this poor little \n> standards conforming proxy can do is deal with it as well. \n> \n> It doesn't, of course, have to do anything other than drop it from \n> forwarded requests, but that still involves dealing with it.\n> \n> In fact, you even run into this problem sometimes in situations\n> where the proxy thinks it is talking directly to the origin server,\n> due to hacks like broken \"transparent proxies\" (which, of course, by\n> definition are broken), broken \"reverse proxies\" that think they are\n> still proxies (eg. www.novell.com used to be behind one), etc.\n> These cases are different though, since it is much more arguable\n> that the \"normal\" proxy isn't the thing that should be fixed.\n> \n\n\n\n", "id": "lists-012-9399718"}, {"subject": "Re: ProxyConnection header definition", "content": "Yaron Goland wrote:\n> \n> Sorry, a slip on the term authenticate. A persistent connection is needed\n> for some of our authentication algorithms to work properly.\n> \n\nDo you happen to know which authentication algorithms you use that\nrequire persistent connections???\n\n> >\n> > What the Proxy-Connection header has done is require that every\n> > proxy which can pass requests to an \"upstream\" proxy of some type\n> > know about it or be \"broken\".  In reality, it is the concept of the\n> > Proxy-Connection header that is broken, but given that\n> > clients implement\n> > it and other proxies implement it, the only thing this poor little\n> > standards conforming proxy can do is deal with it as well.\n> >\n> > It doesn't, of course, have to do anything other than drop it from\n> > forwarded requests, but that still involves dealing with it.\n> >\n\nSo does that mean that a proxy that knows about the Proxy-Connection\nheader should remove it from the request message and should not try to\nset up a persistent connection??? Or is there another way that you can\ndeal with it???\n\nCheers,\nKal.\n\n-- \n    .      Kalvinder Singh                     singh@ozy.dec.com\n _-_|\\     Software Engineering Australia\n/     \\<-- Compaq       \n\\_.-._/    Research Park, Bond University      Phone: +61 7 5575 0106\n     v     Gold Coast, Qld, 4229, Australia    Fax:   +61 7 5575 0100\n\n\n\n", "id": "lists-012-9410936"}, {"subject": "Re: ProxyConnection header definition", "content": "On Wed, 27 Jan 1999, Kalvinder Singh wrote:\n\n> So does that mean that a proxy that knows about the Proxy-Connection\n> header should remove it from the request message and should not try to\n> set up a persistent connection??? Or is there another way that you can\n> deal with it???\n\nI guess there is a simpler way of stating this.\n\nAll proxies should always strip the Proxy-Connection header from requests\nthat are forwarded.  ie. it is a hop-by-hop header.  This is one of the\nseveral undocumented HTTP \"must do's\" that software needs to deal with.\n\nIn addition to the above, if that proxy is capable, it can act on the\nProxy-Connection header to control the connection between the client and\nthe proxy as appropriate, if it wants to.  It should not act on the\nProxy-Connection header if it isn't a \"real\" proxy; ie. the client does\nnot know that it is talking to a proxy.\n\n\n\n", "id": "lists-012-9420238"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": ">An IPP server is not going to be a general-purpose HTTP server,\n>and implementing it as a CGI script would be braindead anyway,\n\nThat sounds like a very considered opinion, can you elucidate?\n\nFor the record, we've implemented IPP as CGI (well, a set of C++ functions\ncalled by our general purpose embedded web server) and all seems to be\nrunning fine. I can think of many reasons why you'd want to base it on an\nexisting web server, not the least of which is keeping the code size\nreasonable. I would imagine that most printer OEMs would do a lot of soul\nsearching before deciding to use two separate instances of much the same\ncode in a printer...\nCheers,\nNick Webb\nAuco, Inc.\n\n\n\n", "id": "lists-012-9428575"}, {"subject": "RE: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "Roy and Carl,\n\nI am now quite worried that there may be many cases of HTTP servers refusing\nto accept chunked POST requests.  Rejection is not just because of CGI\nscripts.  Apparently, denial of service is a reason that a conforming\nHTTP/1.1 server might reject chunked encoding POST.  Doesn't such rejection\njeopardize interoperability with IPP clients that send chunked POST\nrequests?\n\nFirst, I'd like to understand more about such denial of service, since I\nwould think that a server would just timeout if the client didn't send the\nnext chunk in a reasonable amount of time.  Also if the client sent too much\ndata with multiple chunks, the server could reject the next chunk with the\n413 error code.  So I don't understand the denial of service reason to allow\na conforming HTTP/1.1 server to reject a chunked request and am seeking\nenlightenment.\n\nThanks,\nTom\n\n\n>-----Original Message-----\n>From: Roy T. Fielding [mailto:fielding@kiwi.ics.uci.edu]\n>Sent: Friday, January 22, 1999 09:55\n>To: kugler@us.ibm.com\n>Cc: http-wg@cuckoo.hpl.hp.com; ipp@pwg.org\n>Subject: Re: Resend: Re: IPP> Chunked POST: SUMMARY \n>\n>\n>>The IPP WG would really like clarification on this point:  Is \n>the intent of\n>>the HTTP/1.1 spec to say that an HTTP/1.1 server MAY reject \n>any request\n>>without a defined Content-Length?  This would imply that a conformant\n>>HTTP/1.1 server MAY reject any request with the \"chunked\" \n>transfer-coding.\n>\n>Yes.  A conformant HTTP/1.1 server MAY reject any request for \n>any reason,\n>just one of them being 411 Length Required.  There would be no \n>reason to\n>define 411 if it could never be used by a conformant server.  \n>The wording\n>in the spec is poor -- it should have said that an HTTP/1.1 application\n>is required to understand the \"chunked\" transfer-coding, not accept it,\n>since it is referring to message parsing and not the response status.\n>\n>Why is this necessary?  Because an Internet protocol cannot require a\n>server to accept denial of service attacks.\n>\n>....Roy\n>\n\n\n\n", "id": "lists-012-9437058"}, {"subject": "RE: ProxyConnection header definition", "content": "Sounds about right.\n\n> -----Original Message-----\n> From: Marc Slemko [mailto:marcs@znep.com]\n> Sent: Wednesday, January 27, 1999 1:18 AM\n> To: Kalvinder Singh\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Proxy-Connection header definition?\n> \n> \n> On Wed, 27 Jan 1999, Kalvinder Singh wrote:\n> \n> > So does that mean that a proxy that knows about the Proxy-Connection\n> > header should remove it from the request message and should \n> not try to\n> > set up a persistent connection??? Or is there another way \n> that you can\n> > deal with it???\n> \n> I guess there is a simpler way of stating this.\n> \n> All proxies should always strip the Proxy-Connection header \n> from requests\n> that are forwarded.  ie. it is a hop-by-hop header.  This is \n> one of the\n> several undocumented HTTP \"must do's\" that software needs to \n> deal with.\n> \n> In addition to the above, if that proxy is capable, it can act on the\n> Proxy-Connection header to control the connection between the \n> client and\n> the proxy as appropriate, if it wants to.  It should not act on the\n> Proxy-Connection header if it isn't a \"real\" proxy; ie. the \n> client does\n> not know that it is talking to a proxy.\n> \n\n\n\n", "id": "lists-012-9449135"}, {"subject": "HTTP/1.1 revision 6 concerns/issue", "content": "Jim Gettys told me I should address these items to this mailing list for\npossible additiion to your errata list.  There are five items I noticed,\nof varying significance to the spec.\n\n\n1. (technical-ish)\n\nThe \"TE\" header should be added to the list of hop-by-hop headers in\nsection 13.5.1.  This is implied by the Connection-header protection\nmandated in 14.39 (the TE definition), but for completeness it should be\npresent in both places.\n\n2. (editorial)\n\nSection 8.2.3, first sentance: \"an client\" should be \"a client\"\n\n3. (editorial, significant)\n\nSection 8.2.4 refers to section 8.2.3 for guidelines on when a client\nshould retry a request.  The section 8.2.3 it refers to was deleted in\nrevision 6; 8.2.3 in Rev6 discusses the 100 (Continue) Status.\n\n4. (editorial)\n\nSection 8.2.3, the second bullet under \"Requirements for HTTP/1.1 \nproxies\", I assume this requirement is subject to the same conditions as\nthe first bullet, but this could be made clear.\n\n5. (technical?)\n\nSection 12.2, paragraph 1 refers to the specification reserving the header\nname \"Alternates\", but this header only appears in RFC2068 and in the\nbackward compatibility section (19.6.3) where it is mentioned in passing.\nThe suggestion that \"Alternates\" is a viable way to select the best\nrepresentation when Alternates is (presumably) deprecated and (clearly)\nnot defined in the current specification may be out-of-place.  (If nothing\nelse, a reference to RFC2068 would be appropriate since the only other\nmention of Alternates in this draft is in 19.6.3 which says \"we removed\nit.\")\n\n\nThanks for your attention,\nAdam\n--\nYour lives aren't small, but    \\\\ Adam Davenport Bradley,  Grad Student\nyou're living them in a small    \\\\ Boston University   Computer Science\nway. Live openly and expansively! \\\\ artdodge@cs.bu.edu  353-8921/MCS211\nII Cor 6:12-13 (The Message)  <><  \\\\ http://www.netwinder.org/~artdodge\n\n\n\n", "id": "lists-012-9458296"}, {"subject": "Refresh: header specs", "content": "I can't find any mention of the Refresh: header - implemented by every\nbrowser that I can think of - anywhere in draft-ietf-http-v11-spec-rev-06.\n\nShould it be there?  Will it be there?\n\n\n\n", "id": "lists-012-9467040"}, {"subject": "RE: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "On Tue, 26 Jan 1999, Chris Newman wrote:\n\n> On Sun, 24 Jan 1999, Roy T. Fielding wrote:\n> > >Should every\n> > >application that uses HTTP as a transport build in its own HTTP\n> > >implementation, even if one is already available on the platform?\n> > \n> > No application should use HTTP as a transport.\n> \nWell, I WAS hoping this statement didn't cover surfing the Web:-}\n\nSpencer\n\n\n\n", "id": "lists-012-9473385"}, {"subject": "GET or method (HTTP 1.1 rev 6", "content": "Hello,\n\nRFC 2068) and I am just wondering, whether all the GETs in 10.3.5 are there\nintentionally. This doesn't look very consistent. While 14.25 (14.24)\nhas been discussed concerning the use of \"GET\", \"GET or HEAD\", and\n\"method\", I didn't find anything discussing the implications on\n10.3.5 in the mailing list archive.\n\nsection 10.3.5 (304 Not Modified):\n  If the client has performed a conditional _GET_ request, [...]\n\nand section 14.25 (If-Modified-Since) (this was restricted to GET in earlier \nversions (notably RFC2068)):\n  The If-Modified-Since request-header field is used with a _method_ to\n  make it conditional: if the requested variant has not been modified [...]\n  a 304 (not modified) response will be returned [...].\n \nsection 14.26 (If-None_match) talkes about \"GET and HEAD\" concerning the\n  304 status.\n\nJacob\n\n-- \n                                             Jacob Schroeder\n                                             Dipl. Informatiker\n                                             eMail: jschroeder@becomsys.de\n\n\n\n", "id": "lists-012-9482016"}, {"subject": "Re: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "John Franks wrote:\n> \n> The problem is that there is no way to use CGI with chunked message-bodies.\n> This is can be viewed as a limitation of CGI (CGI's problem) or a\n> limitation of the most popular implementations of HTTP (HTTP implementor's\n> problem).\n:\n> You might also blame the CGI protocol for not permitting the use of\n> chunked input.  This could only be changed in a new version of CGI.\n:\n> You could also try to support a new version of the CGI spec which would\n> permit CGI to take chunked input.  Neither of these would deal with the\n> existing base of installed servers, though.\n\nChunking is on the to-discuss list for CGI/1.1+.  We're waiting until\nthe CGI/1.1 stuff is complete before really gearing up on the next\nversion.  The list is currently quiet, but please feel free to\njoin it; see <http://Web.Golux.Com/coar/cgi/>.\n-- \n#kenP-)}\n\nKen Coar                    <http://Web.Golux.Com/coar/>\nApache Group member         <http://www.apache.org/>\n\"Apache Server for Dummies\" <http://Web.Golux.Com/coar/ASFD/>\n\n\n\n", "id": "lists-012-9489721"}, {"subject": "Editorial note and question on connection", "content": "First an editorial point on Rev 6 of the 1.1 spec:\n\nThere seems to be a missing word in the second sentence of\nsection 8.1.4 Practical Considerations:\n\n   A single-user client SHOULD NOT maintain more than\n   connections with any server or proxy.\n\nThere needs to be something between \"than\" and \"connections\".\n\nSecond, I was looking through this section trying to find\nout if it's legal for proxies to multiplex requests from\ndifferent clients on the same connection to a server.  I\ncouldn't find a clear answer in the spec or in the\narchives.  That is given two clients A and B talking to the\nsame server S via the same proxy P, what is minimum number\nof connections required.  Clearly there must be a connection\nfrom A to P and from B to P.  Can the proxy use just one\nconnection to S or must it open separate connections for\neach client?\n\nHoward\n\n\n\n", "id": "lists-012-9498736"}, {"subject": "Re: Editorial note and question on connection", "content": "    Second, I was looking through this section trying to find\n    out if it's legal for proxies to multiplex requests from\n    different clients on the same connection to a server.  I\n    couldn't find a clear answer in the spec or in the\n    archives.  That is given two clients A and B talking to the\n    same server S via the same proxy P, what is minimum number\n    of connections required.  Clearly there must be a connection\n    from A to P and from B to P.  Can the proxy use just one\n    connection to S or must it open separate connections for\n    each client?\n    \nIts's legal, but not advisable.  Section 8.1.4 says\n\n               A proxy SHOULD use up to 2*N connections to another\n   server or proxy, where N is the number of simultaneously active\n   users.\n\nIt might have been a good idea to explain the reasons behind\nthis a bit more carefully.\n\nThe problem is \"head-of-line blocking.\"  Consider, in your\nscenario, what would happen if A makes a request that generates\na really lengthy response, and then B issues a request.  Because\nthere is no packet-level multiplexing, B has to wait until\nA's response is fully transmitted.\n\nMore concretely, suppose that\nA = Administrative Assistant\nB = Big Boss\nand A requests a download of, say, the latest MP3 hit just\nbefore B requests, say, a short piece of critical business\ninformation.  (Not likely that these two resources would be\non the same server, I guess, but this is just an example.)\n\nIf we ever get a decent QoS infrastructure, this would make\nit even more desirable to segregate request streams from\ndifferent users onto different TCP connections.\n\n-Jeff\n\n\n\n", "id": "lists-012-9506545"}, {"subject": "Re: Editorial note and question on connection", "content": "On Fri Feb 12, 1999, Jeffrey Mogul wrote:\n\n> > Second, I was looking through this section trying to find\n> > out if it's legal for proxies to multiplex requests from\n> > different clients on the same connection to a server.  I\n> > couldn't find a clear answer in the spec or in the\n> > archives.  That is given two clients A and B talking to the\n> > same server S via the same proxy P, what is minimum number\n> > of connections required.  Clearly there must be a connection\n> > from A to P and from B to P.  Can the proxy use just one\n> > connection to S or must it open separate connections for\n> > each client?\n>     \n> Its's legal, but not advisable.  Section 8.1.4 says\n> \n>                A proxy SHOULD use up to 2*N connections to another\n>    server or proxy, where N is the number of simultaneously active\n>    users.\n\nI saw this, but took it to mean a recommended maximum bound,\nwith out recommending any minimum bound.  I wasn't sure if\na recommended minimum would be N or 1.\n\n> It might have been a good idea to explain the reasons behind\n> this a bit more carefully.\n> \n> The problem is \"head-of-line blocking.\"  Consider, in your\n> scenario, what would happen if A makes a request that generates\n> a really lengthy response, and then B issues a request.  Because\n> there is no packet-level multiplexing, B has to wait until\n> A's response is fully transmitted.\n> \n> More concretely, suppose that\n> A = Administrative Assistant\n> B = Big Boss\n\nmuch more illustrative than Alice and Bob :-)\n\n> and A requests a download of, say, the latest MP3 hit just\n> before B requests, say, a short piece of critical business\n> information.  (Not likely that these two resources would be\n> on the same server, I guess, but this is just an example.)\n\nSo there is an ordering issue.  The proxy must wait for the\nresponse before sending a request from B.  Is this also the\ncase in handling 100 Continue responses to a POST or PUT\nrequest?  Must the proxy wait for the 100, and then wait\nagain to get the body from the client, send it to the server\nand wait for the response before sending a request for B?\n\n> If we ever get a decent QoS infrastructure, this would make\n> it even more desirable to segregate request streams from\n> different users onto different TCP connections.\n\nThanks.\n\nHoward\n\n\n\n", "id": "lists-012-9515843"}, {"subject": "Re: Editorial note and question on connection", "content": "> >     Second, I was looking through this section trying to find\n> >     out if it's legal for proxies to multiplex requests from\n> >     different clients on the same connection to a server.\n\nJeffrey Mogul wrote:\n\n> Its's legal, but not advisable.  [...] \n\n> It might have been a good idea to explain the reasons behind\n> this a bit more carefully.\n> \n> The problem is \"head-of-line blocking.\"  Consider, in your\n> scenario, what would happen if A makes a request that generates\n> a really lengthy response, and then B issues a request.  Because\n> there is no packet-level multiplexing, B has to wait until\n> A's response is fully transmitted.\n\nJeff, your answer deals well with the question of whether or not a proxy\nshould pipeline requests from different clients on a single connection -\nthat is, whether or not it should send the request from B on the same\nconnection before the response for A has completed.  I agree that for\nthe reason you gave, this is a bad idea; but this is just a partial\nanswer to the original question.  \n\nSuppose that the proxy is 1.1, sends a request and receives a 1.1\nresponse that does not specify 'Connection: close'.  There is no reason\nwhy the proxy should not keep that now-idle connection open for a\nreasonable time in anticipation of another request to the same server,\neven if that request comes from a different client.  This might often be\nvery advantageous if the target server is a busy sight often accessed\nthrough the proxy (an upstream proxy, altavista, the corporate internal\nweb site at HQ overseas).\n\nFor this reason, origin servers should never assume that requests\nreceived on the same TCP connection are from the same user agent.\n\n-- \nScott Lawrence           Director of R & D        <lawrence@agranat.com>\nAgranat Systems, Inc.  Embedded Web Technology   http://www.agranat.com/\n\n\n\n", "id": "lists-012-9525669"}, {"subject": "some questions and notes on HTTP/1.1 rev ", "content": "Hello,\n\nI have some comments and questions on the revision 6 of HTTP 1.1.\n\n1) section 3.2.3 URI comparison\n  This chapters refers to \"reserved\" and \"unsafe\", both of them are neither\n  defined in the draft nor are they \"imported\" explicitly from some RFC.\n  The \"(see section 3.2)\" doesn't help very much here, since we \n  are in 3.2 already.\n  The definition of \"reserved\" in RFC 2396 differs from the one in RFC 2068,\n  because it contains additional characters: \"$\" and \",\" which are\n  considered \"safe\" rsp. \"extra\" in RFC 2068. \n\n2) Is \"Trailer\" not a hop-by-hop header (it is not mentioned in 13.5.1)?\n\n3) section 14.9 Cache Control\n  Definition of \"cache-response-directive\":\n    \"max-age\" in requests is not defined in 14.9.4 like it has been stated\nin the comment. It is only being described within the first paragraph of\n14.9.3 without any indentation.\n\n4) section  2.1, definition of implied LWS.\n  I have some problems when I try to apply this to the byte ranges in \n  section 14.35.1 (Range)\n\n       ranges-specifier = byte-ranges-specifier\n   byte-ranges-specifier = bytes-unit \"=\" byte-range-set\n   byte-range-set  = 1#( byte-range-spec | suffix-byte-range-spec )\n       byte-range-spec = first-byte-pos \"-\" [last-byte-pos]\n   first-byte-pos  = 1*DIGIT\n   last-byte-pos   = 1*DIGIT\n\n  and section 3.12 Range units\n  \n       range-unit       = bytes-unit | other-range-unit\n   bytes-unit       = \"bytes\"\n   other-range-unit = token\n\n  My question is: may I write \"bytes =\" in a Range header field?\n  According 2.1 implied LWS is only allowed between words (token or\n  quoted-string), or words and separators. \n  \"bytes\" is none of them, it is only a literal that\n  accidentally matches the token definition. And \"=\" is not a separator, for\n  the same reason. Sounds silly, doesn't it? But can I assume that\n  any literal that matches a token (or separator) may be treated as such?\n  This would allow to insert LWS between the header name and the \":\" as\n  well, like \"Range : bytes = 99-100\". Maybe section 4.2 solves this, but\n  the use of a LWS in that place isn't prohibited there explicitely.\n  \n  I'm asking this, because Apache requires \"bytes=\" without any LWS, but I am\n  not sure whether this is a bug. So I would like to ask here before reporting\n  it.\n    \nThanks in advance,\n\nJacob\n-- \n                                             Jacob Schroeder\n                                             Dipl. Informatiker\n                                             eMail: jacob.schroeder@gmx.de\n\n\n\n", "id": "lists-012-9534543"}, {"subject": "IESG approves HTTP/1.1 as Draft Standar", "content": "On Thursday, the IESG approved  Hypertext Transfer Protocol -- \n    HTTP/1.1 <draft-ietf-http-v11-spec-rev-06.txt> as a Draft Standard.\n\nThere are a small number of editorial errors that have been raised\nin the interim that I believe we can correct as part of the RFC editor\nprocess.\n\nI will review these on the list, along with the appropriate deltas\nfrom the -07.txt draft.\n\nThere are also a number of other issues that are substantive, that\nI don't think we should handle as editorial issues, but rather prepare\nas separate documents.\n\nI'll send out mail about those, too.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-9543994"}, {"subject": "minor editorial issues in draft-ietf-http-v11-spec-rev06.tx", "content": "I propose that these editorial changes be made by the RFC editor\n(if the RFC editor prefers, I can prepare a new version with\nthese changes.)\n\n=========================\nSection 14.11:\n   If the content-coding of an entity is not \"identity\", then\n   the response MUST including a Content-Encoding\n   entity-header (section 14.11) that lists the non-identity\nchange 'including' to 'include'.\n==========================\nSection 3.6.1 \"Chunked Transfer Coding\"\nChange\n   trailer        = *entity-header\nto\n   trailer        = *(entity-header CRLF)\n===========================\nSection 8.1.4 Practical Considerations, last para.:\n  A single-user client SHOULD NOT maintain more than connections with\nany\n  server or proxy.\n\nchange 'more than connections' to 'more than 2 connections'.\n=======================\nsection 3.2.3 URI comparison\n\n   Characters other than those in the \"reserved\" and \"unsafe\" sets (see\n   section 3.2) are equivalent to their \"\"%\" HEX HEX\" encoding.\n\nChange 'section 3.2' to 'RFC 2396 [42]'. The definition of URI syntax\nhad been removed but the reference was wrong.\n======================\nRemove \"URI. See RFC 2068\" from the index.\n======================\nsection 13.15.1\n\nAdd 'TE' and 'Trailer' to the list of Hop-by-Hop headers.\n======================\nsection 14.9 Cache Control\n         | \"max-age\" \"=\" delta-seconds            ; Section 14.9.4\n\nchange the reference to 'Section 14.9.4' to 'Section 14.9.3'\n=======================\nSection 3.2.2 change\n    http_URL = \"http:\" \"//\" host [ \":\" port ] [ abs_path ]\nto\n    http_URL = \"http:\" \"//\" host [ \":\" port ] [ abs_path [ \"?\" query ]]\n\nThis is another artifact of the removal of the URI syntax in\ndeference to RFC 2396.\n=======================\nSection 8.2.3, first sentence:\nchange \"an client\" to \"a client\"\n======================\nSection 8.2.4\nDelete the phrase\n  \", subject to the restrictions in section 8.2.3\"\n\nsince section 8.2.3 was deleted.\n========================\nSection 12.2\n\ndelete the phrase \"(this specification reserves the header name\nAlternates)\"\nsince the reservation was in RFC 2068 but has since been removed.\n=========================\n\n\n\n", "id": "lists-012-9551277"}, {"subject": "RE: Resend: Re: IPP&gt; Chunked POST: SUMMAR", "content": "Although this seems like an important clarification, I don't think\nit fits simply as an editorial change. If it's worth pursuing this\nat all, it will belong in another document (a HTTP implementation\nguide?) or in the HTTP/1.1 revision for \"Standard\".\n\n================\n> I'd like to propose that the wording be clarified in the spec.  I have\n> encountered servers that \"accept\" a chunked POST with 200 (OK) and\nthen\n> silently discard the message body, passing a zero-length entity-body\nto the\n> service layer (CGI or servlet), so I think some implementors are\n> misinterpreting the current wording.\n>\n> I propose adding something along these lines:\n> \"If a server disallows message bodies encoded with the chunked\n> transfer-coding in requests to some resource, it MUST return an error\ncode\n> in the response to such requests.  If this is the primary reason for\n> rejecting a request, the response MUST contain the 411\n> (Length Required) error code.\"\n>\n> Also, this sentence should be reworded:\n>\n> Section 4.4, Message Length:\n> All HTTP/1.1 applications that receive entities MUST accept the\n?chunked?\n> transfer-coding (section 3.6), thus allowing this mechanism to be used\nfor\n> messages when the message length cannot be determined in advance.\n>\n> becomes something like:\n>\n> All HTTP/1.1 applications that receive entities MUST understand the\n> ?chunked? transfer-coding (section 3.6), thus allowing this\n> mechanism to be used for messages when the message length cannot be\n> determined in advance.\n> This does NOT mean that servers must accept messages containing bodies\n> encoded with the chunked transfer-coding.\n\n\n\n", "id": "lists-012-9561699"}, {"subject": "Remaining editorial issue", "content": "In reviewing Jim's and my file of remaining editorial issues,\nthe following issues were not dealt with:\n\nRoss Patterson re-raised\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1998q4/0019.html\nconcerning the ability to omit LWS between the colon and the field\nvalue.\nI thought the v1.1 spec should remain as is, and so did not propose\nhandling this as an 'editorial' issue.\n-\nJeff Mogul pointed out that \"Pragma: no-cache\" is not a response\nheader, and \"in retrospect, I think the wording of the specification of\nPragma should have included a Note to this effect, because\nlots of people seem to be confused by this (even me, at times).\"\nBut I couldn't think of a simple 'Note' to add that would be only\neditorial, so I decided not to pursue this.\n-\nAdam Bradley pointed out:\n\n\"Section 8.2.3, the second bullet under \"Requirements for HTTP/1.1\nproxies\", I assume this requirement is subject to the same conditions as\nthe first bullet, but this could be made clear.\"\n\nI didn't see a simple editorial fix for making this 'clear'.\n-\nJacob Schroeder wrote:\n4) section  2.1, definition of implied LWS.\n  I have some problems when I try to apply this to the byte ranges in\n  section 14.35.1 (Range)\n\n       ranges-specifier = byte-ranges-specifier\n   byte-ranges-specifier = bytes-unit \"=\" byte-range-set\n   byte-range-set  = 1#( byte-range-spec | suffix-byte-range-spec )\n       byte-range-spec = first-byte-pos \"-\" [last-byte-pos]\n   first-byte-pos  = 1*DIGIT\n   last-byte-pos   = 1*DIGIT\n\n  and section 3.12 Range units\n\n       range-unit       = bytes-unit | other-range-unit\n   bytes-unit       = \"bytes\"\n   other-range-unit = token\n\n  My question is: may I write \"bytes =\" in a Range header field?\n  According 2.1 implied LWS is only allowed between words (token or\n  quoted-string), or words and separators.\n  \"bytes\" is none of them, it is only a literal that\n  accidentally matches the token definition. And \"=\" is not a separator,\nfor\n  the same reason. Sounds silly, doesn't it? But can I assume that\n  any literal that matches a token (or separator) may be treated as\nsuch?\n  This would allow to insert LWS between the header name and the \":\" as\n  well, like \"Range : bytes = 99-100\". Maybe section 4.2 solves this,\nbut\n  the use of a LWS in that place isn't prohibited there explicitely.\n\n  I'm asking this, because Apache requires \"bytes=\" without any LWS, but\nI am\n  not sure whether this is a bug. So I would like to ask here before\nreporting\n  it.\n\n\n\n", "id": "lists-012-9571065"}, {"subject": "Re: minor editorial issues in draft-ietf-http-v11-spec-rev06.tx", "content": ">Section 3.6.1 \"Chunked Transfer Coding\"\n>Change\n>   trailer        = *entity-header\n>to\n>   trailer        = *(entity-header CRLF)\n\n\nUmmm, while you are at it, you might also want to fix\n\n================\n4.1 Message Types:\nChange\n        generic-message = start-line\n                          *message-header\n                          CRLF\n                          [ message-body ]\nto\n        generic-message = start-line\n                          *(message-header CRLF)\n                          CRLF\n                          [ message-body ]\n================\n4.2 Message Headers:\nChange\n       message-header = field-name \":\" [ field-value ] CRLF\nto\n       message-header = field-name \":\" [ field-value ]\n================\n5 Request\nChange\n        Request       = Request-Line              ; Section 5.1\n                        *( general-header         ; Section 4.5\n                         | request-header         ; Section 5.3\n                         | entity-header )        ; Section 7.1\n                        CRLF\n                        [ message-body ]          ; Section 4.3\nto\n        Request       = Request-Line              ; Section 5.1\n                        *(( general-header        ; Section 4.5\n                          | request-header        ; Section 5.3\n                          | entity-header ) CRLF) ; Section 7.1\n                        CRLF\n                        [ message-body ]          ; Section 4.3\n================\n6 Response\nChange\n       Response      = Status-Line               ; Section 6.1\n                       *( general-header         ; Section 4.5\n                        | response-header        ; Section 6.2\n                        | entity-header )        ; Section 7.1\n                       CRLF\n                       [ message-body ]          ; Section 7.2\nto\n       Response      = Status-Line               ; Section 6.1\n                       *(( general-header        ; Section 4.5\n                         | response-header       ; Section 6.2\n                         | entity-header ) CRLF) ; Section 7.1\n                       CRLF\n                       [ message-body ]          ; Section 7.2\n================\n\nNote that these are all the same error, and it's been in the spec\nsince the very first draft (November 28, 1994).  *sigh*\nI'll let you decide whether or not to forward this to the RFC editor.\n\n....Roy\n\n\n\n", "id": "lists-012-9580800"}, {"subject": "RE: minor editorial issues in draft-ietf-http-v11-spec-rev06.tx", "content": "I see that we've been inconsistent about whether -header BNF\nnon-terminals include or do not include CRLF.\n\nIf there's no objection, I will forward these requests to\nthe RFC editor as well; I imagine that Keith will need to\nask for IESG approval.\n\nI hope there aren't a lot more like these, or we might be\nforced to reballot.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n> -----Original Message-----\n> From: Roy T. Fielding [mailto:fielding@kiwi.ics.uci.edu]\n> Sent: Monday, March 01, 1999 5:22 AM\n> To: Larry Masinter\n> Cc: Keith Moore; http-wg@hplb.hpl.hp.com\n> Subject: Re: minor editorial issues in\n> draft-ietf-http-v11-spec-rev-06.txt \n> \n> \n> >Section 3.6.1 \"Chunked Transfer Coding\"\n> >Change\n> >   trailer        = *entity-header\n> >to\n> >   trailer        = *(entity-header CRLF)\n> \n> \n> Ummm, while you are at it, you might also want to fix\n> \n> ================\n> 4.1 Message Types:\n> Change\n>         generic-message = start-line\n>                           *message-header\n>                           CRLF\n>                           [ message-body ]\n> to\n>         generic-message = start-line\n>                           *(message-header CRLF)\n>                           CRLF\n>                           [ message-body ]\n> ================\n> 4.2 Message Headers:\n> Change\n>        message-header = field-name \":\" [ field-value ] CRLF\n> to\n>        message-header = field-name \":\" [ field-value ]\n> ================\n> 5 Request\n> Change\n>         Request       = Request-Line              ; Section 5.1\n>                         *( general-header         ; Section 4.5\n>                          | request-header         ; Section 5.3\n>                          | entity-header )        ; Section 7.1\n>                         CRLF\n>                         [ message-body ]          ; Section 4.3\n> to\n>         Request       = Request-Line              ; Section 5.1\n>                         *(( general-header        ; Section 4.5\n>                           | request-header        ; Section 5.3\n>                           | entity-header ) CRLF) ; Section 7.1\n>                         CRLF\n>                         [ message-body ]          ; Section 4.3\n> ================\n> 6 Response\n> Change\n>        Response      = Status-Line               ; Section 6.1\n>                        *( general-header         ; Section 4.5\n>                         | response-header        ; Section 6.2\n>                         | entity-header )        ; Section 7.1\n>                        CRLF\n>                        [ message-body ]          ; Section 7.2\n> to\n>        Response      = Status-Line               ; Section 6.1\n>                        *(( general-header        ; Section 4.5\n>                          | response-header       ; Section 6.2\n>                          | entity-header ) CRLF) ; Section 7.1\n>                        CRLF\n>                        [ message-body ]          ; Section 7.2\n> ================\n> \n> Note that these are all the same error, and it's been in the spec\n> since the very first draft (November 28, 1994).  *sigh*\n> I'll let you decide whether or not to forward this to the RFC editor.\n> \n> ....Roy\n> \n\n\n\n", "id": "lists-012-9591520"}, {"subject": "Re: minor editorial issues in draft-ietf-http-v11-spec-rev06.tx", "content": "\"Roy T. Fielding\" wrote:\n> \n> >Section 3.6.1 \"Chunked Transfer Coding\"\n> >Change\n> >   trailer        = *entity-header\n> >to\n> >   trailer        = *(entity-header CRLF)\n> \n\n\nHas this one been picked up...\n\nSection 3.6\n\nChange\n  transfer-coding     = \"chunked\" | transfer-extension\nto\n  transfer-encoding   = \"chunked\" | transfer-extension\n\n\nFound in draft-ietf-http-v11-spec-rev-06.\n\nCheers,\nKal.\n\n-- \n    .      Kalvinder Singh                     singh@ozy.dec.com\n _-_|\\     Software Engineering Australia\n/     \\<-- Compaq       \n\\_.-._/    Research Park, Bond University      Phone: +61 7 5575 0106\n     v     Gold Coast, Qld, 4229, Australia    Fax:   +61 7 5575 0100\n\n\n\n", "id": "lists-012-9603882"}, {"subject": "Re: minor editorial issues in draft-ietf-http-v11-spec-rev06.tx", "content": ">Has this one been picked up...\n>\n>Section 3.6\n>\n>Change\n>  transfer-coding     = \"chunked\" | transfer-extension\n>to\n>  transfer-encoding   = \"chunked\" | transfer-extension\n\nNo, that is not an error. tansfer-coding and transfer-encoding are two\ndifferent BNF rules.\n\n....Roy\n\n\n\n", "id": "lists-012-9613342"}, {"subject": "pragma as response header (was Re: Remaining editorial issues", "content": "    Jeff Mogul pointed out that \"Pragma: no-cache\" is not a response\n    header, and \"in retrospect, I think the wording of the specification of\n    Pragma should have included a Note to this effect, because\n    lots of people seem to be confused by this (even me, at times).\"\n    But I couldn't think of a simple 'Note' to add that would be only\n    editorial, so I decided not to pursue this.\n\nIs this simple enough?\n\n(at the end of section 14.32)\n\nNote: because the meaning of \"Pragma: no-cache\" as a\nresponse header field is not actually specified, it does not\nprovide a reliable replacement for \"Cache-control: no-cache\"\nin a response.\n\n-Jeff\n\n\n\n", "id": "lists-012-9622126"}, {"subject": "Re: minor editorial issues in draft-ietf-http-v11-spec-rev06.tx", "content": "\"Roy T. Fielding\" wrote:\n> \n> >Has this one been picked up...\n> >\n> >Section 3.6\n> >\n> >Change\n> >  transfer-coding     = \"chunked\" | transfer-extension\n> >to\n> >  transfer-encoding   = \"chunked\" | transfer-extension\n> \n> No, that is not an error. tansfer-coding and transfer-encoding are two\n> different BNF rules.\n> \n\nSome Questions then...\n\nQuestion 1)\n\nWhy isn't there a header definition of \"transfer-coding\" in Section 14?\n\n\nQuestion 2)\n\nIn \"Section 14.41 Transfer-Encoding\" the following is stated...\n\nTransfer-codings are defined in section 3.6. An example is:\n       Transfer-Encoding: chunked\n\nIs this an error???\n\n\nCheers,\nKal.\n\n-- \n    .      Kalvinder Singh                     singh@ozy.dec.com\n _-_|\\     Software Engineering Australia\n/     \\<-- Compaq       \n\\_.-._/    Research Park, Bond University      Phone: +61 7 5575 0106\n     v     Gold Coast, Qld, 4229, Australia    Fax:   +61 7 5575 0100\n\n\n\n", "id": "lists-012-9630081"}, {"subject": "Re: minor editorial issues in draft-ietf-http-v11-spec-rev06.tx", "content": "Sorry about the bandwidth...:(\n\nI just answered my own questions, or I should say that you answered it\nin your first reply...;)\n\nOnce again I am sorry.\n\nCheers,\nKal.\n\nKalvinder Singh wrote:\n> \n> \"Roy T. Fielding\" wrote:\n> >\n> > >Has this one been picked up...\n> > >\n> > >Section 3.6\n> > >\n> > >Change\n> > >  transfer-coding     = \"chunked\" | transfer-extension\n> > >to\n> > >  transfer-encoding   = \"chunked\" | transfer-extension\n> >\n> > No, that is not an error. tansfer-coding and transfer-encoding are two\n> > different BNF rules.\n> >\n> \n> Some Questions then...\n> \n> Question 1)\n> \n> Why isn't there a header definition of \"transfer-coding\" in Section 14?\n> \n> Question 2)\n> \n> In \"Section 14.41 Transfer-Encoding\" the following is stated...\n> \n> Transfer-codings are defined in section 3.6. An example is:\n>        Transfer-Encoding: chunked\n> \n> Is this an error???\n> \n> Cheers,\n> Kal.\n> \n> --\n>     .      Kalvinder Singh                     singh@ozy.dec.com\n>  _-_|\\     Software Engineering Australia\n> /     \\<-- Compaq\n> \\_.-._/    Research Park, Bond University      Phone: +61 7 5575 0106\n>      v     Gold Coast, Qld, 4229, Australia    Fax:   +61 7 5575 0100\n\n-- \n    .      Kalvinder Singh                     singh@ozy.dec.com\n _-_|\\     Software Engineering Australia\n/     \\<-- Compaq       \n\\_.-._/    Research Park, Bond University      Phone: +61 7 5575 0106\n     v     Gold Coast, Qld, 4229, Australia    Fax:   +61 7 5575 0100\n\n\n\n", "id": "lists-012-9639812"}, {"subject": "Protocol Action: Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.1 to Draft Standar", "content": "The IESG has approved publication of the following Internet-Drafts as\nDraft Standards:\n\n o Hypertext Transfer Protocol -- HTTP/1.1\n<draft-ietf-http-v11-spec-rev-06.txt>\n\n   This document replaces RFC2068, currently a Proposed Standard.\n\n\n o HTTP Authentication: Basic and Digest Access Authentication \n<draft-ietf-http-authentication-03.txt> \n\n   This document replaces RFC2069, currently a Proposed Standard.\n\nThese documents are the product of the HyperText Transfer Protocol\nWorking Group.  The IESG contact persons are Keith Moore and Patrik\nFaltstrom.\n \n \nTechnical Summary\n \n HTTP/1.1 is the primary data transfer protocol used by the world\n wide web.  This Draft Standard revision contains numerous\n clarifications and corrections to its predecessor, RFC 2068.\n\n Basic Access Authentication is an insecure authentication method\n which was present in HTTP/1.0.  Even though it exposes the user's\n password to eavesdroppers, it is still needed for backward\n compatibility.  Digest Access Authentication is designed as\n an improvement to Basic authentication.  While Digest provides\n no confidentiality or integrity service, it at least provides\n improved protection (as compared to Basic) for the user's password.\n\nWorking Group Summary\n\n A large number of issues were debated at length.\n (The list of issues is documented at\n http://www.w3.org/Protocols/HTTP/Issues/\n and http://www.w3.org/Protocols/HTTP/Issues/DSI.html\n including pointers into the mailing list archive where the\n issue was discussed, and, usually, the resolution.)\n\n Many design choices were subtle and difficult.  HTTP has\n been widely implemented and extended by many different\n parties in a short amount of time, and this made it\n difficult to define the proper interaction between\n features originally specified by different parties.\n In addition, the interaction of multiple roles (browser,\n local cache, proxy, origin server, authentication service)\n and conflicting goals (performance, reliability, privacy,\n managability) made analysis of the choices more difficult.\n\n Most decisions were made quickly, but some required\n extensive discussion and multiple position papers.\n At least rough consensus was reached on all design choices.\n\nProtocol Quality\n\n Keith Moore reviewed the spec for IESG.\n There are several implementations of HTTP/1.1, and at least two\n implementations of each protocol feature as required by RFC 2026\n for Draft Standard protocols.\n\n\n\n", "id": "lists-012-9649814"}, {"subject": "RE: Protocol Action: Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.1 to Draft Standar", "content": "With the approval of HTTP for Draft Standard, I propose\nthat the HTTP working group officially close; the mailing list\nwill remain open for discussion of issues that arise and that\nshould be resolved before HTTP progresses to \"Standard\".\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-9660418"}, {"subject": "about TCP connection up timer in HTTP sessio", "content": "Hi,\nAnyone can give me a clue about the following question:\nFor the first SYN packet of TCP connection setup for HTML doc., the\nretransmission\ntimer I measured are about: 3.234 (first retransmission), 6.577(sec. ),\n13.169 (third), ....\nThat is new = 2*previous.\nThe first timeout for the ACK & SYN from the other side is about 2.95 sec.\n\nBut in the following simultaneous TCP connections, I observed that the first\ntimeout for the ACK & SYN \nfrom the other side for some connection is about 1.069 sec.\n\nHow does TCP start the timer from the response side in Netscape server? \n\nThank you\n\nDabin\n\n\n\n", "id": "lists-012-9668828"}, {"subject": "nice Fello", "content": "Congratulations to Larry Masinter on his being named a 1999 ACM Fellow.  \n\nFurther congratulations to him (and to all of us, really) on\nsuccessfully steering the HTTP specification through the IETF\nstandardization shoals.\n\nOh, and while we're at it, thanks to Jim Gettys for the tremendous\namount of fine work he performed to edit the document.\n\nDave Kristol\n\n\n\n", "id": "lists-012-9676016"}, {"subject": "Minneapoli", "content": "Based on the IETF agenda, it appears that this group is not meeting in\nMinneapolis.  Is that correct?  If not, when and where will the meeting be\nheld?  Is there an agenda available?\n\nAlso, could someone please tell me whether it is expected that the draft:\ndraft-ietf-http-authentication-03 will enter the standards track?\n\nThank you,\nMargaret \n\n\nMargaret Wasserman\nmrw@tiac.net\n\n\n\n", "id": "lists-012-9682579"}, {"subject": "RE: Minneapoli", "content": "> Based on the IETF agenda, it appears that this group is not meeting in\n> Minneapolis.  Is that correct?\n\nThe HTTP working group is not meeting in Minneapolis.\n\n>  If not, when and where will the meeting be\n> held?  Is there an agenda available?\n\n\nThe HTTP working group, having concluded its work, will close.\nThere will not be any additional meetings.\n\n\n> Also, could someone please tell me whether it is expected that the draft:\n> draft-ietf-http-authentication-03 will enter the standards track?\n\nIt has been approved as Draft Standard, and the RFC editor\nis working on editing the draft so that the RFC can issue.\nI expect this to happen in the next couple of weeks.\n\n\n\n", "id": "lists-012-9689589"}, {"subject": "disposition of draft-ietf-http* internet draft", "content": "In the APPS area meeting, I was reminded that we need to\nofficially deal with all draft-ietf-http-* documents in the\nInternet drafts area. Enclosed is a review of the documents\nthat are currently in the Internet Drafts directory (or, rather,\nlisted in the 1id-abstracts.txt document).\n****************************************\nActions for the Internet Drafts editor:\n      remove draft-ietf-http-pep-05.txt\n             draft-ietf-http-trust-state-mgt-02.txt\n             draft-ietf-http-alternates-01.txt\n             draft-ietf-http-req-sum-00.txt\n             draft-mogul-http-age-00.txt\n*****************************************\nI note that we cannot officially close until \"HTTP State\nManagement Mechanism\" has been completed:\n\n  \"HTTP State Management Mechanism\", D. Kristol, L. Montulli, 07/29/1998, \n  <draft-ietf-http-state-man-mec-10.txt,.ps>                               \n\nis marked as awaiting an applicability statement, which I presume is:\n\n  \"Applicability Statement for HTTP State Management\", Keith Moore, \n  11/23/1998, <draft-iesg-http-cookies-00.txt>                             \n\n***************************\nWe have two drafts in the RFC editor's queue:\n\n  \"Hypertext Transfer Protocol -- HTTP/1.1\", J Mogul, Tim Berners-Lee, \n  Larry Masinter, P. Leach, R. Fielding, H. Nielsen, Jim Gettys, \n  11/24/1998, <draft-ietf-http-v11-spec-rev-06.txt,.ps>                    \n\n  \"HTTP Authentication: Basic and Digest Access Authentication\", J. Franks,\n  A. Luotonen, P. Leach, J. Hostetler, P. Hallam-Baker, L. Stewart, Scott \n  Lawrence, 09/11/1998, <draft-ietf-http-authentication-03.txt>            \n\n***************************\n  \"PEP - an Extension Mechanism for HTTP\", D. Connolly, H. Nielsen, R. \n  Khare, Eric Prud''hommeaux, 12/04/1997, <draft-ietf-http-pep-05.txt>     \n\nThis document is obsolete. (I don't know why it's still in the\ndrafts directory.) There is a possible replacement:\n  <draft-frystyk-http-mandatory-00.txt>.\n***************************\n  \"HTTP Trust Mechanism for State Management\", D. Jaye, 03/06/1998, \n  <draft-ietf-http-trust-state-mgt-02.txt>                                 \n\nThe HTTP working group declined to pursue this.\n***************************\n  \"The Alternates Header Field\", E. Hardie, K. Holtman, A. Mutz, \n  11/13/1997, <draft-ietf-http-alternates-01.txt>                          \n\nThis document is obsolete; some kinds of content negotiation\nwere published as Experimental RFCs, but we declined to pursue\nthis.\n***************************\n  \"Format and Example of HTTP/1.1 Requirements Summary\", J Mogul, \n  09/15/1997, <draft-ietf-http-req-sum-00.txt>                             \n\nThis internet draft was produced as a part of the HTTP working\ngroup process, but can be removed at this time.\n****************************\n  \"Generation of the Age header field in HTTP/1.1\", J Mogul, 09/15/1997, \n  <draft-mogul-http-age-00.txt>                                            \n\nThis internet draft was submitted as part of the WG internal\ndiscussion and can be removed.\n****************************\n\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-012-9697162"}, {"subject": "Re: disposition of draft-ietf-http* internet draft", "content": "Larry Masinter wrote:\n> [...]\n> I note that we cannot officially close until \"HTTP State\n> Management Mechanism\" has been completed:\n> \n>   \"HTTP State Management Mechanism\", D. Kristol, L. Montulli, 07/29/1998,\n>   <draft-ietf-http-state-man-mec-10.txt,.ps>\n> \n> is marked as awaiting an applicability statement, which I presume is:\n> \n>   \"Applicability Statement for HTTP State Management\", Keith Moore,\n>   11/23/1998, <draft-iesg-http-cookies-00.txt>\n> \n\nYes and no.  The applicability statement needs to be revised.  But the\nreal problem IMO is that state-man-mec is caught in IESG process hell,\nwith some current IESG members unhappy with wording that was accepted by\nformer IESG members.\n\nDave Kristol\n\n\n\n", "id": "lists-012-9709127"}, {"subject": "Re: disposition of draft-ietf-http* internet draft", "content": "> Yes and no.  The applicability statement needs to be revised.  But the\n> real problem IMO is that state-man-mec is caught in IESG process hell,\n> with some current IESG members unhappy with wording that was accepted by\n> former IESG members.\n\nI don't think this is an accurate description of the problem.  \n\nKeith\n\n\n\n", "id": "lists-012-9718313"}, {"subject": "Re: disposition of draft-ietf-http* internet draft", "content": "Keith Moore wrote:\n> \n> > Yes and no.  The applicability statement needs to be revised.  But the\n> > real problem IMO is that state-man-mec is caught in IESG process hell,\n> > with some current IESG members unhappy with wording that was accepted by\n> > former IESG members.\n> \n> I don't think this is an accurate description of the problem.\n\nFair enough.  I'd really like to understand.  What *is* the problem that\nprevents an I-D that was last-called before July, 1998, from advancing\nto RFC status or being dropped outright?\n\nDave Kristol\n\n\n\n", "id": "lists-012-9727392"}, {"subject": "Re: disposition of draft-ietf-http* internet draft", "content": "At 15:10 15/03/1999 PST, Larry Masinter wrote:\n>We have two drafts in the RFC editor's queue:\n>\n>  \"Hypertext Transfer Protocol -- HTTP/1.1\", J Mogul, Tim Berners-Lee, \n>  Larry Masinter, P. Leach, R. Fielding, H. Nielsen, Jim Gettys, \n>  11/24/1998, <draft-ietf-http-v11-spec-rev-06.txt,.ps>                    \n\nThe authoring ordering has been swapped here - the right order should be\nthe one listed in the rev-06 draft.\n\n>  \"PEP - an Extension Mechanism for HTTP\", D. Connolly, H. Nielsen, R. \n>  Khare, Eric Prud''hommeaux, 12/04/1997, <draft-ietf-http-pep-05.txt>     \n>\n>This document is obsolete. (I don't know why it's still in the\n>drafts directory.) There is a possible replacement:\n>  <draft-frystyk-http-mandatory-00.txt>.\n\nThe latest version of this draft is\n\n   draft-frystyk-http-extensions-03.txt\n\nwhich contains the changes requested from Koen Holtman during IESG last\ncall for Proposed. This draft was submitted on March 15 but will not be\nannounced before the end of the IETF meeting.\n\nIn the meantime you can get this draft from\n\n\nhttp://www.w3.org/Protocols/HTTP/ietf-http-ext/draft-frystyk-http-extensions\n-03\n\nlinked from\n\nwww.w3.org/Protocols/HTTP/ietf-http-ext/\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n", "id": "lists-012-9736239"}, {"subject": "Re: disposition of draft-ietf-http* internet draft", "content": "> From: Henrik Frystyk Nielsen <frystyk@w3.org>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Wed, 17 Mar 1999 12:24:18 -0500\n> To: \"Larry Masinter\" <masinter@parc.xerox.com>,\n>         \"Keith Moore\" <moore+iesg@cs.utk.edu>,\n>         =?iso-8859-1?Q?=22Patrik_F=E4ltstr=F6mdHL2bQ=3D=3D=22?=\n>   <paf@swip.net>,\n>         <internet-drafts@ietf.org>\n> Cc: \"HTTP Working Group\" <http-wg@hplb.hpl.hp.com>\n> Subject: Re: disposition of draft-ietf-http-* internet drafts\n> -----\n> At 15:10 15/03/1999 PST, Larry Masinter wrote:\n> >We have two drafts in the RFC editor's queue:\n> >\n> >  \"Hypertext Transfer Protocol -- HTTP/1.1\", J Mogul, Tim Berners-Lee,\n> >  Larry Masinter, P. Leach, R. Fielding, H. Nielsen, Jim Gettys,\n> >  11/24/1998, <draft-ietf-http-v11-spec-rev-06.txt,.ps>                   \n> \n> The authoring ordering has been swapped here - the right order should be\n> the one listed in the rev-06 draft.\n> \n\nI talked with Steve Coya about this a while ago (in Florida, I believe):\n\nThe silly database software they use for issuing drafts takes the\nauthors out of a database and generates the announcement message; this\nis an artifact of the announcement message only.  Steve said the next\ntime they had a contractor go fondle that particular part of the code\nthey use, he'd try to get it fixed, but not to hold our breath...\n- Jim\n\n\n\n", "id": "lists-012-9746477"}, {"subject": "Re: disposition of draft-ietf-http* internet draft", "content": "> Actions for the Internet Drafts editor:\n>       remove draft-ietf-http-pep-05.txt\n>              draft-ietf-http-trust-state-mgt-02.txt\n>              draft-ietf-http-alternates-01.txt\n>              draft-ietf-http-req-sum-00.txt\n>              draft-mogul-http-age-00.txt\n\nyou don't actually have to remove them; all you need to do is to\nget them reclassified as \"individual\" documents rather than WG\ndocuments.  you should be able to do this by mailing scoya@ietf.org\n\nKeith\n\n\n\n", "id": "lists-012-9758335"}, {"subject": "RE: disposition of draft-ietf-http* internet draft", "content": "(Trying to limit the cc's)\n\n> We have received positive interest from current user agent and server\n> implementers for draft-ietf-http-trust-state-mgt-02.txt.  However, given\n> current widespread acceptance of current cookie implementations and\n> default handling, there is no impetus for implementation.   My\n> understanding and opinion is, however, that the implementers of the most\n> ubiquitous user agents will not fully implement state-man-mec-10.txt and\n> all of its recommendations and defaults without some solution like\n> trust-state-mgt.\n\nConditions under which someone might use state-man-mec sound like\nthings that might belong in an applicability statement for it.\n\nI suggest you consider pursuing draft-ietf-http-trust-state-mgt-02.txt\nas an individual contribution \"Experimental\" RFC. If the IESG chooses,\nit could reference it in the applicability statement.\n\nLarry\n\n\n\n", "id": "lists-012-9767612"}, {"subject": "RE: disposition of draft-ietf-http* internet draft", "content": "We have received positive interest from current user agent and server\nimplementers for draft-ietf-http-trust-state-mgt-02.txt.  However, given\ncurrent widespread acceptance of current cookie implementations and\ndefault handling, there is no impetus for implementation.   My\nunderstanding and opinion is, however, that the implementers of the most\nubiquitous user agents will not fully implement state-man-mec-10.txt and\nall of its recommendations and defaults without some solution like\ntrust-state-mgt.\n\n\n\n-----Original Message-----\nFrom: Dave Kristol [mailto:dmk@bell-labs.com]\nSent: Tuesday, March 16, 1999 9:30 AM\nTo: Larry Masinter\nCc: Keith Moore; Patrik FBltstrBm; internet-drafts@ietf.org; HTTP\nWorking Group\nSubject: Re: disposition of draft-ietf-http-* internet drafts\n\n\nLarry Masinter wrote:\n> [...]\n> I note that we cannot officially close until \"HTTP State\n> Management Mechanism\" has been completed:\n> \n>   \"HTTP State Management Mechanism\", D. Kristol, L. Montulli, 07/29/1998,\n>   <draft-ietf-http-state-man-mec-10.txt,.ps>\n> \n> is marked as awaiting an applicability statement, which I presume is:\n> \n>   \"Applicability Statement for HTTP State Management\", Keith Moore,\n>   11/23/1998, <draft-iesg-http-cookies-00.txt>\n> \n\nYes and no.  The applicability statement needs to be revised.  But the\nreal problem IMO is that state-man-mec is caught in IESG process hell,\nwith some current IESG members unhappy with wording that was accepted by\nformer IESG members.\n\nDave Kristol\n\n\n\n", "id": "lists-012-9775935"}, {"subject": "HTTP/1.1 Error Response", "content": "I have been reading the HTTP/1.1 spec (rev. 6, 18th Nov, expires May 18, 1999).\n\nI have a question.  I suspect that the answer should be obvious, but since\nit isn't, and since I think it ought to be a common question, maybe some\nwording somewhere could be clarified.\n\nMy question concerns the appropriate response code to use when rejecting\na request as incomplete or otherwise unacceptable.  For instance, if\nthe users submits a form without supplying a value for a necessary\nfield (say an email address), or with an in-appropriate value (say a\nnon-existent file type).  \n\nThe server process detects this and wants to reply to say that it\nhas rejected the request.  What code should be used in the HTTP header?\n\nTypically, I suspect a CGI application will use \"200 OK\", even though the\nbody of the message then says \"I can't do that because....\".  However,\nSection 10.2.1 says that \"200 OK\" means the request has succeeded, which it\nhasn't....\n\nIt seems that a \"4xx\" code would be right.  But which one?  \n\nBased on Section 10.4, it seems to be that \"403 Forbidden\" is the right\nanswer.  But I am not confident of this. I associate \"403\" with \"Permission\ndenied\" messages, and I've never noticed anyone using it to tell me\nI've forgotten to specify my email address....  But looking at 10.4.4, it\nseems it is actually very general, and applies in this case: the server\n*has* understood the request, and is refusing to carry it out.  \n\nIf 403 is the right response, could I suggest adding a sentence to 10.4.4\nsomething along the lines of \"This code is appropriate when a server\nrejects an incomplete or incoherent request, such as from an incorrectly\nfilled form\".\n\nIf 403 isn't right, could someone tell me what is (and why): I've\nthought about, and rejected, \"400 Bad Request\", \"406 Not Acceptable\" and\n\"409 Conflict\".... \n\nRobert.\n\n\n\n", "id": "lists-012-9787935"}, {"subject": "Doubt regarding quotedpai", "content": "If a header (say, Content-Disposition) contains a quoted-string that\nincludes a quoted-pair,\nhow should the quoted-pair sequence be treated - if the character\nfollowing the backslash is\nNOT a \"special\" character.\n\nExample:  Given the string \"C:\\dir\\file.txt\", within a\nContent-Disposition header, if I use the\nRFC822 parsing rules, I'll end up with \"C:dirfile.txt\", since as per\nRFC822 (actually from:\nthe rfc822 update - draft-ietf-drums-msg-fmt):\n\n3.2.2 Quoted characters\n\nSome characters are reserved for special interpretation, such as\ndelimiting\nlexical tokens. To permit use of these characters as uninterpreted data,\na\nquoting mechanism is provided.\n\nquoted-pair     =       (\"\\\" text) / obs-qp\n\nWhere any quoted-pair appears, it should be interpreted as the text\ncharacter alone.\n--------\nI think in this case, the string should be :C:\\\\dir\\\\file.txt\n\nI don't see anything in RFC 2068 regarding this, leaving this case\nambiguos - I think.\n\nComments ? (I'm not in this mailing list, so include my email address as\nwell)\n\n-john\n\n\n\n", "id": "lists-012-9796420"}, {"subject": "Re: HTTP/1.1 Error Response", "content": ">My question concerns the appropriate response code to use when rejecting\n>a request as incomplete or otherwise unacceptable.  For instance, if\n>the users submits a form without supplying a value for a necessary\n>field (say an email address), or with an in-appropriate value (say a\n>non-existent file type).  \n>\n>The server process detects this and wants to reply to say that it\n>has rejected the request.  What code should be used in the HTTP header?\n>\n>Typically, I suspect a CGI application will use \"200 OK\", even though the\n>body of the message then says \"I can't do that because....\".  However,\n>Section 10.2.1 says that \"200 OK\" means the request has succeeded, which it\n>hasn't....\n\nActually, it has.  POST means only that data has been supplied to the\nprocess, to do with as it wishes. As far as HTTP is concerned, the request\nwas successfully completed.\n\n>It seems that a \"4xx\" code would be right.  But which one?  \n\nEither 200 or 400.  None of the others are appropriate because they refer\nto specific HTTP interoperability concerns, not to POST data.\n\n....Roy\n\n\n\n", "id": "lists-012-9803565"}, {"subject": "basic and md5 authenticatio", "content": "HI, \n\nI wonder if anyone here has statistics on the number of web servers\nand browsers that support \na) basic authentication (I think most does)\nb) message digest authentication (I think most doesn't)\nand the popularity of people deploying them on today's Internet. \n\nAlso, how widely support of HTTP/1.1 spec is there by web servers and\nclients?\n\nThanks,\nWendy\n\n\n\n", "id": "lists-012-9811880"}, {"subject": "The HTTP 'link' and 'ContentBase' Header", "content": "Hi.\n\nIt has been brought to my attention that the HTTP 'Link' header has been\nremoved from the latest version of HTTP 1.1 (IETF's Internet Draft dated \nNovember 1998).\n\nI understand that the reason for this change was that the HTTP WG\nconsidered the semantics to have been poorly defined. For example, one\npost on HTTP-wg says:\n\n: There are known problems with some of the proposals: e.g. Link sytax\n: does not quite seem to be what the HTML group needs for enabling \n: server setting of sytle sheets; its grammar is suspect. [1]\n\nCould you please explain what is \"not quite what the HTML group needs\"\nand why RFC 2068's definition of the grammar is 'suspect'?\n\nNGLayout, the engine behind version 5 of Netscape's browser, currently\ncomprehensively supports the HTTP Link header, as defined in RFC 2068.\nNo particular difficulties in interpreting the semantics of the 'Link'\nheader were hit when working on getting this feature implemented.\n\nThis header is _incredibly_ useful, for, eg, setting the webmaster\nemail address of every document in a site or for changing the\nstylesheet selected in every page of a 2000 document site, without\nhaving to edit each file individually. (The \"Default-Style\" header is\nvery useful in this respect too.)\n\nI have also noticed that the 'Content-Base' header was also deleted.\nNGLayout also supports this HTTP header.\n\nYour time is greatly appreciated.\n\n-- References --\n[1] quoted in:\nhttp://hplbwww.hpl.hp.com/people/ange/archives/archives-97/http-wg-archive/1603.html\n\nTest cases exist for both of those headers at\nhttp://www.bath.ac.uk/%7Epy8ieh/internet/importtest/\n\nNGLayout latest build is available from\nftp://ftp.mozilla.org/pub/mozilla/nightly/\n\nNote that NGLayout currently only supports HTTP headers specified in\nMETA elements. This is due to a bug that is preventing any HTTP header\nfrom reaching the document's DOM, and will be fixed prior to release.\n\n-- \nIan Hickson \nU+2642 U+2651\nU+262E U+2603 U+263A\n\n\n\n", "id": "lists-012-9818933"}, {"subject": "Re: HTTP/1.1 Error Response", "content": "    > Date: Wed, 24 Mar 1999 13:43:01 -0800\n    > From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n    > \n    > >My question concerns the appropriate response code to use when rejecting\n    > >a request as incomplete or otherwise unacceptable.  For instance, if\n    > >the users submits a form without supplying a value for a necessary\n    > >field (say an email address), or with an in-appropriate value (say a\n    > >non-existent file type).  \n    > >\n    > >The server process detects this and wants to reply to say that it\n    > >has rejected the request.  What code should be used in the HTTP header?\n    > >\n    > >Typically, I suspect a CGI application will use \"200 OK\", even though the\n    > >body of the message then says \"I can't do that because....\".  However,\n    > >Section 10.2.1 says that \"200 OK\" means the request has succeeded, which it\n    > >hasn't....\n    > \n    > Actually, it has.  POST means only that data has been supplied to the\n    > process, to do with as it wishes. As far as HTTP is concerned, the request\n    > was successfully completed.\n\nReally???!!!!\n\nI find that statement very surprising.  \n\nIt certainly does not fit with my reading of the start of Section 10.2 \n  \"This class of status code indicates that the clients request was\n   successfully received, understood AND ACCEPTED\"\nand 10.2.1\n   \"The request has succeeded\"\n\nIt seems I should understand \"succeeded\" as nothing more than \"delivered\nand understood\".  \n\nMy example of a request rejected because of missing or inappropriate data\nhas surely no more \"succeeded\" than an attempt to execute a cgi file with\nno execute permission---and none of the servers I have used return \"200 OK\"\nfor that....\n\n    > >It seems that a \"4xx\" code would be right.  But which one?  \n    > \n    > Either 200 or 400.  \n\nBut 400 explicitly says it deals with requests that could not be\nunderstood because of \"malformed syntax\".  My worry is when there\nis a problem iwht the content of the request.\n\n    > None of the others are appropriate because they refer\n    > to specific HTTP interoperability concerns, not to POST data.\n\nBut my uncertainty applies equally to form data encoded as \"GET\"....\n\n    > \n    > ....Roy\n\nRobert.\n\n--------------------------------------------------------------------------\nRobert Inder,                               (http://www.hcrc.ed.ac.uk/~robert/)\nNEDO Visiting Fellow, Electrotechnical Laboratory (ETL) (http://www.etl.go.jp/)\n                      Umezono 1-1-4, Tsukuba, Ibaraki 305, JAPAN\n     Research Fellow, HCRC, Edinburgh University    (http://www.hcrc.ed.ac.uk/)\n      2, Buccleuch Place, Edinburgh EH8 9LW SCOTLAND\n--------------------------------------------------------------------------\n\n\n\n", "id": "lists-012-9829117"}, {"subject": "typo in draft-ietf-http-v11-spec-rev0", "content": "Section 8.1.4, last paragraph, 2nd sentence, there seems to be a word\nmissing:\n\n\"...maintain more than [???] connections with...\"\n\n\nRoy Smith <roy@popmail.med.nyu.edu>\nNew York University School of Medicine\n550 First Avenue, New York, NY  10016\n\n\n\n", "id": "lists-012-9839626"}, {"subject": "Re: The HTTP 'link' and 'ContentBase' Header", "content": "In order for items in a protocol to advance to Draft Standard, there\nmust be at least 2 tested interoperable implementations of both client\nand server.  By IETF rules, such items must be removed for draft standard,\nand there was no claim last summer that people would get them implemented\nand tested.  There were no complaints at the time about this action,\nnor were there promises of quick implementation.\n\nNeither of these made the cut, at the time (late last summer).  See detailed\ncomments below.\n\nThe way to make progress on these in the IETF, is to write a separate (short)\ninternet draft defining them, show that you have the implementations (client,\nserver, and proxy (if there is any proxy specific behavior),\nand they can/could/should progress (given that the headers were in the\nproposed standard).  Note that the working group does NOT have to be\n\"open\" at the time; the mailing list (will) continue to exist, and\nthe only difference in procedural rules is that there would have to\nbe a one month last call rather than a 2 week last call.  So I recommend\nsubmitting such a draft, and it can be acted on presuming general agreement.\n\nDefault-Style is something I've NEVER heard of.  You can write an\nID for it, but do so separately, so that it can be acted on independently\n(and not affect the other headers procedurally).  It could progress to\nproposed standard in a similar fashion.\nHope this helps,\n- Jim Gettys\n\n\n\n\n> From: Ian Hickson <py8ieh@bath.ac.uk>\n> Resent-From: Andy Norman <ange@hplb.hpl.hp.com>\n> Date: Wed, 24 Mar 1999 18:28:42 GMT\n> To: http-wg@hplb.hpl.hp.com\n> Cc: connolly@w3.org, \"L. David Baron\" <dbaron@fas.harvard.edu>,\n>         James Clark <jjc@jclark.com>\n> Subject: The HTTP 'link' and 'Content-Base' Headers\n> -----\n> Hi.\n> \n> It has been brought to my attention that the HTTP 'Link' header has been\n> removed from the latest version of HTTP 1.1 (IETF's Internet Draft dated\n> November 1998).\n> \n> I understand that the reason for this change was that the HTTP WG\n> considered the semantics to have been poorly defined. For example, one\n> post on HTTP-wg says:\n> \n> : There are known problems with some of the proposals: e.g. Link sytax\n> : does not quite seem to be what the HTML group needs for enabling\n> : server setting of sytle sheets; its grammar is suspect. [1]\n> \n> Could you please explain what is \"not quite what the HTML group needs\"\n> and why RFC 2068's definition of the grammar is 'suspect'?\n> \n\nI don't remember the exact details; maybe Dan Connolly does...\n\nIn any case, as I remember we were left with two definitions: one in the \nHTTP spec, and one in the HTML spec, and without knowledge of which was \nexactly correct.\n\nFurthermore, one needs 2 interoperable implementations to progress to\ndraft standard; at the time the interoperability reports were being\ndone last summer, there weren't known to be two clients, servers and\nproxies.\n\n\n> NGLayout, the engine behind version 5 of Netscape's browser, currently\n> comprehensively supports the HTTP Link header, as defined in RFC 2068.\n> No particular difficulties in interpreting the semantics of the 'Link'\n> header were hit when working on getting this feature implemented.\n\nGreat.\n\n> \n> This header is _incredibly_ useful, for, eg, setting the webmaster\n> email address of every document in a site or for changing the\n> stylesheet selected in every page of a 2000 document site, without\n> having to edit each file individually. (The \"Default-Style\" header is\n> very useful in this respect too.)\n\nNever heard of a \"Default-Style\" header. What is it?\n\n> \n> I have also noticed that the 'Content-Base' header was also deleted.\n> NGLayout also supports this HTTP header.\n> \n> Your time is greatly appreciated.\n> \n> -- References --\n> [1] quoted in:\n> http://hplbwww.hpl.hp.com/people/ange/archives/archives-97/http-wg-archive/160\n> 3.html\n> \n> Test cases exist for both of those headers at\n> http://www.bath.ac.uk/%7Epy8ieh/internet/importtest/\n> \n> NGLayout latest build is available from\n> ftp://ftp.mozilla.org/pub/mozilla/nightly/\n> \n> Note that NGLayout currently only supports HTTP headers specified in\n> META elements. This is due to a bug that is preventing any HTTP header\n> from reaching the document's DOM, and will be fixed prior to release.\n> \n> --\n> Ian Hickson\n> U+2642 U+2651\n> U+262E U+2603 U+263A\n\n\n\n", "id": "lists-012-9846413"}, {"subject": "FINAL: minor editorial issues in draft-ietf-http-v11-spec-rev06.tx", "content": "I've finally gotten to put the edits for the rfc editor together in\none file (this one), in order, in the format the RFC editor likes.\n\nThey are relative to the draft: draft-ietf-http-v11-spec-rev-06.txt.  No\none seems to have reported any others for a while, so lets put this puppy\nto bed, and get the RFC issued.\n\nKeith: this is it, as far as I'm concerned.  It is your judgement as to\nwhether you want to have the IESG look these over.\n\nHTTP working group: Unless very serious, forever hold your peace (or at \nleast until we prepare the full standard draft).  You should have spoken \nup by now (several times).  Please check for any final editing mistakes \nI may have made. After the RFC editor has done their job on a plaintext \nversion, I'll produce a postscript version we'll also distill to PDF; \nI'm still looking for a reasonable way to get to clean HTML and XML.\n\nYour editor,\n- Jim Gettys\nReferences:\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1999q1/0055.html\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1999q1/0058.html\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1999q1/0062.html\n\n=======================\nSection 3.2.2 change:\n\nOLD:\n    http_URL = \"http:\" \"//\" host [ \":\" port ] [ abs_path ]\n\nNEW:\n    http_URL = \"http:\" \"//\" host [ \":\" port ] [ abs_path [ \"?\" query ]]\n                                                         ^^^^^^^^^^^^^\n\nThis is another artifact of the removal of the URI syntax in\ndeference to RFC 2396.\n===========================\n\nsection 3.2.3 URI comparison\n\nOLD:\n   Characters other than those in the \"reserved\" and \"unsafe\" sets (see\n   section 3.2) are equivalent to their \"\"%\" HEX HEX\" encoding.\n\nNEW:\n   Characters other than those in the \"reserved\" and \"unsafe\" sets (see\n   RFC 2396 [42]) are equivalent to their \"\"%\" HEX HEX\" encoding.\n   ^^^^^^^^^^^^^\n\nChange 'section 3.2' to 'RFC 2396 [42]'. The definition of URI syntax\nhad been removed so the reference was wrong.\n==========================\nSection 3.6.1 \"Chunked Transfer Coding\"\nOLD:\n   trailer        = *entity-header\n\nNEW:\n   trailer        = *(entity-header CRLF)\n                     ^             ^^^^^^\nNote that these are all the same error, and it's been in the spec\nsince the very first draft (November 28, 1994).  *sigh*\n================\n4.1 Message Types:\nOLD:\n        generic-message = start-line\n                          *message-header\n\nNEW\n        generic-message = start-line\n                          *(message-header CRLF)\n                           ^              ^^^^^^\n\nNote that these are all the same error, and it's been in the spec\nsince the very first draft (November 28, 1994).  *sigh*\n================\n4.2 Message Headers:\nOLD:\n       message-header = field-name \":\" [ field-value ] CRLF\n                                                      ^^^^^\n\nNEW:\n       message-header = field-name \":\" [ field-value ]\n\nNote that these are all the same error, and it's been in the spec\nsince the very first draft (November 28, 1994).  *sigh*\n================\n5 Request\nOLD\n        Request       = Request-Line              ; Section 5.1\n                        *( general-header         ; Section 4.5\n                         | request-header         ; Section 5.3\n                         | entity-header )        ; Section 7.1\n                        CRLF\n                        [ message-body ]          ; Section 4.3\nNEW\n        Request       = Request-Line              ; Section 5.1\n                        *(( general-header        ; Section 4.5\n                          | request-header        ; Section 5.3\n                          | entity-header ) CRLF) ; Section 7.1\n                        CRLF\n                        [ message-body ]          ; Section 4.3\n\nNote that these are all the same error, and it's been in the spec\nsince the very first draft (November 28, 1994).  *sigh*\n================\n6 Response\nOLD\n       Response      = Status-Line               ; Section 6.1\n                       *( general-header         ; Section 4.5\n                        | response-header        ; Section 6.2\n                        | entity-header )        ; Section 7.1\n                       CRLF\n                       [ message-body ]          ; Section 7.2\nNEW\n       Response      = Status-Line               ; Section 6.1\n                       *(( general-header        ; Section 4.5\n                         | response-header       ; Section 6.2\n                         | entity-header ) CRLF) ; Section 7.1\n                       CRLF\n                       [ message-body ]          ; Section 7.2\n\nNote that these are all the same error, and it's been in the spec\nsince the very first draft (November 28, 1994).  *sigh*\n==========================\nSection: 8.1.4 Practical Considerations, last paragraph:\n\nOLD:\n   Clients that use persistent connections SHOULD limit the number of\n   simultaneous connections that they maintain to a given server. A\n   single-user client SHOULD NOT maintain more than connections with any\n\nNEW:\n   Clients that use persistent connections SHOULD limit the number of\n   simultaneous connections that they maintain to a given server. A\n   single-user client SHOULD NOT maintain more than 2 connections with any\n                                                    ^\n\nMissing \"2\" connections.  Very important! \n=======================\nSection 8.2.3, first sentence:\n\nOLD:\n   The purpose of the 100 (Continue) status (see section 10.1.1) is to\n   allow an client that is sending a request message with a request body\n\nNEW:\n   The purpose of the 100 (Continue) status (see section 10.1.1) is to\n   allow a client that is sending a request message with a request body\n         ^^\nchange \"an client\" to \"a client\"\n\n======================\nSection 8.2.4\nOLD:\n   connection close before receiving any status from the server, the\n   client SHOULD retry the request, subject to the restrictions in\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   section 8.2.3. If the client does retry this request,\n^^^^^^^^^^^^^^^^\n\nNEW:\n   connection close before receiving any status from the server, the\n   client SHOULD retry the request. If the client does retry this request,\n\n\nDelete the phrase\n  \", subject to the restrictions in section 8.2.3\"\nsince section 8.2.3 was deleted.\n\n========================\nSection 12.2:\n\nOLD:\n   With agent-driven negotiation, selection of the best representation\n   for a response is performed by the user agent after receiving an\n   initial response from the origin server. Selection is based on a list\n   of the available representations of the response included within the\n   header fields (this specification reserves the header name\n   Alternates) or entity-body of the initial response, with each\n   representation identified by its own URI. Selection from among the\n   representations may be performed automatically (if the user agent is\n   capable of doing so) or manually by the user selecting from a\n   generated (possibly hypertext) menu.\n\nNEW:\n   With agent-driven negotiation, selection of the best representation\n   for a response is performed by the user agent after receiving an\n   initial response from the origin server. Selection is based on a list\n   of the available representations of the response included within the\n   header fields or entity-body of the initial response, with each\n   representation identified by its own URI. Selection from among the\n   representations may be performed automatically (if the user agent is\n   capable of doing so) or manually by the user selecting from a\n   generated (possibly hypertext) menu.\n\n\n\ndelete the phrase \"(this specification reserves the header name\nAlternates)\"\nsince the reservation was in RFC 2068 but has since been removed.\n======================\nsection 13.5.1:\nOLD:\n      . Proxy-Authorization\n\n      . Transfer-Encoding\n\n      . Upgrade\n\nNEW:\n      . Proxy-Authorization\n\n      . TE\n      ^^^^\n\n      . Trailers\n      ^^^^^^^^^^\n\n      . Transfer-Encoding\n\n      . Upgrade\n\n\nAdd 'TE' and 'Trailer' to the list of Hop-by-Hop headers.\n======================\nsection 14.9 Cache Control:\n\nOLD:\n         | \"max-age\" \"=\" delta-seconds            ; Section 14.9.4\n\nNEW:\n         | \"max-age\" \"=\" delta-seconds            ; Section 14.9.3\n                                                                 ^\n\n\nchange the reference to 'Section 14.9.4' to 'Section 14.9.3'\n\n======================\nAdd at the end of section 14.32:\n\n        Note: because the meaning of \"Pragma: no-cache\" as a\n        response header field is not actually specified, it does not\n        provide a reliable replacement for \"Cache-control: no-cache\"\n        in a response.\n\n======================\nSection 14.11:\n\nOLD:\n\n   If the content-coding of an entity is not \"identity\", then\n   the response MUST including a Content-Encoding\nNEW:\n   If the content-coding of an entity is not \"identity\", then\n   the response MUST include a Content-Encoding\n                           ^\n\nchange 'including' to 'include'.\n\n======================\n21 Index:\n\nOLD:\n  URI. See RFC 2068                     wkday, 21\nNew:\n  URI. See RFC 2396                     wkday, 21\n  ^^^^^^^^^^^^^^^^^\n\nUpdate reference to URI draft standard syntx document\n======================\n\n\n\n", "id": "lists-012-9860521"}, {"subject": "Re: HTTP/1.1 Error Response", "content": ">> Actually, it has.  POST means only that data has been supplied to the\n>> process, to do with as it wishes. As far as HTTP is concerned, the request\n>> was successfully completed.\n>\n>Really???!!!!\n>\n>I find that statement very surprising.  \n>\n>It certainly does not fit with my reading of the start of Section 10.2 \n>  \"This class of status code indicates that the clients request was\n>   successfully received, understood AND ACCEPTED\"\n>and 10.2.1\n>   \"The request has succeeded\"\n>\n>It seems I should understand \"succeeded\" as nothing more than \"delivered\n>and understood\".  \n\nThat is the definition of POST.  If you want more semantics than that,\nyou need to define new status codes, but it makes no difference to a\nclient whether or not it is a success code because only a human user\ncould fix such a problem.  Interoperability is not recoverable.\n\n>But my uncertainty applies equally to form data encoded as \"GET\"....\n\nSame thing.  If the response is consistent, an error message is just\nas valid a representation of what was requested as normal content.\n\nThe reason I suggested 400 is because the x00 codes are also treated\nas the default for each class.  A new 4xx would be better, but not\nwithout a separate Internet Draft that defines the need for the code\nand how it can be used to improve interoperability.  For example,\nsomething that defined the set of fields required and optional, or\ndirected the client to a more appropriate interface.  The reason such\na thing is not in HTTP/1.1 is because nobody has implemented it.\n\n....Roy\n\n\n\n", "id": "lists-012-9877723"}, {"subject": "terminology questio", "content": "Hello,\n\nI'm just trying to understand the terms \n  \"representation\" and \"variant\".\n  \nAfter reading the spec for a while I suspect that\nevery possible entity from a given resource is a\n\"variant\" and if content negotiation comes into play\nit is termed \"representation\" as well. \n\nIf I have a document that changes only over time (eg CGI output), then\nare the different entities termed as different \"variants\"\nof the same resource as well? Or is it only sensible at a given\ninstant? Is there any practical explanation available?\n\nAny help is appreciated very much.\n\nThanks a lot in advance,\n\nJacob\n\n\n\n", "id": "lists-012-9886128"}, {"subject": "RE: terminology questio", "content": "> From: Jacob Schroeder [mailto:js@catilina.becomsys.de]\n> Subject: terminology question\n\n> I'm just trying to understand the terms\n>   \"representation\" and \"variant\".\n>\n> After reading the spec for a while I suspect that\n> every possible entity from a given resource is a\n> \"variant\" and if content negotiation comes into play\n> it is termed \"representation\" as well.\n\nI would rephrase that to make one distinction - every possible entity\nreturned for a resource is a variant of that resource.  Any particular\nentity returned for a particular request is a representation of that\nresource.\n\n> If I have a document that changes only over time (eg CGI output), then\n> are the different entities termed as different \"variants\"\n> of the same resource as well?\n\nOnly in some theoretical sense, I guess.\n\nIs there some particular rule that seems ambiguous for your case that hinges\non the definition of these terms?\n\n\n\n", "id": "lists-012-9893542"}, {"subject": "Re: terminology questio", "content": ">After reading the spec for a while I suspect that\n>every possible entity from a given resource is a\n>\"variant\" and if content negotiation comes into play\n>it is termed \"representation\" as well. \n\nNope, that's backwards.  Each possible entity from a resource is\na \"representation\" of that resource at the time the message originated.\nA representation is a variant if, at origination time, the set of\npossible representations has a membership greater than one.  It is called\na variant because the chosen representation varies based on the request\nparameters (content negotiation).\n\n>If I have a document that changes only over time (eg CGI output), then\n>are the different entities termed as different \"variants\"\n>of the same resource as well? Or is it only sensible at a given\n>instant? Is there any practical explanation available?\n\nA resource is a mapping function based on time, so its value set is\nbased on the given instant.\n\nA more extensive explanation is in my web architecture paper (submitted\nto ESEC/FSE99, so I can't give people a copy right now).\n\n....Roy\n\n\n\n", "id": "lists-012-9901857"}, {"subject": "Re: terminology questio", "content": "\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> writes:\n\n    >If I have a document that changes only over time (eg CGI output), then\n    >are the different entities termed as different \"variants\"\n    >of the same resource as well? Or is it only sensible at a given\n    >instant? Is there any practical explanation available?\n    \n    A resource is a mapping function based on time, so its value set is\n    based on the given instant.\n    \nI've suggested the term \"instance\" as a way to describe a member\nof this instantaneous value set:\n\n       The entity that would be returned in a status-200\n       response to a GET request, at the current time, for\n       the selected variant of the specified resource, but\n       without the application of any content-coding or\n       transfer-coding.\n\n(see http://www.ietf.org/internet-drafts/draft-mogul-http-digest-01.txt)\n\nWithout such a definition, it's fairly difficult to precisely specify\nmechanisms such as delta encoding.\n\n-Jeff\n\n\n\n", "id": "lists-012-9910224"}, {"subject": "Re: terminology questio", "content": "Thank you (all) very much for answering so promptly!\n\nSo it seems at least to be consesus, that the definition in the actual HTTP/1.1\ndocument is obsolete :( , but since the terms seem to be used consistently\nthere, this does not much harm to the document itself.\n\nI'm writing on a text covering some parts of HTTP and CN and I don't want\nto use the terms \"wrongly\".\n\nOn Tue, Apr 06, 1999 at 06:52:48AM -0700, Roy T. Fielding wrote:\n> >After reading the spec for a while I suspect that\n> >every possible entity from a given resource is a\n> >\"variant\" and if content negotiation comes into play\n> >it is termed \"representation\" as well. \n> \n> Nope, that's backwards.  Each possible entity from a resource is\n> a \"representation\" of that resource at the time the message originated.\n> A representation is a variant if, at origination time, the set of\n> possible representations has a membership greater than one.  It is called\n> a variant because the chosen representation varies based on the request\n> parameters (content negotiation).\n> \nSo the different possible entities produced by some CGI script (maybe\nincluding the remote IP address) would be termed \"variant\" as well, and this\ncould be considered a special case of content negotiation? (I know this sounds\ntheoretically, but this kind of questions are the ones that help me most)\n\nThanks a lot\n\nJacob\n\n> ...\n\n\n\n", "id": "lists-012-9918364"}, {"subject": "Re: terminology questio", "content": ">So the different possible entities produced by some CGI script (maybe\n>including the remote IP address) would be termed \"variant\" as well, and this\n>could be considered a special case of content negotiation? (I know this sounds\n>theoretically, but this kind of questions are the ones that help me most)\n\nIf they are all for the same resource, yes.  A CGI script is a handler\nfor a (possibly infinite) number of resources, so it depends on how the\nscript binds URI to representations.\n\n....Roy\n\n\n\n", "id": "lists-012-9926924"}, {"subject": "Re: terminology questio", "content": "Jacob Schroeder:\n>\n>Thank you (all) very much for answering so promptly!\n>\n>So it seems at least to be consesus, that the definition in the actual HTTP/1.1\n>document is obsolete :( , but since the terms seem to be used consistently\n>there, this does not much harm to the document itself.\n\nI don't see how the 1.1 terms would be obsolete.  It is true that some\nconcepts needed when one is to extend HTTP are not defined as terms in\nHTTP/1.1.\n\n>On Tue, Apr 06, 1999 at 06:52:48AM -0700, Roy T. Fielding wrote:\n>> Nope, that's backwards.  Each possible entity from a resource is\n>> a \"representation\" of that resource at the time the message originated.\n\nThe best way to think of an entity, in my opinion, is to consider it\nto be a _copy_ of a representation which was made at some time.  A\nrepresentation exists internally in a server.  An entity exists in a\nHTTP response (or request).\n\n>> A representation is a variant if, at origination time, the set of\n>> possible representations has a membership greater than one.\n\nNo, this is not how the 1.1 spec defines it.  I would say that in 1.1,\n'representation' and 'variant' are synonymous terms.  A cut-and-paste\nof the definition:\n\n   variant\n      A resource may have one, or more than one, representation(s)\n      associated with it at any given instant. Each of these\n      representations is termed a `variant.' Use of the term `variant'\n      does not necessarily imply that the resource is subject to\n      content negotiation.\n\nAll this means that the term 'variant' is not very useful when\ndefining details of content negotiation.  Changing the meaning of\n'variant' from the 1.1 definition, so that it signifies something more\nspecific than 'representation', is not the way to go.  Jeff defined a\nterm 'instance' to mean something more specific in his work, and I\ndefined another term 'variant resource' for transparent content\nnegotiation (rfc2295), also to mean something more specific.\n\n[...]\n\n>So the different possible entities produced by some CGI script (maybe\n>including the remote IP address) would be termed \"variant\" as well, and this\n>could be considered a special case of content negotiation? (I know this sounds\n>theoretically, but this kind of questions are the ones that help me most)\n\nDue to their usage in various specs and discussions (rather than their\ndefinitions in 1.1), the terms 'variant' and 'content negotiation'\nimply, for most readers, that a choice is being made between a few\ndifferent representations which are present _at one point in time_.\n\nIf a CGI script constructs a completely different entity from scratch\nfor every remote IP address, it is better to call this 'dynamic\ncontent' or a 'dynamic resource', and not talk about content\nnegotiation.  One would say that the script computes 'representations'\nor 'entities', not 'variants'.\n\nWhile one could theoretically say that dynamic content is a special\ncase of, or the same as, content negotiation, doing so in practice\nwould dilute the term content negotiation, and would be more confusing\nthan helpful.\n\n>\n>Thanks a lot\n>\n>Jacob\n\nKoen.\n\n\n\n", "id": "lists-012-9934313"}, {"subject": "Re: terminology questio", "content": ">>> Nope, that's backwards.  Each possible entity from a resource is\n>>> a \"representation\" of that resource at the time the message originated.\n>\n>The best way to think of an entity, in my opinion, is to consider it\n>to be a _copy_ of a representation which was made at some time.  A\n>representation exists internally in a server.  An entity exists in a\n>HTTP response (or request).\n\nThe representation is the information you get when the entity is\nextracted from the HTTP message -- it is the data transferred, not\nthe internal machine representation on the server.\n\n>>> A representation is a variant if, at origination time, the set of\n>>> possible representations has a membership greater than one.\n>\n>No, this is not how the 1.1 spec defines it.  I would say that in 1.1,\n>'representation' and 'variant' are synonymous terms.  A cut-and-paste\n>of the definition:\n>\n>   variant\n>      A resource may have one, or more than one, representation(s)\n>      associated with it at any given instant. Each of these\n>      representations is termed a `variant.' Use of the term `variant'\n>      does not necessarily imply that the resource is subject to\n>      content negotiation.\n>\n>All this means that the term 'variant' is not very useful when\n>defining details of content negotiation.\n\nNo, it means the definition in the spec is wrong, as I said a couple\nhundred times in our teleconferences.  You cannot make a wrong thing right\njust because I was outvoted.  These are terms defined by the architectural\nmodel of the Web, not defined by HTTP.\n\n....Roy\n\n\n\n", "id": "lists-012-9944706"}, {"subject": "Re: terminology questio", "content": "Roy T. Fielding:\n>\n\n     [Roy:]\n>>>> A representation is a variant if, at origination time, the set of\n>>>> possible representations has a membership greater than one.\n>>\n  [Koen:]\n>>No, this is not how the 1.1 spec defines it.  I would say that in 1.1,\n>>'representation' and 'variant' are synonymous terms.  A cut-and-paste\n>>of the definition:\n>>\n>>   variant\n>>      A resource may have one, or more than one, representation(s)\n>>      associated with it at any given instant. Each of these\n>>      representations is termed a `variant.' Use of the term `variant'\n>>      does not necessarily imply that the resource is subject to\n>>      content negotiation.\n>>\n>>All this means that the term 'variant' is not very useful when\n>>defining details of content negotiation.\n\n [Roy:]\n>No, it means the definition in the spec is wrong, as I said a couple\n>hundred times in our teleconferences.  You cannot make a wrong thing right\n>just because I was outvoted.  These are terms defined by the architectural\n>model of the Web, not defined by HTTP.\n\nRoy,\n\nI was not aware that you were somehow advocating a 'right', but\ndifferent definition of the term variant all along.  I don't recall\nany big fuss about the term any teleconference, but then again I was\nnot in all teleconferences.\n\nIt is generally not a good idea to redefine terms, even if the\noriginal definition is somehow wrong.  Having two incompatible\ndefinitions of 'variant' around can only lead to confusion.  If you\nhave some interesting concept you want to define a term for, there are\nplenty of other, uncontaminated, words you could pick.\n\n>....Roy\n\nKoen.\n\n\n\n", "id": "lists-012-9953680"}, {"subject": "Re: terminology questio", "content": ">It is generally not a good idea to redefine terms, even if the\n>original definition is somehow wrong.  Having two incompatible\n>definitions of 'variant' around can only lead to confusion.  If you\n>have some interesting concept you want to define a term for, there are\n>plenty of other, uncontaminated, words you could pick.\n\nI agree.  From the Merriam Webster online dictionary:\n\n    variant\n\n    noun\n    Date: 1848\n\n    : one of two or more persons or things exhibiting\n    usually slight differences: as a : one that exhibits\n    variation from a type or norm b : one of two or\n    more different spellings (as labor and labour) or\n    pronunciations (as of economics \\ek-, Ek-\\) of the\n    same word c : one of two or more words (as\n    geographic and geographical) or word elements\n    (as mon- and mono-) of essentially the same\n    meaning differing only in the presence or absence\n    of an affix \n\nIt makes absolutely no sense whasoever to use the term variant to\ndescribe a singleton representation.  The HTTP spec does so because\nsomebody wanted a shortcut and didn't like the word \"representation\".\nThe HTTP spec only needs to be self-consistent, so this is not a\ntechnical error -- it is just confusing.  I refuse to allow that confusion\nto propagate into anything else I write about the Web, including e-mail.\n\n....Roy\n\n\n\n", "id": "lists-012-9963150"}, {"subject": "Download With Starting Byte !", "content": "Is there someone know Which header from http that i can use to send\nstarting byte from the file that i want to download ?    \n\nThanks For All Attention\nRonny\n\n\n\n", "id": "lists-012-9971359"}, {"subject": "Proxies and persistent connection", "content": "Hi all,\n\nMe and a colleague are starting to work on upgrading a proxy server\nproduct from HTTP/1.0 to HTTP/1.1 and in the course of a design\ndiscussion, a question came up.\n\nUpon the opening of a persistent connection from a user-agent to a\nproxy, is it OK for the user-agent to send multiple requests *for\ndifferent hosts* down this same connection to the proxy?\n\nMy reading of the spec makes it seem that it is indeed OK, but I don't\nhave an HTTP/1.1 proxy handy to be able to see how browsers behave.\nBut it seems almost mandatory since clients are encouraged to open\nno-more than two connections to a given proxy...  Does that imply that\nproxies should/must be able to \"multiplex\" responses - ie send\nmultiple HTTP/1.0 and HTTP/1.1 responses from various servers in the\nproper order into a single persistent connection with a client?\n\nThis of course increases proxy implementation complexity quite a bit\nwrt the other possibility, which is: one (or two) persistent\nconnection(s) per origin server (even through proxies)...\n\nCan anyone help clarify?\n\n\nBTW, I there is a typo in draft-ietf-http-v11-spec-rev-06.txt, section\n8.1.4, last paragraph. \"A single-user client SHOULD NOT maintain more\nthan connections with any server or proxy.\" should, I think read \"A\nsingle-user client SHOULD NOT maintain more than 2 connections with\nany server or proxy.\"\n\nTIA,\n===\nWham! <wham_bang@yahoo.com>\n\n\n\n_________________________________________________________\nDo You Yahoo!?\nGet your free @yahoo.com address at http://mail.yahoo.com\n\n\n\n", "id": "lists-012-9977895"}, {"subject": "Re: Proxies and persistent connection", "content": "> From: Wham Bang <wham_bang@yahoo.com>\n> Resent-From: http-wg@hplb.hpl.hp.com\n> Date: Fri, 23 Apr 1999 12:43:09 -0700 (PDT)\n> To: http-wg@cuckoo.hpl.hp.com\n> Subject: Proxies and persistent connections\n> -----\n> Hi all,\n> \n> Me and a colleague are starting to work on upgrading a proxy server\n> product from HTTP/1.0 to HTTP/1.1 and in the course of a design\n> discussion, a question came up.\n> \n> Upon the opening of a persistent connection from a user-agent to a\n> proxy, is it OK for the user-agent to send multiple requests *for\n> different hosts* down this same connection to the proxy?\n\nOf course.\n\nWhat you should not do as a proxy is try to simultaneously use the\nsame connection on behalf of two different clients; otherwise, you could\nhave denial of service attacks.\n\nSerial reuse of a connection is just fine. (anotherwards, as soon as you\nget all responses back from a set of requests, you are free to reuse that\nconnection on behalf of another or the same client.\n\n> \n> My reading of the spec makes it seem that it is indeed OK, but I don't\n> have an HTTP/1.1 proxy handy to be able to see how browsers behave.\n> But it seems almost mandatory since clients are encouraged to open\n> no-more than two connections to a given proxy...  Does that imply that\n> proxies should/must be able to \"multiplex\" responses - ie send\n> multiple HTTP/1.0 and HTTP/1.1 responses from various servers in the\n> proper order into a single persistent connection with a client?\n\nYup.\n\nMust be in order, as HTTP has no request number to allow a client to\nget out of order responses.  This is a basic \"bug\" (some have claimed\nfeature, I, and many other claim bug) in HTTP design.\n\n> \n> This of course increases proxy implementation complexity quite a bit\n> wrt the other possibility, which is: one (or two) persistent\n> connection(s) per origin server (even through proxies)...\n> \n> Can anyone help clarify?\n> \n> \n> BTW, I there is a typo in draft-ietf-http-v11-spec-rev-06.txt, section\n> 8.1.4, last paragraph. \"A single-user client SHOULD NOT maintain more\n> than connections with any server or proxy.\" should, I think read \"A\n> single-user client SHOULD NOT maintain more than 2 connections with\n> any server or proxy.\"\n> \nYup; we know about the typo; will be fixed in the RFC.\n- Jim\n\n\n\n", "id": "lists-012-9986619"}, {"subject": "Semantics of chunkexts", "content": "I'm implementing Transfer-Encoding: chunked, and I'm trying to\nfigure out what to do if I get a chunk-ext I don't understand\n(which is to say, any chunk-ext, since RFC-2068 doesn't define\nany).  Currently I ignore it and log a warning, on the theory\nthat any future extension should be backwards compatible--but\n2068 is silent on whether I can expect compatibility, and I'm not\nsure what kind of compatible extensions might make sense anyway.\nAny advice?\n\n--\n/=============================================================\\\n|John Stracke    | My opinions are my own | S/MIME & HTML OK  |\n|francis@ecal.com|============================================|\n|Chief Scientist | NT's lack of reliability is only surpassed |\n|eCal Corp.      |  by its lack of scalability. -- John Kirch |\n\\=============================================================/\n\n\n\n", "id": "lists-012-9996552"}]