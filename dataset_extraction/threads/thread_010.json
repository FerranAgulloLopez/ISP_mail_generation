[{"subject": "I need more informatio", "content": " Hi,\n\n My name is Djoko Kris Sundoro from Indonesia. I'm collage student\n at STMIK Gunadarma Jakarta Indonesia. I need more information about\n hypertext and Internet. I need that information because I want to\n learn to write a book about Hypertext. The Book title is \"Mengapa\n Hypertext begitu di gandrungi pada Internet\" or in English language\n is \" Why the Hyperext its very populer in Internet user \".\n\n And I hope you can give more information about :\n\n 1. What it's Internet\n 2. What it's HTTP and Hypertext\n\n I think for this time I just give you two question and I hope you\n will be answer for the question.\n\n I was really sorry if i make mistake in my English Languge.\n Thanks before.\n\n Best Regards\n\n djoko@pusdata.dprin.go.id\n\n\n... Blue Wave - World Tour - 1996\n___ Blue Wave/DOS v2.30\n\n\n\n", "id": "lists-010-0000000"}, {"subject": "Re: About that Host: header...", "content": "> Discussion:\n> \n> I personally see only options 2 and 4 having a high likelyhood of\n> the \"correct outcome\" allowing multiple sites to be served from the\n> same server, within finite time of starting to transition to 1.1.  I\n> believe the W.G. needs to select either 2 or 4 to resolve this\n> issue.\n> \n> There was a statement by Ari Luotonen at the IETF meeting that he\n> believed that solution 2) to be unacceptable to Netscape.  Ari, is\n> this true now that you've had time to think about it?  Paul, can you\n> see what Microsoft's opinion on this topic is?\n\nYes, 2 is unacceptable.  We'd be breaking the entire universe, or\nmaking an ugly kludge if we then require a retry with 1.0 if and when\nthe server doesn't accept the 1.1 request.  Furthermore, HTTP/1.x is\ndefined to be backword and forward compatible with other HTTP/1.x, so\neven in that light this would be unacceptable, because a valid\nHTTP/1.1 request would not yield a correct response from a HTTP/1.0\nserver.\n\nMaking the Host: header a _required_ header in HTTP/1.1 clearly solves\nthe problem, and is painless, with no interoperability problems.  How\nyou choose to enforce it I don't really have a strong opinion about\n(error msg vs ignore); I don't think an error response should be\nrequired from servers that don't exploit the Host: header, but it\nwould be fine for servers that serve multiple domains.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n685 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n", "id": "lists-010-0007239"}, {"subject": "Re: About that Host: header...", "content": "At 11:52 am 3/18/96 -0500, jg@w3.org spake:\n>What say you all?\n\n   #2 (Full URL) may give server admins a real incentive to \nswitch over quickly--the impulse to jealously guard bandwidth \ncan be powerful. \n\n   I may have to run a 1.0 and 1.1 server simultaneously on separate \nIP addresses for a while, but I don't think it would take long to \nswitch over with that kind of incentive. \n\n   #4 (Host+Error) doesn't give any such incentive. \n\n   I vote for #2 (if my vote counts) and I'll recover 6 IP addresses \nthat much sooner! ;^)   \n\n\n+--------------------------------------------------------------------------+\n| Author of The CGI Book -- http://www.bearnet.com/cgibook/ \n| http://www.bearnet.com/  ||  http://www.weinman.com/wew/\n\n\n\n", "id": "lists-010-0017713"}, {"subject": "Re: About that Host: header...", "content": ">    #2 (Full URL) may give server admins a real incentive to \n> switch over quickly--the impulse to jealously guard bandwidth \n> can be powerful. \n> \n>    I may have to run a 1.0 and 1.1 server simultaneously on separate \n> IP addresses for a while, but I don't think it would take long to \n> switch over with that kind of incentive. \n\nPlease be realistic -- full HTTP/1.1 support in a server can't be done\novernight -- and the spec is much less tolerant about non-fully\ncompliant servers this time.  Hence -- you won't necessarily have your\nfavorite HTTP/1.1 server available the moment there is a HTTP/1.1\nclient around, and you have bug reports flowing in because your server\nis 'broken' for the new 1.1 client.\n\nIn any case -- whether the host is in the URL or the Host: header is a\nmere matter of taste.  One of them breaks things, the other doesn't --\nso which one do we pick?\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n685 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n", "id": "lists-010-0026005"}, {"subject": "Re: About that Host: header...", "content": ">In any case -- whether the host is in the URL or the Host: header is a\n>mere matter of taste.  One of them breaks things, the other doesn't --\n>so which one do we pick?\n\nI wouldn't say \"mere\" taste. If one has one wart on the spec thats OK.\nTen or twenty and the thing will start to get raqged. As I see it we have\ntwo issues:\n\n1) Backwards compatiblility with installed server base.\n\n2) Desire to have 1.2 be clean.\n\n\nCan we enforce this as follows:\n\n1) A http server must accept either a full URI _or_ a host header but\nnot both.\n\n2) A http 1.2 client will be required to use a full URI if 1.2 is reported.\n\n\nNote that a 1.2 server would be required to accept a host header even\nthough a 1.2 client would no longer generate one. In that sense the\nstrict forwards compatibility of the 1.x series would be preserved while\nensuring that we would move to using only full URIs in a reasonably rapid\ntimeframe. \n\nJims suggestion that a 1.1 server produce an error if a message claims to \nbe 1.1 but is not also sounds a good move.\n\n\nI think that John has a point about wanting to avoid carbunkles but\nwe can't really expect to introduce a change that will cause undue numbers\nof breakages.\n\nPhill\n\n\n\n", "id": "lists-010-0034798"}, {"subject": "Re: About that Host: header...", "content": "> \n> \n> >In any case -- whether the host is in the URL or the Host: header is a\n> >mere matter of taste.  One of them breaks things, the other doesn't --\n> >so which one do we pick?\n> \n> I wouldn't say \"mere\" taste. If one has one wart on the spec thats OK.\n> Ten or twenty and the thing will start to get raqged. As I see it we have\n> two issues:\n> \n> 1) Backwards compatiblility with installed server base.\n> \n<snip>\n> \n> Phill\n\nI don't think people have adequately considered the impact of the installed\nserver base.  In a crawl last fall, we found several 1.1 & 1.2 version of the\nNCSA server still out there, and a large number NCSA 1.3 out there despite\nour efforts to get people to upgrade to either 1.4 or 1.5.  If it isn't \nbroken, you aren't going to get alot of sites to upgrade.\n\nSo, the question I have is: Is it better to accomodate those old sites or\nto intentionally break the software to force them to upgrade?\n\nI think any response that is hidden from the users (ie handled automatically by\na client with a retry) will simply increase bandwidth without any noticable\nnoise from users.  Without noise from the users, I doubt alot of sites will\nupgrade.  I oppose the retry idea.  I'm mildly in favor the Host header since\nit has already been implemented by some of the browsers and clients, but we'll\nsupport whatever is contained in the final specification.  \n\n-- \nElizabeth(Beth) Frank\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n", "id": "lists-010-0043367"}, {"subject": "HTTP protocol and COBOL program", "content": "Hi everybody, \n\ndoes anybody know if it is possible (or even already done) to implement\na HTTP programming interface in order to have a COBOL program to communicate\nwith a web browser ???\n\nThanking you in advance\n\n!----------------------------------------------------------------------------!\n!Christophe Furet     !\n!Institut National Agronomique de Paris-Grignon     !\n!Laboratoire d'Informatique     !\n!16 rue Claude Bernard.      !\n!75231 Paris cedex 05. France.     !\n!     !\n!Tel  : (33) (1) 44 08 16 64     !\n!Fax  : (33) (1) 44 08 16 66     !\n!Mail : furet@inapg.inra.fr     !\n!----------------------------------------------------------------------------!\n\n\n\n", "id": "lists-010-0052179"}, {"subject": "Re: About that Host: header...", "content": "Ari Luotonen:\n>> There was a statement by Ari Luotonen at the IETF meeting that he\n>> believed that solution 2) to be unacceptable to Netscape.  Ari, is\n>> this true now that you've had time to think about it?  Paul, can you\n>> see what Microsoft's opinion on this topic is?\n>\n>Yes, 2 is unacceptable.  We'd be breaking the entire universe, or\n>making an ugly kludge if we then require a retry with 1.0 if and when\n>the server doesn't accept the 1.1 request.\n\nI too find that 2 is unacceptable.  Protocol easthetics is not nearly\ngood enough a reason to break compatibility on such a fundamental\nlevel.\n\n>Ari Luotonenari@netscape.com\n\nKoen.\n\n\n\n", "id": "lists-010-0059343"}, {"subject": "Re: About that Host: header...", "content": ">I think any response that is hidden from the users (ie handled\n>automatically by\n>a client with a retry) will simply increase bandwidth without any noticable\n>noise from users.  Without noise from the users, I doubt alot of sites will\n>upgrade.  I oppose the retry idea.\n\nI strongly agree with Beth here. Silently retries on 1.0 servers is going\nto increase bandwidth, decrease functionality to the users, and probably\nnot be cared about by HTTP admins. This assures us that when we go to 1.2\n(full URI in request), there will be mass breakage on the Internet, and all\nfingers will point at the IETF. Again.\n\nJim's (4) is best in my mind. With this scheme, someone can easily write a\ntesting robot that wanders the Web checking for conformant and\nnon-conformant hosts, creating a list of shame for those that run buggy\nserver software. Trust me, sites that get on this list will either demand a\nquick upgrade of their software or switch vendors. Wearing my WebCompare\nhat, I'll certainly write and run such software, but I imagine others in\nthe press will beat me to the punch, and that's just fine with me.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-0068968"}, {"subject": "Re: About that Host: header...", "content": "Would it be possible to cover some middle ground,\n\nI'd be very happy which something like the following scheme, which fits well\nwith some of the technology out there:\n\n\n1. All 'new' servers should be able to understand both\n\nGET http://something/somewhere HTTP/1.x\n*and*\nGET somewhere HTTP/1.y \n(with an optional host; which if there should be\ndealt with accordingly, regardless of the IP)\n\n   Wether x and y can be 1 and 0 is open to debate, I would be in\n   favour of having y=0 only and x=0 or 1. \n\n   This regardless of wether their marketing drones claim that\n   is speaks http/1.9 or wathever.\n\n2. All (current) servers should be able to understand \n   the Host: something line, at best they should not break\n   when one is flashed at them. \n\n3. All 'new' clients should do a \n\nGET somewhere HTTP/1.0\nHost: something\n\n   while we are still in http 1.0 and a full\n\n    GET http://something/somewhere HTTP/1.1\n\n    when they are really a 1.1 (and know (how?) that the\n    server will swallow 1.1).\n\nDoes this make sense ? I agree, it is not perfect, but it\nallows some of us to slowly start to optimizing our servers\nand installatins without breaking toooo much.\n\nDoes this make sense as a bad kind of middle ground.\n\nDw.\n\n\n\n", "id": "lists-010-0077166"}, {"subject": "Re: About that Host: header...", "content": "Paul Hoffman:\n>[...]\n>This assures us that when we go to 1.2 (full URI in request) [...]\n\nAs far as I know, we have not yet decided that 1.2 will require a full\nURI in the request.\n\nKoen.\n\n\n\n", "id": "lists-010-0085407"}, {"subject": "Re: About that Host: header...", "content": "At 17:50 96.03.19 +0100, Koen Holtman wrote:\n>I too find that 2 is unacceptable.  Protocol easthetics is not nearly\n>good enough a reason to break compatibility on such a fundamental\n>level.\n\nThis is not a matter of aesthetics, it is a matter of long-term operability/\nsurvivability of HTTP on the network.  If we could *guarantee* accurate\nimplementations of other strategies, I'd actually have fewer problems with them.\n\nFor example, let's assume that we \"require\" that \"host\" be present in all\ncases as a way out.  That certainly simplifies the protocol a bit, because\nclient implementations don't have to make choices about when to send it.\nBut \"require\" doesn't mean anything -- there is no way to enforce the\nrequirement.  Even if we \"require\" that servers return an error message, it\njust pushes the problem a bit further out.  We will find, I'm afraid\ninevitably, that some idiot will decide to not bother sending \"host\" in the\ninterest of a few extra cycles of efficiency and that other idiots will make\nthe server error message a configurable option, also in the interest of\nefficiency.    Extrapolation from the history of the Internet predicts to a\nlot of such idiots.\n\nAnd, behold, we will be exactly where we are today, but with one more\nsometimes-implemented, effectively optional, bag on the side of the protocol. \n\nTo fix it, we will have to change \"GET\" (and \"POST\", etc.).  Maybe we will\ncall what we get then HTTP 1.3, maybe 2.0, but that is another decision that\nis in the hands of the WG (and the industry).  But, if the big sticking\npoint now is \"1.1 has to be compatible with 1.0, and we can't put a change\nlike this in without calling it 2.0\", then I suggest the problem is\nimportant enough to justify a full version number.  The assumption that this\nnew collection of features and patches has to be \"1.1\" should, IMO, be\ntreated as just that -- a working assumption, not something inevitable if\ngood engineering requires otherwise.\n\n    john\n\n\n\n", "id": "lists-010-0092771"}, {"subject": "Re: About that Host: header...", "content": "While I agree with John I believe that we need to give something more than a\nfew weeks notice of a major change. There are some people who are very keen\nto have a 1.1 spec out in time for the May 1st parade (oops wrong country).\n\nPhill\n\n\n\n", "id": "lists-010-0104370"}, {"subject": "Re: About that Host: header...", "content": "John C Klensin:\n>\n>At 17:50 96.03.19 +0100, Koen Holtman wrote:\n>>I too find that 2 is unacceptable.  Protocol easthetics is not nearly\n>>good enough a reason to break compatibility on such a fundamental\n>>level.\n>\n>This is not a matter of aesthetics, it is a matter of long-term\n>operability/ survivability of HTTP on the network.  If we could\n>*guarantee* accurate implementations of other strategies, I'd\n>actually have fewer problems with them.\n\nWe cannot *guarantee* accurate implementations of anything.\n\n>For example, let's assume that we \"require\" that \"host\" be present in all\n>cases as a way out.\n\nThis is what I propose.\n\n>  That certainly simplifies the protocol a bit, because\n>client implementations don't have to make choices about when to send it.\n\nYes, and a simple protocol has the highest change of being implemented\naccurately.  Plus, major browsers already do this.\n\nI do not have to prove to you that leaving the request line format as\nit is and requiring Host in 1.1 guarantees an accurate implementation.\nI can't prove that anyway.  I believe this option gives the best\nchanges of accurate implementation, though.\n\nI would like to see some arguments from your side on why the\n\"long-term operability/ survivability of HTTP on the network\" depends\non 2, which is requiring the full URI in the 1.1 request line.\nEspecially because 2 will break the short-term operability.\n\nIn my opinion, introducing mass breakage by requiring the full URI in\nthe 1.1 request line will _not_ make implementations more accurately\nfollow the 1.1 protocol.  Quite the contrary.  \n\n>But, if the big sticking\n>point now is \"1.1 has to be compatible with 1.0, and we can't put a change\n>like this in without calling it 2.0\",\n\nYes, this is the sticking point.\n\n> then I suggest the problem is\n>important enough to justify a full version number.\n\nI don't think it is, but we can have this discussion after may 1.\n\n>    john\n\nKoen.\n\n\n\n", "id": "lists-010-0113464"}, {"subject": "Re: About that Host: header...", "content": "At 10:11 PM 3/19/96 -0500, John C Klensin wrote:\n>requirement.  Even if we \"require\" that servers return an error message, it\n>just pushes the problem a bit further out.  We will find, I'm afraid\n>inevitably, that some idiot will decide to not bother sending \"host\" in the\n>interest of a few extra cycles of efficiency and that other idiots will make\n>the server error message a configurable option, also in the interest of\n>efficiency.    Extrapolation from the history of the Internet predicts to a\n>lot of such idiots.\n\nThose same idiots would enable 200 OK responses to HTTP/1.1 requests that\ndon't have the full URL in the request line and use the 1.0 partial URL\nstyle, or they'd make the error response a configurable option.  So it's a\nmoot point.\n\nI think Host: will be just fine.\n\n\n-----\nthe Programmer formerly known as Dan          \n                                     http://www.spyglass.com/~ddubois/\n\n\n\n", "id": "lists-010-0124483"}, {"subject": "Issues list..", "content": "I've spent a number of hours just reorganizing the\nissues list, to make it less random.\nAs usual, it is: \nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/http-wg.html\n\nAs usual (until Larry returns), let me know\nof problems with it.\n- Jim\n\n\n\n", "id": "lists-010-0134794"}, {"subject": "RE: About that Host: header...", "content": ">----------\n>From: Ari Luotonen[SMTP:luotonen@netscape.com]\n>Sent: Monday, March 18, 1996 10:39 PM\n\n>In any case -- whether the host is in the URL or the Host: header is a\n>mere matter of taste.  One of them breaks things, the other doesn't --\n>so which one do we pick?\n\n>An observation: Another way to \"clean up\" the protocol is to move the\n>host part of the Request-URI in requests to proxies to the Host:\n>header. 1.1 proxies would have to accept them in both the Request-URI\n>and the Host: and send a warning to 1.1 clients that put them in the\n>Request-URI;  1.2 proxies (2.0?) could require that they be in the\n>Host: header.\n>\n>I personally prefer option #2 if I were designing the protocol from\n>scratch, but think that we may have to compromise here. I also agree\n>with John Klensin's observations 100%, but the each such cleanup takes\n>a \"silver bullet\" and I'd rather save mine for a case where the\n>workarounds are much more complicated and error prone than Host:.\n>(I.e., his general principal is correct, but this case may not be the\n>best application of it...)\n>\n>Paul\n>\n\n\n\n", "id": "lists-010-0141006"}, {"subject": "Re: About that Host: header...", "content": "At 7:38 AM 3/20/96, Daniel DuBois wrote:\n>At 10:11 PM 3/19/96 -0500, John C Klensin wrote:\n>>requirement.  Even if we \"require\" that servers return an error message, it\n>>just pushes the problem a bit further out.  We will find, I'm afraid\n>>inevitably, that some idiot will decide to not bother sending \"host\" in the\n>>interest of a few extra cycles of efficiency and that other idiots will make\n>>the server error message a configurable option, also in the interest of\n>>efficiency.    Extrapolation from the history of the Internet predicts to a\n>>lot of such idiots.\n>\n>Those same idiots would enable 200 OK responses to HTTP/1.1 requests that\n>don't have the full URL in the request line and use the 1.0 partial URL\n>style, or they'd make the error response a configurable option.  So it's a\n>moot point.\n\nI completely agree with Daniel here. To assume that \"idiots\" will change\nthe request URL handing correctly, but would not change the Host: handling\ncorrectly, is plain silly.\n\nIn fact, I believe that of the two, they are *much* more likely to make\nsettings purposely allowing either form of request URLs than to make\nsettings allowing/disallowing Host: handling. This is the opposite of what\nany of us wants.\n\nAs John says, we can't force correct action on the part of the server or\nclient makers. Both solutions give the same long-term gain: when they're\nfully implemented, vanity IPv4 addresses go away. Given that, which of the\nsolutions will do the least short term harm? Clearly, forcing the \"Host:\"\nheader.\n\nAnd, again, we can definitely put lots of public pressure on server and\nbrowser makers who do it wrong. They'll be easy to determine, and easy to\nlampoon. The folks in the now-popular Internet print press will report\nincompatibility with HTTP/1.1 in a matter of days after the release of a\nbroken server or browser, if we educate them about the issue and purpose of\nthe feature. I grant you, this is not part of the IETF's engineering\ncharter, but it helps us get to that end quickly. :-)\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-0151253"}, {"subject": "Re: About that Host: header...", "content": "Although I agree with you (John) that the medium-term viability\nof the Internet may depend on getting Host: widely implemented,\nI think you are fantasizing if you think that any punitive\nmechanism will make this happen any faster.  I would count\n\"changing the version number\" or \"requiring full URLs in HTTP/1.1\"\nas punitive mechanisms, since the penalty for not conforming\nto these will be non-interoperation.\n\nIf we try to impose these requirements, the major vendors won't\ngo along (because their customers will feel the pain immediately,\nand will not see any immediate gain).  So instead of solving\nwhat is by any measure NOT a short-term problem, we will destroy\nthe rather fragile cooperative environment that allows us to\nhave an HTTP standard at all.\n\nNo matter how important Host: is to the Internet, trying to\nimpose it in such a way that causes immediate pain WILL NOT WORK.\n\nI submit that\n(1) As long as the major browser and server vendors\nare cooperating with the HTTP standardization process,\nwe should expect that the vast majority of HTTP/1.1\nimplementations will do Host: according to the current\nproposed spec.; i.e., we ought to trust people for now.\n\n(2) There will always be a few browsers out there that\ndon't send Host:; there is absolutely nothing we can\ndo about it.  So we should be working on getting HTTP/1.1\nout as soon as possible, and with as much cooperation\nfrom vendors, with the intent of reducing the number\nof obsolete browsers as quickly as possible.  But we\nshouldn't fool ourselves that this will eliminate\nvanity host addresses any time soon (but neither will\ntrying to impose a standard that nobody implements).\n\n(3) The best way to get people to change their behavior\nis to provide an incentive based on self-interest.\nVan Jacobson's slow-start mechanisms became popular\nnot just because they made the Internet work better,\nbut because they also made individual connections work\nbetter, and so even greedy people wanted Van's code.\nSo if we want to increase the rate of adoption for\nHost:, perhaps we should find some incentive to tie\nto it.\n\nI don't have a great suggestion for #3, but I suggest that\nthis is the direction we ought to be thinking in.\n\nI think we all agree that, in retrospect, the original HTTP\ndesign was faulty in not requiring full URLs.  But forming\na circular firing squad is no solution.\n\n-Jeff\n\n\n\n", "id": "lists-010-0160264"}, {"subject": "Re: About that Host: header...", "content": ">While I agree with John I believe that we need to give something more than a\n>few weeks notice of a major change. There are some people who are very keen\n>to have a 1.1 spec out in time for the May 1st parade (oops wrong country).\n\nPhill,\n\nI'm one of the people who is very keen to have a 1.1 spec out before May.\nI'm even more keen, from my MCI perspective, to start telling vendors about\nthings we insist that they do in order for us to buy their products... a\nlist that prominently features \"things that make service-bureau-like Web\nsites manageable and keep the net from melting down\".   I have an extremely\nstrong preference for stating those requirements in terms of reference to\nthe words in a standards-track document.  Why?  \n   (i)  Because it is The Right Thing To Do and \n   (ii) I really believe that a WG rough consensus position is more likely\n       to be correct --by virtue of reflecting analysis from \n       more different perspectives-- than anything I (or my \n       organization come up with by myself (ourselves)\n\nrom my MCI-ish point of view (people associated with vendors read that as\n\"this is a customer speaking\"), I've got no particular incentive to invest\nin deploying 1.1 products in the absence of a solution to the \"domain name\nnot delivered to server\" problem.  That doesn't mean we wouldn't deploy such\na thing, but we are not actively on the market for one.  Conversely, I do\nhave a lot of incentive to invest in and deploy a 1.1 (or 2.0) that does\ncontain a fix for this problem.\n\nrom my internet experience point of view, the current \"use 'host' if the\ndomain name isn't otherwise supplied\" language isn't a fix -- it is an\nattempted patch that won't work well enough.  I actually feel a little\nbetter about \"host required always\", but I don't think it is good enough and\nthat the rate of growth issues argue extremely strongly for \"fix it now, and\nfix it right\".\n\nAnd, while the schedule is tight, nothing has convinced me that we can make\nbetter decisions in an area like this if we spend the next six months\nthinking about it than if we, well, decide.  The marketplace won't wait\nwhile we engage in extended self-contemplation.\n\n   john\n\n\n\n", "id": "lists-010-0169716"}, {"subject": "Re: About that Host: header...", "content": "On Wed, 20 Mar 1996, John C Klensin wrote:\n\n> \n> And, while the schedule is tight, nothing has convinced me that we can make\n> better decisions in an area like this if we spend the next six months\n> thinking about it than if we, well, decide.  The marketplace won't wait\n> while we engage in extended self-contemplation.\n\nThere is clear concenus that #2 is unacceptable. I also would have\nprefered #2 but it can't be. #4 can be defined to interoperate and\nsolve the problem as defined.\n\nThis discussion has been useful because it saved me from having to\nargue in the coming weeks that for Host: to be meaningful it \nhad to be required. That's decided. Lets move on to our remaing\nissues as well as making sure the description of host: is correct.\n\nOne of the earlier posts which mentioned host: indicated that the\nhost port should be included with the host name. I fully support\nthat approach:\n  1.  It simplifies the definition of the content of host: ... just\n      take the whole host address portion of the URL\n  2.  It retains potentially useful information which is otherwise\n      lost.  For example, a firewall or other gateway might desire\n      to map two external listening ports to a single internal\n      server. In essence, the current protocol has the problem \n      because information is thrown away.  WHy take the risk.\n  3.  It happens to correspond to NETSCAPE's implementation.\n\nDave Morris\n\n\n\n", "id": "lists-010-0181747"}, {"subject": "Re: About that Host: header...", "content": "My other problem with Host: and keeping partial URLs is that the partial URL\nhack will get into the way of most serious things we can do where we\ndivorce the URL format from the HTTP protocol (like fetching things by\nother locators).\nAnd it encourages caches to continue breaking things they should have no\nbusiness breaking.\nThe Host: header is putting half the information in the wrong place.\n\nStrawman proposal:\n\n- HTTP/1.1 servers MUST respond correctly to full URLs.\n  HTTP/1.1 servers MUST also serve partial URLs that come in with a\n  HTTP/1.0 version field, MAY do so on HTTP/1.1 requests and SHOULD NOT\n  do so on HTTP/1.2 requests.\n\n- HTTP/1.1 clients MUST be able to be configured to send full URLs, for\n  compatibility with HTTP/1.2 servers.\n  If so configured, they SHOULD implement a per-server fallback strategy\n  to partial URLs for compatibility with old servers.\n  They MAY implement a configuration option to send partial URLs first,\n  and provide per-server fallback to full URLs.\n\n- Advance warning: Partial URLs will not be part of the HTTP/1.2 spec.\n\nThis gets us a clear statement that the future is full URLs, labels\n1.1 clearly as a transition spec, and gets the software out there (I HOPE!)\n\nWhat it doesn't get you is service bureau compatibility in minimal\nimplementations of 1.1 configured for speed over sanity.\n\nBTW, the fallback from full to partial URL has exactly the same roundtrip\ntiming as fallback from http://host/~home to http://host/~home/ on NCSA\nservers (other servers may be more optimized).\nGiven people's carelessness in specifying trailing slashes, do we have\nclear evidence that they care about one more RTT, especially if it is\nonly once per server?\n\n                               Harald A\n\n\n\n", "id": "lists-010-0193103"}, {"subject": "Re: About that Host: header...", "content": "I've been a bit reluctant to comment on this discussion to the list, since I'm\nserving as editor.  But in any case, here is my technical view.  And\nthe view is based on previous scars and overall technical design experience,\nrather than the particular issue at hand. \n\nSeveral points to note:\n\n1) the write up I made for Host: that sparked this discussion\nrequires servers to report an error if host: is not recieved AND the \nclient claims to be 1.1.  It also requires servers to handle full URL's.\nThis currently feels to be the rough consensus of the group about how\nto proceed.\n\nI believe this error reporting will likely eliminate the problem\nin a timely fashion, which satisfies the very valid needs of operational\nservers and address use on the internet, even if the solution isn't\ngraceful.\n\nWe may be able to remove this lint in HTTP 1.2,\nif 1.1 really becomes ubiquitous.  But then again, we may not, if 1.0 servers\ndon't die a timely death.\n\nNone the less, I believe we should understand the consequences of this\ndecision, and the possible outcome.\n\n2) I agree with John Klensin on this one: protocols have a lifetime,\nand the more lint they accumulate, the sooner they die, and in this\ncase, we are adding lint to the basic (most frequently used) request\nin the protocol, which we may or may not be able to remove.\n\nHTTP has already accumulated much lint in its evolution.\n\nMy experience with X's evolution drives this home to me personally; we started\nwith a protocol, which while a binary protocol, shared much of the same\nfundamental problems that HTTP does (inability to understand the length \nof requests/responses without parsing the messages in detail, slow\nto parse, various funny leftovers such as graphics derived from an obsolete \ngraphics device (the HTTP analogy is the use of MIME for messages), etc.\n(X Versions 1-10).\n\nWe came to the point, as X succeeded much more quickly than\nwe ever anticipated, that we had to redesign the X \nprotocol entirely; this redesign was X version 11.  This is what\npeople now refer to X.  While small potatoes compared to the Web, it\ndid spark a $10^9 industry.\n\nIn the X case, we BARELY got away with this redesign, and only because \neveryone bought into it.  The redesign (X11) was hurried (by probably 6 months), \nwhich has hurt in various subtle ways to this day.  It is clearly\na vast improvement over X10.  But various things in X11 \nare broken, and require work-arounds or cause performance problems; \nI will not bore this list with a  description of them.  \nI am happy it has worked as well as it has, over the \nlast 8 years of deployment.  But John's concerns about longevity of protocols\nis VERY real, and the experience of those who have designed protocols\nover the years should not be disregarded lightly.\n\nThe Host decision, if made where the current rough consensus I feel is,\nwill accellarate the day when we must deploy a replacement protocol.\nNow this isn't entirely bad.  I believe the historical baggage of\nHTTP 1.X is high enough in other areas to eventually force such a transition,\n\nWe should understand that we are hastening the day when HTTP 1.X will\nneed repacement, and will have less time to design whatever HTTP-NG \nends up being.  And I can tell you that being hurried in the design \nof such a protocol can result in serious problems, you get to live with\nfor many years.\n\nSo lets make this decision (which sounds to me like it has been made\nalready, unless this message sparks people to reconsider their positions)\nand move on; I just want everyone to realize the possible consequences.\n- Jim Gettys\n\n\n\n", "id": "lists-010-0204209"}, {"subject": "Re: About that Host: header...", "content": "(CC lines trimmed - they were getting hairy...)\n\nLet me say that I as AD will go along with the group consensus on three\nconditions:\n\n1) I believe that there is a rough consensus in the group\n2) I believe that the consensus is clearly documented, and that the\n   documentation reflects the consensus\n3) I believe that the consensus will not cause grievous harm to the\n   infrastructure.\n\nIn this case I don't believe that condition (2) is satisfied: I can't tell\nfrom looking at documents exactly what the consensus is.\nCondition (1) may be satisfied; the mailing list appeared to be in rough\nconsensus, but it's not clear to me what they had consent about.\nJohn Klensin seems to contend that condition (3) is not satisfied.\n\nJG's recent message <mid:9603160004.aa17589@paris.ics.uci.edu>\nseems to be uncontroversial with respect to what the host: header should\nlook like if there is one, so I'll assume that's settled.\n\nThe issues list has the text:\n\n Full URL must be accepted by server, may be sent by client.\n\nSince JG seems to think that uncontroversial, I'll assume that's settled too.\nThe questions before the WG seems to be:\n\n- Should full URLs be mandatory to send for all HTTP/1.1 clients when\n  they don't know that they are talking to a HTTP/1.0 server?\n- Should the host: header be deleted from the spec?\n- Are there less extreme positions that make sense?\n\nI think we actually need concrete text to be sure what we're talking about.\nJim, where would such text actually fit into the current or next draft?\n\n                            Harald A\n\n\n\n", "id": "lists-010-0217043"}, {"subject": "Re: About that Host: header...", "content": "Harald.T.Alvestrand@uninett.no:\n>\n>JG's recent message <mid:9603160004.aa17589@paris.ics.uci.edu>\n>seems to be uncontroversial with respect to what the host: header should\n>look like if there is one, so I'll assume that's settled.\n\nJust as a data point: I too think JG's recent message is\nuncontroversial.\n\n>The issues list has the text:\n>\n> Full URL must be accepted by server, may be sent by client.\n>\n>Since JG seems to think that uncontroversial, I'll assume that's settled too.\n\nMe too.\n\n>The questions before the WG seems to be:\n>\n>- Should full URLs be mandatory to send for all HTTP/1.1 clients when\n>  they don't know that they are talking to a HTTP/1.0 server?\n\nMaking this mandatory is unacceptable for me.\n\n>- Should the host: header be deleted from the spec?\n\nNo.\n\n>- Are there less extreme positions that make sense?\n\nYes.  Make the Host header required if a 1.1 request line without a\nhost name is used.  This does not mean that we have to keep Host in\n1.2.  As Jim said:\n\n|We may be able to remove this lint in HTTP 1.2, if 1.1 really becomes\n|ubiquitous.  But then again, we may not, if 1.0 servers don't die a\n|timely death.\n\n\nThe arguments in favor of making the host name required in request\nlines sent by 1.1 clients have a fatal flaw.  They start out with\nnoting (in Jim's words):\n\n|The Host decision, if made where the current rough consensus I feel is,\n|will accellarate the day when we must deploy a replacement protocol.\n\nThen, they go on to argue that a switch to an incompatible replacement\nprotocol is very painful, so accelerating the day when the switch must\nbe made is bad.\n\nHowever, the proposed alternative, making the host name required in\nrequest lines sent by 1.1 clients, would mean making a painful switch\n*right now*: all 1.0 servers have to be upgraded as soon as the first\n1.1 client is released.\n\nSurely, if accelerating a switch is bad, requiring a switch right now\nis _extremely_ bad.  Not that I think that a switch to host names in\nthe request line will actually happen if we require it in 1.1:\nbrowser authors will correctly predict that their customer support\ndepartments would get swamped with bug reports (your new beta does not\nwork with site X!) if they would implement this part of 1.1.\n\nHost is the lesser of two evils.\n\n>                            Harald A\n\nKoen.\n\n\n\n", "id": "lists-010-0226556"}, {"subject": "Re: About that Host: header...", "content": ">- Should full URLs be mandatory to send for all HTTP/1.1 clients when\n>  they don't know that they are talking to a HTTP/1.0 server?\n\nAbsolutely not. This breaks the current use of the Web by giving the users\nextremely astounding and negative results:\nOpen up FooClient 1.0, enter the URL, and get the page.\nOpen up FooClient 1.1, enter the same URL, and get an error or a long delay\n*for each request you send*\n\nGuess what someone will do with FooClient 1.1?\n\nYou are not helping the cause here: you are making people want to stay with\nHTTP/1.0 software.\n\n>- Should the host: header be deleted from the spec?\n\nNo, no, no. I've just reread all the messages on this thread, and I find\nalmost no one supporting this idea.\n\n>- Are there less extreme positions that make sense?\n\nYes, and they are the ones I thought there was consensus on. Most people\nindicated they supported Jim's \"#4\", which, to quote it exactly, says:\n\na) Add host header, with improved wording in the specification.\nb) Require 1.1 server to accept full URL from 1.1 or later client.\n  (so far, same as option 3).\nc) Require server to generate an error if a 1.1 client is detected, and no host\ninformation present (or more strongly, at the expense of extra bytes on\nthe wire, no host header present).\nTransition to requiring full URL in 1.2, after 1.0 servers have been\nreplaced.\n\nWith some exact wording, that seems completely clear to me. It will not\nbreak the current Web, it will not greatly increase the traffic silently,\nit will not cause people to shun 1.1 clients, and will be easy to check\ncompliance on.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-0236902"}, {"subject": "Process for closing out issues.  (Please Read This Message)", "content": "Until Larry Masinter returns from his boondoggle, I guess I have to deal \nwith process stuff.  So here's my take on a process to close out our\noutstanding issues, while converging on exact wording.\n\nStarting in the next few days there will be a series of messages\nfrom the editorial group to the working group on outstanding issues.\nAdditionally, there will be I.D's in a number of areas\nbeing handled independently (e.g. digest authentication, content\nnegotiation, and so on).\n\nThe next question is how to get W.G. review of exact wording\nproposed to close out issues, without us all drowing in a flood of E-mail\non topics most w.g. members aren't concerned with or interested in.\n\nEach issue in the issues list has been given a name, and it has\nan issue owner.  The issue owner is responsible for\ndrafting the text (or getting it drafted with others) required \nto resolve the issue.\n\n1) The issue owner will draft the text/changes to existing text to resolve \nthe issue, and post the text changes (along with whatever explanitory material\nis necessary) to the list. Issue owners should normally include the\nissue name in the subject line.  In some areas, those typified\nby the earlier subgroups, (e.g. caching), one\nset of changes/additions/removals will claim to close out a whole\nraft of issues.  If you care about an issue, you should\nread the relevant messages, and if you participated in that\nsubgroup, you should review those messages particularly carefully.\n\n\nIf every issue, given the number of issues we have,\ngenerates N mail messages, discussing this or that\nsentence needs work, we'll all go nuts, particularly if we all have\nto wade through each message to figure out if it is an issue you\ncare about.\n\n2) So here's my suggestion: first try private e-mail to the issue\nowner to resolve problems, rather than CC'ing the whole list.\n\nIf you can't resolve the problem privately, then by all means\ntake the issue public for public discussion.  But at least this\nway wording problems won't generate a flood of mail.\nPlease try to keep the issue name in the subject line (or beginning\nof larger messages that close out a raft of issues), so that\neveryone doesn't have to wade through every message to figure out\nif they care about it or not.\n\nNext problem: how to get a feeling for concensus...\n\n3) When the issue owner believes he has IETF \"rough concensus\" on an issue,\nhe'll repost the changes to the list with the final wording additions/changes, \nand will indicate he believes the issue has reached rough concensus.\n(again, with the issue name(s) attached). \nIn many circumstances, it would be useful for private mail to be\nsummarized with the issue so that others can see the reasoning\nbehind the resolution, so that we don't have to repeat the\nwhole discussion on subtleties of the issue on the general list.\nFor obvious, non controversial changes, this will result in 1 or 2 messages \nto the list to close out an issue.\n\n4) At this point, you should have commented in private if you care about\nthe issue, and if it isn't resolved to your satisfaction, we\ncan hash it all out on the general list.  I hope, however, that\nas much technical and editorial stuff as possible can be resolved without every\nargument being done on the general list, and we'll be down\nto disputing if the text actually reflects the concensus on the issue,\nand issues that are still really in dispute.\n\nMy hope is to have another complete draft together by around 4/1,\nwith a final draft in mid-april.  As they become available, we'll\ndo something similar to the above on problems that turn up in the integrated\ndrafts.\n\nIf I'm all wet or you think of a better process, do let\nme know.\n\nI will be keeping issues status until Larry Masinter returns, as usual, \nin the issues list found at:\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/http-wg.html\n\nPlease refer to the issues list before raising what may\nbe a resolved issue, or for the status of an issue.  \nI'll try to keep pointers there to messages resolving issues.\n- Jim\n\n\n\n", "id": "lists-010-0245666"}, {"subject": "Issues Beyond HTTP/1.1 ??", "content": "Issue 1:\n\nWhen a user stops a transmission, the CGI's generated do not get \nkilled. Am I correct to say that this is so because the HTTP protocol\ndoes not contemplate an \"transmission interrupted\" signal?\n\nIs there any knoen solution known to this problem that does not\ninvolve changes to the protocol?\n\n\nIssue 2:\n\nLocal data shows that our connetions will be swamped by the end of\nthe year. Caching seems to be a good way to handle this problem.\nHowever, most of the caching schemes in use right now are client side. \nIn fact, some forms of traffic can only be efficiently cached\nby server inititiated and managed caches (see Push-Caching\nhttp://daisy.uwaterloo.ca/~alopez-o/cspap/cache/Overview.html)\n\n\nAlex\n\n\n-- \nAlex Lopez-Ortiz                                       alexlo@opentext.com\nResearch & Development                               Open Text Corporation\nhttp://daisy.uwaterloo.ca/~alopez-o                     (519)-888-7111x318\n\n\n\n", "id": "lists-010-0256081"}, {"subject": "Re: Issues Beyond HTTP/1.1 ??", "content": "alopez-o@barrow.uwaterloo.ca writes:\n\n\n>Issue 1:\n>\n>When a user stops a transmission, the CGI's generated do not get \n>killed. Am I correct to say that this is so because the HTTP protocol\n>does not contemplate an \"transmission interrupted\" signal?\n>\n>Is there any knoen solution known to this problem that does not\n>involve changes to the protocol?\n\nI think its because httpd's usually buffer the whole object.  That is,\nthey don't write anything until the whole thing has been read.  I\nsuppose this is to provide an accurate Content-Length.\n\nDuane W.\n\n\n\n", "id": "lists-010-0264536"}, {"subject": "Re: About that Host: header...", "content": "<snip>\n> >- Are there less extreme positions that make sense?\n> \n> Yes, and they are the ones I thought there was consensus on. Most people\n> indicated they supported Jim's \"#4\", which, to quote it exactly, says:\n> \n> a) Add host header, with improved wording in the specification.\n> b) Require 1.1 server to accept full URL from 1.1 or later client.\n>   (so far, same as option 3).\n> c) Require server to generate an error if a 1.1 client is detected, and no host\n> information present (or more strongly, at the expense of extra bytes on\n> the wire, no host header present).\n> Transition to requiring full URL in 1.2, after 1.0 servers have been\n> replaced.\n> \n> With some exact wording, that seems completely clear to me. It will not\n> break the current Web, it will not greatly increase the traffic silently,\n> it will not cause people to shun 1.1 clients, and will be easy to check\n> compliance on.\n> \n> --Paul Hoffman\n> --Internet Mail Consortium\n\nThe area that is unclear to me, is.. Are 1.1 servers REQUIRED to understand\nthe full URL?\n\nI think it makes sense to have 1.1 servers understand either format.\n\n-- \nElizabeth(Beth) Frank\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n", "id": "lists-010-0272080"}, {"subject": "Issues Beyond HTTP/1.1 ??", "content": "Alex Lopez-Ortiz writes:\n > \n > Issue 1:\n > \n > When a user stops a transmission, the CGI's generated do not get \n > killed. Am I correct to say that this is so because the HTTP protocol\n > does not contemplate an \"transmission interrupted\" signal?\n > \n > Is there any knoen solution known to this problem that does not\n > involve changes to the protocol?\n > \n\nYour CGI program ought to get a SIGPIPE signal when the server\ninstance goes away, which will happen when the TCP connection goes away.\n\n\n\n", "id": "lists-010-0280406"}, {"subject": "Re: Issues Beyond HTTP/1.1 ??", "content": ">\n>Alex Lopez-Ortiz writes:\n> > \n> > Issue 1:\n> > \n> > When a user stops a transmission, the CGI's generated do not get \n> > killed. Am I correct to say that this is so because the HTTP protocol\n> > does not contemplate an \"transmission interrupted\" signal?\n> > \n> > Is there any knoen solution known to this problem that does not\n> > involve changes to the protocol?\n> > \n>\n>Your CGI program ought to get a SIGPIPE signal when the server\n>instance goes away, which will happen when the TCP connection goes away.\n\nFirst off, thanks to all who replied. SIGPIPE works. \n\nAre there any other situations (with persitent connections and the like) in \nwhich we might need to explicitly inform the server of the interruption?\nWould that be just a waste of bandwith? Comment? Opinions?\n\nAlex\n\n\n\n-- \nAlex Lopez-Ortiz                                       alexlo@opentext.com\nResearch & Development                               Open Text Corporation\nhttp://daisy.uwaterloo.ca/~alopez-o                     (519)-888-7111x319\n\n\n\n", "id": "lists-010-0288174"}, {"subject": "Via Header Field (replaces Forwarded", "content": "Koen asked:\n>    Upgraded-From: HTTP/1.0, HTTP/1.1\n> \n> With this header, my detectability requirements are met.  Roy, is this\n> header acceptable to you?\n\nJeff and I talked about this concept during the LA IETF.  I think that\nit belongs in the Forwarded header.  It was also pointed out (by JimG)\nthat the Forwarded header as currently defined has too many unnecessary\nbytes, so we thought about coming up with a more compact encoding.\n\nHow about this as a complete replacement for the current Forwarded?\n===================================================================\n\n10.xx  Via\n\n   The Via general-header field is used by gateways and proxies to\n   indicate the intermediate protocols and recipients between the user\n   agent and the server on requests, and between the origin server and\n   the client on responses. It is analogous to the \"Received\" field of\n   RFC 822 [9] and is intended to be used for tracking message forwards,\n   avoiding request loops, and identifying the protocol capabilities of\n   all senders along the request/response chain.\n\n      Via   =   \"Via\" \":\" 1#( received-protocol received-by [ comment ] )\n\n      received-protocol = [ protocol-name \"/\" ] protocol-version\n\n      received-by       = ( host [ \":\" port ] ) | pseudonym )\n      pseudonym         = token\n\n   The protocol-name is optional if and only if it would be \"HTTP\".  The\n   received-by field is normally the host and optional port number of\n   a recipient server or client that subsequently forwarded the message.\n   However, if the real host is considered to be sensitive information,\n   it may be replaced by a pseudonym.\n\n   Multiple Via field values are allowed and represent each proxy or\n   gateway that has forwarded the message.  Each recipient must append\n   their information such that the end result is ordered according to\n   the sequence of forwarding applications.\n\n   Comments may be used in the Via header field to identify the software\n   of the recipient proxy or gateway, analogous to the User-Agent and\n   Server header fields.  However, all comments in the Via field are\n   optional and may be removed by any recipient prior to forwarding the\n   message.\n\n   For example, a request message could be sent from an HTTP/1.0 user\n   agent to an internal proxy code-named \"fred\", which uses HTTP/1.1\n   to forward the request to a public proxy at nowhere.com, which\n   completes the request by forwarding it to the origin server at\n   www.ics.uci.edu.  The request received by www.ics.uci.edu would then\n   have the following Via header field:\n\n       Via: 1.0 fred, 1.1 nowhere.com (Apache/1.1)\n\n   Proxies and gateways used as a portal through a network firewall\n   should not, by default, forward information about the internal hosts\n   within the firewall region. This information should only be\n   propagated if explicitly enabled. If not enabled, the received-by\n   host of any host behind the firewall should be replaced by an\n   appropriate pseudonym for that host.\n\n      Note: The Via header field replaces the Forwarded header field\n      which was present in earlier drafts of this protocol.\n\n===================================================================\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-0297052"}, {"subject": "Re: About that Host: header...", "content": "> The issues list has the text:\n> \n>  Full URL must be accepted by server, may be sent by client.\n> \n> Since JG seems to think that uncontroversial, I'll assume that's settled too.\n\nHmmmm, not quite.  It is okay for the WG to propose an incompatible change\nto the HTTP protocol for the proposed standard.  It is not okay to do so\nwithout indicating it through a change to the HTTP major protocol version\nnumber.  The ability of a client to send a full URL to the origin server \nis an incompatible change, even when it is optional.\n\nThe HTTP version number is not some vague token indicating an arbitrary\nrevision of the specification; it has a specific set of semantics designed\nto communicate the protocol capabilities and requirements of the sender.\nIt is a significant aspect of the protocol and is designed to maintain\ncoherence among the family of protocols that can be used to access services\nidentified by the \"http\" URL.\n\nThis means that the most we can say for anything called HTTP/1.x is\n\n   a) Host header field must be supplied by client\n   b) Full URL must be accepted by server\n\nWe can bypass this issue by changing the proposed version number to HTTP/2.0.\nHowever, doing so will cost us greatly in terms of deployment.  In fact,\nI will go on record for the following:\n\n   I am not willing to make any incompatible protocol upgrades to my\n   client and server software until I can include multiplexing support\n   within the protocol.\n\nIn other words, I would rather have any valid HTTP/1.1 than a proposed\nstandard HTTP/2.0, unless the latter includes a multiplexed\nsession layer (which is 6-12 months away).  Even if the WG decides that an\nincompatible protocol is the way to go, it must be called HTTP/2.0\nso that I can define a meaningful subset for HTTP/1.x which is not\nincompatible and thus can be used as a stepping-stone for deployment.\nIf HTTP/2.0 does not include multiplexing, then I will continue to\nuse HTTP/1.x until HTTP/3.0 is defined to include multiplexing.\n\nThis opinion is not mine alone -- I have received the same or similar\ninput from many vendors since the start of the WG.\n\nIn any case, such a decision would make my careful design of the protocol\ntransition irrelevant.  As such, I think the proponents of such a change\nmust provide significant proof that the change is necessary -- just saying\nthat you think it is an issue is not sufficient.\n\nHost already provides a technical solution for the problem being discussed.\nThe only objection I have heard is that \"people may implement it wrong\".\nSince that is also true of any other solution we may try, I do not\nconsider it to be a valid argument.  More importantly, the Host header\nfield can be implemented in HTTP/1.0 -- the solution does not depend\non the version number and thus anybody can implement it today.\n\nThere is nothing new about the Host issue.  We have been planning this\nphased solution for almost two years now -- only the details have changed\nin terms of the name of the 1.x header field.  We do intend to have\nfull URIs sent in HTTP/2.0 (regardless of what else is in that protocol)\nbecause it is precisely at that point in which we can do all sorts of\nmessage translation necessary for a variety of known extensions to the\nprotocol.  The reason for not doing it ALL RIGHT NOW is because the\nexisting HTTP/1.0 protocol was not sufficiently defined to provide a\nbasis for HTTP/2.0 and did not include the mechanisms necessary to\ndeploy an incompatible protocol.\n\n> The questions before the WG seems to be:\n> \n> - Should full URLs be mandatory to send for all HTTP/1.1 clients when\n>   they don't know that they are talking to a HTTP/1.0 server?\n\nImpossible without violating the protocol design -- this can only be done\nin HTTP/2.0.\n\n> - Should the host: header be deleted from the spec?\n\nNot if we want a solution that can be deployed with existing practice.\nIf we can't deploy it, then we are wasting our time.  That is why the\nHost header field is the preferred solution.\n\nThe following is an updated section on the Host header (updated to\nreflect Lou's comments on the default port number). \n\n========================================================================\n10.22  Host\n\n   The Host request-header field specifies the Internet host and port\n   number of the resource being requested, as obtained from the original\n   URL given by the user or referring resource (generally an http URL,\n   as described in Section 3.2.2).  The Host field value must represent\n   the network location of the origin server or gateway given by the\n   original URL.  This allows the origin server or gateway to\n   differentiate between internally-ambiguous URLs, such as the root \"/\"\n   URL of a server harboring multiple vanity hostnames on a single IP\n   address.\n\n       Host = \"Host\" \":\" host [ \":\" port ]    ; see Section 3.2.2\n\n   A \"host\" without any trailing port information implies the default\n   port for the service requested (e.g., \"80\" for an http URL).  For\n   example, a request on the origin server for\n   <http://www.w3.org/pub/WWW/> must include:\n\n       GET /pub/WWW/ HTTP/1.1\n       Host: www.w3.org\n\n   The Host header field must be included in all HTTP/1.1 request\n   messages on the Internet (i.e., on any message corresponding to a\n   request for a URL which includes an Internet host address for the\n   service being requested).  If the Host field is not already present,\n   an HTTP/1.1 proxy must add a Host field to the request message prior\n   to forwarding it on the Internet.  All Internet-based HTTP/1.1\n   servers must respond with a 400 status code to any HTTP/1.1 request\n   message which lacks a Host header field.\n\n========================================================================\n\nAdditions to the spec regarding the requirement that all servers must\naccept a full URI would be put in the section on Request-URI (5.1.2).\nI think someone else is working on that.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-0308159"}, {"subject": "Issue: interoperability between HTTP/1.0 and HTTP/1.1 (was: Via Header Field", "content": "Roy T. Fielding:\n>Koen asked:\n>>    Upgraded-From: HTTP/1.0, HTTP/1.1\n>> \n>> With this header, my detectability requirements are met.  Roy, is this\n>> header acceptable to you?\n>\n>Jeff and I talked about this concept during the LA IETF.  I think that\n>it belongs in the Forwarded header.  It was also pointed out (by JimG)\n>that the Forwarded header as currently defined has too many unnecessary\n>bytes, so we thought about coming up with a more compact encoding.\n>\n>How about this as a complete replacement for the current Forwarded?\n>===================================================================\n>\n>10.xx  Via\n\n[...]\n\nThis looks like a great improvement to me.  The Via header meets my\ndetectability requirements.  Roy, I will propose some minor edits to\nyour header text in private e-mail, these edits will not change the\nheader semantics.\n\nThe introduction of Via changes the status of the issue 'Section 3.1:\ninteroperability between HTTP/1.0 and HTTP/1.1' on the issues list.\n\nOld text:\n\n  KH: Section 3.1: interoperability between HTTP/1.0 and HTTP/1.1.\n\n  Do version handling rules for servers and proxies need to be changed?\n\n  Review the whole spec for interoperability problems.\n\nNew text:\n\n  KH: Section 3.1: interoperability between HTTP/1.0 and HTTP/1.1. \n\n  Do version handling rules for servers and proxies need to be changed?\n\n  Status: No, the Via header will make version upgrades and downgrades\n  by proxies detectable to parties that need to know.\n\n  Review the whole spec for interoperability problems.\n\n  Status: to be done by the whole http-wg in April.\n\n[Note to Jim Gettys: In the last version of the issues list I have,\nthe above old text appears twice.  Please delete the first instance.]\n\nKoen.\n\n\n\n", "id": "lists-010-0322275"}, {"subject": "Re: Some randomness concerning chunked encoding", "content": "On Mar 2, Phill wrote:\n> Subject: Some randomness concerning chunked encoding.\n> The point is that in future we may want to have attributes\n> associated with each chunk...\n\nIsn't this asymtotically converging with multipart?\n\n  -- Bob\n\n\n\n", "id": "lists-010-0331832"}, {"subject": "Re: About that Host: header...", "content": "On Sat, 23 Mar 1996, Roy T. Fielding wrote:\n\n> Hmmmm, not quite.  It is okay for the WG to propose an incompatible change\n> to the HTTP protocol for the proposed standard.  It is not okay to do so\n> without indicating it through a change to the HTTP major protocol version\n> number.  The ability of a client to send a full URL to the origin server \n> is an incompatible change, even when it is optional.\n\nI don't see this as an incompatible change, assuming wording is put into\nthe spec along the lines of the following: \"A client may not send request\nto an HTTP server using a full URL in the request until it has determined\nthat the server is compatible with HTTP/1.1 or later.\"\n\nIf it is required that HTTP/1.1 servers support full URL requests, then\nonce a client has sent a partial URL request to a server, and gotten back\na response tagged with \"HTTP/1.1\", it seems to me that there is no reason\nthat the next request to that server cannot be in full URL form.\n\nAm I missing something (most likely)?\n\n--// Alexei Kosut // <akosut@nueva.pvt.k12.ca.us> // Lefler on IRC --//\n-----------------// <http://www.nueva.pvt.k12.ca.us/~akosut> -------// \n\"To get the full effect of Pat Buchanan's speeches, they should be\nread in the original German.\" //--------------------------------------\n\n\n\n", "id": "lists-010-0338672"}, {"subject": "Re: About that Host: header...", "content": "I agree with all of Roy says here, although I believe that we don't need to\nuse his provocative wording. I propose changing:\n\n>   This allows the origin server or gateway to\n>   differentiate between internally-ambiguous URLs, such as the root \"/\"\n>   URL of a server harboring multiple vanity hostnames on a single IP\n>   address.\n\nTo:\n\n>   This allows the origin server or gateway to\n>   differentiate between internally-ambiguous URLs, such as the root \"/\"\n>   URL of a server that serves HTTP for many hostnames on a single IP\n>   address.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-0348594"}, {"subject": "Re: About that Host: header...", "content": "> \n> On Sat, 23 Mar 1996, Roy T. Fielding wrote:\n> \n> > Hmmmm, not quite.  It is okay for the WG to propose an incompatible change\n> > to the HTTP protocol for the proposed standard.  It is not okay to do so\n> > without indicating it through a change to the HTTP major protocol version\n> > number.  The ability of a client to send a full URL to the origin server \n> > is an incompatible change, even when it is optional.\nAlaxei Kosut adds:\n> I don't see this as an incompatible change, assuming wording is put into\n> the spec along the lines of the following: \"A client may not send request\n> to an HTTP server using a full URL in the request until it has determined\n> that the server is compatible with HTTP/1.1 or later.\"\n> \n> If it is required that HTTP/1.1 servers support full URL requests, then\n> once a client has sent a partial URL request to a server, and gotten back\n> a response tagged with \"HTTP/1.1\", it seems to me that there is no reason\n> that the next request to that server cannot be in full URL form.\n> \n> Am I missing something (most likely)?\n\nIn theory Roy is right, in practice the situation is somewhat easier:\nI added the following two lines to my CERN servers config file:\n\nMaphttp://gatekeeper.bne.private**\nMaphttp://bne.ind.eunet.hu**\n\nand now the old 1.0 server accepts full URLs.\n\nI guess that the NCSA server can be tricked similarly.\n(NCSA gurus, is this true? And other servers can be configured in the same way?)\n\nAnd going even further:\nPasshttp://gatekeeper.bne.private*/var/httpd/htdocs/internal*\nPasshttp://bne.ind.eunet.hu*/var/httpd/htdocs/public*\nPasshttp://www.CareNet.hu*/var/httpd/htdocs/company*\n\n... and the CERN server goes virtual!\n\nAlmost all servers have some mapping rules from web (URL) name space to \nfilesystem name space, and using full URLs instead of Host: may give us \nmore benefits!\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n", "id": "lists-010-0356261"}, {"subject": "Proposed revision to security consideration", "content": "Attached is my revised version of section 14.1.  This is the section\nof the spec dealing with security considerations and authentication.\n\nIf the digest authentication document \"docks\" with the HTTP/1.1 spec\nthen its security considerations should perhaps dock with this one.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n14.1 Authentication of Clients\n\nAs mentioned in Section 11.1, the Basic authentication scheme is not a\nsecure method of user authentication, nor does it in any way protect\nthe Entity-Body, which is transmitted in clear text across the\nphysical network used as the carrier. HTTP does not prevent additional\nauthentication schemes and encryption mechanisms from being employed\nto increase security or the addition of enhancements (such as schemes\nto use one-time passwords) to Basic authentication.\n\nThe most serious flaw in Basic authentication is that it results in\nthe essentially clear text transmission of the user's password over\nthe physical network.  It is this problem which Digest Authentication\nattempts to address.\n\nBecause Basic authentication involves the clear text transmission of\npasswords it should never be used (without enhancements) to protect\nsensitive or valuable information.\n\nA common use of Basic authentication is for identification purposes --\nrequiring the user to provide a username and password as means of\nidentification, for example, for purposes of gathering accurate usage\nstatistics on a server.  When used in this way it is tempting to think\nthat there is no danger in its use if illicit access to the protected\ndocuments is not a major concern.  This is only correct if the server\nissues both username and password to the users and in particular does\nnot allow the user to choose his or her own password.  The danger\narises because naive users frequently reuse a single password to avoid\nthe task of maintaining multiple passwords.\n\nIf a server permits users to select their own passwords, then the\nthreat is not only illicit access to documents on the server but also\nillicit access to the accounts of all users who have chosen to use\ntheir account password.  If users are allowed to choose their own\npassword that also means the server must maintain files containing the\n(presumably encrypted) passwords.  Many of these may be the account\npasswords of users perhaps at distant sites.  The owner or\nadministrator of such a system could conceivably incur liability if\nthis information is not maintained in a secure fashion.\n\nBasic Authentication is also vulnerable to spoofing by counterfeit\nservers.  If a user can be led to believe that he is connecting to a\nhost containing information protected by basic authentication when in\nfact he is connecting to a hostile server or gateway then the attacker\ncan request a password, store it for later use, and feign an error.\nThis type of attack is not possible with Digest Authentication.\nServer implementors should guard against the possibility of this sort\nof counterfeiting by gateways or CGI scripts.  In particular it is\nvery dangerous for a server to simply turn over a connection to a\ngateway since that gateway can then use the persistant connection\nmechanism to engage in multiple transactions with the client while\nimpersonating the original server in a way that is not detectable by\nthe client.\n\n\n\n", "id": "lists-010-0365993"}, {"subject": "Re: About that Host: header...", "content": "Balint Nagy Endre <bne@bne.dial.eunet.hu> said:\n> In theory Roy is right, in practice the situation is somewhat easier:\n> I added the following two lines to my CERN servers config file:\n> \n> Maphttp://gatekeeper.bne.private**\n> Maphttp://bne.ind.eunet.hu**\n> \n> and now the old 1.0 server accepts full URLs.\n> \n> I guess that the NCSA server can be tricked similarly.\n\nMaybe -- I know that Apache/1.1b with the proxy additions is capable of\nthat, yes.  However, this is not relevant to what I was objecting to.\nIt is relatively easy to create a new HTTP/1.0 server which accepts\nfull URIs; in fact, I have been encouraging that for ages.  We can and\nshould require that capability for HTTP/1.1 servers.\n\nThe protocol question is what does the client send when it is requesting\na resource on an origin server or gateway (not a proxy).  The HTTP/1.0\nprotocol requires that the client only send the absolute URL path.\nThat cannot change and remain compatible with HTTP/1.0.\n\nWe then have two options:\n\n  1) Send the Host header field in HTTP/1.x requests and remain in HTTP/1.x\n  2) Send the full URI in HTTP/2.0 requests\n\nThere is no value in sending both Host and the full URI.  Both options\nequally solve the problem of vanity hostname using up IP numbers.\nHTTP/2.0 requires message conversion anyway, so Host can be dropped at\nthat time without impact to the protocol.\n\nOption (1) can be implemented now by any client and has no impact\non deployment.\n\nOption (2) cannot be implemented by any client until the IETF has completed\nwork on HTTP/2.0.  Even then, a first request on an unfamiliar server will\nhave to be done in HTTP/1.x in order to avoid a high entry-barrier to\ndeployment.\n\nOption (1) has already been deployed in current practice.\n\nOption (2) won't be deployable for at least six months, and more likely\na year from now.\n\nNow, please explain to me why option (2) is a better technical solution\nto the problems we are facing today.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-0376542"}, {"subject": "Re: About that Host: header...", "content": "> I agree with all of Roy says here, although I believe that we don't need to\n> use his provocative wording. I propose changing:\n\nWhat is provocative about \"vanity hostnames\"?  That is, after all, the\ncommon name and purpose for this feature.  I was just trying to be\nspecific so that people can find the topic using a search service. ;-)\n\n>>   This allows the origin server or gateway to\n>>   differentiate between internally-ambiguous URLs, such as the root \"/\"\n>>   URL of a server harboring multiple vanity hostnames on a single IP\n>>   address.\n> \n> To:\n> \n>>   This allows the origin server or gateway to\n>>   differentiate between internally-ambiguous URLs, such as the root \"/\"\n>>   URL of a server that serves HTTP for many hostnames on a single IP\n>>   address.\n\nIn this case, \"a server that serves HTTP\" is redundant.  How about\n\n     URL of a server for multiple hostnames on a single IP address.\n\n......Roy\n\n\n\n", "id": "lists-010-0386342"}, {"subject": "Re: About that Host: header...", "content": "> I don't see this as an incompatible change, assuming wording is put into\n> the spec along the lines of the following: \"A client may not send request\n> to an HTTP server using a full URL in the request until it has determined\n> that the server is compatible with HTTP/1.1 or later.\"\n\nBecause it doesn't gain us anything more than just sending Host on\nall HTTP/1.x requests.  If there was something wrong with the Host\nsolution, then I could understand the desire to do that.  However,\ngiven that there is nothing wrong with the Host solution, and Host\nhas already been deployed, this all seems like a waste of time.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-0394639"}, {"subject": "Re: About that Host: header...", "content": "> \n> Balint Nagy Endre <bne@bne.dial.eunet.hu> said:\n> > In theory Roy is right, in practice the situation is somewhat easier:\n....\nRoy T. Fielding:\n> Now, please explain to me why option (2) is a better technical solution\n> to the problems we are facing today.\n\nI tried to explain, that (hopely) most servers can be tricked to accept full URLs\nwith changes in config files, even without looking at sources. \nBecause all HTTP servers (serving files from a filesystem) have mapping rules\nfrom web name space to filesystem name space, I hope that the situation is not\nthat critical to require accepting full URLs in 1.1 servers.\nOf course this requires reconfiguration of all running servers, but that's not\nthat hopeless than upgrading them. (independently when this happens: now or somewhere\nin the unforeseeable future.)\n\nOn the other hand: host headers are interpretable only by host-aware servers,\nwhile full URLs are visible in every server.\nTroubles may arise with trivial servers, which map URL-space / to a fixed\nfilesystem name, but clients may record how every server contacted reacts\nto full URLs.\n\nAs I see, non-proxy aware clients should die rapidly, because the net can't keep up\nwith current web traffic:\nI see home.netscape.com with about 100 cps, if my request doesn't get timed out,\nother US servers are visible at about 300 cps, and I rarely see better\nperformance, however I will see my provides proxy-cache at full line speed (about 1400 cps\nor sometimes even 1700 cps because ppp compression and V42bis compression works.)\nSurely, when my provider installs some proxy cache, I will buy a 28.8Kbps modem, but not \nbefore. (Formerly I played around a bit with the CERN server as proxy on host ind.eunet.hu\n- which is two hops away- , and the results were promising.)\n\nWhen a proxy-aware client talks to a proxy, it will send necessarily full URLs.\nThe up-to-date proxy will forward that request according to the remembered type of the\norigin server:\n0.9 servers: will send 1.0 requests and will try other practices when sees a full-response.\n(the server upgraded) \n\n1.0 servers: may try full-request using full URLs, upon failure record that the server\ndoesn't accept full URLs and re-test once a week or a month.\nthis means one failed request once a week or month on every 1.0 origin server per proxy\n- a reasonable cost for rapid convergence to full URLs.\n\n1.1 servers: full requests with full URLs.\n\n1.1 clients may act similarly, but it is not neccessary.\n\nIf the WG sees enough motivation around to install (1.1 or better) proxy servers,\nwe can go forward with full URLs.\n\nHey HTTP gurus, is this approach usable?\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n", "id": "lists-010-0403412"}, {"subject": "Re: About that Host: header...", "content": "> On the other hand: host headers are interpretable only by host-aware servers,\n> while full URLs are visible in every server.\n\nBut that is exactly the problem -- think of the economics of the situation.\n\nThe only servers that care about the host header are those that serve\nmultiple hostnames on a single IP address.  This will not change.\nNote, however, that there are NO currently deployed servers which do\nthe above, since the HTTP/1.0 protocol (sans Host) does not allow for\nthem to exist.  This means that such servers can be made host-aware\nat no cost at the moment the owner installs them.\n\nFull URLs require upgrading most existing servers before any client can\nafford to send the full URL on a request to an origin server or gateway.\nHost doesn't require any server to upgrade, which is why it can be used\nwith HTTP/1.0 clients.\n\nKeep in mind that we are trying to solve a particular problem:\nthe IP address space being used up too quickly due to vanity hostnames.\nThat problem cannot be solved until a sufficient number of clients\n(I'd say about 80%) have implemented the solution.  With Host, all\nclients can implement it now -- hell, we could probably reach the 80%\nnumber by the time we finish arguing about the solution!  In contrast,\nsending the full URL requires that existing servers change FIRST, and\nonly then can the clients start implementing the solution.\n\nI must emphasize that servers do not get upgraded with the same frequency\nas clients.  Over 10% of existing practice are still using servers that had\nwell-known security holes discovered in them a year ago!  There is no\nevidence to suggest that people can be compelled to upgrade their server\nsoftware any faster.\n\nThe question is not which solution is more elegant or which solution\nwould be better if there were no current practice.  The question is\nwhich solution will better solve the problem.  I claim that sending\nthe full URL will not solve the problem because it cannot be deployed\nin a timely fashion.  Furthermore, attempting to deploy a full URL\nalong with all our other improvements to the protocol will just delay\neverything.  The full URL can and should be deployed when we make\nall the other incompatible changes to the protocol -- in HTTP/2.0.\n\nEnough on this topic -- I hope that I have covered all the questions\nthat the ADs wanted to see addressed.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-0413846"}, {"subject": "Re: About that Host: header...", "content": "Roy says:\n> \n> Because it doesn't gain us anything more than just sending Host on\n> all HTTP/1.x requests.  If there was something wrong with the Host\n> solution, then I could understand the desire to do that.  However,\n> given that there is nothing wrong with the Host solution, and Host\n> has already been deployed, this all seems like a waste of time.\n\nThe things that are wrong with the Host: solution, to my mind, are:\n- It STILL leaves us with a protocol where URLs are things that protocol\n  entities have to break into little pieces and chew upon.\n- It STILL leaves us with an UA-to-cache protocol that is incompatible with\n  the UA-to-server and cache-to-server protocol.\n- It STILL loses the method information.\n- It STILL gives us no path to where it seems everyone wants to be, namely\n  with full URLs, until we throw out HTTP/1.x altogether\n\nThese aren't \"nothing\". The question is which pain is lesser.\n\n           Harald A\n\n\n\n", "id": "lists-010-0424020"}, {"subject": "Re: About that Host: header...", "content": "> The things that are wrong with the Host: solution, to my mind, are:\n> - It STILL leaves us with a protocol where URLs are things that protocol\n>   entities have to break into little pieces and chew upon.\n\nYes.  URLs are things that protocol entities have to break into little\npieces and chew upon.  That is true of all URL schemes.  It will remain\ntrue even with full URLs since intermediary caches will break them\ninto pieces prior to internal canonicalization and key matching.\n\n> - It STILL leaves us with an UA-to-cache protocol that is incompatible with\n>   the UA-to-server and cache-to-server protocol.\n\nOn the contrary, the UA-to-cache protocol is identical to the UA-to-proxy\nprotocol, even when the cache is internal to the proxy.  It is true that\nthe client-to-proxy protocol is different than the client-to-server\nprotocol, but that is something which cannot be fixed in a timely fashion\nbecause it requires the origin servers to change first.\n\n> - It STILL loses the method information.\n\nI don't understand this one -- the method is still there.  The only thing\nthat is lost is the original URL scheme and that information is inherent\nin the connection to the origin server or gateway.\n\n> - It STILL gives us no path to where it seems everyone wants to be, namely\n>   with full URLs, until we throw out HTTP/1.x altogether\n\nWe have a path.  HTTP/1.0 now --> HTTP/1.1 on May 1 --> HTTP/2.0 as soon\nas multiplexing + PEP + cookies have a solid, agreed-upon syntax.\nHTTP/1.1 contains the mechanisms necessary to make deployment of HTTP/2.0\npossible.\n\n> These aren't \"nothing\". The question is which pain is lesser.\n\nAs I said, I think that the question is which one will solve the problem.\nNo matter what we decide, we cannot solve the problem if the solution\ncannot be deployed.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-0432570"}, {"subject": "Re: About that Host: header...", "content": "Just clearing out the tail end of the chain here..\n\n\"Roy T. Fielding\":\n> > - It STILL loses the method information.\n> \n> I don't understand this one -- the method is still there.  The only thing\n> that is lost is the original URL scheme and that information is inherent\n> in the connection to the origin server or gateway.\n\nMy point: I think that it was a design mistake to link together:\n- The thing is identified by a \"name-like\" thing that starts with HTTP,\n  and has a last resort access method using the HTTP protocol\n- The fact that we are currently using the HTTP protocol to access it.\nEven on the link to the origin server.\n\nBut if we all accept that:\n- HTTP/1.1 is a stopgap until multiplexed HTTP (2.0) comes along\n- HTTP/2.0 will have full URLs and no Host: header\nthen I don't see any reason to keep on beating this dead horse at this moment.\n\n> > - It STILL gives us no path to where it seems everyone wants to be, namely\n> >   with full URLs, until we throw out HTTP/1.x altogether\n> \n> We have a path.  HTTP/1.0 now --> HTTP/1.1 on May 1 --> HTTP/2.0 as soon\n> as multiplexing + PEP + cookies have a solid, agreed-upon syntax.\n> HTTP/1.1 contains the mechanisms necessary to make deployment of HTTP/2.0\n> possible.\n> \nIs it possible to get this strategy written down in a 4-page \"strategy\" \ndocument?\nIt might be nice to have the words down on phosphor that we agree upon.\n(Or I might just have missed the URL of the document, of course....)\n\nThanks for caring!\n\n           Harald A\n\n\n\n", "id": "lists-010-0442329"}, {"subject": "Administrivia: digest of list availabl", "content": "Folks,\n\nThe http-wg mailing list now has a digest, called http-wg-d.  Digests of\npostings to http-wg are sent to subscribers on this list on a daily basis.\n\nTo subscribe to this digest list, send a message to:\n\nhttp-wg-d-request@cuckoo.hpl.hp.com\n\nTo unsubscribe from the original list, send a message to:\n\nhttp-wg-request@cuckoo.hpl.hp.com\n\nIf all else fails, send a message directly to me.\n\n(typed solely by voice)-- ange -- <><\n\nange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-010-0451425"}, {"subject": "I-D ACTION:draft-ietf-http-digest-aa03.tx", "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : A Proposed Extension to HTTP : Digest Access \n                   Authentication                                          \n       Author(s) : J. Hostetler, J. Franks, P. Hallam-Baker, \n                   P. Leach, A. Luotonen, E. Sink, L. Stewart\n       Filename  : draft-ietf-http-digest-aa-03.txt\n       Pages     : 9\n       Date      : 03/22/1996\n\nThe protocol referred to as \"HTTP/1.0\" includes specification for a Basic \nAccess Authentication scheme.  This scheme is not considered to be a secure\nmethod of user authentication, as the user name and password are passed \nover the network in an unencrypted form.  A specification for a new \nauthentication scheme is needed for future versions of the HTTP protocol.  \nThis document provides specification for such a scheme, referred to as \n\"Digest Access Authentication\".  The encryption method used by default is \nthe RSA Data Security, Inc. MD5 Message-Digest Algorithm [2].              \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-digest-aa-03.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-digest-aa-03.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-digest-aa-03.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-0459047"}, {"subject": "Re: I-D ACTION:draft-ietf-http-digest-aa03.tx", "content": "On Mon, 25 Mar 1996 Internet-Drafts@CNRI.Reston.VA.US wrote:\n> A Revised Internet-Draft is available from the on-line Internet-Drafts \n> directories. This draft is a work item of the HyperText Transfer Protocol \n> Working Group of the IETF.                                                 \n> \n>        Title     : A Proposed Extension to HTTP : Digest Access \n>                    Authentication                                          \n>        Author(s) : J. Hostetler, J. Franks, P. Hallam-Baker, \n>                    P. Leach, A. Luotonen, E. Sink, L. Stewart\n>        Filename  : draft-ietf-http-digest-aa-03.txt\n>        Pages     : 9\n>        Date      : 03/22/1996\n\nThis draft was submitted on March 1.  Since that time there have\nbeen some significant revisions.  The current working version still\nneeds some editorial revisions at which point we should be able to\nrelease draft 04 superceding this one.  The process of making revisions\nhas been more complicated than I expected, but hopefully we are\nconverging on a better result.\n\nIf you are interested in digest authentication you can have a look at\nthe current working document at\n\n   http://hopf.math.nwu.edu/~john/new_rfc.txt\n\nwhich, as I said, is waiting on some editorial revisions, but (hopefully)\nwill not change in substance.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-0470130"}, {"subject": "Re: About that Host: header...", "content": ">In this case, \"a server that serves HTTP\" is redundant.  How about\n>\n>     URL of a server for multiple hostnames on a single IP address.\n\nWorks for me just fine. And, it leaves my vanity intact. :-)\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-0479172"}, {"subject": "Re: About that Host: header...", "content": "I agree with Roy.\n\nRoy T. Fielding said:\n\n> > On the other hand: host headers are interpretable only by host-aware servers,\n> > while full URLs are visible in every server.\n> \n> But that is exactly the problem -- think of the economics of the situation.\n> \n> The only servers that care about the host header are those that serve\n> multiple hostnames on a single IP address.  This will not change.\n\nYup, those sites that need it will have deployed the servers that\ncan handle the host header.  The rest won't care, and the Host header\nwill have considerably less impact on those other servers than the\nfull URL will.  The problem is getting the clients that send the Host\nheader adequately deployed.\n\n> Note, however, that there are NO currently deployed servers which do\n                                ^^^^^^^^^^^^^^^^^^^^^^^\nNot true, all the NCSA 1.5 servers handle the host header.\n\n> the above, since the HTTP/1.0 protocol (sans Host) does not allow for\n> them to exist.  This means that such servers can be made host-aware\n> at no cost at the moment the owner installs them.\n> \n> Full URLs require upgrading most existing servers before any client can\n> afford to send the full URL on a request to an origin server or gateway.\n> Host doesn't require any server to upgrade, which is why it can be used\n> with HTTP/1.0 clients.\n> \n> Keep in mind that we are trying to solve a particular problem:\n> the IP address space being used up too quickly due to vanity hostnames.\n> That problem cannot be solved until a sufficient number of clients\n> (I'd say about 80%) have implemented the solution.  With Host, all\n> clients can implement it now -- hell, we could probably reach the 80%\n> number by the time we finish arguing about the solution!  In contrast,\n> sending the full URL requires that existing servers change FIRST, and\n> only then can the clients start implementing the solution.\n\nI believe Netscape clients already support the host header (but you better\nask someone from Netscape) and the change is already in the pipeline for\nthe Mosaic clients.  So yes, I'd say the Host header is well on the way\nto being supported already. \n\n> I must emphasize that servers do not get upgraded with the same frequency\n> as clients.  Over 10% of existing practice are still using servers that had\n> well-known security holes discovered in them a year ago!  There is no\n> evidence to suggest that people can be compelled to upgrade their server\n> software any faster.\n\nYes, Yes!  Additionally, I think that most server sites tend to upgrade\nonly when they get complaints.  So the previous solutions suggested where\nthe client silently corrects the URL based on the HTTP version number will\nNOT encourage sites to upgrade, they will just increase traffic.\n\n> The question is not which solution is more elegant or which solution\n> would be better if there were no current practice.  The question is\n> which solution will better solve the problem.  I claim that sending\n> the full URL will not solve the problem because it cannot be deployed\n> in a timely fashion.  Furthermore, attempting to deploy a full URL\n> along with all our other improvements to the protocol will just delay\n> everything.  The full URL can and should be deployed when we make\n> all the other incompatible changes to the protocol -- in HTTP/2.0.\n> \n> Enough on this topic -- I hope that I have covered all the questions\n> that the ADs wanted to see addressed.\n> \n>  ...Roy T. Fielding\n>     Department of Information & Computer Science    (fielding@ics.uci.edu)\n>     University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n>     http://www.ics.uci.edu/~fielding/\n> \n> \n\n\n-- \nElizabeth(Beth) Frank\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n", "id": "lists-010-0486257"}, {"subject": "Re: About that Host: header...", "content": ">I believe Netscape clients already support the host header (but you better\n>ask someone from Netscape) and the change is already in the pipeline for\n>the Mosaic clients.  So yes, I'd say the Host header is well on the way\n>to being supported already. \n\nThis is the case, I have trapped many host header lines from my \nNetscape 2.0 client.\n\nI think that this discussion is going round in circles. We may prefer to have\nthe full URI but there is a vast 1.0 infrastructure out there. The best we can\ndo is to start a phased change which may well take several years.\n\nI can imagine that 1.2 might reuire a full URI for new methods (e.g. WRAPPED) \nbut it looks like Jim's option 4 is the only viable one. Since Roy, Jim and the \nvenodrs appear to be supporting the \"use host but under protest and require 1.1 \nclients accept full URIs\" strategy.\n\n\nCan we close this discussion? I'm sure there are other issues we should be \npanicing about...\n\nPhill\n\n\n\n", "id": "lists-010-0498038"}, {"subject": "Re: About that Host: header...", "content": "Strategies that are 4 pages long are usually incomprehensible.\n\nHere is my personal belief of a reasonable evolutionary strategy for\nHTTP (but in one page):\n\nHTTP 1.0.  Current deployed protocol\nHTTP 1.1   Has host header, etc, given the current consensus on\nthe mailing list.  Current list of other stuff that makes 1.1.\nHTTP 1.2?\n\nHTTP 2.X Multiplexing protocol, still MIME based.\nA number of us have been working on such a thing for the last few\nmonths. See http://www.w3.org/pub/WWW/Protocols/HTTP-NG/mux/WD-mux-960315.html\nfor a multiplexing protocol design (a more compact and extensible version\nof Simon Spero's SCP).\n(Note that this is a draft of a draft and is guaranteed to change, rather \nthan an actual W3 working draft).  Also note that this MUX protocol \ncan multiplex almost anything, so would likely be used with 3.X. \nWe're prototyping implementations of this now at W3C to get some\nexperience with it, and Paul Leach is threatening an implementation as well\nin his research server.  The big issue here is not\nthe mux protocol itself (though I'm sure you can find ways to improve it :-)).\nbut as usual, the transition strategy (how to know when you can\nuse the new, improved protocol).\n\nDepending on timing of the multiplexing protocol, one might either do\na version 1.2 or go directly to a multiplexing protocol.\n\nHTTP 3.X complete redesign, once we really understand what we're doing with the\nweb. Likely a binary protocol of some sort. A.K.A. HTTP-NG.  I think this\nwill take longer to do than we can wait for multiplexing, so think that\nthe multiplexing intermediate step is worthwhile.\n- Jim\n\n\n\n", "id": "lists-010-0506142"}, {"subject": "What is the purpose of Keep alive", "content": "I'm rather new at this, but what does the Connection field do?\nWhat is the purpose of the \"Keep alive\" statement that Netscape 2.0\nsend to servers along with the URL.  Your input is appreciated.\n\n\n\n", "id": "lists-010-0515067"}, {"subject": "the Host: sag", "content": "There's still something I don't understand in the whole Host: story.\n\nFirst of all, it seems to me that there is a consensus about having the full\nURL in HTTP/2.0, and Roy frowns upon dictating it in 1.1 because of the\nlonger deployment time. But what if people write in the specs that \n1.1 servers MUST recognize both form, and 1.1 clients SHOULD send full URL,\nbut MUST fall back to the Host: form in case of a 404 error sent by a\n1.0 server? True, there is a bandwidth waste, but it should not be\nrelevant. And in this way, the transition to 2.0 will be smoother.\n\nThe second thing which puzzles me is the statement by Harald, namely\nthat the Host: solution\n\n% - STILL loses the method information.\n\nWhat's the advantage of having the method information? I cannot see \nthe utility of having a http server which understands other methods - I'd\npreder to leave them to other servers on other ports. Surely I forgot\nsomething trivial: could someone please explain this to me?\n\nciao, .mau.\n\n\n\n", "id": "lists-010-0522029"}, {"subject": "Re: About that Host: header...", "content": "Maurizio Codogno writes in <199603261223.NAA05572@beatles.cselt.stet.it>:\n>But what if people write in the specs that\n>1.1 servers MUST recognize both form, and 1.1 clients SHOULD send full\n>URL,\n>but MUST fall back to the Host: form in case of a 404 error sent by a\n>1.0 server? True, there is a bandwidth waste, but it should not be\n>relevant. And in this way, the transition to 2.0 will be smoother.\n\nEeesh, this is a big, big lose IMHO.  Sending the full URI would mean a 404 \non almost every request for quite some time to come on the Internet.  Jim's \n#4 is the reasonable choice.\n\nBTW, I can testify to the truthfulness of \"admins don't change servers until \nthe users complain\".  That is why I am still running CERN 3.0 on my server, \nand why we ran it on our proxy until recently.  If it has the capabilities \nyou need, and it isn't broke, you don't fix it.\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-0529679"}, {"subject": "Host: header and port numbe", "content": "So what was the final decision on the header and port number?\n\nI got the feeling back then that it was to be dropped from\nthe Header:, but now when I check the mail archives, it is not so\nclear any more.\n\nAlex\n\n\n\n-- \nAlex Lopez-Ortiz                             alopez-o@neumann.UWaterloo.ca\nhttp://daisy.uwaterloo.ca/~alopez-o                     FAX (519)-885-1208\nDepartment of Computer Science                      University of Waterloo\nWaterloo, Ontario N2L 3G1                                           Canada\n\n\n\n", "id": "lists-010-0537881"}, {"subject": "(RFC 1900) Consensu", "content": "At the Los Angeles HTTP working group meeting, Brian Carpenter of the\nIAB raised the issue of RFC 1900 and the use of IP addresses in URL's,\ngiven the increasing of renumbering of networks.\nI've also raised the issue with Dave Raggett to deal with in HTML,\nwhere the problem is most visible.  I did a search on the document,\nand there seems to be few references to IP addresses in the document\nas is, but some warning is in order.\n\nI do not expect that anyone will object to the following additional\nwording to close out Brian's issue, so I'm going to try to close this\none in one message rather than two (objections to me, as Larry is not\nback yet).  If I've missed somewhere else in the document, please let\nme know.\n- Jim Gettys\n\n\nSection 3.2.2: (Uniform Resource Identifiers). \n\nAdd sentence:\n\nThe use of IP addresses in URL's should be avoided whenever possible \n[RFC 1900].\n\nSection 16: (References)\n\n[RFC 1900]\nB. Carpenter, Y. Rekhter, \n<a href=\"http://info.internet.isi.edu:80/in-notes/rfc/files/rfc1900.txt\">\nRenumbering Needs Work</a>. RFC 1900, IAB, February 1996.\n\n\n\n", "id": "lists-010-0545683"}, {"subject": "(DNS) draft wording for W.G. review", "content": "JG DNS\n        Issue: \n<A href=\"http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/dns-usage.txt\">\nspec is silent about \n        clients/servers caching of DNS information.</A>\n        <BR>\n        Status: resolution is to add verbiage that implemeters who cache\n        DNS information inside an implementation, rather than using DNS\n        lookups each time MUST obey DNS TTL rules.  Add words as well\n        to security considerations.\n\n\nI've written a section outlining this requirement; it is currently\na sub-section to be added to the security considerations section,\nas there seems to be no general discussion of how servers are contacted\nelsewhere in the document.  If people think it necessary, I can\nadd such a section to the document, but this seems the easiest way to \nclose the issue.\n- Jim Gettys\n\n\nProposed resolution:\n====================\n\nSection 14 (new subsection to Security Considerations):\n\nDNS Spoofing\n------------\n\nHTTP use relies heavily on the Internet's name service (DNS), and is prone to\nthe attacks generally possible via name server attack.\nDNSSEC deployment should improve this situation.\n\nThe current implementation of many HTTP 1.0 client libraries are prone\nto a particular attack, however.\n\nHTTP clients should generally rely on their system's name resolver for\nlookup when contacting a server, rather than caching the result of host \nname lookups.  Many platforms already cache the results of name\nlookups locally, or can (and should) be configured to do so.\n\nIf clients cache the results of name lookups for performance\nreasons, HTTP clients MUST observe the TTL (Time To Live) information \nreported by the name server.\n\nA server could be spoofed when that server's IP address \nchanges if this rule is not observed.  As renumbering is \nexpected to become increasingly common [RFC 1900], \nthis problem will grow.  This requirement reduces this\npotential security vulnerability.\n\nThis requirement also reduces failures observed by users,\nand improves load-balancing behavior of clients for replicated servers\nhiding behind the same DNS name, as often occurs with large loaded \nHTTP servers.\n\nSection 16: (References)\n\n[RFC 1900]\nB. Carpenter, Y. Rekhter, \n<a href=\"http://info.internet.isi.edu:80/in-notes/rfc/files/rfc1900.txt\">\nRenumbering Needs Work</a>. RFC 1900, IAB, February 1996.\nSection 16: (References)\n\n[RFC 1900]\nB. Carpenter, Y. Rekhter, \n<a href=\"http://info.internet.isi.edu:80/in-notes/rfc/files/rfc1900.txt\">\nRenumbering Needs Work</a>. RFC 1900, IAB, February 1996.\n\n\n\n", "id": "lists-010-0552804"}, {"subject": "Re: About that Host: header...", "content": "I would like to see one thing (as discussed at the LA IETF):\n\nI think we should require (MUST) that servers accept full URL's with all\nmethods in HTTP 1.1. I think this will greatly help our transition strategy\nin the future. I do not think that we should have clients sending full URLs\n(Maybe unless they already know that a particular server is a HTTP 1.1\nserver in which case they could omit the Host field and just send the full URL?)\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n", "id": "lists-010-0562560"}, {"subject": "(CONTENT) draft wording for W.G. revie", "content": "JG CONTENTSection 3.5: Content codings: There are other new codings \nsupplied, e.g., CONDENSE. IANA registers these? If not, who?\n        e.g. URL:ftp://prep.ai.mit.edu/pub/gnu not adequate for reference. \n        Status: not in 1.1, yank to separate document, \ncall for IANA to register. Maybe add CONDENSE. \nSee http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v11-spec-01.html#Content-Codings for this section.\n\nI've had a conversation with Jon Postel (Mr. IANA) over this one.  He\nsays that the references as is in the document for gzip should be OK,\ndespite the lack of a document (though one is in internet-draft form).\nJon recommends staying away from the heavy-weight process that was\ndefined for MIME types, as it has generated much trouble since.  \nSo Roy's draft is close to enough.\n\nThe section does need to reference IANA to register future tokens.\nIANA does need enough in the specification to understand if IANA \nshould register future tokens, when requests come in.  I've drafted\na paragraph to cover this which is in the document below.\nNote the \"should\" wording of documentation for future content-codings;\nagain, Jon recommended against absolute requirements, as such requirements\nin MIME have generated much trouble since.\n\nThere is an internet draft describing gzip being written by\nPeter Deutch; however, according to current IETF rules as I understand them, \nwe cannot reference it except as a \"work in progress\" until it becomes an RFC.\nI've sent mail to Peter to see what the likely outcome of his Internet\ndraft is.  Does anyone have any sort of reference for compress?\n- Jim\n\n\n\nProposed Resolution:\n====================\n\nSection 3.5:\n============\nChange:\n-------\nContent coding values are used to indicate\nTo:\n---\nContent coding values indicate\n\n\nChange:\n-------\n\ncontent-coding= \"gzip\"|\"compress\"|token\n\nto:\n---\ncontent-coding= \"gzip\"|\"x-gzip\"|\"compress\"|\"x-compress\"|token\n\n\nChange:\n-------\nGzip is available from the GNU project at \n<URL:ftp://prep.ai.mit.edu/pub/gnu/>\n\nto:\n---\n[gzip]\n\n\nAdd to section 3.5 (Content Codings):\n-------------------------------------\n\nHTTP defines a registration process which uses the Internet Assigned\nNumbers Authority (IANA) as a central registry for content-coding value\ntokens.  Additional content-coding value tokens should be registered\nwith the IANA beyond the \"gzip\", \"x-gzip\", \"compress\" and \"x-compress\" tokens\ndefined in this document.  To allow interoperability between clients\nand servers, specifications of the content coding algorithms used\nto implement a new value should be publically available and \nadaquate for independent implementation, and must conform to \nthe purpose of content coding defined in this section.\n\nAdd to section 16: (References)\n-------------------------------\n\n[gzip] Gzip is available from the GNU project at \n<URL:ftp://prep.ai.mit.edu/pub/gnu/>, and a\nwork-in-progress describes the gzip data format\nin detail.\n\n\n\n", "id": "lists-010-0569828"}, {"subject": "Resend: URI http: specificatio", "content": "[ this is a resend of a message originally sent on March 4 ]\n\nHello!\n\nI apologize if this question is inappropriate in this forum, but it does\nraise an issue with the current specification that may be of interest to\nthis group. If it is inappropriate, then please indicate what you feel\nwould be a more correct venue.\n\nI'm having a problem resolving the \"correct\" specification for URIs using\nthe \"http\" scheme.\n\nIn the HTTP specification (draft-ietf-http-v10-spec-05.html) the grammar\nshows that \"params\" are legal in a URI in section 3.2.1 and 3.2.2 (http_URL\nhas an abs_path which allows params).\n\nThe conflict is where the HTTP spec refers to RFC 1738, section 3.3. That\nsection does not show parameters.\n\nWhich is the correct form? I would like to use the parameters in a system I\nam putting together (passing parameters in the query section is a bit weird\nsemantically). Most software seems to deal with the params fine, but I'd\nlike some form of acceptance before relying on this aspect of URIs.\n\nThank you.\n\nRegards,\n\nGreg Stein, eShop Inc.\ngreg_stein@eshop.com\n\n\n\n", "id": "lists-010-0579523"}, {"subject": "(SECFILE) Consensu", "content": "The following was added to the 1.0 document after the last\n1.1 draft and needs to be added into the 1.1 document in section 14 \n(Security Considerations).\n\nUnless I hear complaint, the same will be added to the 1.1 draft.\n- Jim\n\n\nAdd subsection to section 14: (Security Considerations)\n\nAttacks Based On File and Path Names\n\nImplementations of HTTP origin servers should be careful to restrict\nthe documents returned by HTTP requests to be only those that were\nintended by the server administrators. If an HTTP server translates\nHTTP URIs directly into file system calls, the server must take special\ncare not to serve files that were not intended to be delivered to HTTP\nclients. For example, Unix, Microsoft Windows, and other operating\nsystems use \"..\" as a path component to indicate a directory level above\nthe current one. On such a system, an HTTP server must disallow any\nsuch construct in the Request-URI if it would otherwise allow access\nto a resource outside those intended to be accessible via the HTTP\nserver. Similarly, files intended for reference only internally to the\nserver (such as access control files, configuration files, and script code)\nmust be protected from inappropriate retrieval, since they might\ncontain sensitive information. Experience has shown that minor bugs\nin such HTTP server implementations have turned into security risks.\n\n\n\n", "id": "lists-010-0587079"}, {"subject": "(PRIVACY) Draft words for W.G. review", "content": "The following sections is intended to emphasize the privacy problems\nthat have been coming up with client implementations (e.g. Javascript problem)\nthat are not careful about the information stored inside themselves.\nComments to me,\nJim Gettys\n\nAdd to Section 14.4:\n-------------------\nHTTP clients are often privy to large amounts of personal information\n(e.g. the user's name, location, mail address, passwords, \nencryption keys, etc.), and should be very careful to prevent unintentional\nleakage of this information via the HTTP protocol to other sources.  We\nsuggest, though do not require, that a convenient interface be provided for\nthe user to control dissemination of such information, and that \ndesigners and implementors be particularly careful in this area.  \nHistory shows that errors in this area are often both serious security\nand/or privacy problems, and often generate very adverse publicity \nfor the implemetor's company.\n\n\n\n", "id": "lists-010-0594381"}, {"subject": "Re: the Host: sag", "content": "Maurizio Codogno wrote:\n>There's still something I don't understand in the whole Host: story.\n>\n>First of all, it seems to me that there is a consensus about having the full\n>URL in HTTP/2.0, and Roy frowns upon dictating it in 1.1 because of the\n>longer deployment time. But what if people write in the specs that \n>1.1 servers MUST recognize both form, and 1.1 clients SHOULD send full URL,\n>but MUST fall back to the Host: form in case of a 404 error sent by a\n>1.0 server? True, there is a bandwidth waste, but it should not be\n>relevant. And in this way, the transition to 2.0 will be smoother.\n\nI still can't imagine any major browser vendor sending the full URL on the\nfirst request right now. At least not unless John Klensin writes me a $5\nmillion contract to provide him with such a browser. But normal users are\njust not going to be picking software which is twice as slow at getting the\ninitial document display on the current installed base of servers.\n\nI do strongly agree with saying that HTTP 1.1 servers MUST recognize both forms.\n\n>The second thing which puzzles me is the statement by Harald, namely\n>that the Host: solution\n>\n>% - STILL loses the method information.\n>\n>What's the advantage of having the method information? I cannot see \n>the utility of having a http server which understands other methods - I'd\n>preder to leave them to other servers on other ports. Surely I forgot\n>something trivial: could someone please explain this to me?\n\nWell one use of sending method information is for proxy-servers. For example\nwe use a proxy server which receives HTTP requests with FTP URL's and then\ngets the file and returns it in an HTTP response. But then again its normal\nto send full URLs to proxies anyway.\n\n\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n", "id": "lists-010-0601375"}, {"subject": "Persistent Connection Questio", "content": "In working on the latest persistent connection draft I have come upon an\nissue that I would like some comments on.\n\nWhen a client wishes to close a persistent connection (after timing out or\nby whatever mechanism), should it:\n\na) Close the TCP connection\nor\nb) Reset the TCP connection\n\nA few notes:\n1) Closing seems initially to be cleaner. It is the \"polite\" technique.\n\nhowever...\n2) Many clients reset every connection (From what I have observed. Maybe I\nam wrong).\n3) Some transport stacks have a more difficult time detecting a closed\nremote connection than a reset one (The reset causes all calls to report an\nerror while a close causes many to continue to work).\n\nAny advice on what I should say?\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n", "id": "lists-010-0609789"}, {"subject": "Re: Persistent Connection Questio", "content": "    3) Some transport stacks have a more difficult time detecting a\n    closed remote connection than a reset one (The reset causes all\n    calls to report an error while a close causes many to continue to\n    work).\n    \nSince HTTP/1.0 uses close-of-connection as the end-of-entity-body\nindicator, how could such stacks support HTTP/1.0?  This seems\nso fundamentally broken that I find it hard to believe we should\nwarp the protocol to deal with it.\n\n-Jeff\n\n\n\n", "id": "lists-010-0617602"}, {"subject": "Re: Persistent Connection Questio", "content": "Jeffrey Mogul wrote:\n>    3) Some transport stacks have a more difficult time detecting a\n>    closed remote connection than a reset one (The reset causes all\n>    calls to report an error while a close causes many to continue to\n>    work).\n>    \n>Since HTTP/1.0 uses close-of-connection as the end-of-entity-body\n>indicator, how could such stacks support HTTP/1.0?  This seems\n>so fundamentally broken that I find it hard to believe we should\n>warp the protocol to deal with it.\nJeff, I think you misunderstand what I was saying. I am not suggesting a\nreset after the entity-body. I am suggesting allowing a reset _from the\nclient_ rather than the client sending a subsequent request.\n\nI agree that the server must use a close rather than a reset, unless it is\nresponding to a network failure, etc.\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n", "id": "lists-010-0625082"}, {"subject": "Re: (DNS) draft wording for W.G. review", "content": "It is as simple as it might seem.\n\nA couple points:\n\n- dns is a real performance pain\n- gethostbyname call doesn't give you ttl information back\n  So, if a ttl info is needed, you have to basicially integrate all \n  resolver code, talking udp to DNS server. Also server selection, timing out,\n  exponential back off and all that. Basically, it is a integration of the whole\n  BIND into client/server.\n\nAnawat\n\n\n\n", "id": "lists-010-0633051"}, {"subject": "Re: (DNS) draft wording for W.G. review", "content": "> It is as simple as it might seem.\n\nI meant \"It is not as simple as it might seem.\".\n\n> - gethostbyname call doesn't give you ttl information back\n>   So, if a ttl info is needed, you have to basicially integrate all \n\nI answered myself here. yes you can use res_mkquery to do raw dns. \nIt's still a pain though.\n\nAnawat\n\n\n\n", "id": "lists-010-0641192"}, {"subject": "Re: (DNS) draft wording for W.G. review", "content": "On Wed, 27 Mar 1996, Anawat Chankhunthod wrote:\n\n> Date: Wed, 27 Mar 1996 01:09:09 -0800\n> From: Anawat Chankhunthod <chankhun@catarina.usc.edu>\n> To: jg@w3.org\n> Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> \n> ...\n>\n> - gethostbyname call doesn't give you ttl information back\n>   So, if a ttl info is needed, you have to basicially integrate all \n>   resolver code, talking udp to DNS server. Also server selection, timing out,\n>   exponential back off and all that. Basically, it is a integration of the whole\n>   BIND into client/server.\n\nSeems like a terrible idea given how quickly and continuously BIND has \nbeen evolving to make it more robust in the face of various security \nattacks.\n\n> Anawat\n\nDonald\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\nhttp://www.cybercash.com           http://www.eff.org/blueribbon.html\n\n\n\n", "id": "lists-010-0649201"}, {"subject": "Re: About that Host: header...", "content": "Ok, I believe we have consensus (not even just rough concensus)\non my option 4.  I've been corresponding with John Klensin \n(slowly; John's been travelling), and he also now agrees to this solution.\n\nTo remind people:\n\n4) (Host+Error) Suggested at the IETF meeting:\n\na) Add host header, with improved wording in the specification.\nb) Require 1.1 server to accept full URL from 1.1 or later client.\n  (so far, same as option 3).\nc) Require server to generate an error if a 1.1 client is detected, and no host\ninformation present (or more strongly, at the expense of extra bytes on\nthe wire, no host header present). \nTransition to requiring full URL in future HTTP version, once 1.0 servers\nhave been replaced.\n\nThe final key observation, which John was not aware of until\nyesterday (with the resulting response of \"Arggh\" from John), \nis that HTTP 1.0 servers always close their connection \nafter a request, even an error request.  A retry after an error,\nwhich would be required if the protocol were fixed properly,\nwould cost another full connection open, not just a extra packet exchange.\nThis second open, would exacerbate the Internet's congestion problem.\n\nJohn was mislead by the fact that FTP and telnet are not so stupid to\nclose the connection after error.  I guess we just have to\nregard HTTP 1.0 about as stupid as can be imagined.\n\nI will draft changes implementing the above and circulate for review\nin the next day.\n- Jim Gettys\n\n\n\n", "id": "lists-010-0659344"}, {"subject": "Re: (DNS) draft wording for W.G. review", "content": ">- dns is a real performance pain\n>- gethostbyname call doesn't give you ttl information back\n>  So, if a ttl info is needed, you have to basicially integrate all \n>  resolver code, talking udp to DNS server. Also server selection, timing out,\n>  exponential back off and all that. Basically, it is a integration of the \nwhole\n>  BIND into client/server.\n\nGethostbyname is not rocket science. Its a few UDP calls and a bit of unpacking. \nIt should not be very hard. I wrote an interface in a couple of days. \n\nNot using the Berkley interface is an obvious thing to do to improve performance \nif you do not have threads. It is a bit sad that Netscape hangs while waiting to \ndo a dns lookup.\n\nI don't think we should restrict the spec on the basis of an inadequate API to \nwhat is only a few lines of code. The complexity of BIND lies in the server not \nin the client. Anwat's post pretty much describes what one has to do.\n\nNote that there are some platforms where it is difficult to interface directly \nto the bind daemon because someone has tried some half baked optimisations. I \nremember when we used to ship libwww with two versions of code, one for the \npeople using the O/S version and the other (and by far the more widely used) \nwhich used a direct interface to dns. I don't know whether that is still an \nissue.\n\n\nIf people like I will post the code [just don't clutter the list with requests \nor JG will be upset]. It may requires some tweaks since I use a garbage \ncollector and a few layers of macros, I think its only about 100 lines of code \nthough].\n\nPhill\n\n\n\n", "id": "lists-010-0667473"}, {"subject": "gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "Note that for DNS security, the prefered interface to the gethostbyname \nfunctionality needs to change anyway to return an indicator as to whether \nthe information has been authenticated, is from an insecure zone so it \ncan't be authenticated, etc.  Perhaps while that additional return info \nin being added, adding return of the TTL so an applicaiton could cache \nthe data and discard it properly would be reasonable...\n\nDonald\n\nOn Wed, 27 Mar 1996 hallam@w3.org wrote:\n\n> Date: Wed, 27 Mar 96 14:08:11 -0500\n> From: hallam@w3.org\n> To: Anawat Chankhunthod <chankhun@catarina.usc.edu>\n> Cc: hallam@w3.org, http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: (DNS) draft wording for W.G. review. \n> \n> \n> >- dns is a real performance pain\n> >- gethostbyname call doesn't give you ttl information back\n> >  So, if a ttl info is needed, you have to basicially integrate all \n> >  resolver code, talking udp to DNS server. Also server selection, timing out,\n> >  exponential back off and all that. Basically, it is a integration of the \n> whole\n> >  BIND into client/server.\n> \n> Gethostbyname is not rocket science. Its a few UDP calls and a bit of unpacking. \n> It should not be very hard. I wrote an interface in a couple of days. \n> \n> Not using the Berkley interface is an obvious thing to do to improve performance \n> if you do not have threads. It is a bit sad that Netscape hangs while waiting to \n> do a dns lookup.\n> \n> I don't think we should restrict the spec on the basis of an inadequate API to \n> what is only a few lines of code. The complexity of BIND lies in the server not \n> in the client. Anwat's post pretty much describes what one has to do.\n> \n> Note that there are some platforms where it is difficult to interface directly \n> to the bind daemon because someone has tried some half baked optimisations. I \n> remember when we used to ship libwww with two versions of code, one for the \n> people using the O/S version and the other (and by far the more widely used) \n> which used a direct interface to dns. I don't know whether that is still an \n> issue.\n> \n> \n> If people like I will post the code [just don't clutter the list with requests \n> or JG will be upset]. It may requires some tweaks since I use a garbage \n> collector and a few layers of macros, I think its only about 100 lines of code \n> though].\n> \n> Phill\n> \n> \n\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\nhttp://www.cybercash.com           http://www.eff.org/blueribbon.html\n\n\n\n", "id": "lists-010-0676789"}, {"subject": "RE: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "If the API change will require new implementations of gethostbyname(),\nwhy not get it (and friends) to do the caching, or reasonably efficient\naccess to the local resolver's cache, instead of pushing this into the\napp layer? \n\n>----------\n>From: Donald E. Eastlake 3rd[SMTP:dee@cybercash.com]\n>Sent: Wednesday, March 27, 1996 11:48 AM\n>To: hallam@w3.org\n>Cc: Anawat Chankhunthod; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com;\n>Paul Vixie; iip@tis.com\n>Subject: gethostbyname (was Re: (DNS) draft wording for W.G. review. )\n>\n>Note that for DNS security, the prefered interface to the gethostbyname\n>\n>functionality needs to change anyway to return an indicator as to\n>whether \n>the information has been authenticated, is from an insecure zone so it \n>can't be authenticated, etc.  Perhaps while that additional return info\n>\n>in being added, adding return of the TTL so an applicaiton could cache \n>the data and discard it properly would be reasonable...\n>\n>Donald\n>\n>On Wed, 27 Mar 1996 hallam@w3.org wrote:\n>\n>> Date: Wed, 27 Mar 96 14:08:11 -0500\n>> From: hallam@w3.org\n>> To: Anawat Chankhunthod <chankhun@catarina.usc.edu>\n>> Cc: hallam@w3.org, http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>> Subject: Re: (DNS) draft wording for W.G. review. \n>> \n>> \n>> >- dns is a real performance pain\n>> >- gethostbyname call doesn't give you ttl information back\n>> >  So, if a ttl info is needed, you have to basicially integrate all \n>> >  resolver code, talking udp to DNS server. Also server selection,\n>>timing out,\n>> >  exponential back off and all that. Basically, it is a integration\n>>of the \n>> whole\n>> >  BIND into client/server.\n>> \n>> Gethostbyname is not rocket science. Its a few UDP calls and a bit of\n>>unpacking. \n>> It should not be very hard. I wrote an interface in a couple of days. \n>> \n>> Not using the Berkley interface is an obvious thing to do to improve\n>>performance \n>> if you do not have threads. It is a bit sad that Netscape hangs while\n>>waiting to \n>> do a dns lookup.\n>> \n>> I don't think we should restrict the spec on the basis of an\n>>inadequate API to \n>> what is only a few lines of code. The complexity of BIND lies in the\n>>server not \n>> in the client. Anwat's post pretty much describes what one has to do.\n>> \n>> Note that there are some platforms where it is difficult to interface\n>>directly \n>> to the bind daemon because someone has tried some half baked\n>>optimisations. I \n>> remember when we used to ship libwww with two versions of code, one\n>>for the \n>> people using the O/S version and the other (and by far the more widely\n>>used) \n>> which used a direct interface to dns. I don't know whether that is\n>>still an \n>> issue.\n>> \n>> \n>> If people like I will post the code [just don't clutter the list with\n>>requests \n>> or JG will be upset]. It may requires some tweaks since I use a garbage \n>> collector and a few layers of macros, I think its only about 100 lines\n>>of code \n>> though].\n>> \n>> Phill\n>> \n>> \n>\n>=====================================================================\n>Donald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n>   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\n>Carlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\n>http://www.cybercash.com           http://www.eff.org/blueribbon.html\n>\n>\n\n\n\n", "id": "lists-010-0692101"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "gethostbyname() is part of the BSD System Interface and we can't change it\nlightly.  Note that it is very general, pertaining equally well to NIS and\n/etc/hosts as to DNS -- therefore widening it to include authentication or\nTTL information would be irrelevant and possibly incorrect.\n\nI am anxious to make progress on a DNS-specific host name lookup function\nthat can be optimized for the kinds of data that DNS has available.  But\ngethostbyname() is not even the beginning of that answer.  Neither, for\nthat matter, is getconninfo() or whatever else Posix.12 specifies.\n\nThe right place for this battle to take place is actually Posix.12, since\napplications will in the long term just call getconninfo() (or whatever),\nand if we want those applications to care about authentication and TTL, we\nneed to make that argument _now_ while the interface is still malleable.\n\n\n\n", "id": "lists-010-0711127"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "So how do you find out what the current state of the Posix standard is \nand how do you provide input to them?\n\nDonald\n\nOn Wed, 27 Mar 1996, Paul A Vixie wrote: \n\n> Date: Wed, 27 Mar 1996 12:30:12 -0800\n> From: Paul A Vixie <paul@vix.com>\n> To: \"Donald E. Eastlake 3rd\" <dee@cybercash.com>\n> Cc: hallam@w3.org, Anawat Chankhunthod <chankhun@catarina.usc.edu>,\n>     http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com, iip@tis.com\n> Subject: Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ) \n> \n> gethostbyname() is part of the BSD System Interface and we can't change it\n> lightly.  Note that it is very general, pertaining equally well to NIS and\n> /etc/hosts as to DNS -- therefore widening it to include authentication or\n> TTL information would be irrelevant and possibly incorrect.\n> \n> I am anxious to make progress on a DNS-specific host name lookup function\n> that can be optimized for the kinds of data that DNS has available.  But\n> gethostbyname() is not even the beginning of that answer.  Neither, for\n> that matter, is getconninfo() or whatever else Posix.12 specifies.\n> \n> The right place for this battle to take place is actually Posix.12, since\n> applications will in the long term just call getconninfo() (or whatever),\n> and if we want those applications to care about authentication and TTL, we\n> need to make that argument _now_ while the interface is still malleable.\n> \n> \n\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\nhttp://www.cybercash.com           http://www.eff.org/blueribbon.html\n\n\n\n", "id": "lists-010-0720885"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "Now I'm back into remembering when and why I wrote the code I remember that I \ngave up on it precisely because of the security stuff.\n\nI'm pretty indifferent to the interface except that:-\n\n1) It should allow access to whatever data is avaliable\n\n2) It should be possible to use it in a non blocking fashion.\n\n(2) is a problem since it would mean that the interface could not be abstracted \noff the network layer to the extent I would like. On the other hand it might be \npossible to work out something that would be based on sockets as opposed to UDP.\nE.g.\n\nDNS_initialize (&context, DNS_default)\n//returns a dns query context with the system default\n//DNS lookup daemons instantiated. Note that context is an\n//opaque structure, the calling program is not allowed to \n//meddle with it.\nDNS_add_resolver (context, &address}\n//address = {18.24.1.191, weight} i.e. we give it an extra\n//resolver to look at, this structure could have a weight\n//associated with it allowing information. \n//This is particularly usefull when doing higher level\n//hacking, e.g. looking to find an authoratative server.\nDNS_query (context, &name, &address, &query_id, flag)\n//flag can be\n//  DNS_SYNC- wait for DNS query to return\n//  DNS_ASYNC- asynchronous, if query cannot be resolved\n//immediately will return a socket which can be\n//waited on as part of query_id.\nDNS_test (context, query_id, flag)\n//Checks to see if a query has completed. Flag allows caller to \n//chose to wait. Note that this function would generally\n//be called after a select on the query_id socket. The idea\n//being to allow multiple DNS queries to be grouped on the\n//same socket.\n//Alternatively context could have a pointer to a linked list\n//of resolved query_id structures. An application could then\n//avoid having to check each outstanding DNS query in turn.\n//Internaly the code is likely to want to use a hash table or\n//balanced tree to match queries with responses.\nDNS_close (context, query_id)\n//Dispose of query, return\nDNS_terminate (context)\n//Free any structures held open (e.g. application level cache)\n\nNote that this is a considerably more complex interface than people are used to. \nI think that this is unavoidable if we are to have something that will work in \nmultiprocessor environments (I believe that we will be using boxes with large \narrays of MIMD processors in the realatively near future. By large I mean 64+, \nnot the 4 to 16 that shared memory models give you).\n\nNote that the idea of the context is not a very Berkley or UNIX style of API but \nit does mean that progams can avoid using global variables. This is essential if \ncode is to run in multithreaded, multiprocessor environments.\n\nThe sort of place where this code would be of most use would be in code to do \nbulk DNS queries to build up databases - e.g. to asist in analysis of log files \nor as a prefilter inside a mailing list distributor.\n\n\nI think that the right place for this battle is a cabal in the back of an IETF \nmeeting rather than POSIX or such like. \n\nPhill\n\n\n\n", "id": "lists-010-0734503"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "> gethostbyname() is part of the BSD System Interface and we can't change it\n> lightly.  Note that it is very general, pertaining equally well to NIS and\n> /etc/hosts as to DNS -- therefore widening it to include authentication or\n> TTL information would be irrelevant and possibly incorrect.\n\nThis is a real problem that puts software developers between a rock and hard\nplace. On the one hand, you have to replace gethostbyname because of its\nlimitations (doesn't return all the info you need, not thread safe on some\nsystems, etc.). But in turn gethostbyname may provide the only useful means of\naccessing some sorts of host data your application needs to see. Some sites\ndepend on host data kept in non-DNS databases (NIS isn't the only one by any\nmeans).\n\nIn a perfect world there would be a single interface provided by the various\nOSes that provides thread-safe access to all of the information incorporating\naccess to local information sources as well as DNS. But quite frankly I have no\nidea how to achieve this state of perfection -- I'm skeptical that it's even\npossible to get there. The one thing I do know, however, is that neither\ncoding a replacement for gethostbyname nor sticking with the existing\nroutine and all its warts are acceptable answers.\n\nNed\n\n\n\n", "id": "lists-010-0746645"}, {"subject": "Re: (DNS) draft wording for W.G. review", "content": "> Seems like a terrible idea given how quickly and continuously BIND has \n> been evolving to make it more robust in the face of various security \n> attacks.\n\nGiven the current DNS performance. You don't have a choice if you want  \na decent performance. You just have to cache dns info. \n\nI think the best you can do without integrating the whole BIND is to integrate\njust result interpreter, gethostnamadr.c into your code. So you can get ttl.\nand then use res_send call to resolver library. Not to mention to blocking\nbehavior of res_send, which can block up to 5-10 seconds... It's just really\npainful.\n\nAnawat\n\n\n\n", "id": "lists-010-0757022"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "> DNS_initialize (&context, DNS_default)\n\n[...]\n\nIf we can have this kind of interface as a standard, that'll be great.\nEven better if a resolver lib does a client side caching. So app programmer\ndon't have to mess with it. However, api to control caching should be provided,\nlike, set cache size and cache flush, stuff like that.\n\n> The sort of place where this code would be of most use would be in code to do \n> bulk DNS queries to build up databases - e.g. to asist in analysis of log \n> files or as a prefilter inside a mailing list distributor.\n\nanother example, high volume www proxy/cache server. say > 50,000 requests per\nhour.\n\nAnawat\n\n\n\n", "id": "lists-010-0765150"}, {"subject": "Apache LogFormat  addition of %v virtual server I", "content": "Upgrading our server from httpd to Apache, I needed the %v addition\nmentioned in mod_log_config.c, and added this to apache.conf.\n\nLogFormat \"%h %l %u %t %v \\\"%r\\\" %s %b\"\n\n\n * The way this is supposed to work with virtual hosts is as follows:\n * a virtual host can have its own LogFormat, or its own TransferLog.\n * If it doesn't have its own LogFormat, it inherits from the main\n * server.  If it doesn't have its own TransferLog, it writes to the\n * same descriptor (meaning the same process for \"| ...\").\n *\n * That means that you can do things like:\n *\n * <VirtualHost hosta.com>\n * LogFormat \"hosta ...\"\n * ...\n * </VirtualHost>\n *\n * <VirtualHost hosta.com>\n * LogFormat \"hostb ...\"\n * ...\n * </VirtualHost>\n *\n * ... to have different virtual servers write into the same log file,\n * but have some indication which host they came from, though a %v\n * directive may well be a better way to handle this.\n *\n * --- rst\n\nSo, I hacked it in.\n\nHere is the patch :-\n\n*** apache_1.0.0/src/mod_log_config.c   Fri Mar  1 07:42:55 1996\n--- apache_1.0.3/src/mod_log_config.c   Wed Mar 27 19:50:49 1996\n***************\n*** 159,164 ****\n--- 159,169 ----\n  \n  char *constant_item (request_rec *dummy, char *stuff) { return stuff; }\n  \n+ /* added for %v functionality - andyr@wizzy.com */\n+ \n+ char *log_server_hostname (request_rec *r, char *a)\n+ { return r->server->server_hostname; }\n+ \n  char *log_remote_host (request_rec *r, char *a)\n  { return r->connection->remote_name; }\n  \n***************\n*** 223,228 ****\n--- 228,234 ----\n      { 'l', log_remote_logname, 0 },\n      { 'u', log_remote_user, 0 },\n      { 't', log_request_time, 0 },\n+     { 'v', log_server_hostname, 0 },  /* added - andyr@wizzy.com */\n      { 'r', log_request_line, 1 },\n      { 's', log_status, 1 },\n      { 'b', log_bytes_sent, 0 },\n\nCheers,     Andy!\n\n--\n-- Andy Rabagliati . andyr@wizzy.com\n--\n-- http://altavista.digital.com/cgi-bin/query?pg=q&q=Rabagliati\n\n\n\n", "id": "lists-010-0773614"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "> If the API change will require new implementations of gethostbyname(),\n> why not get it (and friends) to do the caching, or reasonably efficient\n> access to the local resolver's cache, instead of pushing this into the\n> app layer? \n\nAgree. We need client side (resolver's client) caching. I think we can't afford\nto make a network conversation, even on udp on every single dns lookups. \n\nAlso, if resolver library does a caching/nb-lookup, we can feel more \ncomfortable about DNS security. As least we will be using the same algorithm,\nnot 100 different caching scheme by 100 different implementations/programmers.\n\nAnawat\n\n\n\n", "id": "lists-010-0783443"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "> So how do you find out what the current state of the Posix standard is \n> and how do you provide input to them?\n\ni'd talk to keith sklower.\n\n\n\n", "id": "lists-010-0791947"}, {"subject": "Re: Apache LogFormat  addition of %v virtual server I", "content": "Hi each,\n\nthanks for the patch Andy, I've forwarded it to the Apache \ndevelopers.  If anyone else is iterested in contributing patches\nor advice to Apache Group the best way to make contact is by sending\na mesage to:\n\napache-bugs@hyperreal.com\n\nApologies for off-topic noise.\n\nCheers,\nAy.\n\nAndrew.Wilson@cm.cf.ac.uk http://www.cm.cf.ac.uk/User/Andrew.Wilson/\n\nAndy Rabagliati . <andyr@wizzy.com>:\n> Upgrading our server from httpd to Apache, I needed the %v addition\n> mentioned in mod_log_config.c, and added this to apache.conf.\n> \n> LogFormat \"%h %l %u %t %v \\\"%r\\\" %s %b\"\n\n[snip...]\n\n\n\n", "id": "lists-010-0800692"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "gethostbyname() needs to be updated, and someone on the BIND team is working\non that.  it cannot become any more DNS-centric than it is now, however.\n\ngetconninfo() or whatever POSIX calls it needs to be implemented and shipped\nwith BIND, and that will be done once it stops changing.  keith sklower and\neric allman have an implementation of the current draft specification.\n\na DNS-specific accessor function needs to be provided, that will allow\nunmarshalling of DNS responses so that applications who don't want to call\nGETSHORT() and its ilk, won't have to.  a thread safe API for this will\nappear in version current+2 of BIND.\n\n> > gethostbyname() is part of the BSD System Interface and we can't change it\n> > lightly.  Note that it is very general, pertaining equally well to NIS and\n> > /etc/hosts as to DNS -- therefore widening it to include authentication or\n> > TTL information would be irrelevant and possibly incorrect.\n> \n> This is a real problem that puts software developers between a rock and hard\n> place. On the one hand, you have to replace gethostbyname because of its\n> limitations (doesn't return all the info you need, not thread safe on some\n> systems, etc.). But in turn gethostbyname may provide the only useful means of\n> accessing some sorts of host data your application needs to see. Some sites\n> depend on host data kept in non-DNS databases (NIS isn't the only one by any\n> means).\n> \n> In a perfect world there would be a single interface provided by the various\n> OSes that provides thread-safe access to all of the information incorporating\n> access to local information sources as well as DNS. But quite frankly I have no\n> idea how to achieve this state of perfection -- I'm skeptical that it's even\n> possible to get there. The one thing I do know, however, is that neither\n> coding a replacement for gethostbyname nor sticking with the existing\n> routine and all its warts are acceptable answers.\n> \n> Ned\n\n\n\n", "id": "lists-010-0809321"}, {"subject": "Re: gethostbyname (was Re: (DNS) draft wording for W.G. review. ", "content": "\"Donald E. Eastlake 3rd\" writes:\n\n> So how do you find out what the current state of the Posix standard is \n> and how do you provide input to them?\n\nIt is IEEE that handles POSIX standardization. They have a web page:\nhttp://stdsbbs.ieee.org/groups/pasc/index.html\n\nKeld\n\n\n\n", "id": "lists-010-0820213"}, {"subject": "(CODES) Draft additions/changes to section 9.4 for 4xx class status codes", "content": "Additions/changes to section 9.4 of the 1.1 specification.   Send me any\nchanges or improvements.\n- Jim Gettys\n\n\n\n412 Precondition Failed\n\n    Same as current 412 -- just the reason phrase has changed and\n    the explanation associated with the as-yet-undefined precondition\n    header field name.  This may end up as \"reserved for future use\",\n    but the code will remain in the spec.\n\n413 Request Entity Too Large  [proposed by Jeff Mogul on http-wg, 07 Dec 1995]\n\n    If the server doesn't want to receive the large body, it\n    immediately replies with a 413 response, and\n    immediately closes (not resets) the connection.\n    \n    If the client manages to read the 413 response, it must\n    honor it and should reflect it to the user.\n\n    If this restriction is considered temporary, the server may include\n    a Retry-After header field to indicate that it is temporary and after\n    what time the client may try again.\n\n414 Request-URI Too Large\n\n    The server is refusing to service the request because the\n    Request-URI is longer than the server is willing to interpret.\n    This rare condition is only likely to occur when a client has\n    improperly converted a POST request to a GET request with long query\n    info, when the client has descended into a URL \"black hole\" of\n    redirections (e.g., a redirected URL prefix that points to a suffix\n    of itself), or when the server is under attack by a client\n    attempting to exploit security holes present in some servers using\n    fixed-length buffers for reading or manipulating the Request-URI.\n\n415 Unsupported Media Type\n\n    The server is refusing to service the request because the entity\n    body of the request is in a format not supported by the requested\n    resource for the requested method.\n\n\n\n", "id": "lists-010-0829700"}, {"subject": "(MEDIATYPES) consensus", "content": "Included below are diffs made to the HTTP 1.0 specification between\nDraft 4 (which the HTTP 1.1 specification was based on) to draft 5 to\nclarify its relationship to MIME.  The same modifications will be\nmade to the HTTP 1.1 specification to bring it up to the same\nexplanations as already exist in the 1.0 draft.\n\nIf there are any problems you have with this, please let me know, or\nI will believe consensus exists to make the same changes to the 1.1 draft\nto close this issue.\n- Jim\n\n\n***************\n*** ???,??? ****\n+ 1.4  HTTP and MIME\n+ \n+    HTTP/1.0 uses many of the constructs defined for MIME, as defined \n+    in RFC 1521 [5]. Appendix C describes the ways in which the context \n+    of HTTP allows for different use of Internet Media Types than is \n+    typically found in Internet mail, and gives the rationale for those \n+    differences.\n+ \n***************\n*** 656,658 ****\n  \n!        pchar          = uchar | \":\" | \"@\" | \"&\" | \"=\"\n         uchar          = unreserved | escape\n--- 696,698 ----\n  \n!        pchar          = uchar | \":\" | \"@\" | \"&\" | \"=\" | \"+\"\n         uchar          = unreserved | escape\n***************\n*** 660,670 ****\n  \n!        escape         = \"%\" hex hex\n!        hex            = \"A\" | \"B\" | \"C\" | \"D\" | \"E\" | \"F\"\n!                       | \"a\" | \"b\" | \"c\" | \"d\" | \"e\" | \"f\" | DIGIT\n! \n!        reserved       = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\"\n!        safe           = \"$\" | \"-\" | \"_\" | \".\" | \"+\"\n         extra          = \"!\" | \"*\" | \"'\" | \"(\" | \")\" | \",\"\n!        national       = <any OCTET excluding CTLs, SP,\n!                          ALPHA, DIGIT, reserved, safe, and extra>\n  \n--- 700,708 ----\n  \n!        escape         = \"%\" HEX HEX\n!        reserved       = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\" | \"+\"\n         extra          = \"!\" | \"*\" | \"'\" | \"(\" | \")\" | \",\"\n!        safe           = \"$\" | \"-\" | \"_\" | \".\"\n!        unsafe         = CTL | SP | <\"> | \"#\" | \"%\" | \"<\" | \">\"\n!        national       = <any OCTET excluding ALPHA, DIGIT,\n!                         reserved, extra, safe, and unsafe>\n  \n***************\n*** 790,791 ****\n--- 828,834 ----\n  \n+        Note: This use of the term \"character set\" is more commonly \n+        referred to as a \"character encoding.\" However, since HTTP \n+        and MIME share the same registry, it is important that the \n+        terminology also be shared.\n+ \n     HTTP character sets are identified by case-insensitive tokens. The \n***************\n*** 814,819 ****\n  \n!        Note: This use of the term \"character set\" is more commonly \n!        referred to as a \"character encoding.\" However, since HTTP \n!        and MIME share the same registry, it is important that the \n!        terminology also be shared.\n  \n--- 857,862 ----\n  \n!    The character set of an entity body should be labelled as the \n!    lowest common denominator of the character codes used within that \n!    body, with the exception that no label is preferred over the labels \n!    US-ASCII or ISO-8859-1.\n  \n***************\n*** 845,849 ****\n         \"gzip\" (GNU zip) developed by Jean-loup Gailly. This format is \n!        typically a Lempel-Ziv coding (LZ77) with a 32 bit CRC. Gzip is \n!        available from the GNU project at \n!        <URL:ftp://prep.ai.mit.edu/pub/gnu/>.\n  \n--- 888,890 ----\n         \"gzip\" (GNU zip) developed by Jean-loup Gailly. This format is \n!        typically a Lempel-Ziv coding (LZ77) with a 32 bit CRC.\n  \n***************\n*** 863,871 ****\n     field (Section 10.5) in order to provide open and extensible data \n!    typing. For mail applications, where there is no type negotiation \n!    between sender and recipient, it is reasonable to put strict limits \n!    on the set of allowed media types. With HTTP, where the sender and \n!    recipient can communicate directly, applications are allowed more \n!    freedom in the use of non-registered types. The following grammar \n!    for media types is a superset of that for MIME because it does not \n!    restrict itself to the official IANA and x-token types.\n  \n--- 904,906 ----\n     field (Section 10.5) in order to provide open and extensible data \n!    typing.\n  \n***************\n*** 886,908 ****\n     LWS must not be generated between the type and subtype, nor between \n!    an attribute and its value.\n  \n!    Many current applications do not recognize media type parameters. \n!    Since parameters are a fundamental aspect of media types, this must \n!    be considered an error in those applications. Nevertheless, \n!    HTTP/1.1 applications should only use media type parameters when \n!    they are necessary to define the content of a message.\n  \n!    If a given media-type value has been registered by the IANA, any \n!    use of that value must be indicative of the registered data format. \n!    Although HTTP allows the use of non-registered media types, such \n!    usage must not conflict with the IANA registry. Data providers are \n!    strongly encouraged to register their media types with IANA via the \n!    procedures outlined in RFC 1590 [13].\n  \n-    All media-type's registered by IANA must be preferred over \n-    extension tokens. However, HTTP does not limit applications to the \n-    use of officially registered media types, nor does it encourage the \n-    use of an \"x-\" prefix for unofficial types outside of explicitly \n-    short experimental use between consenting applications.\n- \n  3.6.1 Canonicalization and Text Defaults\n--- 921,936 ----\n     LWS must not be generated between the type and subtype, nor between \n!    an attribute and its value. Upon receipt of a media type with an \n!    unrecognized parameter, a user agent should treat the media type as \n!    if the unrecognized parameter and its value were not present.\n  \n!    Some older HTTP applications do not recognize media type \n!    parameters. HTTP/1.0 applications should only use media type \n!    parameters when they are necessary to define the content of a \n!    message.\n  \n!    Media-type values are registered with the Internet Assigned Number \n!    Authority (IANA [15]). The media type registration process is \n!    outlined in RFC 1590 [13]. Use of non-registered media types is \n!    discouraged.\n  \n  3.6.1 Canonicalization and Text Defaults\n***************\n*** 909,958 ****\n  \n!    Media types are registered in a canonical form. In general, entity \n!    bodies transferred via HTTP must be represented in the appropriate \n!    canonical form prior to transmission. If the body has been encoded \n!    via a Content-Encoding, the data must be in canonical form prior to \n!    that encoding. However, HTTP modifies the canonical form \n!    requirements for media of primary type \"text\" and for \"application\" \n!    types consisting of text-like records.\n  \n!    HTTP redefines the canonical form of text media to allow multiple \n!    octet sequences to indicate a text line break. In addition to the \n!    preferred form of CRLF, HTTP applications must accept a bare CR or \n!    LF alone as representing a single line break in text media. \n!    Furthermore, if the text media is represented in a character set \n!    which does not use octets 13 and 10 for CR and LF respectively, as \n     is the case for some multi-byte character sets, HTTP allows the use \n!    of whatever octet sequence(s) is defined by that character set to \n!    represent the equivalent of CRLF, bare CR, and bare LF. It is \n!    assumed that any recipient capable of using such a character set \n!    will know the appropriate octet sequence for representing line \n!    breaks within that character set.\n  \n!        Note: This interpretation of line breaks applies only to the \n!        contents of an Entity-Body and only after any \n!        Content-Encoding has been removed. All other HTTP constructs \n!        use CRLF exclusively to indicate a line break. Content \n!        codings define their own line break requirements.\n  \n!    A recipient of an HTTP text entity should translate the received \n!    entity line breaks to the local line break conventions before \n!    saving the entity external to the application and its cache; \n!    whether this translation takes place immediately upon receipt of \n!    the entity, or only when prompted by the user, is entirely up to \n!    the individual application.\n  \n-    HTTP also redefines the default character set for text media in an \n-    entity body. If a textual media type defines a charset parameter \n-    with a registered default value of \"US-ASCII\", HTTP changes the \n-    default to be \"ISO-8859-1\". Since the ISO-8859-1 [18] character set \n-    is a superset of US-ASCII [17], this has no effect upon the \n-    interpretation of entity bodies which only contain octets within \n-    the US-ASCII set (0 - 127). The presence of a charset parameter \n-    value in a Content-Type header field overrides the default.\n- \n-    It is recommended that the character set of an entity body be \n-    labelled as the lowest common denominator of the character codes \n-    used within a document, with the exception that no label is \n-    preferred over the labels US-ASCII or ISO-8859-1.\n- \n  3.6.2 Multipart Types\n--- 937,978 ----\n  \n!    Internet media types are registered with a canonical form. In \n!    general, an Entity-Body transferred via HTTP must be represented in \n!    the appropriate canonical form prior to its transmission. If the \n!    body has been encoded with a Content-Encoding, the underlying data \n!    should be in canonical form prior to being encoded.\n  \n!    Media subtypes of the \"text\" type use CRLF as the text line break \n!    when in canonical form. However, HTTP allows the transport of text \n!    media with plain CR or LF alone representing a line break when used \n!    consistently within the Entity-Body. HTTP applications must accept \n!    CRLF, bare CR, and bare LF as being representative of a line break \n!    in text media received via HTTP.\n! \n!    In addition, if the text media is represented in a character set \n!    that does not use octets 13 and 10 for CR and LF respectively, as \n     is the case for some multi-byte character sets, HTTP allows the use \n!    of whatever octet sequences are defined by that character set to \n!    represent the equivalent of CR and LF for line breaks. This \n!    flexibility regarding line breaks applies only to text media in the \n!    Entity-Body; a bare CR or LF should not be substituted for CRLF \n!    within any of the HTTP control structures (such as header fields \n!    and multipart boundaries).\n  \n!    The \"charset\" parameter is used with some media types to define the \n!    character set (Section 3.4) of the data. When no explicit charset \n!    parameter is provided by the sender, media subtypes of the \"text\" \n!    type are defined to have a default charset value of \"ISO-8859-1\" \n!    when received via HTTP. Data in character sets other than \n!    \"ISO-8859-1\" or its subsets must be labelled with an appropriate \n!    charset value in order to be consistently interpreted by the \n!    recipient.\n  \n!        Note: Many current HTTP servers provide data using charsets \n!        other than \"ISO-8859-1\" without proper labelling. This \n!        situation reduces interoperability and is not recommended. \n!        To compensate for this, some HTTP user agents provide a \n!        configuration option to allow the user to change the default \n!        interpretation of the media type character set when no \n!        charset parameter is given.\n  \n  3.6.2 Multipart Types\n***************\n*** 964,975 ****\n     each type in order to correctly interpret the purpose of each \n!    body-part. Ideally, an HTTP user agent should follow the same or \n!    similar behavior as a MIME user agent does upon receipt of a \n!    multipart type.\n  \n!    As in MIME [5], all multipart types share a common syntax and must \n!    include a boundary parameter as part of the media type value. The \n!    message body is itself a protocol element and must therefore use \n!    only CRLF to represent line breaks between body-parts. Unlike in \n!    MIME, multipart body-parts may contain HTTP header fields which are \n!    significant to the meaning of that part.\n  \n--- 984,996 ----\n     each type in order to correctly interpret the purpose of each \n!    body-part. An HTTP user agent should follow the same or similar \n!    behavior as a MIME user agent does upon receipt of a multipart \n!    type. HTTP servers should not assume that all HTTP clients are \n!    prepared to handle multipart types.\n  \n!    All multipart types share a common syntax and must include a \n!    boundary parameter as part of the media type value. The message \n!    body is itself a protocol element and must therefore use only CRLF \n!    to represent line breaks between body-parts. Multipart body-parts \n!    may contain HTTP header fields which are significant to the meaning \n!    of that part.\n  \n***************\n*** 1085,1088 ****\n         General-Header = Date                     ; Section 10.6\n!                       | MIME-Version             ; Section 10.12\n!                       | Pragma                   ; Section 10.13\n  \n--- 1106,1108 ----\n         General-Header = Date                     ; Section 10.6\n!                       | Pragma                   ; Section 10.12\n  \n***************\n*** 1190,1192 ****\n     The Request-URI is transmitted as an encoded string, where some \n!    characters may be escaped using the \"% hex hex\" encoding defined by \n     RFC 1738 [4]. The origin server must decode the Request-URI in \n--- 1210,1212 ----\n     The Request-URI is transmitted as an encoded string, where some \n!    characters may be escaped using the \"% HEX HEX\" encoding defined by \n     RFC 1738 [4]. The origin server must decode the Request-URI in \n***************\n*** 1333,1337 ****\n     information about the response which cannot be placed in the \n!    Status-Line. These header fields are not intended to give \n!    information about an Entity-Body returned in the response, but \n!    about the server itself.\n  \n--- 1356,1360 ----\n     information about the response which cannot be placed in the \n!    Status-Line. These header fields give information about the server \n!    and about further access to the resource identified by the \n!    Request-URI.\n  \n***************\n*** 1648,1649 ****\n--- 1673,1678 ----\n  \n+        Note: When automatically redirecting a POST request after \n+        receiving a 301 status code, some existing user agents will \n+        erroneously change it into a GET request.\n+ \n     302 Moved Temporarily\n***************\n*** 1663,1664 ****\n--- 1692,1697 ----\n  \n+        Note: When automatically redirecting a POST request after \n+        receiving a 302 status code, some existing user agents will \n+        erroneously change it into a GET request.\n+ \n     304 Not Modified\n***************\n*** 2090,2112 ****\n  \n- 10.12  MIME-Version\n- \n-    HTTP is not a MIME-compliant protocol (see Appendix C). However, \n-    HTTP/1.0 messages may include a single MIME-Version general-header \n-    field to indicate what version of the MIME protocol was used to \n-    construct the message. Use of the MIME-Version header field should \n-    indicate that the message is in full compliance with the MIME \n-    protocol (as defined in [5]). Unfortunately, some older versions of \n-    HTTP/1.0 clients and servers use this field indiscriminately, and \n-    thus recipients must not take it for granted that the message is \n-    indeed in full compliance with MIME. Proxies and gateways are \n-    responsible for ensuring this compliance (where possible) when \n-    exporting HTTP messages to strict MIME environments. Future \n-    HTTP/1.0 applications must only use MIME-Version when the message \n-    is fully MIME-compliant.\n- \n-        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT\n- \n-    MIME version \"1.0\" is the default for use in HTTP/1.0. However, \n-    HTTP/1.0 message parsing and semantics are defined by this document \n-    and not the MIME specification.\n- \n--- 2123,2123 ----\n  \n***************\n*** 2427,2428 ****\n--- 2444,2463 ----\n  \n+ 12.5  Attacks Based On File and Path Names\n+ \n+    Implementations of HTTP origin servers should be careful to \n+    restrict the documents returned by HTTP requests to be only those \n+    that were intended by the server administrators. If an HTTP server \n+    translates HTTP URIs directly into file system calls, the server \n+    must take special care not to serve files that were not intended to \n+    be delivered to HTTP clients. For example, Unix, Microsoft Windows, \n+    and other operating systems use \"..\" as a path component to \n+    indicate a directory level above the current one. On such a system, \n+    an HTTP server must disallow any such construct in the Request-URI \n+    if it would otherwise allow access to a resource outside those \n+    intended to be accessible via the HTTP server. Similarly, files \n+    intended for reference only internally to the server (such as \n+    access control files, configuration files, and script code) must be \n+    protected from inappropriate retrieval, since they might contain \n+    sensitive information. Experience has shown that minor bugs in such \n+    HTTP server implementations have turned into security risks.\n+ \n***************\n*** 2633,2635 ****\n  \n!    HTTP/1.0 reuses many of the constructs defined for Internet Mail \n     (RFC 822 [7]) and the Multipurpose Internet Mail Extensions \n--- 2668,2670 ----\n  \n!    HTTP/1.0 uses many of the constructs defined for Internet Mail \n     (RFC 822 [7]) and the Multipurpose Internet Mail Extensions \n***************\n*** 2636,2649 ****\n     (MIME [5]) to allow entities to be transmitted in an open variety \n!    of representations and with extensible mechanisms. However, HTTP is \n!    not a MIME-compliant application. HTTP's performance requirements \n!    differ substantially from those of Internet mail. Since it is not \n!    limited by the restrictions of existing mail protocols and SMTP \n!    gateways, HTTP does not obey some of the constraints imposed by \n!    RFC 822 and MIME for mail transport.\n  \n!    This appendix describes specific areas where HTTP differs from \n!    MIME. Proxies/gateways to MIME-compliant protocols must be aware of \n!    these differences and provide the appropriate conversions where \n!    necessary.\n  \n  C.1  Conversion to Canonical Form\n--- 2671,2691 ----\n     (MIME [5]) to allow entities to be transmitted in an open variety \n!    of representations and with extensible mechanisms. However, \n!    RFC 1521 discusses mail, and HTTP has a few features that are \n!    different than those described in RFC 1521. These differences were \n!    carefully chosen to optimize performance over binary connections, \n!    to allow greater freedom in the use of new media types, to make \n!    date comparisons easier, and to acknowledge the practice of some \n!    early HTTP servers and clients.\n  \n!    At the time of this writing, it is expected that RFC 1521 will be \n!    revised. The revisions may include some of the practices found in \n!    HTTP/1.0 but not in RFC 1521.\n  \n+    This appendix describes specific areas where HTTP differs from RFC \n+    1521. Proxies and gateways to strict MIME environments should be \n+    aware of these differences and provide the appropriate conversions \n+    where necessary. Proxies and gateways from MIME environments to \n+    HTTP also need to be aware of the differences because some \n+    conversions may be required.\n+ \n  C.1  Conversion to Canonical Form\n***************\n*** 2650,2733 ****\n  \n!    MIME requires that an entity be converted to canonical form prior \n!    to being transferred, as described in Appendix G of RFC 1521 [5]. \n!    Although HTTP does require media types to be transferred in \n!    canonical form, it changes the definition of \"canonical form\" for \n!    text-based media types as described in Section 3.6.1.\n  \n! C.1.1 Representation of Line Breaks\n  \n!    MIME requires that the canonical form of any text type represent \n!    line breaks as CRLF and forbids the use of CR or LF outside of line \n!    break sequences. Since HTTP allows CRLF, bare CR, and bare LF (or \n!    the octet sequence(s) to which they would be translated for the \n!    given character set) to indicate a line break within text content, \n!    recipients of an HTTP message cannot rely upon receiving \n!    MIME-canonical line breaks in text.\n  \n!    Where it is possible, a proxy or gateway from HTTP to a \n!    MIME-compliant protocol should translate all line breaks within \n!    text/* media types to the MIME canonical form of CRLF. However, \n!    this may be complicated by the presence of a Content-Encoding and \n!    by the fact that HTTP allows the use of some character sets which \n!    do not use octets 13 and 10 to represent CR and LF, as is the case \n!    for some multi-byte character sets. If canonicalization is \n!    performed, the Content-Length header field value must be updated to \n!    reflect the new body length.\n  \n! C.1.2 Default Character Set\n  \n!    MIME requires that all subtypes of the top-level Content-Type \n!    \"text\" have a default character set of US-ASCII [17]. In contrast, \n!    HTTP defines the default character set for \"text\" to be \n!    ISO-8859-1 [18] (a superset of US-ASCII). Therefore, if a text/* \n!    media type given in the Content-Type header field does not already \n!    include an explicit charset parameter, the parameter\n  \n!        ;charset=\"iso-8859-1\"\n  \n!    should be added by the proxy/gateway if the entity contains any \n!    octets greater than 127.\n  \n! C.2  Conversion of Date Formats\n  \n!    HTTP/1.0 uses a restricted subset of date formats to simplify the \n!    process of date comparison. Proxies/gateways from other protocols \n!    should ensure that any Date header field present in a message \n!    conforms to one of the HTTP/1.0 formats and rewrite the date if \n!    necessary.\n  \n! C.3  Introduction of Content-Encoding\n  \n!    MIME does not include any concept equivalent to HTTP's \n!    Content-Encoding header field. Since this acts as a modifier on the \n!    media type, proxies/gateways to MIME-compliant protocols must \n!    either change the value of the Content-Type header field or decode \n!    the Entity-Body before forwarding the message.\n  \n!        Note: Some experimental applications of Content-Type for \n!        Internet mail have used a media-type parameter of \n!        \";conversions=<content-coding>\" to perform an equivalent \n!        function as Content-Encoding. However, this parameter is not \n!        part of the MIME specification at the time of this writing.\n  \n! C.4  No Content-Transfer-Encoding\n  \n!    HTTP does not use the Content-Transfer-Encoding (CTE) field of \n!    MIME. Proxies/gateways from MIME-compliant protocols must remove \n!    any non-identity CTE (\"quoted-printable\" or \"base64\") encoding \n!    prior to delivering the response message to an HTTP client. \n!    Proxies/gateways to MIME-compliant protocols are responsible for \n!    ensuring that the message is in the correct format and encoding for \n!    safe transport on that protocol, where \"safe transport\" is defined \n!    by the limitations of the protocol being used. At a minimum, the \n!    CTE field of\n  \n!        Content-Transfer-Encoding: binary\n  \n!    should be added by the proxy/gateway if it is unwilling to apply a \n!    content transfer encoding.\n  \n!    An HTTP client may include a Content-Transfer-Encoding as an \n!    extension Entity-Header in a POST request when it knows the \n!    destination of that request is a proxy/gateway to a MIME-compliant \n!    protocol.\n--- 2692,2877 ----\n  \n!    RFC 1521 requires that an Internet mail entity be converted to \n!    canonical form prior to being transferred, as described in Appendix \n!    G of RFC 1521 [5]. Section 3.6.1 of this document describes the \n!    forms allowed for subtypes of the \"text\" media type when \n!    transmitted over HTTP.\n  \n!    RFC 1521 requires that content with a Content-Type of \"text\" \n!    represent line breaks as CRLF and forbids the use of CR or LF \n!    outside of line break sequences. HTTP allows CRLF, bare CR, and \n!    bare LF to indicate a line break within text content when a message \n!    is transmitted over HTTP.\n  \n!    Where it is possible, a proxy or gateway from HTTP to a strict RFC \n!    1521 environment should translate all line breaks within the text \n!    media types described in Section 3.6.1 of this document to the RFC \n!    1521 canonical form of CRLF. Note, however, that this may be \n!    complicated by the presence of a Content-Encoding and by the fact \n!    that HTTP allows the use of some character sets which do not use \n!    octets 13 and 10 to represent CR and LF, as is the case for some \n!    multi-byte character sets.\n  \n! C.2  Conversion of Date Formats\n! \n!    HTTP/1.0 uses a restricted set of date formats (Section 3.3) to \n!    simplify the process of date comparison. Proxies and gateways from \n!    other protocols should ensure that any Date header field present in \n!    a message conforms to one of the HTTP/1.0 formats and rewrite the \n!    date if necessary.\n! \n! C.3  Introduction of Content-Encoding\n! \n!    RFC 1521 does not include any concept equivalent to HTTP/1.0's \n!    Content-Encoding header field. Since this acts as a modifier on the \n!    media type, proxies and gateways from HTTP to MIME-compliant \n!    protocols must either change the value of the Content-Type header \n!    field or decode the Entity-Body before forwarding the message. \n!    (Some experimental applications of Content-Type for Internet mail \n!    have used a media-type parameter of \";conversions=<content-coding>\" \n!    to perform an equivalent function as Content-Encoding. However, \n!    this parameter is not part of RFC 1521.)\n! \n! C.4  No Content-Transfer-Encoding\n! \n!    HTTP does not use the Content-Transfer-Encoding (CTE) field of RFC \n!    1521. Proxies and gateways from MIME-compliant protocols to HTTP \n!    must remove any non-identity CTE (\"quoted-printable\" or \"base64\") \n!    encoding prior to delivering the response message to an HTTP client.\n! \n!    Proxies and gateways from HTTP to MIME-compliant protocols are \n!    responsible for ensuring that the message is in the correct format \n!    and encoding for safe transport on that protocol, where \"safe \n!    transport\" is defined by the limitations of the protocol being \n!    used. Such a proxy or gateway should label the data with an \n!    appropriate Content-Transfer-Encoding if doing so will improve the \n!    likelihood of safe transport over the destination protocol.\n  \n! C.5  HTTP Header Fields in Multipart Body-Parts\n  \n!    In RFC 1521, most header fields in multipart body-parts are \n!    generally ignored unless the field name begins with \"Content-\". In \n!    HTTP/1.0, multipart body-parts may contain any HTTP header fields \n!    which are significant to the meaning of that part.\n  \n\n\n\n", "id": "lists-010-0837988"}, {"subject": "(DNS) consensus wordin", "content": "This sparked no fewer than 16 messages to the mailing list since I posted the\ndraft on Tuesday.\n\nHowever, no one argued with the requirement itself; all comments appeared\nto be related to the implementation of the requirement rather than the\nrequirement itself.  There wasn't even any comment on the wording.\n\nSo I believe consensus on this issue exists, but I am less confident\nabout the wording, as the discussion devolved into a discussion\nof the implementation effort required.\n\nPlease let me know if you believe consensus for this requirement does not\nexist, and please let me know if you believe there is better wording possible.\n- Jim Gettys\n\nP.S.  Note that the discussion indicates that many people would like a better\n(portable) DNS resolver library than exists today.  Might I suggest that\npeople get together and implement it?  It did not sound like a tremendous\namount of work, but long overdue.\n\n\nProposed wording:\n====================\n\nSection 14 (new subsection to Security Considerations):\n\nDNS Spoofing\n------------\n\nHTTP use relies heavily on the Internet's name service (DNS), and is prone to\nthe attacks generally possible via name server attack.\nDNSSEC deployment should improve this situation.\n\nThe current implementation of many HTTP 1.0 client libraries are prone\nto a particular attack, however.\n\nHTTP clients should generally rely on their system's name resolver for\nlookup when contacting a server, rather than caching the result of host \nname lookups.  Many platforms already cache the results of name\nlookups locally, or can (and should) be configured to do so.\n\nIf clients cache the results of name lookups for performance\nreasons, HTTP clients MUST observe the TTL (Time To Live) information \nreported by the name server.\n\nA server could be spoofed when that server's IP address \nchanges if this rule is not observed.  As renumbering is \nexpected to become increasingly common [RFC 1900], \nthis problem will grow.  This requirement reduces this\npotential security vulnerability.\n\nThis requirement also reduces failures observed by users,\nand improves load-balancing behavior of clients for replicated servers\nhiding behind the same DNS name, as often occurs with large loaded \nHTTP servers.\n\nSection 16: (References)\n\n[RFC 1900]\nB. Carpenter, Y. Rekhter, \n<a href=\"http://info.internet.isi.edu:80/in-notes/rfc/files/rfc1900.txt\">\nRenumbering Needs Work</a>. RFC 1900, IAB, February 1996.\nSection 16: (References)\n\n[RFC 1900]\nB. Carpenter, Y. Rekhter, \n<a href=\"http://info.internet.isi.edu:80/in-notes/rfc/files/rfc1900.txt\">\nRenumbering Needs Work</a>. RFC 1900, IAB, February 1996.\n\n------- End of Forwarded Message\n\n\n\n", "id": "lists-010-0872098"}, {"subject": "(HOST) (FULLURL) draft of changes for W.G. review", "content": "To Roy's rewrite of 10.22 Host he circulated a while back (which looks pretty\ngood to me), I've added a few other sentences in other sections, \nand drafted a section to the Changes from HTTP/1.0 appendix.\nThis also resolves the FULLURL issue, so that we have a transition plan\nto full URL's in some future HTTP version.\n\nI believe this issue has been talked to death, so now is the time to\ncomplain about exact wording and/or nits.  We must get this one so that no fool can\nmisunderstand it.\n- Jim Gettys\n\n\nAdd to section 5.1.2 Request-URI\n   To allow for transition to absoluteURI's in all requests in future versions\n   of HTTP, HTTP/1.1 servers must accept the absoluteURI form in requests, \n   even though HTTP/1.1 clients will not likely normally generate them.\n   Later HTTP versions (beyond HTTP 1.1) may require absoluteURI's everywhere,\n   once HTTP 1.0 clients and servers are no longer in service.\n\nChange the sentence:\n   The absoluteURI form is only allowed when the request is being made to\n   a proxy.\nTo:\n   The absoluteURI form is required when the request is being made to\n   a proxy.  The absoluteURI form is only allowed to an origin server\n   if the client knows the server supports HTTP/1.1 or later.\n\n\nAdd the following sentence to Section 8. Method Definitions:\n\n   Note that the Host request-header field (Section 10.22) must accompany\n   HTTP 1.1 requests.\n\nReplace the current 10.22 with the following:\n\n10.22  Host\n\n   The Host request-header field specifies the Internet host and port\n   number of the resource being requested, as obtained from the original\n   http URL (Section 3.2.2) given by the user or referring resource.\n   The Host field value must represent the network location of the\n   origin server or gateway given by the original http URL.  This allows\n   the origin server or gateway to differentiate between\n   internally-ambiguous URLs (such as the root \"/\" URL of a server\n   harboring multiple virtual hostnames).\n\n       Host = \"Host\" \":\" host [ \":\" port ]    ; see Section 3.2.2\n\n   A \"host\" without any trailing port information is equivalent to\n   a value of \"host:80\".  For example, a request on the origin server\n   for <http://www.w3.org/pub/WWW/> must include:\n\n       GET /pub/WWW/ HTTP/1.1\n       Host: www.w3.org\n\n   The Host header field must be included in all HTTP/1.1 request\n   messages on the Internet (i.e., on any message corresponding to a\n   request for an http URL, as described in Section 3.2.2).  If the\n   Host field is not already present, an HTTP/1.1 proxy must add a Host\n   field to the request message prior to forwarding it on the Internet.\n   If an HTTP/1.1 request message lacking a Host header field is\n   received via the Internet by an origin server or gateway, that server\n   must respond with a 400 status code.\n\nNote\n   It is extremely important that HTTP/1.1 client implementations use the \n   Host request-headers and server implementations both accept absoluteURI's\n   and also report errors if Host request-headers are not recieved with \n   HTTP/1.1 requests (see Appendix D.1).\n\n\nAdd to Appendix D (Chagnes from HTTP/1.0:\n\n   D.1 Changes to Simplify Vanity Web Servers and Conserve IP Addresses\n\n   The requirements on clients and servers to support the Host \n   request-header, report an error if the Host request-header is \n   missing from an HTTP/1.1 request (Section 10.22), \n   and accept absolute URI's (Section 5.1.2) are the most important \n   changes from HTTP/1.0.\n\n   In HTTP/1.0 there is a one to one relationship of IP addresses and servers,\n   as there is no other way to disambiguate the server of a request\n   than the IP address of that request.\n   This change to HTTP will allow the Internet, once HTTP/1.0\n   clients and servers are no longer common, to support multiple Web sites \n   from a single IP address, greatly simplifying large operational Web \n   servers sites, where the routing of many IP addresses to a single \n   host has created serious problems.  The Internet will also be \n   able to recover the IP addresses that have been used for this purpose.\n   Given the rate of growth of the Web, and the number of servers already\n   deployed, it is extremely important that implementations of HTTP/1.1 \n   correctly implement these requirements before deployment.\n\n\n\n", "id": "lists-010-0881387"}, {"subject": "Re: (DNS) consensus wordin", "content": "jg@w3.org:\n>\n>This sparked no fewer than 16 messages to the mailing list since I posted\nthe\n>draft on Tuesday.\n>\n>However, no one argued with the requirement itself; all comments appeared\n>to be related to the implementation of the requirement rather than the\n>requirement itself.  There wasn't even any comment on the wording.\n\nI think you are mis-interpreting the responses to your message.\n\nI do not interpret the messages as not arguing with the requirement\nitself.  The first comment was \"It is not as simple as it might\nseem.\".  I think \"it\" was your \"HTTP clients MUST observe the TTL\n(Time To Live) information reported by the name server.\".  The second\ncomment was that writing code for observing this information now would\nbe \"a terrible idea given how quickly and continuously BIND has been\nevolving\".\n\nThe subsequent discussion convinced me that, if we put this MUST in the\nspec, it is very well possible that we require more effort from\nbrowser authors than most can afford to spend, especially given the\nfact that their code may require updating to interface with a more\nsecure DNS system one year from now.\n\nThe result will be creative mis-interpretations of this part of the\nspec in order to achieve compliance with your \"must\", or, even worse,\nbrowser vendors taking a lot of time hacking together low level DNS\ninterface code and then strongly resisting any change at this\ninterface level, so that DNS can't evolve into a more secure system.\n\n[...]\n\n>Please let me know if you believe consensus for this requirement does not\n>exist, \n\nI believe consensus for this requirement does not exist.\n\nI propose that the \"must\" is changed into a \"should\" or \"are strongly\nencouraged to\", and that text is added requiring some reasonable\nthings of clients which can't get TTL.\n\n>and please let me know if you believe there is better wording\n>possible.\n\nIncluded below.\n\n>                                - Jim Gettys\n\n[....]\n>If clients cache the results of name lookups for performance\n>reasons, HTTP clients MUST observe the TTL (Time To Live) information \n>reported by the name server.\n\nChange to:\n\n If clients cache the results of name lookups for performance reasons,\n they are strongly encouraged to get and observe the TTL (Time To\n Live) information reported by the name server.  If this is not\n feasible, clients must not cache the results of name lookups for\n longer than 10 minutes, and must immediately discard a name lookup\n result if a network error occurs when using the result to initiate a\n connection.\n\n\nKoen.\n\n\n\n", "id": "lists-010-0892851"}, {"subject": "Re: (DNS) consensus wordin", "content": "> Change to:\n> \n>  If clients cache the results of name lookups for performance reasons,\n>  they are strongly encouraged to get and observe the TTL (Time To\n>  Live) information reported by the name server.  If this is not\n>  feasible, clients must not cache the results of name lookups for\n>  longer than 10 minutes, and must immediately discard a name lookup\n>  result if a network error occurs when using the result to initiate a\n>  connection.\n\n\nI have some additional ideas. Danzig., et.al (SIGCOMM92 ??) shows that \nNEGATIVE caching DNS will help to reduce DNS traffic quite a bit. \nNegative cache means when DNS lookup fail (may be after 2-3 trails) for\nthat particular host, we should cache the result (the failure) for a short\ntime to avoid subsequent look up (which likely to fail again). Say 2 minutes.\n\nAnawat\n\n\n\n", "id": "lists-010-0903037"}, {"subject": "(INTEGOK) rough consensu", "content": "What follows is text for the (currently TBS) section on Content-MD5\nheader. There are objections to it, but rough consensus is believed to\nexist. The most likely outcome of the objections (IMHO) is that a new\nheader for content integrity and authentication mechanisms will be\nproposed for HTTP/1.2.\n\nIf there are any problems you have with this, please let me know, or\nI will believe enough consensus exists to make add this to the 1.1 draft\nto close this issue.\n\n--------------------\n10.13Content-MD5\n\nThe Content-MD5 entity-header field is an MD5 digest of the entity-body,\nas defined in [RFC 1864], for the purpose of providing an end-to-end\nintegrity check of the entity-body.\n\nContentMD5= \"Content-MD5\" \":\" digest\ndigest= <base64 of 128 bit MD5 digest as per RFC 1864>\n\nThe Content-MD5 header may optionally be generated by origin servers,\nand functions as an integrity check of the entity-body. Only\norigin-servers may generate the Content-MD5 headers: proxies and\ngateways MUST NOT generate it, as this would defeat its value and an\nend-to-end integrity check. Any recipient of the entity-body, including\ngateways and proxies, MAY check that the digest value in the header\nmatches that of the entity-body as received. \n\nWhen being generated, the MD5 digest is computed based on the value of\nthe entity-body after Content-Encoding (if any) but before\nTransfer-Encoding (if any) is applied; when being checked, after\nTransfer-Encoding has been removed, but before Content-Encoding has been\nremoved.\n\nThere are several ways in which the application of Content-MD5 to HTTP\nentity-bodies differs from its application to MIME entity-bodies. One is\nthat HTTP, unlike MIME, does not use Content-Transfer-Encoding, and does\nuse Transfer-Encoding and Content-Encoding. Another is that, unlike\nMIME, the digest is computed over the entire entity-body, even if it\nhappens to be a MIME multi-part content-type. (Note that the multi-part\nbodies may themselves have Content-MD5 headers.) Another is that HTTP\nmore frequently uses binary content types than MIME, so it is worth\nnoting that in such cases, the byte order used to compute the digest is\nnetwork byte order. Lastly, the canonical form of text types in HTTP\nincludes several line break conventions, so conversion to CR-LF is not\nalways required before computing or checking the digest: any acceptable\nconvention should be left unaltered for inclusion in the digest.\n\n>      Note: the net result of the above is that the digest is\n      computed on the content that would be sent over-the-wire, in\n>      network byte order, but prior to any transfer coding being \n>      applied.\n\n\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-0911408"}, {"subject": "CHUNKED: draft change", "content": "Following is draft wording changes to section 3.6 of the draft HTTP 1.1\nspec to allow future extensions to chunked transfer encoding, such as\ndigital signatures, etc. Changes are marked with change bars (manually\ngenerated, so 100% guarantees can not be made...)\n\nIf there are objections, please let me know; otherwise, we will conclude\nthat sufficient consensus exists to close out this issue.\n>--------\n>\n>3.6  Transfer Codings\n>\n>   Transfer coding values are used to indicate an encoding \n>   transformation that has been, can be, or may need to be applied to \n>   an Entity-Body in order to ensure safe transport through the \n>   network. This differs from a content coding in that the transfer \n>   coding is a property of the message, not of the original resource.\n>\n>|      transfer-coding         = \"chunked\" | transfer-extension\n>|      transfer-extension      = token\n>\n>   All transfer-coding values are case-insensitive. HTTP/1.1 uses \n>   transfer coding values in the Transfer-Encoding header field \n>   (Section 10.39).\n>\n>   Transfer codings are analogous to the Content-Transfer-Encoding \n>   values of MIME [7], which were designed to enable safe transport of \n>   binary data over a 7-bit transport service. However, \"safe \n>   transport\" has a different focus for an 8bit-clean transfer \n>   protocol. In HTTP, the only unsafe characteristic of message bodies \n>   is the difficulty in determining the exact body length \n>   (Section 7.2.2), or the desire to encrypt data over a shared \n>   transport.\n>\n> | All HTTP/1.1 applications must be able to receive and decode the \n> | \"chunked\" transfer coding, and ignore chunked extensions they do\n> | not understand. The chunked encoding modifies the body \n>   of a message in order to transfer it as a series of chunks, each \n>   with its own size indicator, followed by an optional footer \n>   containing entity-header fields. This allows dynamically-produced \n>   content to be transferred along with the information necessary for \n>   the recipient to verify that it has received the full message.\n>\n>       Chunked-Body   = *chunk\n>                        \"0\" CRLF\n>                        footer\n>                        CRLF\n>\n>|      chunk          = chunk-size [ chunk-ext ] CRLF\n>                        chunk-data CRLF\n>\n>       chunk-size     = hex-no-zero *HEX\n>|      chunk-ext      = *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] )\n>|      chunk-ext-name = token\n>|      chunk-ext-val  = token | quoted-string\n>       chunk-data     = chunk-size(OCTET)\n>\n>   |      footer         = *<Content-MD5 and future headers that\n>specify\n>|                         they are allowed in footer>\n>\n>       hex-no-zero    = <HEX excluding \"0\">\n>\n>   Note that the chunks are ended by a zero-sized chunk, followed by \n>   the footer and terminated by an empty line. An example process for \n>   decoding a Chunked-Body is presented in Appendix C.5.\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-0922555"}, {"subject": "URLLEN: consensu", "content": "This is a replacement for section 3.2.1 on the syntax of URUs. It adds a\nparagraph to clarify the issue of URL length. (Changes are marked with\nchange bars.)\n\nThis issue was discussed on the list, and this is believed to represent\nthe consensus on this issue. If you believe otherwise, please let me\nknow; otherwise, this issue will be closed.\n-------------------------\n\n3.2.1 General Syntax\n\n   URIs in HTTP can be represented in absolute form or relative to \n   some known base URI [11], depending upon the context of their use. \n   The two forms are differentiated by the fact that absolute URIs \n   always begin with a scheme name followed by a colon.\n\n       URI            = ( absoluteURI | relativeURI ) [ \"#\" fragment ]\n\n       absoluteURI    = scheme \":\" *( uchar | reserved )\n\n       relativeURI    = net_path | abs_path | rel_path\n\n       net_path       = \"//\" net_loc [ abs_path ]\n       abs_path       = \"/\" rel_path\n       rel_path       = [ path ] [ \";\" params ] [ \"?\" query ]\n\n       path           = fsegment *( \"/\" segment )\n       fsegment       = 1*pchar\n       segment        = *pchar\n\n       params         = param *( \";\" param )\n       param          = *( pchar | \"/\" )\n\n       scheme         = 1*( ALPHA | DIGIT | \"+\" | \"-\" | \".\" )\n       net_loc        = *( pchar | \";\" | \"?\" )\n       query          = *( uchar | reserved )\n       fragment       = *( uchar | reserved )\n\n       pchar          = uchar | \":\" | \"@\" | \"&\" | \"=\"\n       uchar          = unreserved | escape\n       unreserved     = ALPHA | DIGIT | safe | extra | national\n\n       escape         = \"%\" HEX HEX\n       reserved       = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\"\n       extra          = \"!\" | \"*\" | \"'\" | \"(\" | \")\" | \",\"\n       safe           = \"$\" | \"-\" | \"_\" | \".\" | \"+\"\n       unsafe         = CTL | SP | <\"> | \"#\" | \"%\" | \"<\" | \">\"\n       national       = <any OCTET excluding ALPHA, DIGIT,\n                        reserved, extra, safe, and unsafe>\n\n   For definitive information on URL syntax and semantics, see RFC \n   1738 [4] and RFC 1808 [11]. The BNF above includes national \n   characters not allowed in valid URLs as specified by RFC 1738, \n   since HTTP servers are not restricted in the set of unreserved \n   characters allowed to represent the rel_path part of addresses, and \n   HTTP proxies may receive requests for URIs not defined by RFC 1738.\n\n|   The HTTP protocol does not place any a-priori limit on the length of\na URI.\n|   Servers MUST be able to handle the URI of any resource they serve,\n|   and SHOULD be able to handle URIs of unbounded length if they\nprovide\n|   GET-based forms that could generate such URIs. A server SHOULD\nreturn\n|   a status code of \n|414 Request-URI Too Large\n|   if a URI is longer than the server can handle.\n|\n|Note:\n|   Servers should be cautious about depending on URI lengths above\n|     255 bytes, because some older client or proxy implementations may\n|   not properly support these.\n|\n|   All client and proxy implementations MUST be able to handle a URI\n|   of any finite length. \n\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-0933208"}, {"subject": "REWRITE: consensu", "content": "Here is a replacement for section 5.1.2, to close the issue of whether\nproxies should rewrite URLs. The only change is the addition of the last\nparagraph (marked with change bars); the rest of the section is here\njust for context, and there may be other changes to it in resolution of\nother issues.\n\nThis issue has been discussed on the list, so it is believed that this\nrepresents the consensus on this issue. If you disagree, please let me\nknow; otherwise we will close this issue.\n-------------------------\n5.1.2 Request-URI\n\n   The Request-URI is a Uniform Resource Identifier (Section 3.2) and \n   identifies the resource upon which to apply the request.\n\n       Request-URI    = \"*\" | absoluteURI | abs_path\n\n   The three options for Request-URI are dependent on the nature of \n   the request. The asterisk \"*\" means that the request does not apply \n   to a particular resource, but to the server itself, and is only \n   allowed when the Method used does not necessarily apply to a \n   resource. One example would be\n\n       OPTIONS * HTTP/1.1\n\n   The absoluteURI form is only allowed when the request is being made \n   to a proxy. The proxy is requested to forward the request and \n   return the response. If the request is GET or HEAD and a prior \n   response is cached, the proxy may use the cached message if it \n   passes any restrictions in the Cache-Control and Expires header \n   fields. Note that the proxy may forward the request on to another \n   proxy or directly to the server specified by the absoluteURI. In \n   order to avoid request loops, a proxy must be able to recognize all \n   of its server names, including any aliases, local variations, and \n   the numeric IP address. An example Request-Line would be:\n\n       GET http://www.w3.org/pub/WWW/TheProject.html HTTP/1.1\n\n   The most common form of Request-URI is that used to identify a \n   resource on an origin server or gateway. In this case, only the \n   absolute path of the URI is transmitted (see Section 3.2.1, \n   abs_path). For example, a client wishing to retrieve the resource \n   above directly from the origin server would create a TCP connection \n   to port 80 of the host \"www.w3.org\" and send the line:\n\n       GET /pub/WWW/TheProject.html HTTP/1.1\n\n   followed by the remainder of the Full-Request. Note that the \n   absolute path cannot be empty; if none is present in the original \n   URI, it must be given as \"/\" (the server root).\n\n   If a proxy receives a request without any path in the Request-URI \n   and the method used is capable of supporting the asterisk form of \n   request, then the last proxy on the request chain must forward the \n   request with \"*\" as the final Request-URI. For example, the request\n\n       OPTIONS http://www.ics.uci.edu:8001 HTTP/1.1\n\n   would be forwarded by the proxy as\n\n       OPTIONS * HTTP/1.1\n\n   after connecting to port 8001 of host \"www.ics.uci.edu\".\n\n   The Request-URI is transmitted as an encoded string, where some \n   characters may be escaped using the \"% hex hex\" encoding defined by \n   RFC 1738 [4]. The origin server must decode the Request-URI in \n   order to properly interpret the request.\n\n|   In requests that they forward, proxies MUST NOT rewrite the\n\"abs_path\"\n|   part of a Request-URI in any way except as noted above to replace\n|   a null abs_path with \"*\". Illegal Request-URIs should be responded\nto with\n|   an appropriate status code. (Proxies may canonicalize the\nRequest-URI,\n|   according to the canonicalization rules in section 3.2.2, for\ninternal\n|   processing puposes, e.g., for comparison of cache keys when doing\ncache\n|   lookups or updates, but should not use this form in forwarded\nrequests.)\n|   The main reason for this rule is to make sure that the form of\nRequest-URIs\n|   is well specified, to enable future extensions without fear that\nthey will\n|   break in the face of some rewritings. Another is that one\nconsequence of\n|   rewriting the Request-URI is that integrity or authentication checks\nby the\n|   server may fail; since rewriting must be avoided in this case, it\nmay as\n|   well be proscribed in general. Note: servers writers should be aware\nthat\n|   some existing proxies do some rewriting.\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-0944716"}, {"subject": "CONTENTMD5: duplicat", "content": "This is the same as the INTEGOK issue -- see previous post with the\nproposed resolution.\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-0956514"}, {"subject": "ENDHEAD: duplicates CHUNKE", "content": "The ENDHEAD issue duplicates the CHUNKED issue; see previous post on\nthat topic for the resolution.\n\n(If you disagree that it is a duplicate, please let me know; otherwise\nwe will close this issue.)\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-0964076"}, {"subject": "Re: (DNS) consensus wordin", "content": "On Sat, 30 Mar 1996, Koen Holtman wrote:\n\n[...]\n> >and please let me know if you believe there is better wording\n> >possible.\n> \n> Included below.\n> \n> >                                - Jim Gettys\n> \n> [....]\n> >If clients cache the results of name lookups for performance\n> >reasons, HTTP clients MUST observe the TTL (Time To Live) information \n> >reported by the name server.\n> \n> Change to:\n> \n>  If clients cache the results of name lookups for performance reasons,\n>  they are strongly encouraged to get and observe the TTL (Time To\n>  Live) information reported by the name server.  If this is not\n>  feasible, clients must not cache the results of name lookups for\n>  longer than 10 minutes, and must immediately discard a name lookup\n>  result if a network error occurs when using the result to initiate a\n>  connection.\n\nIt is my understanding that MUST and SHOULD are defined terms and\nstrongly encouraged is not as far as RFCs are concerned. Thus, I\noffer the following editorial alternative to Koen's suggestion (which\nI endorse):\n\n  If a client caches the result of a DNS lookups, it should observe the\n  TTL (Time To Live) reported by the DNS server. If the TTL value is\n  not available, the client must not cache the result of a DNS lookup\n  for longer than XX minutes. In either case, the client must immediately \n  discard a name lookup result if a network error occurs when using the \n  result to initiate a connection.\n\nRationale for other changes:\n1. I believe this paragraph is about DNS name lookups and should be\n   specific\n2. We don't care what the motivation is for the caching\n3. I'm not sure that 10 minutes is the right number ... my IPSs tell me\n   that 24 hours must be allowed for DNS change propigation. Given\n   rational expectation for rate of change of the value, I would prefer\n   a larger number ... or if we have a DNS expert, perhaps there is\n   a DNS defined default TTL for cases where not is specified.\n   I can live with the 10 minutes but it was a detail which I felt should\n   surface for expert comment.\n\nDave Morris\n\n\n\n", "id": "lists-010-0971827"}, {"subject": "Re: (DNS) consensus wordin", "content": "On Sat, 30 Mar 1996, Anawat Chankhunthod wrote:\n\n> I have some additional ideas. Danzig., et.al (SIGCOMM92 ??) shows that \n> NEGATIVE caching DNS will help to reduce DNS traffic quite a bit. \n> Negative cache means when DNS lookup fail (may be after 2-3 trails) for\n> that particular host, we should cache the result (the failure) for a short\n> time to avoid subsequent look up (which likely to fail again). Say 2 minutes.\n\nWhile this is a useful suggestion, it doesn't fall in the domain of being\na security issue. It might be considered for an HTTP implementation \nrecommendations document.\n\nDave Morris\n\n\n\n", "id": "lists-010-0981833"}, {"subject": "Re: URLLEN: consensu", "content": "On Sun, 31 Mar 1996, Paul Leach wrote:\n\n> This is a replacement for section 3.2.1 on the syntax of URUs. It adds a\n> paragraph to clarify the issue of URL length. (Changes are marked with\n> change bars.)\n> \n> This issue was discussed on the list, and this is believed to represent\n> the consensus on this issue. If you believe otherwise, please let me\n> know; otherwise, this issue will be closed.\n\nThe \"+\" is incorrect ... it is reserved in 1.0 (see JG's latest diff)\nand must be either reserved or unsafe.  It isn't 'safe'. Of the three\nwho last debated this point on the list it was two for reserved and one\n(me) for unsafe.\n\n\n> -------------------------\n> \n> 3.2.1 General Syntax\n> \n>    URIs in HTTP can be represented in absolute form or relative to \n>    some known base URI [11], depending upon the context of their use. \n>    The two forms are differentiated by the fact that absolute URIs \n>    always begin with a scheme name followed by a colon.\n> \n>        URI            = ( absoluteURI | relativeURI ) [ \"#\" fragment ]\n> \n>        absoluteURI    = scheme \":\" *( uchar | reserved )\n> \n>        relativeURI    = net_path | abs_path | rel_path\n> \n>        net_path       = \"//\" net_loc [ abs_path ]\n>        abs_path       = \"/\" rel_path\n>        rel_path       = [ path ] [ \";\" params ] [ \"?\" query ]\n> \n>        path           = fsegment *( \"/\" segment )\n>        fsegment       = 1*pchar\n>        segment        = *pchar\n> \n>        params         = param *( \";\" param )\n>        param          = *( pchar | \"/\" )\n> \n>        scheme         = 1*( ALPHA | DIGIT | \"+\" | \"-\" | \".\" )\n>        net_loc        = *( pchar | \";\" | \"?\" )\n>        query          = *( uchar | reserved )\n>        fragment       = *( uchar | reserved )\n> \n>        pchar          = uchar | \":\" | \"@\" | \"&\" | \"=\"\n>        uchar          = unreserved | escape\n>        unreserved     = ALPHA | DIGIT | safe | extra | national\n> \n>        escape         = \"%\" HEX HEX\n>        reserved       = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\"\n>        extra          = \"!\" | \"*\" | \"'\" | \"(\" | \")\" | \",\"\n>        safe           = \"$\" | \"-\" | \"_\" | \".\" | \"+\"\n>        unsafe         = CTL | SP | <\"> | \"#\" | \"%\" | \"<\" | \">\"\n>        national       = <any OCTET excluding ALPHA, DIGIT,\n>                         reserved, extra, safe, and unsafe>\n> \n>    For definitive information on URL syntax and semantics, see RFC \n>    1738 [4] and RFC 1808 [11]. The BNF above includes national \n>    characters not allowed in valid URLs as specified by RFC 1738, \n>    since HTTP servers are not restricted in the set of unreserved \n>    characters allowed to represent the rel_path part of addresses, and \n>    HTTP proxies may receive requests for URIs not defined by RFC 1738.\n> \n> |   The HTTP protocol does not place any a-priori limit on the length of\n> a URI.\n> |   Servers MUST be able to handle the URI of any resource they serve,\n> |   and SHOULD be able to handle URIs of unbounded length if they\n> provide\n> |   GET-based forms that could generate such URIs. A server SHOULD\n> return\n> |   a status code of \n> |414 Request-URI Too Large\n> |   if a URI is longer than the server can handle.\n> |\n> |Note:\n> |   Servers should be cautious about depending on URI lengths above\n> |     255 bytes, because some older client or proxy implementations may\n> |   not properly support these.\n> |\n> |   All client and proxy implementations MUST be able to handle a URI\n> |   of any finite length. \n> \n> \n> ----------------------------------------------------\n> Paul J. Leach            Email: paulle@microsoft.com\n> Microsoft                Phone: 1-206-882-8080\n> 1 Microsoft Way          Fax:   1-206-936-7329\n> Redmond, WA 98052\n> \n> \n\n\n\n", "id": "lists-010-0990109"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": ">>        It might be better, then, if Roy and Koen finished their\n>>discussion of what we have term the \"rel=source alternative\",\n> \n> I too have been waiting for Roy (or Larry) to comment on my objections\n> to rel=source.  I have been thinking about writing a `safe: yes'\n> mini-ID in case nobody else volunteers, but would like to see the\n> `rel=source' issue settled first.\n\nYour objection was that returning a safe URL for the redo, as opposed\nto an indication that the form data is safe to be reposted, is too much\nof a burden on CGI authors.\n\nMy response is that I don't care -- CGI authors will need to do at least\nthat much work, if not more, to ensure that the POST action is indeed\nsafely repost-able, and having them actually decode the semantics of\nthe original request is the best way to ensure that.  Lazy CGI developers\ncreate security holes and protocol meltdown, so I have no desire to\ncoddle to their needs.  People who don't want to do the work can just\nlive with the annoying \"do you really want to repost this form?\" query --\nthere is no loss of functionality.  The side benefit is the definition\nand implementation of various forms of Link, which is something we need\nfor almost every major enhancement currently under discussion for the Web\n(collaboration, annotation, link maintenance, style sheets, ...).\n\nI have no interest in discussing any lesser issue -- my only purpose in\ndiscussing this one was to inform folks on the variety of ways that\nwere already defined to do the same thing.  Once you know what options\nare available, there's not much else I can contribute to the discussion.\n\nI don't see any reason to standardize something that is no more than a\nmidnight hack, at least until someone convinces the midnight hackers\nout there to implement it.  Either way, it isn't something I'm willing\nto spend time on, nor is it necessary for me, or even the IETF, to approve\nof something before it is implemented in practice.\n\nCheers,\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-10003408"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "On Wed, 9 Oct 1996, Koen Holtman wrote:\n\n> I personally have little interest in taking the work beyond a `safe:\n> yes' mini-ID.  I don't think a bookmarking standard would create much\n> added value.\n\nAfter some reflection, I realized that this concern might be in the\ndomain of User Agent Hints as a more general category. So for starters,\nI'd suggest;\n   UA-Hints: safe=[yes|no]\nAnd then I'd propose adding some server requests re history list\nhandling:\n         history=no : This response SHOULD NOT be included in a UA\n                      history list.\n         history=noredo:  Show the current response in the history list\n                      w/o ever reperforming the request. RELOAD/REFRESH\n                      by the user follows safe= semantics.\n         history=refresh: Never reshow the current response, rather \n                      repeat the request or at least check it.\n         history=replace: The page from which the current response was\n                      requested is replaced. This is similar to =no except\n                      that =no causes the next page to replace while\n                      =replace allows the current response to replace the\n                      previous page\n\nThe default would be the current haphazzard muddle implemented by the \nindividual UAs.\n\nThe control these options (including safe=) would greatly improve the\nusability of three complex WWW applications I've assisted in the \ndevelopment of.\n\nDave Morris\n\n\n\n", "id": "lists-010-10014054"}, {"subject": "contentdisposition heade", "content": "Should content-disposition from RFC 1806 be considered for HTTP?\n\n------- Start of forwarded message -------\nDate: Tue, 8 Oct 1996 04:38:10 PDT\nFrom: Michael Naumann - ESO Garching +49 89 32006 430 <mnaumann@eso.org>\nOrganization: European Southern Observatory, Garching\nTo: www-talk@w3.org\nSubject: Re: [Q] Changing default name in a FORM\nReferences: <199610080948.LAA26948@beatles.cselt.stet.it>\n\nMaurizio Codogno wrote:\n> I have a script which selects a file and send it to the user. It\n> is accessed via a form, with action POST.\n> Unfortunately, the file is sent with the form's name as default name,\n> and the user should rename it by hand.\n> \n> Does HTTP/1.0 have any header line which can be used to overcome it?\n\nTry sending \n\n  Content-type:        application/octet-stream\n  Content-Disposition: attachment; filename=\"fname.ext\"\n\nHTTP header fields in the response of your script. This\nat least works with Netscape Navigator since version 2.0\n\nAlthough not specified in the HTTP 1.0/1.1 drafts the\nthe definition of Content-Disposition: can be found in \nhttp://www.internic.net/rfc/rfc1806.txt\n\n<michael\n\n-- \n** Michael Naumann * ESO * mnaumann@eso.org \n** http://www.eso.org/~mnaumann/\n\n\n------- End of forwarded message -------\n\n\n\n", "id": "lists-010-10022990"}, {"subject": "Re: (DNS) consensus wordin", "content": "David W. Morris:\n>\n>It is my understanding that MUST and SHOULD are defined terms and\n>strongly encouraged is not as far as RFCs are concerned.\n\nYes.  `strongly encouraged' seems to be a HTTP invention.  I don't see\nthis as a real problem, but it you would rather see SHOULD, that is\nfine with me.\n\n> Thus, I\n>offer the following editorial alternative to Koen's suggestion (which\n>I endorse):\n>\n>  If a client caches the result of a DNS lookups, it should observe the\n>  TTL (Time To Live) reported by the DNS server. If the TTL value is\n>  not available, the client must not cache the result of a DNS lookup\n>  for longer than XX minutes. In either case, the client must immediately \n>  discard a name lookup result if a network error occurs when using the \n>  result to initiate a connection.\n\nYour version looks OK to me.  \n\nI have no real rationale for my XX=10 minutes, other than that it\nseemed like a good enough XX to allow load balancing between mirrors\nthrough dynamic DNS remapping.  Too high a value for XX (like the DNS\nworst case default, if such a thing exists) would not sufficiently\nencourage client authors in making code to get the real TTL.\n\n>Dave Morris\n\nKoen.\n\n\n\n", "id": "lists-010-1002458"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "Roy T. Fielding:\n>\n  [Koen:]\n>>  I have been thinking about writing a `safe: yes'\n>> mini-ID in case nobody else volunteers, but would like to see the\n>> `rel=source' issue settled first.\n>\n>Your objection was that returning a safe URL for the redo, as opposed\n>to an indication that the form data is safe to be reposted, is too much\n>of a burden on CGI authors.\n\nYes, that was my main objection.\n\n>My response is that I don't care -- CGI authors will need to do at least\n>that much work, if not more, to ensure that the POST action is indeed\n>safely repost-able,\n\nNonsense.  Many POST actions are safe by nature, and if POST is to be\nused for i18n, many more will be.  Examples are POSTs to search\nengines and to HTML checkers with a forms interface.\n\n[....]\n>The side benefit is the definition\n>and implementation of various forms of Link, which is something we need\n>for almost every major enhancement currently under discussion for the Web\n>(collaboration, annotation, link maintenance, style sheets, ...).\n\nI care little about the side benefits on the status of Link.  I want\nsomething which promotes i18n, and `safe: yes' is better at that.\n\n>I have no interest in discussing any lesser issue \n\nI don't think i18n is a lesser issue.\n\n>-- my only purpose in\n>discussing this one was to inform folks on the variety of ways that\n>were already defined to do the same thing.\n\nSo tell me where to look for the existing definition of `rel=source',\nlike I asked.  I might even copy it into the `safe: yes' ID.\n\n[....]\n>I don't see any reason to standardize something that is no more than a\n>midnight hack, at least until someone convinces the midnight hackers\n>out there to implement it.  Either way, it isn't something I'm willing\n>to spend time on, nor is it necessary for me, or even the IETF, to approve\n>of something before it is implemented in practice.\n\n_Some_ headers can be deployed without IETF action, but `safe: yes' is\nnot one of them.  As long as `safe: yes' is not approved by the IETF,\nany browser which uses it as a license to treat a POST as safe would\nbe clearly non-conformant to the 1.1 protocol.\n\n>Cheers,\n>\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-10033029"}, {"subject": "CLOSURE on REPOST/Idempotent/RedoSafe/Saf", "content": "If we waited until everybody had a 'clear role', we'd never get\nanything done.  An \"Internet Draft\" is just a working document, and\nthe only reason for having one is so that we can have a discussion\nwhere the referent in the discussion is concrete.  Too often there's a\nlong mail interchange where someone makes a proposal and then changes\nit 10 times during the discussion, and then someone else says \"I\nreally like it\" and other people don't know which 'it' is really the\nfinal 'it'.  If something is going to happen on this, *someone* must\nwrite an Internet draft describing what that someone wants to have\nhappen.\n\nWe've gone on about this for a long time. I would like to declare this\ntopic closed until there is a volunteer to write up and submit an\ninternet draft. If you would like to volunteer, let me know.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-10043497"}, {"subject": "Re: REPOS", "content": "> I was however wondering if any addition to 1.1 should be lumped together\n> and put in waiting list for 1.2 , or there is some shortcut, something\n> like tables' RFC in HTML. I know that probably it is more a political\n> issue, but for \"simple\" modifications a preferential route would be\n> great.\n\nRemember when we had subcommittees, many of the subgroups wrote an\ninternet draft that described their conclusions. Most of those IDs\nwere then merged into the main document.\n\nI think we need to decide on the 'editorial' issues around the next\nversion of the HTTP specification (including how many documents we\nhave) later. In the meanwhile, though, I think ANY proposed change or\naddition to HTTP must appear in SOME Internet-draft.\n\nIf there are really separable issues, it's probably better to deal\nwith them as separate drafts. An Internet draft does not need a lot of\nboilerplate, and can be very short. It just needs to be written in a\nway that the idea stands on its own outside of the context of the\ntorrent of mail.\n\nI've gotten feedback from many quarters that we should not try to move\ntoo quickly on HTTP/1.2 until there's more experience with HTTP/1.1.\nThis is influencing my thoughts on the schedule.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-10051494"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "koen@win.tue.nl (Koen Holtman) wrote:\n>Roy T. Fielding:\n>>[...]\n>>My response is that I don't care -- CGI authors will need to do at least\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>>that much work, if not more, to ensure that the POST action is indeed\n>>safely repost-able,\n>\n>Nonsense.  Many POST actions are safe by nature, and if POST is to be\n !!!!!!!!\n>used for i18n, many more will be.  Examples are POSTs to search\n>engines and to HTML checkers with a forms interface.\n\n\n>>I have no interest in discussing any lesser issue \n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^     ^^^^^^^^^^^^\n>I don't think i18n is a lesser issue.\n\n\n>>were already defined to do the same thing.\n>\n>So tell me where to look for the existing definition of `rel=source',\n>like I asked.  I might even copy it into the `safe: yes' ID.\n>\n>[....]\n>>I don't see any reason to standardize something that is no more than a\n                                                          ^^^^^^^^^^^^^^\n>>midnight hack, at least until someone convinces the midnight hackers\n  ^^^^^^^^^^^^^\n>>out there to implement it.  Either way, it isn't something I'm willing\n>>to spend time on, nor is it necessary for me, or even the IETF, to approve\n>>of something before it is implemented in practice.\n>\n>_Some_ headers can be deployed without IETF action, but `safe: yes' is\n>not one of them.  As long as `safe: yes' is not approved by the IETF,\n>any browser which uses it as a license to treat a POST as safe would\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>be clearly non-conformant to the 1.1 protocol.\n\nYour last point, Koen, is the bottom line.  If Roy refuses\nto make clear how his intellectual midnight hack could actually be\nmade to work in the real world, and no header like Safe is added to\nthe Standard, then even the vendors who don't really care about IETF\nstandards may be reluctant to deploy because of possible legal\nrepercussions.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-10059846"}, {"subject": "Re: contentdisposition heade", "content": "> Should content-disposition from RFC 1806 be considered for HTTP?\n\nIm alredy using it in some HTTP bulk transfer applications, and it\ncertainly seems to work. the inline/attachment distinction is a bit\nmeaningless, but it's a nice standard way to send filename and other\ninfo.\n\nIm not on http-wg, so please CC me if there's any new params proposed\nor the like that would make it HTTP-friendly.\n\nIve been hideously delinquent in cutting the next version of the RFC;\nIll get off my posterior now.\n\n\n-Rens\n\n\n\n", "id": "lists-010-10070368"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "koen@win.tue.nl (Koen Holtman) wrote:\n>I personally have little interest in taking the work beyond a `safe:\n>yes' mini-ID.  I don't think a bookmarking standard would create much\n>added value.\n\nI wasn't referring to the bookmark file structure, but to\nstandards regulating privacy and security issues, beyond safety, per\nse, if a server saved the post-content for access via a network URL,\nas the \"rel=source alternative\" would appear to require, if it could\nbe made workable at all.  This wouldn't be necessary if you contribute\na mini-ID just for the Safe: yes | no header, as I hope you do.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-10078481"}, {"subject": "Fwd: Discussion of new object-oriented binary wire protocol for HTTPN", "content": "Larry asked me to forward a copy to this list...\n\nBill\n---------- Forwarded message begins here ----------\n\nDate: Wed, 9 Oct 1996 15:45:03 PDT\nFrom: Bill Janssen <janssen@parc.xerox.com>\nSubject: Discussion of new object-oriented binary wire protocol for HTTP-NG\n\nI've been encouraged by both the OMG and the W3C to start considering\nwhat a distributed object wire protocol might look like, if it was to\nsupport an object-oriented variant of HTTP, whatever that might mean.\nI intend to post a straw-man document describing such a wire protocol\nto the HTTP-NG mailing list next week some time, with some initial\nthinking in order to start a discussion about this topic.\n\nIf you're interested in participating in such a discussion, you should\nread the archive of the HTTP-NG list at \n\n   http://sunsite.unc.edu/ses/http-ng-archive/\n\nAlso, explore the documents about HTTP-NG thinking so far, at\n\n   http://www.w3.org/pub/WWW/Protocols/HTTP-NG/\n\nIn particular, read about the MUX protocol\n\n   http://www.w3.org/pub/WWW/Protocols/HTTP-NG/mux/simplemux.html\n\nas session state established with such a protocol will be important in\nthe design of an efficient wire protocol.\n\nYou might also download and build ILU, the distributed object system\nfrom PARC, which has already been used in some of the HTTP-NG\nexperiments, and which I propose to continue using for prototyping\nand experimentation:\n\n   ftp://ftp.parc.xerox.com/pub/ilu/ilu.html\n\nFinally, to subscribe to http-ng, send the following in the body (not\nthe subject line) of an email message to \"Majordomo@tipper.oit.unc.edu\":\n\n   subscribe http-ng\n\nThis will subscribe the account from which you send the message to\nthe http-ng list.\n\nIf you wish to subscribe another address instead (such as a local\nredistribution list), you can use a command of the form:\n\n   subscribe http-ng other-address@your_site.your_net\n\nBill\n-----------\n Bill Janssen  <janssen@parc.xerox.com> (415) 812-4763  FAX: (415) 812-4777\n Xerox Palo Alto Research Center, 3333 Coyote Hill Rd, Palo Alto, CA  94304\n URL:  ftp://ftp.parc.xerox.com/pub/ilu/misc/janssen.html\n\n\n\n", "id": "lists-010-10086837"}, {"subject": "draft-holtman-http-safe00.tx", "content": "Larry requested an internet draft to progress the\nREPOST/Idempotent/Redo-Safe/Safe issue, so I made one.\n\nHappy reading!\n\nKoen.\n\n----snip---\n\nHTTP Working Group                                     Koen Holtman, TUE\nInternet-Draft\nExpires: April 10, 1997                                 October 10, 1996\n\n\n                       The Safe Response Header\n\n                     draft-holtman-http-safe-00.txt\n\n\nSTATUS OF THIS MEMO\n\n        This document is an Internet-Draft. Internet-Drafts are\n        working documents of the Internet Engineering Task Force\n        (IETF), its areas, and its working groups. Note that other\n        groups may also distribute working documents as\n        Internet-Drafts.\n\n        Internet-Drafts are draft documents valid for a maximum of\n        six months and may be updated, replaced, or obsoleted by\n        other documents at any time. It is inappropriate to use\n        Internet-Drafts as reference material or to cite them other\n        than as \"work in progress\".\n\n        To learn the current status of any Internet-Draft, please\n        check the \"1id-abstracts.txt\" listing contained in the\n        Internet-Drafts Shadow Directories on ftp.is.co.za\n        (Africa), nic.nordu.net (Europe), munnari.oz.au (Pacific\n        Rim), ds.internic.net (US East Coast), or ftp.isi.edu (US\n        West Coast).\n\n        Distribution of this document is unlimited.  Please send\n        comments to the HTTP working group at\n        <http-wg@cuckoo.hpl.hp.com>.  Discussions of the working\n        group are archived at\n        <URL:http://www.ics.uci.edu/pub/ietf/http/>.  General\n        discussions about HTTP and the applications which use HTTP\n        should take place on the <www-talk@w3.org> mailing list.\n\nABSTRACT\n\n   This document proposes a HTTP response header called Safe, which\n   can be used to label the corresponding POST request as being safe.\n   This labeling will allow user agents to present services which use\n   safe POSTs in a more user-friendly way.  Improving the\n   user-friendliness of safe POSTs is considered important, because\n   web internationalization will depend for a large part on the use of\n   safe POSTs.\n\n\n1  Introduction\n\n This document proposes a HTTP response header called Safe, which can\n be used to label the corresponding POST request as being safe.  This\n labeling will allow user agents to present services which use safe\n POSTs in a more user-friendly way.  Improving the user-friendliness\n of safe POSTs is considered important, because web\n internationalization will depend for a large part on the use of safe\n POSTs.\n\n\n2 Background\n\n According to Section 9.1.1 (Safe Methods) of the HTTP/1.1 draft\n specification [1], POST requests are assumed to be `unsafe' by\n default.  `Unsafe' means `causes side effects for which the user will\n be held accountable'.\n\n If the POST request is unsafe, explicit user confirmation is\n necessary before the request is repeated.  User agents will repeat\n POST requests when the user presses the RELOAD button while a POST\n result is displayed, or when the history function is used to\n redisplay a POST result which is no longer in the history buffer.\n\n The necessary confirmation dialog often takes the form of a `repost\n form data?'  dialog box.  The dialog is confusing to many users, and\n slows down navigation in any case.\n\n In theory, if the repeated POST request is safe, the user-unfriendly\n confirmation dialog can be omitted.  However, up till now, HTTP has\n no mechanism by which agents can tell if POST requests are safe.\n This proposal adds such a mechanism.\n\n Many content authors have managed to avoid the confirmation dialog\n problem by using GETs for form submission instead of safe POSTs.\n However, this escape is not possible for forms\n\n    a) which are (sometimes) used to submit large amounts of data\n    b) which are (sometimes) used to submit data in a charset other\n       than ISO-8859-1.\n\n Case b) will be the increasingly common; web internationalization [2]\n makes it necessary to use the POST method for form submission.\n\n It is therefore considered important to eliminate the unnecessary\n confirmation dialogs for safe POSTs as soon as possible.  They are a\n counter-incentive to the upgrading of GET based forms services (like\n search engines) to internationalized POST based forms services.\n\n\n3 The Safe response header\n\n This header is proposed as an addition to the HTTP/1.x suite.\n\n The Safe response header field indicates whether the corresponding\n request is safe in the sense of Section 9.1.1 (Safe Methods) of the\n HTTP/1.1 draft specification [1].\n\n   Safe        = \"Safe\" \":\" safe-nature\n   safe-nature = \"yes\" | \"no\"\n\n An example of the field is:\n\n       Safe: yes\n\n If the Safe header is absent, the corresponding request must be\n considered unsafe, unless it is a GET or HEAD request.  GET and HEAD\n requests are safe by definition, user agents must ignore a `Safe: no'\n header field in GET and HEAD responses.\n\n    Note: User agents can use the received information about safety\n    when repeating an earlier request.  If the request is known to be\n    safe, it can be silently repeated without asking for user\n    confirmation.\n\n\n4 Smooth upgrade path\n\n That the Safe header provides a smooth upgrade path; if a service\n switches from GETs to safe POSTs, existing browsers will still be\n able to access the service.  Its use requires little re-coding effort\n for service authors and user agent authors, and is optional in any\n case.\n\n\n5 About syntax\n\n This document mainly intends to recommend a _mechanism_; the syntax\n of the corresponding header is considered less important.  One\n alternative to the addition of a Safe header would be the addition of\n a `safe' response directive to the Cache-Control header.\n\n\n6 Security considerations\n\n This proposal adds no new HTTP security considerations.\n\n\n7 References\n\n   [1] Fielding et al, Hypertext Transfer Protocol -- HTTP/1.1.\n       Internet-Draft draft-ietf-http-v11-spec-07.txt, HTTP Working\n       Group, August 12, 1996\n\n   [2] Yergeau et al, Internationalization of the Hypertext Markup\n       Language, Internet-Draft draft-ietf-html-i18n-05.txt, Network\n       Working Group, August 7, 1996\n\n\n8 Author's address\n\n   Koen Holtman\n   Technische Universiteit Eindhoven\n   Postbus 513\n   Kamer HG 6.57\n   5600 MB Eindhoven (The Netherlands)\n   Email: koen@win.tue.nl\n\n\nExpires: April 10, 1997\n\n\n\n", "id": "lists-010-10096852"}, {"subject": "UPGRADE: Wordin", "content": "I looked through the mailing list archives and found no reference to\nthe upgrade header other than Larry's remark whether it should be\nincluded or not.\n\nI think that Roy's wording is a good start, but it has two important\nlimitations that should be resolved:\n\n1) It allows for switching protocols on the _same_ connection only. I\nthink it is important that the header allows for upgrade on a\ndifferent connection in order to provide sufficient support for having\nHTTP being a control connection for real time protocols, for example.\n\n2) The naming of the protocol may not be sufficiently detailed as it\ndoes not allow for composite names and versions built together in a\nprotocol stack.\n\n3) The client can not require that a protocol specified in the upgrade\nheader is used by the server.\n\nThere are a number of possible solutions to this. First, we could use\nthe notion from Simon Spero's original draft on HTTP-NG that also\nsupports spawning off other connections or we could use the more\ntraditional FTP solution for PORT and PASV. Second we could use a\nprotocol naming scheme as used by ILU (32 bit hash) or as suggested by\nRohit Khare in his PEP proposal.\n\nHowever, as a result of the tight time constraint on the HTTP draft, I\nthink it is more important to provide a working solution than a\ncomplete technical solution. As the upgrade header provides an\nimportant transition path within HTTP itself to change the protocol\nversion, the best solution for now is to keep the upgrade header as it\nis but with a comment on the limitations above and then work on a\nbetter solution for the next HTTP version. I have therefore added the comments \nat the end (marked with change bars)\n\n9.1 Informational 1xx\n\nThis class of status code indicates a provisional response, consisting\nonly of the Status-Line and optional headers, and is terminated by an\nempty line. Since HTTP/1.0 did not define any 1xx status codes,\nservers should not send a 1xx response to an HTTP/1.0 client except\nunder experimental conditions.\n\n100 Continue\n\nThe client may continue with its request. This interim response is\nused to inform the client that the initial part of the request has\nbeen received and has not yet been rejected by the server. The client\nshould continue by sending the remainder of the request or, if the\nrequest has already been completed, ignore this response. The server\nmust send a final response after the request has been completed.\n\n101 Switching Protocols\n\nThe server understands and is willing to comply with the client's\nrequest, via the Upgrade message header field (Section 10.41), for a\nchange in the application protocol being used on this connection. The\nserver will switch protocols to those defined by the response's\nUpgrade header field immediately after the empty line which terminates\nthe 101 response.\n\nThe protocol should only be switched when it is advantageous to do\nso. For example, switching to a newer version of HTTP is advantageous\nover older versions, and switching to a real-time, synchronous\nprotocol may be advantageous when delivering resources that use such\nfeatures.\n\n------\n\n10.41 Upgrade\n\nThe Upgrade general-header allows the client to specify what\nadditional communication protocols it supports and would like to use\nif the server finds it appropriate to switch protocols. The server\nmust use the Upgrade header field within a 101 (switching protocols)\nresponse to indicate which protocol(s) are being switched.\n\n       Upgrade        = \"Upgrade\" \":\" 1#product\n\nFor example, \n\n       Upgrade: HTTP/2.0, SHTTP/1.3, IRC/6.9, RTA/x11\n\n| The purpose of the Upgrade header is to allow easier migration across \n| protocols in order to better match the application needs with \n| protocol capabilities. The client can not demand that a protocol \n| specified in the upgrade header is used by the server. However, the \n| indication given by the upgrade header field should be followed.\n|\n| The upgrade header does not allow for switching protocols on a \n| different connection than the one in use. It also does not provide \n| any means for switching back to the original protocol used to \n| transmit the upgrade header in the first place.\n|\n| This specification does not define any protocol names other than \n| \"HTTP\" but others can be used.\n\n\n-- \n\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld-Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-1011030"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "This issue has nothing whatsoever to do with i18n.  i18n does not\n*need* to use POST requests any more than non-i18n activities need\nto use POST requests, and if you folks would stop knee-jerking every time\nthe subject was brought up you might get a little more accomplished.\n\nBoth Link and rel=source are very old (pre-1992) and well understood.\nWhat they lack is a complete specification, and even that is only because\npeople become preoccupied with midnight hacks instead of long-term design.\nIf you want to standardize a midnight hack, go ahead, but stop insisting\nthat I respond to your questions about it.  If you want to standardize Link,\nthen I will help.\n\n> Your last point, Koen, is the bottom line.  If Roy refuses\n> to make clear how his intellectual midnight hack could actually be\n> made to work in the real world, and no header like Safe is added to\n> the Standard, then even the vendors who don't really care about IETF\n> standards may be reluctant to deploy because of possible legal\n> repercussions.\n\nAs I said before, I see no need for it because there is no *problem*.\nGetting a warning message about reposting a POST is an inconvenience,\nnot a problem.  Being unable to bookmark POST input-data is a user agent\nimplementation issue, not an HTTP problem (and can be solved via Link\nor usage of 303).\n\nAs for refusing \"to make clear\" how to do it, that's your problem.\nTake the specifications and design notes already provided by TimBL\nand two IETF WG's (HTML and HTTP), write down the solution, implement it,\nand then standardize the implementation.  If I had the time to do it,\nthe work would have already been done.  I don't have that time.\n\nSo, like Larry said, this discussion should be closed until someone\ndoes the work necessary to define and solve the problem in the form\nof an ID.  How much work to put into it, and the scope of the problem\nthat is actually solved, is up to you.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-10112552"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "Clarification ...\n\n>  Many content authors have managed to avoid the confirmation dialog\n>  problem by using GETs for form submission instead of safe POSTs.\n>  However, this escape is not possible for forms\n> \n>     a) which are (sometimes) used to submit large amounts of data\n>     b) which are (sometimes) used to submit data in a charset other\n>        than ISO-8859-1.\n> \n>  Case b) will be the increasingly common; web internationalization [2]\n>  makes it necessary to use the POST method for form submission.\n\nThis is not true.  The only time that web internationalization might\nimpact the choice of POST vs GET is when it is not known what the input\ncharacter set will be *and* it is possible for the user agent to submit\ndata in some character set other than what is expected *and* the form\ndoes not contain an entry box for the user to select which particular\ncharacter set they are using *and* nobody has convinced the browser\ncommunity to include a standard hidden form field containing the charset\nwhenever the charset is not the same as that of the output form.\n\nIn other words, i18n has nothing to do with it.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-10122724"}, {"subject": "Re: contentdisposition heade", "content": "> Should content-disposition from RFC 1806 be considered for HTTP?\n\nAs a MIME entity-header, I already consider it to be in HTTP.\nThe same goes for content-description (a.k.a. Title).\nThe real question is whether (or how) it should be described\nwithin the next HTTP spec.\n\nI think it falls into the \"damned if we do; damned if we don't\" category. ;)\n\n.....Roy\n\n\n\n", "id": "lists-010-10131857"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "Roy T. Fielding:\n>\n>Clarification ...\n>\n>>  Many content authors have managed to avoid the confirmation dialog\n>>  problem by using GETs for form submission instead of safe POSTs.\n>>  However, this escape is not possible for forms\n>> \n>>     a) which are (sometimes) used to submit large amounts of data\n>>     b) which are (sometimes) used to submit data in a charset other\n>>        than ISO-8859-1.\n>> \n>>  Case b) will be the increasingly common; web internationalization [2]\n>>  makes it necessary to use the POST method for form submission.\n>\n>This is not true.  \n\nQuoting from section 5.2 of [2] (draft-ietf-html-i18n-05.txt):\n\n   The best solution is to use the \"multipart/form-data\" media type\n   ^^^^^^^^^^^^^^^^^\n   described in [RFC1867] with the POST method of form submission.\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nDo you disagree with draft-ietf-html-i18n-05.txt, or with my\ninterpretation of it?  I can't really tell from your comments.\n\nI have not read all of draft-ietf-html-i18n-05.txt, so I may be\nmissing something, but the quote above seems quite clear.  POSTs are\nthe route draft-ietf-html-i18n-05.txt seems to be taking, and\ndraft-ietf-html-i18n-05.txt is approved as a proposed standard.  \n\nMy proposal attempts to identify and clear away an obstacle to the\ndeployment of draft-ietf-html-i18n-05.txt.  If you can convince me it\ndoes not, I will retract my proposal.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-10139528"}, {"subject": "Re: contentdisposition heade", "content": "Roy T. Fielding:\n>\n>> Should content-disposition from RFC 1806 be considered for HTTP?\n>\n>As a MIME entity-header, I already consider it to be in HTTP.\n>The same goes for content-description (a.k.a. Title).\n>The real question is whether (or how) it should be described\n>within the next HTTP spec.\n>\n>I think it falls into the \"damned if we do; damned if we don't\" category. ;)\n\nThat means it would fit perfectly into appendix 19.6.2 (Additional\nHeader Field Definitions).  Putting it into 19.6.2 seems like a good\nway to record current practice.\n\n>.....Roy\n\nKoen.\n\n\n\n", "id": "lists-010-10148925"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "> Quoting from section 5.2 of [2] (draft-ietf-html-i18n-05.txt):\n> \n>    The best solution is to use the \"multipart/form-data\" media type\n>    ^^^^^^^^^^^^^^^^^\n>    described in [RFC1867] with the POST method of form submission.\n>                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> \n> Do you disagree with draft-ietf-html-i18n-05.txt, or with my\n> interpretation of it?  I can't really tell from your comments.\n\nYes, I disagree with <draft-ietf-html-i18n-05.txt> and I did so\nmore than once on the HTML WG list.  I'd rather not discuss it again.\n\n> I have not read all of draft-ietf-html-i18n-05.txt, so I may be\n> missing something, but the quote above seems quite clear.  POSTs are\n> the route draft-ietf-html-i18n-05.txt seems to be taking, and\n> draft-ietf-html-i18n-05.txt is approved as a proposed standard.  \n\nFor the particular problem they were discussing, yes.  That particular\nproblem only occurs under ALL of the conditions I described.  What i18n\ndid was try to find a theoretically cleaner solution to a problem that\nhas already been solved in a number of ways in practice.  What they should\nhave done is just recommend using the existing methods of handling\ncharset-varying input until such time as there was a new method for GET+body.\n\n> My proposal attempts to identify and clear away an obstacle to the\n> deployment of draft-ietf-html-i18n-05.txt.  If you can convince me it\n> does not, I will retract my proposal.\n\nWhat obstacle?  Having the user hit the OK button is an inconvenience.\nGiven that the conditions under which a charset-sensitive input dialog\nis needed is ridiculously low (and no, this has nothing to do with\nwestern vs non-western -- it is only when the input text is in a different\ncharset than what the server would expect, the charset of the form),\nI just don't consider it a worthy justification for yet-another HTTP\ncontrol field. [And if it was, the browser community would implement it\nlong before the standards committee, regardless of other HTTP requirements.]\n\nI don't mind the proposal -- just be sure the justification for it\nmatches the actual need and doesn't invent problems that don't exist.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-10156741"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">    b) which are (sometimes) used to submit data in a charset other\n>       than ISO-8859-1.\n>\n> Case b) will be the increasingly common; web internationalization [2]\n> makes it necessary to use the POST method for form submission.\n\nThe I18N draft does not make POST use mandatory at all. A Safe header\ncould equally well be used to indicate that a GET-with-body result\ncan be cached/reused.\n\n\n\n", "id": "lists-010-10166748"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": ">This issue has nothing whatsoever to do with i18n.  i18n does not\n>*need* to use POST requests any more than non-i18n activities need\n>to use POST requests, and if you folks would stop knee-jerking every time\n>the subject was brought up you might get a little more accomplished.\n\nCorrect, though I18N does need message bodies for correct labelling\nof submitted data.\n\nDo I always jerk my knees?\n\n\n\n", "id": "lists-010-10174630"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "Roy says:\n\n>has already been solved in a number of ways in practice.  What they should\n>have done is just recommend using the existing methods of handling\n>charset-varying input until such time as there was a new method for GET+body.\n\nThis is actually very close to what I had in mind, though I tend to \nbe more of the \"let's get the GET-with-body ASAP\" mind. That is, I \nthink, the correct solution. The other I18N authors did not agree\nentirely with my stance.\n\n\n\n", "id": "lists-010-10182702"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">>has already been solved in a number of ways in practice.  What they should\n>>have done is just recommend using the existing methods of handling\n>>charset-varying input until such time as there was a new method for GET+body.\n> \n> This is actually very close to what I had in mind, though I tend to \n> be more of the \"let's get the GET-with-body ASAP\" mind. That is, I \n> think, the correct solution. The other I18N authors did not agree\n> entirely with my stance.\n\nWell, I'd say this is the time to define it.  If there is a working\ndefinition and evidence that it is useful, a new method is the way to go.\nThe hard part is deciding on a name.  My vote is for QUERY, but someone\nelse gets to write the ID.\n\n.....Roy\n\n\n\n", "id": "lists-010-10190698"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "koen@win.tue.nl (Koen Holtman) wrote:\n>Roy T. Fielding:\n>>\n>>Clarification ...\n>>\n>>>  Many content authors have managed to avoid the confirmation dialog\n>>>  problem by using GETs for form submission instead of safe POSTs.\n>>>  However, this escape is not possible for forms\n>>> \n>>>     a) which are (sometimes) used to submit large amounts of data\n>>>     b) which are (sometimes) used to submit data in a charset other\n>>>        than ISO-8859-1.\n>>> \n>>>  Case b) will be the increasingly common; web internationalization [2]\n>>>  makes it necessary to use the POST method for form submission.\n>>\n>>This is not true.  \n>\n>Quoting from section 5.2 of [2] (draft-ietf-html-i18n-05.txt):\n>\n>   The best solution is to use the \"multipart/form-data\" media type\n>   ^^^^^^^^^^^^^^^^^\n>   described in [RFC1867] with the POST method of form submission.\n>                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>\n>Do you disagree with draft-ietf-html-i18n-05.txt, or with my\n>interpretation of it?  I can't really tell from your comments.\n>\n>I have not read all of draft-ietf-html-i18n-05.txt, so I may be\n>missing something, but the quote above seems quite clear.  POSTs are\n>the route draft-ietf-html-i18n-05.txt seems to be taking, and\n>draft-ietf-html-i18n-05.txt is approved as a proposed standard.  \n>\n>My proposal attempts to identify and clear away an obstacle to the\n>deployment of draft-ietf-html-i18n-05.txt.  If you can convince me it\n>does not, I will retract my proposal.\n\nA minor change in wording to include reference to\n\"multipart/form-data\", as in i18n, which does require POST, \nmight be helpful.\n\nPlease, by no means retract the proposal!\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-10198755"}, {"subject": "Fwd: Discussion of new object-oriented binary wire protocol for HTTPN", "content": "Bill Janssen <janssen@parc.xerox.com> forwarded from Larry Masinter:\n  > [...]\n  > If you're interested in participating in such a discussion, you should\n  > read the archive of the HTTP-NG list at \n  > \n  >    http://sunsite.unc.edu/ses/http-ng-archive/\n\nI just looked at that site.  There are 11 items, the newest being about\na year old.   That's also the time of last update.  So where is the\narchive really?  Or is that it?  Or is it just that someone hasn't run\na cron job in nearly a year?\n\nDave Kristol\n\n\n\n", "id": "lists-010-10208251"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "At 05:09 PM 10/10/96 +0200, Koen Holtman wrote:\n>some HTML form hacks would be needed to provide the same level of downwards\n>compatibility with existing browsers that Safe can provide, for example\n>\n>  <form action=\"...\" method=post preferred_method=get-with-body>\n>   ....\n>  </form>.\n>So it boils down to cruft in HTTP vs. cruft in HTML.\n\nAren't proxies disallowed from forwarding methods they don't understand?\nWouldn't GETWITHBODY require a HTTP/1.2 (or rather, a 1.3, since servers\nwould be forced to accept it in 1.2, but clients would need to not send it\nuntil 1.3, ala FullURL)?  Safe: yes could be sent today.\n\n-----\nDaniel DuBois\nI travel, I code, I'm a Traveling Coderman         \nhttp://www.spyglass.com/~ddubois/\n\n\n\n", "id": "lists-010-10215911"}, {"subject": "(DNS) consensus wordin", "content": "Ted Hardie redrafted my words on DNS.  I like his wording better than\nmine.  I added a reference to DNSSEC, which should be issued later\nthis month.  \n\nKoen Holtman and Dave Morris have expressed concerns about the\nrequirement being mandantory (Must vs. should), and proposed an\nalternate based on a arbitrary timeout (with no defense as to how\nthat timeout might be chosen), believing that the implementation\nis difficult..  Personally, I believe this\nrequirement not too hard to implement (given the discussion,\nand the claim of Phil Hallam-Baker that he's implementing it.),\nand believe the requirement should be \"must\".\n\nHowever, I've not head much from those in support of the requirement.\n(which is how the draft read, afterall; to first order, silence\nwas taken as agreement).\nIf you believe this requirement should be a \"must\" please send\nme mail (privately, so we don't get a flurry of messages on the list).\n\nIf you believe it should be a \"should\", then please also send me mail.\nIf should, please also indicate how you would resolve the timeout\nissue which results if the recommendation is not mandantory.\n- Jim Gettys\n\n\n\n====================\n\nSection 14 (new subsection to Security Considerations):\n\nDNS Spoofing\n------------\n\nClients using HTTP rely heavily on the Domain Name Service, and are\nthus generally prone to security attacks based on the deliberate\nmis-association of IP addresses and DNS names.  The deployment of\n|DNSSEC[DNSSEC] should help this situation.  In advance of this deployment,\nhowever, clients need to be cautious in assuming the continuing\nvalidity of an IP number/DNS name association.\n\nIn particular, HTTP clients should rely on their name resolver for\nconfirmation of an IP number/DNS name association, rather than caching\nthe result of previous host name lookups.  Many platforms already can\ncache host name lookups locally when appropriate, and they should be\nconfigured to do so.  These lookups should be cached, however, only\nwhen the TTL (Time To Live) information reported by the name server\nmakes it likely that the cached information will remain useful.\n\nIf HTTP clients cache the results of a host name lookups in order to\nachieve a performance improvement, they MUST observe the TTL\ninformation reported by the name server\n\nIf HTTP clients do not observe this rule, they could be spoofed when a\npreviously-accessed server's IP address changes.  As renumbering is\nexpected to become increasingly common [RFC 1900], the possibility of\nthis form of attack will grow.  Observing this requirement thus\nreduces this potential security vulnerability.\n\nThis requirement also improves the load-balancing behavior of clients\nfor replicated servers using the same DNS name and reduces the\nlikelihood of a user's experiencing failure in accessing sites which\nuse that strategy.\n\n\nAddition to 16. References:\n[dnssec]Whatever is appropriate; it is up for a vote at the IESG this\nmonth, and may be issued as an RFC in time.\n[RFC 1900]\nB. Carpenter, Y. Rekhter,\n<a href=\"http://info.internet.isi.edu:80/in-notes/rfc/files/rfc1900.txt\">\nRenumbering Needs Work</a>. RFC 1900, IAB, February 1996.\n\n\n\n", "id": "lists-010-1022417"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "Daniel DuBois:\n>\n>At 05:09 PM 10/10/96 +0200, Koen Holtman wrote:\n>>some HTML form hacks would be needed to provide the same level of downwards\n>>compatibility with existing browsers that Safe can provide, for example\n>>\n>>  <form action=\"...\" method=post preferred_method=get-with-body>\n>>   ....\n>>  </form>.\n>>So it boils down to cruft in HTTP vs. cruft in HTML.\n>\n>Aren't proxies disallowed from forwarding methods they don't\n>understand?\n\nNot as far as I know.  A proxy is always allowed to act as a gateway\nwhen getting an unknown method.\n\n>Wouldn't GETWITHBODY require a HTTP/1.2 (or rather, a 1.3, since servers\n>would be forced to accept it in 1.2, but clients would need to not send it\n>until 1.3, ala FullURL)?\n\nI don't think there are problems like this with GETWITHBODY.  User\nagents which support it will send it to origin servers asking for it;\nthere is no need for any party to look at version numbers.\n\n>  Safe: yes could be sent today.\n>\n>-----\n>Daniel DuBois\n\nKoen.\n\n\n\n", "id": "lists-010-10224621"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "Daniel DuBois <dan@spyglass.com> wrote:\n>At 05:09 PM 10/10/96 +0200, Koen Holtman wrote:\n>>some HTML form hacks would be needed to provide the same level of downwards\n>>compatibility with existing browsers that Safe can provide, for example\n>>\n>>  <form action=\"...\" method=post preferred_method=get-with-body>\n>>   ....\n>>  </form>.\n>>So it boils down to cruft in HTTP vs. cruft in HTML.\n>\n>Aren't proxies disallowed from forwarding methods they don't understand?\n>Wouldn't GETWITHBODY require a HTTP/1.2 (or rather, a 1.3, since servers\n>would be forced to accept it in 1.2, but clients would need to not send it\n>until 1.3, ala FullURL)?  Safe: yes could be sent today.\n\nWhat the GETwithBody would be replacing in this discussion is\nnot just any GET, but ones which would otherwise have a ?searchpart.\n\nThe HTTP/1.1 draft states that Cache-Control and Expires headers\n*can* be used to yield and regulate caching of replies from POST requests.\nWhat exactly is still being sought via a GETwithBodyInsteadOfSearchpart\nthat can't be achieved via a POST with \"Safe: yes\" and Cache-Control/Expires\nheaders?  Are there *any* headers or procedures which can't be made to treat\na POST with \"Safe: yes\" as, in effect, a GETwithBodyInsteadOfSearchpart?\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-10233183"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">>The I18N draft does not make POST use mandatory at all. A Safe header\n>>could equally well be used to indicate that a GET-with-body result\n>>can be cached/reused.\n>\n>Yes, but as things *presently* stand in existing IETF RFCs\n>and IDs, to use ENCTYPE=\"multipart/form-data\" the FORM will specify\n>METHOD=POST. \n\nThe \"will\" here implies a requirement, which I have not seen. There are\nstrong recommendations, yes, but no requirements.\n\n\n\n", "id": "lists-010-10242717"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">Aren't proxies disallowed from forwarding methods they don't understand?\n>Wouldn't GETWITHBODY require a HTTP/1.2 (or rather, a 1.3, since servers\n>would be forced to accept it in 1.2, but clients would need to not send it\n>until 1.3, ala FullURL)?  Safe: yes could be sent today.\n\nWhat form of proxy? 0.9, 1.0. 1.1 ?\n\nIt seems silly to have a protocol that allows arbitrary methods, but\nproxies that don't...\n\n\n\n", "id": "lists-010-10250343"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">headers?  Are there *any* headers or procedures which can't be made to treat\n>a POST with \"Safe: yes\" as, in effect, a GETwithBodyInsteadOfSearchpart?\n\nThis is not the real issue. Here are 4 system calls:\n\n   char* get_message(void);\n   char* get_selected_message(int argc, char* argv[]);\n   char* post_message(const char* message);\n   void  write_data(const char* data);\n\nHere is typical usage:\n\n   const char* message   = get_message();\n   const char* result    = get_message(1,\"John Smith\");\n   const char* response  = post_message(\"Wow. I'm tired.\");\n   write_data(\"Here is some text.\");\n\nHow would you feel if I did this?\n\n   const char* message   = get_message();\n   const char* result    = post_message(\"John Smith\");\n   const char* response  = post_message(\"Wow. I'm tired.\");\n   write_data(\"Here is some text.\");\n\nAs you can see, it's mostly semantics. One emphasises the data\nretrieved, the other, the data sent.\n\n\n\n", "id": "lists-010-10257995"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "> The HTTP/1.1 draft states that Cache-Control and Expires headers\n> *can* be used to yield and regulate caching of replies from POST requests.\n> What exactly is still being sought via a GETwithBodyInsteadOfSearchpart\n> that can't be achieved via a POST with \"Safe: yes\" and Cache-Control/Expires\n> headers?  Are there *any* headers or procedures which can't be made to treat\n> a POST with \"Safe: yes\" as, in effect, a GETwithBodyInsteadOfSearchpart?\n\nIt tells the user agent (and thus the user) that it is safe to use\nthat method even before the first time the method is applied.  That is\nwhy there is a recommended presentational difference between safe and\nunknown-to-be-safe methods -- so that the user cannot be tricked into\nperforming an action that they expected to be safe. This concern was\nthe basis for TimBL's original security note, and why the HTTP spec\ntalks about safe methods.\n\n....Roy\n\n\n\n", "id": "lists-010-10266229"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "> Aren't proxies disallowed from forwarding methods they don't understand?\n\nNope, that restriction was eliminated by defining the message body\nsyntax independent of the method name (i.e., a message has a body\nif it includes Content-Length or Transfer-Encoding, with the only\nexceptions being HEAD responses and 304).\n\nThere will still exist problems forwarding new methods through\nrestrictive firewalls, but then that's the nature of a firewall.\n\n.....Roy\n\n\n\n", "id": "lists-010-10274285"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">This is not true.  The only time that web internationalization might\n>impact the choice of POST vs GET is when it is not known what the input\n>character set will be *and* it is possible for the user agent to submit\n>data in some character set other than what is expected *and* the form\n>does not contain an entry box for the user to select which particular\n>character set they are using *and* nobody has convinced the browser\n>community to include a standard hidden form field containing the charset\n>whenever the charset is not the same as that of the output form.\n\nIncorrect. I18N needs message bodies if any one of the following is true.\n\n1) The coded character set or encodiung of transmitted data is\n   known and it is anything other than ISO 8859-1.\n2) The server indicates that it can accept multiple coded character\n   sets and encodings, and one from the list is being used for the\n   transmitted data.\n\n>In other words, i18n has nothing to do with it.\n\nI think I18N has everything to do with the choice of using message\nbodies or not.\n\n\n\n", "id": "lists-010-10281861"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "gtn@ebt.com (Gavin Nicol) wrote:\n>>headers?  Are there *any* headers or procedures which can't be made to treat\n>>a POST with \"Safe: yes\" as, in effect, a GETwithBodyInsteadOfSearchpart?\n>\n>This is not the real issue. [...]\n\nWell..  As it is people are so on edge from the recent spate\nof holier, wiser, greater and busier than thou put downs that we're\nquibbling over individual words instead of communicating (very bad\nfor someone as typo prone as I :).  Let's not take it to the point\nof reading each others' minds about what issues are knocking around\nin them. :) :)\n\n\n>As you can see, it's mostly semantics. One emphasises the data\n>retrieved, the other, the data sent.\n\nYes, there's that difference in semantics, but once you add\na body to GET which will be sent to the server, the clarity of that\ndifference is gone (you *are* POSTing content, even though it may\nresult only in retrieval, as many POSTs presently do as well).  That\nperhaps was behind the QUERY suggestion, though it's too specific.\nThe FORM may be using POST instead of GET because the subordinate\ncontent (body) will include \"private\" or \"secure\" information which\nis \"safe\" but shouldn't be \"visible\" in the ?searchpart of a URL,\nand no search will be done.\n\nThe key difference, I agree, is that a GET by convention must\nbe safe, so you don't need a reply header to know that.  What you're\ntrading for that, if you add a body, but not also expand the http URL\nspecs, is ability to specify the resource completely in any context,\ni.e., independently of HTML markup (that's what ?searchpart is doing\nfor you).  If you don't want to lose that for *all* methods (and only\nGET has it at present), you will have to specify two components in the\nURL, the Request-URI and particular subordinate content(s), which are\nlikely to be in different locations, and may involve privacy and security\nconsiderations, beyond safety, per se.  I don't see the need for that\ncomplication which a GETwithBodyInsteadOfSearchpart would create, and\nnot just for POST with Safe: yes, unless there's something in those 8\nthousand lines of the HTTP/1.1 draft that's wanted and can't be had\nwith the latter.\n\nThere may be something that's wanted and can't otherwise be had,\nbut what, beyond perceptions of semantics?  ('Cuz a \"GET with body to\nPOST safely\" feels a bit twisted to me. :) :) :)\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n\n", "id": "lists-010-10290410"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">Quoting from section 5.2 of [2] (draft-ietf-html-i18n-05.txt):\n>\n>   The best solution is to use the \"multipart/form-data\" media type\n>   ^^^^^^^^^^^^^^^^^\n>   described in [RFC1867] with the POST method of form submission.\n>                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>\n>Do you disagree with draft-ietf-html-i18n-05.txt, or with my\n>interpretation of it?  I can't really tell from your comments.\n\nThis should be \"best solution within the current framework\". All the\nI18N draft authors agree that message bodies are needed. Method\nselection is not dictated by this however.\n\n\n\n", "id": "lists-010-10300639"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "gtn@ebt.com (Gavin Nicol) wrote:\n>>In other words, i18n has nothing to do with it.\n>\n>I think I18N has everything to do with the choice of using message\n>bodies or not.\n\nHmm.  Here's some more quibbling about individual words in our\nemail communications.  The \"nothing\" and \"everything\" in those two\nsentences make both of them hyperbole.  :) :)\n\nReason b) in the Safe ID does need a tweak in its wording, and\nperhaps some reasons c) and d) need to be added for why POST often is\nused despite the Request-URI-plus-subordinate-content being safe.\n\nBut it's otherwise an excellent -00 draft. :) :)\n\nThanks Koen!  You made my day!!!!\n\nFote \n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-10308505"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "gtn@ebt.com (Gavin Nicol) wrote:\n>>    b) which are (sometimes) used to submit data in a charset other\n>>       than ISO-8859-1.\n>>\n>> Case b) will be the increasingly common; web internationalization [2]\n>> makes it necessary to use the POST method for form submission.\n>\n>The I18N draft does not make POST use mandatory at all. A Safe header\n>could equally well be used to indicate that a GET-with-body result\n>can be cached/reused.\n\nYes, but as things *presently* stand in existing IETF RFCs\nand IDs, to use ENCTYPE=\"multipart/form-data\" the FORM will specify\nMETHOD=POST.  This is simply an issue of minor wording changes.\nThe header is not solely for i18n, but i18n is an important factor,\nand encouragement of i18n in its wording is desireable.\n\nI agree that the wording should not appear to ignore additional\npossibilies for handling a variety of charsets in FORM content.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-10316983"}, {"subject": "RE: URLLEN: consensu", "content": "Ooops.  I wasn't aware of conflicting changes to this section. I only\nreally intended to add the last paragraph, with the rest for context,\nnot claim the last word on the whole section.\n\nJim -- please just take the added paragraph.\n\nThanks,\nPaul\n\n>----------\n>From: David W. Morris[SMTP:dwm@shell.portal.com]\n>Sent: Sunday, March 31, 1996 9:54 PM\n>To: Paul Leach\n>Cc: 'http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com'\n>Subject: Re: URLLEN: consensus\n>\n>\n>\n>On Sun, 31 Mar 1996, Paul Leach wrote:\n>\n>> This is a replacement for section 3.2.1 on the syntax of URUs. It adds a\n>> paragraph to clarify the issue of URL length. (Changes are marked with\n>> change bars.)\n>> \n>> This issue was discussed on the list, and this is believed to represent\n>> the consensus on this issue. If you believe otherwise, please let me\n>> know; otherwise, this issue will be closed.\n>\n>The \"+\" is incorrect ... it is reserved in 1.0 (see JG's latest diff)\n>and must be either reserved or unsafe.  It isn't 'safe'. Of the three\n>who last debated this point on the list it was two for reserved and one\n>(me) for unsafe.\n>\n>\n>> -------------------------\n>> \n>> 3.2.1 General Syntax\n>> \n>>    URIs in HTTP can be represented in absolute form or relative to \n>>    some known base URI [11], depending upon the context of their use. \n>>    The two forms are differentiated by the fact that absolute URIs \n>>    always begin with a scheme name followed by a colon.\n>> \n>>        URI            = ( absoluteURI | relativeURI ) [ \"#\" fragment ]\n>> \n>>        absoluteURI    = scheme \":\" *( uchar | reserved )\n>> \n>>        relativeURI    = net_path | abs_path | rel_path\n>> \n>>        net_path       = \"//\" net_loc [ abs_path ]\n>>        abs_path       = \"/\" rel_path\n>>        rel_path       = [ path ] [ \";\" params ] [ \"?\" query ]\n>> \n>>        path           = fsegment *( \"/\" segment )\n>>        fsegment       = 1*pchar\n>>        segment        = *pchar\n>> \n>>        params         = param *( \";\" param )\n>>        param          = *( pchar | \"/\" )\n>> \n>>        scheme         = 1*( ALPHA | DIGIT | \"+\" | \"-\" | \".\" )\n>>        net_loc        = *( pchar | \";\" | \"?\" )\n>>        query          = *( uchar | reserved )\n>>        fragment       = *( uchar | reserved )\n>> \n>>        pchar          = uchar | \":\" | \"@\" | \"&\" | \"=\"\n>>        uchar          = unreserved | escape\n>>        unreserved     = ALPHA | DIGIT | safe | extra | national\n>> \n>>        escape         = \"%\" HEX HEX\n>>        reserved       = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\"\n>>        extra          = \"!\" | \"*\" | \"'\" | \"(\" | \")\" | \",\"\n>>        safe           = \"$\" | \"-\" | \"_\" | \".\" | \"+\"\n>>        unsafe         = CTL | SP | <\"> | \"#\" | \"%\" | \"<\" | \">\"\n>>        national       = <any OCTET excluding ALPHA, DIGIT,\n>>                         reserved, extra, safe, and unsafe>\n>> \n>>    For definitive information on URL syntax and semantics, see RFC \n>>    1738 [4] and RFC 1808 [11]. The BNF above includes national \n>>    characters not allowed in valid URLs as specified by RFC 1738, \n>>    since HTTP servers are not restricted in the set of unreserved \n>>    characters allowed to represent the rel_path part of addresses, and \n>>    HTTP proxies may receive requests for URIs not defined by RFC 1738.\n>> \n>> |   The HTTP protocol does not place any a-priori limit on the length of\n>> a URI.\n>> |   Servers MUST be able to handle the URI of any resource they serve,\n>> |   and SHOULD be able to handle URIs of unbounded length if they\n>> provide\n>> |   GET-based forms that could generate such URIs. A server SHOULD\n>> return\n>> |   a status code of \n>> |414 Request-URI Too Large\n>> |   if a URI is longer than the server can handle.\n>> |\n>> |Note:\n>> |   Servers should be cautious about depending on URI lengths above\n>> |     255 bytes, because some older client or proxy implementations may\n>> |   not properly support these.\n>> |\n>> |   All client and proxy implementations MUST be able to handle a URI\n>> |   of any finite length. \n>> \n>> \n>> ----------------------------------------------------\n>> Paul J. Leach            Email: paulle@microsoft.com\n>> Microsoft                Phone: 1-206-882-8080\n>> 1 Microsoft Way          Fax:   1-206-936-7329\n>> Redmond, WA 98052\n>> \n>> \n>\n\n\n\n", "id": "lists-010-1032254"}, {"subject": "Redirect Syntax!", "content": "SaiRam!!\n\nCan someone tell me how to make my CGI program redirect to another \nlocation instead of the one that is executing this script? \n\nI don't seem to be ablt to do it with just a \nprintf(toClient, \"Location: http://npac.syr.edu/somepage.html\");\nand writing toClient to the client.\n\nDo I need to have \"Content-type: text/html\" also! Can anyone point me to \nsome examples of how you achieved redirection using HTTP headers?\n\nAny help is very much appreciated.\n\nSaiRam!\n+--------------------------------------------------------------------------+\n|               Money comes and goes, Morality comes and grows             |\n|                                        - Bhagawan Sri Satya Sai Baba     |\n+--------------------------------------------------------------------------+\n\n\n\n", "id": "lists-010-10325644"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "Gavin Nicol:\n>\n  [Koen:]\n>>    b) which are (sometimes) used to submit data in a charset other\n>>       than ISO-8859-1.\n>>\n>> Case b) will be the increasingly common; web internationalization [2]\n>> makes it necessary to use the POST method for form submission.\n>\n>The I18N draft does not make POST use mandatory at all.\n\nTrue, and I did not mean to imply it did.  If I understand the messages in\nthis thread correctly, the I18N draft make the uses of _method bodies_\nnecessary, and in current envirionents, that means using POST.\n\n> A Safe header\n>could equally well be used to indicate that a GET-with-body result\n>can be cached/reused.\n\nThe existing Cache-Control header can already be used to indicate\ncache/reuse for a GET-WITH-BODY.  \n\nAnd if a new GET-WITH-BODY is defined, one would not need the Safe header\nanymore, one could simply define GET-WITH-BODY as always safe.  However,\nsome HTML form hacks would be needed to provide the same level of downwards\ncompatibility with existing browsers that Safe can provide, for example\n\n  <form action=\"...\" method=post preferred_method=get-with-body>\n   ....\n  </form>.\n\nSo it boils down to cruft in HTTP vs. cruft in HTML.\n\nKoen.\n\n\n\n", "id": "lists-010-10333381"}, {"subject": "[raylutz&#64;cognisys.com: Negotiations of Content type, etc.", "content": "Background reading for those reviewing the feature & transparent negotiation\ndraft...\n\n------- Start of forwarded message -------\nDate: Thu, 10 Oct 1996 06:56:31 PDT\nTo: ietf-fax@imc.org\nFrom: raylutz@cognisys.com (Raymond Lutz)\nSubject: Negotiations of Content type, etc.\n\nThe negotiations of content type and the like is a key aspect to proper\noperation of the \"future\" version of internet fax. This topic is tending\nmore toward \"job submission\" than what is commonly described as \"fax\". See\n\n        ftp.mfpa.org/tdf004.txt\n\nand other tdf... files. They describe a multi-pass negotiation protocol and\nan attempt to distill the attributes needed from a variety of standards,\nincluding T.30, T.434, TIFF, T.611, MIME, and others.\n\n-Raymond\n\n/***********************************************************************\n** Raymond Lutz                             Voice: 619-447-3246\n** Director R & D, Cognisys, Inc.           Fax:   619-447-6872 \n** MFPA EC Chair                            BBS:   619-447-2223\n** 1010 Old Chase Ave., Suite B             EMail: raylutz@cognisys.com\n** El Cajon (San Diego Co.), CA 92020 USA   MFPA:  1-800-603-MFPA\n** WWW:   http://www.cognisys.com                  http://www.mfpa.org\n***********************************************************************/\n\n\n------- End of forwarded message -------\n\n\n\n", "id": "lists-010-10342092"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">The hard part is deciding on a name.  My vote is for QUERY, but someone\n>else gets to write the ID.\n\nI favour PGET (Parameterised GET).\n\n\n\n", "id": "lists-010-10351108"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "gtn@ebt.com (Gavin Nicol) wrote:\n>>The hard part is deciding on a name.  My vote is for QUERY, but someone\n>>else gets to write the ID.\n>\n>I favour PGET (Parameterised GET).\n\nThat seems general enough, with the right and full semantic\nconnotations.\n\nIn addition to the requirements that it result only in\nretrieval and be safe, what about idempotence (as actually defined\nin Section 9.1.2 :)?\n\nThat appears to pertain to automatic retries on interrupted\nor segmental transmissions between HTTP/1.1 UAs and servers.  How\nwould the PGET body affect those considerations?\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-10358068"}, {"subject": "TransferEncoding question", "content": "When decoding chunked (section 3.6): \n \n1) chunk-size is an arbitrary length hex string; however, it does not \n   say anything about byte order. \n   I presume, since the first digit cannot be zero except for the \n   ending chunk, that it is intended to be little endian? \n \n2) the spec is very vague on chunk-ext: it appears to be an extenstion \n   mechanism; what am I supposed to do if I don't understand it? \n   I presume ignore it. \n \n3) what is the point of the CRLF after the chunk-data?  We already know \n   the size; it is only likely to introduce bugs in implementations \n   that decide (falsely) that it is safe to just scan for CRLF. \n \n(a year too late once again)\n\nThank You, \nRichard L. Gray\n\n\n\n", "id": "lists-010-10366187"}, {"subject": "Re:  TransferEncoding question", "content": "rlgray@raleigh.ibm.com wrote:\n  > When decoding chunked (section 3.6): \n  >  \n  > 1) chunk-size is an arbitrary length hex string; however, it does not \n  >    say anything about byte order. \n  >    I presume, since the first digit cannot be zero except for the \n  >    ending chunk, that it is intended to be little endian? \n\nYou're thinking too hard. :-)  It's just a number.  Convert it with\nstrtol().\n  >  \n  > 2) the spec is very vague on chunk-ext: it appears to be an extenstion \n  >    mechanism; what am I supposed to do if I don't understand it? \n  >    I presume ignore it. \n\nIt is an extension mechanism.  Yes, ignore what you don't understand.\n\n  >  \n  > 3) what is the point of the CRLF after the chunk-data?  We already know \n  >    the size; it is only likely to introduce bugs in implementations \n  >    that decide (falsely) that it is safe to just scan for CRLF. \n\nIt makes debugging easier, because each chunk-size begins on a new\nline.  It would be reckless to scan for CRLF anyway, because CRLF could\nbe part of the chunked data.  So you obviously *have* to use the count.\n\nDave Kristol\n\n\n\n", "id": "lists-010-10373174"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": "Well, for the sake of implementers I'll explain my reasoning on ...\n\n>>This is not true.  The only time that web internationalization might\n>>impact the choice of POST vs GET is when it is not known what the input\n>>character set will be *and* it is possible for the user agent to submit\n>>data in some character set other than what is expected *and* the form\n>>does not contain an entry box for the user to select which particular\n>>character set they are using *and* nobody has convinced the browser\n>>community to include a standard hidden form field containing the charset\n>>whenever the charset is not the same as that of the output form.\n> \n> Incorrect. I18N needs message bodies if any one of the following is true.\n> \n> 1) The coded character set or encodiung of transmitted data is\n>    known and it is anything other than ISO 8859-1.\n\nNope.  Regardless of whether the charset of transmitted data is known or\nunknown, what gets stuck in the GET URL query part is the sequence of\n8-bit bytes encoded as if the bytes were ISO 8859-1 -- the server is\nunder no obligation, when decoding the URL query part, to treat those\nbytes as anything other than binary data after removing the URL encoding.\nIt's messy, but that's what happens.  If the server knows what the charset\nis intended to be, it can interpret the data in that charset (even if it\nvaries between form elements).\n\n> 2) The server indicates that it can accept multiple coded character\n>    sets and encodings, and one from the list is being used for the\n>    transmitted data.\n\nAnd the server provides no other means for the client to indicate what\nthe charset is.  One other such means is a FORM selection field above\nthe entry box.  Another, not currently available, would be an agreement\nwith user agent vendors to include the hidden field xxxx_CHARSET with\nany input field xxxx for which the charset used is different than the\ncharset of the document containing the FORM.\n\nThere are many ways to solve the i18n problems using the existing\ntechnology.  If that wasn't true, the solution would have been in HTTP/1.1.\nIt is only when we want the \"best\" or \"cleanest\" mechanism that we end up\nrunning into barriers, but at least we are making progress on removing\nthose as well.\n\nGET+body is useful, beyond i18n, because we also want to do things like\nsubmit complex search queries using portable scripting languages, where\nthe language is indicated by the Content-Type and the request applies\nto the entire server or a server subspace.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-10380638"}, {"subject": "Re: criticism of http 1.1 cachin", "content": "    I have written a paper which extensively criticizes HTTP 1.1's\n    cache consistency features.  The paper recommends numerous small\n    but important changes to the specification.  I presented the paper\n    at the Web Caching workshop this week in Warsaw, Poland\n    (http://w3cache.icm.edu.pl/workshop/).\n\nFirst of all, I'd suggest that people interested in Web caching\ntake a look at the online papers available from this page.  Does\nanyone know if a printed proceedings is available to non-attendees?\n    \n    The paper is on the Web at\n    \n    http://libra.ms.mff.cuni.cz/internet/caching/consistency.html\n    \nI enjoyed reading this paper, even if I don't agree with all of\nthe points you raised.  It's clear that you have given this a\nlot of careful thought, and your Java implementation shows that\nthe HTTP/1.1 spec probably does not require a tremendous amount\nof code to implement at the proxy (although you only implemented\npart of it, so far).\n\nOne general comment: in many cases, you point out that the HTTP/1.1\nspec is silent about certain aspects of cache behavior, and you\nrecommend that it be more explicit.  We tried not to constrain\ncache implementors more than necessary for ensuring interoperability\nand \"correct\" operation (whatever \"correct\" was taken to mean), and\nso we intentionally left some things unspecified.  This clearly\ngives some freedom to a cache implementor, either to do something\nclever that we hadn't considered, or do the minimum necessary\nimplementation, or (in some cases) to do something that is stupid\nand inefficient, but still interoperable.\n\nFor example, the spec does not say anything (that I can recall)\nabout prefetching or postfetching, which you write about\nunder \"When to update cached pages\".  As long as the prefetching\nor postfetching doesn't violate the rest of the spec, it is\nnot necessary for the spec to say anything about it.  Especially\nsince we don't really know exactly what the best policies might be.\n\nSpecific comments:\n--------------\n    Thus, the notion of whether a page is stale or fresh at a given\n    time is independent of any single user request for that page; this\n    fact is not stated clearly in the HTTP 1.1 specification, and\n    should be.\n\nActually, if you read these four definitions together:\n    explicit expiration time\n      The time at which the origin server intends that an entity should no\n      longer be returned by a cache without further validation.\n    \n    heuristic expiration time\n      An expiration time assigned by a cache when no explicit expiration\n      time is available.\n    \n    freshness lifetime\n      The length of time between the generation of a response and its\n      expiration time.\n    \n    fresh\n      A response is fresh if its age has not yet exceeded its freshness\n      lifetime.\n\nthen you should be able to see that whether or not a response is\nfresh depends on the current time, and either:\n(1) the explicit expiration time supplied by the origin server\nor\n(2) a heuristic expiration time assigned by a cache\nwhich implicitly excludes any dependency on the specific request.\n\n--------------\n\n    The HTTP 1.1 specification refers to \"the least [sic] restrictive\n    freshness requirement of the client, server, and cache\", a related\n    concept, in section 13.1.1 \"Cache Correctness\". First, this is\n    certainly an error: the term \"least\" should be \"most\".\n\nActually, this is really what was intended.  The reason is that\nsome people believe that a client ought to be able to loosen\nthe freshness requirement beyond what is specified by the\norigin server.\n\nThe confusion may come, as you note, because the next item:\n\n  3. It includes a warning if the freshness demand of the client or the\n     origin server is violated (see section 13.1.5 and 14.45).\n\nseems to result in a contradiction.  For example, if the origin\nserver says \"max-age=10\", the client says \"max-age=100\", and\nthe actual age is 50, a response with a Warning is consistent\nwith both conditions (#2, which you quote from, and #3 above).\n\nThe missing piece is that this particular section does not\nexplictly require a Warning in this case, although this is\nclearly required by the specification of Warning in 14.45.\nThat is, 13.1.1 describes some but not all of the criteria\nfor correctness.\n\nIt seems reasonable that we should try to reword 13.1.1 so\nthat it makes this point clearer, although the current wording\nis not actually in error.\n\n--------------\n\n    There has been some controversy over how to handle\n    If-Modified-Since in a cache hierarchy.  Apparently some people\n    feel that every If-Modified-Since request should be passed all the\n    way up the cache hierarchy to the origin server; others feel that\n    If-Modified-Since requests should stop at some point on the cache\n    hierarchy if a cache has a copy of the requested page that is new\n    enough. The HTTP 1.1 specification is unfortunately vague in this\n    respect.\n\nDuring our discussions, I believe it was implicitly agreed that\nif a conditional (e.g., If-Modified-Since or If-None-Match) request\nis satisfiable by an intermediate cache, then it should not be\nforwarded all the way to the origin server.  \n    \nAlso, 13.1.1 allows a cache to respond with an \"appropriate 304\n(Not Modified)\" message, which would only be possible if it were\nintercepting conditional requests.  So I think any reasonable\nreading of the HTTP/1.1 spec allows the interception behavior\n(although it does not make it mandatory).\n\n--------------\n\n    In fact, in the definition of cache correctness in section 13.1.1\n    the specification lists Not Modified responses as being exempt from\n    the freshness requirements placed on responses containing a\n    document; this implies that Not Modified responses may be\n    arbitrarily old! (The specification text does say \"4. It is an\n    appropriate 304 (Not Modified) ... response message\", but it is\n    anyone's guess just what the adjective \"appropriate\" means here.)\n\nyou apparently failed to read this section:\n\n    13.4 Response Cachability\n\n[...]\n    A response received with any other status code MUST NOT be returned\n    in a reply to a subsequent request unless there are Cache-Control\n    directives or another header(s) that explicitly allow it. For\n    example, these include the following: an Expires header (section\n    14.21); a \"max-age\", \"must-revalidate\", \"proxy-revalidate\",\n    \"public\" or \"private\" Cache- Control directive (section 14.9).\n\nSo, if you think about it, the \"304 (Not Modified)\" message cannot\nbe fresh any longer than the actual response would have been.\n\nAs you point out, there may be some ambiguity in the wording in 13.1.1.\nPerhaps this sentence:\n    \n      4. It is an appropriate 304 (Not Modified), 305 (Proxy Redirect), or\n error (4xx or 5xx) response message.\n\nshould be    \n\n      4. It is an appropriate 304 (Not Modified), 305 (Proxy Redirect), or\n error (4xx or 5xx) response message, generated at that cache.\n\n--------------\n\n    Section 13.1.1 \"Cache Correctness\" of the HTTP 1.1 specification\n    says that a response must include a warning \"if the freshness\n    demand of the client or the origin server is violated\". HTTP 1.1\n    defines a warning \"10 Response is stale\" to be issed when the\n    server's maximum age requirement is violated, even if the server's\n    age requirement is explicitly relaxed by the client. Unfortunately,\n    the specification defines no warning to be issued when the client's\n    freshness demand is violated!\n\nThis is a good point.  However, it might be acceptable to let\nWarning 10 apply to this case, too (by changing the definition);\nis there any real need to distinguish the two cases?\n\n----------------\n\nYou end by summarizing about 15 recommended changes to the\nspecification.  I think it might be helpful if you could divide\nthese into three categories:\n\n(1) issues where an actual change to the intent of the\nspecification is required, or at least where a change\nmight be useful.\n\n(2) ways in which the specification's wording or organization\nleads to confusion, although the actual intent is right.\nI.e., clarification is needed.\n\n(3) \"advice to implementors\": areas where the specification\nproperly leaves some freedom to implementors, but we can\ngive some advice that seems to be \"best current practice\".\n\nWe have decided that items in category #3 should not appear in\nthe actual specification.  Martin Hamilton (MARTIN@MRRL.LUT.AC.UK)\nwas collecting a set of these, for possible use in a companion\ndocument.\n\n-Jeff\n\n\n\n", "id": "lists-010-10390999"}, {"subject": "Re: draft-holtman-http-safe00.tx", "content": ">> 1) The coded character set or encodiung of transmitted data is\n>>    known and it is anything other than ISO 8859-1.\n>\n>Nope.  Regardless of whether the charset of transmitted data is known or\n>unknown, what gets stuck in the GET URL query part is the sequence of\n>8-bit bytes encoded as if the bytes were ISO 8859-1 -- the server is\n\nRight, and this is the problem. It doesn;t work in the general case.\nThat is why bodies are needed in this case.\n\n>There are many ways to solve the i18n problems using the existing\n>technology. \n\nYes, and all of them distasteful, nonscaleable, or fail in the \ngeneral case. \n\n>GET+body is useful, beyond i18n, because we also want to do things like\n>submit complex search queries using portable scripting languages, where\n>the language is indicated by the Content-Type and the request applies\n>to the entire server or a server subspace.\n\nRight. That is why I have been pushing for GET+body (Parameterised\nGET) for over a year now....\n\n\n\n", "id": "lists-010-10406916"}, {"subject": "TransferEncoding question", "content": "Thanks for the answers.  One more question: \n \nI see nothing in the spec that prevents a PUT of a chunked (possibly \nmultipart) byterange. \n \nIs this intentional, did I miss something, or am I the only one who \nhas no clue why one would want to do it?\n\nThank You, \nRichard L. Gray\n\n\n\n", "id": "lists-010-10414938"}, {"subject": "Re: TransferEncoding question", "content": "At 04:38 PM 10/14/96 EST, rlgray@raleigh.ibm.com wrote:\n>Thanks for the answers.  One more question: \n> \n>I see nothing in the spec that prevents a PUT of a chunked (possibly \n>multipart) byterange. \n> \n>Is this intentional, did I miss something, or am I the only one who \n>has no clue why one would want to do it?\n\nNo, you may indeed want to upload a document using chunked transfer\nencoding. libwww in fact does that if it for some reason doesn't know the\nsize. Data objects can just as well be generated dynamically on client side\nas on server side.\n\nByte ranges are likewise most useful for example if you are handling very\nlarge objects. \n\nHenrik\n\n\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-10422188"}, {"subject": "Re: TransferEncoding question", "content": "> No, you may indeed want to upload a document using chunked transfer \n> encoding. libwww in fact does that if it for some reason doesn't know the \n> size. Data objects can just as well be generated dynamically on client side \n> as on server side. \n>    \n> Byte ranges are likewise most useful for example if you are handling very \n> large objects.  \n>    \n> Henrik \n \nSorry, I failed to make myself clear.  I can imagine wanting to do one \nor the other, but the question is *both* at the same time.\n\nDogbert '96\n\ncc: Henrik Frystyk Nielsen <frystyk@w3.org>\n\n\n\n", "id": "lists-010-10430731"}, {"subject": "Re: TransferEncoding question", "content": "> Sorry, I failed to make myself clear.  I can imagine wanting to do one \n> or the other, but the question is *both* at the same time.\n\nNo, I wouldn't do both at the same time either.  OTOH, there is no reason\nfor the HTTP spec to prevent use of both at the same time.  There are\na lot of things in the HTTP spec which aren't mentioned simply because\nno one would want to do them.\n\n......Roy\n\n\n\n", "id": "lists-010-10438686"}, {"subject": "Date in If-modifiedsince heade", "content": "I have been brought up the issue of the date string used in the\nIf-modified-since header.  Current practice is to use the\nLast-modified header, and given clock skew, it's also the only\nfoolproof way to do IMS requests.  Could the wording in the spec be\nchanged to reflect that, and eliminate this source of data\nincorrectness?  Basically, change the semantics of \"If-modified-since\"\nbe \"In-not-equal-last-modified\".\n\nNetscape servers and proxy were changed to use an equality check,\nbecause there were cases, with older software, incorrectly set\ntimezones or machine times and daylight savings times where the dates\nwould be off and stale data would be rendered up-to-date.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-10445939"}, {"subject": "Re: Date in If-modifiedsince heade", "content": "> I have been brought up the issue of the date string used in the\n> If-modified-since header.  Current practice is to use the\n> Last-modified header, and given clock skew, it's also the only\n> foolproof way to do IMS requests.  Could the wording in the spec be\n> changed to reflect that, and eliminate this source of data\n> incorrectness?  Basically, change the semantics of \"If-modified-since\"\n> be \"In-not-equal-last-modified\".\n\nNo.  While exact cache comparison is the most frequent use of IMS,\nit is not the only use, and sending an IMS value containing the Date\n(instead of the Last-Modified) field value is normal and accepted practice.\nThe server will not adequately support HTTP clients if it does a strict\nequality check.\n\n> Netscape servers and proxy were changed to use an equality check,\n> because there were cases, with older software, incorrectly set\n> timezones or machine times and daylight savings times where the dates\n> would be off and stale data would be rendered up-to-date.\n\nThat is the wrong fix.  The right fix is described under how to deal\nwith last-mod dates which are later than the server's date (the remote\nfilesystem problem) and how to interpret IMS dates which are later\nthan the server's date (the bad clock problem).  Additional opaque\nvalidation can be achieved by using the Etag and If-None-Match header fields.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-10453985"}, {"subject": "Re: Date in If-modifiedsince heade", "content": "> I have been brought up the issue of the date string used in the\n> If-modified-since header.  Current practice is to use the\n> Last-modified header, and given clock skew, it's also the only\n> foolproof way to do IMS requests.  Could the wording in the spec be\n> changed to reflect that, and eliminate this source of data\n> incorrectness?  Basically, change the semantics of \"If-modified-since\"\n> be \"In-not-equal-last-modified\".\n\nAlthough I think, if I-M-S were used *only* for cache validation,\nthis might have been a better design that the one that we now have\nfor I-M-S, it may not be the way that all existing clients expect\nthe servers to behave.  That's why I insisted that the entity-tag\nmechanism be entirely opaque, and I encourage Netscape to help the\ninstalled base move to a caching scheme based on entity-tags.\n\nAnd I basically agree with Roy that there may be other reasons\nto use I-M-S, other than strict cache comparisons.  (As long as\npeople are clear on why they are using I-M-S in this way!)\n\nHowever, as an \"advice to implementors\" kind of thing, I would\nsuggest that an HTTP client ought to act as if the If-Modified-Since\nheaders that it sends *for cache validation* are going to be\ninterpreted as \"If-Modification-date-does-not-match-exactly\".\nIn other words, an HTTP client ought to be careful to preserve\nall of the accuracy in the Last-Modified date, rather than (perhaps)\nplaying games based on the official semantics of If-Modified-Since.\n\nThis might also avoid certain potential problems related to time\nzone changes :-)\n\n-Jeff\n\n\n\n", "id": "lists-010-10463281"}, {"subject": "Document Action: Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.0 to Informationa", "content": "The IESG has reviewed the Internet-Draft \"Hypertext Transfer Protocol\n-- HTTP/1.0\" <draft-ietf-http-v10-spec-05.txt, .ps> and recommends that\nit be published by the RFC Editor as an Informational RFC, but with the\nfollowing IESG Note:\n\nThe IESG has concerns about this protocol, and expects this\ndocument to be replaced relatively soon by a standards track\ndocument.\n\n  This document is the product of the HyperText Transfer Protocol\n  Working Group. The IESG contact persons are Keith Moore and Harald\n  Alvestrand.\n\n\n\n", "id": "lists-010-1046472"}, {"subject": "Re: Date in If-modifiedsince heade", "content": "> However, as an \"advice to implementors\" kind of thing, I would\n> suggest that an HTTP client ought to act as if the If-Modified-Since\n> headers that it sends *for cache validation* are going to be\n> interpreted as \"If-Modification-date-does-not-match-exactly\".\n> In other words, an HTTP client ought to be careful to preserve\n> all of the accuracy in the Last-Modified date, rather than (perhaps)\n> playing games based on the official semantics of If-Modified-Since.\n\nYep.\n\n......Roy\n\n\n\n", "id": "lists-010-10472244"}, {"subject": "latest html specificatio", "content": "Hello,\n\nI obtained RFC1866 describing HTML 2.0.  \n\nI would like to known where I can find the specification of the latest html\nversion which I beleve is 3.2 or 3.3.\n\nThank you,\n\n\nMarc-Andr? Joyal\nMatrox Electronics Systems\nNetworks Division\nMontr?al,Qu?bec \n(514)-685-7230 ext.:2122\n\n\n\n", "id": "lists-010-10479365"}, {"subject": "Unicode support?", "content": "> I obtained RFC1866 describing HTML 2.0.\n> \n> I would like to known where I can find the specification of the latest html\n> version which I beleve is 3.2 or 3.3.\n> \n> Thank you,\n> \n> Marc-Andr? Joyal\n> Matrox Electronics Systems\n> Networks Division\n> Montr?al,Qu?bec\n> (514)-685-7230 ext.:2122\n\n\n\n", "id": "lists-010-10486387"}, {"subject": "Re: Unicode support?", "content": "Hi,\n \n  Can some one point to me right place to find out about the\n internationalization of html and http.Or lemme put it in other way.\n \n How do I have a japanese document without having to have it in a\n image.i.e. Client should get the character bytes(unicode or whatever it\n is) and browser should display the corresponding language.Is it\npossible\n in the current state of affairs.If not what is the effort that is put\n into make this possible.\n \n Forgive my ignorance if I missed any point.\n \nSorry if this post made it twice.\n Regards,\n sateesh.\n\n\n\n", "id": "lists-010-10493706"}, {"subject": "HTTP working group status &amp; issue", "content": "I'm majorly busy from now until the end of the year, so I'm not going\nto be able to send out frequent updates. The usual disclaimers\napply even more so: please check this over and see if it matches your\nunderstanding.\n\n- HTTP/1.1 & digest:\n   Expecting RFC Real Soon Now.\n   Complaints, editorial advice, ambiguities welcome.\n   I was assuming someone else was collecting these, but I shouldn't.\n\n- state (cookies):\n   Some minor comments from IESG, resolution & progress\n   expected soon.\n\n- security \n   draft-ietf-wts-shttp-03.txt 'bounced' as Proposed Standard. Most\n   likely if it progresses at all it will go forward as Experimental,\n   after editorial work happens (or a disclaimer gets added.)\n\n   I think that the separation and lack of coordination between\n   WTS and HTTP-WG was a serious problem, and that I should have done\n   more to prevent this morass.\n\n   I believe the Internet needs documented standards-track security\n   mechanisms for HTTP & distributed content. Perhaps we should\n   re-charter a new group to work on standards for SSL and applet\n   signing.\n\n- hit metering: draft-ietf-http-hit-metering-00.txt\n   Jeff and Paul promised a new draft when I said we were going to\n   drop this, but we've yet to see one.\n\n- PEP: draft-khare-http-pep-01.txt\n   After I'd miscategorized the status the last time I sent this out, \n   Rohit promised timely attention on PEP. However, I've yet to\n   see any traffic on PEP.\n\n- transparent negotiation:\n   draft-holtman-http-negotiation-03.txt\n\n- features registration:\n    expecting 'discussion draft' soon\n\n- GET-with-body / Safe: yes\n   Internet draft?\n\n- versioning & distributed authoring\n   This group meets in November, and has a workshop scheduled\n   for the week before IETF. I suggest a BOF/Working group\n   for review of specifications.\n\n- HTTP-NG\n   I hope a joint HTTP/Transport meeting & BOF for HTTP-NG\n\n- Nits:\n   Content-disposition -> 19.6.2\n   Mallery's mid:v03007808ae4bdef7de86@%5B128.52.39.15%5D\n\nMEETING IN DECEMBER\n   Next IETF is already being scheduled. We have Monday & Tuesday\n   morning.  There will likely be a HTTP-NG BOF, probably jointly with\n   'transport'. There is possibly a 'versioning' BOF, although to be\n   determined. \n\n\nSCHEDULE AND MILESTONES\n\n\n# 12/96: submit negotiation draft(s) for Proposed Standard\n#  1/97: revised HTTP/1.x internet drafts (intended for Draft Standard)\n#  6/97: submit HTTP/1.x (suite) of documents for Draft (or Proposed) Standard,\n#        superceding previous RFCs. WG closes, mailing list stays open.\n\n# Whether 1.x = 1.1 or 1.2 depends on whether any protocol changes need\n# a version number increment. Whether 6/97 submits for Draft or Proposed\n# depends on whether we decide to make any additions.\n\nI am not getting any pressure from anyone that we should move\naggressively on 1.2, and some indication that we should push back our\nschedule so that we don't shut down before implementations of 1.1\nare deployed and there's some clear experience with it. As for the\noriginal schedule I posted a month ago, it's pretty clear that 12/96\nis too soon for negotiation, and likely that 1/97 is too soon for a\nrevised HTTP/1.x.\n\nNote that I've not added back PEP or Hit Metering until there's more\nvisible progress.\n\nAnd even though 'safe' is a separate draft, it's separate for\ndiscussion purposes, and should likely get pushed back into the main\nHTTP draft.\n\n\n\n", "id": "lists-010-10500892"}, {"subject": "Re: Unicode support?", "content": "For background on web internationalization, you might see, for\nexample,\n   http://www.w3.org/pub/WWW/International/\n\nand, of course, the soon-to-be-RFC HTTP/1.1 specification:\n   http://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-07.txt\n\nLarry\n\n\n\n", "id": "lists-010-10511554"}, {"subject": "HTTPNG", "content": "Is there an HTTP-NG WG? A list?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10518786"}, {"subject": "Re: HTTP working group status &amp; issue", "content": "Larry Masinter wrote:\n> \n> I'm majorly busy from now until the end of the year, so I'm not going\n> to be able to send out frequent updates. The usual disclaimers\n> apply even more so: please check this over and see if it matches your\n> understanding.\n[...]\n> - security\n[...]\n>    I think that the separation and lack of coordination between\n>    WTS and HTTP-WG was a serious problem, and that I should have done\n>    more to prevent this morass.\n> \n>    I believe the Internet needs documented standards-track security\n>    mechanisms for HTTP & distributed content. Perhaps we should\n>    re-charter a new group to work on standards for SSL and applet\n>    signing.\n\nThere is, of course, an effort underway (ietf-tls) to standardize a\ngeneralized form of SSL.  Did you have something else in mind?\n\nDave Kristol\n\n\n\n", "id": "lists-010-10526121"}, {"subject": "Request for informatio", "content": "Hello\n\nIn discussions with colleague of mine about the http and\nsome ideas for improvements, he directed me to the working group. I have\nread the proposed http/1.1 document and cannot find any references to the\nchanges that I have in mind, nor can I find the archives for this list to\nsee if this has been proposed before. Could some kind soul contact me\ndirectly and point me to the right forum/protocol for sharing these ideas?\n\nThanks\n\n-Doug\n\n-- \nDouglas A. Denny                        denny@rns.net\nNetwork Operations Specialist           DIRECT:    +1 416 443 7941\nRogers Network Services                 TOLL-FREE: +1 800 267 DATA\n          \"Shoot the messenger first, ask questions later\"\n\n\n\n", "id": "lists-010-10534212"}, {"subject": "Re: HTTPNG", "content": "> Is there an HTTP-NG WG? A list?\n> \n> Cheers,\n> \n> Ben.\n\nThis WAS it for awhile...  I dunno if I missed a change, or if gossip and\nunorganization of the workgroup has gotten to everyone...\n\nI've just been waiting for everything to come back to normal so I can\ncontinue...\n\nBrad Douglas\nVice President of Networks\nCampbell Network Systems\n(616)-774-3131\n\n\n\n", "id": "lists-010-10541282"}, {"subject": "Re: HTTP working group status &amp; issue", "content": "On Thu, 17 Oct 1996, Larry Masinter wrote:\n\n> Date: Thu, 17 Oct 1996 00:32:04 PDT\n> From: Larry Masinter <masinter@parc.xerox.com>\n> To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: HTTP working group status & issues\n> \n> ...\n> \n> - security \n>    draft-ietf-wts-shttp-03.txt 'bounced' as Proposed Standard. Most\n>    likely if it progresses at all it will go forward as Experimental,\n>    after editorial work happens (or a disclaimer gets added.)\n> \n>    I think that the separation and lack of coordination between\n>    WTS and HTTP-WG was a serious problem, and that I should have done\n>    more to prevent this morass.\n\nWas this mis-coordination the real problem here?\n\n>    I believe the Internet needs documented standards-track security\n>    mechanisms for HTTP & distributed content. Perhaps we should\n>    re-charter a new group to work on standards for SSL and applet\n>    signing.\n\nAre you aware of the TLS working group which is aiming at a merged \nSSL/PCT?\n\n> ...\n> \n> MEETING IN DECEMBER\n>    Next IETF is already being scheduled. We have Monday & Tuesday\n>    morning.  There will likely be a HTTP-NG BOF, probably jointly with\n>    'transport'. There is possibly a 'versioning' BOF, although to be\n>    determined. \n> \n> ...\n> \n\nDonald\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\nhttp://www.cybercash.com           http://www.eff.org/blueribbon.html\n\n\n\n", "id": "lists-010-10548309"}, {"subject": "(HOST) (FULLURL) consensus wording", "content": "I received a number of comments; thanks to all.  Hopefully\nthis now covers all the bases.  Changes from draft indicated by ! and +.\n\n- Jim Gettys\n\n\nAdd to section 5.1.2 Request-URI\n   To allow for transition to absoluteURIs in all requests in future versions\n   of HTTP, HTTP/1.1 servers must accept the absoluteURI form in requests, \n!  even though HTTP/1.1 clients will not normally generate them.\n!  Versions of HTTP after HTTP/1.1 may require absoluteURIs everywhere,\n!  after HTTP/1.1 or later have become the dominant implementations.\n\nChange the sentence:\n   The absoluteURI form is only allowed when the request is being made to\n   a proxy.\nTo:\n   The absoluteURI form is required when the request is being made to\n   a proxy.  The absoluteURI form is only allowed to an origin server\n   if the client knows the server supports HTTP/1.1 or later.\n!  If the absoluteURI form is used, any Host request-header included\n!  with the request must be ignored.\n\nAdd the following sentence to Section 8. Method Definitions:\n\n   Note that the Host request-header field (Section 10.22) must accompany\n   HTTP 1.1 requests.\n\nReplace the current 10.22 with the following:\n\n10.22  Host\n\n   The Host request-header field specifies the Internet host and port\n   number of the resource being requested, as obtained from the original\n   http URL (Section 3.2.2) given by the user or referring resource.\n   The Host field value must represent the network location of the\n   origin server or gateway given by the original http URL.  This allows\n   the origin server or gateway to differentiate between\n   internally-ambiguous URLs (such as the root \"/\" URL of a server\n   harboring multiple virtual hostnames).\n\n       Host = \"Host\" \":\" host [ \":\" port ]    ; see Section 3.2.2\n\n!  A \"host\" without any trailing port information implies the default\n!  port for the service requested (e.g., \"80\" for an http URL). \n!  For example, a request on the origin server\n   for <http://www.w3.org/pub/WWW/> must include:\n\n       GET /pub/WWW/ HTTP/1.1\n       Host: www.w3.org\n\n   The Host header field must be included in all HTTP/1.1 request\n   messages on the Internet (i.e., on any message corresponding to a\n   request for an http URL, as described in Section 3.2.2).  If the\n   Host field is not already present, an HTTP/1.1 proxy must add a Host\n   field to the request message prior to forwarding it on the Internet.\n   If an HTTP/1.1 request message lacking a Host header field is\n!  received via the Internet by an origin server or gateway, that server\n   must respond with a 400 status code.\n\n! (I eliminated the note that was in the draft, and added a list to Appendix D.)\n\nAdd to Appendix D (Changes from HTTP/1.0:\n\n!   D.1 Changes to Simplify Multi-homed Web Servers and Conserve IP Addresses\n\n!  The requirements that clients and servers support the Host \n   request-header, report an error if the Host request-header is \n   missing from an HTTP/1.1 request (Section 10.22), \n!  and accept absolute URIs (Section 5.1.2) are among the most important \n   changes from HTTP/1.0.\n\n!  In HTTP/1.0 there is a one-to-one relationship of IP addresses and servers.\n   There is no other way to disambiguate the server of a request\n   than the IP address of that request.\n   This change to HTTP will allow the Internet, once HTTP/1.0\n   clients and servers are no longer common, to support multiple Web sites \n   from a single IP address, greatly simplifying large operational Web \n!  servers sites, where allocation of many IP addresses to a single \n   host has created serious problems.  The Internet will also be \n   able to recover the IP addresses that have been used for the sole\n   purpose of allowing root-level domain names to be used in HTTP URLs.\n   Given the rate of growth of the Web, and the number of servers already\n!  deployed, it is extremely important that implementations of HTTP/1.1 \n+  correctly implement these new requirements:\n+o both clients and servers must support the Host request-header\n+o Host request-headers are required in HTTP 1.1 requests.\n+o servers must report an error if an HTTP/1.1 request does \n+not include a Host request-header\n+o servers must accept absolute URIs\n\n\n\n", "id": "lists-010-1055343"}, {"subject": "Re: HTTP working group status &amp; issue", "content": ">> - security \n>>    draft-ietf-wts-shttp-03.txt 'bounced' as Proposed Standard. Most\n>>    likely if it progresses at all it will go forward as Experimental,\n>>    after editorial work happens (or a disclaimer gets added.)\n>> \n>>    I think that the separation and lack of coordination between\n>>    WTS and HTTP-WG was a serious problem, and that I should have done\n>>    more to prevent this morass.\n>\n>Was this mis-coordination the real problem here?\n\nI don't think that mis-coordination was an issue at all. Once security was \nseparated from the main HTTP spec the end result was inevitable. S-HTTP has \nsevere problems and I don't know that its possible to fix them without a very \nradical change of approach. S-HTTP is simply too complex.\n\n\n>>    I believe the Internet needs documented standards-track security\n>>    mechanisms for HTTP & distributed content. Perhaps we should\n>>    re-charter a new group to work on standards for SSL and applet\n>>    signing.\n\n>Are you aware of the TLS working group which is aiming at a merged \n>SSL/PCT?\n\nWhere? according to the IETF the official mailing list for WTS is \nwww-security@rutgers. I can find no pointer anywhere to the list where the\nreal discussion has been going on. I think that that is in itself a serious\nproblem.\n\nMerging SSL and PCT is not a difficult matter and a good thing to do. I \nthink that overall this approach has a lot to offer the Internet \ngenerally and may well end up rendering the IP-SEC work moot. It is\nnot a panacea however. Working at the transport layer prevents any\nusefull interaction between the application and transport to create the\nappropriate security for a particular transaction. \n\nI think that keeping SSL and transaction layer security separate is essential. \nIf people attempt to force SSL to support application oriented features in the \ntransport layer the result will be an abomination.\n\n\nAs part of our project I had to create a scheme to allow two web servers to be \nkept in lockstep via authenticated HTTP exchanges. It provides the functionality \nof S-HTTP with the exception of negotiation but uses only five headers instead \nof twenty, does not involve ASN.1 and has a much cleaner interface to a \nnegotiation system. If anyone is interested I can finish the writeup and post \nit.\n\nI was planning to write an active-x control and experiment with the protocol \nincorporated into a browser. If anyone has experience of implementing such a \ncontrol or knows of an example I would be very pleased to know! Unfortunately \nMicrosoft's documentation is more forthcomming on what one can do with active-x \nthan how to do any of it.\n\nIf active-x makes any sense it is as a general purpose operating system level \n\"component-ware\" solution. If I don't like the features provided by the O/S \nprovided jpeg filter or HTTP transport I swap in my own. According to the \npropaganda it should provide me with an open testbed to experiment with \nextending the protocols without having to write a browser and server. It should \nbe possible for an end user to adopt my modified protocol without having to \nreplace his whole browser. Unrfortunately there is a general agreement amongst \nthose of us in this building that have tried to do such things that there is a \ndistinct lack of concrete examples in this area.\n\n\nPhill\n\n\n\n", "id": "lists-010-10559129"}, {"subject": "WordHTML &quot;Pipeline&quot; for HTTP/1.1 draf", "content": "A \"pipeline\" for converting the Word document of the spec. into HTML is\nattached (I needed this for internal reasons, but I thought it might be\nuseful generally).\n\nThis is not as easy as it sounds (unless someone knows something I don't) as\nInternet Assistant for Word wipes all the internal anchors on the section\nheadings if you've got an auto-generated table of contents in there. I even\nhad to lower myself to learning pidgin Word Basic macro language, as well as\nremembering my sed and awk.\n\nAlso, the internal structure of the spec has become terribly messy since\nit's been in Word. I spent a lot of time fixing all the internal links and\nanchors (there are 335 links in draft 7) and other things like bulletted and\nnumbered lists done inconsistently. I've carefully documented the internal\nchanges I've made below.\n\nThe pipeline isn't particularly generic and involves some manual\nintervention (it wouldn't do if I could be bothered to learn perl), but it\nworks and you get clean HTML out the other end.\n\nI've passed draft 7 through it, so when the RFC comes out, someone may wish\nto use this pipeline for that too.\n\nIf anyone wants, I can stick these on our ftp server:\na) A clean Word file of draft 7 with all the internal linkage ready for\nconversion.\nb) A clean HTML file of draft 7\n\nI would add that neither a) nor b) would produce straight text that would be\nidentical to the current plain/text draft 7. This is because:\n- Many of the internal links to the references became squashed out of the\nspec. at various stages, but still exist internal to the Word file data\nstructure, so I've re-revealed these.\n- I tried to avoid altering the linear white space, but probably failed.\n\nWhether this can still be called draft 7 is up for debate. This is why I\nhaven't made it available already, in case someone objects on a change\ncontrol basis.\n\nAttached is the process.\n\nBob\n\n================================================================================\n\nStep 0: Removed auto-generated table of contents\n (unfortunately this doesn't remove the anchors it refers to)\n\nStep 1: WINWORD: Unpicked bugs in bookmark naming and linkage as follows...\n\n* Heading 3.11 Entity Tags\nDouble bookmarked as \"Entity_Tags\" & \"Opaque_Tags\"\nRemoved latter\n* Heading 3.12 Range Units\nDouble bookmarked as \"Range_Units\" & \"Range_Protocol_Param\"\nRemoved latter\n* Heading 8.1.3 Proxy Servers\nNo bookmark\nAdded bookmark \"Persist_Proxy_Servers\"\n* Heading 9.1 Safe and Idempotent Methods\nNo bookmark\nAdded bookmark \"Safe_Idem_Methods\"\n* Heading 12.1 Server-driven Negotiation\nNo bookmark\nAdded bookmark \"Server_driven_Negotiation\"\n* Heading 12.2 Agent-driven Negotiation\nNo bookmark\nAdded bookmark \"Agent_driven_Negotiation\"\n* Heading 12.3 Transparent Negotiation\nNo bookmark\nAdded bookmark \"Transparent_Negotiation\"\n* Heading 13.3.2 Entity Tag Cache Validators\nwas bookmarked as \"Tags\" & nested within it was the\n\"Entity_Tag_Cache_Validators\" bookmark\nRemoved former and replaced it with latter\n* Heading 13.4 Response Cachability\nTriple bookmarked as \"Response_Cachability\", \"Caching_and_Status_C\" &\n\"Constructing_Respons\"\nRemoved all but first\n* Heading 13.6 Caching Negotiated Responses\nDouble bookmarked as \"Vary_Header_Use\" & \"Caching_and_Varying_\"\nRemoved latter\n* Section 14.4\nArbitrary sentence at end was bookmarked \"OLE_LINK8\"\nRemoved bookmark\n* Section 14.5\nWhole section was bookmarked as \"OLE_LINK1\"\nRemoved bookmark\n* Heading 14.9.2 What May be Stored by Caches\nBookmark ended a character early\nShifted bookmark end right\n* Heading 14.16 Content-MD5\nBookmark ended after para mark\nShifted bookmark end left\n* Heading 14.25 If-Match\nDouble bookmarked as \"If_Match\" & \"If_Valid\"\nRemoved latter\n* Heading 14.28 If-Unmodified-Since\nDouble bookmarked as \"If_Unmodified_Since\" & \"Unless_Modified_Sinc\"\nRemoved latter\n* Heading 14.44 Via\nDouble bookmarked as \"Via\" & \"Forwarded\"\nRemoved latter\n* Heading 15.2 Offering a Choice of Authentication Schemes \nNo bookmark\nAdded bookmark \"Choice_of_Authentication\"\n* Ref [26] Improving HTTP Latency\nNo bookmark\nAdded bookmark \"RefLatency\"\n* Ref [27] Analysis of HTTP Performance\nNo bookmark & URL was bookmarked as \"OLE_LINK2\"\nAdded bookmark named \"RefPerformance\" & Removed \"OLE_LINK2\"\n* Ref [29] , RFC 1951\nNo bookmark\nAdded bookmark \"Ref1951\"\n* Ref [30] Analysis of HTTP Performance Problems\nNo bookmark\nAdded bookmark \"RefPerfProbs\"\n* Ref [31] , RFC 1950\nNo bookmark\nAdded bookmark \"Ref1950\"\n* Ref [32] Work In Progress for Digest authentication\nNo bookmark, but \"DigestRef\" bookmark spanned refs [26] & [27]\nMoved \"DigestRef\" bookmark to ref [32] \n* Heading 18. Authors\n\"Authors\" bookmark spanned previous section\nMoved start of bookmark to start of heading\n* Roy T. Fielding hyper link\nduplicated internal to macrobutton\nOpened up with <SHIFT>F9 & removed one of two\n* Heading 19.4 Differences Between HTTP Entities and RFC 1521 Entities \nBookmark ended a word early\nShifted bookmark end right\n* Heading 19.5.1 Changes to Simplify Multi-homed Web Servers and Conserve IP\nAddresses\nwas bookmarked as \"Changes_For_Host_Support\" and \"OLE_LINK4\" & nested within\nit was the \"AppHost\" bookmark\nRemoved last two bookmarks\n* Heading 19.8 Notes to the RFC Editor and IANA\nBookmark \"Notes_to_RFC_Editor\" spanned previous two sections\nMoved start of bookmark to start of heading\n* Heading 19.8.1 Charset Registry\nNo bookmarks\nAdded bookmark \"Charset_Registry\"\n* Heading 19.8.2 Content-coding Values\nNo bookmarks\nAdded bookmark \"Content_coding_Values\"\n* Heading 19.8.3 New Media Types Registered\nNo bookmarks\nAdded bookmark \"New_Media_Types_Registered\"\n* Heading 19.8.4 Possible Merge With Digest Authentication Draft\nNo bookmarks\nAdded bookmark \"Possible_Merge_With_Digest_Authen\"\n* Heading 19.8.5 Media type parameters named \"q\"\nNo bookmarks\nAdded bookmark \"Media_type_parameters_named_q\"\n\nStep 2: WINWORD Fix bullet lists and numbered lists that\n have been done manually rather than as a style\n\nStep 3: WINWORD: Add internal hyperlinks to refs. from all [xx] references,\n including ones invisible within Word datastructure (revealed with <ALT><F9>)\n\nSave (http117a.doc)\n\nStep 4: WINWORD: Correct typos accepted by working group (haven't done this)\n\nSave ()\n\nStep 5: WINWORD: Run my TypeBkmks Word Macro\n\nStep 6: WINWORD: Format Heading Numbering Remove\n\nSave (http117b.doc)\n\nStep 7: WINWORD: Save As http117b.htm with Internet Assistant v2.03z\n\nStep 8: UNIX: dos2unix http117b.htm http117c.htm\nStep 9: UNIX: sed -f sedfile http117c.htm > http117d.htm\n\n================================================================================\nsedfile\n================================================================================\ns!<A NAME=\"_Toc[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\">{{A NAME={{!<A\nNAME=\"!gp\ns!{{A NAME={{.*}}!!gp\ns!}}!\">!gp\ns!<A NAME=\"_Toc[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\">\\(.*\\)</A>!\\1!gp\ns!<FONT [^>]*>!!gp\ns!</FONT>!!gp\n/<H[2-8]><A NAME=/ { N\ns!<A NAME=!  <A NAME=!\ns!\\n! !\ns!</H!\\\n</H!gp\n}\n================================================================================\n\nStep 10: TEXT EDITOR:\nDealt with any remaining anchors split across lines containing '{{' manually.\nRemoved six remaining _Toc anchors manually\nAdded another </H1> at end of title to match second <H1> at start\nRemoved some spurious <B><LWS></B> and altered three top <B> headings to <H2>\n\nStep 11: UNIX: awk -f awkfile d.htm > e.htm\n\n================================================================================\nawkfile\n================================================================================\nBEGIN { th2 = \"\"; th3 = \"\"; th4 = \"\"; th5 = \"\";\np2 = \". \"; p3 = \" \"; p4 = \" \"; p5 = \" \"}\n/<H2>  <A NAME=/ { th2 = ++h2;\nif (h2 > 9) p2 = \".\";\n$1 = $1 th2 p2;\nh3 = 0; h4 = 0; h5 = 0;\np3 = \" \"; p4 = \" \"; p5 = \" \"}\n/<H3>  <A NAME=/ { th3 = ++h3;\nif (h3 > 9) p3 = \"\";\n$1 = $1 th2 \".\" th3 p3;\nh4 = 0; h5 = 0;\np4 = \" \"; p5 = \" \"}\n/<H4>  <A NAME=/ { th4 = ++h4;\nif (h4 > 9) p4 = \"\";\n$1 = $1 th2 \".\" th3 \".\" th4 p4;\nh5 = 0;\np5 = \" \"}\n/<H5>  <A NAME=/ { th5 = ++h5;\nif (h5 > 9) p5 = \"\";\n$1 = $1 th2 \".\" th3 \".\" th4 \".\" th5 p5}\n {print}\n================================================================================\n\nStep 12: UNIX: sed -n -f sedfile2 http117e.htm > contents.htm\n\n================================================================================\nsedfile2\n================================================================================\n/^<H[2-4]>[0-9.]*  *<A NAME=/ {s!<A NAME=\"!<A HREF=\"#!\ns!^<H2>!!\ns!^<H3>!    !\ns!^<H4>\\([0-9]\\.\\)!         \\1!\ns!^<H4>\\([0-9][0-9]\\)!          \\1!\ns!</H[2-4].*>!!\np\n}\n================================================================================\n\nStep 13: TEXT EDITOR: Paste contents.htm into http117e.htm surrounded by\n<PRE></PRE>\n\nStep 14: Learn to avoid using Microsoft Internet Assistant for anything complex\n\nStep 15: Say phew\n================================================================================\n____________________________________________________________________________\nFrom:    Bob Briscoe,                                BT, Distributed Systems\nPost:    B54 74, BT Labs,    Martlesham Heath,   Ipswich, IP5 7RE,   England\nE-Mail:  rbriscoe@jungle.bt.co.uk\nTel:     +44 1473 645196                                Fax: +44 1473 640929\nWWW:     http://www.jungle.bt.co.uk/people/rbriscoe.html  (BT intranet only)\n\n\n\n", "id": "lists-010-10570670"}, {"subject": "Re: HTTP working group status &amp; issue", "content": "I will refrain from responding to your various dogmantic and absolutist \nstatements other than correcting one clear error.\n\nOn Thu, 17 Oct 1996 hallam@vesuvius.ai.mit.edu wrote: \n\n> Date: Thu, 17 Oct 96 11:13:54 -0400\n> From: hallam@vesuvius.ai.mit.edu\n> To: \"Donald E. Eastlake 3rd\" <dee@cybercash.com>,\n>     http-wg@cuckoo.hpl.hp.com\n> Cc: hallam@vesuvius.ai.mit.edu\n> Subject: Re: HTTP working group status & issues \n> \n> \n> >> - security \n> >>    ...\n> ...\n> \n> >>    I believe the Internet needs documented standards-track security\n> >>    mechanisms for HTTP & distributed content. Perhaps we should\n> >>    re-charter a new group to work on standards for SSL and applet\n> >>    signing.\n> \n> >Are you aware of the TLS working group which is aiming at a merged \n> >SSL/PCT?\n> \n> Where? according to the IETF the official mailing list for WTS is \n> www-security@rutgers. I can find no pointer anywhere to the list where the\n> real discussion has been going on. I think that that is in itself a serious\n> problem.\n\nNot the WTS working group at www-security@nsmx.rutgers.edu, but as I said,\nthe *TLS* working group, whose mailing list is at ietf-tls@w3.org with\narchives at http://lists.w3.org/Archives/Public/ietf-tls.  This information\nis not obscure but easily findable from the IETF web page. \n\n(I have not been active in the TLS working group.)\n\n> ...\n> \n> Phill\n\nDonald\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\nhttp://www.cybercash.com           http://www.eff.org/blueribbon.html\n\n\n\n", "id": "lists-010-10588252"}, {"subject": "13.1.2 Warning", "content": "> - HTTP/1.1 & digest:\n>    Expecting RFC Real Soon Now.\n>    Complaints, editorial advice, ambiguities welcome.\n\n13.1.2 Warnings says:\n\n    ...\n    Warnings are always cachable, because they never weaken the transparency\n    of a response. This means that warnings can be passed to HTTP/1.0 caches\n    without danger; such caches will simply pass the warning along as an\n    entity-header in the response.\n    ...\n\nThis is not right.  HTTP/1.0 cache will cache this header, and the\nWarning will remain in the cache file even if the entity is up-to-date\nchecked later.  So clients could e.g. see a warning saying that the\nresponse may be stale even if the proxy just did an up-to-date check\nand it was ok.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-10600402"}, {"subject": "Re: 13.1.2 Warning", "content": "Ari Luotonen wrote:\n> \n> \n> > - HTTP/1.1 & digest:\n> >    Expecting RFC Real Soon Now.\n> >    Complaints, editorial advice, ambiguities welcome.\n> \n> 13.1.2 Warnings says:\n> \n>     ...\n>     Warnings are always cachable, because they never weaken the transparency\n>     of a response. This means that warnings can be passed to HTTP/1.0 caches\n>     without danger; such caches will simply pass the warning along as an\n>     entity-header in the response.\n>     ...\n> \n> This is not right.  HTTP/1.0 cache will cache this header, and the\n> Warning will remain in the cache file even if the entity is up-to-date\n> checked later.  So clients could e.g. see a warning saying that the\n> response may be stale even if the proxy just did an up-to-date check\n> and it was ok.\n\nWouldn't that mean that the HTTP/1.0 cache was out-of-date wrt the upstream\ncache (and hence would refetch and lose the warning header)? Or am I missing\nsomething?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10609272"}, {"subject": "Re: 13.1.2 Warning", "content": ">    Warnings are always cachable, because they never weaken the transparency\n>    of a response. This means that warnings can be passed to HTTP/1.0 caches\n>    without danger; such caches will simply pass the warning along as an\n>    entity-header in the response.\n>    ...\n>\n>This is not right.  HTTP/1.0 cache will cache this header, and the\n>Warning will remain in the cache file even if the entity is up-to-date\n>checked later.  So clients could e.g. see a warning saying that the\n>response may be stale even if the proxy just did an up-to-date check\n>and it was ok.\n\nWhat part is not right?   \"never weaken the transparency\" is right.  A\nwarning that the thing is stale even if it's not, doesnt weaken\ntransparency. \"without danger\" might not be right if you use an extremely\nliberal definition of danger.\n\n-----\nDaniel DuBois\nI travel, I code, I'm a Traveling Coderman         \nhttp://www.spyglass.com/~ddubois/\n\n\n\n", "id": "lists-010-10618334"}, {"subject": "Re: 13.1.2 Warning", "content": "Ben Laurie wrote:\n> \n> Ari Luotonen wrote:\n> > \n> > \n> > > - HTTP/1.1 & digest:\n> > >    Expecting RFC Real Soon Now.\n> > >    Complaints, editorial advice, ambiguities welcome.\n> > \n> > 13.1.2 Warnings says:\n> > \n> >     ...\n> >     Warnings are always cachable, because they never weaken the transparency\n> >     of a response. This means that warnings can be passed to HTTP/1.0 caches\n> >     without danger; such caches will simply pass the warning along as an\n> >     entity-header in the response.\n> >     ...\n> > \n> > This is not right.  HTTP/1.0 cache will cache this header, and the\n> > Warning will remain in the cache file even if the entity is up-to-date\n> > checked later.  So clients could e.g. see a warning saying that the\n> > response may be stale even if the proxy just did an up-to-date check\n> > and it was ok.\n> \n> Wouldn't that mean that the HTTP/1.0 cache was out-of-date wrt the upstream\n> cache (and hence would refetch and lose the warning header)? Or am I missing\n> something?\n\nBefore anyone else brings me down to Earth, I'll admit I am missing something.\nOf course, the upstream cache may be a different one next time, or any one of\na whole host of possibilities too tedious to enumerate. This would appear to\nbe a problem.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10626885"}, {"subject": "Re: 13.1.2 Warning", "content": "Daniel DuBois wrote:\n> \n> >    Warnings are always cachable, because they never weaken the transparency\n> >    of a response. This means that warnings can be passed to HTTP/1.0 caches\n> >    without danger; such caches will simply pass the warning along as an\n> >    entity-header in the response.\n> >    ...\n> >\n> >This is not right.  HTTP/1.0 cache will cache this header, and the\n> >Warning will remain in the cache file even if the entity is up-to-date\n> >checked later.  So clients could e.g. see a warning saying that the\n> >response may be stale even if the proxy just did an up-to-date check\n> >and it was ok.\n> \n> What part is not right?   \"never weaken the transparency\" is right.  A\n> warning that the thing is stale even if it's not, doesnt weaken\n> transparency. \"without danger\" might not be right if you use an extremely\n> liberal definition of danger.\n\nPerhaps we have different ideas of what \"transparency\" means, but it seems to\nme that receiving an entity with an erroneous header is not transparent. It\ncould certainly lead to undesirable behaviour on the part of downstream 1.1\ncaches, such as periodically trying to refetch the \"stale\" entity (and, of\ncourse, getting another stale one).\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10636377"}, {"subject": "Re: 13.1.2 Warning", "content": "> > What part is not right?   \"never weaken the transparency\" is right.  A\n> \n> Perhaps we have different ideas of what \"transparency\" means, but it seems to\n> me that receiving an entity with an erroneous header is not transparent. It\n> could certainly lead to undesirable behaviour on the part of downstream 1.1\n> caches, such as periodically trying to refetch the \"stale\" entity (and, of\n> course, getting another stale one).\n\nAgree with Ben.  I think it's undesirable if I get a Warning header\nwhen there it's not suppose to be there.\n\nSay I hav document X, last-modified at Y.  The document is cached in\nHTTP/1.1 cache A.  An HTTP/1.0 cache B requests for this document X\nfrom the cache A.  Cache A returns it directly from its cache without\nan up-to-date check, and attaches the Warning header of that fact.\nCache B caches the Warning header.\n\nLater cache B receives another request for the document X and\ndetermines it's time to do an up-to-date check.  Cache A has also\nreached the time to do an up-to-date check.  So an up-to-date check is\nmade, and the remote server reponds that X has not been modified since\ndate Y, and is hence still up-to-date.\n\nNow, cache A responds to B saying B's copy is up-to-date.\nUnfortunately, the Warning header will (erroneously) remain in the\ncache of B and will be passed to the user, forever giving the\nperception that the returned copy may be stale, even though in this\ncase it was guaranteed to be up-to-date.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-10646034"}, {"subject": "Re: 13.1.2 Warning", "content": ">Perhaps we have different ideas of what \"transparency\" means, but it seems to\n>me that receiving an entity with an erroneous header is not transparent. It\n\nOK, you are right.  I was thinking about transparancy of the entity body.\nBut in the HTTP spec, semantic transparency is defined as 'exactly the same\nresponse (except for hop-by-hop headers)'.  Warning, although added in a\nmiddle hop, I dont believe can be considered a hop-by-hop header.\n\nSeems clear the authors of that section of the spec must have meant\ntransparency of the entity body, since it's impossible to have a\nWarning:stale header originate from an origin server.  By definition, a\nWarning:stale header means its not transparent.\n\nHmmmm.... Can we really guarantee semantic transparency of headers?\nCertainly there must be some headers that can change without the resource\nchanging, and that won't be reported on the 304 response.  If I reconfigure\nmy server to start adding \"Cache-control: private\" to all responses, but\ndont modify foobar.txt, my If-Modifed-Since might still return 304 despite\nthe fact that truly semantic transparent headers would be different than the\ndownstream cache has.  Oh well - implementation issue.  (Keep last-restart\ndate of server and compare additionally compare that date against IMS times.)\n\n>could certainly lead to undesirable behaviour on the part of downstream 1.1\n>caches, such as periodically trying to refetch the \"stale\" entity (and, of\n>course, getting another stale one).\n\nWell, 'undesirable behavior' is a different beast (and one that's often at\nodds with semantic tranparency).\n\n-----\nDaniel DuBois\nI travel, I code, I'm a Traveling Coderman         \nhttp://www.spyglass.com/~ddubois/\n\n\n\n", "id": "lists-010-10655824"}, {"subject": "Re: (HOST) (FULLURL) draft of changes for W.G. review", "content": "> To Roy's rewrite of 10.22 Host he circulated a while back (which looks pretty\n> good to me), I've added a few other sentences in other sections, \n> and drafted a section to the Changes from HTTP/1.0 appendix.\n> This also resolves the FULLURL issue, so that we have a transition plan\n> to full URL's in some future HTTP version.\n\nOoops, it looks like you missed the last version (23 Mar) that included the\nchanges suggested by Lou.  Here is the final section for Host, also including\nPaul Hoffman's change (no vanity) with your Note added as well:\n\n========================================================================\n10.22  Host\n\n   The Host request-header field specifies the Internet host and port\n   number of the resource being requested, as obtained from the original\n   URL given by the user or referring resource (generally an http URL,\n   as described in Section 3.2.2).  The Host field value must represent\n   the network location of the origin server or gateway given by the\n   original URL.  This allows the origin server or gateway to\n   differentiate between internally-ambiguous URLs, such as the root \"/\"\n   URL of a server for multiple hostnames on a single IP address.\n\n       Host = \"Host\" \":\" host [ \":\" port ]    ; see Section 3.2.2\n\n   A \"host\" without any trailing port information implies the default\n   port for the service requested (e.g., \"80\" for an http URL).  For\n   example, a request on the origin server for\n   <http://www.w3.org/pub/WWW/> must include:\n\n       GET /pub/WWW/ HTTP/1.1\n       Host: www.w3.org\n\n   The Host header field must be included in all HTTP/1.1 request\n   messages on the Internet (i.e., on any message corresponding to a\n   request for a URL which includes an Internet host address for the\n   service being requested).  If the Host field is not already present,\n   an HTTP/1.1 proxy must add a Host field to the request message prior\n   to forwarding it on the Internet.  All Internet-based HTTP/1.1\n   servers must respond with a 400 status code to any HTTP/1.1 request\n   message which lacks a Host header field.\n\n      Note: It is extremely important that HTTP/1.1 clients use the Host\n      request-header and that HTTP/1.1 servers both accept absoluteURI's\n      and report errors if the Host request-header is not received with \n      an HTTP/1.1 request (see Appendix D.1).\n\n========================================================================\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1065839"}, {"subject": "Re: 13.1.2 Warning", "content": ">Agree with Ben.  I think it's undesirable if I get a Warning header\n>when there it's not suppose to be there.\n\nI might agree with you that it's undesirable.  But if I was Jeff Mogul I'd\nretort lengthily how this insures that users always get the right document.\nThen if I was Roy Fielding I'd jump on Jeff for destroying the entire New\nZealand network and making his aunt pay huge network charges.\n\n>Say I hav document X, last-modified at Y.  The document is cached in\n>[...]\n>case it was guaranteed to be up-to-date.\n\nThis analysis is completely correct.  Caches will misbehave on the side of\ndocument accuracy, to a fault.\n\nBut given that I've never seen a browser that handles caching/history\nbuffers the way I'd like (which changes on the situation), I'm not going to\nlose sleep over a 1.0 cache that erroneously/needlessly re-requests\ndocuments.  But that's just me.\n\nLarry, it would seem a slight rewording of the warning Note: would be\nappropriate given the wording of the definition of semantic transparancy.\n\n-----\nDaniel DuBois\nI travel, I code, I'm a Traveling Coderman         \nhttp://www.spyglass.com/~ddubois/\n\n\n\n", "id": "lists-010-10665659"}, {"subject": "Re: 13.1.2 Warning", "content": "You've made a convincing case that the existing design for Warning\ncan yield bogus Warnings when HTTP/1.0 and HTTP/1.1 caches are\ncombined.  (I believe we wrote the HTTP/1.1 caching rules so that\nan HTTP/1.1 cache in the position of your cache \"B\" would remove\nthe Warning after doing a successful validation from cache \"A\",\nbut of course it's too late to apply that to the HTTP/1.0 caches\nout there.)\n\nSo please suggest a solution!\n\nI can see three options:\n(1) Live with it.  This can only happen when an HTTP/1.0\ncache is a client of an HTTP/1.1 cache, and presumably\nin somewhat unusual cases, so maybe it's a temporary\nproblem.\n(2) Remove Warning: stale from the protocol, on the grounds\nthat it's better to silently give many users stale pages,\ninstead of bogusly warning a few users about non-stale pages.\n[I don't consider this option to be a wise choice.]\n(3) Fix the design so that it works with HTTP/1.0 caches.\nPerhaps, for example, this means that one can't send a\n\"Warning\" to an HTTP/1.0 client (but this would also\ncause a lack of Warning in cases where it would be\napppropriate).\n\nOne possibility: HTTP/1.1 clients (the only ones that could\ncare about a Warning header anyway) should turn a Reload on\na page with a \"Warning: stale\" into a \"Pragma: no-cache\".  That\nwould cause a few extra cache misses, but would break the\ninfinite loop that you are worried about.\n\n-Jeff\n\n\n\n", "id": "lists-010-10674629"}, {"subject": "Re: 13.1.2 Warning", "content": "> Larry, it would seem a slight rewording of the warning Note: would be\n> appropriate given the wording of the definition of semantic transparancy.\n\nWe're not going to change the HTTP/1.1 RFC at this point: it's\nimportant to let it progress, with the understanding that it's a\nProposed Standard. I'm not really worried about jeopardizing the\n'Draft Standard' status of a revision, either.\n\nThe main concern I have is that we keep a complete and coherent list\nof editorial revisions. The mailing list archive isn't sufficient,\nsince the traffic level is high. I'm soliciting for a donation of\n'forum' space on someone's distributed web collaboration tool that\nwould let us maintain this. Any donors?\n\nLarry\n\n\n\n", "id": "lists-010-10683220"}, {"subject": "Re: 13.1.2 Warning", "content": "Jeffrey Mogul wrote:\n> \n> You've made a convincing case that the existing design for Warning\n> can yield bogus Warnings when HTTP/1.0 and HTTP/1.1 caches are\n> combined.  (I believe we wrote the HTTP/1.1 caching rules so that\n> an HTTP/1.1 cache in the position of your cache \"B\" would remove\n> the Warning after doing a successful validation from cache \"A\",\n> but of course it's too late to apply that to the HTTP/1.0 caches\n> out there.)\n> \n> So please suggest a solution!\n> \n> I can see three options:\n> (1) Live with it.  This can only happen when an HTTP/1.0\n> cache is a client of an HTTP/1.1 cache, and presumably\n> in somewhat unusual cases, so maybe it's a temporary\n> problem.\n\nSomething you'd have to live with for a long time, I suspect. Not an acceptable\noption, IMHO.\n\n> (2) Remove Warning: stale from the protocol, on the grounds\n> that it's better to silently give many users stale pages,\n> instead of bogusly warning a few users about non-stale pages.\n> [I don't consider this option to be a wise choice.]\n\nAgreed.\n\n> (3) Fix the design so that it works with HTTP/1.0 caches.\n> Perhaps, for example, this means that one can't send a\n> \"Warning\" to an HTTP/1.0 client (but this would also\n> cause a lack of Warning in cases where it would be\n> apppropriate).\n\nThis would seem the appropriate solution. Not giving a Warning where the client\ndoesn't understand it would seem a small price to pay.\n\n> \n> One possibility: HTTP/1.1 clients (the only ones that could\n> care about a Warning header anyway) should turn a Reload on\n> a page with a \"Warning: stale\" into a \"Pragma: no-cache\".  That\n> would cause a few extra cache misses, but would break the\n> infinite loop that you are worried about.\n\nI prefer the idea of leaving the Warning out. I see no gain from forcibly\nremoving the stale entry from downstream caches.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10691850"}, {"subject": "Re: HTTP working group status &amp; issue", "content": "Folks,\n\nI said:\n\n>>    I think that the separation and lack of coordination between\n>>    WTS and HTTP-WG was a serious problem, and that I should have done\n>>    more to prevent this morass.\n\nI did not mean to start a debate on what the 'real' problem was or the\n'most important problem', I just meant to say that 'no coordination'\nis a problem.\n\nTo make this forward-looking rather than backward-looking, if there's\nan ongoing effort to merge SSL and PCT, that's fine, and we don't need\n(in HTTP-WG) to be concerned with it.\n\nWhat I'm concerned about as 'undocumented' is the entire scheme of the\nintegration of \"shttp:\" URLs, warnings about secure forms and insecure\nforms, interactions with other parts of HTTP and cookies,\netc. Ultimately these are important to document and standardize.\n\nRegards,\n\nLarry\n \n\n\n\n", "id": "lists-010-10702225"}, {"subject": "Re: 13.1.2 Warning", "content": "> > (3) Fix the design so that it works with HTTP/1.0 caches.\n> > Perhaps, for example, this means that one can't send a\n> > \"Warning\" to an HTTP/1.0 client (but this would also\n> > cause a lack of Warning in cases where it would be\n> > apppropriate).\n> \n> This would seem the appropriate solution. Not giving a Warning where\n> the client doesn't understand it would seem a small price to pay.\n\nSeconded.  This solution will maintain things correct.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-10711064"}, {"subject": "Re: 13.1.2 Warning", "content": "Larry Masinter:\n>\n[...]\n>\n>The main concern I have is that we keep a complete and coherent list\n>of editorial revisions. The mailing list archive isn't sufficient,\n>since the traffic level is high. I'm soliciting for a donation of\n>'forum' space on someone's distributed web collaboration tool that\n>would let us maintain this. Any donors?\n\nIn a previous life, I created a distributed web collaboration tool.  I\nhave seen people collaborate with it.  Trust me, having some forum\nspace will _not_ get us a complete and coherent list of (calls for)\neditorial revisions.  In my opinion, the _only_ way in which we will\nget such a list is if one person publicly commits to maintaining it.\n\n>Larry\n\nKoen.\n\n\n\n", "id": "lists-010-10719460"}, {"subject": "Re: 13.1.2 Warning", "content": "Larry Masinter wrote:\n> \n> > Larry, it would seem a slight rewording of the warning Note: would be\n> > appropriate given the wording of the definition of semantic transparancy.\n> \n> We're not going to change the HTTP/1.1 RFC at this point: it's\n> important to let it progress, with the understanding that it's a\n> Proposed Standard. I'm not really worried about jeopardizing the\n> 'Draft Standard' status of a revision, either.\n> \n> The main concern I have is that we keep a complete and coherent list\n> of editorial revisions. The mailing list archive isn't sufficient,\n> since the traffic level is high. I'm soliciting for a donation of\n> 'forum' space on someone's distributed web collaboration tool that\n> would let us maintain this. Any donors?\n\nIf no web-based tools are available (and, to be honest, I doubt the state of\nthe art is such that we would all be able to use it with ease) can I suggest\nCVS?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10727889"}, {"subject": "Re: 13.1.2 Warning", "content": "Ben Laurie:\n>Jeffrey Mogul wrote:\n>> \n>> You've made a convincing case that the existing design for Warning\n>> can yield bogus Warnings when HTTP/1.0 and HTTP/1.1 caches are\n>> combined.  (I believe we wrote the HTTP/1.1 caching rules so that\n>> an HTTP/1.1 cache in the position of your cache \"B\" would remove\n>> the Warning after doing a successful validation from cache \"A\",\n>> but of course it's too late to apply that to the HTTP/1.0 caches\n>> out there.)\n>> \n>> So please suggest a solution!\n>> \n>> I can see three options:\n>>       (1) Live with it.  This can only happen when an HTTP/1.0\n>>       cache is a client of an HTTP/1.1 cache, and presumably\n>>       in somewhat unusual cases, so maybe it's a temporary\n>>       problem.\n>\n>Something you'd have to live with for a long time, I suspect. Not an\n>acceptable option, IMHO.\n\nI think (1) is the most acceptable solution of the three.  At some\ntime, we need to add a note to the spec telling that 1.1 clients\nshould be aware that 1.0 caches could send erroneous staleness related\nWarnings. The note should outline a method to deal with this.\n\nIt is not very bad if that method causes a few cache misses.  Even if\nchains with 1.0 caches in them will become common, responses with\nWarnings won't, so the impact on overall efficiency should be small.\n\nKoen.\n\n\n\n", "id": "lists-010-10736974"}, {"subject": "Re: 13.1.2 Warning", "content": "Koen Holtman wrote:\n> \n> Ben Laurie:\n> >Jeffrey Mogul wrote:\n> >> \n> >> You've made a convincing case that the existing design for Warning\n> >> can yield bogus Warnings when HTTP/1.0 and HTTP/1.1 caches are\n> >> combined.  (I believe we wrote the HTTP/1.1 caching rules so that\n> >> an HTTP/1.1 cache in the position of your cache \"B\" would remove\n> >> the Warning after doing a successful validation from cache \"A\",\n> >> but of course it's too late to apply that to the HTTP/1.0 caches\n> >> out there.)\n> >> \n> >> So please suggest a solution!\n> >> \n> >> I can see three options:\n> >>       (1) Live with it.  This can only happen when an HTTP/1.0\n> >>       cache is a client of an HTTP/1.1 cache, and presumably\n> >>       in somewhat unusual cases, so maybe it's a temporary\n> >>       problem.\n> >\n> >Something you'd have to live with for a long time, I suspect. Not an\n> >acceptable option, IMHO.\n> \n> I think (1) is the most acceptable solution of the three.  At some\n> time, we need to add a note to the spec telling that 1.1 clients\n> should be aware that 1.0 caches could send erroneous staleness related\n> Warnings. The note should outline a method to deal with this.\n> \n> It is not very bad if that method causes a few cache misses.  Even if\n> chains with 1.0 caches in them will become common, responses with\n> Warnings won't, so the impact on overall efficiency should be small.\n\nIt doesn't cause cache misses. It causes downstream 1.1 caches to operate\nincorrectly. The solution (3) which works doesn't cause cache misses, either.\nIt causes clients downstream of a 1.0 cache to be unaware that a 1.1 cache\nupstream of the 1.0 cache has served a stale document (which is no worse than\nthey would have got if directly connected to a 1.0 cache which had failed to\nfetch the entity). In short, if you use method 3 to solve the problem, you\nget service that is only as bad as you'd expect from a 1.0 cache. If you use\nmethod 1, you poison downstream 1.1 caches.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10746033"}, {"subject": "HTTP Log file", "content": "I have seen where groups are publishing specWeb96 numbers approaching 1000 URLs per second.  Using the common log file format and assuming an average entry of 100 bytes this translates to over 8 Gigabytes of data in a 24 hour period.  \n\nGiven the importance of tracking hit information with respect to advertising revenue and in some cases user profiling, what are servers doing with the size of this log information?  \n\nIs the hit-metering draft that I have seen reference to going to address this?\n\nIs there a more appropriate forum for a question such as this?\n\n-Doug\n\n\n\n", "id": "lists-010-10756543"}, {"subject": "(HOST) (FULLURL) consensus wording. (try, try again", "content": "If at first you don't succeed, try try again.  My mail and Roy's passed\nin the night; this version has his 10.22 incorporated, (by cut and paste).\n\n- Jim Gettys\n\n\nAdd to section 5.1.2 Request-URI\n   To allow for transition to absoluteURIs in all requests in future versions\n   of HTTP, HTTP/1.1 servers must accept the absoluteURI form in requests, \n!  even though HTTP/1.1 clients will not normally generate them.\n!  Versions of HTTP after HTTP/1.1 may require absoluteURIs everywhere,\n!  after HTTP/1.1 or later have become the dominant implementations.\n\nChange the sentence:\n   The absoluteURI form is only allowed when the request is being made to\n   a proxy.\nTo:\n   The absoluteURI form is required when the request is being made to\n   a proxy.  The absoluteURI form is only allowed to an origin server\n   if the client knows the server supports HTTP/1.1 or later.\n!  If the absoluteURI form is used, any Host request-header included\n!  with the request must be ignored.\n\nAdd the following sentence to Section 8. Method Definitions:\n\n   Note that the Host request-header field (Section 10.22) must accompany\n   HTTP 1.1 requests.\n\nReplace the current 10.22 with the following (From Roy's mail of 1 april):\n\n10.22  Host\n\n   The Host request-header field specifies the Internet host and port\n   number of the resource being requested, as obtained from the original\n   URL given by the user or referring resource (generally an http URL,\n   as described in Section 3.2.2).  The Host field value must represent\n   the network location of the origin server or gateway given by the\n   original URL.  This allows the origin server or gateway to\n   differentiate between internally-ambiguous URLs, such as the root \"/\"\n   URL of a server for multiple hostnames on a single IP address.\n\n       Host = \"Host\" \":\" host [ \":\" port ]    ; see Section 3.2.2\n\n   A \"host\" without any trailing port information implies the default\n   port for the service requested (e.g., \"80\" for an http URL).  For\n   example, a request on the origin server for\n   <http://www.w3.org/pub/WWW/> must include:\n\n       GET /pub/WWW/ HTTP/1.1\n       Host: www.w3.org\n\n   The Host header field must be included in all HTTP/1.1 request\n   messages on the Internet (i.e., on any message corresponding to a\n   request for a URL which includes an Internet host address for the\n   service being requested).  If the Host field is not already present,\n   an HTTP/1.1 proxy must add a Host field to the request message prior\n   to forwarding it on the Internet.  All Internet-based HTTP/1.1\n   servers must respond with a 400 status code to any HTTP/1.1 request\n   message which lacks a Host header field.\n\n! (I eliminated the note that was in the draft, and added a list to Appendix D.)\n\nAdd to Appendix D (Changes from HTTP/1.0:\n\n!   D.1 Changes to Simplify Multi-homed Web Servers and Conserve IP Addresses\n\n!  The requirements that clients and servers support the Host \n   request-header, report an error if the Host request-header is \n   missing from an HTTP/1.1 request (Section 10.22), \n!  and accept absolute URIs (Section 5.1.2) are among the most important \n   changes from HTTP/1.0.\n\n!  In HTTP/1.0 there is a one-to-one relationship of IP addresses and servers.\n   There is no other way to disambiguate the server of a request\n   than the IP address of that request.\n   This change to HTTP will allow the Internet, once HTTP/1.0\n   clients and servers are no longer common, to support multiple Web sites \n   from a single IP address, greatly simplifying large operational Web \n!  servers sites, where allocation of many IP addresses to a single \n   host has created serious problems.  The Internet will also be \n   able to recover the IP addresses that have been used for the sole\n   purpose of allowing root-level domain names to be used in HTTP URLs.\n   Given the rate of growth of the Web, and the number of servers already\n!  deployed, it is extremely important that implementations of HTTP/1.1 \n+  correctly implement these new requirements:\n+o both clients and servers must support the Host request-header\n+o Host request-headers are required in HTTP 1.1 requests.\n+o servers must report an error if an HTTP/1.1 request does \n+not include a Host request-header\n+o servers must accept absolute URIs\n\n\n------- End of Forwarded Message\n\n\n\n", "id": "lists-010-1076248"}, {"subject": "Re: 13.1.2 Warning", "content": "In response to my option #1:\n> (1) Live with it.  This can only happen when an HTTP/1.0\n> cache is a client of an HTTP/1.1 cache, and presumably\n> in somewhat unusual cases, so maybe it's a temporary\n> problem.\n\nYou replied:\n    Something you'd have to live with for a long time, I suspect. Not\n    an acceptable option, IMHO.\nI.e., you expect this kind of situation to persist for a while:\n\n [Server]---[HTTP/1.1 cache]---[HTTP/1.0 cache]---[HTTP/1.1 client]\n    \nBut then, regarding my suggestion to not send Warning to an HTTP/1.0\nclient,\n    This would seem the appropriate solution. Not giving a Warning\n    where the client doesn't understand it would seem a small price\n    to pay.\n\nBut in the situation that is of interest, this is not a case\nwhere the *end-client* doesn't understand the Warning (or else\nit wouldn't be an issue).  It's a case where a proxy *between*\ntwo systems that understand Warning doesn't understand it.\nSo we're back to risking \"undetected stale pages\" in a situation\nwhere we could, in fact, detect them.\n\n    > One possibility: HTTP/1.1 clients (the only ones that could\n    > care about a Warning header anyway) should turn a Reload on\n    > a page with a \"Warning: stale\" into a \"Pragma: no-cache\".  That\n    > would cause a few extra cache misses, but would break the\n    > infinite loop that you are worried about.\n    \n    I prefer the idea of leaving the Warning out. I see no gain from\n    forcibly removing the stale entry from downstream caches.\n\nThe main point of my previous message was not to enumerate the\ncomplete set of possible solutions, but to prod the people who\nare complaining about the current situation to suggest some\nalternative that isn't worse than the problem it purports to\nsolve.\n\nSending \"Pragma: no-cache\" might not be the best solution.  I\nstrongly encourage people to improve upon it.  Be creative!\n\n-Jeff\n\n\n\n", "id": "lists-010-10764068"}, {"subject": "Re: HTTP Log file", "content": "    I have seen where groups are publishing specWeb96 numbers\n    approaching 1000 URLs per second.  Using the common log file format\n    and assuming an average entry of 100 bytes this translates to over\n    8 Gigabytes of data in a 24 hour period.\n\n    Given the importance of tracking hit information with respect to\n    advertising revenue and in some cases user profiling, what are\n    servers doing with the size of this log information?\n\nI would imagine that very few servers are actually running at\nthe 1000 requests/second rate for a 24 hour period.  (Actually,\nthe highest reported SPECweb96 number, from a Digital AlphaServer,\nis still slightly below 1000 requests/second.)\n\nThe SPECweb96 rules, which are available at\nhttp://www.specbench.org/osg/web96/runrules.html\ndo require that something equivalent to a CLF log entry is\nwritten to stable storage for each request.  At 1000 request/sec,\nand using your figure of 100 bytes/request, this works out to\n100 KB/sec, which is at least a factor of ten below the rates\nthat modern disk drives can sustain.\n\nOn the other hand, the rules also say:\n    RUNTIME, the time of measurement for which results are reported,\n    must be the default 600 seconds for reportable results. The\n    WARMUP_TIME must be set to the default of 300 seconds for\n    reportable results.\nIn other words, nobody runs one of these benchmarks for 24 hours,\nand so nobody has to collect 8 GB of logging info to meet the\nSPECweb96 rules.\n\nThe busiest Web site that I know of is still somewhere below\n100 million requests/day, the last time I checked.  That works\nout to about 10 GB/day of log info, but this site spreads it\nout over lots of machines.\n\nI would guess that they probably do something like compressing\nit and storing it offline (I used gzip on a CLF file and got it\ndown to under 7 bytes/request).  Or they run it through some\nsoftware to extract a statistical summary, and then delete the\nfull log.  Or both.\n\n    Is the hit-metering draft that I have seen reference to going to\n    address this?\n\nNot if you're referring to the one that Paul Leach and I have\nbeen working on.  We're working on a heavily revised draft, but\nit still won't discuss log formats.  Phill Hallam has issued\nsome drafts on ways for servers to ask proxies for their logs,\nand I would imagine that he has considered the costs associated\nwith retrieving tens or hundreds of megabytes of log info like\nthis; the hit-metering work that Paul and I am doing is designed\nto avoid this kind of thing.\n\n    Is there a more appropriate forum for a question such as this?\n\nI think any discussion of log-exchange over the network had better\nface up to the log-volume issue.  As for what a server does, locally,\nI think that's probably off-topic for the HTTP-WG.\n\n-Jeff\n\n\n\n    \n\n\n\n", "id": "lists-010-10773105"}, {"subject": "Re: 13.1.2 Warning", "content": "Ummm, is there some reason why an HTTP/1.1 user agent cannot tell\nfor itself whether or not a message is stale?  After all, the\nsame information that made the proxy decide to add the Warning\nheader is still present in the cached message, and that information\ncan be interpreted by the HTTP/1.1 user agent just as easily as the\nHTTP/1.1 proxy.\n\nBTW, that begs the question of why the client is being warned about\nsomething that should be obvious from the Date, Age, and Expires/max-age?\n\nAs a separate issue, Warning is one of the headers that should be\nlisted as MUST be sent in a 304 response, with the lack of such a header\nmeaning remove any existing Warning messages from the cached entity.\nThat would settle the problem entirely, I think.\n\n.....Roy\n\n\n\n", "id": "lists-010-10782965"}, {"subject": "Re: 13.1.2 Warning", "content": "> But in the situation that is of interest, this is not a case\n> where the *end-client* doesn't understand the Warning (or else\n> it wouldn't be an issue).  It's a case where a proxy *between*\n> two systems that understand Warning doesn't understand it.\n> So we're back to risking \"undetected stale pages\" in a situation\n> where we could, in fact, detect them.\n\nWhich is what is the case always with HTTP/1.0 caches.  An HTTP/1.1\ncache upstream shouldn't be required to try to \"fix\" a downstream\nHTTP/1.0 cache (not that it's even able to, see my earlier scenario).\n\n1.0 doesn't support stale data notification, period.  If you have 1.0,\nyou won't be notified, and you'll have to live with it, just like thus\nfar.  If you need notification, you need an all-1.1 proxy chain.\n\nBut you don't want 1.1 behaviour to make 1.0 caches work more wrong,\nwhich is what would happen if a Warning header was sent to a 1.0\ncache.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-10790905"}, {"subject": "Re: 13.1.2 Warning", "content": "> Ummm, is there some reason why an HTTP/1.1 user agent cannot tell\n> for itself whether or not a message is stale?  After all, the\n> same information that made the proxy decide to add the Warning\n> header is still present in the cached message, and that information\n> can be interpreted by the HTTP/1.1 user agent just as easily as the\n> HTTP/1.1 proxy.\n\nOnly the proxy knows its own configuration, which contributes to the\ndecision of whether or not the proxy does an up-to-date check or not.\nThe Warning header is the only means of telling the client of that\nfact.\n\n> BTW, that begs the question of why the client is being warned about\n> something that should be obvious from the Date, Age, and Expires/max-age?\n\nSee above.  The proxy has config parameters only known to itself,\nwhich affect the cache control policy.\n\n> As a separate issue, Warning is one of the headers that should be\n> listed as MUST be sent in a 304 response, with the lack of such a header\n> meaning remove any existing Warning messages from the cached entity.\n> That would settle the problem entirely, I think.\n\nNot, because HTTP/1.0 doesn't do any of that header wiggling stuff in\nthe cache.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-10799495"}, {"subject": "Unidentified subject", "content": "unsbscribe\n\n\n\n", "id": "lists-010-10808601"}, {"subject": "Re: 13.1.2 Warning", "content": "Roy T. Fielding:\n>\n>Ummm, is there some reason why an HTTP/1.1 user agent cannot tell\n>for itself whether or not a message is stale? \n\nNot as far as I can see.  However, the warning code also gives\npotentially valuable information on _why_ the message is stale.  This\nis why I prefer dealing with the problem when getting a warning out of\na 1.0 proxy, not when sending one to it.\n\nThe warning codes in the draft are:\n\n  10 Response is stale\n  11 Revalidation failed\n  12 Disconnected operation\n  13 Heuristic expiration\n  14 Transformation applied\n\nNote that warning 14 is not staleness-related, so whatever else we end\nup doing, we must not remove the 14 warnings when sending a response\nto a 1.0 proxy.\n\n[...]\n>As a separate issue, Warning is one of the headers that should be\n>listed as MUST be sent in a 304 response,\n\nI think you are right.  I recall that we updated the rules for\ncreating 304 (not modified) responses in a great hurry; we may have\noverlooked more than just the warning stuff.  (Aside: I now suspect\nthat the Alternates header caching rules I listed in the TCN draft are\nbroken too because of similar 304 compatibility subtleties.)\n\n>.....Roy\n\nKoen.\n\n\n\n", "id": "lists-010-10814638"}, {"subject": "Re: 13.1.2 Warning", "content": "Daniel DuBois:\n>\n>>Perhaps we have different ideas of what \"transparency\" means, but it seems to\n>>me that receiving an entity with an erroneous header is not transparent. It\n>\n>OK, you are right.  I was thinking about transparancy of the entity body.\n>But in the HTTP spec, semantic transparency is defined as 'exactly the same\n>response (except for hop-by-hop headers)'.  Warning, although added in a\n>middle hop, I dont believe can be considered a hop-by-hop header.\n\nI don't think there is a wording problem here: the spec talks about\n_weakening_ the transparency.  I read \n\n Warnings are always cachable, because they never weaken the transparency\n of a response.\n\nto mean `.., because they never cause a stale response to be\ninterpreted as fresh'.\n\nAside: though I don't think that the sentence is _wrong_, I do think\nit is potentially confusing.  I have argued repeatedly in the past\nthat the term `semantic transparency' is useless for describing HTTP,\nand that it should therefore not be used in the draft.  Like\n`idempotence', the term `semantic transparency' will haunt us for\nyears to come.\n\n>Daniel DuBois\n\nKoen.\n\n\n\n", "id": "lists-010-10823086"}, {"subject": "Re: 13.1.2 Warning", "content": "Jeffrey Mogul wrote:\n> \n> In response to my option #1:\n> > (1) Live with it.  This can only happen when an HTTP/1.0\n> > cache is a client of an HTTP/1.1 cache, and presumably\n> > in somewhat unusual cases, so maybe it's a temporary\n> > problem.\n> \n> You replied:\n>     Something you'd have to live with for a long time, I suspect. Not\n>     an acceptable option, IMHO.\n> I.e., you expect this kind of situation to persist for a while:\n> \n>  [Server]---[HTTP/1.1 cache]---[HTTP/1.0 cache]---[HTTP/1.1 client]\n>     \n> But then, regarding my suggestion to not send Warning to an HTTP/1.0\n> client,\n>     This would seem the appropriate solution. Not giving a Warning\n>     where the client doesn't understand it would seem a small price\n>     to pay.\n> \n> But in the situation that is of interest, this is not a case\n> where the *end-client* doesn't understand the Warning (or else\n> it wouldn't be an issue).  It's a case where a proxy *between*\n> two systems that understand Warning doesn't understand it.\n> So we're back to risking \"undetected stale pages\" in a situation\n> where we could, in fact, detect them.\n> \n>     > One possibility: HTTP/1.1 clients (the only ones that could\n>     > care about a Warning header anyway) should turn a Reload on\n>     > a page with a \"Warning: stale\" into a \"Pragma: no-cache\".  That\n>     > would cause a few extra cache misses, but would break the\n>     > infinite loop that you are worried about.\n>     \n>     I prefer the idea of leaving the Warning out. I see no gain from\n>     forcibly removing the stale entry from downstream caches.\n> \n> The main point of my previous message was not to enumerate the\n> complete set of possible solutions, but to prod the people who\n> are complaining about the current situation to suggest some\n> alternative that isn't worse than the problem it purports to\n> solve.\n> \n> Sending \"Pragma: no-cache\" might not be the best solution.  I\n> strongly encourage people to improve upon it.  Be creative!\n\nOK, here's my creative solution ... a 1.1 proxy receiving a Warning should\nmake the entity look out-of-date to any 1.0 clients. The Warning can then be\nsafely cached in 1.0 caches, because it will be cleared by a fresh copy of the\nsame entity without the Warning which looks more up-to-date than the cached\ncopy.\n\nWhether 1.0 caches will store an \"older\" copy is an interesting question,\nthough. If they don't, then this solution has the same net effect as leaving\nthe Warning out, if they do, then the Warning can be propogated to downstream\ncaches without poisoning them.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10832200"}, {"subject": "HTTP Log file", "content": "Doug Crow writes:\n > I have seen where groups are publishing specWeb96 numbers approaching =\n > 1000 URLs per second.  Using the common log file format and assuming an =\n > average entry of 100 bytes this translates to over 8 Gigabytes of data =\n > in a 24 hour period. =20\n > \n > Given the importance of tracking hit information with respect to =\n > advertising revenue and in some cases user profiling, what are servers =\n > doing with the size of this log information? =20\n > \n > Is the hit-metering draft that I have seen reference to going to address =\n > this?\n > \n > Is there a more appropriate forum for a question such as this?\n\nYou may consider giving a look to:\n\nhttp://www.w3.org/pub/WWW/TR/WD-logfile.html\n\nWhich propose a smart log file format to deal with the number of write\naccesses to the http log.\n\nAnselm.\n\n\n\n", "id": "lists-010-10843025"}, {"subject": "Re: 13.1.2 Warning", "content": "> OK, here's my creative solution ... a 1.1 proxy receiving a Warning\n> should make the entity look out-of-date to any 1.0 clients. The\n> Warning can then be safely cached in 1.0 caches, because it will be\n> cleared by a fresh copy of the same entity without the Warning which\n> looks more up-to-date than the cached copy.\n\nI would consider that kind of distortion of meta data to be bad.  I\nthink we're going too far if we're starting to modify our response to\nfool 1.0 cache to do what 1.1 prefers.  May as well just send a\nstandard error message saying \"buzz off 1.0 cache, you need to upgrade\nto 1.1, or I will forever torture you with bogus responses that will\nconfuse you out of your little mind\".\n\nEither (1) live with it (this only occurs with cascades), or (2)\nsuppress the warning headers when giving a response to 1.0 cache (with\nthe possible exception of \"14 Transformation applied\").  I would\nprefer (2).\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-10850833"}, {"subject": "Re: 13.1.2 Warning", "content": "Ari Luotonen wrote:\n> \n> \n> > OK, here's my creative solution ... a 1.1 proxy receiving a Warning\n> > should make the entity look out-of-date to any 1.0 clients. The\n> > Warning can then be safely cached in 1.0 caches, because it will be\n> > cleared by a fresh copy of the same entity without the Warning which\n> > looks more up-to-date than the cached copy.\n> \n> I would consider that kind of distortion of meta data to be bad.  I\n> think we're going too far if we're starting to modify our response to\n> fool 1.0 cache to do what 1.1 prefers.  May as well just send a\n> standard error message saying \"buzz off 1.0 cache, you need to upgrade\n> to 1.1, or I will forever torture you with bogus responses that will\n> confuse you out of your little mind\".\n> \n> Either (1) live with it (this only occurs with cascades), or (2)\n> suppress the warning headers when giving a response to 1.0 cache (with\n> the possible exception of \"14 Transformation applied\").  I would\n> prefer (2).\n\nI didn't say it was a good idea, just that it was creative. If Jeff wants\ncreativity, who am I to deny him? ;-)\n\nI've already stated my preference. I'm in agreement with you.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-10859593"}, {"subject": "Re: 13.1.2 Warning", "content": ">> Ummm, is there some reason why an HTTP/1.1 user agent cannot tell\n>> for itself whether or not a message is stale?  After all, the\n>> same information that made the proxy decide to add the Warning\n>> header is still present in the cached message, and that information\n>> can be interpreted by the HTTP/1.1 user agent just as easily as the\n>> HTTP/1.1 proxy.\n> \n> Only the proxy knows its own configuration, which contributes to the\n> decision of whether or not the proxy does an up-to-date check or not.\n> The Warning header is the only means of telling the client of that\n> fact.\n\nThat the validation failed, yes, but whether or not a message is stale\nis a property of the message, not of the proxy.  The fact that no validation\nwas performed is visible in the Date and Age, so there is no need to warn\nthe user agent about that. It just strikes me as a bit weird to add the\nwarning, and I can't remember why it was needed in the first place.\n\n>> BTW, that begs the question of why the client is being warned about\n>> something that should be obvious from the Date, Age, and Expires/max-age?\n> \n> See above.  The proxy has config parameters only known to itself,\n> which affect the cache control policy.\n\nBut the cache control policy has no impact on whether the message is\nstale or not -- freshness is a simple calculation based on the data included\nwith the message and the time the proxy requested it, and all of that data\nis required to be passed on to the client when the response is retrieved\nfrom a cache.\n\n>> As a separate issue, Warning is one of the headers that should be\n>> listed as MUST be sent in a 304 response, with the lack of such a header\n>> meaning remove any existing Warning messages from the cached entity.\n>> That would settle the problem entirely, I think.\n> \n> Not, because HTTP/1.0 doesn't do any of that header wiggling stuff in\n> the cache.\n\nAny valid HTTP/1.0 cache must do the header wiggling stuff -- that has been\npart of the 304 definition since we defined it on www-talk 2.5 years ago.\nOf course, this only matters with proxy and server-based caches, many of\nwhich ignore the basic requirements of caching.  Holding on to stale\nwarnings is the least of their worries.\n\n....Roy\n\n\n\n", "id": "lists-010-10868942"}, {"subject": "(CONTENT NEGOTIATION,VARY,ACCEPT*) Strategic decision", "content": "The editorial group working on the new HTTP/1.1 draft has agreement on\nthe following strategy to deal with content negotiation.\n\nIf anyone strongly disagrees with this strategy, now is the time to\nspeak up.\n\nI will soon post draft text that reflects this strategy.\n\nThe main goals of the strategy are 1) to avoid the risk that\ndisagreement about content negotiation would delay consensus on the 1.1\ndraft until after May 1, while 2) not blocking a smooth introduction\nof content negotiation after May 1.\n\nThe plans are as follows:\n\n- as announced earlier in a message by Jim Gettys, (Proposed structure\nof HTTP 1.1 document(s), Mon, 18 Mar 96 15:03:18 -0500), the content\nnegotiation text in draft-holtman-http-content-negotiation-00.txt will\nnot be merged with the main HTTP/1.1 document.  An updated version of\ndraft-holtman-http-content-negotiation-00.txt will appear in April,\nand it is hoped that consensus on it can be reached in May.\n\n- Section 12 of the old 1.1 draft (which gave an incomplete definition\nof a content negotiation mechanism) will not be present in the new 1.1\ndraft.\n\n- There will be a Vary header in the 1.1 draft, so that HTTP/1.1 can\nsupport opaque negotiation on language in a reasonably efficient\nway.\n\n- There will be Accept* headers in the 1.1 draft.  With respect to the\nAccept* headers previous draft, numerous small improvements are made;\nthese improvements reflect the consensus of the content negotiation\nsubgroup.\n\n- There will be `hooks' in the 1.1 draft to ensure that all HTTP/1.1\ncaches will be compatible, though not in an optimally efficient way,\nwith a transparent content negotiation mechanisms like the mechanism\ndefined in draft-holtman.  Thus, transparent content negotiation\n(which is what Section 12 of the old 1.1 draft covered incompletely)\nwon't have to wait for HTTP/1.2 if HTTP/1.2 turns out to take too\nlong, it can be done on top of HTTP/1.1. \n\n- The `hooks' for transparent content negotiation consist mainly of an\nAlternates header definition which defines the Alternates header as\nsynonymous with a certain Vary header.  Also, some language in the\ndraft will announce that a negotiation mechanism using Alternates is\nplanned.\n\nNote for the content negotiation subgroup: I previously announced to\nyou that I would try to keep a complete Vary header definition out of\nthe main 1.1 draft, because of concerns that it being in the main\ndraft could prevent the Alternates header from becoming more popular\nthan Vary because Vary `got there first', even though Alternates is\nmuch better than Vary in 95% of the cases.  I failed: the editorial\ngroup voted on it, and it turned out that I was the only one one who\nthought that putting Vary in the negotiation draft would be better.\n\nKoen.\n\n\n\n", "id": "lists-010-1087506"}, {"subject": "PEP Battle Pla", "content": "to move PEP forward on standards-track. \n\nThere are several changes I have discussed privately with PEP reviewers\nfrom several organizations. Many of them are presented herein. Other novel\nquestions have only been sketched out, and will require vigorous debate\nhere: how does PEP interact with caching? How can PEP be used to implement\nother proposed features?\nProposed PEP changes\nI will soon forward a revised drafting of PEP that simplifies the spec into\ntwo components: an Extension Protocol and an Extension Negotiation Protocol\nthat builds on top of EP. \n\nEP has two components:\nProtocol: indicates that this message has been extended\nProtocol-Info: advertises that an extension is available for future use at\nthis extension\nWe hope to separately motivate Protocol-Request and Protocol-Query\nmachinery as an extension upon that base which says when to begin using an\nextension, why, and what compatible extensions may be substituted. ENP,\ntentatively, is a separate add-on draft that we at W3C, through long\nexperience with PEP application scenarios believe is required, but can be\nsafely split from the mandatory core HTTP requires.\n\nWe also hope to demonstrate that EP can be deployed without a version\nnumber upgrade and perhaps without a new method, either ? using Connection:\nand Cache-Control:. It will cost one round-trip to be absolutely sure your\ncounterparty supports PEP. \n\nOther changes to the PEP draft include:\nid # tagging of each Protocol-* directive. The id# allows us to 1) claim\nall headers beginning with that id #, 2) use in Content-Encoding for\npipeline order, 3) pairing up related directives across request-response\npairs\nmodifying the *-matching rule for the URIs in a for list (essentially, *\nanywhere in a URI)\nuse of headers to pass all extension-specific data (less religion about\nbags)\nBetter explanation of error reporting\n? and more\nOf course, I need to forward the revised draft before any detailed\ndiscussion can begin.\n\nProgress on an extension mechanism is essential because it is the future of\n1.x and binary encodings of it. Most of what we are discussing for 1.2 can\nbe accommodated over PEP. W3C has been gaining experience in a number of\ndomains which leverage such a facility. Experience gained today will port\nover to more efficient encodings of 1.x. Caching and backwards\ncompatibility problems  can  be solved. \n\nThat's the battle plan from this end.\nNext Steps\ncreate a PEP home page in the W3C Protocols area\nsubmit new draft ? by midweek\ntrack libWWW5 implementation\ninvestigate technical writing support for white papers, implementation\nguides, perhaps the spec itself\n\n\n\n", "id": "lists-010-10878102"}, {"subject": "Re: PEP Battle Pla", "content": "I sorry for all the time you spent trying to help me,and greatful for the\nmany hours that you worked in trying to do so.Only recently did i find out\nthat Mac,s (Mother Bord) and chip never Worked,and their is no time limet on\nwhen if ever their going to get the parts.Over five thousand dollars and\nfeeling handicapped/mentality incommunicable not one word from Apple about\nthis problem.I had no other choice but to buy another computer and start\nlegal action with MAC.I would just like to thank you for a(ALL) you have\ndone,trust me mac can HTTP this with my lawyers.\n                                             If prayers could be answered you\nwere my angle\n\n\n\n\n\n\n\n                                                              Pixie02@aol.com\n\n\n\n", "id": "lists-010-10887247"}, {"subject": "PEP Battle Plan [rexmit, garbled", "content": "PEP Battle Plan\n\nThe PEP extension strategy has been on the shelf for a while now. I have\nbeen working in this space for a year now, and like a killer mountain, I\nhave learned to respect this problem. It is easy to fall down ratholes, or\nto ascend along the wrong path. Only two months ago did I learn this\nfundamental lesson about the PEP drafts that have passed so far:\n\nExtension and Negotiation are separate problems.\n\nThere has been a lot of resistance to PEP because even if it was well\nspecified on paper, the operational model has never been clear because\nthese two problems were intertwingled ? lots of black boxes that said \"and\nthe HTTP agent will satisfice all of the counterparty's requests\".\n\nHistorically, we at W3C have been pushing a PEP-like strategy since our\ninception. Many of the problems we/I tackled were inherently\nnegotiation-centric: the SHTTP-like Security Extension Architecture, the\nJEPI payments negotiation project, the PICS label request protocol? so for\nus, to make HTTP extension natural for legions of 4th party developers, it\nis absolutely critical to build upon both of these concepts. On the other\nhand, it is clear that to the smaller community of HTTP designers,\ncorrectness, compactness, and simplicity are more important. The complexity\nof selecting an extension, initiating it, and coordinating it with other\nextensions is like toothpaste in a tube: it can be squeezed back and forth\nfrom the HTTP agent to the plug-in extension.\n\nThe August PEP draft was designed to be the bedrock of the JEPI project: on\nthe basis of that draft, the JEPI project is going forward, leaving change\ncontrol now firmly in the hands of this IETF HTTP-WG as long as it wishes\nto move PEP forward on standards-track. \n\nThere are several changes I have discussed privately with PEP reviewers\nfrom several organizations. Many of them are presented herein. Other novel\nquestions have only been sketched out, and will require vigorous debate\nhere: how does PEP interact with caching? How can PEP be used to implement\nother proposed features?\nProposed PEP changes\nI will soon forward a revised drafting of PEP that simplifies the spec into\ntwo components: an Extension Protocol and an Extension Negotiation Protocol\nthat builds on top of EP. \n\nEP has two components:\nProtocol: indicates that this message has been extended\nProtocol-Info: advertises that an extension is available for future use at\nthis extension\nWe hope to separately motivate Protocol-Request and Protocol-Query\nmachinery as an extension upon that base which says when to begin using an\nextension, why, and what compatible extensions may be substituted. ENP,\ntentatively, is a separate add-on draft that we at W3C, through long\nexperience with PEP application scenarios believe is required, but can be\nsafely split from the mandatory core HTTP requires.\n\nWe also hope to demonstrate that EP can be deployed without a version\nnumber upgrade and perhaps without a new method, either ? using Connection:\nand Cache-Control:. It will cost one round-trip to be absolutely sure your\ncounterparty supports PEP. \n\nOther changes to the PEP draft include:\nid # tagging of each Protocol-* directive. The id# allows us to 1) claim\nall headers beginning with that id #, 2) use in Content-Encoding for\npipeline order, 3) pairing up related directives across request-response\npairs\nmodifying the *-matching rule for the URIs in a for list (essentially, *\nanywhere in a URI)\nuse of headers to pass all extension-specific data (less religion about\nbags)\nBetter explanation of error reporting\n? and more\nOf course, I need to forward the revised draft before any detailed\ndiscussion can begin.\n\nProgress on an extension mechanism is essential because it is the future of\n1.x and binary encodings of it. Most of what we are discussing for 1.2 can\nbe accommodated over PEP. W3C has been gaining experience in a number of\ndomains which leverage such a facility. Experience gained today will port\nover to more efficient encodings of 1.x. Caching and backwards\ncompatibility problems  can  be solved. \n\nThat's the battle plan from this end.\nNext Steps\ncreate a PEP home page in the W3C Protocols area\nsubmit new draft ? by midweek\ntrack libWWW5 implementation\ninvestigate technical writing support for white papers, implementation\nguides, perhaps the spec itself\n\n\n\n", "id": "lists-010-10894544"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "The more essential problem is that there is not enough evidence -- by\nthe lack of traffic on the list -- that others besides Rohit are\ncommitted to PEP or even care about PEP.\n\nRight now, Rohit's battle plan has Rohit wrangling this problem\nalone. I just don't think that's workable.\n\nI'd like to see a small subcommittee of 3-5 people commit to producing\na PEP draft that they agree on, to be produced within a fixed period\nof time. If Rohit the appropriate leader for that subcommittee that's\nOK with me, but we need some others to explicitly step forward,\nidentify themselves, commit to working on it, etc.\n\nLarry\n\n\n\n", "id": "lists-010-10906621"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "> Progress on an extension mechanism is essential because it is the future of\n> 1.x and binary encodings of it.\n\nI realize this is heresy here, but I have to wonder if it's worth\nbuilding the extension mechanism into HTTP.  An efficient URI\nresolution protocol would allow for a smooth transition away from\nHTTP 1.x and to 2.x or other protocols (smb? webnfs? multicast?), \nwithout invalidating old clients and without the overhead of \nestablishing a TCP connection.  New protocols could then be designed \nfrom scratch to take into account everything that has been learned \nfrom HTTP, without inheriting the complexity.\n\nIt would also improve scalability, fault tolerance, ability to \nscreen files (for content ratings, price, language, etc.) before \ndownloading, selection of multiple variants of a resource (by \nallowing the client, rather than the server, to make the selection),  \nclient selection of multiple locations of a resource, etc. \n\nSeems like we need to take a step back and look at the web as a\nwhole before we commit to the direction of adding more complexity\nto HTTP.\n\nKeith\n\n\n\n", "id": "lists-010-10914254"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": ">> Progress on an extension mechanism is essential because it is the future of\n>> 1.x and binary encodings of it.\n>\n>I realize this is heresy here, but I have to wonder if it's worth\n>building the extension mechanism into HTTP.\n\nI don't think there's anything heretical about \"declare victory and\nmove on.\"\n\nI, ,too, don't think PEP is the future of HTTP.\n\n>It would also improve scalability, fault tolerance,\n\nI have my own favorite site of wire protocols to suggest here. :)\n\n>ability to \n>screen files (for content ratings, price, language, etc.) before \n>downloading, selection of multiple variants of a resource (by \n>allowing the client, rather than the server, to make the selection),  \n>client selection of multiple locations of a resource, etc. \n\nSounds like a call for a directory service.\n/r$\n\n\n\n", "id": "lists-010-10923068"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "> Sounds like a call for a directory service.\n\nClose, but not quite.  A resolution service, not a directory service.\n\n(Where by \"directory\" I mean something that can do content-based\nsearching.)\n\n*much* easier to build and deploy than a searchable directory.\n\nKeith\n\n\n\n", "id": "lists-010-10931426"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": ">Close, but not quite.  A resolution service, not a directory service.\n>\n>(Where by \"directory\" I mean something that can do content-based\n>searching.)\n\nBy directory, I mean attribute-based searching.  That seems to be both\nthe past (X.500), the would-be future (XFN), and the fad-of-the-moment (LDAP).\n\n>*much* easier to build and deploy than a searchable directory.\n\nBut the latter is where the world is going.  Someone could make a real\nname for themselves by publishing a list of OID's, representations,\nand semantics for web-page attributes.\n/r$\n\n\n\n", "id": "lists-010-10939262"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "> >Close, but not quite.  A resolution service, not a directory service.\n> >\n> >(Where by \"directory\" I mean something that can do content-based\n> >searching.)\n> \n> By directory, I mean attribute-based searching.  That seems to be both\n> the past (X.500), the would-be future (XFN), and the fad-of-the-moment (LDAP).\n\nThere's nothing wrong with having attribute-based searching, but\nit's better if it's viewed as layered on top of a name-resolution\nsubstrate. One reason for this is that much of the time\nyou don't need to do a search; having the two layers separated\nenhances the scalability of the system.  Another reason is that\nthe most effective search engines are specific to a subject domain,\nrather than generic.  So we need a single, generic, distributed \ndocument store as a substrate for both generic and domain specific\nresource discovery tools.\n\nOf course, if some providers want to provide attribute-based\nsearching, co-locate the two services, and even return name resolution \ninformation along with the response to a directory query, that's fine \n-- as long as there's a well-defined standard for doing the name\nresolution by itself.\n \n> But the latter is where the world is going.  \n\nNo offense, but I've heard the official wisdom about the future\nof the world so many times that ... well, I'm skeptical.\n\nRelatively simple solutions to problems, which fit in well with\nthe existing infrastructure, offered at the right time,\nseem to be the best way to acheive success.\n\nIn this case, I'm talking about ~10000 lines of code for each of\nserver and client glue, not counting the rpc and db libraries.\n(actually, the current implementation of RCDS is ~4000 lines in\nthe server, 2500 lines in the client, and 3300 lines of common\ncode...but of course it will have to grow somewhat)\n\nWhich is not to say that there's no need for attribute-\nbased searching, it's just not what I'm talking about here.\n\n(Actually, this discussion probably belongs on the URI mailing list.)\n\n-Keith\n\n\n\n", "id": "lists-010-10947578"}, {"subject": "Server response version numbe", "content": "It has been suggested that servers should always return the client's version.\n\nI'm not sure this has been carefully thought through.\n\nWhat happens in the cases of PUT, DELETE where we have 100 codes?\n\nHow does the client decide to send a 1.1 request in the first place?\n\nJust try, and if a 1.0 server is confused, revert to 1.0?\n\nI'd like to a definitive answer to:\n\nThe server should:\n\na. Always return its http version.\n\nb. Always return the http version of the request.\n\nPick a or b.\n\n\n\n", "id": "lists-010-10957196"}, {"subject": "Re: Server response version numbe", "content": "Ahe server should always return its http version.  I'm concerned there\nmay be some yet-unidentified parts of the current HTTP/1.1\nspecification that are inconsistent with that approach, but we should\nfix those, since the alternative just doesn't make sense.\n\nLarry\n\n\n\n", "id": "lists-010-10965740"}, {"subject": "(ACCEPT*) Draft text for Accept header", "content": "As I said in my earlier message about strategy:\n\n- There will be Accept* headers in the 1.1 draft.  With respect to the\nAccept* headers previous draft, numerous small improvements are made;\nthese improvements reflect the consensus of the content negotiation\nsubgroup.\n\nThe sections below are proposed as replacements of the sections with\nthe same numbers and names in the current 1.1 draft.\n\nThe text below was mostly lifted from draft-holtman, but does have\nsome changes with respect to that draft.  All references to content\nnegotiation were deleted.  I deleted the mxb= parameter in the Accept\nheader, though it could be re-introduced later.\n\nThe main improvement with respect to the old draft is in clarifying\nthe matching rules of the Accept-Language header.\n\nIf you have comments on this text, now is the time to comment.  I\nintend to close this issue at the end of the week.  This means that I\nwill send a last call for disagreement with perceived consensus,\ntogether with a possibly improved version of the text below, in a few\ndays.\n\nThe change bars below were added by hand, and indicate change with\nrespect to the last 1.1 draft.  Merely moved text was not marked with\nchange bars.\n\n--snip--\n\n3  Protocol parameter descriptions\n\n3.10  Language Tags\n\n   [##Note: I moved the language tag matching discussion that used to\n   be in this Section to Section 10.4 (Accept-Language).  Some other\n   minor edits were made.##]\n\n   A language tag identifies a natural language spoken, written, or\n   otherwise conveyed by human beings for communication of information\n   to other human beings. Computer languages are explicitly excluded.\n   HTTP/1.1 uses language tags within the Accept-Language and\n|  Content-Language fields.\n\n   The syntax and registry of HTTP language tags is the same as that\n   defined by RFC 1766 [1]. In summary, a language tag is composed of 1\n   or more parts: A primary language tag and a possibly empty series of\n   subtags:\n\n        language-tag  = primary-tag *( \"-\" subtag )\n\n        primary-tag   = 1*8ALPHA\n        subtag        = 1*8ALPHA\n\n   Whitespace is not allowed within the tag and all tags are\n   case-insensitive. The namespace of language tags is administered by\n   the IANA. Example tags include:\n\n       en, en-US, en-cockney, i-cherokee, x-pig-latin\n\n   where any two-letter primary-tag is an ISO 639 language abbreviation\n   and any two-letter initial subtag is an ISO 3166 country code.\n\n|\n|\n\n9. Status Code Definitions\n\n413 Not Acceptable\n\n   [#Note: the new 413 is similar to the 406 response code in the old\n   draft.  406 cannot be used for content negotiation compatibility\n   reasons##]\n\n|  The resource identified by the Request-URI and Host request header\n|  (present if the request-URI is not an absoluteURI) is only capable\n|  of generating response entities which have content characteristics\n|  not acceptable according to the accept headers sent in the request.\n\n|  HTTP/1.1 servers are allowed to return responses which are not\n|  acceptable according to the accept headers sent in the request.  In\n|  some cases, this may even be preferable over sending a 408\n|  response.  User agents are encouraged to inspect the headers of an\n|  incoming response to determine if it is acceptable.  If this is not\n|  the case, user agents should interrupt the receipt of the response\n|  if doing so would save network resources.  If it it unknown whether\n|  an incoming response would be acceptable, a user agent should\n|  temporarily stop receipt of more data and query the user for a\n|  decision on further actions.\n\n   [## Note: the paragraph above could be moved to a more convenient\n   location in the 1.1 document if the editor finds one.  Note that\n   the above rule was discussed extensively on the content negotiation\n   mailing list.  A short summary of the main reason behind this rule:\n   20 line HTTP servers.##]\n\n\n10  Header field definitions\n\n10.1  Accept\n\n|  The Accept request-header field can be used to specify certain\n   media types which are acceptable for the response.  Accept headers\n   can be used to indicate that the request is specifically limited to\n   a small set of desired types, as in the case of a request for an\n   in-line image.\n\n   The field may be folded onto several lines and more than one\n   occurrence of the field is allowed, with the semantics being the same\n   as if all the entries had been in one field value.\n\n       Accept         = \"Accept\" \":\" #(\n                        ( media-range\n|                         [ ( \":\" | \";\" ) \n|                           range-parameter \n|                           *( \";\" range-parameter ) ] )\n|                       | extension-token )\n\n       media-range     = ( \"*/*\"\n                       |   ( type \"/\" \"*\" )\n                       |   ( type \"/\" subtype )\n                         ) *( \";\" parameter )\n\n|      range-parameter = ( \"q\" \"=\" qvalue ) \n|                      | extension-range-parameter\n|\n|      extension-range-parameter = ( token \"=\" token )\n|\n|      extension-token = token \n\n   The asterisk \"*\" character is used to group media types into ranges,\n   with \"*/*\" indicating all media types and \"type/*\" indicating all\n   subtypes of that type.\n\n|  The range-parameter q is used to indicate the media type quality\n|  factor for the range, which represents the user's preference for\n|  that range of media types.  The default value is q=1.  In Accept\n|  headers generated by HTTP/1.1 clients, the character separating\n|  media-ranges from range-parameters should be a \":\".  HTTP/1.1\n|  servers should be tolerant of use of the \";\" separator by HTTP/1.0\n|  clients.\n\n   The example\n\n       Accept: audio/*:q=0.2, audio/basic\n\n   should be interpreted as \"I prefer audio/basic, but send me any audio\n   type if it is the best available after an 80% mark-down in quality.\"\n\n   If no Accept header is present, then it is assumed that the client\n|  accepts all media types.  If Accept headers are present, and if the\n|  resource cannot send a response which is acceptable according to\n|  the Accept headers, then the server should send an error response\n|  with the 413 (not acceptable) status code, though the sending of an\n|  un-acceptable response is also allowed.\n\n   A more elaborate example is\n\n|      Accept: text/plain:q=0.5, text/html,\n|              text/x-dvi:q=0.8, text/x-c\n\n   Verbally, this would be interpreted as \"text/html and text/x-c are\n   the preferred media types, but if they do not exist, then send the\n|  text/x-dvi entity, and if that does not exist, send the text/plain\n   entity.\"\n\n   Media ranges can be overridden by more specific media ranges or\n   specific media types. If more than one media range applies to a given\n   type, the most specific reference has precedence. For example,\n\n|      Accept: text/*, text/html, text/html;level=1, */*\n\n   have the following precedence:\n\n|      1) text/html;level=1\n       2) text/html\n       3) text/*\n       4) */*\n\n|  The media type quality factor associated with a given type is\n   determined by finding the media range with the highest precedence\n   which matches that type.\n\n   For example,\n\n|      Accept: text/*:q=0.3, text/html:q=0.7, text/html;level=1,\n               */*:q=0.5\n\n   would cause the following type quality factors to be associated:\n\n|      text/html;level=1                          = 1\n       text/html                                  = 0.7\n       text/plain                                 = 0.3\n       image/jpeg                                 = 0.5\n|      text/html;level=3                          = 0.7\n\n|  \n\n       Note: A user agent may be provided with a default set of \n       quality values for certain media ranges. However, unless the \n       user agent is a closed system which cannot interact with \n       other rendering agents, this default set should be \n       configurable by the user.\n\n\n10.2  Accept-Charset\n\n   The Accept-Charset request-header field can be used to indicate what\n   character sets are acceptable for the response. This field allows\n   clients capable of understanding more comprehensive or\n   special-purpose character sets to signal that capability to a server\n   which is capable of representing documents in those character\n   sets. The US-ASCII character set can be assumed to be acceptable to\n   all user agents.\n\n|  [##QUESTION TO BE RESOLVED: Apparently, the latest HTML spec says\n|  that iso-8859-1 can be assumed to be acceptable to all user agents.\n|  Should the above US-ASCII be changed to iso-8859-1??  There has\n|  been lots of discussion on the list, but I have not been able to\n|  detect a consensus opinion.##]\n\n       Accept-Charset = \"Accept-Charset\" \":\" 1#charset\n\n   Character set values are described in Section 3.4. An example is\n\n       Accept-Charset: iso-8859-1, unicode-1-1\n\n|  If no Accept-Charset header is present, the default is that any\n|  character set is acceptable.  If an Accept-Charset header is\n|  present, and if the resource cannot send a response which is\n|  acceptable according to the Accept-Charset header, then the server\n|  should send an error response with the 413 (not acceptable) status\n|  code, though the sending of an un-acceptable response is also\n|  allowed.\n\n\n10.3  Accept-Encoding\n\n   The Accept-Encoding request-header field is similar to Accept, but\n   restricts the content-coding values (Section 3.5) which are\n   acceptable in the response.\n\n       Accept-Encoding         = \"Accept-Encoding\" \":\" \n                                 #( content-coding )\n\n   An example of its use is\n\n       Accept-Encoding: compress, gzip\n\n   If no Accept-Encoding header is present in a request, the server\n   may assume that the client will accept any content coding.  If an\n|  Accept-Encoding header is present, and if the resource cannot send\n|  a response which is acceptable according to the Accept-Encoding\n|  header, then the server should send an error response with the\n|  413 (not acceptable) status code.\n\n\n10.4  Accept-Language\n\n   The Accept-Language request-header field is similar to Accept, but\n   restricts the set of natural languages that are preferred as a\n   response to the request.\n\n       Accept-Language = \"Accept-Language\" \":\"\n|                        1#( language-range [ \";\" \"q\" \"=\" qvalue ] )\n\n|      language-range = ( ( 1*8ALPHA *( \"-\" 1*8ALPHA ) )\n|                       | \"*\" )\n\n|  Each language-range may be given an associated quality value which\n|  represents an estimate of the user's comprehension of the languages\n|  specified by that range.  The quality value defaults to \"q=1\" (100%\n   comprehension).  For example,\n\n|      Accept-Language: da, en-gb;q=0.8, en;q=0.7\n|  \n|  would mean: \"I prefer Danish, but will accept British English (with\n|  80% comprehension) and other types of English (with 70%\n|  comprehension).\"\n\n|  A language-range matches a language-tag if it exactly equals the tag,\n|  or if it is a prefix of the tag such that the first tag character\n|  following the prefix is \"-\".  The special range \"*\", if present in\n|  the Accept-Language field, matches every tag not matched by any other\n|  ranges present in the Accept-Language field.\n\n|      Note: This use of a prefix matching rule does not imply that\n|      language tags are assigned to languages in such a way that it is\n|      always true that if a user understands a language with a certain\n|      tag, then this user will also understand all languages with tags\n|      for which this tag is a prefix.  The prefix rule simply allows\n|      the use of prefix tags if this is the case.\n\n|  The language quality factor assigned to a language-tag by the\n|  Accept-Language field is the quality value of the longest\n|  language-range in the field that matches the language-tag.  If no\n|  language-range in the field matches the tag, the language quality\n|  factor assigned is 0.\n\n   If no Accept-Language header is present in a request, the server\n|  should assume that all languages are equally acceptable.  If an\n|  Accept-Language header is present, and the resource cannot send a\n|  response acceptable to an audience capable of understanding at\n|  least one language that is assigned a quality factor greater than 0\n|  by the Accept-Language header, it is acceptable to send a response\n|  that uses one or more un-accepted languages.\n\n|  It may be contrary to be privacy expectations of the user to send\n|  an Accept-Language header with the complete linguistic preferences\n|  of the user in every request.  For a discussion of this issue, see\n|  Section aa.bb [##see below##].\n\n       Note: As intelligibility is highly dependent on the \n       individual user, it is recommended that client applications \n       make the choice of linguistic preference available to the \n       user. If the choice is not made available, then the \n       Accept-Language header field must not be given in the \n       request.  \n\n14 Security Considerations\n\n14.x  Privacy issues connected to Accept headers\n\n   [## Note: I believe someone else (Brian Behlendorf?) was also\n   writing text about this, so I only include some concerns about\n   Accept-Language important from a European viewpoint.  The concern\n   of user tracking through Accept headers is not covered below, see\n   Section 6.2 of draft-holtman for a discussion of this concern##]\n\n|  Accept request headers can reveal information about the user to all\n|  servers which are accessed.  The Accept-Language header in\n|  particular can reveal information the user would consider to be of\n|  a private nature, because the understanding of particular languages\n|  is often strongly correlated to the membership of a particular\n|  ethnic group.  User agents which offer the option to configure the\n|  contents of an Accept-Language header to be sent in every request\n|  are strongly encouraged to let the configuration process include a\n|  message which makes the user aware of the loss of privacy involved.\n|  An approach that limits the loss of privacy would be for a user\n|  agent to omit the sending of Accept-Language headers by default,\n|  and to ask the user whether it should to start sending\n|  Accept-Language headers to a server if it detects, by looking for\n|  at any Vary or Alternates response headers generated by the server,\n|  that such sending could improve the quality of service.\n\n\n\n[End of document]\n\n\n\n", "id": "lists-010-1096776"}, {"subject": "Re: Server response version numbe", "content": "At 12:49 AM -0700 1996-10-20, Larry Masinter wrote:\n>Ahe server should always return its http version.  I'm concerned there\n>may be some yet-unidentified parts of the current HTTP/1.1\n>specification that are inconsistent with that approach, but we should\n>fix those, since the alternative just doesn't make sense.\n>\n\nOK. A bunch of implementors are busy committing to the server returning\nthe version of the client.  I just changed my server to return the client\nversion.\n\nI would really appreciate a definitive statement from the WG on this matter\nas I need to release a new version of my server NOW (!) and I would prefer not\nto pollute the world if at all possible.\n\n\n\n", "id": "lists-010-10973970"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "Keith Moore:\n>\n [Rohit:]\n>> Progress on an extension mechanism is essential because it is the future\nof\n>> 1.x and binary encodings of it.\n>\n>I realize this is heresy here, but I have to wonder if it's worth\n>building the extension mechanism into HTTP.  An efficient URI\n>resolution protocol would allow for a smooth transition away from\n>HTTP 1.x and to 2.x or other protocols (smb? webnfs? multicast?), \n>without invalidating old clients and without the overhead of \n>establishing a TCP connection.\n\nI don't see PEP primarily as a means to enable smooth transitions to\nfaster content transmission protocols.\n\nPEP will (at least, I hope it will) enable the addition of new\n_services_ on top of the HTTP content transmission service.  Examples\nof such services are content rating and (micro)payments.\n\nPEP will (at least, I hope it will) allow such services to have a very\nlow overhead, by piggy-backing them onto HTTP transactions which are\nalready happening.\n\nThis is the part of the PEP draft that makes me interested in PEP:\n\n   In addition to reliably describing statically extended HTTP servers\n   and clients, PEP will work with dynamically extended agents. Indeed,\n   the authors expect that PEP will drive the deployment of a new\n   generation of exensible agents (such as W3C's Jigsaw server and libWWW\n   reference library).\n\nNow, I also see some problems with PEP:\n\n- Dynamic service extension is still somewhat of a research problem.\nI don't know how much of this research problem has already been solved\nbehind the w3c member-only firewall.  I don't think this WG will want\nto commit itself to solving huge research problems in the PEP area,\nbut it may want to commit itself to making the tradeoffs left when the\nresearch problems are solved.\n\n- The vision of intelligently cooperating clouds of\nobjects/components/applets/agents adding huge value to the internet\nexperience has been around for some time.  I have so far not been able\nto determine how much of this vision the W3C wants to enable with PEP.\nPEP will probably not be able to live up completely to this vision,\nbut how close do we want to get instead?  Should PEP just be a simple\nheader collision avoidance protocol, or should it define a powerful\nframework for intelligent cooperation?\n\n- PEP is not the only mechanism claiming to allow for the smooth\naddition of services.  Java is another one.  (I know too little about\nplugins to determine if they too claim something here.)  If some\npowerful vendor starts trying to set ad-hoc standards in this area to\nget a competitive advantage, PEP may go the way of HTML 3.0.\n\n[...]\n> selection of multiple variants of a resource (by \n>allowing the client, rather than the server, to make the selection),  \n\nPEP negotiates on _services_.  Negotiation on _content_ is orthogonal\nto PEP, and this WG is already working on a content negotiation\nmechanism with the attributes you mention above.\n\n>Keith\n\nKoen.\n\n\n\n", "id": "lists-010-10982490"}, {"subject": "Re: Server response version numbe", "content": "> OK. A bunch of implementors are busy committing to the server\n> returning the version of the client.  I just changed my server to\n> return the client version.\n\n> I would really appreciate a definitive statement from the WG on this\n> matter as I need to release a new version of my server NOW (!) and I\n> would prefer not to pollute the world if at all possible.\n\nThe WG would like a definitive statement from the implementors about\nwhich option makes the most sense.\n\nIn general, IETF process works better when the implementors ARE the\nworking group.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-10993267"}, {"subject": "Repost: WordHTML &quot;Pipeline&quot; for HTTP/1.1 draf", "content": "[Re-posted - (I missed one correction of the Word file, I've now included\nthe WordBasic macro source and my awk was bugged).]\n\nA \"pipeline\" for converting the Word document of the spec. into HTML is\nattached (I needed this for internal reasons, but I thought it might be\nuseful generally).\n\nThis is not as easy as it sounds (unless someone knows something I don't) as\nInternet Assistant for Word wipes all the internal anchors on the section\nheadings if you've got an auto-generated table of contents in there. I even\nhad to lower myself to learning pidgin Word Basic macro language, as well as\nremembering my sed and awk.\n\nAlso, the internal structure of the spec has become terribly messy since\nit's been in Word. I spent a lot of time fixing all the internal links and\nanchors (there are 335 links in draft 7) and other things like bulletted and\nnumbered lists done inconsistently. I've carefully documented the internal\nchanges I've made below.\n\nThe pipeline isn't particularly generic and involves some manual\nintervention (it wouldn't do if I could be bothered to learn perl), but it\nworks and you get clean HTML out the other end.\n\nI've passed draft 7 through it, so when the RFC comes out, someone may wish\nto use this pipeline for that too.\n\nIf anyone wants, I can stick these on our ftp server:\na) A clean Word file of draft 7 with all the internal linkage ready for\nconversion.\nb) A clean HTML file of draft 7\n\nI would add that neither a) nor b) would produce straight text that would be\nidentical to the current plain/text draft 7. This is because:\n- Many of the internal links to the references became squashed out of the\nspec. at various stages, but still exist internal to the Word file data\nstructure, so I've re-revealed these.\n- I tried to avoid altering the linear white space, but probably failed.\n\nWhether this can still be called draft 7 is up for debate. This is why I\nhaven't made it available already, in case someone objects on a change\ncontrol basis.\n\nAttached is the process.\n\nBob\n\n================================================================================\n\nStep 0: Removed auto-generated table of contents\n (unfortunately this doesn't remove the anchors it refers to)\n\nStep 1: WINWORD: Unpicked bugs in bookmark naming and linkage as follows...\n\n* Heading 3.11 Entity Tags\nDouble bookmarked as \"Entity_Tags\" & \"Opaque_Tags\"\nRemoved latter\n* Heading 3.12 Range Units\nDouble bookmarked as \"Range_Units\" & \"Range_Protocol_Param\"\nRemoved latter\n* Heading 8.1.3 Proxy Servers\nNo bookmark\nAdded bookmark \"Persist_Proxy_Servers\"\n* Heading 9.1 Safe and Idempotent Methods\nNo bookmark\nAdded bookmark \"Safe_Idem_Methods\"\n* Heading 12.1 Server-driven Negotiation\nNo bookmark\nAdded bookmark \"Server_driven_Negotiation\"\n* Heading 12.2 Agent-driven Negotiation\nNo bookmark\nAdded bookmark \"Agent_driven_Negotiation\"\n* Heading 12.3 Transparent Negotiation\nNo bookmark\nAdded bookmark \"Transparent_Negotiation\"\n* Heading 13.3.2 Entity Tag Cache Validators\nwas bookmarked as \"Tags\" & nested within it was the\n\"Entity_Tag_Cache_Validators\" bookmark\nRemoved former and replaced it with latter\n* Heading 13.4 Response Cachability\nTriple bookmarked as \"Response_Cachability\", \"Caching_and_Status_C\" &\n\"Constructing_Respons\"\nRemoved all but first\n* Heading 13.6 Caching Negotiated Responses\nDouble bookmarked as \"Vary_Header_Use\" & \"Caching_and_Varying_\"\nRemoved latter\n* Section 14.4\nArbitrary sentence at end was bookmarked \"OLE_LINK8\"\nRemoved bookmark\n* Section 14.5\nWhole section was bookmarked as \"OLE_LINK1\"\nRemoved bookmark\n* Heading 14.9.2 What May be Stored by Caches\nBookmark ended a character early\nShifted bookmark end right\n* Heading 14.16 Content-MD5\nBookmark ended after para mark\nShifted bookmark end left\n* Heading 14.25 If-Match\nDouble bookmarked as \"If_Match\" & \"If_Valid\"\nRemoved latter\n* Heading 14.28 If-Unmodified-Since\nDouble bookmarked as \"If_Unmodified_Since\" & \"Unless_Modified_Sinc\"\nRemoved latter\n* Heading 14.44 Via\nDouble bookmarked as \"Via\" & \"Forwarded\"\nRemoved latter\n* Heading 15.2 Offering a Choice of Authentication Schemes \nNo bookmark\nAdded bookmark \"Choice_of_Authentication\"\n* Ref [26] Improving HTTP Latency\nNo bookmark\nAdded bookmark \"RefLatency\"\n* Ref [27] Analysis of HTTP Performance\nNo bookmark & URL was bookmarked as \"OLE_LINK2\"\nAdded bookmark named \"RefPerformance\" & Removed \"OLE_LINK2\"\n* Ref [29] , RFC 1951\nNo bookmark\nAdded bookmark \"Ref1951\"\n* Ref [30] Analysis of HTTP Performance Problems\nNo bookmark\nAdded bookmark \"RefPerfProbs\"\n* Ref [31] , RFC 1950\nNo bookmark\nAdded bookmark \"Ref1950\"\n* Ref [32] Work In Progress for Digest authentication\nNo bookmark, but \"DigestRef\" bookmark spanned refs [26] & [27]\nMoved \"DigestRef\" bookmark to ref [32] \n* Heading 18. Authors\n\"Authors\" bookmark spanned previous section\nMoved start of bookmark to start of heading\n* Roy T. Fielding hyper link\nduplicated internal to macrobutton\nOpened up with <SHIFT>F9 & removed one of two\n* Heading 19.4 Differences Between HTTP Entities and RFC 1521 Entities \nBookmark ended a word early\nShifted bookmark end right\n* Heading 19.5.1 Changes to Simplify Multi-homed Web Servers and Conserve IP\nAddresses\nwas bookmarked as \"Changes_For_Host_Support\" and \"OLE_LINK4\" & nested within\nit was the \"AppHost\" bookmark\nRemoved last two bookmarks\n* Heading 19.8 Notes to the RFC Editor and IANA\nBookmark \"Notes_to_RFC_Editor\" spanned previous two sections\nMoved start of bookmark to start of heading\n* Heading 19.8.1 Charset Registry\nNo bookmarks\nAdded bookmark \"Charset_Registry\"\n* Heading 19.8.2 Content-coding Values\nNo bookmarks\nAdded bookmark \"Content_coding_Values\"\n* Heading 19.8.3 New Media Types Registered\nNo bookmarks\nAdded bookmark \"New_Media_Types_Registered\"\n* Heading 19.8.4 Possible Merge With Digest Authentication Draft\nNo bookmarks\nAdded bookmark \"Possible_Merge_With_Digest_Authen\"\n* Heading 19.8.5 Media type parameters named \"q\"\nNo bookmarks\nAdded bookmark \"Media_type_parameters_named_q\"\n\nStep 2: WINWORD Fix bullet lists and numbered lists that\n have been done manually rather than as a style\n\nStep 3: WINWORD: Add internal hyperlinks to refs. from all [xx] references,\n including ones invisible within Word datastructure (revealed with <ALT><F9>)\n\nSave (http117a.doc)\n\nStep 4: WINWORD: Correct typos accepted by working group (haven't done this)\n\nSave ()\n\nStep 5: WINWORD: Run my TypeBkmks Word Macro\n\nStep 6: WINWORD: Format Heading Numbering Remove\n\nSave (http117b.doc)\n\nStep 7: WINWORD: Save As http117b.htm with Internet Assistant v2.03z\n\nStep 8: UNIX: dos2unix http117b.htm http117c.htm\nStep 9: UNIX: sed -f sedfile http117c.htm > http117d.htm\n\n================================================================================\nsedfile\n================================================================================\ns!<A NAME=\"_Toc[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\">{{A NAME={{!<A\nNAME=\"!gp\ns!{{A NAME={{.*}}!!gp\ns!}}!\">!gp\ns!<A NAME=\"_Toc[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\">\\(.*\\)</A>!\\1!gp\ns!<FONT [^>]*>!!gp\ns!</FONT>!!gp\n/<H[2-8]><A NAME=/ { N\ns!<A NAME=!  <A NAME=!\ns!\\n! !\ns!</H!\\\n</H!gp\n}\n================================================================================\n\nStep 10: TEXT EDITOR:\nDealt with any remaining anchors split across lines containing '{{' manually.\nRemoved six remaining _Toc anchors manually\nAdded another </H1> at end of title to match second <H1> at start\nRemoved some spurious <B><LWS></B> and altered three top <B> headings to <H2>\n\nStep 11: UNIX: awk -f awkfile d.htm > e.htm\n\n================================================================================\nawkfile\n================================================================================\nBEGIN { th2 = \"\"; th3 = \"\"; th4 = \"\"; th5 = \"\";\np2 = \". \"; p3 = \" \"; p4 = \" \"; p5 = \" \"}\n/<H2>  <A NAME=/ { th2 = ++h2;\nif (h2 > 9) p2 = \".\";\n$1 = $1 th2 p2;\nh3 = 0; h4 = 0; h5 = 0;\np3 = \" \"; p4 = \" \"; p5 = \" \"}\n/<H3>  <A NAME=/ { th3 = ++h3;\nif (h3 > 9) p3 = \"\";\n$1 = $1 th2 \".\" th3 p3;\nh4 = 0; h5 = 0;\np4 = \" \"; p5 = \" \"}\n/<H4>  <A NAME=/ { th4 = ++h4;\nif (h4 > 9) p4 = \"\";\n$1 = $1 th2 \".\" th3 \".\" th4 p4;\nh5 = 0;\np5 = \" \"}\n/<H5>  <A NAME=/ { th5 = ++h5;\nif (h5 > 9) p5 = \"\";\n$1 = $1 th2 \".\" th3 \".\" th4 \".\" th5 p5}\n {print}\n================================================================================\n\nStep 12: UNIX: sed -n -f sedfile2 http117e.htm > contents.htm\n\n================================================================================\nsedfile2\n================================================================================\n/^<H[2-4]>[0-9.]*  *<A NAME=/ {s!<A NAME=\"!<A HREF=\"#!\ns!^<H2>!!\ns!^<H3>!    !\ns!^<H4>\\([0-9]\\.\\)!         \\1!\ns!^<H4>\\([0-9][0-9]\\)!          \\1!\ns!</H[2-4].*>!!\np\n}\n================================================================================\n\nStep 13: TEXT EDITOR: Paste contents.htm into http117e.htm surrounded by\n<PRE></PRE>\n\nStep 14: Learn to avoid using Microsoft Internet Assistant for anything complex\n\nStep 15: Say phew\n================================================================================\n____________________________________________________________________________\nFrom:    Bob Briscoe,                                BT, Distributed Systems\nPost:    B54 74, BT Labs,    Martlesham Heath,   Ipswich, IP5 7RE,   England\nE-Mail:  rbriscoe@jungle.bt.co.uk\nTel:     +44 1473 645196                                Fax: +44 1473 640929\nWWW:     http://www.jungle.bt.co.uk/people/rbriscoe.html  (BT intranet only)\n\n\n\n", "id": "lists-010-11001495"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": ">The more essential problem is that there is not enough evidence -- by the \n>lack of traffic on the list -- that others besides Rohit are committed to \n>PEP or even care about PEP. Right now, Rohit's battle plan has Rohit \n>wrangling this problem alone. I just don't think that's workable. I'd like \n>to see a small subcommittee of 3-5 people commit to producing a PEP draft \n>that they agree on, to be produced within a fixed period of time. If Rohit \n>the appropriate leader for that subcommittee that's OK with me, but we need \n>some others to explicitly step forward, identify themselves, commit to \n>working on it, etc.\n\nI think this is a good idea and I think that I have subscribed myself to be\npart of that group. What if I (and other interested) together with Rohit\nfigure out a plan for PEP and send this plan to the list by Wednesday?\n\nThanks,\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-11019234"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "At 05:34 PM 10/19/96 -0400, Rich Salz wrote:\n>>> Progress on an extension mechanism is essential because it is the\nfuture of\n>>> 1.x and binary encodings of it.\n>>\n>>I realize this is heresy here, but I have to wonder if it's worth\n>>building the extension mechanism into HTTP.\n\nEvery protocol needs an extension model, otherwise they will become\nobsolute before you know it. What PEP is all about is realizing that the\ncurrent RFC-822 extension mechanism inherited by HTTP in many cases isn't\nenough. The PEP framework provides three types of services:\n\nA) PEP gives the parties the possibility of enquering and enumerating\navailable extensions.\n\nB) PEP gives you the possibility of defining three important attributes of\nany extension:\n\n- consequence - what is the consequence of having / not having an extension? \n- ordering - does extension A come before or after B?\n- scope - this is partially solved with the HTTP/1.1 Connection header but\nit needs to be bound to the PEP frame work as well\n\nC) PEP helps avoiding header name collisions.\n\nNeither of these services are provided by the traditional model of simply\nadding new headers. I don't say that the existing model isn't enough in\nsome cases - just not en all of them.\n\n>I don't think there's anything heretical about \"declare victory and\n>move on.\"\n>\n>I, ,too, don't think PEP is the future of HTTP.\n\nHang on, after much discussion at the Montreal IETF http-wg meeting, it was\ndecided to continue working on Content negotiation, User Agent, and PEP.\nPlease check the minutes as posted to the mailing list\n\nhttp://www.w3.org/pub/WWW/Protocols/IETF/9606-Montreal/http-wg-minutes.txt\n\nThis has nothing to do with victory or any other terms borrowed from the\nbattlefield. If you have constructive critisism of the current draft as\nissued August 19th then you should forward them to this list.\n\n\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-11027885"}, {"subject": "Re: Server response version numbe", "content": "At 12:05 PM 10/20/96 PDT, Larry Masinter wrote:\n>> OK. A bunch of implementors are busy committing to the server\n>> returning the version of the client.  I just changed my server to\n>> return the client version.\n>\n>> I would really appreciate a definitive statement from the WG on this\n>> matter as I need to release a new version of my server NOW (!) and I\n>> would prefer not to pollute the world if at all possible.\n>\n>The WG would like a definitive statement from the implementors about\n>which option makes the most sense.\n\nAs \"one of the implementors\" here is what I recommend as a practical solution:\n\n1) A client always sends a request using the latest version it supports.\nOnly if the client already knows the version of the server and this version\nis inferrior to the version of the client, then it should downgrade to a\nversion understood by the server.\n\n2) A server always responds with the same version as the request. Only if\nthis version is not directly supported by the server, it should take the\nversion that comes closest. If the semantics of the response requires a\nspecific version and this is not the version of the client then it should\nreturn \"505 HTTP Version not supported\".\n\nThis is basically the same as is stated in section 19.7:\n\nIt is beyond the scope of a protocol specification to mandate compliance\nwith previous versions. HTTP/1.1 was deliberately designed, however, to\nmake supporting previous versions easy. It is worth noting that at the time\nof composing this specification, we would expect commercial HTTP/1.1\nservers to:\n\n - recognize the format of the Request-Line for HTTP/0.9, 1.0, and 1.1\nrequests;\n - understand any valid request in the format of HTTP/0.9, 1.0, or 1.1;\n - respond appropriately with a message in the same major version used by\nthe client. \n\n>In general, IETF process works better when the implementors ARE the\n>working group.\n\nYep - you said it!\n\nHenrik\n\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-11038177"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "At 03:38 PM 10/20/96 +0200, Koen Holtman wrote:\n\n>[...]\n>> selection of multiple variants of a resource (by \n>>allowing the client, rather than the server, to make the selection),  \n>\n>PEP negotiates on _services_.  Negotiation on _content_ is orthogonal\n>to PEP, and this WG is already working on a content negotiation\n>mechanism with the attributes you mention above.\n\nI don't think there's much of a difference at all! PEP is about extensions\nand as more Web applications get beefed up with plug-ins, the capabilities\nthat many content providers in practice infer from the User-Agent becomes\ninvalid or at least a small subset. Current examples are HTML math, style\nsheets, and HTML tables not to mention what versions of these are\nsupported. These things may very well be supported by plug-ins and hence\nthe distance between extensions and content negotiation disappears.\n\nThe only dimension currently in content negotiation that is difficult to\nconsider in this game is natural language but it is by no means\n\"untouchable\". It's all features, really, and this is why PEP is\ninteresting as being part of HTTP.\n\nHenrik\n\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-11048791"}, {"subject": "Re: Server response version numbe", "content": "> 1) A client always sends a request using the latest version it supports.\n> Only if the client already knows the version of the server and this version\n> is inferrior to the version of the client, then it should downgrade to a\n> version understood by the server.\n\nWhy bother downgrading? MAY downgrade, not SHOULD downgrade.\n\n> 2) A server always responds with the same version as the request. Only if\n> this version is not directly supported by the server, it should take the\n> version that comes closest. If the semantics of the response requires a\n> specific version and this is not the version of the client then it should\n> return \"505 HTTP Version not supported\".\n\nYou say \"always\" in the first sentence and then give conditions when\nit doesn't hold in the second. That's confusing.\n\nPerhaps you mean to say that a server SHOULD respond with the same\nversion as the request?\n\nOn a related issue, I'm concerned about the possibility that a single\nserver might 'downgrade' depending on the URL rather than the version\nof the requestor: I think there are clients that presume the version\nof subsequent responses based on the version of previous responses.\n(The problem are CGI-generated headers, and the possibility of\nshortcutting header-rewriting.) I think the issue of 'downgrading'\nshould explicitly prohibit this behavior.\n\nLarry\n\n\n\n", "id": "lists-010-11058710"}, {"subject": "Re: Server response version numbe", "content": "On Sun, 20 Oct 1996, Larry Masinter wrote:\n\n> Ahe server should always return its http version.  I'm concerned there\n> may be some yet-unidentified parts of the current HTTP/1.1\n> specification that are inconsistent with that approach, but we should\n> fix those, since the alternative just doesn't make sense.\n\nI don't think this makes sense .... my implemented conclusion has been:\n\n1.  The protocol level returned in the response should declare the level \n    of the response.\n2.  The response protocol level from a server should never be higher\n    than the request's protocol.\n3.  The client must interpret the response based on the response protocol\n    level, not the request protocol level.\n\nI believe there is potential breakage otherwise.\n\nDave Morris\n\n\n\n", "id": "lists-010-11068230"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "Henrik Frystyk Nielsen:\n>\n>At 03:38 PM 10/20/96 +0200, Koen Holtman wrote:\n>\n>>[...]\n>>> selection of multiple variants of a resource (by \n>>>allowing the client, rather than the server, to make the selection),  \n>>\n>>PEP negotiates on _services_.  Negotiation on _content_ is orthogonal\n>>to PEP, and this WG is already working on a content negotiation\n>>mechanism with the attributes you mention above.\n>\n>I don't think there's much of a difference at all! PEP is about\n>extensions\n ^^^^^^^^^^\n\nWell, I think we need to make a separation between \n\n 1) extensions which offer content rendering facilities and\n\n 2) extensions which offer protocol services.\n\nIn my opinion, not making this separation will destroy cachability.\n\nTransparent content negotiation was designed to handle 1) among other\nthings.  I hope that PEP will handle 2).  If you think that PEP should\nbe made to handle 1) because nothing else does, we have a big\nsynchronization problem.\n\n>Henrik\n\nKoen.\n\n\n\n", "id": "lists-010-11076170"}, {"subject": "Re: Server response version numbe", "content": "At 6:18 PM -0400 1996-10-20, Henrik Frystyk Nielsen wrote:\n>>\n>>The WG would like a definitive statement from the implementors about\n>>which option makes the most sense.\n>\n>As \"one of the implementors\" here is what I recommend as a practical solution:\n>\n>1) A client always sends a request using the latest version it supports.\n>Only if the client already knows the version of the server and this version\n>is inferrior to the version of the client, then it should downgrade to a\n>version understood by the server.\n>\n>2) A server always responds with the same version as the request. Only if\n>this version is not directly supported by the server, it should take the\n>version that comes closest. If the semantics of the response requires a\n>specific version and this is not the version of the client then it should\n>return \"505 HTTP Version not supported\".\n\nHenrik,\n\nThis is a statement of one way to handle the situation.\n\nYou are missing the design rationale for why one should use this approach\nat all.\n\nIt would be most helpful to see a comprehensive analysis of all 1.1 and 1.0 methods\nand proxy situations to see if downgrade is the best approach.\n\nWhat are the reasons for the approach?  How do these reasons evolve\nwhen new protocol versions come out? How much sense do they make when few\n1.0 clients and servers remain?\n\nIf the downgrade approach is best, the spec should read MUST to ensure consistent\nimplementations.\n\n\n\n", "id": "lists-010-11085026"}, {"subject": "Re: Server response version numbe", "content": "Henrik,\n\nThe main problem area is in the behavior of proxy servers.\n\nConsider the cases of:\n\nClient  Proxy Proxy Server\n1.11.11.11.1\n1.11.01.11.1\n1.11.11.01.1\n1.01.01.11.1\n1.01.11.01.1\n\nConsider the methods:\n\nPUT\nPOST\nTRACE\nOPTIONS\nHEAD\nGET\n\nWhat happens when a 1.1 client tries to do a 1.1 put via a 1.0\nproxy to a 1.1 server?\n\n\n\n", "id": "lists-010-11094494"}, {"subject": "Re: Server response version numbe", "content": "John C. Mallery writes:\n > What happens when a 1.1 client tries to do a 1.1 put via a 1.0\n > proxy to a 1.1 server?\n\n(Speaking again of my own implementation which conforms to Henrik\ndescription). The 1.1 client detect that the proxy is 1.0 either:\n\na) Because it has used it in the past, and that 1.0 proxy has replied\n   with a 1.0 reply (to the first 1.1 request) - most probable case.\nb) Because at the time it issued the PUT, it will wait (for some\n   timeout value), for a 100 status code that will never come.\n\nSo at least the client <-> proxy link is ok. Now, the 1.0 proxy speaks\nto the 1.1 server, through a 1.0 request, so the server can deal with\nit properly.\n\nI don't have a deep analyzises to propose here, but my intuition is\nthat Henrik's suggestion (of replying with the version given by the\nclient), is the only one that will not hurt.\n\nAnselm.\n\n\n\n", "id": "lists-010-11102518"}, {"subject": "Re: Server response version numbe", "content": "    OK. A bunch of implementors are busy committing to the server\n    returning the version of the client.  I just changed my server to\n    return the client version.\n\n    I would really appreciate a definitive statement from the WG on\n    this matter as I need to release a new version of my server NOW (!)\n    and I would prefer not to pollute the world if at all possible.\n\nHere's one definitive statement that, as far as I can tell, nobody\nelse has made:\n\nA server MUST NOT return a response with an HTTP version\nnumber if the server does not comply with the specification\nfor that version.  In particular, a server MUST NOT return\na response with a higher version number than the highest\nversion it supports.  No exceptions, not even for really\nstupid people.\n\nPerhaps nobody has said this because it is \"obvious\", yet we\nhave heard in the past of implementors who tried to use the\n(non-existent) exemption-for-really-stupid-people.\n\nAs for whether a server should return\n   (a)min(request-version, server's-highest-version)\nor just\n   (b)server's-highest-version\nI don't have a strong opinion.  Perhaps rather than trying\nto argue this out using the current Proposed Standard as a\nfixed point, we should take a look at the current HTTP/1.1\ndraft and see whether anything in it would break if we didn't\nimpose a choice between (a) and (b).  Maybe if we find something\nthat would break, then one possible option is to treat this\nas a bug in the HTTP/1.1 spec, rather than a reason to make\nthe decision between (a) and (b)?  (Remember, the reason for\nthe Proposed Standard stage is to find such bugs; the current\nwords are not carved in stone.)\n\n-Jeff\n\n\n\n", "id": "lists-010-11111075"}, {"subject": "Re: HTTP Log file", "content": ">Not if you're referring to the one that Paul Leach and I have\n>been working on.  We're working on a heavily revised draft, but\n>it still won't discuss log formats.  Phill Hallam has issued\n>some drafts on ways for servers to ask proxies for their logs,\n>and I would imagine that he has considered the costs associated\n>with retrieving tens or hundreds of megabytes of log info like\n>this; the hit-metering work that Paul and I am doing is designed\n>to avoid this kind of thing.\n\nI have indeed considered these problems. But I distinguish between \nseveral different uses of log files. The main point for a spec\nis to have a format that  is a neutral interchange medium that\nall servers might be expected to implement.\n\nThe Extended log file format does have provision for compression\nby collapsing a series of entires with the same value to a single \nline beginning with a number to show the number of times it occurs.\n\nI don't think that you can fairly claim that I have a \"megabytes\"\nof data problem without accepting that your scheme has a worse\none. Rather than beginn a communication for every hit I parcel up \nthe infomation to be exchanges and pass it in a single communication\nat a cost of a single line of text per hit. In the \"simple\" exchange\nprotocol you have to create an entire message per hit - much\nmore expensive.\n\n\nI would imagine that a large national cache with a trafic in the\ntens of millions of hits per server per day would want to exchange\nlog information frequently. I would expect such a server to be \nkeeping an in-memory index to the cache since without such an index\nit would be unable to keep up with that level of load in any case.\n\nRecording the number of hits would mean a single slot in the index\nstructure. Its probably not even an additional slot since I would \nexpect the cache maintenance algorithm would require the same data. \nAt a chosen time the proxy would simply traverse the list of servers\nwhich had requested notification and walk down the tree of index\nrecords for each one. \n\nIf a more comprehensive exchange of information were required the\nserver would need to keep a per-hit record somewhere. I designed the\nlog file format so as to allow such a server to simply append \ninformation to the end of the log. Such a server would keep an\nadditional separate log for each subscribing server. The additional\neffort required to do this is no more than twice the effort of \ncurrent log keeping methods.\n\nAs to the issue of whether to support fast binary formats for \nlogging I don't think that these should be standardized at this \nmoment in time. The first priority is to have a common interchange \nstandard so that there can be a hope of creating analysis tools.\nA binary format with features such as k-d tree indicies and full\nindexicallity would be nice. I don't think that the perl hackers\nare likely to be able to implement such a scheme and the commercial\nvendors are unlikely to be interested in it unless they can design\nit themselves.\n\n\nPhill\n\n\n\n", "id": "lists-010-11120276"}, {"subject": "Re: Server response version numbe", "content": "** Reply to note from \"David W. Morris\" <dwm@xpasc.com> Mon, 21 Oct 1996 00:10:06 -0700 (PDT) \n> I don't think this makes sense .... my implemented conclusion has been: \n>    \n> 1.  The protocol level returned in the response should declare the level  \n>     of the response. \n> 2.  The response protocol level from a server should never be higher \n>     than the request's protocol. \n> 3.  The client must interpret the response based on the response protocol \n>     level, not the request protocol level. \n>    \n> I believe there is potential breakage otherwise. \n>    \n> Dave Morris \n>    \n \nI agree with Larry that the server should always delcare its \nimplementation level.  For an orgin server, this version is nothing \nmore than a declaration of capabilities, just as the protocol level in \nthe request is a statement of the client's capabilites.  At that point, \nserver-driven negotiation takes over. \n \nDraft 7, section 3.1 (HTTP Version) says: \n\"The protocol versioning policy is intended to allow the sender to \nindicate the format of a message and its capacity for understanding \nfurther HTTP communication, rather than the features obtained via that \ncommunication.\" \n.. \n\"The HTTP version of an application is the highest HTTP version for \nwhich the application is at least coditionally compliant\". \n \nWe all already know that the protocol negotiation capabilities of HTTP \nneed work (which is the reason for PEP, right?), and overloading the \nmeaning of the protocol version is sure to cause confusion and cause \nbugs in implementations. \n \nFor your first case, there is nothing gained, because there is nothing \nto stop, 1.0 clients from using 1.1 features without declaring \nthemselves to be 1.1 compliant. \n \nFor your second case, this is a statement of the server's backward \ncompatibility.  Since the spec does not require backward compatiblity, \nthere is no reason to enforce this.  As a practical matter, \nimplementers will, in general, provide the level of compatibility \nrequired for their market. \n \nFor your third case, the client, if downlevel from the server, will \nonly interpret the things it understands (which could still be more \nthan what is defined in the protocol level it claims compliance with). \nIf the client is uplevel of the server, there is nothing to do. \n \nPerhaps you could give an example of a potential breakage? \n \nThis thread of discussion, however, does raise the question of why \nbackward compatibility is not required by the spec (since it is an \nissue sure to be addressed by implementers). \n \nThis leads me to another question: I have seen some discussion \nin the archives of what version an implementation should claim, but \nnothing definitive (maybe I missed it).  When is it safe for an \nimplementation to claim \"HTTP/1.1\" compliance (in terms of standards \nmilestones)? \n \n \n \n\nThank You, \nRichard L. Gray\n\ncc: \"David W. Morris\" <dwm@xpasc.com>\n\n\n\n", "id": "lists-010-11130376"}, {"subject": "Re: Server response version numbe", "content": "At 09:09 PM 10/20/96 PDT, Larry Masinter wrote:\n>> 1) A client always sends a request using the latest version it supports.\n>> Only if the client already knows the version of the server and this version\n>> is inferrior to the version of the client, then it should downgrade to a\n>> version understood by the server.\n>\n>Why bother downgrading? MAY downgrade, not SHOULD downgrade.\n\nThis was meant as a practical advice and some existing 1.0 server\napplications break or act confused if you send them an HTTP/1.1 request.\nTherefore it was more of a \"if you want to be on the safe side\" should than\ntargeted directly towards the specification. I actually don't think this\nbelongs in the spec except from the part already being there in section 19.7.\n\n>> 2) A server always responds with the same version as the request. Only if\n>> this version is not directly supported by the server, it should take the\n>> version that comes closest. If the semantics of the response requires a\n>> specific version and this is not the version of the client then it should\n>> return \"505 HTTP Version not supported\".\n>\n>You say \"always\" in the first sentence and then give conditions when\n>it doesn't hold in the second. That's confusing.\n\nNo, it's just another way or arguing which I thought was used frequently.\n\na) First you give the general case\nb) Then you give all the exceptions\n\n>Perhaps you mean to say that a server SHOULD respond with the same\n>version as the request?\n\nOnly if it is capable of doing so. Otherwise not. I actually believe this\nis close to what Jeff points out in his (later) mail.\n\nOK - no more on versions from my side!\n\n>On a related issue, I'm concerned about the possibility that a single\n>server might 'downgrade' depending on the URL rather than the version\n>of the requestor: I think there are clients that presume the version\n>of subsequent responses based on the version of previous responses.\n>(The problem are CGI-generated headers, and the possibility of\n>shortcutting header-rewriting.) I think the issue of 'downgrading'\n>should explicitly prohibit this behavior.\n\nI don't follow you on this one: \"'downgrade' depending on the URL\"?\n\nThere are most certainly clients that remember the version of a HTTP\nserver. This is for example part of libwww 5.0. The reason is that this if\nyou know that this the server is a HTTP/1.1 then you can do pipelining\nimmediately after you have opended a new connection. You don't have to wait\nuntil the first response comes back.\n\nThis brings up another problem that I am not sure how to go about. Let's\nsay that we (the client) want to use pipelining as much as we can to all\nHTTP/1.1 servers. Now, first time we talk to a server, we do not know the\nversion so we send a HTTP/1.1 request but don't do any pipelining. If it is\na HTTP/1.1 server and it keeps the connection open then we immediately\nstart sending x new requests down the pipe.\n\nWe then pause for 20 seconds before we are asked to issue 15 new requests\nto the same host. Exactly at that same time, the server times out the\nconnection and sends a FIN. This packet has not reached us yet so we start\nsending the 15 requests and everything is fine.\n\nDepending on timing and how much we wrote we may either get the FIN or a\nbroken pipe on either the read or write. I have the feeling that this also\ndepends on the TCP stack but I am not sure.\n\nThis can (and should) all be dealt with but the remaining problem is\nwhether we can open a new connection and do pipelining immediately or we'll\nhave to wait for the first response before doing pipelining again. The\nsource of the problem is that we don't know _why_ the connection was\nclosed: Was it because of timeout, or because of overload or something\nelse. If we allow pipelining immediately upon opening a new connection\n(which I strongly would prefer) then we assume that the connection was\nclosed due to a timeout. That may be OK, but I am not sure. If not then we\nneed a mechanism for the server to say why it closed the connection.\n\nDoes this story make sense at all?\n\nThanks,\n\nHenrik\n\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-11141018"}, {"subject": "Re: Server response version numbe", "content": "Jeffrey Mogul:\n>\n>As for whether a server should return\n>   (a)  min(request-version, server's-highest-version)\n>or just\n>   (b)  server's-highest-version\n>I don't have a strong opinion.\n\nI don't have a strong opinion either.  (b) makes for more interesting\ndiagnostics, but that is about it.\n\nIt is important to note here that, as the 1.1 spec does not tell\nwhether to do (a) or (b) if the numbers are 1.0 and 1.1, it therefore\nallows both.  So if you are implementing a server, pick either one.\n\nIf this WG were to make a definitive statement that you must either\npick (a) or (b), that would amount to us updating the 1.1 spec, and we\nwant to avoid doing that.\n\nOf course, if things break because the spec allows both (a) and (b),\nwe will have no choice but to update the spec.  We will only find out\nabout such breakage however if some implementers do (a) while others\ndo (b)...\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-11153453"}, {"subject": "Re: Server response version numbe", "content": "Larry> On a related issue, I'm concerned about the possibility that a\nLarry> single server might 'downgrade' depending on the URL rather\nLarry> than the version of the requestor: I think there are clients\nLarry> that presume the version of subsequent responses based on the\nLarry> version of previous responses.\n\nI believe such servers are implicitly allowed by the definition of\n\"server\":\n\n  Likewise, any server may act as an origin server, proxy, gateway, or\n  tunnel, switching behavior based on the nature of each request.\n\nImagine a proxy server that switches to tunneling behavior when\nproxying for a 1.0 server, but is 1.1 otherwise.\n\nHowever, the draft says this:\n\n  Clients SHOULD remember the version number of at least the most\n  recently used server\n\nI don't think the version number of the server is actually knowable;\ninstead all a client can discover is the version number the server\nused on the most recent request.\n\nTom\n-- \ntromey@cygnus.com                 Member, League for Programming Freedom\n\n\n\n", "id": "lists-010-11162133"}, {"subject": "Re: Server response version numbe", "content": "  Larry> On a related issue, I'm concerned about the possibility that a\n  Larry> single server might 'downgrade' depending on the URL rather\n  Larry> than the version of the requestor: I think there are clients\n  Larry> that presume the version of subsequent responses based on the\n  Larry> version of previous responses.\n\n  I believe such servers are implicitly allowed by the definition of\n  \"server\":\n\n    Likewise, any server may act as an origin server, proxy, gateway, or\n    tunnel, switching behavior based on the nature of each request.\n\n  Imagine a proxy server that switches to tunneling behavior when\n  proxying for a 1.0 server, but is 1.1 otherwise.\n\nThe current Apache 1.2-dev code line actually does something rather\nlike this, responding with protocol version HTTP/1.1 when functioning\nas an origin server, and as HTTP/1.0 when functioning as a proxy.\n\nI can certainly imagine that a single client might want to contact the\nsame Apache server in both roles (for instance, in an organization\nwhich used the same Apache server as both client and internal\ninformation server, and which configures their clients to contact\ninternal servers directly, but go through proxies to get to external\nones --- a capability that widely deployed clients do support).\n\nrst\n\n\n\n", "id": "lists-010-11171127"}, {"subject": "Re: Server response version numbe", "content": "  an organization\n  which used the same Apache server as both client and internal\n  information server\n\nBlargh.  I obviously meant \"as both proxy and internal information server\"\n(noting that a single client which used the same server in both roles would\nsee it emit two different protocol versions...).\n\nrst\n\n\n\n", "id": "lists-010-11180077"}, {"subject": "Re: Server response version numbe", "content": "I do believe this has been discussed before, particularly around the\ntime of the LA IETF meeting (on the mailing list).  In any case, this\nis both my personal recommendation as an implementor and the intention\nof what is written in both HTTP specs regarding the HTTP-version.\n\nThe HTTP-version of a message represents the protocol capabilities\nof the sender AND the gross-compatibility (major version number) of\nthe message being sent.  This allows a client to use a safe (HTTP/1.0)\nsubset of features in making a normal (HTTP/1.1) request, while at\nthe same time indicating to the recipient that it is capable of supporting\nfull HTTP/1.1 communication.  In other words, it provides a tentative\nform of protocol negotiation on the HTTP scale.  We do this because it\nallows each connection on a request/response chain to operate at its best\nprotocol level in spite of the limitations of some clients or servers\nthat are parts of the chain.\n\nThe HTTP-version provided by the server should be the highest minor\nversion supported by that server within the same major version as sent\nby the client (i.e., a server today which is at least conditionally compliant\nwith HTTP/1.1 should always send HTTP/1.1 in its responses, even when it\nis only using the features of HTTP/1.0).  This is required to work because\nall HTTP/1.x protocols share a base level of compatibility (if they didn't,\nthey wouldn't be called HTTP/1.x).  Worries about some clients bombing on\nreceipt of \"HTTP/1.1\" in the Status-Line are pointless -- HTTP cannot be\nextensible if it is always tied to the mistakes of the lowest common\ndenominator of client hacker; it is better to force those clients\nto be upgraded than to hamstring the rest of the world forever.\n\nThe answer to the question \"Why doesn't the spec require servers to always\nsend HTTP/1.1 or to always send the client's version?\" is quite simple:\nthe specification only governs an implementation when it sends \"HTTP/1.1\".\nIf the server sends some other version, then it is obeying the requirements\nof some other protocol, over which the HTTP/1.1 specification has no\nrelevance.  If the server decides to switch between the two at random,\nthen I would say that the server is randomly compliant with HTTP/1.1,\nand the results in terms of robustness are equally random.  However, it\nprobably would not adversely affect the communication, since the really\nimportant half of the HTTP-version decision is that the client always\nsend its best version.\n\nI am willing to live (unhappily) with a server that switches from HTTP/1.1\nto HTTP/1.0 when it is used as a proxy because it is unlikely that\nany current browser will treat an origin server and a proxy server as the\n\"same server\", because we will fix or remove that proxy relatively soon\n(probably before any significant HTTP/1.1 browsers are deployed), and because\nthe likelihood that such a protocol downgrade will actually result in\ncommunication problems on later requests is quite small.  I would be less\ninclined to support a server which downgraded based on the client version,\nsince I see the server's version as a guarantee of good behavior, and that\nguarantee is a good thing even when the client doesn't understand it.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-11188282"}, {"subject": "(CODES) Consensu", "content": "I recieved no comments on the draft wording, so unless I hear\nother comments, they are adopted.\n- Jim\n\nAdditions/changes to section 9.4 of the 1.1 specification.   Send me any\nchanges or improvements.\n- Jim Gettys\n\n\n\n412 Precondition Failed\n\n    Same as current 412 -- just the reason phrase has changed and\n    the explanation associated with the as-yet-undefined precondition\n    header field name.  This may end up as \"reserved for future use\",\n    but the code will remain in the spec.\n\n413 Request Entity Too Large  [proposed by Jeff Mogul on http-wg, 07 Dec 1995]\n\n    If the server doesn't want to receive the large body, it\n    immediately replies with a 413 response, and\n    immediately closes (not resets) the connection.\n    \n    If the client manages to read the 413 response, it must\n    honor it and should reflect it to the user.\n\n    If this restriction is considered temporary, the server may include\n    a Retry-After header field to indicate that it is temporary and after\n    what time the client may try again.\n\n414 Request-URI Too Large\n\n    The server is refusing to service the request because the\n    Request-URI is longer than the server is willing to interpret.\n    This rare condition is only likely to occur when a client has\n    improperly converted a POST request to a GET request with long query\n    info, when the client has descended into a URL \"black hole\" of\n    redirections (e.g., a redirected URL prefix that points to a suffix\n    of itself), or when the server is under attack by a client\n    attempting to exploit security holes present in some servers using\n    fixed-length buffers for reading or manipulating the Request-URI.\n\n415 Unsupported Media Type\n\n    The server is refusing to service the request because the entity\n    body of the request is in a format not supported by the requested\n    resource for the requested method.\n\n\n------- End of Forwarded Message\n\n\n\n", "id": "lists-010-1119024"}, {"subject": "Re: Server response version numbe", "content": "> This thread of discussion, however, does raise the question of why \n> backward compatibility is not required by the spec (since it is an \n> issue sure to be addressed by implementers). \n\nIt was, in earlier drafts, but various people insisted that it be removed.\n\n> This leads me to another question: I have seen some discussion \n> in the archives of what version an implementation should claim, but \n> nothing definitive (maybe I missed it).  When is it safe for an \n> implementation to claim \"HTTP/1.1\" compliance (in terms of standards \n> milestones)? \n\nAs of the beginning of last month (when the IESG accepted draft 07 as\nan RFC without any major changes).  It is now unlikely that any significant\nprotocol requirements will be added without a corresponding bump in the\nversion number, which is my definition of being safe to claim \"HTTP/1.1\"\ncompliance, since there *will* be many implementations calling themselves\nHTTP/1.1 before the spec can be revised again.\n\n.....Roy\n\n\n\n", "id": "lists-010-11199123"}, {"subject": "Re: PEP Battle Plan [rexmit, garbled", "content": "> I realize this is heresy here, but I have to wonder if it's worth\n> building the extension mechanism into HTTP.  An efficient URI\n> resolution protocol would allow for a smooth transition away from\n> HTTP 1.x and to 2.x or other protocols (smb? webnfs? multicast?), \n> without invalidating old clients and without the overhead of \n> establishing a TCP connection.  New protocols could then be designed \n> from scratch to take into account everything that has been learned \n> from HTTP, without inheriting the complexity.\n\nYes, if\n\n   a) there was an efficient URI resolution protocol with sufficient\n      deployment and compatibility with old clients;\n\n   b) new clients were developed with such a dynamic protocol interface.\n\nThat is unquestionably the best way to improve the Web's extensibility\nin terms of new protocol capabilities, and I support it whole-heartedly.\nHowever, I've been supporting that for three years now and it is no closer\nto being a reality now than it was then, in spite of a lot of excellent work\nby some brilliant people.  The real problems are economic, not technical,\nand until the service exists it is difficult to say what it will accomplish.\n\n> It would also improve scalability, fault tolerance, ability to \n> screen files (for content ratings, price, language, etc.) before \n> downloading, selection of multiple variants of a resource (by \n> allowing the client, rather than the server, to make the selection),  \n> client selection of multiple locations of a resource, etc. \n> \n> Seems like we need to take a step back and look at the web as a\n> whole before we commit to the direction of adding more complexity\n> to HTTP.\n\nI do not see any conflict here.  While a URI resolution service would\nadd a selection and indirection layer above the hard-wired URL, it does\nnothing to change the work required to actually apply a method to the\nresource once it has been located.  That work will still need a protocol\nthat understands hierarchical proxies and can reasonably sustain\nextensions which ensure robust handling across all recipients.  Since none\nof the above-mentioned protocols can do what HTTP/1.1 already does, let\nalone support the reasoning about extensions proposed by EP/PEP/whatever,\nI don't see the creation of a URI resolution service as having an impact\non how or how not to extend HTTP.\n\nHaving said that, I would also not commit the IETF toward any single\ndirection, PEP or otherwise.  I am inclined to leave research to individuals\nand not assign things to a WG until the solution is (at least believed to be)\nknown.  The problem is deciding when that transition should occur, and\nwhether it should be part of this WG or a different WG or outside the IETF.\n\nFinally, I think there is something missing from this discussion. Extensions\nhave occurred, and will continue to occur, regardless of IETF opinions.\nThey occur because users need them, which results in implementers\nimplementing them, often in spite of the WG's recommendations.\nAlmost all of my work invested in HTTP/1.1 was toward making the protocol\n*more* extensible in areas that had previously faltered due to poor\nimplementations, so that people out there can implement what they want and\nat least have some inkling of what effect it will have on correctly\nimplemented applications.\n\nWe now have a protocol that should be able to sustain any optional extensions,\nbut we also know that some people want more than just optional extensions.\nNot defining son-of-PEP will not stop people from making those extensions,\nnor would it make those extensions any less complex in terms of their\naddition to HTTP (quite the opposite, in fact).  A better question, then,\nis whether we would prefer those extensions to be added within a framework\napproved by the IETF, or one outside the IETF?\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-11207352"}, {"subject": "Re: HTTPNG", "content": "Ben Laurie <ben@gonzo.ben.algroup.co.uk> writes:\n\n> Is there an HTTP-NG WG? A list?\n\n  http-ng[-request]@cuckoo.hpl.hp.com\n\n-- \n-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-010-11218810"}, {"subject": "Re: HTTPNG", "content": "Andy Norman wrote:\n> \n> Ben Laurie <ben@gonzo.ben.algroup.co.uk> writes:\n> \n> > Is thist?\n> \n>   http-ng[-request]@cuckoo.hpl.hp.com\n> \n> --\n>                                                 -- ange -- <><\n> \n> http://www-uk.hpl.hp.com/people/ange            ange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-010-11226142"}, {"subject": "HTTP/1.0 persistent connection", "content": "Section 19.7.1 of the draft says:\n\nWhen it connects to an origin server, an HTTP client MAY send the Keep-\nAlive connection-token in addition to the Persist connection-token:\n\n       Connection: Keep-Alive\n\nWhat is the \"Persist connection-token\"?  I couldn't find another\nreference to it anywhere.  Is there another document describing\n1.0-style keepalives?\n\nTom\n-- \ntromey@cygnus.com                 Member, League for Programming Freedom\n\n\n\n", "id": "lists-010-11233305"}, {"subject": "Re:  HTTP/1.0 persistent connection", "content": "Tom Tromey <tromey@creche.cygnus.com> wrote:\n  > Section 19.7.1 of the draft says:\n  > \n  > When it connects to an origin server, an HTTP client MAY send the Keep-\n  > Alive connection-token in addition to the Persist connection-token:\n  > \n  >        Connection: Keep-Alive\n  > \n  > What is the \"Persist connection-token\"?  I couldn't find another\n  > reference to it anywhere.  Is there another document describing\n  > 1.0-style keepalives?\n\nThat sentence is vestigial, lingering from the time before we made\npersistent connections the default.  The example used to read:\nConnection: Keep-Alive,Persist\n\nSo the wording in the draft should be corrected to:\n\nWhen it connects to an origin server, an HTTP client MAY send the Keep-\nAlive connection-token:\n\n       Connection: Keep-Alive\n\nDave Kristol\n\n\n\n", "id": "lists-010-11240083"}, {"subject": "HTTP/1.1 servers", "content": "Forgive me for asking such a question, but is an experimental 1.1 server\navailable that will run under Win32? (Windows 95 would be nice.) Better\nyet, how about a Java implementation? \n\nI've been experimenting with the idea of using HTTP/1.1 as a messaging\nprotocol, and it would be very useful to be able to have a system with\nwhich I could work. Unfortunately, Unix boxes are hard to come by at the\nmoment.\n\nOn a related note when the PEP question was posted I was so swamped with\nother things that I really didn't have time to go through the draft and\ndetermine how important these extensions would be for my purposes. Where\ndoes PEP stand?\n\n---\nGregory Woodhouse     gjw@wnetc.com\nhome page:            http://www.wnetc.com/\nresource page:        http://www.wnetc.com/resource/\n\n\n\n", "id": "lists-010-11247566"}, {"subject": "HTTP/1.1 servers", "content": "Gregory J. Woodhouse writes:\n > Forgive me for asking such a question, but is an experimental 1.1 server\n > available that will run under Win32? (Windows 95 would be nice.) Better\n > yet, how about a Java implementation? \n > \n\nJigsaw is what you are looking for, it also includes a 1.1 compliant\n(as far as I can tell ;-) caching proxy module.\n\nhttp://www.w3.org/pub/WWW/Jigsaw/\n\nThe HTTP/1.1 client side API can be used to run HotJava, so you also\nhave a 1.1 browser for free !\n\nAnselm.\n\n\n\n", "id": "lists-010-11255474"}, {"subject": "Persistent Connections  Mandating who closes", "content": "HTTP/1.1 draft 7 (RFC to be) makes no recommendation on which of client or\nserver should close the transport connection after a response has finished\nwhich included a \"Connection: close\" in either request or response.\n\nrom a TCP perspective, this is v. significant. First end to close has to go\ninto (typically) 4 min. TIME-WAIT state continuing to reserve the RAM\nassociated with the control block etc. Obviously the spec. has to allow\neither end to close the transport connection, but it should be possible to\nmandate that to even be stamped \"conditionally compliance\" a client\nimplementation MUST be responsible for closing the transport connection in\nresponse to a \"Connection: close\" header, but it SHOULD also be able to\nhandle the server closing the transport connection. Otherwise, the \"problem\nof the commons\" says the client won't close it unless the standard tells it\nto. Mandating such asymmetry would spread the cost of the few extra bytes\nneeded for each connection in TIME-WAIT across the large number of users,\ninstead of the smaller number of server operators. This should reduce the\ntotal cost of the Web to the world (taking into account that server\noperators recover their costs from users).\n\nIf this is what the spec is meant to say, it doesn't read that way to me. If\nit wasn't meant to say this, then surely it's not too late to open up a\ndebate as to whether it should say this?\n\nBTW, I've checked and can find no mention of this in the list archives.\n\nBob\n____________________________________________________________________________\nFrom:    Bob Briscoe,                                BT, Distributed Systems\nPost:    B54 74, BT Labs,    Martlesham Heath,   Ipswich, IP5 7RE,   England\nE-Mail:  rbriscoe@jungle.bt.co.uk\nTel:     +44 1473 645196                                Fax: +44 1473 640929\nWWW:     http://www.jungle.bt.co.uk/people/rbriscoe.html  (BT intranet only)\n\n\n\n", "id": "lists-010-11262748"}, {"subject": "Re: Document Action: Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.0 toInformationa", "content": "At 9:38 AM 4/1/96, The IESG wrote:\n>The IESG has reviewed the Internet-Draft \"Hypertext Transfer Protocol\n>-- HTTP/1.0\" <draft-ietf-http-v10-spec-05.txt, .ps> and recommends that\n>it be published by the RFC Editor as an Informational RFC, but with the\n>following IESG Note:\n>\n>The IESG has concerns about this protocol, and expects this\n>document to be replaced relatively soon by a standards track\n>document.\n\nMaybe I'm being a bit picky here, but it should be made clear that there\nare no known plans in the HTTP WG to replace the HTTP/1.0 document, nor to\nreplace the HTTP/1.0 protocol described in the document in question. There\ncertainly are plans to come out soon with a *different document*, and that\ndocument will described a *different protocol*, namely HTTP/1.1.\n\nOf course, there are also plans to try to get as much of the world to adopt\nthe new protocol as soon as we can. However, this is an implementation\nissue.\n\n\n\n", "id": "lists-010-1126972"}, {"subject": "Caching multipart dat", "content": "It occurs to me that under many circumstances the entity carried by an HTTP\nmessage could consist of a mixture of very static information and highly\nvolatile data. For example, if HTTP is used to retrieve a database record,\nmany fields included in the reply will be very stable (e.g., name, address,\nhair color, etc.) and other fields will be quite volatile, possibly\nchanging daily or hourly. It makes sense to bundle such a message in MIME\nformat with the stable fields grouped together (albeit with a strong\nvalidator) and the less stable fields in related groups. The idea is that a\ncache that could handle the parts of a multipart MIME message separately\nwould be able to validate a message with considerably less overhead. In\nfact, it would be possible to have data that should not be cached at all\n(such as a stock price, humidity, heart rate, etc.) retrieved from the\norigin server with each request, but information that can be cached can\nsimply be revalidated. \n\n---\nGregory Woodhouse     gjw@wnetc.com\nhome page:            http://www.wnetc.com/\nresource page:        http://www.wnetc.com/resource/\n\n\n\n", "id": "lists-010-11272035"}, {"subject": "Re: Caching multipart dat", "content": "    It occurs to me that under many circumstances the entity carried by\n    an HTTP message could consist of a mixture of very static\n    information and highly volatile data. For example, if HTTP is used\n    to retrieve a database record, many fields included in the reply\n    will be very stable (e.g., name, address, hair color, etc.) and\n    other fields will be quite volatile, possibly changing daily or\n    hourly. It makes sense to bundle such a message in MIME format with\n    the stable fields grouped together (albeit with a strong validator)\n    and the less stable fields in related groups. The idea is that a\n    cache that could handle the parts of a multipart MIME message\n    separately would be able to validate a message with considerably\n    less overhead. In fact, it would be possible to have data that\n    should not be cached at all (such as a stock price, humidity, heart\n    rate, etc.) retrieved from the origin server with each request, but\n    information that can be cached can simply be revalidated.\n\nThis is a good point.  The current HTTP design does not have a way\nto provide meta-data (i.e., caching-related headers) for units\nwith finer grain than an entire HTTP response message, and so it\nmight be quite difficult to create a compatible extension that\nallowed cache validation of individual pieces of a Multipart response.\n\nHowever, if you view the problem more abstractly, what you are really\nasking for is a kind of data-compression mechanism.  I.e., your actual\ngoal is not really individual validation of the pieces, but rather to\nprevent the unnecessary transfer of the stable pieces when the unstable\npieces change.\n\nWhen one takes that (more abstract) view, it can be applied to all\nsorts of resources, not just multipart ones.  For example, imagine\na one-piece HTML file showing a lot of information about a company,\nincluding its current stock price (which changes frequently).  If\nwe could somehow arrange to transfer just the stock price info\non an update, and rely on the cache for the rest, then we could\nsave a lot of bits.\n\nThis is basically \"delta-encoding\": saving time by transmitting\nthe \"delta\" (difference) between two successive data elements,\nrather than transmitting each in its entirety.  And, in fact,\nseveral research projects have already been looking at this\npossibility.  For example, there was a brief mention buried in\n        Removal Policies in Network Caches for World-Wide Web Documents,\n        S. Williams, M. Abrams, C.R. Standridge, G. Abdulla, and E.A. Fox\n(Virginia Tech) Proc. SIGCOMM '96 (August, 1996)\nand there will be a paper with a related approach in the forthcoming\nUSENIX conference (Jan. 1997):\nOptimistic Deltas for WWW Latency Reduction\nGaurav Banga, Fred Douglis, and Michael Rabinovich, AT&T Research \n\nThere's also a paper at Mobicom that (from its title) might be\nrelated, but I haven't seen a copy yet:\n  WebExpress: A System for Optimizing Web Browsing in a Wireless Environment\n  B.C. House and D.B. Lindquist, IBM Corporation\n\nAlthough the basic concept is pretty simple, there are a lot of\nreally hard research problems to solve.  For example, what is\nthe best way to compute and encode the difference between two\ninstances of a resource?  This probably varies based on Content-type!\nAnd how many different \"base versions\" should the servers and proxy\ncaches keep around, and for how long?  And how do the client and servers\ncommunicate the necessary meta-information?\n\nAnd, finally, how well would this actually work in practice?  Or\ndo most changeable documents change enough that delta-encoding doesn't\nsave anything?\n\nI've been talking with the AT&T folks and the Virginia Tech folks\nabout capturing a day or so's worth of the content that flows through\nDigital's proxy server (we're up to about 1.5 million requests on\na good day), and then trying to compute deltas based on various\nalgorithms.  But this is going to require some hacking on our proxy\ncode, and a fair amount of disk space, so I haven't had a chance\nto get started on it.\n\n-Jeff\n\n\n\n", "id": "lists-010-11280505"}, {"subject": "Re: Caching multipart dat", "content": "Of course, Jeffrey Mogul is correct that at least in part this really a URI\nissue in that there is no Content-ID scheme which will allow us to implement\ncaching at this level of granularity. Something off the top of my head\n(which probably means it has a lot of associated problems) is that complex\nobjects could be encoded using external-body components with HTTP as the\naccess method. Of course, if a component has a URI which references the\nsame host as the origin server it (What if the URI is a URN and the host\nis unknown? Or for that matter, how many times do we want to query name\nauthorities? It seems we might want to restrict ourselves to URLs here, but\nI digress) should be possible to use the same  connection. Better yet,\nindividual components should be validated upstream if possible (perhaps even\non the origin server itself!) \n---\nGregory Woodhouse     gjw@wnetc.com\nhome page:            http://www.wnetc.com/\nresource page:        http://www.wnetc.com/resource/\n\n\n\n", "id": "lists-010-11291815"}, {"subject": "Re: Caching multipart dat", "content": "I think in the current web architecture, the granularity of caching is\nexactly 'what can be pointed to by a URL', and if you want to deliver\na database record and cache some parts and not others, what you'd do\nis deliver a composite object that explicitly includes the other\nparts. If you want to use 'multipart/related' for the container,\nthat's fine, but you might find each part being delivered as\n'message/external-body;access-type=URL\".\n\n\n\n", "id": "lists-010-11300430"}, {"subject": "RE: secure htt", "content": "The SSL spec is available at http://home.netscape.com/eng/ssl3/ssl-toc.html.\nYaron\n\n-----Original Message-----\nFrom:Tobias Walkowiak [SMTP:walko@cadlab.tu-berlin.de]\nSent:Saturday, October 26, 1996 12:20 PM\nTo:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nSubject:secure http\n\n\nhi,\n\ncan anybody help me with some information on s-http? i like to know about\nimplementations mostly. references are also very welcome.\n\nthanx in advance\n\nwalko\n-- \n-- walko@cadlab.tu-berlin.de       http://www.cadlab.tu-berlin.de/~walko/ --\n-- pgp key available via finger walko@cadillac.fb12.tu-berlin.de          --\n-- pgp key fingerprint = 91 96 22 DA 5E 5B 6B 98  97 59 0C BA D2 06 EF 50 --\n--\n\n\n\n", "id": "lists-010-11307799"}, {"subject": "secure htt", "content": "hi,\n\ncan anybody help me with some information on s-http? i like to know about\nimplementations mostly. references are also very welcome.\n\nthanx in advance\n\nwalko\n-- \n-- walko@cadlab.tu-berlin.de       http://www.cadlab.tu-berlin.de/~walko/ --\n-- pgp key available via finger walko@cadillac.fb12.tu-berlin.de          --\n-- pgp key fingerprint = 91 96 22 DA 5E 5B 6B 98  97 59 0C BA D2 06 EF 50 --\n--\n\n\n\n", "id": "lists-010-11316859"}, {"subject": "status code 100 (continue", "content": "The way I read section 8.2 (message transmission requirements) is: \n \nIF ((client is 1.1) AND (NOT connection:close)) then I MUST send \nstatus 100 or an error. \n \nWill clients get confused receiving 100 even if connection:close? \n \nDo I *really* have to send a Date header? \n(Well, according to 14.19 I do, but that is for caching, and as far as \nI can tell, only terminal stupidity - in the form of a cache-control \ndirective - will cause a 100 response to be cached). \n \nAnd, (once again) what is the rationale for the 100 response? \nIt seems unnecessary and wasteful; what am I missing here? \n\n\nThank You, \nRichard L. Gray\n\n\n\n", "id": "lists-010-11324031"}, {"subject": "New draft: draft-ietf-http-feature-reg00.txt (Feature Tag Registration Procedures", "content": "We have just finished draft-ietf-http-feature-reg-00.txt, which\ndefines feature tag registration procedures.  I have made a copy\navailable at\n\n<URL:http://gewis.win.tue.nl/~koen/conneg/draft-ietf-http-feature-reg-00.txt>.\n\nThe draft is intended as a discussion draft.  It is partly based on\nmaterial taken from draft-ietf-822ext-mime-reg-04.txt.\n\nTitle page and abstract:\n\n HTTP Working Group                                     Koen Holtman, TUE\n Internet-Draft                              Andrew Mutz, Hewlett-Packard\n Expires: April 30, 1997                                 October 30, 1996\n\n\n                     Feature Tag Registration Procedures\n\n                      draft-ietf-http-feature-reg-00.txt\n\n ABSTRACT\n\n   The internet draft draft-holtman-http-negotiation-03.txt\n   (Transparent Content Negotiation in HTTP) specifies a `feature\n   negotiation' mechanism for negotiation on content characteristics\n   other than MIME type, charset, and language.  Feature negotiation\n   allows the quick introduction of new dimensions of negotiation\n   through the registration of `feature tags'.  A feature tag\n   identifies a capability of a user agent or a preference of a user.\n\n   This document discusses considerations related to feature tag\n   registration, and contains a proposed definition of a feature tag\n   registration procedure.  Feature tag registration is foreseen as an\n   ongoing, open process. It should keep pace with the introduction\n   of new rendering features by web software vendors, and other\n   parties such as standards bodies.\n\n\nGeneral information about transparent content negotiation is at\n\n<URL:http://gewis.win.tue.nl/~koen/conneg>\n\nHappy reading!\n\nKoen.\n\n\n\n", "id": "lists-010-11331639"}, {"subject": "Use of server expiration mode", "content": "I am developing a product that uses the Server Expiration model for caching.\nI was looking for information on how well this feature is implemented, and if\nthere was any measurement of cache hits/misses when this header is present.\n\nDoes the Apache, Netscape and IIS servers send Expires headers, and do\ncontent providers use it extensively? Also, do intermediate Cache Proxy\nservers implement the TRACE header to record their intermediate domain name\naddresses on the headers that they process?\n\nThanks very much for this information.\n\n-venkat\n\n\n\n", "id": "lists-010-11342184"}, {"subject": "I-D ACTION:draft-ietf-http-feature-reg00.tx", "content": " A New Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Feature Tag Registration Procedures                     \n       Author(s) : K. Holtman, A. Mutz\n       Filename  : draft-ietf-http-feature-reg-00.txt\n       Pages     : 14\n       Date      : 10/30/1996\n\nThe internet draft draft-holtman-http-negotiation-03.txt (Transparent \nContent Negotiation in HTTP) specifies a `feature negotiation' mechanism \nfor negotiation on content characteristics other than MIME type, charset, \nand language.  Feature negotiation allows the quick introduction of new \ndimensions of negotiation through the registration of `feature tags'.  A \nfeature tag identifies a capability of a user agent or a preference of a \nuser.    \n                                                                  \nThis document discusses considerations related to feature tag registration,\nand contains a proposed definition of a feature tag registration procedure.\nFeature tag registration is foreseen as an ongoing, open process. It should\nkeep pace with the introduction of new rendering features by web software \nvendors, and other parties such as standards bodies.                       \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-feature-reg-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-feature-reg-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  nic.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-feature-reg-00.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-11349111"}, {"subject": "FYI: Payload Format for HTTP Encoding in RT", "content": "I don't think this is a topic for HTTP-WG directly, but we should be\naware of the contexts in which HTTP responses might be encapsulated:\n\n================================================================\n A New Internet-Draft is available from the on-line Internet-Drafts \n directories.                                                              \n\n       Title     : Payload Format for HTTP Encoding in RTP                 \n       Author(s) : B. Aboba\n       Filename  : draft-aboba-rtp-http-00.txt\n       Pages     : 7\n       Date      : 10/30/1996\n\nThis document specifies a payload format for use in encoding HTTP within \nRTP. This payload format can be used for unreliable multicasting of \nresources such as Web pages, stock tickers, etc.  As with other RTP \napplications, receiver feedback and group membership information is \nprovided via RTCP.                                                         \n================================================================\n\n\n\n", "id": "lists-010-11359297"}, {"subject": "Re: (ACCEPT*) Draft text for Accept header", "content": ">10.2  Accept-Charset\n>\n>   The Accept-Charset request-header field can be used to indicate what\n>   character sets are acceptable for the response. This field allows\n>   clients capable of understanding more comprehensive or\n>   special-purpose character sets to signal that capability to a server\n>   which is capable of representing documents in those character\n>   sets. The US-ASCII character set can be assumed to be acceptable to\n>   all user agents.\n>\n>|  [##QUESTION TO BE RESOLVED: Apparently, the latest HTML spec says\n>|  that iso-8859-1 can be assumed to be acceptable to all user agents.\n>|  Should the above US-ASCII be changed to iso-8859-1??  There has\n>|  been lots of discussion on the list, but I have not been able to\n>|  detect a consensus opinion.##]\n\nSome considerable effort was put into the wording of the HTML 2.0 spec, to\nkeep  iso-8859-1 as what one might call the \"least common denominator\"\ncharacter set.  Because the HTML spec is written in the framework of SGML,\na lot of this wording has to do with the form of the SGML document\ncharacter set rather than the character encoding, a.k.a.  charset.\n\nI'd tend to agree with the position that *if* we are specifying one charset\nall user agents should support, or that should be assumed by default, that\nshould be iso-8859-1 both for historical reasons and for compatiblity with\nthe HTML 2.0 spec.\n\n(I can think of counter-arguments involving MIME and/or US_ASCII as a\ncommon subset of iso-8859-X charsets for X<>1)\n\nHowever, I'm not sure that is the issue here: we are not talking about the\ndefaults if no headers are specifed, but a charset that can aways be\nassumed accepable for a particular content negotiation. I'm less sure of\nthe pros and cons of this.\n\n\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-010-1136577"}, {"subject": "I-D ACTION:draft-mogul-http-hit-metering00.tx", "content": "For some reason, this appeared Tuesday on the main IETF list, but not on\nthe HTTP-WG list:\n\n------- Forwarded Message\n\n A New Internet-Draft is available from the on-line Internet-Drafts \n directories.                                                              \n\n       Title     : Simple Hit-Metering for HTTP Preliminary Draft          \n       Author(s) : J. Mogul, P. Leach\n       Filename  : draft-mogul-http-hit-metering-00.txt\n       Pages     : 26\n       Date      : 10/28/1996\n\nThis draft proposes a simple extension to HTTP, using a new ``Meter'' \nheader, to permit demographic information to be reported by caches to \norigin servers, and to permit an origin server to control the number of \ntimes a cache uses a cached response.  It also outlines techniques that \norigin servers can use to capture referral information without \n\"cache-busting.\"                                                           \n\n[...]\n\nftp://ds.internic.net/internet-drafts/draft-mogul-http-hit-metering-00.txt\n\n------- End of Forwarded Message\n\nNote that this is NOT the same proposal that Paul and I circulated\nat the end of July.  Although the latest proposal has the same goals\nand shares some of the same design concepts, we've made several\nsignificant design changes that (we believe) results in a much simpler,\nmore elegant design with far fewer loose ends.  Please ignore our\nprevious (July 31) draft!\n\n-Jeff\n\n\n\n", "id": "lists-010-11367222"}, {"subject": "Re: [URN] HTTP resolution protoco", "content": ">>All the instances that suit the Accept info the client has sent?\n>>Does that leave us with nothing between asking for one instance\n>>(N2R) and all (N2Rs), or is there some way to quality the request\n>>so that one receives no more than, say, 13 instances?  (Happy\n>>Halloween!)\n>\n>I will add some words to the effect that the resolver MAY restrict\n>the resources returned to those that match the Accept: header.\n\nThis is inconsistent with the latest HTTP 1.1 draft section 14.1, \"Accept\":\n\n     If an Accept header field is present, and if\n     the server cannot send a response which is acceptable according to the\n     combined Accept field value, then the server SHOULD send a 406 (not\n     acceptable) response.\n\nIMHO, HTTP URN resolvers should follow HTTP 1.1 on use of \"Accept:\".\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-11375016"}, {"subject": "RE: [URN] HTTP resolution protoco", "content": ">How about:\n>   1) We define text/uri-list as a new media type?\n>   2) We allow the Accept: header to be used to pick between returning\n>      the result as text/html or text/uri-list.\n>\n>I think that the primary use of the lists will be for automated processing,\n>thus the new media type allows us to launch those content handlers easily.\n>Clients that don't support text/uri-list have a fallback that will allow\n>humans to pick URI if necessary. New clients of the agent variety should\n>use text/uri-list.\n\nSounds OK to me.  Automated processing is important!\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-11383265"}, {"subject": "Re: [URN] HTTP resolution protoco", "content": "Thus spoke Fisher Mark (at least at 12:34 PM 11/4/96 EST)\n\n>>I will add some words to the effect that the resolver MAY restrict\n>>the resources returned to those that match the Accept: header.\n>\n>This is inconsistent with the latest HTTP 1.1 draft section 14.1, \"Accept\":\n>\n>     If an Accept header field is present, and if\n>     the server cannot send a response which is acceptable according to the\n>     combined Accept field value, then the server SHOULD send a 406 (not\n>     acceptable) response.\n>\n>IMHO, HTTP URN resolvers should follow HTTP 1.1 on use of \"Accept:\".\n\nOK, I will add words to the effect that the resolver SHOULD\nrestrict the resources returned to those in the Accept: header.\n\nRon Daniel Jr.                       email: rdaniel@lanl.gov\nAdvanced Computing Lab               voice: +1 505 665 0597\nMS B287                                fax: +1 505 665 4939\nLos Alamos National Laboratory        http://www.acl.lanl.gov/~rdaniel/\nLos Alamos, NM, USA  87545    obscure_term: \"hyponym\"\n\n\n\n", "id": "lists-010-11391226"}, {"subject": "Re: [URN] HTTP resolution protoco", "content": "Fisher Mark:\n>\n\n>>How about:\n>>   1) We define text/uri-list as a new media type?\n>>   2) We allow the Accept: header to be used to pick between returning\n>>      the result as text/html or text/uri-list.\n>>\n>>I think that the primary use of the lists will be for automated processing,\n>>thus the new media type allows us to launch those content handlers easily.\n>>Clients that don't support text/uri-list have a fallback that will allow\n>>humans to pick URI if necessary. New clients of the agent variety should\n>>use text/uri-list.\n>\n>Sounds OK to me.  Automated processing is important!\n\nHmmm.  This is starting to sound like the use of variant lists in\ntransparent content negotiation.  What does a `HTTP resolution\nprotocol' do?\n\nBy the way, it is OK for a protocol extension on top of HTTP/1.1 to\nextend (i.e. be inconsistent with) the HTTP/1.1 semantics of the\nAccept header.  A protocol extended server should however be careful\nto avoid that responses inconsistent with HTTP/1.1 end up at plain\nHTTP/1.1 clients.  This means in particular that you have to be\ncareful in making your responses cacheable.\n\n>Mark Leighton Fisher                   Thomson Consumer Electronics\n\nKoen.\n\n\n\n", "id": "lists-010-11399616"}, {"subject": "I-D ACTION:draft-ietf-http-state-mgmt04.txt, .p", "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\nNote: This revision reflects comments received during the last call period.\n\n       Title     : HTTP State Management Mechanism                         \n       Author(s) : D. Kristol, L. Montulli\n       Filename  : draft-ietf-http-state-mgmt-04.txt, .ps\n       Pages     : 19\n       Date      : 11/04/1996\n\nThis document specifies a way to create a stateful session with HTTP \nrequests and responses.  It describes two new headers, Cookie and \nSet-Cookie, which carry state information between participating origin \nservers and user agents.  The method described here differs from Netscape's\nCookie proposal, but it can interoperate with HTTP/1.0 user agents that \nuse Netscape's method.  (See the HISTORICAL section.)                          \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-state-mgmt-04.txt\".\n Or \n     \"get draft-ietf-http-state-mgmt-04.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-mgmt-04.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  nic.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-state-mgmt-04.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-state-mgmt-04.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-11408151"}, {"subject": "&quot;Internet Cache Protocol (ICP), version 2&quot", "content": "I don't think this is a 'work item' of HTTP-WG, but you might want to\nbe aware of a non-HTTP protocol being used by HTTP caches.\n================================================================\n       Title     : Internet Cache Protocol (ICP), version 2                \n       Author(s) : D. Wessels, K. Claffy\n       Filename  : draft-claffy-icp-v2-00.txt\n       Pages     : 7\n       Date      : 11/05/1996\n\nThis draft document describes the Internet Cache Protocol (ICP) currently \nimplemented in a few World-Wide Web proxy cache packages.   ICP was \ninitially developed by Peter Danzig, et. al. at the Univerisity of Southern\nCalifornia.  It evolved as an important part of hierarchical caching on the\nHarvest research project.                                                  \n\nhttp://ds.internic.net/internet-drafts/draft-claffy-icp-v2-00.txt\n================================================================\n\n\n\n", "id": "lists-010-11418164"}, {"subject": "Re: status code 100 (continue", "content": "rlgray@raleigh.ibm.com:\n>\n>\n>The way I read section 8.2 (message transmission requirements) is: \n> \n>IF ((client is 1.1) AND (NOT connection:close)) then I MUST send \n>status 100 or an error. \n\nThe connection:close conjunct does not come into it.  It is \n\nIF (client is 1.1) then ....\n\nbut only for responses to certain methods.  In the text of 14.10:\n\n|HTTP/1.1 defines the \"close\" connection option for the sender to signal\n|that the connection will be closed after completion of the response.\n\n`completion of the response' does not mean completion of the 100\nresponse, but completion of the entire HTTP/1.1 response, possibly\nconsisting of a 100 followed by something else.  A 100 response is an\n`interim response' only.\n\n>Will clients get confused receiving 100 even if connection:close? \n\nNot if they are correctly implemented 1.1 clients.\n\n>Do I *really* have to send a Date header? \n\nYes, but not in the preliminary 100 response part.  The spec wants you\nto send:\n\n -begin-\n 100 Continue\n\n 200 OK\n Date: Tue, 15 Nov 1994 08:12:31 GMT\n ....\n\n ....\n -end--\n\nand the whole thing should be seen as a single HTTP response.  I\nnotice that the spec is not very clear on this, there should probably\nbe more explanatory text in 10.1.\n\n[...]\n>And, (once again) what is the rationale for the 100 response? \n>It seems unnecessary and wasteful; what am I missing here? \n\nI never heard a rationale that convinced me we needed it.\n\n>Thank You, \n>Richard L. Gray\n\nKoen.\n\n\n\n", "id": "lists-010-11425792"}, {"subject": "Re: status code 100 (continue", "content": "I,am not sure what took place on my computer,but who ever or what ever(help)\nthat was, Thank You ,Thank you,& Thank You\n\n\n\n", "id": "lists-010-11434773"}, {"subject": "HTTP header suggestion/reques", "content": "I've noticed more and more software vendors using CGI applications to allow\nusers to download software.  Unfortuately at this time there isn't a way\nto tell user-agents to save the code under a name other than the script name.\nThis problematic especially when there is more than one selection on a form.\n\nI would like to suggest a new header for HTTP/1.1:\nSave-As:\n\nA CGI application could return this header with a file name (and possibly\npath, although that has security implications - Save-As: /etc/passwd for\ninstance.  Though it would fail on a well maintained system, it is still a\nrisk) which would be used by the user agent instead of the script name for\nsaving.\n\nI would have a number of uses for this tag, and I've seen numerous sites\nthat have the same aplication.  I think this would be a very useful addition.\n\n-MZ\n--\nLivingston Enterprises - Chair, Department of Interstitial Affairs\nPhone: 800-458-9966 510-426-0770 FAX: 510-426-8951 megazone@livingston.com\nFor support requests: support@livingston.com  <http://www.livingston.com/> \nSnail mail: 6920 Koll Center Parkway  #220, Pleasanton, CA 94566\n\n\n\n", "id": "lists-010-11442023"}, {"subject": "Re: HTTP header suggestion/reques", "content": "On Thu, 7 Nov 1996, MegaZone wrote:\n\n>[...]\n> I would like to suggest a new header for HTTP/1.1:\n> Save-As:\n> \n> A CGI application could return this header with a file name (and possibly\n> path, although that has security implications - Save-As: /etc/passwd for\n> instance.  Though it would fail on a well maintained system, it is still a\n> risk) which would be used by the user agent instead of the script name for\n> saving.\n\nThe risk could be minimized by not allowing path information in the\nvalue. The UA would 'know' the root path or prompt the user..\n\n> I would have a number of uses for this tag, and I've seen numerous sites\n> that have the same aplication.  I think this would be a very useful addition.\n\nDepending on your server and the ability it may have to map request URLs\nto cgi scripts, it may be possible to include a realistic file name\nas part of the URL ... for example, we use a url of the form:\n\n   http://xxxx/export/zwexport.zwe\n\nto convince the UA to save the data under the name zwexport.zwe but the\n'script name' is 'export' and the remainder of the path is simply\narguments to the script. The server that does this is the user\ninterface server built into ZooWorks so we own the server and \nthe exact options for other servers are not something I've studied.\n\nDave Morris\n\n\n\n", "id": "lists-010-11450615"}, {"subject": "(CONTENT NEGOTIATION,VARY) Draft text for Vary header and content negotiation `hooks", "content": "As I said in my earlier message about strategy:\n\n- Section 12 of the old 1.1 draft (which gave an incomplete definition\nof a content negotiation mechanism) will not be present in the new 1.1\ndraft.\n\n- There will be a Vary header in the 1.1 draft, so that HTTP/1.1 can\nsupport opaque negotiation on language in a reasonably efficient\nway.\n\n- There will be `hooks' in the 1.1 draft to ensure that all HTTP/1.1\ncaches will be compatible, though not in an optimally efficient way,\nwith a transparent content negotiation mechanisms like the mechanism\ndefined in draft-holtman.  Thus, transparent content negotiation\n(which is what Section 12 of the old 1.1 draft covered incompletely)\nwon't have to wait for HTTP/1.2 if HTTP/1.2 turns out to take too\nlong, it can be done on top of HTTP/1.1. \n\n- The `hooks' for transparent content negotiation consist mainly of an\nAlternates header definition which defines the Alternates header as\nsynonymous with a certain Vary header.  Also, some language in the\ndraft will announce that a negotiation mechanism using Alternates is\nplanned.\n\nThe sections below are proposed as new or changed text for the current\n1.1 draft.\n\nIf you have comments on this text, now is the time to comment.  I\nintend to close this issue at the end of the week.  This means that I\nwill send a last call for disagreement with perceived consensus,\ntogether with a possibly improved version of the text below, in a few\ndays.  If I get many negative reactions, the one-week schedule will of\ncourse be reconsidered.\n\nThere are no change bars, all text below is new, though it is mostly\nbased on old ideas.\n\n--snip--\n\n\n** I. Vary+content negotiation new/changed header descriptions\n\n[##Note: The current section 12 needs to be deleted completely from the\nApril 1.1 draft.##]\n\n10.v  Vary\n\n   The Vary response-header field is used by origin servers to signal\n   that the resource identified by the request-URI and the Host\n   request header (present if the request-URI is not an absoluteURI)\n   has the capability to send different responses depending on the\n   contents of particular header fields in the request message, and\n   maybe even depending on other information pertaining to the\n   request, for example the network address of the sending client.\n   Resources that have this capability are called varying resources.\n   Responses from varying resources must contain at least one Vary\n   header or Alternates header (Section 10.a) to signal this variance.\n   If a resource is varying, this has an important effect on cache\n   management, particularly for caching proxies which service a\n   diverse set of user agents.\n\n   If no Vary headers and no Alternates headers are present in a\n   response, then caches may assume, as long as the response is fresh,\n   that the resource in question is not varying.  Note however that\n   the fixed response that will be sent by an un-varying resource can\n   still change through time, as possibly indicated by a Cache-Control\n   response header (section 10.cc).  Also, if an un-varying resource\n   is access authenticated, its response can always change depending\n   on the presence of an Authorization request header (Section\n   10.auth), and depending on the contents of this Authorization\n   request header if present.\n\n   Request headers whose contents are used by a varying resource to\n   select its response are called selecting request headers.  The Vary\n   header field specifies selecting request headers and any other\n   selection parameters used by the varying resource.\n\n       Vary                 = \"Vary\" \":\" 1#selection-parameter\n\n       selection-parameter  = field-name\n                            | \"{\" \"accept-headers\" \"}\"\n                            | \"{\" \"other\" \"}\"\n                            | \"{\" \"unknown\" \"}\"\n                            | \"{\" extension-parameter \"}\"\n\n       extension-parameter  = token\n\n   The presence of a field-name signals that the request-header field\n   with this name is selecting.  The field-name will usually be, but\n   need not be, a request-header field name defined in this\n   specification.  Note that field names are case-insensitive.  The\n   presence of the \"accept-headers\" parameter signals that all request\n   headers whose names start with \"accept\" are selecting.\n\n   The inclusion of the \"other\" parameter in a Vary field signals that\n   parameters other than the contents of request headers, for example\n   the network address of the sending party, play a role in the\n   selection of the response.  The \"unknown\" parameter signals that\n   the origin server is not willing or able to specify the selection\n   parameters used.  If an extension-parameter unknown to the cache is\n   present in a Vary header, the cache must treat it as the \"unknown\"\n   parameter.\n\n   If multiple Vary and Alternates header fields are present in a\n   response, these must be combined to give all selecting parameters.\n   Note that the inclusion of the field names \"Host\" or\n   \"Authorization\" into a Vary response header is always redundant.\n\n   A cache may always store the relayed 200 (OK) responses from a\n   varying resource, and can refresh them according to the rules in\n   Section aa.bb [##Which will be written by Jeff Mogul##].  When\n   getting a request on a varying resource, a cache can only return a\n   cached response to one of its clients in two particular cases.\n\n   First, if a cache gets a request on a varying resource for which it\n   has cached one or more responses with Vary or Alternates headers,\n   it can relay that request towards the origin server, adding an\n   Unless-Cval [##or Unless-VID, exact names to be specified by Jeff\n   Mogul##] header listing the cache validators in the Cval headers of\n   the cached responses.  If it then gets back a 3xx (Ppp Qqq) [##TBS\n   ##] response with the cache validator of a cached 200 (OK) response\n   in its Cval header, it can return this cached 200 (OK) response to\n   its client, after merging in any of the 3xx response headers as\n   specified in Section xx.yy [##Which will be written by Jeff\n   Mogul##].\n\n   Second, if a cache gets a request on a varying resource, it can\n   return to its client a cached, fresh 200 (OK) response which has\n   Vary or Alternates headers, provided that\n\n       - the Vary and Alternates headers of this fresh response\n         specify that only request header fields are selecting\n         parameters,\n\n       - the specified selecting request header fields of the current\n         request match the specified selecting request header fields\n         of a previous request on the resource relayed towards the\n         origin server,\n\n       - this previous request got a 200 (OK) or 3xx (Ppp Qqq)\n         response which had the same cache validator in its CVal header\n         as the cached, fresh 200 (OK) response.\n\n   Two sequences of selecting request header fields match if and only\n   if the first sequence can be transformed into the second sequence\n   by only adding or removing whitespace at places in fields where\n   this is allowed according to the syntax rules in this\n   specification.\n\n   [##Note that a more complicated matching rule could be defined in a\n   future specification.  The rule above reflects the consensus of the\n   editorial group on how complex we can get in HTTP/1.1##]\n\n         Note: Implementation of support for the second case above is\n         mainly interesting in user agent caches, as a user agent\n         cache will generally have an easy way of determining whether\n         the sequence of request header fields of the current request\n         equals the sequence sent in an earlier request on the same\n         resource.  Proxy caches supporting the second case would have\n         to record diverse sequences of request header fields\n         previously relayed; the implementation effort associated with\n         this may not be balanced by a sufficient payoff in traffic\n         savings.  A planned specification of a content negotiation\n         mechanism will define additional cases in which proxy caches\n         can return a cached 200 (OK) response without contacting the\n         origin server.  The implementation effort associated with\n         support for these additional cases is expected to have a much\n         better cost/benefit ratio.\n\n  [##Note that the `planned specification of a content negotiation\n  mechanism' above does not necessarily have to be draft-holtman!'  In\n  theory, a content negotiation mechanism totally unlike draft-holtman\n  could just as well live up to these cost/benefit expectations.##]\n\n10.a  Alternates\n\n   The Alternates response-header field is used by origin servers to\n   signal that the resource identified by the request-URI and the Host\n   request header (present if the request-URI is not an absoluteURI)\n   has the capability to send different responses depending on the\n   accept headers in the request message.  This has an important\n   effect on cache management, particularly for caching proxies which\n   service a diverse set of user agents.  This effect is covered in\n   Section 10.v.\n\n       Alternates           = \"Alternates\" \":\" opaque-field\n\n       opaque-field         = field-value\n\n   The Alternates header is included into HTTP/1.1 to make HTTP/1.1\n   caches compatible with a planned content negotiation mechanism.\n   HTTP/1.1 allows a future content negotiation standard to define the\n   format of the Alternates header field-value, as long as the defined\n   format satisfies the general rules in Section 4.2.\n\n   To ensure compatibility with future experimental or standardized\n   software, caching HTTP/1.1 clients must treat all Alternates\n   headers in a response as synonymous to the following Vary header:\n\n         Vary: {accept-headers}\n\n   and follow the caching rules associated with the presence of this\n   Vary header, as covered in Section 10.v.  HTTP/1.1 allows origin\n   servers to send Alternates headers under experimental conditions.\n\n\n10.u  URI\n\n   The URI entity-header field is used to inform the recipient of\n   other Uniform Resource Identifiers (Section 3.2) by which\n   the resource can be identified.\n\n       URI-header  = \"URI\" \":\" 1#( uri-mirror | uri-name )\n\n       uri-mirror  = \"{\" \"mirror\" <\"> URI <\"> \"}\"\n       uri-name    = \"{\" \"name\" <\"> URI <\"> \"}\"\n\n   Any URI specified in this field can be absolute or relative to the\n   Request-URI. The \"mirror\" form of URI refers to a location which is a\n   mirror copy of the Request-URI. The \"name\" form refers to a\n   location-independent name corresponding to the Request-URI.\n\n   [##Note: According to the issues list, Roy is working on text that\n   explains better what \"mirror\" and \"name\" actually mean.##]\n\n\n** II. Changed status code descriptions\n\n300 Multiple Choices\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 300\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\n406 None Acceptable\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 406\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\n\n** III.  New text for the (new) caching section\n\n13.x Interoperability of varying resources with HTTP/1.0 proxy caches\n\n  [## Note: the text in 13.x could be part of a larger subsection in\n  the 1.1 document##]\n\n  If the correct handling of responses from a varying resource\n  (Section 10.v) by HTTP/1.0 proxy caches in the response chain is\n  important, HTTP/1.1 origin servers can include the following Expires\n  (Section 10.exp) response header in all responses from the varying\n  resource:\n\n     Expires: Thu, 01 Jan 1980 00:00:00 GMT\n\n  If this Expires header is included, the server should usually also\n  include a Cache-Control header for the benefit of HTTP/1.1 caches,\n  for example\n\n     Cache-Control: max-age=604800\n\n  which overrides the freshness lifetime of zero seconds specified by\n  the included Expires header.\n\n\n13.y Cache replacement for varying resources\n\n  If a new 200 (OK) response is received from a non-varying resource\n  while an old 200 (OK) response is cached, caches can delete this old\n  response from cache memory and insert the new response.  For 200\n  (OK) responses from varying resources (Section 10.v), cache\n  replacement is more complex.\n\n  HTTP/1.1 allows the authors of varying resources to guide cache\n  replacement by the inclusion of elements of so-called replacement\n  keys in the responses of these resources.  The replacement key of a\n  varying response consists of two elements, both of which may be\n  empty strings, separated by a semicolon:\n\n       replacement-key  =  variant-id \";\" absoluteURI\n\n  The variant-id element of the replacement key is the variant-id\n  value in the Cval [#VID?#] header of the response, if a Cval header\n  which such a value is present, and an empty string otherwise.  The\n  absoluteURI element of the replacement key is the absolute URI given\n  in, or derived from, the Content-Location header of the response if\n  present, and and an empty string if no Content-Location header is\n  present.\n\n  If a response from a varying resource has the one-character\n  replacement key \";\", a cache should interpret this as a signal from\n  the resource author that storing this particular response in cache\n  memory will never lead to a saving of network resources.  If a cache\n  has stored in memory a 200 (OK) response with a certain replacement\n  key, and receives, from the same resource, a new 200 (OK) response\n  which has the same replacement key, this should be interpreted as a\n  signal from the resource author that the old response can be deleted\n  from cache memory and replaced by the new response.\n\n  The replacement key mechanism cannot cause deletion from cache\n  memory of old responses with replacement keys that will no longer be\n  used.  It is expected that the normal `least recently used'\n  replacement heuristics employed by caches will eventually cause such\n  old responses to be deleted.\n\n       Note: Varying resources which use a Vary header to signal\n       variance should put a variant-id value in the Cval header to\n       supply a replacement key, and should not include a\n       Content-Location header.  It is expected that resources using a\n       planned content negotiation mechanism will use the Alternates\n       header to signal variance, and Content-Location headers to\n       supply replacement keys.\n\n\n[End of document]\n\n\n\n", "id": "lists-010-1145902"}, {"subject": "Re: HTTP header suggestion/reques", "content": "Once upon a time David W. Morris shaped the electrons to say...\n>The risk could be minimized by not allowing path information in the\n>value. The UA would 'know' the root path or prompt the user..\n\nThat would be my choice.  But I wanted to open the option.\n\n>to cgi scripts, it may be possible to include a realistic file name\n>as part of the URL ... for example, we use a url of the form:\n>   http://xxxx/export/zwexport.zwe\n\nThe problem is this:\n\nYou have a form, say 'download'.\n\nThat form has one ACTION.  On that form you can check a box to download\ncode.tar, code.tar.Z, or code.tar.gz.  The user picks the option they want\nthen submits the form to the CGI.  Since the form has one ACTION only one\nCGI can be called, and since there are 3 possibly names putting an extension\non the ACTION will be wrong for any 2 of them.\n\nI've explored this one already.  This is a real world situation today.\n\nAnd soon I'll have one where users may pick from at least 5 choices on the\nsame form for variations of the same software.  It doesn't make sense to\nhave 5 forms and/or 5 cgi applications.\n\nSo a 'Save-As:' HTTP header would be most useful in this situation.  And\nolder browsers would behave as they do today, not a problem for combatibility.\n\nI think we need to look at HTTP's growing use as a replacement or supliment\nfor FTP - especially in software download situations where it is desirable\nto force the user to accept a licensing agreement before getting the code.\n\nHeck - wouldn't it be nice to make users get code only after having seen\nthe Release Notes. ;-)\n\n-MZ\n--\nLivingston Enterprises - Chair, Department of Interstitial Affairs\nPhone: 800-458-9966 510-426-0770 FAX: 510-426-8951 megazone@livingston.com\nFor support requests: support@livingston.com  <http://www.livingston.com/> \nSnail mail: 6920 Koll Center Parkway  #220, Pleasanton, CA 94566\n\n\n\n", "id": "lists-010-11459636"}, {"subject": "Re: HTTP header suggestion/reques", "content": "MegaZone:\n>\n>I would like to suggest a new header for HTTP/1.1:\n>Save-As:\n\nThere already is an ad-hoc standard for it:\n\n  Content-type:        application/octet-stream\n  Content-Disposition: attachment; filename=\"fname.ext\"\n\nSee http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q4/0048.html\nand followups for more information.\n\nIf you want to make it into a real standard for HTTP/1.x, you should\nstart by writing a small internet-draft.\n\n>-MZ\n\nKoen.\n\n\n\n", "id": "lists-010-11469490"}, {"subject": "Re: HTTP header suggestion/reques", "content": "MegaZone wrote:\n> [...]\n> That form has one ACTION.  On that form you can check a box to download\n> code.tar, code.tar.Z, or code.tar.gz.  The user picks the option they want\n> then submits the form to the CGI.  Since the form has one ACTION only one\n> CGI can be called, and since there are 3 possibly names putting an extension\n> on the ACTION will be wrong for any 2 of them.\n\nyou could use the Content-Disposition: header field\n(beware: it's not in the http-v11-spec07 but borrowed\nfrom MIME) ie send\n\n  Content-Type:        application/octet-stream\n  Content-Disposition: attachment; filename=code.tar\n\nfrom your ACTION script.\n\n<michael\n\n** Michael Naumann * ESO * mnaumann@eso.org \n** http://www.eso.org/~mnaumann/\n\n\n\n", "id": "lists-010-11477329"}, {"subject": "Re: HTTP header suggestion/request (fwd", "content": "Once upon a time Koen Holtman shaped the electrons to say...\n>There already is an ad-hoc standard for it:\n>  Content-type:        application/octet-stream\n>  Content-Disposition: attachment; filename=\"fname.ext\"\n>See http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q4/0048.html\n>and followups for more information.\n\nThank you very much.  I'd done quite a bit of digging and asking around but\nnever stumbled over this one.  I'd just given up and was ready to write a\nnew proposal.\n\n>If you want to make it into a real standard for HTTP/1.x, you should\n>start by writing a small internet-draft.\n\nIs anyone else interested in reviving RFC1806 for incorporation into HTTP/1.1?\n<URL:http://www.internic.net/rfc/rfc1806.txt>\n\nFrankly I've never written an internet-draft and I wouldn't mind working with\nsomeone with some experience the first time.\n\nThanks again.\n-MZ\n--\nLivingston Enterprises - Chair, Department of Interstitial Affairs\nPhone: 800-458-9966 510-426-0770 FAX: 510-426-8951 megazone@livingston.com\nFor support requests: support@livingston.com  <http://www.livingston.com/> \nSnail mail: 6920 Koll Center Parkway  #220, Pleasanton, CA 94566\n\n\n\n", "id": "lists-010-11485645"}, {"subject": "Re: HTTP header suggestion/reques", "content": "On Nov 8,  4:16am, Koen Holtman wrote:\n> Subject: Re: HTTP header suggestion/request\n> MegaZone:\n> >\n> >I would like to suggest a new header for HTTP/1.1:\n> >Save-As:\n>\n> There already is an ad-hoc standard for it:\n>\n>   Content-type:        application/octet-stream\n>   Content-Disposition: attachment; filename=\"fname.ext\"\n>\n  What UAs support this?  Or are you supplying a \"plug-in\" to the UA?\n\n  I've been having trouble with NS trying to name files with characters that\nthe UA end OS can not deal with.  I tried RFC1806 without any success.\n\nAny ideas?\n\n[Snip]...\n> >-MZ\n>\n> Koen.\n>\n>-- End of excerpt from Koen Holtman\n\n\n\n--\n=============================================================================\nKevin J. DyerDraper Laboratory  MS 23.\nEmail: <kdyer@draper.com>        555 Tech. Sq.\nPhone: 617-258-4962Cambridge, MA 02139\nFAX: 617-258-2121\n-----------------------------------------------------------------------------\nLesson learned by a user: \"Beware geeks bearing GIFs\"\n=============================================================================\n\n\n\n", "id": "lists-010-11493999"}, {"subject": "Re: HTTP header suggestion/reques", "content": "On Fri, 8 Nov 1996, MegaZone wrote:\n\n> Once upon a time David W. Morris shaped the electrons to say...\n> >The risk could be minimized by not allowing path information in the\n> >value. The UA would 'know' the root path or prompt the user..\n> \n> That would be my choice.  But I wanted to open the option.\n> \n> >to cgi scripts, it may be possible to include a realistic file name\n> >as part of the URL ... for example, we use a url of the form:\n> >   http://xxxx/export/zwexport.zwe\n> \n> The problem is this:\n> \n> You have a form, say 'download'.\n> \n> That form has one ACTION.  On that form you can check a box to download\n> code.tar, code.tar.Z, or code.tar.gz.  The user picks the option they want\n> then submits the form to the CGI.  Since the form has one ACTION only one\n> CGI can be called, and since there are 3 possibly names putting an extension\n> on the ACTION will be wrong for any 2 of them.\n> \n> I've explored this one already.  This is a real world situation today.\n> \n\nCode 302. Issue a redirect from the script to the final file. That is what\nmost sites that do this do. There is no real reason to feed it out of the\nscript itself. This also allows you to try to load balance download sites\nif that that is desirable. \n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-11503002"}, {"subject": "Re: HTTP header suggestion/reques", "content": "Once upon a time Kevin J. Dyer shaped the electrons to say...\n>  What UAs support this?  Or are you supplying a \"plug-in\" to the UA?\n\nI tested it in Netscape 3.0 on SunOS 4.1.4 so far and it worked fine.  \nLynx 2.5 on IRIX ignored it.\n\nI'll test it on MSIE 3.0 and NS 3.0 on Win95 when I get home.\n\nLet's take this to www-talk@w3.org instead of the WG.\n\n-MZ\n--\nLivingston Enterprises - Chair, Department of Interstitial Affairs\nPhone: 800-458-9966 510-426-0770 FAX: 510-426-8951 megazone@livingston.com\nFor support requests: support@livingston.com  <http://www.livingston.com/> \nSnail mail: 6920 Koll Center Parkway  #220, Pleasanton, CA 94566\n\n\n\n", "id": "lists-010-11512724"}, {"subject": "Re: HTTP header suggestion/reques", "content": "Once upon a time Benjamin Franz shaped the electrons to say...\n>most sites that do this do. There is no real reason to feed it out of the\n>script itself. This also allows you to try to load balance download sites\n\nYes there is - lawyers.  They wanted it so that there was no way to the\nfile except via the script which contains a licensing agreement.  Redirecting\nto another URL would allow that URL to leak and people to get the code\ndirectly.\n\nYes - cookies could maintain state - and exclude a lot of people.\n\nNote replies redirected to www-talk@w3.org so we stop bothering the WG.\n\n-MZ\n--\nLivingston Enterprises - Chair, Department of Interstitial Affairs\nPhone: 800-458-9966 510-426-0770 FAX: 510-426-8951 megazone@livingston.com\nFor support requests: support@livingston.com  <http://www.livingston.com/> \nSnail mail: 6920 Koll Center Parkway  #220, Pleasanton, CA 94566\n\n\n\n", "id": "lists-010-11521808"}, {"subject": "[mnaumann&#64;eso.org: Re: [Q] Changing default name in a FORM", "content": "Sigh, this belongs in the FAQ. Maybe it belongs in the revision of the\nHTTP/1.1 spec, too.\n\n\n------- Start of forwarded message -------\nDate: Tue, 8 Oct 1996 04:38:10 PDT\nFrom: Michael Naumann - ESO Garching +49 89 32006 430 <mnaumann@eso.org>\nOrganization: European Southern Observatory, Garching\nTo: www-talk@w3.org\nSubject: Re: [Q] Changing default name in a FORM\nReferences: <199610080948.LAA26948@beatles.cselt.stet.it>\nContent-Type: text/plain; charset=us-ascii\n\nMaurizio Codogno wrote:\n> I have a script which selects a file and send it to the user. It\n> is accessed via a form, with action POST.\n> Unfortunately, the file is sent with the form's name as default name,\n> and the user should rename it by hand.\n> \n> Does HTTP/1.0 have any header line which can be used to overcome it?\n\nTry sending \n\n  Content-type:        application/octet-stream\n  Content-Disposition: attachment; filename=\"fname.ext\"\n\nHTTP header fields in the response of your script. This\nat least works with Netscape Navigator since version 2.0\n\nAlthough not specified in the HTTP 1.0/1.1 drafts the\nthe definition of Content-Disposition: can be found in \nhttp://www.internic.net/rfc/rfc1806.txt\n\n<michael\n\n-- \n** Michael Naumann * ESO * mnaumann@eso.org \n** http://www.eso.org/~mnaumann/\n\n\n------- End of forwarded message -------\n\n\n\n", "id": "lists-010-11531289"}, {"subject": "introduction of our internetdraf", "content": "Hi,\n\nWe (NTT Software Laboratories) wrote an internet-draft about version\nmanagement of Web contents by using meta-level links.\n(ftp://ietf.org/internet-drafts/draft-ota-http-version-00.txt)\n\nWe implemented the server that is based on our draft. It was written\nin perl. Also, we are now implementing the server written in java.\nThe java version server will be completed by the end of December.\nWe would appreciate it if you could send any comments or questions to\nota@nttlab.com, kt@nttlabs.com or sekiya@ntts.com.\n\nThe server is a component of our developing tool, EColabor,\nthat is a hypermedia system for collaborative document production. \nplease see \"http://www.nttlabs.com/~kt/ecolabor\" for more details.\nAlso, you can find information about an another tool using meta-level\nlinks at \"http://www.nttlabs.com/~kt/WebArchitect\".\n\nThank you.\n\n-----------------------------------\nKenji OTA (ota@nttlabs.com)\nNTT Software Laboratories Palo Alto\n\n\n\n", "id": "lists-010-11541047"}, {"subject": "Re: HTTP header suggestion/reques", "content": "On Fri, 8 Nov 1996, Benjamin Franz wrote:\n\n> Code 302. Issue a redirect from the script to the final file. That is what\n> most sites that do this do. There is no real reason to feed it out of the\n> script itself. This also allows you to try to load balance download sites\n\nDepends on the specifics of the content ... if its generated dynamically\nit needs to come from the application.\n\nDave\n\n\n\n", "id": "lists-010-11550311"}, {"subject": "Re: HTTP header suggestion/reques", "content": "MegaZone wrote:\n> \n> I've noticed more and more software vendors using CGI applications to allow\n> users to download software.  Unfortuately at this time there isn't a way\n> to tell user-agents to save the code under a name other than the script name.\n> This problematic especially when there is more than one selection on a form.\n> \n> I would like to suggest a new header for HTTP/1.1:\n> Save-As:\n> \n> A CGI application could return this header with a file name (and possibly\n> path, although that has security implications - Save-As: /etc/passwd for\n> instance.  Though it would fail on a well maintained system, it is still a\n> risk) which would be used by the user agent instead of the script name for\n> saving.\n> \n> I would have a number of uses for this tag, and I've seen numerous sites\n> that have the same aplication.  I think this would be a very useful addition.\n> \n\nThere is a solution that should already serve your purpose.\n\nThe Content-disposition header can contain a \"filename\"\nparameter for naming a file.  When the Navigator sees this\nparameter it will use it as a default filename for saving.\n\nThe following use should work when returned from a CGI script:\n\nContent-disposition: file; filename=foo.exe\n\nThe Navigator only uses the filename parameter, everything\nelse in the header is currently ignored.\n\nThe Content-disposition header is also used in HTTP file upload\nand is documented in Larry Masinter's RFC on file upload.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n", "id": "lists-010-11559434"}, {"subject": "agenda items for December IETF meetin", "content": "I'm late putting together the schedule for the December IETF; I expect\nto put together a draft agenda in the next few days, but if you have\nany items you would like to see on the agenda, please let me know.\n\nMy guess is that it is the best use of our time to discuss those\nInternet Drafts that have been submitted.\n\nNote that there are tentatively two _other_ HTTP-related BOFs\nscheduled for this IETF: a joint 'Transport' session on HTTP-NG and a\nBOF to discuss the creation of a Distributed Authoring and Versioning\nworking group.\n\nLarry\n\n\n\n", "id": "lists-010-11567921"}, {"subject": "Content Negotiation: state of the art", "content": "(At the suggestion of Larry Masinter, i'm making this note to the\nHTTP-wg mailing list, although i and my mailbox cannot afford to\nsubscribe to yet another mailing list -- please cc replies directly.)\n\nOn www-talk, Larry Masinter wrote:\n> The work on content negotiation and feature negotiation is active in\n> the HTTP working group (http-wg@cuckoo.hpl.hp.com). There's a draft by\n> Andy Mutz and Koen Holtmann, and comments on their draft would be\n> appreciated.\n\nAnd i replied:\n> Thought i'd note -- i just went to have a look at this draft (i assume\n> \n> ftp://ftp.ietf.org/internet-drafts/draft-holtman-http-negotiation-03.txt\n> \n> is the current version) and i'd like to say i rather like it, for what\n> that's worth.  I'd be happy to see implementations of it get rolling!\n> The sooner they appear, the faster all kinds of problems will be solved.\n\nI'm basically writing this message to display support for this Con-Neg\ndraft and offer congratulations for Koen Holtman on producing this work.\nI was even happier when i recently discovered the implementation on his\nsite, which i've tried out and works nicely.\n\nSo, for whatever my comments are worth, i'd like to call attention to\nthis draft because it looks like a good and flexible solution to me,\nand i'm pretty thrilled that it actually exists and is implemented!\n\nCould i make a polite request to Henrik Frystyk Nielsen to update the\nlist of drafts at http://www.w3.org/pub/WWW/Protocols/Specs.html in\norder to provide the HTML version of the draft and mention the sample\nimplementation at http://gewis.win.tue.nl/~koen/conneg/?\n\n---\nWhile skimming through the draft i wondered about two things:\n\nWhy do feature expressions allow \"<=\" but not \">=\" ?  And as for the\nfollowing note:\n\n>    The length attribute (if present) must reflect the length of the\n>    variant alone, and not the sum of the lengths of the variant and\n>    any objects inlined or embedded by the variant.\n\ncan i ask why?\n\nCurious,\n\n\nPing\n                                 Developer, Alias|Wavefront Inc. (Tokyo)\nhttp://www.lfw.org/math/ brings math to the Web as easy as <se>?pi?</se>\n\n\n\n", "id": "lists-010-11574922"}, {"subject": "Re: Content Negotiation: state of the art", "content": "Ka-Ping Yee:\n>\n\nFirst, thanks for your support.\n\n[...]\n>\n>While skimming through the draft i wondered about two things:\n>\n>Why do feature expressions allow \"<=\" but not \">=\" ?\n\nSome background: we now have:\n\n     ftag=V \n          ftag is present with the value V \n     !ftag=V \n          ftag is present, but not with the value V \n     ftag={V} \n          ftag is present with the value V, and not with any other values \n     ftag<=N \n          ftag is present with the numeric values from 0 up to and\n          including N, and not with any other values\n\nIn principle, the two forms ftag=V and !ftag=V are all you ever need\nif you want to negotiate on a feature tag which has some values.  If\nyou have a feature tag with the values 100 -- infinity, you would use\nexpressions like !ftag=99 and ftag=101 to negotiate on it.\n\nThe two special forms ftag={V} and ftag<=N are there to optimise the\nfrequently occuring cases of having a tag with a single value and a\ntag with a numeric range 0 -- N of values.  \n\nWe don't think that a numeric range N -- infinity is a frequently\noccuring case, so there is no ftag>=N form to optimize it.\n\n>  And as for the\n>following note:\n>\n>>    The length attribute (if present) must reflect the length of the\n>>    variant alone, and not the sum of the lengths of the variant and\n>>    any objects inlined or embedded by the variant.\n>\n>can i ask why?\n\nIt could have been defined the other way around, but you have to pick\none of the two possibilities.  We picked the first one, mainly because\nof symmetry with the content-length header and because the length of\nthe variant alone is easier for a server to measure.\n\nNote that the spec leaves open the possibility of defining a\n`length-with-inlined-and-embedded-objects' attribute in a future\nspecification.\n\n>Curious,\n>\n>\n>Ping\n\nKoen.\n\n\n\n", "id": "lists-010-11584833"}, {"subject": "Re: Content Negotiation: state of the art", "content": "On Mon, 11 Nov 1996, Koen Holtman wrote:\n\n> We don't think that a numeric range N -- infinity is a frequently\n> occuring case, so there is no ftag>=N form to optimize it.\n\nHuh? I would expect a version check for at least N to be heavily used\njust like size<=N might be heavily used. Sometimes logical completeness\nis sufficient to justify inclusion of a feature and that should be\nthe case here.\n\nDave Morris\n\n\n\n", "id": "lists-010-11594881"}, {"subject": "Re: Content Negotiation: state of the art", "content": "David W. Morris:\n>On Mon, 11 Nov 1996, Koen Holtman wrote:\n>\n>> We don't think that a numeric range N -- infinity is a frequently\n>> occuring case, so there is no ftag>=N form to optimize it.\n>\n>Huh? I would expect a version check for at least N to be heavily used\n\nI do too, and it is accounted for.  What happens is that the user\nagent uses a feature-expr \"x_version<=100\" to say:\n\n I support X versions 0 -- 100\n\nand that an origin server uses an fpred (feature predicate)\n\"x_version=80\" to say:\n\n This variant uses version 80 of X\n\nSo a >= operator is never used in version checks.\n\n>just like size<=N might be heavily used. Sometimes logical completeness\n>is sufficient to justify inclusion of a feature and that should be\n>the case here.\n\nAs the draft is large already, I've been operating on the principle\n`never include an optimisation you cannot justify', so I did not\ninclude >=.\n\nHowever, adding >= would only cost a few lines of code in the average\nimplementation.  If you feel that symmetry considerations justify\nthese lines, I'll add >= in the next version.\n\n>Dave Morris\n\nKoen.\n\n\n\n", "id": "lists-010-11603160"}, {"subject": "December agenda checklis", "content": "The tentative list of topics for the HTTP working group meeting in\nDecember is in:\n\n   ftp://ftp.parc.xerox.com/pub/masinter/http-wg-agenda.html\n\nI've not resolved 'how long' for each item or whether there are\npresentations or discussion leaders.\n\nI'll probably not revise the agenda again until November 26.\n\n================================================================\nMonday, December 9, 9:30-11:30\nTuesday, December 10, 9-11:30\n\nTentative list of topics:\n   o 'Issue list' for HTTP/1.2?\n   o Safe: response header\n   o Status report & other working groups\n   o Transparent Content Negotiation\n   o Feature tag registration\n   o User Agent display attributes\n   o content-disposition\n   o Hit metering\n   o PEP\n   o HTTP/1.1 to Draft Standard revision plan\n================================================================\n\n\n\n", "id": "lists-010-11612231"}, {"subject": "please get me off!!!", "content": ">  http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n\nplease take me off this list...thank you\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\nJotham Read\nGraphic Artist and Web Designer\nEmail: jotham@voyager.co.nz\nPhone: 07) 5520066\nWeb: http://www.voyager.co.nz/~jotham/\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\n", "id": "lists-010-11619471"}, {"subject": "Re: Content Negotiation: state of the art", "content": "On Mon, 11 Nov 1996, Koen Holtman wrote:\n\n> I do too, and it is accounted for.  What happens is that the user\n> agent uses a feature-expr \"x_version<=100\" to say:\n> \n>  I support X versions 0 -- 100\n\nWhile it seems safer to state support for a bounded interval, it seems\nlikely that some will wish to state support for 101 --> oo\nbecause of a significant change in 101. Probably better to\nsay x_version >=101 and x_version <= 200 but until a conflict on the\nupside is known, x_version>=101 would work. But I may also wish to\nsay I support colors >= 256, etc.\n\nDave\n\n\n\n", "id": "lists-010-11626618"}, {"subject": "FW: please get me off!!!", "content": ">  http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n\nplease take me off this list...thank you\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\nEmail: bbzz@msn.com\nPhone:408-934-1850\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\n", "id": "lists-010-11634956"}, {"subject": "Re: Content Negotiation: state of the art", "content": "David W. Morris:\n>On Mon, 11 Nov 1996, Koen Holtman wrote:\n>\n>> I do too, and it is accounted for.  What happens is that the user\n>> agent uses a feature-expr \"x_version<=100\" to say:\n>> \n>>  I support X versions 0 -- 100\n>\n>While it seems safer to state support for a bounded interval, it seems\n>likely that some will wish to state support for 101 --> oo\n>because of a significant change in 101.  Probably better to\n>say x_version >=101 and x_version <= 200 \n\nI think you make a good case for adding complexity with the `I support\nversions 101 -- 200' example.\n\n>but until a conflict on the\n>upside is known, x_version>=101 would work.\n\nx_version>=101 would work at first, but it is fundamentally broken.\nIf 201 is a newer version of x with a significant change, then a\nbrowser which sends only x_version>=101 will choke when getting a 201\nversion it reported it could handle.  The broad deployment of a\nbrowser which sends only x_version>=101 will thus _prevent_ the smooth\nintroduction of version 201.  This is the kind of trickiness we are\ndealing with.\n\nSo I think that if we have \">=\", because it is useful in some cases,\nwe should also make it very easy to say that you support an interval\nlike 101 -- 200.\n\nWe cannot go for the interval notation \"x_version=101-200\" because we\nwould get a parsing ambiguity: x_version could also be a non-numeric\ntag with the value \"101-200\".\n\nI guess that adding a notation\n\n  x_version=<101-200>\n\nwhich allows the shorter versions\n\n  blah=<255->    wox=<-200>\n\nmeets all concerns best.  I'll add it in the next version.  If you\nknow a nicer notation, let me know.\n\nNote that I'll also keep the wox<=200 notation.\n\n> But I may also wish to\n>say I support colors >= 256, etc.\n\nI don't know if you would want that for the color case.  If your\nscreen can show at most 2^8 colors and you say you can handle more,\nyou run the risk of getting the best variant with 2^24 colors, which\ntakes twice as long to download and 5 times as long to render in 2^8\ncolors, instead of the 2^8 color variant the server also had.\n\n>Dave\n\nKoen.\n\n\n\n", "id": "lists-010-11642229"}, {"subject": "reminder of InternetDraft cutoff dat", "content": "This is just a reminder that the cutoff for Internet-Draft submissions\nprior to the San Jose IETF meeting is Tuesday, November 26, 1996 at\n1700 Eastern Standard US time.  (I think that's 2200 UTC)\n\nProposals to be discussed at IETF WG meetings must be submitted\nas Internet-Drafts prior to discussion.  So if you want to have \na proposal discussed at the San Jose IETF, you need to get it \nin before this deadline.  \n\nPlease also note that due to the the large number of last-minute \ninternet-draft submissions, last-minute drafts require more time \nto appear in the Internet-Drafts repositories.  Given the US \nThanksgiving holidays on 28-29 November, I wouldn't expect last-minute\ndrafts to be generally available before 2-3 December or so.\n\nSo to give other working group members more time to review drafts,\nI encourage people to submit their drafts a few days before the deadline.\n\nFor information on how to submit an Internet-Draft, look at\nftp://ftp.ietf.org/ietf/1id-guidelines.txt\n\nKeith Moore\nAPPS AD \n\n\n\n", "id": "lists-010-11651793"}, {"subject": "FW: please get me off!!!", "content": "Please get me off this list!\n\n\n\n", "id": "lists-010-11659074"}, {"subject": "Re: Another fractional BO", "content": "Having negotiated with myself as well as the area directors and the\nagenda gods, I wish to announce a schedule change:\n\n*  The HTTP-WG meeting at the December San Jose IETF will end\n   35 minutes early on Tuesday. That is, HTTP has\n     Mon Dec  9 9:30-11:30\n     Tue Dec 10 9:00-10:55\n   see ftp://ftp.parc.xerox.com/pub/masinter/http-wg-agenda.html\n   for tentative list of topics.\n\n*  There will be a \"URL BOF\", in the same room\n    Tue Dec 10 11:00-11:30\n\nThe agenda of the URL-BOF is:\n\n   -   review status of Internet Drafts intending to\n       replace 'generic syntax' in RFC 1738 and RFC 1808\n\n      (I believe this will be \n      draft-ietf-uri-url-syntax-00.txt )\n\n   -  Review current RFCs and Internet drafts\n      describing URL schemes (ftp, file, news, telnet\n      mailto, vemmi, data, etc.)\n\n   -  Decide on whether there is a need to (re)constitute\n      a URL working group to\n      - move URL from 'Proposed' to 'Draft' standard status\n      - create a BCP for URL registration mechanisms\n\nRegards,\n\nLarry\n(off-net for a week starting tomorrow, so don't expect replies.)\n\n\n\n", "id": "lists-010-11666084"}, {"subject": "Hitmetering: to Proposed Standard", "content": "Several weeks ago, Paul Leach and I submitted an Internet-Draft\non \"Simple Hit-Metering for HTTP\":\n\n   ftp://ds.internic.net/internet-drafts/draft-mogul-http-hit-metering-00.txt\n\nand announced it to the HTTP-WG mailing list.\n\nAlthough our previous (and quite different) proposal, at the end of July,\nresulted in some discussion, this one has raised no comments on the\nmailing list (although we have received a few private comments).\n\nSince we have not seen any criticism of our latest proposal, we would\nlike to interpret this as lack of criticism rather than lack of\ninterest, because we already have evidence that several large customers\nare eager to deploy implementations of our proposal.\n\nTherefore, we intend to submit this I-D, or a minor revision thereof,\nto the IESG as a Proposed Standard as soon as possible.\n\nNote that the criteria for Proposed Standard in RFC2026 says\n\n   A Proposed Standard specification is generally stable, has resolved\n   known design choices, is believed to be well-understood, has received\n   significant community review, and appears to enjoy enough community\n   interest to be considered valuable.  However, further experience\n   might result in a change or even retraction of the specification\n   before it advances.\n\nso we *would* like to encourage further community review.\n\nIf there is significant criticism based on technical merit, then we\nwill reconsider our intention to submit it as a Proposed Standard.\nOf course, we are also eager to hear from people who support our\nproposal, or who would like to suggest revisions.  There are a\nfew minor open \"Design Questions\" still listed in this draft.\n\n-Jeff and Paul\n\nP.S.: We should also note that Larry Masinter has suggested that\nthis should be submitted for \"Experimental\" rather than \"Proposed\nStandard\" status.  Our reading of RFC2026, however, convinces\nus that \"Experimental\" would be inappropriate.  Larry may still\ndisagree.\n\n\n\n", "id": "lists-010-11674778"}, {"subject": "(PRIVACY) Consensu", "content": "Unless further comment is heard, this will be used to close out\nthis issue.\n \nThe following sections is intended to emphasize the privacy problems\nthat have been coming up with client implementations (e.g. Javascript problem)\nthat are not careful about the information stored inside themselves.\n\nThe only comment was from Paul Leach on wording; I've adopted his\nchange below.\nComments to me,\nJim Gettys\n\nAdd to Section 14.4:\n-------------------\nHTTP clients are often privy to large amounts of personal information\n(e.g. the user's name, location, mail address, passwords, \nencryption keys, etc.), and should be very careful to prevent unintentional\nleakage of this information via the HTTP protocol to other sources.  We\n!very strongly recommend that a convenient interface be provided for\nthe user to control dissemination of such information, and that \ndesigners and implementors be particularly careful in this area.  \nHistory shows that errors in this area are often both serious security\nand/or privacy problems, and often generate very adverse publicity \nfor the implemetor's company.\n\n\n\n", "id": "lists-010-1168357"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Jeffrey Mogul:\n>\n>Several weeks ago, Paul Leach and I submitted an Internet-Draft\n>on \"Simple Hit-Metering for HTTP\":\n>\n> ftp://ds.internic.net/internet-drafts/draft-mogul-http-hit-metering-00.txt\n>\n>and announced it to the HTTP-WG mailing list.\n[....]\n>\n>Since we have not seen any criticism of our latest proposal, we would\n>like to interpret this as lack of criticism rather than lack of\n>interest,\n\nOops.  I have not yet reviewed your draft, but I plan to do so before\nDecember.  I will not bore you with explanations of my slowness.\n\nNote that I had some problems with the previous draft; I have no idea\nyet whether they were resolved in this one.\n\nI would interpret the lack of criticism so far as a lack of time, and\nas a sign that most WG members are using `shortest job first'\nscheduling.  Based on this, I feel that you need to wait at least\nuntil after the December IETF before submitting as a proposed\nstandard.\n\nI believe that those people who have not read the draft before the\nDecember IETF will more or less by default place themselves outside of\nthe `community' which works on completing the `community review'\nrequired for a proposed standard.  Note that Larry may disagree.  Of\ncourse, if the reviewing community is small, you will have to come\nwith some convincing evidence that there is enough community interest.\n\n>-Jeff and Paul\n\nKoen.\n\n\n\n", "id": "lists-010-11685433"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Koen writes:\n    I would interpret the lack of criticism so far as a lack of time, and\n    as a sign that most WG members are using `shortest job first'\n    scheduling.  Based on this, I feel that you need to wait at least\n    until after the December IETF before submitting as a proposed\n    standard.\n    \nWe understand this.  We sent our message about seeking Proposed Standard\nstatus to help prod people into treating this as something to look at soon.\n\n    I believe that those people who have not read the draft before the\n    December IETF will more or less by default place themselves outside of\n    the `community' which works on completing the `community review'\n    required for a proposed standard.\n\nMy understanding is that the standard ground rules for an IETF working\ngroup session is \"if you haven't read the draft, you aren't allowed\nto discuss it at the meeting.\"  So you are certainly right with respect\nto the WG meeting.\n\n    Of course, if the reviewing community is small, you will have to come\n    with some convincing evidence that there is enough community interest.\n\nAs we wrote, we have strong evidence that certain large customers are\neager to deploy this extension.  I'm not sure whether you can conclude\nfrom Paul's authorship that certain large vendors are eager to ship\nit, but perhaps Paul will enlighten us on that point.\n\n-Jeff\n\n\n\n", "id": "lists-010-11694466"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Jeff, Paul,\nI'm afraid silence doesn't always indicate consent; in my case\nit indicated an expectation that this draft would be a major point of\ndiscussion in San Jose and a desire to discuss it in person.  Given\nthe eagerness of your customers to implement a cache-metering\nsolution, though, we should probably start on this right away.  Let me\nstart by saying that I appreciate the work that has gone into this\ndraft, but that I feel that there are some very basic design changes\nhere that need critical discussion.  \nAs your draft makes admirably clear, this new mechanism\ncreates a duty between proxy and an origin server, which is a\nfundamentally different relationship than obtained before.  As was\npointed out at previous meetings, proxy servers currently act on\nbehalf of the end-user; to change that behavior without some way of\nletting the end user know it has changed has privacy implications (and\nthese are not necessarily the same privacy implications as exist when\n\"proxy-revalidate\" is sent from origin servers, because proxies have\naccess to data across multiple servers).\nIf we accept that any hit-metering proposal must create such a\nduty, we need to be very careful about the complexity of the duty that\nis assigned.  Your proposal seems to me to have three different layers\nof potential duties: the duty to limit usage; the duty to report\nusage; and the duty to report how Vary headers resulted in usage.  The\nfirst duty seems to me easy to implement and non-invasive in results;\nit is less accurate than reportage, but within a scope which is\ncontrolled by the origin server and which can be modified as time goes\non.  The second duty begins to create a more complex duty; it is not\nthat difficult to implement but may actual be a serious burden on very\nlarge proxy caches, especially if they make a best effort to limit\nnetwork traffic by chaining reportage of hits against different\nresources on the same origin server.  The third level seems to me far\nbeyond what should be expected of a proxy server; it asks the proxy to\ntrack the kind of demographic data that can be both very complex and\nan invasion of privacy. (I say that with some disappointment, frankly,\nas the third level of data is really the only kind that I am\nprofessionally interested in, as that level would give me information\nI need to plan for future resources).\nYour proposal says very clearly that \"any proxy that transmits\nthe Meter header in a request MUST implement every requirement of this\nspecification, without exception or amendment.\"  I don't think that\nthis is reasonable; I realize that your proposal includes methods\nwhich allow a proxy to \"implement\" a requirement by failing to offer a\nservice, but I think a design in which there were some true MUSTs, and\nother SHOULDs and MAYs would be more appropriate.  If we can establish\nwhich duty is a \"MUST\" for this scheme to work, we can make this\nradically easier to implement and use; especially as that\ndetermination will also make clear to origin servers which of the\nmechanisms (reportage or usage-exhaustion) is going to be the basic\nmethod for usage counting.  If there are multiple methods which can\nproduce different counts, there are going to be problems, even if the\nrange of difference is small on a per-proxy basis.\nEven assuming that this basic design is accepted, there are\nsome problems in your current proposal.  The description of the\n\"metering subtree\", for example, imples that proxies working together\nwithin a \"tree\" to obey usage limits and maintain counts; there is an\nunderlying assumption, however, that the proxies in that tree will\nmaintain a particular \"path\" of proxies back to the origin server,\nwhich may not always be the case.  The use of Meter as a sticky header\nalso presents some problems, as a meter request directive is sticky\nand other meter headers are not.  You also provide no method of\nunsticking the meter-request directive other than closing the\nconnection.  I frankly think the whole mechanism of creating sticky\nheaders needs to be worked out in other draft, rather implied by the\nbehavior of one header made sticky here.\nAgain, let me say that I appreciate the work that has gone\ninto this, and I believe that we now have a very good document from\nwhich to hash out the issues.  My belief that we have issues to hash\nout should in no way imply that I believe the work isn't important\nand urgent.\nregards,\nTed Hardie\nNASA NIC\n\n\n\n\n\n\n> Since we have not seen any criticism of our latest proposal, we would\n> like to interpret this as lack of criticism rather than lack of\n> interest, because we already have evidence that several large customers\n> are eager to deploy implementations of our proposal.\n> \n> Therefore, we intend to submit this I-D, or a minor revision thereof,\n> to the IESG as a Proposed Standard as soon as possible.\n> \n> Note that the criteria for Proposed Standard in RFC2026 says\n> \n>    A Proposed Standard specification is generally stable, has resolved\n>    known design choices, is believed to be well-understood, has received\n>    significant community review, and appears to enjoy enough community\n>    interest to be considered valuable.  However, further experience\n>    might result in a change or even retraction of the specification\n>    before it advances.\n> \n> so we *would* like to encourage further community review.\n> \n> If there is significant criticism based on technical merit, then we\n> will reconsider our intention to submit it as a Proposed Standard.\n> Of course, we are also eager to hear from people who support our\n> proposal, or who would like to suggest revisions.  There are a\n> few minor open \"Design Questions\" still listed in this draft.\n> \n> -Jeff and Paul\n> \n> P.S.: We should also note that Larry Masinter has suggested that\n> this should be submitted for \"Experimental\" rather than \"Proposed\n> Standard\" status.  Our reading of RFC2026, however, convinces\n> us that \"Experimental\" would be inappropriate.  Larry may still\n> disagree.\n> \n> \n\n\n\n", "id": "lists-010-11702906"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": ">     Of course, if the reviewing community is small, you will have to come\n>     with some convincing evidence that there is enough community interest.\n> \n> As we wrote, we have strong evidence that certain large customers are\n> eager to deploy this extension.  I'm not sure whether you can conclude\n> from Paul's authorship that certain large vendors are eager to ship\n> it, but perhaps Paul will enlighten us on that point.\n\nThis is grounds for de-facto standards, but not necessarily\nIETF standards-track. If the community doesn't step forward\nwithin the IETF, then maybe this track is premature.\n\nJoe\n----------------------------------------------------------------------\nJoe Touch - touch@isi.edu    http://www.isi.edu/~touch/\nISI / Project Leader, ATOMIC-2, LSAM       http://www.isi.edu/atomic2/\nUSC / Research Assistant Prof.                http://www.isi.edu/lsam/\n\n\n\n", "id": "lists-010-11716695"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Jeffrey Mogul:\n>Koen writes:\n>    Of course, if the reviewing community is small, you will have to come\n>    with some convincing evidence that there is enough community interest.\n>\n>As we wrote, we have strong evidence that certain large customers are\n>eager to deploy this extension.\n\nSo who are these customers?  Proxy cache vendors?  Proxy cache\noperators?  Eagerness in origin server vendors/operators alone would\nnot necessarily convince me that this protocol extension will be\ndeployed.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-11724741"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "> Although our previous (and quite different) proposal, at the end of July,\n> resulted in some discussion, this one has raised no comments on the\n> mailing list (although we have received a few private comments).\n> \n> Since we have not seen any criticism of our latest proposal, we would\n> like to interpret this as lack of criticism rather than lack of\n> interest, because we already have evidence that several large customers\n> are eager to deploy implementations of our proposal.\n\nMy comments on your last proposal still apply to this one. From\n   <http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q3/0396.html>\n==================\n[\"coop\" is now called \"Meter\"]\nthat is not an appropriate use of Connection.  For one thing, assuming\nthat all intermediaries have caches (and thus would assign any meaning to\nbeing cooperative) is wrong.  For another, it doesn't take advantage of\nthe extensibility mechanism inherent in cache-control.  Instead of all\nthe coop negotiation, an origin server should decide whether being\ncooperative is required or optional.  If it is required, then send\n\n    Cache-control: proxy-revalidate, coop\n\nwith coop (or something similar) being defined as a modifier on\nproxy-revalidate such that caches which obey the coop directive\n(whatever that may imply) may ignore the proxy-revalidate.\nIf cooperation is considered optional, then just send\n\n    Cache-control: coop\n\nThe advantage here is that you don't need to mess with Connection and\nthe directives will propagate to all recipients instead of just the\nnearest neighbor on the response chain.\n==================\n\nWhen I said the above, I should have been more clear in my objection.\nConnection cannot be used as a means for two caches to communicate because\nnot all proxies have caches.  Requiring that a non-cache proxy fiddle\nwith Cache-Control on the basis of what they must drop from Connection\nis just not appropriate.  Use a design which doesn't fail through such\nproxies.\n\nIn addition, the current design, if implemented, forces the proxy to add\n\n    Connection: Meter\n    Meter: will-report-and-limit\n\n(or \"wont-report\" or \"wont-limit\") to every single request forwarded,\nregardless of the likelihood that the origin server and the requested\nresource happen to use hit-metering.  In other words, \"good citizen\"\nproxies are forced to do extra work on every request just to support\na few \"bad citizen\" service providers who are too lazy to perform\nstatistical sampling.  Any reasonable estimate of the percentage of\nresources requiring \"hit-metering\", versus those that don't, will show\nthat the amount of extra bytes sent by the proxy to support those\nfew lazy servers will far exceed the amount of extra bytes that would be\nsent by cache-busting.  As such, the proposal is actually less efficient\nfor the Internet than doing nothing at all.\n\n> P.S.: We should also note that Larry Masinter has suggested that\n> this should be submitted for \"Experimental\" rather than \"Proposed\n> Standard\" status.  Our reading of RFC2026, however, convinces\n> us that \"Experimental\" would be inappropriate.  Larry may still\n> disagree.\n\nAgain, my previous comments still apply to this proposal.\n==================\nI still don't believe that such count-forwarding is appropriate for a\nproposed standard (experimental is okay), since I don't think that\npeople disable caching just to record hit-counts (which are already\nknown not to be an accurate measure of readers).  Most people disable\ncaching by accident, and those that do it on purpose are normally\nlooking for Referer and IP/hostname (more than just a request count).\n==================\n\nI do not believe that the proposal is valuable to the Internet community.\nIn fact, I believe it will cause more harm than good if implemented,\nand would strongly recommend not implementing it as it currently stands.\nOn that basis, I oppose it going forward as a Proposed Standard.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-11732828"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Koen Holtman wrote:\n> Jeffrey Mogul:\n> >As we wrote, we have strong evidence that certain large customers are\n> >eager to deploy this extension.\n> \n> So who are these customers?  Proxy cache vendors?  Proxy cache\n> operators?  Eagerness in origin server vendors/operators alone would\n> not necessarily convince me that this protocol extension will be\n> deployed.\n\nAll of the above.  This statement is backed up by continuous\noverwhealming demand for this feature from lots of Netscape Proxy\ncustomers.\n\nThe need obviously originates from origin server operators, as they\nneed their true statistics for billing based on ad exposure etc.  I\nthink most people would agree that they are entitled to that\ninformation.\n\nSome content providers (1) pressure online service providers and other\nproxy operators to give them their statistics.  Others (most) simply\n(2) disable caching intentionally.\n\n(1) forces some large online service providers to run an up-to-date\ncheck for every file in their cache for every access, which increases\nlatencies and wastes resources on their already otherwise busy\nservers, somewhat defeating the benefits that they are trying to gain\nby running proxy.\n\n(2) defeats the whole caching idea.\n\nNetscape, in all these roles (Proxy cache vendor, origin server\nvendor, and a content provider), is eager to see this issue reach\nconsensus, so we can support it in our software.  In my opinion this\nis one of the hottest items that the Working Group should solve in the\nimmediate future.  It has been deferred far too long already.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-11745031"}, {"subject": "(CONTENT) consensu", "content": "There was only one comment on the draft, from Rich Salz.  I made\na minor edit in response.  Unless further comment is forthcoming,\nthis will closeout the CONTENT issue.\n\nAnd to answer his question about whether a IANA registration of\nsomething implies an open specification, the answer is no, not\nnecessarily.  We must provide guidance to IANA on what/how things\nshould be registered.  The MIME experience is that making such\na registration dependent on such a specification being available \ncan be more trouble than it is worth.  Ergo, we use \"should\" rather\nthan \"must\" for that recommendation.\n- Jim\n\n\nProposed Resolution:\n====================\n\nSection 3.5:\n============\nChange:\n-------\nContent coding values are used to indicate\nTo:\n---\nContent coding values indicate\n\n\nChange:\n-------\n\ncontent-coding= \"gzip\"|\"compress\"|token\n\nto:\n---\ncontent-coding= \"gzip\"|\"x-gzip\"|\"compress\"|\"x-compress\"|token\n\n\nChange:\n-------\nGzip is available from the GNU project at \n<URL:ftp://prep.ai.mit.edu/pub/gnu/>\n\nto:\n---\n[gzip]\n\n\nAdd to section 3.5 (Content Codings):\n-------------------------------------\n\nHTTP defines a registration process which uses the Internet Assigned\nNumbers Authority (IANA) as a central registry for content-coding value\n!tokens.  Additional content-coding value tokens beyond the four \n!defined in this document (gzip xgzip compress xcompress)\n!should be registered with the IANA.\nTo allow interoperability between clients\nand servers, specifications of the content coding algorithms used\nto implement a new value should be publically available and \nadaquate for independent implementation, and must conform to \nthe purpose of content coding defined in this section.\n\nAdd to section 16: (References)\n-------------------------------\n\n[gzip] Gzip is available from the GNU project at \n<URL:ftp://prep.ai.mit.edu/pub/gnu/>, and a\nwork-in-progress describes the gzip data format\nin detail.\n\n\n------- End of Forwarded Messages\n\n\n\n", "id": "lists-010-1175452"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "On Wed, 20 Nov 1996, Roy T. Fielding wrote:\n> \n> I still don't believe that such count-forwarding is appropriate for a\n> proposed standard (experimental is okay), since I don't think that\n> people disable caching just to record hit-counts (which are already\n> known not to be an accurate measure of readers).  Most people disable\n> caching by accident, and those that do it on purpose are normally\n> looking for Referer and IP/hostname (more than just a request count).\n\nI concur. Hit counts are *inherently* unreliable. The 'hit-metering'\nproposal will do very little to attack the fundamental causes of that and\ndoes not address the real reasons most deliberately anti-cached sites\ndefeat caching. In my experience, it is the requirement for highly\nvolatile sites where *every single request* will return information that\nchanges in important ways that leads to deliberate cache defeating. We are\ntalking shopping baskets, web chat systems, reporting systems for highly\nvolatile information sources, and similar things. \n\nAttempts to finess the system for the sake of improving hit counting are\ndoomed from the start by the simple fact that most *browsers* have their\nown user selectable options for caching that are completely independant of\nthe standards: 'Check every time','Check once per session', 'Never check'.\nThis *alone* is enough to make efforts to make proxies report hits nearly\nirrelevant. Are they reporting 20 repeat hits from someone who 'checks\nevery time' or 1 new hit each from people who 'Never check'? You don't\nknow. We don't know. NO ONE knows. A server can *guess* based on referrer\nand IP address in their logs, or come very close to exact counts by\nanti-caching. But the necessary abstraction of data by the proxies on\nsummary reports for hit-metering will defeat these efforts in log\nanalysis and passing raw log information would defeat the *purpose* of\nproxies. \n\nThis also does not begin to address the questions of privacy and security\nand their impact on the usage of hit-metering.. Many corporate proxies\nwould more than reluctant to be sending out information about their\ninternal usages to anyone who asked - they would be actively opposed to\nit. \n\nIn my view, the hit-metering proposal seems to request large amounts of\nwork for proxies at nearly no benefit - to anyone.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-11754893"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "On Wed, 20 Nov 1996, Ari Luotonen wrote:\n> \n> The need obviously originates from origin server operators, as they\n> need their true statistics for billing based on ad exposure etc.  I\n> think most people would agree that they are entitled to that\n> information.\n\nNo - I think that after some thought, most people would conclude that they\n*want* that information, but are not necessarily *entitled* to that\ninformation. The key here is that they are paying their *service provider*\nnot *us*. The *obligations* are therefore between them and their service\nprovider - not between them and the net at large. This is an important\ndistinction. The relevant model is caller ID to my mind. \n\nBusinesses pay for phone lines so that they can communicate with their\ncustomers. Many would *like* to identify their customers phone numbers. \nBut they are not *entitled* to it, and I can block caller ID and something\nlike 50% of people in California do have full blocking. Californians value\ntheir privacy. \n\n> Some content providers (1) pressure online service providers and other\n> proxy operators to give them their statistics.  Others (most) simply\n\n> (2) disable caching intentionally.  \n\nInterestingly, PacBell is now mounting an ad campaign bordering on the\nactively mis-leading to try and get people to quit using the full blocking\noption. for caller id. I would guess they are *also* receiving pressure\nfrom their business customers to make caller id more effective than it is\nnow. \n\n> \n> (1) forces some large online service providers to run an up-to-date\n> check for every file in their cache for every access, which increases\n> latencies and wastes resources on their already otherwise busy\n> servers, somewhat defeating the benefits that they are trying to gain\n> by running proxy.\n\nAm I missing something here? Why would large online services give *any*\ninformation about their proxy stats to an outside group? I certainly would\nnot do so for Joe Q. Not My Customer.\n\n> (2) defeats the whole caching idea.\n\nI am getting quite close to crossing a couple of my favorite search\nengines off my lists because of aggressive use of decaching to force a\n*new* advert to come up everytime I do anything (ok - so maybe I will\nsettle for turning off graphics when I visit the search engines.)  I load\n10K of search results and 20K of animated advertisement. I don't think\nthat *improving* caching is exactly their high-priority goal. They are\ngoing a long way out of their way to make sure that I get the dubious\nhonor of seeing a *different* advertisment everytime I click on anything. \n\nHmmm - here is something I think people actually *would* like. The ability\nto *selectively* turn off graphics for certain sites permanently.  What a\nconcept - being able to visit a search engine and actually get *search\nresults* instead of waiting twenty seconds for an animated advert to\nload. I wonder if it could be implemented in the browser proxy settings\nor via a plug in... \n\nhttp://www.alta-vista.comimage-loading-off\n\nJust like a bookmark list.  You know - I bet we could save *TONS* of\nbandwidth that way. Much more than any hit metering proposal would ever\ndo. If a site's advertisements get too persistantly annoying: <poof>. \n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-11764803"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "On Wed, 20 Nov 1996, Benjamin Franz wrote:\n\n> Date: Wed, 20 Nov 1996 05:22:56 -0800 (PST)\n> From: Benjamin Franz <snowhare@netimages.com>\n> To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: Hit-metering: to Proposed Standard? \n> \n> On Wed, 20 Nov 1996, Roy T. Fielding wrote:\n> > \n> > I still don't believe that such count-forwarding is appropriate for a\n> > proposed standard (experimental is okay), since I don't think that\n> > people disable caching just to record hit-counts (which are already\n> > known not to be an accurate measure of readers).  Most people disable\n> > caching by accident, and those that do it on purpose are normally\n> > looking for Referer and IP/hostname (more than just a request count).\n\nI havne't been following this thread very closely and I don't claim to know\nthe answers but I can't let that pass.  A lot of people are billing based on\nhit count and deliberately disabling caching so as to get that data as well\nas other data.  That's was the experts at the W3C sponsored meeting some\nmonths ago in Cambridge Mass (on how to decrease melt-down caused by web\ntraffic) who had studied the matter had to say. \n\n> ...\n> \n> Attempts to finess the system for the sake of improving hit counting are\n> doomed from the start by the simple fact that most *browsers* have their\n> own user selectable options for caching that are completely independant of\n> the standards: 'Check every time','Check once per session', 'Never check'.\n> This *alone* is enough to make efforts to make proxies report hits nearly\n> irrelevant. Are they reporting 20 repeat hits from someone who 'checks\n> every time' or 1 new hit each from people who 'Never check'? You don't\n> know. We don't know. NO ONE knows. A server can *guess* based on referrer\n> and IP address in their logs, or come very close to exact counts by\n> anti-caching. But the necessary abstraction of data by the proxies on\n> summary reports for hit-metering will defeat these efforts in log\n> analysis and passing raw log information would defeat the *purpose* of\n> proxies. \n\nPerfection is not generally needed for utility.\n\n> ...\n> \n> Benjamin Franz\n\nDonald\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\nhttp://www.cybercash.com           http://www.eff.org/blueribbon.html\n\n\n\n", "id": "lists-010-11775520"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Benjamin Franz wrote:\n> No - I think that after some thought, most people would conclude that they\n> *want* that information, but are not necessarily *entitled* to that\n> information. The key here is that they are paying their *service provider*\n> not *us*.\n\nThey are paying for their online service provider for their internet\nconnection, but they are not paying their online service provider for\nthe services rendered by origin servers, nor is the online service\nprovider paying those origin servers for their services.\n\n\n> The *obligations* are therefore between them and their service\n> provider - not between them and the net at large.\n\nIf this is the case, this means that there is a corresponding\nobligation between the service provider and the origin server\n(otherwise you could get total immunity, and no one would be liable to\nthe origin server), which brings us to the issue at hand, and it *is*\nthe service provider's responsibility to report back accesses to the\norigin server.\n\n\n> This is an important\n> distinction. The relevant model is caller ID to my mind. \n> \n> Businesses pay for phone lines so that they can communicate with their\n> customers. Many would *like* to identify their customers phone numbers. \n> But they are not *entitled* to it, and I can block caller ID and something\n> like 50% of people in California do have full blocking.\n\nThe Caller ID analogy is not relevant to the hit reporting draft.  The\nhit reporting draft doesn't give out any information about the\nrequesting user/client, only the fact that it was requested.\n\nPhone usage billing might be a closer analogy.  The origin servers are\nonly given enough information so they know how many hits they really\ngot -- that is, what is the quantity of their services used.\n\n\n> Californians value their privacy. \n\nBefore there's any confusion on this matter: the hit reporting in\nMogul, Leach draft discloses ONLY the number of hits, NOTHING else.\nThat's the least bit of information you need to have in order to find\nout how much of your services were used.  No individual users' private\ninformation was disclosed.\n\n\n> Am I missing something here? Why would large online services give *any*\n> information about their proxy stats to an outside group? I certainly would\n> not do so for Joe Q. Not My Customer.\n\nBecause you _are_ relaying _their_ services, for _your_ customers,\nyour paying customers that have chosen to use that service.  By\nco-operating you can serve that data faster from your cache, and\nyou're not \"stealing\" the data and making your own illegal copies.\n\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-11786496"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": ">Before there's any confusion on this matter: the hit reporting in\n>Mogul, Leach draft discloses ONLY the number of hits, NOTHING else.\n>That's the least bit of information you need to have in order to find\n>out how much of your services were used.  No individual users' private\n>information was disclosed.\n\nThis is precisely what worries me about the draft. I don't believe it\ngives anything like sufficient information to the advertisers. Having\nspoken at length to this community at the workshop we organised on the\ntopic at MIT I'm afraid that the draft simply does not meet the needs\nof either side.\n\nRather than attempt to make this decision on our own how about we\narrange a meeting with the actual advertisers. They are not engineers\nand do not contribute to IETF mailing lists. But if we are to produce\na system that meets their needs while satisfying our privacy concerns\nwe need to have a much better overview of the requirements.\n\nThe IETF was able to successfully operate in mailing list only mode\nin the days when the IETF engineers were the principal users of their\nproducts (telne, ftp et al). I don't consider listening to customer \nreports filtered through the marketing department is going to be\nparticularly useful either.\n\nAnother cause for concern is the breakdown of the password \nauthentication and cookie model of doing user authentication. I don't\nknow if other people have noticed but the trend amongst the sites seems\nto be towards using password authentication which may indicate that\nthe content providers are switching, posibly because there are now\n\"cookie aware\" caches.\n\nSince the content provider has available in the protocol a means of\nforcing every hit to arrive and thus be counted I think that providing \nany less information that they would gain thereby is likely to be an\nexercise in futility. At the end of the day I doubt that many people \non this list would forego doubling their income to save internet \nbandwidth.\n\n\nPhill\n\n\n\n", "id": "lists-010-11797415"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "> Jeffrey Mogul:\n> \n> Since we have not seen any criticism of our latest proposal, we would\n> like to interpret this as lack of criticism rather than lack of\n> interest, because we already have evidence that several large customers\n> are eager to deploy implementations of our proposal.\n\nI have some questions to the draft as it stands.\n\nThe concept of hit-metering is reasonably clear (as cachemaster I get phone \ncalls asking for hitcounts and refuse to give them out), but the reason for \nusage-limiting is less clear to me. Either I implement hitmetering, in which \ncase the webserver gets its counts; or I do not, in which case I ignore the \nusage-limiting. \n\nMy question is: why bother with usage-limiting ?\n(and implementing it in a caching mesh with multiple co-operating caches on \neach level in a \"hierarchy\" will be a pain). The purpose of bounding \ninaccuracy in a count is not achieved by adding usage-limiting, as the primary \ninaccuracy will be added by ill-behaving caches, users' inimitable inaccurate \nways (check once per session, always check, never check), servers/network \nfailure and so on (including all those that do hitmetering but do not honour \nthe usage-limiting).  The only useful thing is that it does limit the number \nof times a \"well-behaved\" cache server hands out the same ad. Is the \ncomplexity of usage-limiting worth it?\n\nTo illustrate the mesh problem: If I am a cache that gets handed 2 usages \nthrough my first parent and 4 through my second parent, do I have to report \nback through the parent who gave me the usage-limit or can I freely chose to \nreport back through my third parent (who has not connected to the origin \nserver before and is likely to report 4 out of zero and 2 out of zero or \ngenerally confuse the origin server)? If I have to report through the \nappropriate parent, this requires me to store where I got the document from; \nand it heavily influences traffic patterns, robustness and the redundancy of \nmy mesh.\n\n<<\n3.3 Negotiation of hit-metering and usage-limiting\n   An origin server that wants to collect hit counts for a resource, by\n   simply forcing all requests to bypass any proxy caches, would respond\n   to requests on the resource with \"Cache-control: proxy-revalidate\".\n   (An origin server wishing to prevent HTTP/1.0 proxies from improperly\n   caching the response could also send both \"Expires: <now>\", to\n   prevent such caching, and \"Cache-control: max-age=NNNN\", to allow\n   newer proxies to cache the response).\n>>\n\nIn reading the HTTP/1.1 spec, I had the understanding that must-revalidate and \nproxy-revalidate as defined in 14.9 only requires a cache to revalidate if the \nresponse is stale, not every time a request is made (unless there is an \nAuthorization field). Please correct me if I am wrong.\n\n> draft-ietf-http-v11-spec-07.txt:\n> When the must-revalidate directive is present in a response received \n> by a cache, that cache MUST NOT use the entry after it becomes stale \n> to respond to a subsequent request without first revalidating it with \n> the origin server. (I.e., the cache must do an end- to-end \n> revalidation every time, if, based solely on the origin server's \n> Expires or max-age value, the cached response is stale.)\n> [..]\n> The proxy-revalidate directive has the same meaning as the must- \n> revalidate directive, except that it does not apply to non-shared \n> user agent caches.\n\nIf the proxy handing out a \"Cache-control: proxy-revalidate\" also is expected \nto modify staleness parameters like \"Cache-control: max-age=0\" this should be \nstated.  Are caches allowed to change max-age ??\n\nMore questions to come.\n\n\nIngrid\n-- \n  Ingrid.Melve@uninett.no            MIME, PGP and PEM email encouraged\n        UNINETT, Postboks 6883 Elgeseter, N-7002 Trondheim, Norway \n  Oj, der telte han meg. Der telte han meg en gang til! For en t?ffing!\n\n\n\n", "id": "lists-010-11806937"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "> Ari Luotonen:\n> Before there's any confusion on this matter: the hit reporting in\n> Mogul, Leach draft discloses ONLY the number of hits, NOTHING else.\n> That's the least bit of information you need to have in order to find\n> out how much of your services were used.  No individual users' private\n> information was disclosed.\n\nThat is the part of the draft that I really like and appriciate, handing out \nsimple counts is OK as long as I preserve the privacy of my users.\n\n> > Am I missing something here? Why would large online services give *any*\n> > information about their proxy stats to an outside group? I certainly would\n> > not do so for Joe Q. Not My Customer.\n> \n> Because you _are_ relaying _their_ services, for _your_ customers,\n> your paying customers that have chosen to use that service.  By\n> co-operating you can serve that data faster from your cache, and\n> you're not \"stealing\" the data and making your own illegal copies.\n\nThe other way to look at that is that I as cachemaster am giving the \ninformation providers a free ride: they get free diskspace and the illusion of \ngood connectivity, for which they should pay me and be grateful since I serve \n_their_ consumers. Demanding hitcounts from me is demanding that I provide \nthem with further service for which I get nothing but more network traffic and \nadded complexity. There are at least two sides to every coin.\n\n\nIngrid\n PS: Until they cull me on my head with something hard, I will argue for web \ncaches as pure network buffers; no stealing or illegality in buffering bits.\n-- \n  Ingrid.Melve@uninett.no          MIME, PGP and PEM email encouraged\n  Der telte han meg. O????????, han telte meg, han telte meg.\n\n\n\n", "id": "lists-010-11818530"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": ">  PS: Until they cull me on my head with something hard, I will argue\n>  for web caches as pure network buffers; no stealing or illegality\n>  in buffering bits.\n\nThat's how I see caches, too -- I just wanted to add some melodrama.\n\nYour point about caches providing content providers a service by\ncaching them is true, too.  It's a two-way street -- on the other\nhand, the content provider is doing you a favor by providing a service\nthat your users find useful, so they stay online and use your\nservices.  There are benefits to both parties, that's why I think it\nmakes sense to co-operate.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-11828388"}, {"subject": "Re: (INTEGOK) rough consensu", "content": "I am having a hell of a time keeping connected to the discussions -- our\nnetwork and workstations keep failing due to multiple power failures\nand server problems.\n\nTypos and rough wording changes:\n\n> 10.13Content-MD5\n> \n> The Content-MD5 entity-header field is an MD5 digest of the entity-body,\n\n                                field provides an MD5 digest ...\n\n> as defined in [RFC 1864], for the purpose of providing an end-to-end\n\n                RFC 1864 [xx], \n\n> integrity check of the entity-body.\n> \n> ContentMD5= \"Content-MD5\" \":\" digest\n> digest= <base64 of 128 bit MD5 digest as per RFC 1864>\n\n        Content-MD5     = \"Content-MD5\" \":\" md5-digest\n        md5-digest      = <base64 of 128 bit MD5 digest as per RFC 1864>\n\n> The Content-MD5 header may optionally be generated by origin servers,\n> and functions as an integrity check of the entity-body. Only\n\n  The Content-MD5 header may be generated by an origin server to function\n  as an integrity check of the entity-body. Only,\n\n> origin-servers may generate the Content-MD5 headers: proxies and\n\n                                              header field; proxies and\n\n> gateways MUST NOT generate it, as this would defeat its value and an\n\n                                                          value as an\n\n> end-to-end integrity check. Any recipient of the entity-body, including\n> gateways and proxies, MAY check that the digest value in the header\n\n                                                        in this header field\n\n> matches that of the entity-body as received. \n> \n> When being generated, the MD5 digest is computed based on the value of\n> the entity-body after Content-Encoding (if any) but before\n> Transfer-Encoding (if any) is applied; when being checked, after\n> Transfer-Encoding has been removed, but before Content-Encoding has been\n> removed.\n\nThe above is too terse.\n  \n  The MD5 digest is computed based on the content of the entity body,\n  including any Content-Encoding that has been applied, but not including\n  any Transfer-Encoding.  If the entity is received with a\n  Transfer-Encoding, that encoding must be removed prior to checking\n  the Content-MD5 value against the received entity.\n\n> There are several ways in which the application of Content-MD5 to HTTP\n> entity-bodies differs from its application to MIME entity-bodies. One is\n> that HTTP, unlike MIME, does not use Content-Transfer-Encoding, and does\n> use Transfer-Encoding and Content-Encoding. Another is that, unlike\n> MIME, the digest is computed over the entire entity-body, even if it\n\n                   may be computed\n\n> happens to be a MIME multi-part content-type. (Note that the multi-part\n\n             be a \"multipart\" type.  ...\n\n> bodies may themselves have Content-MD5 headers.) Another is that HTTP\n> more frequently uses binary content types than MIME, so it is worth\n> noting that in such cases, the byte order used to compute the digest is\n> network byte order. Lastly, the canonical form of text types in HTTP\n> includes several line break conventions, so conversion to CR-LF is not\n\n                                           so conversion of all line\n  breaks to CRLF form is not\n\n> always required before computing or checking the digest: any acceptable\n> convention should be left unaltered for inclusion in the digest.\n> \n>>      Note: the net result of the above is that the digest is\n>       computed on the content that would be sent over-the-wire, in\n>>      network byte order, but prior to any transfer coding being \n>>      applied.\n\nWhy not just say that and leave out the rest?  I think the note is far\nmore effective (and more likely to always be accurate) then the paragraph\nabove it.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1183483"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Ted, thanks for your thoughtful comments.  I think you raise some\nimportant issues, but I believe that we have covered them in our\ndesign.  We may not, however, have expressed this clearly enough.\n(So if you end up agreeing with me after reading this message, then\nI guess we need to improve the clarity of the I-D).\n\n    As your draft makes admirably clear, this new mechanism creates a\n    duty between proxy and an origin server, which is a fundamentally\n    different relationship than obtained before.\n\nWell, I guess we didn't make this clear.  The new mechanism does\nNOT create a duty.  What it does is to allow a proxy and server\n(not necessarily an origin server!) to agree on a connection-by-\nconnection basis to enter into a \"contract\" of sorts.  So the\nonly duty imposed by our proposal is that if you send a Meter\nheader with certain directives, then you are bound to honor a\nstandardized interpretation of those directives.\n\n    As was pointed out at\n    previous meetings, proxy servers currently act on behalf of the\n    end-user; to change that behavior without some way of letting the\n    end user know it has changed has privacy implications\n\nThis is another good point, but we have been quite sensitive to\nprivacy and autonomy issues.  First of all, a proxy is NEVER required\nto agree to provide hit-metering, period.\n\nSecond, and perhaps most important, our design does not transmit\nmore data about a user to the origin server (or any inbound server)\nthan would be transmitted using the existing features of HTTP/1.1.\nI believe this is an unconditionally true statement: the use of\nthe Meter mechanism does not result in the communication of any\ninformation beyond that provided if the Meter mechanism is not used\n(assuming that the proxy conforms to the HTTP/1.1 spec).  In fact,\nI believe that it communicates significantly less information about\nindividual clients.\n\nIf someone is able to describe a specific scenario where the use\nof the Meter mechanism, as proposed in our draft, does in fact provide\nmore per-client information than the existing HTTP/1.1 mechanisms,\nthen we would regard this as a bug in our specification that needs\nto be fixed (or at least, that needs to be called out in the Security\nConsiderations section).\n\nNote that a proxy that chooses not to conform to the existing HTTP/1.1\ncaching mechanisms (e.g., \"proxy-revalidate\") for privacy reasons is\nnot required to use Meter, and so is equally able to follow the same\nprivacy policy.\n\n    (and these\n    are not necessarily the same privacy implications as exist when\n    \"proxy-revalidate\" is sent from origin servers, because proxies\n    have access to data across multiple servers).\n\nI'm afraid that I don't understand your point here.  Could you\nillustrate with an example?\n\n    If we accept that any hit-metering proposal must create such a\n    duty, we need to be very careful about the complexity of the duty\n    that is assigned.  Your proposal seems to me to have three\n    different layers of potential duties: the duty to limit usage; the\n    duty to report usage; and the duty to report how Vary headers\n    resulted in usage.  The first duty seems to me easy to implement\n    and non-invasive in results; it is less accurate than reportage,\n    but within a scope which is controlled by the origin server and\n    which can be modified as time goes on.  The second duty begins to\n    create a more complex duty; it is not that difficult to implement\n    but may actual be a serious burden on very large proxy caches,\n    especially if they make a best effort to limit network traffic by\n    chaining reportage of hits against different resources on the same\n    origin server.\n\nAgain, a proxy is NEVER required to offer to usage-limit a resource.\nNEVER.  Our proposal provides a way for a proxy to make this offer,\nbut does not require it to make this offer.\n\n    The third level seems to me far beyond what should\n    be expected of a proxy server; it asks the proxy to track the kind\n    of demographic data that can be both very complex and an invasion\n    of privacy. (I say that with some disappointment, frankly, as the\n    third level of data is really the only kind that I am\n    professionally interested in, as that level would give me\n    information I need to plan for future resources).\n\nAgain, if privacy is a concern, the proxy need not offer to hit-meter\nor usage-limit.\n\nAnd even if a proxy does offer to hit-meter or usage-limit, it is\nalways allowed to meet its \"contractual duty\" for a given resource\nby simply doing the equivalent of \"proxy-revalidate\" for the response.\nIt can make this determination *after* examining the response, to\nsee if the \"Vary\" header in the response asks it for information that\nit would not normally provide.\n\nSince we expect that an origin server would normally request hit-metering\nor usage-limiting for precisely those resources for which it would\nnormally send \"proxy-revalidate\", this seems to be neutral as far\nas privacy is concerned.\n\nOr perhaps even better than neutral.  Suppose you (at an origin server)\nwant to know how your user community breaks down by User-agent, but you\nhave no need for other per-request headers (such as Accept-* headers,\nVia headers, etc.)  The ability to say\nMeter: do-report\nVary: User-agent\nmeans that you will end up with the counts that you need, but WITHOUT\ncollecting a lot of irrelevant information (and so without collecting\ninformation that may compromise other privacy considerations).\n\n    Your proposal says very clearly that \"any proxy that transmits the\n    Meter header in a request MUST implement every requirement of this\n    specification, without exception or amendment.\"  I don't think that\n    this is reasonable; I realize that your proposal includes methods\n    which allow a proxy to \"implement\" a requirement by failing to\n    offer a service, but I think a design in which there were some true\n    MUSTs, and other SHOULDs and MAYs would be more appropriate.  If we\n    can establish which duty is a \"MUST\" for this scheme to work, we\n    can make this radically easier to implement and use; especially as\n    that determination will also make clear to origin servers which of\n    the mechanisms (reportage or usage-exhaustion) is going to be the\n    basic method for usage counting.  If there are multiple methods\n    which can produce different counts, there are going to be problems,\n    even if the range of difference is small on a per-proxy basis.\n\nAgain, the proposal NEVER requires that a proxy agree to do ANYTHING.\nThe \"MUST\" that you quote simply requires that if a proxy does use\nthe Meter header to offer to do something, then it must faithfully\ncarry out what it offers to do.  If you can find a MUST in our\nproposal that somehow binds a proxy to do something against its explicit\nchoice, then this is a bug that we will fix.\n\n    Even assuming that this basic design is accepted, there are some\n    problems in your current proposal.  The description of the\n    \"metering subtree\", for example, imples that proxies working\n    together within a \"tree\" to obey usage limits and maintain counts;\n    there is an underlying assumption, however, that the proxies in\n    that tree will maintain a particular \"path\" of proxies back to the\n    origin server, which may not always be the case.\n\nRemember that the \"contracts\" it creates are for a single hop, they are\nNOT necessarily between a proxy and the origin server.  So we envision\nthat even if a proxy's path to the origin server does change, it's\n\"contract\" with the previous inbound proxy still holds.  And remember\nthat our proposal explicitly uses a \"best-efforts\" model, which means\nthat if the previous inbound proxy is not reachable, your proxy is\nunder no obligation to try forever to report the hit-counts.\n\nYour comment does raise a subtle point that we had ignored, which is\nthat a faithful implementation of hit-counting would, in theory,\nhave to record the identity of the inbound proxy from which each\nresponse is received.  In practice, there are numerous ways around\nthis (for example, flushing out the pending hit-count reports before\nchanging the path configuration), but in any case it does not seem\ntoo onerous to do a naive implementation which simply records the\nIP address of the source of each response.  I believe that systems\nlike Harvest/Squid probably already need to do something like this.\n\n    The use of Meter as a sticky header also presents some problems, as\n    a meter request directive is sticky and other meter headers are\n    not.  You also provide no method of unsticking the meter-request\n    directive other than closing the connection.  I frankly think the\n    whole mechanism of creating sticky headers needs to be worked out\n    in other draft, rather implied by the behavior of one header made\n    sticky here.\n\nThe \"stickiness\" of the Meter request-directive is only a performance\noptimization, and if there are serious technical arguments against\nit, we could remove that without affecting any other aspect of the\nproposal.\n\nBut I do not think it is accurate to think of this in the same way\nthat we have previously discussed \"sticky\" headers, since those\nwere for actual request-headers.  The Meter request header is a sort of\nunusual thing that applies to transport-level connections, not to\nindividual requests, and so it might probably be better to use a\nterm other than \"sticky\" here.  (The Meter response directives are\nper-response, but hop-by-hop, and so if there is a general \"sticky\"\nmechanism agreed upon for the rest of HTTP, then it could take advantage\nof this.)\n\nAs to the issue of \"unsticking\" (\"unstickying?\") the meter-request\ndirective: remember that a proxy that has offered, say, to hit-meter\nresponses it receives on a connection is able to meet this obligation by\n(in effect) removing the Meter header and adding \"proxy-revalidate\".\nWhile this may result in its generating conditional GETs on responses\nthat the server doesn't want hit-metered, this is again just a performance\nissue, not a correctness one.  But if you are concerned about performance\nand you think that it's worth including an \"unsticking\" feature, please\nsuggest something.\n\n-Jeff\n\n\n\n", "id": "lists-010-11837317"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "    Attempts to finess the system for the sake of improving hit\n    counting are doomed from the start by the simple fact that most\n    *browsers* have their own user selectable options for caching that\n    are completely independant of the standards: 'Check every\n    time','Check once per session', 'Never check'.  This *alone* is\n    enough to make efforts to make proxies report hits nearly\n    irrelevant. Are they reporting 20 repeat hits from someone who\n    'checks every time' or 1 new hit each from people who 'Never\n    check'? You don't know. We don't know. NO ONE knows. A server can\n    *guess* based on referrer and IP address in their logs, or come\n    very close to exact counts by anti-caching. But the necessary\n    abstraction of data by the proxies on summary reports for\n    hit-metering will defeat these efforts in log analysis and passing\n    raw log information would defeat the *purpose* of proxies.\n\nMy analysis of header logs from a very popular browser suggests that\nits \"checks\" (whether per-session or every-time) are done using\nIf-modified-since headers.  Our proposal specifically separates\nthe counting of 304 responses from the counting of other responses\n(see section 5.3).  Because of this, we can accurately count the\nnumber of non-checking GETs.\n\nWhat we cannot do is to accurately distinguish between a large\nnumber of users looking at a document with large browser caches,\nand a smaller number of users with small browser caches.  But\nneither can any other hit-counting scheme that doesn't involve\nsome sort of per-user data (such as cookies).\n\nAnd we are in no way proposing the transmission of raw logs!\n(our proposal takes no stand on this topic; the word \"log\"\ndoes not appear in it).\n\n    This also does not begin to address the questions of privacy and\n    security and their impact on the usage of hit-metering.. Many\n    corporate proxies would more than reluctant to be sending out\n    information about their internal usages to anyone who asked - they\n    would be actively opposed to it.\n\nWe run a large corporate proxy in our building (>1500000 refs/day)\nand we are extremely sensitive to privacy issues, so I'm not ignoring\nthem.  No corporate (or any other) proxy is required to do anything by\nour proposal.\n\nAside from that, I addressed the privacy considerations in my response\nto Ted Hardie.\n\n-Jeff\n\n\n\n", "id": "lists-010-11855125"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "    Connection cannot be used as a means for two caches to communicate\n    because not all proxies have caches.  Requiring that a non-cache\n    proxy fiddle with Cache-Control on the basis of what they must drop\n    from Connection is just not appropriate.  Use a design which\n    doesn't fail through such proxies.\n\nIt is certainly true that our proposal does not provide hit-metering\nthrough non-caching proxies.  It may be possible to extend it in\nsuch a way that it does so, and I will think about whether this is\nfeasible.  In fact, it may be possible to do this without changing the\nspecification of the headers in our proposal, but rather only changing\nthe rules for interpreting them.\n\nHowever, we believe that, in order for origin servers to be willing\nto using hit-metering instead of cache-busting, that they need to\nhave very strong assurances that if a proxy appears to offer to\nhit-meter, then it will in fact do so.  We do not believe that a\nsystem based on Cache-control can do this, unless it is made mandatory\nfor all HTTP/1.1 implementations ... and we did not expect this to\nbe politically feasible (or even to be wise).\n\nSo we settled for a mechanism that \"fails conservatively\"; i.e., if\nthere is a non-metering proxy in the path, then the rest of the\ndistribution path reverts to using \"proxy-revalidate\" in those cases\nwhere the origin server wants to see a hit-count.\n\nWhen you say something is \"just not appropriate\", it might be useful\nto provide a specific example of what could go wrong.\n\n    In addition, the current design, if implemented, forces the proxy to add\n    \nConnection: Meter\nMeter: will-report-and-limit\n    \n    (or \"wont-report\" or \"wont-limit\") to every single request forwarded,\n    regardless of the likelihood that the origin server and the requested\n    resource happen to use hit-metering.\n\nYou apparently failed to read the part that says:\n\n   By definition, an empty Meter header:\n\n       Meter:\n\n   is equivalent to \"Meter: will-report-and-limit\", and so, by the\n   definition of the Connection header (see section 14.10 of the\n   HTTP/1.1 specification [1]), a request that contains\n\n       Connection: Meter\n\n   and no explicit Meter header is equivalent to a request that contains\n\n       Connection: Meter\n       Meter: will-report-and-limit\n\n   This makes the default case more efficient.\n\nIn other words, we do expect to see 14 extra bytes per connection\n(not per request!) as a consequence of this proposal.  We regard\nthis as a reasonable tradeoff for not insisting that Meter support\nbe mandatory in HTTP/1.1 (as I said, we didn't think this would be\na good idea), and a very reasonable tradeoff if it averts just\n1 request out of 20*N, where N is the mean number of requests\nper connection (since the average request seems to involve\naround 300 bytes of headers these days, and I'm not even counting\nthe averted bytes for 304 responses).\n\nAnd remember that if you don't want to use your inbound bandwidth\nfor this, you don't have to send it.  Period.\n\n    I do not believe that the proposal is valuable to the Internet\n    community.  In fact, I believe it will cause more harm than good if\n    implemented, and would strongly recommend not implementing it as it\n    currently stands.  On that basis, I oppose it going forward as a\n    Proposed Standard.\n\nThe only \"harm\" that you described in your message was the sending\nof the 14 extra bytes per connection, based on your (unsupported)\nestimate that most resources would not be hit-metered.  If this is\nthe only specific harm that you can point to, maybe we should be\nadopting Paul Leach's proposal for header abbreviations.\n\n-Jeff\n\n\n\n", "id": "lists-010-11864696"}, {"subject": "Re: hit metering  abbreviation", "content": "    > 5.2 Abbreviations for Meter directives\n    >   To allow for the most efficient possible encoding of Meter headers,\n    >   we define abbreviated forms of all Meter directives.  These are\n    \n    Didn't we just go through this with 1.1? I think consistency in the\n    protocol is an important consideration. Either it should be a human\n    readable protocol, or a binary machine optimized protocol..\n    defining synonyms for terms seems to add nothing but confusion.\n\nWhile I supported the proposal for abbreviations for HTTP headers,\nI can see some merit in the opposing argument: because of the need\nto interoperate with HTTP/1.0 (and earlier) systems, the use of\nheader-name abbreviations would have to be negotiated.\n\nWe do not, in this draft, propose abbreviating header names.  But\nsince we are in a situation where backwards compatibility is\nnot an issue, there is no need to include a negotiation mechanism.\nTherefore, the situation is not exactly analogous to the proposal\nfor abbreviating header names.\n\nHowever, if we did NOT introduce the abbreviation mechanism in\nthe initial version of the Meter header, then (if/when it has\nbeen deployed), if we later on decide that abbreviations are\ndesirable, we could not safely introduce them without a negotiation\nmechanism.\n\nSince it seems relatively easy to implement abbreviated Meter\ndirectives now, and it would be much harder to do it later on,\nand the bandwidth savings are clear (if perhaps small), I see\nno reason not to do it.\n\n-Jeff\n\n\n\n", "id": "lists-010-11875831"}, {"subject": "Content negotiation draft 04 submitte", "content": "We have just submitted version 04 of the transparent content\nnegotiation draft to the internet drafts editor.  I have made .txt,\n.html, and versions with changebars available at\n\n      <URL:http://gewis.win.tue.nl/~koen/conneg/>\n\nOur tentative planning is to submit this draft, or a revision thereof,\nto the to the IESG as a Proposed Standard after discussion at the\nDecember IETF.\n\nABSTRACT\n\n        HTTP allows one to put multiple versions of the same\n        information under a single URL.  Transparent content\n        negotiation is a mechanism, layered on top of HTTP, for\n        automatically selecting the best version when the URL is\n        accessed.  This enables the smooth deployment of new web data\n        formats and markup tags.\n\n        Design goals for transparent content negotiation include: low\n        overhead on the request message size, downwards compatibility,\n        extensibility, enabling the rapid introduction of new areas of\n        negotiation, scalability, low cost for minimal support, end\n        user control, and good cacheability.\n\nRevision History\n\n   Major functional changes with respect to version 03 are:\n\n    - The model for the caching of variant lists has been changed\n      extensively. (The previous version had a problem with downwards\n      compatibility.)\n\n    - The definition of the Negotiate header has been changed to\n      include a `trans' keyword.\n\n    - The form `ftag=<N-M>', with N-M a range of numeric values, was\n      added to the feature expressions and feature predicates.\n\n  Major changes in the presentation are:\n\n    - Feature tag registration is now covered in a separate draft\n\n    - Some new explanatory material was added\n\n    - Some existing text has been improved\n\nHappy reading!\n\nKoen.\n\n\n\n", "id": "lists-010-11884167"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Paul Leach:\n>\n>>From:  Roy T. Fielding[SMTP:fielding@kleber.ICS.UCI.EDU]\n[...]\n>>The other harm I mentioned is the implicit suggestion that\n>>\"hit-metering\" should be sanctioned by the IETF.  It should not.\n>>Hit metering is a way for people who don't understand statistical\n>>sampling to bog down all requests instead of just those few requests\n>>needed to get a representative sample.\n\nTo add a note: These are my thoughts on hit metering too (though I\ncould not have expressed them so eloquently as Roy did).  I feel that\nthe IETF should not sanction hit metering _unless_ it can be shown\nthat not doing so will lead to an internet meltdown.\n\nThose sites who don't understand statistics, and who want to double\ntheir income by having a mechanism that will double their directly\nmeasuable hits, will find a hit doubling mechanism whether it is\nsanctioned by the IETF or not.\n\nThose sites who want to report better statistics can be helped in much\ncheaper ways.\n\n[...]\n>Or are you implicitly proposing\n>        Cache-control: proxy-revalidate;stale-probability=.01\n>(where the new directive \"stale-probability=.01\" (spelled however you\n>like) means that the cache should make an entry be stale with\n>probablility .01 at each access;\n\nIt is interesting that you bring this up.  I have been playing with an\nidea like this for the past few days.  My idea is that, whenever a\nuser agent which supports `bogohits' makes a request to its internal\ncache, it must, with a 1 in 1000 probability, add the headers\n\n  Cache-control: no-cache\n  BogoHit: PQR\n\nto the request.  The Cache-Control header ensures that the request\nalways propagates to the origin server.  PQR is some characterization\nthe cause of the request (user clicked on link / loading of inlined\nobject / reload / request by web robot, etc ).  By counting the\nBogoHit headers and multiplying by 1000, the origin server gets an\nestimate of the actual hit count.  Well-known statistical formulae\ngive the accuracy of the obtained number.\n\nThis method has a very low overhead, adds no complexity in caches,\ngives only a minimal loss of privacy, and measures things at the\nactual source.  It thus provides a benchmark/certification for other\nstatistical methods.  Of course, you have to make a correction for\nuser agents which don't support the mechanism; you can use user-agent\nheader statistics for this.  If as a few major user agent vendors\nadopt the system, this correction won't add much uncertainty.\n\nSites who want to get payed by the hit could then in future say\nsomething like: \"We charge $0.01 for every hit on our server.  We\ncalculated 142 clicked links per 1000 hits on our server in the second\nquarter of 1998.\"\n\nKoen.\n\n\n\n", "id": "lists-010-11892710"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Jeffrey Mogul writes:\n> Well, I guess we didn't make this clear.  The new mechanism does\n> NOT create a duty.  What it does is to allow a proxy and server\n> (not necessarily an origin server!) to agree on a connection-by-\n> connection basis to enter into a \"contract\" of sorts.  \n\nI'm finding it difficult to approach this problem without falling into\nthe language of lawyers, and I suspect others are having similar\nproblems in distinguishing between the engineering aspects of this\nproposal and the aspects which deal with the \"'contract' of sorts\" you\nmention above.  I'll fight the impulse, but please forgive any lapses.\n\nI very much like the idea that your proposal describes a \"best effort\"\napproach, rather than a pure contract, and I appreciate the work\nyou've done to allow people to opt out of portions that they see as\nonerous.  Still, what you are doing is creating a technical method by\nwhich a server can say \"Obey me or I won't allow you to serve copies\nof my resources\".  Your assumption is that current servers which\ndesire (defendably high, frankly, rather than accurate) hit counts are\ncache-busting.  According to your scenario, they will switch to this\nmethod and some proxies will obey them, thus reducing network traffic\noverall.  It is possible, however, that the introduction of this\nmethod will induce some marginally interested servers which have not\npreviously engaged in cache-busting to engage in proxy-manipulation.\nWe won't know until its deployed, but we must acknowledge the\npossibility.  \n\nI also personally believe that this possibility represents a\nfundamental change in how the proxy servers must be viewed in the\ninteraction chain; we could debate this, but I would prefer a design\nthat did not make so fundamental a change.  To that end I actually\nprefer the \"usage limit\" aspect of the proposal to the reporting\naspect of the proposal.  From my point of view, it extends a current\nmechanism by creating a new way for documents to \"expire\".\nMetaphorically, it makes a cached web document like a new tire--the\nwarranty expires in 6 years or 60,000 miles.  We already have methods\nfor dealing with expiration and revalidation; we do not already have\nmethods for proxies to report data to origin servers.\n\nI recognize that the method is less intuitive than a reporting\nmechanism; every provider would need a way to handle the uncertainty\ninduced by the range between the first hit assigned to a proxy cache\nand the max-hits allowed it.  You make clear, however, that a server\nneed not give the same number of max-hits every time, and algorithms\nfor keeping that range small are availalbe.  Making sane\nrecommendations for how to do it could eliminate much of the\nconfusion.  \n\nUsing the max-hits method alone also avoids many of the potential\nprivacy issues which forcing the proxy to report may imply.\n\n> If someone is able to describe a specific scenario where the use\n> of the Meter mechanism, as proposed in our draft, does in fact provide\n> more per-client information than the existing HTTP/1.1 mechanisms,\n> then we would regard this as a bug in our specification that needs\n> to be fixed (or at least, that needs to be called out in the Security\n> Considerations section).\n\nThe use of the Vary header in a do-report situation clearly provides\nmore information than is currently the case where a proxy cache is\nbeing used.  Currently, if I employ a proxy-cache and it requests a\nresource on my behalf, the origin server gets the data on the proxy\ncache (the cache may report through some data on the origin requestor,\nbut it doesn't have to).  If the origin server cache-busts, the\nproxy-cache must re-request the data every time, but the origin server\ngets the data on the proxy-cache every time.  With your proposal, it\ncould get aggregates of the data on the actual requestors.  This\ncompromises privacy.  Imagine for a moment that someone used a Vary:\non the Host header with Meter.   \n\n\n> The \"stickiness\" of the Meter request-directive is only a performance\n> optimization, and if there are serious technical arguments against\n> it, we could remove that without affecting any other aspect of the\n> proposal.\n> \n> But I do not think it is accurate to think of this in the same way\n> that we have previously discussed \"sticky\" headers, since those\n> were for actual request-headers.  The Meter request header is a sort of\n> unusual thing that applies to transport-level connections, not to\n> individual requests, and so it might probably be better to use a\n> term other than \"sticky\" here.  (The Meter response directives are\n> per-response, but hop-by-hop, and so if there is a general \"sticky\"\n> mechanism agreed upon for the rest of HTTP, then it could take advantage\n> of this.)\n\nI'm not sure how good an optimization it is.  You mentioned above that\nan server would probably cache-bust now only on those resources for which\nit needs accurate counts (like an ad image).  By making this a per-connection\nheader, you seem to me to force a proxy to report and a server to receive\ninformation it may well throw away (like the counts on every little\nfancy bar or button image).\n\nWhatever it is called, I also suspect that we need a generic method\nfor dealing with this issue, rather than a one-off for this single\nheader.  If a server is currently designed to handle all aspects of\nthe negotation on a per-request basis, redesigning it to allow some\nper-connection headers is enough of a job that we should get it right\nbefore we ask for standardization.  If this goes to experimental,\nrather than standard, then I have no objection, as experimentation on\n\"per-connection\" headers is needed.  \n\nregards,\nTed Hardie\nNASA NIC\n\n\n\n", "id": "lists-010-11903627"}, {"subject": "Brain freeze in previous messag", "content": "I had a brief brain freeze here:\n> compromises privacy.  Imagine for a moment that someone used a Vary:\n> on the Host header with Meter.   \n\nOf course the host header would tell you nothing about the user.  \nImagine some other header instead.\nthanks,\nTed Hardie\nNASA NIC\n\n\n\n", "id": "lists-010-11917583"}, {"subject": "RE: Hitmetering: to Proposed Standard", "content": ">----------\n>From: Benjamin Franz[SMTP:snowhare@netimages.com]\n>Sent: Wednesday, November 20, 1996 5:22 AM\n>To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: Hit-metering: to Proposed Standard? \n>\n>\n>This also does not begin to address the questions of privacy and security\n>and their impact on the usage of hit-metering.. Many corporate proxies\n>would more than reluctant to be sending out information about their\n>internal usages to anyone who asked - they would be actively opposed to\n>it. \n\nIf I ran a corporate cache, I'd want to use hit-metering to whatever\norigin sites supported it. Today, those sites cost me one network round\ntrip for every GET done to fetch a \"cache-busted\" page from the sites,\nand I give away (via Referer, etc.) information with each request.  With\nhit-metering, origin sites that support it could cut the round trips by\nany factor they chose, while still getting hit-count information; and\nthe corporation would be giving away less information about the\nrequests.  I.e., win-win.\n>\n>In my view, the hit-metering proposal seems to request large amounts of\n>work for proxies at nearly no benefit - to anyone.\n\nBenefits to end users -- faster response time\nBenefits to cache owners -- less network bandwidth needed\nBenefit to origin sites -- customer gets page faster, is more pleased\nwith site\n\nPaul\n\n\n\n", "id": "lists-010-11925410"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "    My question is: why bother with usage-limiting ?  (and implementing\n    it in a caching mesh with multiple co-operating caches on each\n    level in a \"hierarchy\" will be a pain). The purpose of bounding\n    inaccuracy in a count is not achieved by adding usage-limiting, as\n    the primary inaccuracy will be added by ill-behaving caches, users'\n    inimitable inaccurate ways (check once per session, always check,\n    never check), servers/network failure and so on (including all\n    those that do hitmetering but do not honour the usage-limiting).\n    The only useful thing is that it does limit the number of times a\n    \"well-behaved\" cache server hands out the same ad. Is the\n    complexity of usage-limiting worth it?\n\nFirst of all, no proxy is required to implement usage-limiting.\n\nSecond, it is not clear that the additional implementation complexity\nis particular large.\n\nThird, we realized that we could do a good job of implementing\nusage-limiting with a mechanism that is very similar to the one\nwe chose for hit-metering, and since the usage-limiting aspect\nis entirely optional, we chose to include it in the specification.\n\nFinally, we agree that the usage-limiting aspect of the design\nis not as clearly useful as the hit-metering aspect.  But the\ndraft hints at a few other useful things that it could be used\nfor.  For example, it could be used with a (still-to-be-defined)\nprefetching mechanism so that a prefetched result could be cached\nfor one use.  The existing \"proxy-revalidate\" mechanism in HTTP/1.1\ndoesn't really work optimally for prefetched responses.\n\n    To illustrate the mesh problem: If I am a cache that gets handed 2\n    usages through my first parent and 4 through my second parent, do I\n    have to report back through the parent who gave me the usage-limit\n    or can I freely chose to report back through my third parent (who\n    has not connected to the origin server before and is likely to\n    report 4 out of zero and 2 out of zero or generally confuse the\n    origin server)? If I have to report through the appropriate parent,\n    this requires me to store where I got the document from; and it\n    heavily influences traffic patterns, robustness and the redundancy\n    of my mesh.\n    \nThe question is meaningless because usage-limiting and hit-metering\nare independent, and usage-limiting does not require you to report\nanything.\n\nIt is true that, for the case of hit-metering, if your proxy receives\notherwise identical responses from two different inbound servers, and\ncombines them according to the rules in section 13.2.6 of the HTTP/1.1\nspec, then one has to decide which inbound server gets the reports.\nBut it seems reasonable to assume that any subsequent cache hit can\nbe mapped onto a \"use\" of one or the other of the responses.  I.e.,\nwhen the cache combines the two responses, it can (arbitrarily) decide\nto act as if the second one replaces the first, or it can act as if\nthe second one is ignored.  Once that choice is made, there is no\nambiguity about which inbound server receives the report.\n\nAs I said in my response to Ted Hardie, our specification probably\nought to say explicitly that the proxy needs to record the identity\nof the immediate source of a response, and this is another example\nwhere that is important.\n\n-Jeff\n\n\n\n", "id": "lists-010-11935519"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Ingrid seems to have found a (fixable) bug in our proposal:\n\n    3.3 Negotiation of hit-metering and usage-limiting\n       An origin server that wants to collect hit counts for a resource, by\n       simply forcing all requests to bypass any proxy caches, would respond\n       to requests on the resource with \"Cache-control: proxy-revalidate\".\n    \n    In reading the HTTP/1.1 spec, I had the understanding that\n    must-revalidate and proxy-revalidate as defined in 14.9 only\n    requires a cache to revalidate if the response is stale, not every\n    time a request is made (unless there is an Authorization field).\n    Please correct me if I am wrong.\n    \n    > draft-ietf-http-v11-spec-07.txt:\n    > When the must-revalidate directive is present in a response received \n    > by a cache, that cache MUST NOT use the entry after it becomes stale \n    > to respond to a subsequent request without first revalidating it with \n    > the origin server.\n    > [..]\n    > The proxy-revalidate directive has the same meaning as the must- \n    > revalidate directive, except that it does not apply to non-shared \n    > user agent caches.\n\n    If the proxy handing out a \"Cache-control: proxy-revalidate\" also\n    is expected to modify staleness parameters like \"Cache-control:\n    max-age=0\" this should be stated. \n\nEven though I wrote the words in the HTTP/1.1 specification for\n\"must-revalidate\" and \"proxy-revalidate\", for some reason when I\nwas writing the hit-metering draft I forgot about this.  Yes, it\nseems necessary for a proxy that is forwarding a response outside\nof the metering subtree to generate\nCache-control: proxy-revalidate,max-age=0\nbecause this is exactly what the origin server would have had to\ndo to get the desired effect in the absence of the Meter mechanism.\n\nThis implies that if the received response said something like:\nCache-control: max-age=3600\nthen the forwarded response would require a change in max-age:\nCache-control: proxy-revalidate,max-age=0\n\n    Are caches allowed to change max-age ??\n\nI'm not sure that the HTTP/1.1 specification is explicit about\nthis.\n\nIt's a shame that we didn't include a Cache-control directive\nin HTTP/1.1 like \"proxy-max-age\" that only applies to proxies.\nOr something like \"proxy-always-revalidate\" (with a shorter name)\nthat means the equivalent of \"proxy-revalidate even if fresh\".\n\nNote that this is independent of the hit-metering issue: Ingrid\nhas pointed out a problem in HTTP/1.1 that would affect any origin\nserver wishing to see all of the unconditional GET requests but\nnot wishing to defeat end-client caching.\n\nIf we can solve that problem with HTTP/1.1, then the hit-metering\nproposal doesn't need to ask proxies to rewrite max-age.\n\n-Jeff\n\n\n\n", "id": "lists-010-11946254"}, {"subject": "RE: (INTEGOK) rough consensu", "content": ">----------\n>From: Roy T. Fielding[SMTP:fielding@avron.ICS.UCI.EDU]\n>Subject: Re: (INTEGOK) rough consensus \n>\n] I like all the suggested wording changes.\n>\n>>>      Note: the net result of the above is that the digest is\n>>       computed on the content that would be sent over-the-wire, in\n>>>      network byte order, but prior to any transfer coding being \n>>>      applied.\n>\n>Why not just say that and leave out the rest?  I think the note is far\n>more effective (and more likely to always be accurate) then the\n>paragraph\n>above it.\n\nHow about switching: the note will be the definitive text, the paragraph\nabove it the explanation/motivational note?\n\nI think the explanaority paragraph is useful to relate RFC 1864 to the\nHTTP context; if everyone else thinks its redundant, I can be persuaded\nto remove it.\n\nSend me private opinions on the utility -- no need to clutter the list:\na one liner with \"useful\" or \"not useful\" will do.\n\nPaul\n\n\n\n", "id": "lists-010-1195075"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "    Demanding hitcounts from me is demanding that I provide them with\n    further service for which I get nothing but more network traffic\n\nBut since the one and only goal of hit-metering is to allow more\ncaching, a cache would only offer to do hit-metering if this actually\nreduces network traffic (on the average).  We have no intention\nof promoting a design that increases network traffic on the average,\nand we fully expect proxy managers to disable it if the consequences\nare bad.  Some people will debate with us whether the frequency\nof hit-meterable resources is above or below the threshold at which\nhit-metering will pay off, but I see no argument against the existence\nof such a threshold.\n\n    and added complexity.\n\nCaching adds complexity, content-negotiation adds complexity, and\nsupport for persistent connections add complexity.  We all seem\nto be willing to add a lot of complexity to support features when\nthe payoff is high enough.\n\n-Jeff\n\n\n\n", "id": "lists-010-11956253"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "> However, we believe that, in order for origin servers to be willing\n> to using hit-metering instead of cache-busting, that they need to\n> have very strong assurances that if a proxy appears to offer to\n> hit-meter, then it will in fact do so.  We do not believe that a\n> system based on Cache-control can do this, unless it is made mandatory\n> for all HTTP/1.1 implementations ... and we did not expect this to\n> be politically feasible (or even to be wise).\n\nI haven't seen anything to indicate that the proposal is any stronger\nthan using a cache-control extension for proxy-revalidate, along with\nthe Expires now, max-age=N trick which is necessary for proxy-revalidate\nto work across HTTP/1.0 caches.  All the negotiating about what will or\nwill not be reported would then be unnecessary, proxies would not have\nto add the proxy-revalidate, and the only resources that are affected\nare those that would have otherwise been non-cachable.\n\n> You apparently failed to read the part that says:\n> \n>    By definition, an empty Meter header:\n> \n>        Meter:\n> \n>    is equivalent to \"Meter: will-report-and-limit\", and so, by the\n>    definition of the Connection header (see section 14.10 of the\n>    HTTP/1.1 specification [1]), a request that contains\n> \n>        Connection: Meter\n> \n>    and no explicit Meter header is equivalent to a request that contains\n> \n>        Connection: Meter\n>        Meter: will-report-and-limit\n> \n>    This makes the default case more efficient.\n\nYou are right, I didn't see that part.  I also did not notice that Meter was\nsupposed to be \"sticky\" throughout the connection, which is a new twist.\n\n>     I do not believe that the proposal is valuable to the Internet\n>     community.  In fact, I believe it will cause more harm than good if\n>     implemented, and would strongly recommend not implementing it as it\n>     currently stands.  On that basis, I oppose it going forward as a\n>     Proposed Standard.\n> \n> The only \"harm\" that you described in your message was the sending\n> of the 14 extra bytes per connection, based on your (unsupported)\n> estimate that most resources would not be hit-metered.  If this is\n> the only specific harm that you can point to, maybe we should be\n> adopting Paul Leach's proposal for header abbreviations.\n\nThe other harm I mentioned is the implicit suggestion that \"hit-metering\"\nshould be sanctioned by the IETF.  It should not.  Hit metering is a way for\npeople who don't understand statistical sampling to bog down all requests\ninstead of just those few requests needed to get a representative sample.\nWhether or not some ISP customers want it does not change the fact that\nit is damaging to the community as a whole, and it's a lot better to inform\npeople on how not to be a \"scum sucking pig\" than it is to have a proposed\nstandard on slightly-less piggish ways to be a pig.\n\n....Roy\n\n\n\n", "id": "lists-010-11964383"}, {"subject": "RE: Hitmetering: to Proposed Standard", "content": ">----------\n>From: Roy T. Fielding[SMTP:fielding@kleber.ICS.UCI.EDU]\n>Sent: Wednesday, November 20, 1996 4:35 PM\n>To: Jeffrey Mogul\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: Hit-metering: to Proposed Standard? \n>\n\n[snip...]\n\n>The other harm I mentioned is the implicit suggestion that \"hit-metering\"\n>should be sanctioned by the IETF.  It should not.  Hit metering is a way for\n>people who don't understand statistical sampling to bog down all requests\n>instead of just those few requests needed to get a representative sample.\n\nAre you saying there's a way right now with the HTTP to do statistical\nsampling?\n\nOr are you implicitly proposing\nCache-control: proxy-revalidate;stale-probability=.01\n(where the new directive \"stale-probability=.01\" (spelled however you\nlike) means that the cache should make an entry be stale with\nprobablility .01 at each access; this is combined with the usual meaning\n\"proxy-revalidate\" to pass a random sample of requests along to the\norigin server). If so, it's an intriguing idea.\n\n>Whether or not some ISP customers want it does not change the fact that\n>it is damaging to the community as a whole, and it's a lot better to inform\n>people on how not to be a \"scum sucking pig\" than it is to have a proposed\n>standard on slightly-less piggish ways to be a pig.\n\nNonesense. If \"people\" won't do the first and will do the second, then\ngetting them to do the second will mean that the community is better off\nthan if they just remain \"scum sucking pigs\". Damage to the community as\na whole only follows if proposing the second is the _cause_ of them not\ndoing the first.\n\nPaul\n\n>\n\n\n\n", "id": "lists-010-11974623"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "> Jeffrey Mogul:\n> Finally, we agree that the usage-limiting aspect of the design\n> is not as clearly useful as the hit-metering aspect.  But the\n> draft hints at a few other useful things that it could be used\n> for.  For example, it could be used with a (still-to-be-defined)\n> prefetching mechanism so that a prefetched result could be cached\n> for one use.  The existing \"proxy-revalidate\" mechanism in HTTP/1.1\n> doesn't really work optimally for prefetched responses.\n\n\"not as clearly useful\" is what has me worried. If it is not needed I do not \nwant it. \n\n>     To illustrate the mesh problem: If I am a cache that gets handed 2\n>     usages through my first parent and 4 through my second parent, do I\n>     have to report back through the parent who gave me the usage-limit\n>     or can I freely chose to report back through my third parent (who\n>     has not connected to the origin server before and is likely to\n>     report 4 out of zero and 2 out of zero or generally confuse the\n>     origin server)? If I have to report through the appropriate parent,\n>     this requires me to store where I got the document from; and it\n>     heavily influences traffic patterns, robustness and the redundancy\n>     of my mesh.\n>     \n> The question is meaningless because usage-limiting and hit-metering\n> are independent, and usage-limiting does not require you to report\n> anything.\n\nThe default use includes both usage-limiting and hit-metering, servers are \nlikely to try to take advantage of the combination. If usage-limiting and \nhit-metering are independent, why have an example that shows how to use \nusage-limiting in combination with hit-metering? If a origin server hands out \nusages only to caches that do hit-metering; and cache-busting documents to the \nrest (I read your draft as suggesting this is a neat way to do it), then the \ncombination does influence which parent I fetch documents from. The security \naspects of this are interesting, because it may force unaware caches to \nchannel their traffic through one branch of the mesh (and possibly take down \nthe top cache server as it gets overloaded). If the origin server keeps track \nof what usage-limiting happens at different cache servers (as your example \nsuggest), this is a bigger problem. \n\n> As I said in my response to Ted Hardie, our specification probably\n> ought to say explicitly that the proxy needs to record the identity\n> of the immediate source of a response, and this is another example\n> where that is important.\n\nHow will this influence load-sharing?\n\n\nIngrid\n-- \n  Ingrid.Melve@uninett.no            MIME, PGP and PEM email encouraged\n        UNINETT, Postboks 6883 Elgeseter, N-7002 Trondheim, Norway \n  Oj, der telte han meg. ?jojoj, er telte han meg en gang til! \n\n\n\n", "id": "lists-010-11985989"}, {"subject": "Re[2]: Hitmetering: to Proposed Standard", "content": "I confess I have not followed the arguments in detail, but a lot of the answers \nseem to be of the flavor \"no proxy is *required* to implement X\". But if many \nchoose not to, doesn't that defeat the whole purpose? You want something that \nmost people WILL want to implement, so it might be more useful to explain why \nthey WILL want to, why it is to their advantage to, rather than saying that \nthey don't have to. Please understand that I am not saying that what you have \nis something people won't want to implement, this is really a meta-comment on \nyour responses.\n\n\n______________________________ Reply Separator _________________________________\nSubject: Re: Hit-metering: to Proposed Standard? \nAuthor:  Jeffrey Mogul <mogul@pa.dec.com> at Internet\nDate:    11/20/96 4:08 PM\n\n\n    My question is: why bother with usage-limiting ?  (and implementing \n    it in a caching mesh with multiple co-operating caches on each \n    level in a \"hierarchy\" will be a pain). The purpose of bounding \n    inaccuracy in a count is not achieved by adding usage-limiting, as \n    the primary inaccuracy will be added by ill-behaving caches, users' \n    inimitable inaccurate ways (check once per session, always check, \n    never check), servers/network failure and so on (including all \n    those that do hitmetering but do not honour the usage-limiting). \n    The only useful thing is that it does limit the number of times a \n    \"well-behaved\" cache server hands out the same ad. Is the \n    complexity of usage-limiting worth it?\n\nFirst of all, no proxy is required to implement usage-limiting.\n\nSecond, it is not clear that the additional implementation complexity \nis particular large.\n\nThird, we realized that we could do a good job of implementing \nusage-limiting with a mechanism that is very similar to the one \nwe chose for hit-metering, and since the usage-limiting aspect\nis entirely optional, we chose to include it in the specification.\n\nFinally, we agree that the usage-limiting aspect of the design \nis not as clearly useful as the hit-metering aspect.  But the \ndraft hints at a few other useful things that it could be used \nfor.  For example, it could be used with a (still-to-be-defined)\nprefetching mechanism so that a prefetched result could be cached \nfor one use.  The existing \"proxy-revalidate\" mechanism in HTTP/1.1 \ndoesn't really work optimally for prefetched responses.\n\n    To illustrate the mesh problem: If I am a cache that gets handed 2 \n    usages through my first parent and 4 through my second parent, do I \n    have to report back through the parent who gave me the usage-limit \n    or can I freely chose to report back through my third parent (who \n    has not connected to the origin server before and is likely to \n    report 4 out of zero and 2 out of zero or generally confuse the \n    origin server)? If I have to report through the appropriate parent, \n    this requires me to store where I got the document from; and it \n    heavily influences traffic patterns, robustness and the redundancy \n    of my mesh.\n\nThe question is meaningless because usage-limiting and hit-metering \nare independent, and usage-limiting does not require you to report \nanything.\n\nIt is true that, for the case of hit-metering, if your proxy receives \notherwise identical responses from two different inbound servers, and \ncombines them according to the rules in section 13.2.6 of the HTTP/1.1 \nspec, then one has to decide which inbound server gets the reports. \nBut it seems reasonable to assume that any subsequent cache hit can\nbe mapped onto a \"use\" of one or the other of the responses.  I.e., \nwhen the cache combines the two responses, it can (arbitrarily) decide \nto act as if the second one replaces the first, or it can act as if the \nsecond one is ignored.  Once that choice is made, there is no ambiguity \nabout which inbound server receives the report.\n\nAs I said in my response to Ted Hardie, our specification probably \nought to say explicitly that the proxy needs to record the identity \nof the immediate source of a response, and this is another example \nwhere that is important.\n\n-Jeff\n\n\n\n", "id": "lists-010-11996726"}, {"subject": "Hit Metering  report of 0/", "content": "Folks,\n\nI've got a comment on draft-mogul-http-hit-metering-00.txt\n\n>   A proxy SHOULD NOT transmit \"Meter: count=0/0\", since this conveys no\n>   useful information.\n\nI believe that this draft should be silent on this issue, largely\nbecause whether a count=0/0 response is useful information or not is a\ncontext sensitive question.\n\nThe origin server is explicitly given the right the right to decide\nwhether or not to enter into a caching 'contract' with a proxy based\nupon any criteria it chooses. I can envision a situation where\nreporting of 0 hit counts would be a valuable criteria to base this\ndecision on.\n\nConsider a server that is very concerned about receiving accurate hit\ncounts. This proposal requires proxies that agree to return hit counts\nto return them with a 'best-effort' (which I think is the appropriate\nrequirement) but doesn't guarantee their return in the case of network\nor system failure. If the server in question has reason to believe\nthat the proxy's 'best-effort' isn't sufficient it may choose to\nanswer every request with a \"cache-control: proxy-revalidate\". I think\nthat one criteria for determining a proxy's success with 'best-effort'\nwould be that proxy's history of returning hit counts in the past that\nit was committed to returning to this server. (for example this\nheuristic could be used to judge network reliability between the two\nmachines).. a report of 0/0 when an unused entry is flushed from the\nproxy's cache still serves as a sense of closure and completion of\ncontract to the origin server and _could_ be useful in future decision\nmaking. Mind you I don't think that reporting 0/0 cases should be\nrequired, or even necessarily encouraged, but discouraging them limits\nspecific options that I think should be implementation issues.\n\n-Patrick\n\n--\nPatrick R. McManus - Applied Theory Communications -Software Engineering\nhttp://pat.appliedtheory.com/~mcmanusProgrammer Analyst\nmcmanus@AppliedTheory.com'Prince of Pollywood'Standards, today!\n*** - You Kill Nostalgia, Xenophobic Fears. It's Now or Neverland. - ***\n\n\n\n", "id": "lists-010-12008528"}, {"subject": "hit metering  abbreviation", "content": "Folks,\n\nre: draft-mogul-http-hit-metering-00.txt\n\nThis is very a minor issue.\n\n> 5.2 Abbreviations for Meter directives\n>   To allow for the most efficient possible encoding of Meter headers,\n>   we define abbreviated forms of all Meter directives.  These are\n\nDidn't we just go through this with 1.1? I think consistency in the\nprotocol is an important consideration. Either it should be a human\nreadable protocol, or a binary machine optimized protocol.. defining\nsynonyms for terms seems to add nothing but confusion.\n\nI think the abbreviations are un-necessary especially when you\nconsider the excellent job Jeff and Paul did at making sure that the\ndefault (empty) conditions of meter: are the common cases. Further\ncondensation seems like a fool-hardy optimization.\n\nWhile I'm willing to trade simplicity for some reasons, I don't find\nthe argument compelling enough here.\n\n-Patrick\n\n--\nPatrick R. McManus - Applied Theory Communications -Software Engineering\nhttp://pat.appliedtheory.com/~mcmanusProgrammer Analyst\nmcmanus@AppliedTheory.com'Prince of Pollywood'Standards, today!\n*** - You Kill Nostalgia, Xenophobic Fears. It's Now or Neverland. - ***\n\n\n\n", "id": "lists-010-12017732"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "    The default use includes both usage-limiting and hit-metering,\n    servers are likely to try to take advantage of the combination.\n\nYes, maybe.  After all, a server that wants to do some form of\nusage-limiting has exactly one option today: cache-busting.  In\nour proposal, it has the option to do usage-limiting without entirely\nbreaking caching (by setting the limit above zero) and without\neliminating the possibility of prefetching.  So I believe our\nproposal at least gives the option of compromise.\n\n    If usage-limiting and hit-metering are independent, why have an\n    example that shows how to use usage-limiting in combination with\n    hit-metering?\n\nPrecisely because they are independent, they can be combined.\nWe gave examples of various combinations of hit-metering and\nusage-limiting to show that they are, in fact, independent.\n\n    If a origin server hands out usages only to caches that do\n    hit-metering; and cache-busting documents to the rest (I read your\n    draft as suggesting this is a neat way to do it), then the\n    combination does influence which parent I fetch documents from. The\n    security aspects of this are interesting, because it may force\n    unaware caches to channel their traffic through one branch of the\n    mesh (and possibly take down the top cache server as it gets\n    overloaded). If the origin server keeps track of what\n    usage-limiting happens at different cache servers (as your example\n    suggest), this is a bigger problem.\n\nWe certainly cannot solve all of the problems arising from trying\nto simultaneously optimize security, bandwidth, latency, and ease\nof management.  Our proposal does not, however, force a cache to\nuse any particular path through the mesh (i.e., if it has multiple\npaths, we don't force GETs to follow any specific path, although\nwe would at least expect HEADs to return reports to the appropriate\nserver.)  Since we open up another possible dimension for optimization\n(i.e., one path allows metering, one does not) this makes the\noptimization problem harder, but the default solution is not any worse.\n    \n    > As I said in my response to Ted Hardie, our specification probably\n    > ought to say explicitly that the proxy needs to record the identity\n    > of the immediate source of a response, and this is another example\n    > where that is important.\n    \n    How will this influence load-sharing?\n\nIt should not.  It only influences who needs to see the hit-meter\nreports, and it would be entirely acceptable for the proxy to store\nmultiple source-identities if it is willing to do the bookkeeping\naccording to the rules we defined.  The implementation becomes somewhat\nmore complex, but this is the tradeoff for trying to optimize things.\n\n-Jeff\n\n\n\n", "id": "lists-010-12025989"}, {"subject": "Re: Hit Metering  report of 0/", "content": "    >   A proxy SHOULD NOT transmit \"Meter: count=0/0\", since this conveys\n    >   no useful information.\n    \n    I believe that this draft should be silent on this issue, largely\n    because whether a count=0/0 response is useful information or not is a\n    context sensitive question.\n    \n    The origin server is explicitly given the right the right to decide\n    whether or not to enter into a caching 'contract' with a proxy based\n    upon any criteria it chooses. I can envision a situation where\n    reporting of 0 hit counts would be a valuable criteria to base this\n    decision on.\n\nYou may be right that a case could be made that this information\nis sometimes useful.  But if so, then I think you need to suggest\na modification to the proposal that would allow the server to\nrequest the 0/0 counts *only* when they are needed.  And this would\nhave to include some careful language around how and when a proxy\nreceiving such a report needs to forward it to the next inbound\nserver.\n\nIn the absence of a specific server request for a 0/0 count,\nthe proxy would not know if the 0/0 report is worth sending.\nSince a significant fraction of the entries in typical proxy\ncaches are removed without ever being reused, if a proxy\nsimply defaulted to sending 0/0 reports in all cases, it\ncould end up sending more messages than if hit-metering were\nnot used at all.\n\nI think there are better metrics for measuring network reliability.\nFor example, with just a little help from the TCP stack, either\nend could count the number of retransmissions and/or timeouts.\nThis shouldn't require us to add extra message traffic to the\nnetwork, via otherwise unnecesary HTTP messages.\n\nAnyway, you might take some comfort in our use of the term\n\"SHOULD NOT\" rather than \"MUST NOT\" in this context :-)\n\n-Jeff\n\n\n\n", "id": "lists-010-12036113"}, {"subject": "Re: (INTEGOK) rough consensu", "content": ">        Content-MD5     = \"Content-MD5\" \":\" md5-digest\n>        md5-digest      = <base64 of 128 bit MD5 digest as per RFC 1864>\n\nNo, you really should remove the claim to be per RFC 1864 since the spec\nas given has no connection with RFC1864 except in using the same hash function.\n\nRFC 1864 in sofar as it says anything states that the entity is canonicalized\nbefore the digest is applied. I believe that we have (rightly) decided not to \nintroduce the canonicalization step. Therefore we are not doing the base64 of\nthe digest \"per RFC1864)\" we are doing the base64 of the digest.\n\n\n>> MIME, the digest is computed over the entire entity-body, even if it\n>\n>                   may be computed\n\nNo, Roy this is wrong the disgest IS computed. If the digest is there the \nvalue is MD5(entity) there is no provision to allow MD5(cononical(entity)).\n\n\nAs specified I think that we are being given a pig in a poke. If one gets a\nContent-MD5 header with this spec one will not know what the digest was\ncalculated on. Are implementations to be required to perform both checks on\nthe offchance that canonicalisation was used?\n\nIf people want to reuse an existing tag then they should accept whatever\nbaggage goes with it. If that involves irrelevant canonicalization steps\nthen so be it. If the spec diverges one should specify a new tag.\n\n\nPhill\n\n\n\n", "id": "lists-010-1204461"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": ">>The other harm I mentioned is the implicit suggestion that \"hit-metering\"\n>>should be sanctioned by the IETF.  It should not.  Hit metering is a way for\n>>people who don't understand statistical sampling to bog down all requests\n>>instead of just those few requests needed to get a representative sample.\n> \n> Are you saying there's a way right now with the HTTP to do statistical\n> sampling?\n\nYes.  Arrange for documents to expire at the beginning of some period,\ncache-bust for that period and collect data, and at the end of that\nperiod restore normal caching behavior.  Depending on how the period(s)\nis selected, actual audience size can be estimated by extrapolating\nfrom the data collected.\n\n.....Roy\n\n\n\n", "id": "lists-010-12045291"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "> Jeffrey Mogul:\n>     The default use includes both usage-limiting and hit-metering,\n>     servers are likely to try to take advantage of the combination.\n> \n> Yes, maybe.  After all, a server that wants to do some form of\n> usage-limiting has exactly one option today: cache-busting.  In\n> our proposal, it has the option to do usage-limiting without entirely\n> breaking caching (by setting the limit above zero) and without\n> eliminating the possibility of prefetching.  So I believe our\n> proposal at least gives the option of compromise.\n\nusage-limiting on its own is no problem, from my point of view. My worry is \nthe combination and how it is going to be used in the combination with \nhit-metering.\n\n>     If a origin server hands out usages only to caches that do\n>     hit-metering; and cache-busting documents to the rest (I read your\n>     draft as suggesting this is a neat way to do it), then the\n>     combination does influence which parent I fetch documents from. The\n>     security aspects of this are interesting, because it may force\n>     unaware caches to channel their traffic through one branch of the\n>     mesh (and possibly take down the top cache server as it gets\n>     overloaded). If the origin server keeps track of what\n>     usage-limiting happens at different cache servers (as your example\n>     suggest), this is a bigger problem.\n> \n> We certainly cannot solve all of the problems arising from trying\n> to simultaneously optimize security, bandwidth, latency, and ease\n> of management.  Our proposal does not, however, force a cache to\n> use any particular path through the mesh (i.e., if it has multiple\n> paths, we don't force GETs to follow any specific path, although\n> we would at least expect HEADs to return reports to the appropriate\n> server.)  Since we open up another possible dimension for optimization\n> (i.e., one path allows metering, one does not) this makes the\n> optimization problem harder, but the default solution is not any worse.\n\nHow about resources that are avilable from several sources (URNs)?\n\nYour proposal does not force a particular path through a mesh, but the \nimplementations of your proposal are likely to do that unless the issues \nconcerning these aspects are raised, discussed and not recommended in your \nproposal.\n\n>     > As I said in my response to Ted Hardie, our specification probably\n>     > ought to say explicitly that the proxy needs to record the identity\n>     > of the immediate source of a response, and this is another example\n>     > where that is important.\n>     \n>     How will this influence load-sharing?\n> \n> It should not.  It only influences who needs to see the hit-meter\n> reports, and it would be entirely acceptable for the proxy to store\n> multiple source-identities if it is willing to do the bookkeeping\n> according to the rules we defined.  The implementation becomes somewhat\n> more complex, but this is the tradeoff for trying to optimize things.\n\nThe proxy may have to store where I did get a document, and the origin server \nmay have to (or want to) store who it gave a document. As a cachemanager I \nwill (probably) have to handle flow-information that I do not have to care \nabout today. I would like to count and send count to the server, without \ncaring about flow. Combinations of hit-metering and usage-limiting may force \nme to store flow-information.\n\n\nIngrid\n-- \n  Ingrid.Melve@uninett.no            MIME, PGP and PEM email encouraged\n        UNINETT, Postboks 6883 Elgeseter, N-7002 Trondheim, Norway \n  Oj, der telte han meg. ?jojoj, er telte han meg en gang til! \n\n\n\n", "id": "lists-010-12053291"}, {"subject": "Re: Hit Metering  report of 0/", "content": "In a previous episode...Jeffrey Mogul said:\n-> \n->     >   A proxy SHOULD NOT transmit \"Meter: count=0/0\", since this conveys\n->     >   no useful information.\n\n->     The origin server is explicitly given the right the right to decide\n->     whether or not to enter into a caching 'contract' with a proxy based\n->     upon any criteria it chooses. I can envision a situation where\n->     reporting of 0 hit counts would be a valuable criteria to base this\n->     decision on.\n-> \n-> You may be right that a case could be made that this information\n-> is sometimes useful.  But if so, then I think you need to suggest\n-> a modification to the proposal that would allow the server to\n-> request the 0/0 counts *only* when they are needed.  And this would\n-> have to include some careful language around how and when a proxy\n-> receiving such a report needs to forward it to the next inbound\n-> server.\n-> \n-> In the absence of a specific server request for a 0/0 count,\n-> the proxy would not know if the 0/0 report is worth sending.\n\nThat's absolutely true, and I think you can take that conclusion one\nstep further. \n\nIn the absence of a specific server request for any minimum return\ncount, the proxy cannot know if the report is worth sending. Some servers\nmay not believe that a return connection is worth the overhead of\nreceiving a 1/0 report, just as some servers may insist on seeing a\n0/0 report to know that the proxy's 'best-effort' is succeeding.\n\nperhaps the initial negotiation could contain a count-range meter\nheader from the proxy indicating the bounds (both hi and low) it is\nwilling to report for, and the server response could return min and\nmax values in that range that represents the reporting interval.\n(i.e. don't report anything less than 2 hits, but never go more than\n15 without sending a report)\n\n This nicely separates the functionality of max-uses from the\nreporting functions in any case. I don't see any reason to believe\nthat their values are going to be derived from the same criteria, just\nbecause you want more frequent reports from a cache that has held your\npage for 2 weeks than every 2 weeks, doesn't mean the resource is now\ninvalid. \n\nWhat do people think of this?\n\n-> I think there are better metrics for measuring network reliability.\n-> For example, with just a little help from the TCP stack, either\n-> end could count the number of retransmissions and/or timeouts.\n\nagreed, but my point is that the server's decision to enter into the\nreporting 'contract' and allow caching is a decision that is opaque to\nyour proposal. There shouldn't be language that excludes some\ntechniques because it isn't relevant or important, in a black-boxed\ndecision we can't know what will be relevant.\n\n-Patrick\n\n--\nPatrick R. McManus - Applied Theory Communications -Software Engineering\nhttp://pat.appliedtheory.com/~mcmanusProgrammer Analyst\nmcmanus@AppliedTheory.com'Prince of Pollywood'Standards, today!\n*** - You Kill Nostalgia, Xenophobic Fears. It's Now or Neverland. - ***\n\n\n\n", "id": "lists-010-12064885"}, {"subject": "I-D ACTION:draft-holtman-http-negotiation04.tx", "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Transparent Content Negotiation in HTTP                 \n       Author(s) : K. Holtman, A. Mutz\n       Filename  : draft-holtman-http-negotiation-04.txt\n       Pages     : 45\n       Date      : 11/21/1996\n\nHTTP allows one to put multiple versions of the same information under a \nsingle URL.  Transparent content negotiation is a mechanism, layered on top\nof HTTP, for automatically selecting the best version when the URL is \naccessed.  This enables the smooth deployment of new web data formats and \nmarkup tags.                    \n                                          \nDesign goals for transparent content negotiation include: low overhead on \nthe request message size, downwards compatibility, extensibility, enabling \nthe rapid introduction of new areas of negotiation, scalability, low cost \nfor minimal support, end user control, and good cacheability.              \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-holtman-http-negotiation-04.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-holtman-http-negotiation-04.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  nic.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-holtman-http-negotiation-04.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-12075760"}, {"subject": "Copy Control (draft-daviel-web-copy-control00.txt", "content": "Re. ftp://ietf.org/internet-drafts/draft-daviel-web-copy-control-00.txt\n\nThis describes a simple HTTP header syntax (or HTML META tag) to\ndefine the copyright status of a document in a machine-readable way,\nand provide a link to a reference document which would define\nthe status explicitly.\n\nThe purpose of this proposed header is twofold.\n1. To make it easier to design agents that may create compilations\nof existing work automatically, or to search for only copyright-free\nmaterial.\n2. To make it clear to consumers (via a popup, for instance)\nthat certain material is copyright when they perform actions such\nas printing or saving a page.\n\nThere is not intention to provide a mechanism for document signatures,\nmodification control, etc.\n\nThis work was in part prompted by a pamphlet on copyright\nissues for schools which included a note \"Don't print anything from the \nInternet\".\n\nBriefly, one includes a header or META tag such as\nCopy-Control: v 1.0 p 1 s 1 q 2 u http://some.org/copy.html\nor\n<META HTTP-EQUIV=\"Copy-Control\" CONTENT=\"v 1.0 p 2 s 2 q 2\">\nwhere the values for p(rint), s(ave), q(uote) are given\n0 = forbidden, 1 = conditional, 2 = allowed.\n\n\"quote\" means to copy a portion of a document, is meant to reflect \nthe common usage in printed material (for purposes of review, etc.),\nand might be checked by a clipboard function.\nThe URL (required for level = 1 \"conditional\") fully defines the\nconditions, copyright holder, etc. (in human-readable form).\n\nThis does not address saving of documents by search engines (addressed\nby the \"ROBOTS\" META tag and /robots.txt file), or saving\nof documents in cache (addressed by the \"Expires\" header and HTTP 1.1\nextensions). It does not enforce copyright control, since it is easily\ncircumvented. It merely provides notification. The syntax is a \ncompromise between a rigorous definition useful for agents,\nand a form that is simple enough to write by hand.\n\nPossible revision (thought of since the original submission):\ndefine a numeric limit below which permission need not be sought, viz.\n\nCopy-Control: v 1.0 p(25) 2 s(1) 2 q 2\n\nwhich states that 25 copies of a document may be printed without permission.\nThis feature would be used by search agents, for example a teacher\nlooking for an article about Mars rovers which could be printed out\nfor a class.\n\nWhy can't the existing &copy; tag be used ? Obviously, this header\ndoesn't replace that, since the header or META tag is invisible to\nthe user. Firstly, the usual copyright notices are small and at the\nend of a document (whereas the notices on videotape are usually large\nand at the beginning ..), and secondly, since there is no defined\nformat it is difficult for agents to interpret the tag (which might\nbe in the body of text and referring to some other document, for\ninstance).\n \n\nAndrew Daviel\n\nmailto:andrew@vancouver-webpages.com \nhttp://vancouver-webpages.com  : home of multisearch, META generator etc.\n\n\n\n", "id": "lists-010-12085770"}, {"subject": "Re: Copy Control (draft-daviel-web-copy-control00.txt", "content": "  >This describes a simple HTTP header syntax (or HTML META tag) to\n  >define the copyright status of a document in a machine-readable way,\n  >and provide a link to a reference document which would define\n  >the status explicitly.\n\nThis is an interesting approach. I've been thinking about IPR protection\nfor a couple of years now. Previously it was with respect to SMPTE headers,\nwhich caused big problems because of the question of what fields and\ninformation to include (proposals for specifying usage rights can become\nquite unwieldy). This was a problem, because we did not have convenient\nability to point elsewhere (as we do on the Web), hence what we packed into\nthe header could become quite a religious issue. However, on the Web we are\na bit more fortunate, _and_ the PICS standard that you can find off of\n(http://www.w3.org) seems to be the perfect method for specifying the type\nof information you propose.\n  \n  >The purpose of this proposed header is twofold.\n  >1. To make it easier to design agents that may create compilations\n  >of existing work automatically, or to search for only copyright-free\n  >material.\n  \nPICS has this goal directly in mind.\n\n>2. To make it clear to consumers (via a popup, for instance)\n  >that certain material is copyright when they perform actions such\n  >as printing or saving a page.\n\nThis type of functionality is already included with respect to\n\"appropriateness\" in IE.  \n\n\n  >Briefly, one includes a header or META tag such as\n  >Copy-Control: v 1.0 p 1 s 1 q 2 u http://some.org/copy.html\n  >or\n  ><META HTTP-EQUIV=\"Copy-Control\" CONTENT=\"v 1.0 p 2 s 2 q 2\">\n  >where the values for p(rint), s(ave), q(uote) are given\n  >0 = forbidden, 1 = conditional, 2 = allowed.\n\nTo map something similar to PICS, consider the case in which Mark Twain\nwrote the material at http://www.twain.com/story.html . He also has a page\ndescribing the copyright statement in fuller detail at\nhttp://www.twain.com/IP-notice.html . He is going to use the system you\nspecify with respect to \"Print\", \"Save\", and \"Quote\", where the specified\nvalues represent 0 - disallowed, 1 - conditionally allowed, 2 -\nunconditionally allowed:\n\n(PICS-1.1 \"http://www.wipo.org/v1.5\"\n  by \"Mark Twain\"\n  labels on \"1994.11.05T08:15-0500\"\n         for \"http://www.twain.com/story.html\"\n         full \"http://www.twain.com/IP-notice.html\"\n         ratings (print 1 save 1 quote 2))\n\nSome of the benefits of using PICS:\n\n1. Multiple distribution methods (embedded within the document, transported\nby the server, or from a label bureaus). This improves the management of\nthe copyright information. (i.e. Organizations can control the use and\naccess to their IPR from their server. Organizations can create \"audit\"\nspiders.)\n2. Generic labeling (implicitly rates every URL for which the specified URL\nis the prefix of) This improves the management of the copyright\ninformation, since I don't have to label every document in a directory if\nthey have a similar status.\n3. Capabilities for digital signatures are provided.\n4. It is being widely leverage in Web development.\n\n\n\n___________________________________________________________\nJoseph Reagle Jr.  Personal: http://rpcp.mit.edu/~reagle/home.html\nPolicy Analyst     World Wide Web Consortium:    http://www.w3.org\n\n\n\n", "id": "lists-010-12096714"}, {"subject": "Re: Copy Control (draft-daviel-web-copy-control00.txt", "content": "Joseph M. Reagle Jr. wrote:\n\n> 2. Generic labeling (implicitly rates every URL for which the specified URL\n> is the prefix of) This improves the management of the copyright\n> information, since I don't have to label every document in a directory if\n> they have a similar status.\n\nThe same thing as with robots exclusion protocol. Regular expressions are\nneeded, URL prefixes are not enough.\n\n-- \nLife is a sexually transmitted disease.\n\ndave@fly.cc.fer.hr\ndave@zemris.fer.hr\n\n\n\n", "id": "lists-010-12108409"}, {"subject": "I-D ACTION:draft-ietf-http-state-mgmt05.txt, .p", "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\nNote: This revision reflects comments received during the last call period.\n\n       Title     : HTTP State Management Mechanism                         \n       Author(s) : D. Kristol, L. Montulli\n       Filename  : draft-ietf-http-state-mgmt-05.txt, .ps\n       Pages     : 19\n       Date      : 11/22/1996\n\nThis document specifies a way to create a stateful session with HTTP \nrequests and responses.  It describes two new headers, Cookie and \nSet-Cookie, which carry state information between participating origin \nservers and user agents.  The method described here differs from Netscape's\nCookie proposal, but it can interoperate with HTTP/1.0 user agents that use\nNetscape's method.  (See the HISTORICAL section.)                          \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-state-mgmt-05.txt\".\n Or \n     \"get draft-ietf-http-state-mgmt-05.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-mgmt-05.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  nic.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-state-mgmt-05.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-state-mgmt-05.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-12116968"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "    > We certainly cannot solve all of the problems arising from trying\n    > to simultaneously optimize security, bandwidth, latency, and ease\n    > of management.  Our proposal does not, however, force a cache to\n    > use any particular path through the mesh (i.e., if it has multiple\n    > paths, we don't force GETs to follow any specific path, although\n    > we would at least expect HEADs to return reports to the appropriate\n    > server.)  Since we open up another possible dimension for optimization\n    > (i.e., one path allows metering, one does not) this makes the\n    > optimization problem harder, but the default solution is not any worse.\n\n    How about resources that are avilable from several sources (URNs)?\n\n    Your proposal does not force a particular path through a mesh, but\n    the implementations of your proposal are likely to do that unless\n    the issues concerning these aspects are raised, discussed and not\n    recommended in your proposal.\n\nIt probably would help to think of our proposed design as counting\n\"uses\" of \"responses\", not as counting \"uses\" of \"resources\".  When\none takes this perspectice, the fact that a resource may have several\nnames is largely irrelevant, as long as the mapping from the name\n(URL) given in an HTTP request to a *cachable* response is stable.\n(If it were not stable, then I would not expect the response to be\ncachable in the first place!)\n    \n    >     > As I said in my response to Ted Hardie, our specification probably\n    >     > ought to say explicitly that the proxy needs to record the identity\n    >     > of the immediate source of a response, and this is another example\n    >     > where that is important.\n    >     \n    >     How will this influence load-sharing?\n    > \n    > It should not.  It only influences who needs to see the hit-meter\n    > reports, and it would be entirely acceptable for the proxy to store\n    > multiple source-identities if it is willing to do the bookkeeping\n    > according to the rules we defined.  The implementation becomes somewhat\n    > more complex, but this is the tradeoff for trying to optimize things.\n\n    The proxy may have to store where I did get a document, and the\n    origin server may have to (or want to) store who it gave a\n    document.\n\nNo, this is not required by our proposal.  The origin server may\nwant to do this if it wants to play complex games with the usage-limiting\nmechanism, but for hit-metering, I see no reason for the origin\nserver to remember which proxies have a copy of a response.  (Some\npeople are working on cache-invalidation schemes that would require\nsome sort of server-side database, but this is a separate issue entirely.)\n\n    As a cachemanager I will (probably) have to handle\n    flow-information that I do not have to care about today. I would\n    like to count and send count to the server, without caring about\n    flow. Combinations of hit-metering and usage-limiting may force me\n    to store flow-information.\n\nI'm not sure what you mean by \"flow-information.\"  Can you give a\nspecific example?\n\n-Jeff\n\n\n\n", "id": "lists-010-12126980"}, {"subject": "Re: UPGRADE: Wordin", "content": "> I think that Roy's wording is a good start, but it has two important\n> limitations that should be resolved:\n> \n> 1) It allows for switching protocols on the _same_ connection only. I\n> think it is important that the header allows for upgrade on a\n> different connection in order to provide sufficient support for having\n> HTTP being a control connection for real time protocols, for example.\n\nI don't.  If such a thing is desirable then it can already be implemented\nvia a redirect to a different URL.  However, I don't know of any real\ntime (or other) protocol for which it makes sense to use HTTP as the\ncontrol.  In any case, this is not a problem which Upgrade was intended\nto solve -- it only handles the case of replacement of the current\napplication protocol with something else on the same connection.\nFurthermore, the new protocol (post-Upgrade) may indeed be capable\nof spawning new connections (or multiplexing within the existing\nconnection).\n\n> 2) The naming of the protocol may not be sufficiently detailed as it\n> does not allow for composite names and versions built together in a\n> protocol stack.\n\nIt provides for a list of protocol names that are accepted by the client\nfor the purpose of an upgraded connection.  Composite names and protocol\nstacks do not have an affect on the protocol name (unless the name\nitself is a composite, which is already supported).  There are other\nthings that will be supported by PEP, but even PEP will need Upgrade\nin order to change from HTTP/1.x to HTTP/2.x.\n\n> 3) The client can not require that a protocol specified in the upgrade\n> header is used by the server.\n\nRight, it doesn't -- that is not the purpose of Upgrade and is the purpose\nof PEP.  At the same time, PEP cannot say \"change the current connection\nto one using HTTP-NG\" because PEP exists within HTTP.\n\n> There are a number of possible solutions to this. First, we could use\n> the notion from Simon Spero's original draft on HTTP-NG that also\n> supports spawning off other connections or we could use the more\n> traditional FTP solution for PORT and PASV. Second we could use a\n> protocol naming scheme as used by ILU (32 bit hash) or as suggested by\n> Rohit Khare in his PEP proposal.\n\nA name is a name -- what you make of it is what you want to make of it.\nThere is no functional difference between a 32 bit hash and an\nunrestrained token.  PEP is far too complex for an optional\napplication-layer protocol switch; that complexity serves its purpose,\nbut PEP's purpose is far more exuberant than that of Upgrade.\n\n> 10.41 Upgrade\n> \n> The Upgrade general-header allows the client to specify what\n> additional communication protocols it supports and would like to use\n> if the server finds it appropriate to switch protocols. The server\n> must use the Upgrade header field within a 101 (switching protocols)\n> response to indicate which protocol(s) are being switched.\n> \n>        Upgrade        = \"Upgrade\" \":\" 1#product\n> \n> For example, \n> \n>        Upgrade: HTTP/2.0, SHTTP/1.3, IRC/6.9, RTA/x11\n> \n> | The purpose of the Upgrade header is to allow easier migration across \n> | protocols in order to better match the application needs with \n> | protocol capabilities. The client can not demand that a protocol \n> | specified in the upgrade header is used by the server. However, the \n> | indication given by the upgrade header field should be followed.\n> |\n> | The upgrade header does not allow for switching protocols on a \n> | different connection than the one in use. It also does not provide \n> | any means for switching back to the original protocol used to \n> | transmit the upgrade header in the first place.\n> |\n> | This specification does not define any protocol names other than \n> | \"HTTP\" but others can be used.\n\nWell, no, that's not quite right.  I understand what you are trying to\nsay, but it is not appropriate to define a header field in terms of\nwhat it doesn't do, and the comment about switching back is wrong\n(that depends on what you change to).  How about the following instead\nof the last three paragraphs above:\n\n   The Upgrade header field is intended to provide a simple mechanism\n   for transition from HTTP/1.1 to some other, incompatible protocol.\n   It does so by allowing the client to advertize its desire to use \n   another protocol, such as a later version of HTTP with a higher major\n   version number, even though the current request has been made using\n   HTTP/1.1.  This eases the difficult transition between incompatible\n   protocols by allowing the client to initiate a request in the more\n   commonly supported protocol while indicating to the server that it\n   would like to use a \"better\" protocol if available (where \"better\"\n   is determined by the server, possibly according to the nature of the\n   method and/or resource being requested).\n \n   The Upgrade header field only applies to switching application-layer\n   protocols upon the existing transport-layer connection.  Upgrade\n   cannot be used to insist on a protocol change; its acceptance and\n   use by the server is optional.  The capabilities and nature of the\n   application-layer communication after the protocol change is entirely\n   dependent upon the new protocol chosen, although the first action\n   after changing the protocol must be a response to the initial HTTP\n   request containing the Upgrade header field.\n\n   The Upgrade header field only applies to the immediate connection.\n   Therefore, the \"upgrade\" keyword must be supplied within a Connection\n   header field (Section 10.9) whenever Upgrade is present in an\n   HTTP/1.1 message.\n\n   The Upgrade header field cannot be used to indicate a switch to a\n   a protocol on a different connection.  For that purpose, it is more\n   appropriate to use a 301, 302, 303, or 305 redirection response.\n\n   This specification only defines the protocol name \"HTTP\" for use by\n   the family of Hypertext Transfer Protocols, as defined by the HTTP\n   version rules of Section 3.1 and future updates to this specification.\n   Any token can be used as a protocol name; however, it will only be\n   useful if both the client and server associate the name with the\n   same protocol.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1212968"}, {"subject": "Re: Hit Metering  report of 0/", "content": "    In the absence of a specific server request for any minimum return\n    count, the proxy cannot know if the report is worth sending. Some\n    servers may not believe that a return connection is worth the\n    overhead of receiving a 1/0 report, just as some servers may insist\n    on seeing a 0/0 report to know that the proxy's 'best-effort' is\n    succeeding.\n\n    perhaps the initial negotiation could contain a count-range meter\n    header from the proxy indicating the bounds (both hi and low) it is\n    willing to report for, and the server response could return min and\n    max values in that range that represents the reporting interval.\n    (i.e. don't report anything less than 2 hits, but never go more\n    than 15 without sending a report)\n\nIt would not complicate things too much to add a Meter response-directive\nalong the lines of\nMeter: want-report= MIN/MAX\nas long as the default was to NOT send this, and that the default\nMIN is equal to 1 and the default MAX is equal to infinity.\n\nI'm really reluctant to add something from the proxy to the server\nto indicate the proxy's \"willingness to report\" limits, since it's\nhard to imagine that a proxy cache with non-infinite disk space\ncould actually guarantee a minimum.  I.e., think of a proxy cache\nwith room for at most two items.  If it has promised not to report\na hit-count for each of these until the count reaches 10, and each\nonly has 2 hits, and it decides it wants to evict one so that it\ncan store a more useful cache entry instead, what does it do?\n\nAnd I don't think it makes sense for the proxy to offer a maximum\nvalue, for similar reasons of enforceability.\n\n-Jeff\n\n\n\n", "id": "lists-010-12137631"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "In the case of URNs that resolve to multiple URLs, it seems the answer\ndepends upon the purpose of hit metering. If it is a server management\ntool, then the most important information is the number of hits to a given\nresource (meaning something identified by a URL). If the point i to measure\nthe usage level of a URN-resource (Surely, there must be some established\nterminology here!) then it seems appropriate to meter the accesses to the\nURN. This is problematic, though. Under the NAPTR proposal (which, of\ncourse, is only one possible approach), then the question arises of how to\nhandle N2L or N2Ls responses, where each URN-request results in a URL\nrequest (terminology?), but with N2R responses, the actual resource could\npotentially be served by any of a number of protocols.\n\nIt seems to me that hit metering (restricted to HTTP) and resource usage\n(where a resourcer is more broadly defined as something identified by a\nURI which needn't necessarily be an http: URL) are different problems. \n\n---\nGregory Woodhouse     gjw@wnetc.com\nhome page:            http://www.wnetc.com/home.html\nresource page:        http://www.wnetc.com/resource/\n\n\n\n", "id": "lists-010-12146648"}, {"subject": "Calculating Age Questio", "content": "I don't mean to bring this up for discusion, rather clarification. \nIn going through the mailing list archives and over draft 07, I  \nwanted to be sure I understood the calculations of the Age value \ncorrectly. \n \nGiven the request/response chain: \n \nUA ----> A ----> B ----> C ----> OS \n \nAnd time values as indicated: \n \nUA request_time  = 0 \nA  request_time  = 1 \nB  request_time  = 2 \nC  request_time  = 3 \nOS request_time  = 4  (date_value) \nC  response_time = 5 \nB  response_time = 6 \nA  response_time = 7 \nUA response_time = 8 \n \nAge at C: \n  apparent_age = max( 0 , 5 - 4 ) = 1 \n  corrected_received_age = max( 1 , - ) = 1 \n  response_delay = 5 - 3 = 2 \n  corrected_initial_age = 1 + 2 = 3 \n  resident_time = 0 \n  current_age = 3 + 0 = 3 (age_value) \n \nAge at B: \n  apparent_age = max( 0 , 6 - 4 ) = 2 \n  corrected_received_age = max( 2 , 3 ) = 3 \n  response_delay = 6 - 2 = 4 \n  corrected_initial_age = 3 + 4 = 7 \n  resident_time = 0 \n  current_age = 7 + 0 = 7 (age_value) \n \nAge at A: \n  apparent_age = max( 0 , 7 - 4 ) = 3 \n  corrected_received_age = max( 3 , 7 ) = 7 \n  response_delay = 7 - 1 = 6 \n  corrected_initial_age = 7 + 6 = 13 \n  resident_time = 0 \n  current_age = 13 + 0 = 13 (age_value) \n \nSo when the User Agent receives the response, the Age value \nshould be 13? Basically, at each cache, the Age value will \nbe increased by the response_delay of that cache (plus \nresident_time if applicable). \n \nAnd if the User Agent calculated the Age value: \n \nAge at UA: \n  apparent_age = max( 0 , 8 - 4 ) = 4 \n  corrected_received_age = max( 4 , 13 ) = 13 \n  response_delay = 8 - 0 = 8 \n  corrected_initial_age = 13 + 8 = 21 \n  resident_time = 0 \n  current_age = 21 + 0 = 21 (age_value) \n \nI know this is suppose to be an especially conservative \nalgorithm so I wanted to make sure I'm interpreting the \nspec correctly. \n \nthanks, \n \n \nPaul \n \n\n\nPaul Hethmon \nphethmon@utk.edu \nphethmon@hethmon.com \n---------------------------------------------------------- \nInet.Mail for OS/2 -- Internet Mail Server \n---------------------------------------------------------- \nwww.hethmon.com -- ftp.hethmon.com \n----------------------------------------------------------\n\n\n\n", "id": "lists-010-12156015"}, {"subject": "DRAFT Agenda for HTTPW", "content": "Here is a rough draft of an agenda for the HTTP WG at the San Jose\nIETF.  The Internet drafts to be discussed are noted in each section.\n  \nPLEASE NOTE THAT THIS IS NOT A FINAL AGENDA.\n\nNewer versions will be posted to\n   ftp://ftp.parc.xerox.com/pub/masinter/http-wg-agenda.html\nafter 11/26.\n\nMonday, December 9, 9:30-11:30\n\n30 minDiscussion of HTTP/1.1 Implementation Experiences\nDevelopment of HTTP/1.1 issues list\nReport from people trying to implement HTTP/1.1 to talk about their\nexperiences, difficulties with the spec, questions.\n\n60 min  Content negotiation\ndraft-holtman-http-negotiation\ndraft-mutz-http-attributes\ndraft-ietf-http-feature-reg\n\n30 minSafe POST / GET-with-body\ndraft-holtman-http-safe\n\n------  \nTuesday, December 10, 9-11:00  \n \n30 minLogging & proxy\ndraft-mogul-http-hit-metering\n\n30 minPEP\ndraft-ietf-http-pep\n\n30 min  Other work:\n        report on WEBDAV, IPP and HTTP extensions\n        proposals from other groups.\n  \n  draft-aboba-rtp-http\n        draft-petke-http-auth-scheme\n\n\n\n", "id": "lists-010-12165579"}, {"subject": "Re: Calculating Age Questio", "content": "At 08:46 PM 11/25/96 EST, Paul Hethmon wrote:\n>I don't mean to bring this up for discusion, rather clarification. \n>In going through the mailing list archives and over draft 07, I  \n>wanted to be sure I understood the calculations of the Age value \n>correctly.\n\nI have included how I implement it libwww 5.0a which is available from\n\nhttp://www.w3.org/pub/WWW/Library/\n\nThe function is part of the HTCache.c module.\n\nHope this helps,\n\nHenrik\n\n-----------------------------------------------------------------\n\n/*\n**  Calculate the corrected_initial_age of the object. We use the time\n**  when this function is called as the response_time as this is when\n**  we have received the complete response. This may cause a delay if\n**  the reponse header is very big but should not cause any incorrect\n**  behavior.\n*/\nPRIVATE BOOL calculate_time (HTCache * me, HTRequest * request,\n                             HTResponse * response)\n{\n    if (me && request) {\n        HTParentAnchor * anchor = HTRequest_anchor(request);\n        time_t date = HTAnchor_date(anchor);\n        me->response_time = time(NULL);\n        me->expires = HTAnchor_expires(anchor);\n        {\n            time_t apparent_age = HTMAX(0, me->response_time - date);\n            time_t corrected_received_age = HTMAX(apparent_age,\nHTAnchor_age(anchor));\n            time_t response_delay = me->response_time -\nHTRequest_date(request);\n            me->corrected_initial_age = corrected_received_age +\nresponse_delay;\n        }\n\n        /*\n        **  Estimate an expires time using the max-age and expires time. If we\n        **  don't have an explicit expires time then set it to 10% of the LM\n        **  date. If no LM date is available then use 24 hours.\n        */\n        {\n            time_t freshness_lifetime = HTResponse_maxAge(response);\n            if (freshness_lifetime < 0) {\n                if (me->expires < 0) {\n                    time_t lm = HTAnchor_lastModified(anchor);\n                    if (lm < 0)\n                        freshness_lifetime = 24*3600;           /* 24 hours */\n                    else \n                        freshness_lifetime = (date - lm) / 10;\n                } else\n                    freshness_lifetime = me->expires - date;\n            }\n            me->freshness_lifetime = HTMAX(0, freshness_lifetime);\n        }\n        if (CACHE_TRACE) {\n            HTTrace(\"Cache....... Received Age %d, corrected %d, freshness\nlifetime %d\\n\",\n                    HTAnchor_age(anchor),\n                    me->corrected_initial_age,\n                    me->freshness_lifetime);\n        }\n        return YES;\n    }\n    return NO;\n}\n\n\n\n", "id": "lists-010-12174666"}, {"subject": "Re: Calculating Age Questio", "content": "> So when the User Agent receives the response, the Age value \n> should be 13? Basically, at each cache, the Age value will \n> be increased by the response_delay of that cache (plus \n> resident_time if applicable). \n\nRegardless of what it says in the spec, the Age value is not touched\nby the cache unless resident_time > 0.  In other words, a cache does\nnot age a response that it has never had in its possession.  It is only\nwhen resident_time > 0 (the response is coming from the cache and\nnot from an upstream server) that the cache sets the Age value in\nthe message.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-12184535"}, {"subject": "Re: Calculating Age Questio", "content": "Addressed to: \"Roy T. Fielding\" <fielding@liege.ICS.UCI.EDU>\n              HTTP-WG <@hplb.hpl.hp.com:http-wg@cuckoo.hpl.hp.com>\n\n** Reply to note from \"Roy T. Fielding\" <fielding@liege.ICS.UCI.EDU> Mon, 25 Nov 1996 19:52:31 -0800 \n \n \n> > should be 13? Basically, at each cache, the Age value will   \n> > be increased by the response_delay of that cache (plus   \n> > resident_time if applicable).   \n>   \n> Regardless of what it says in the spec, the Age value is not touched  \n> by the cache unless resident_time > 0.  In other words, a cache does  \n> not age a response that it has never had in its possession.  It is only  \n> when resident_time > 0 (the response is coming from the cache and  \n> not from an upstream server) that the cache sets the Age value in  \n> the message. \n \nAre we making a distinction here between \"proxies\" and \"caches\" \nthen? It seems the wording in 14.6 still requires caches to \nsend the Age header, even if acting more as a proxy. Henrik's code \nthat he posted earlier seems to follow this too. \n \nAs an implementer, I take the wording to mean calculate the \nAge value and send it everytime even if I've just received the \nresponse and it's not really in the cache yet. \n \nPutting that aside though, my real question went back to the \ndiscussion in July between you and Jeff Mogul over the algorithm. I \nguess your point here still reflects that. It does seem the \nalgorithm (if all caches send Age) can easily inflate the value. \nMore so if there are multiple caches in a chain and the resource \nis kept resident for any length of time in each. \n \nPaul \n \n\n\nPaul Hethmon \nphethmon@utk.edu \nphethmon@hethmon.com \n---------------------------------------------------------- \nInet.Mail for OS/2 -- Internet Mail Server \n---------------------------------------------------------- \nwww.hethmon.com -- ftp.hethmon.com \n----------------------------------------------------------\n\n\n\n", "id": "lists-010-12192963"}, {"subject": "Re: DRAFT Agenda for HTTPW", "content": "Larry Masinter:\n>\n>Here is a rough draft of an agenda for the HTTP WG at the San Jose\n>IETF.  The Internet drafts to be discussed are noted in each section.\n[.....]\n>30 minSafe POST / GET-with-body\n>draft-holtman-http-safe\n\nBy the way: a slightly updated safe internet draft was submitted\nyesterday.  See\n\n  http://gewis.win.tue.nl/~koen/draft-holtman-http-safe-01.txt \n\nfor a copy.\n\nThe main new message in the draft is: we have to solve the\n`unnecessary confirmation dialog' problem, and work on other solutions\nbeing absent, we have to solve it with the Safe header.  I put this\nmessage in the draft because I won't be there to say it in person.\n\nKoen.\n\n\n\n", "id": "lists-010-12203299"}, {"subject": "hit metering as 'Proposed Standard", "content": "My current reading of the situation:\n\nI think we have clear evidence that there is significant interest in\nforwarding this draft along standards track.  However, we also have a\nserious and well reasoned argument that the proposal would cause more\nharm than good if implemented.\n\nPath 1: modify the proposal to remove this objection\nPath 2: leave the proposal alone and show that it does more good\n        than harm.\n\nBoth the benefit and the assertions about 'more harm than good' are\nexpressed as conjectures with out any explicit statistical data to\nback them up. To pursue Path 2, we can either:\n\n Path 1A: release the specification as 'Experimental' as a way of\n          encouraging people in the community to gather data \n Path 1B: release the specification as 'Proposed Standard' but\n          with an applicability statement that limits its deployment\n          to 'experimental' deployment\n Path 1C: Wait until those vendors whose customers are eager to deploy\n          this solution to supply some data that will convince\n          us that the proposal does more good than harm.\n\nI'm not sure I see many other ways of proceeding. I don't think we\nwill want to proceed with 'proposed standard' unless the objections\nabout 'more harm than good' are addressed convincingly.\n\nIn any case, the other comments on the range of applicability,\nconformance requirements, etc. will need to be addressed.\n\nI don't think we need any more testimonials about who does or doesn't\nbelieve this proposal or want it or think it's important. We just need\nsome assertions based on facts that we can review about whether the\nproposal actually addresses the problem it is intended to address.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-12211238"}, {"subject": "hit metering as 'Proposed Standard' (revised", "content": "Sigh, I think I've run out of time to use 'jet lag' as my excuse, but\nbeing off net for a week really did leave me with too much mail; I\nthought I'd reviewed all the messages so far on hit metering, but\nclearly missed most of them when I sent my 'summary'.\n\nSo, here's a revised summary of the issue:\n\nI think we're having a good discussion about the proposal. I think\nthere have been several comments and we should expect to see a revised\ndraft with clarifications and changes that remove some of the\nobjections.\n\nMy current reading of the situation:\n\nThere is significant interest in forwarding this draft along standards\ntrack. There are serious concerns about its applicability (\"does it\ngive providers enough data\") and necessity (\"could providers do the\nsame thing by tweaking max-age\") and alternatives (\"could we add\nheaders for asking for statistical sampling rather than hit\nmetering\").\n\nEven after revising the draft to meet the other objections, I think we\nstill have choices:\n\nA: release the specification as 'Experimental' as a way of\n   encouraging people in the community to gather data \nB: release the specification as 'Proposed Standard' but\n   with an applicability statement that does not encourage\n   its general deployment without additional data and investigation\n   of alternatives or\nC: Wait until those vendors whose customers are eager to deploy\n   this solution to supply some data that will convince\n   us that the proposal is (a) useful to enough content providers\n   (b) cannot be accomplished as easily using current HTTP\n   and (c) better than the alternatives.\n\nI don't think we need any more testimonials about who does or doesn't\nbelieve this proposal or want it or think it's important.  We need\nfacts with data based on actual surveys and measurements.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-12219404"}, {"subject": "Re: DRAFT Agenda for HTTPW", "content": "Larry,\n\nDue to the amount of information that needs to be presented at the Monday\nmorning Opening Plenary, the first working group session will not start until\n10:00 so it will only be a 1-1/2 hour session.  You may want to adjust your\nagenda to reflect this.\n\nThanks,\n\nMarcia\n\nAt 5:08 PM -0800 11/25/96, Larry Masinter wrote:\n>Here is a rough draft of an agenda for the HTTP WG at the San Jose\n>IETF.  The Internet drafts to be discussed are noted in each section.\n>\n>PLEASE NOTE THAT THIS IS NOT A FINAL AGENDA.\n>\n>Newer versions will be posted to\n>   ftp://ftp.parc.xerox.com/pub/masinter/http-wg-agenda.html\n>after 11/26.\n>\n>Monday, December 9, 9:30-11:30\n>\n>30 minDiscussion of HTTP/1.1 Implementation Experiences\n>Development of HTTP/1.1 issues list\n>Report from people trying to implement HTTP/1.1 to talk about their\n>experiences, difficulties with the spec, questions.\n>\n>60 min  Content negotiation\n>draft-holtman-http-negotiation\n>draft-mutz-http-attributes\n>draft-ietf-http-feature-reg\n>\n>30 minSafe POST / GET-with-body\n>draft-holtman-http-safe\n>\n>------\n>Tuesday, December 10, 9-11:00\n>\n>30 minLogging & proxy\n>draft-mogul-http-hit-metering\n>\n>30 minPEP\n>draft-ietf-http-pep\n>\n>30 min  Other work:\n>        report on WEBDAV, IPP and HTTP extensions\n>        proposals from other groups.\n>\n>  draft-aboba-rtp-http\n>        draft-petke-http-auth-scheme\n\n\n\n", "id": "lists-010-12228321"}, {"subject": "RE: hit metering as 'Proposed Standard' (revised", "content": ">A: release the specification as 'Experimental' as a way of\n>   encouraging people in the community to gather data\n\nI would like to see the specification go forward as 'Experimental', so we \ncan see whether it or statistical sampling (a la the 'Nielson family' U.S. \nTV rating service) proves to be the better proposal.\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-12237552"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "Sorry for the slow response ...\n\n    > If someone is able to describe a specific scenario where the use\n    > of the Meter mechanism, as proposed in our draft, does in fact provide\n    > more per-client information than the existing HTTP/1.1 mechanisms,\n    > then we would regard this as a bug in our specification that needs\n    > to be fixed (or at least, that needs to be called out in the Security\n    > Considerations section).\n    \n    The use of the Vary header in a do-report situation clearly\n    provides more information than is currently the case where a proxy\n    cache is being used.  Currently, if I employ a proxy-cache and it\n    requests a resource on my behalf, the origin server gets the data\n    on the proxy cache (the cache may report through some data on the\n    origin requestor, but it doesn't have to).  If the origin server\n    cache-busts, the proxy-cache must re-request the data every time,\n    but the origin server gets the data on the proxy-cache every time.\n    With your proposal, it could get aggregates of the data on the\n    actual requestors.  This compromises privacy.\n\nIf you are comparing the \"Meter: do-report\" situation with the\none for a fully cachable response, then, yes, the origin server\ndoes get more information.\n\nHowever, if you would instead compare the \"Meter: do-report\" situation\nwith the \"Cache-control: max-age, must-revalidate\" situation THAT IS\nALREADY ENABLED BY THE HTTP/1.1 SPECIFICATION (sorry for shouting),\nthen I do not believe it is possible for the origin server to obtain\nmore data using \"Meter: do-report\" than it could without it.\n\nIn fact, because the hit-metering mechanism *does* aggregate data\nregarding multiple requests (and probably multiple clients), it actually\ndelivers *less* data to the origin server than would be the case if\nthe origin server did simple cache-busting.  I.e., the origin server\nwould see the count of the number of clients who preferred to see\ntheir documents in Kurdish, but not the actual request headers.\nI view this as a potential improvement for privacy (although any\nactual improvement clearly depends on the goodwill of the origin server).\n\nNote that this *reduction* in the data is precisely what Phill\nHallam-Baker does not like about our proposal.\n    \nYou wrote: \n    Imagine for a moment that someone used a Vary: on the Host header\n    with Meter.\nand then followed that up with\n    Of course the host header would tell you nothing about the user.\n    Imagine some other header instead.\nWe did try to \"imagine some other header\", and could not find any\nspecific example of a request header (or any other piece of information)\nthat the Meter mechanism would allow the origin server to obtain\nthat was not otherwise obtainable using the features of HTTP/1.1.\n\n-Jeff\n\n\n\n", "id": "lists-010-12245231"}, {"subject": "Re: Hitmetering: to Proposed Standard", "content": "    >The \"stickiness\" of the Meter request-directive is only a\n    >performance optimization, and if there are serious technical\n    >arguments against it, we could remove that without affecting any\n    >other aspect of the proposal.\n\n    >But I do not think it is accurate to think of this in the same way\n    >that we have previously discussed \"sticky\" headers, since those\n    >were for actual request-headers.  The Meter request header is a\n    >sort of unusual thing that applies to transport-level connections,\n    >not to individual requests, and so it might probably be better to\n    >use a term other than \"sticky\" here.  (The Meter response\n    >directives are per-response, but hop-by-hop, and so if there is a\n    >general \"sticky\" mechanism agreed upon for the rest of HTTP, then\n    >it could take advantage of this.)\n\n   I'm not sure how good an optimization it is.  You mentioned above\n   that an server would probably cache-bust now only on those\n   resources for which it needs accurate counts (like an ad image).\n   By making this a per-connection header, you seem to me to force a\n   proxy to report and a server to receive information it may well\n   throw away (like the counts on every little fancy bar or button\n   image).\n\nI don't think my original response was clear enough. The Meter\nresponse directives are NOT STICKY; to quote from the I-D:\n\n   The Meter response-directives are not sticky; they apply only to the\n   specific response that they are attached to.\n\nThis means that we are not \"forcing\" a server ask for information\nthat it will throw away; it must explicitly request Metering for\neach response individually.  We certainly do not expect (or want)\norigin servers to hit-meter those bars and buttons!\n\nThere is a somewhat confusing situation, which may not have been\nhelped by one of the examples in the I-D, which is that one\ncan omit the \"Meter\" header on the *first* (and only the first)\nresponse on a connection, because the \"Connection: meter\" header\nthat must be sent if one is doing Metering has (by the HTTP/1.1\nspec) the implication that the Meter header is present in that\nresponse.  Maybe this example needs some more explanation in the text.\n\n-Jeff\n\n\n\n", "id": "lists-010-12255227"}, {"subject": "Comments on draft-mogul-http-hit-metering00.tx", "content": "[Larry, I know you asked us to stop sending comments, but I promised\nthe authors my comments earlier.  I'll limit my message to things that\nhave not been said before.]\n\n\nSection 3.1:\n\n>   When a proxy forwards a hit-metered or usage-limited response to a\n>   client (proxy or end-client) not in the metering subtree, it MUST\n>   omit the Meter header, and it MUST add \"Cache-control:\n>   proxy-revalidate\" to the response.\n\nI'd rather have the cache leave the Meter header in the request.  With\nyour system, upstream caches cannot tell a `mission critical'\n\"Cache-control: proxy-revalidate\" added to make a shopping cart\napplication safe from a `frivolous' \"Cache-control: proxy-revalidate\"\nadded to get hit counts.\n\nI think it there must be a way for proxies to tell the two uses of\nproxy-revalidate apart, else proxy cache operators who want to have\nnothing to do with hit counting will be tempted to ignore not only the\nthe `frivolous' but also the `mission critical' \"Cache-control:\nproxy-revalidate\" headers.\n\nStated differently: your current system will encourage some people to\nignore all \"Cache-control: proxy-revalidate\" headers, and this is a\nbad thing, because it will make the web an unsafe place.\n\n\nSection 3.5:\n\n>      2. When it forwards a conditional HEAD on the resource\n>         instance on behalf of one of its clients.\n\nHTTP/1.1 does not define conditional HEADs, you will have to define\nthem yourself.\n\n\nSection 4/4.1:\n\n>   We believe that our\n>   design provides adequate support for user-counting, based on the\n>   following analysis.\n\nI don't agree with your analysis.  I think that the difference between\nyour `hit counts' and real `user counts' will be both significant and\nhard to measure, because of\n\n - differing sizes of user agent caches\n - the same user revisiting a page after a long time\n - user agents being shared by multiple users, as in web kiosks\n - web robot activity not being distinguished from user activity\n - user agents with a cache size of 0 (i.e. network computer)\n\nYour `network computers' already exist: On our sun cluster, which has\na central proxy, my user agent disk cache size is set to 0.  A agent\ncache would just waste user filesystem space, and our user filesystems\nare always 97-100% full.\n\nI think your system is very good at replicating the functionality\noffered by hit counting based on cache busting.  But _hit_ counts based\non cache busting are not very good _user_ counts.\n\nI don't think any system can get you good user counts unless it\nincludes per-user global browsing history logs of many megabytes.\n\nNow, on a higher level:\n\nYou want to eliminate cache busting by introducing a cheaper way of\nmeasuring all hits.  We could also try to eliminate cache busting by\neliminating the need to measure hits at all.  Suppose we could\nconvince web advertisers that they ought to stop paying for hits and\nstart paying for _clicks_, with clicks a newly defined measure which\nhas a much better correlation to actual ad exposure.\n\nIn an earlier message, I outlined a cheap `BogoHits' system which\nwould get you good _click_ counts, where a one click was defined as\none mouse click on a page link by an actual end user.\n\n_If_ statistics say that cache busting is done mainly for hit counts,\nthen the click count option may be an attractive alternative to your\nhit count mechanism.\n\n\nSection 4.2:\n\n>Why max-uses is not a Cache-control directive\n\nI think your reasoning is flawed here: the Meter header can be used to\nnegotiate on the honoring of max-uses no matter whether max-uses\nappears in the Cache-Control response header or in the Meter response\nheader.\n\n\nSection 5.3.1:\n\nI believe your system will always count a page _pre_fetch done by a\nproxy as a hit, no matter whether the page is actually seen by a user\nlater.  You need to fix this.\n\n\nIn general:\n\nI think the draft would be improved if you remove usage limiting:\nusage limiting is not necessary anymore if all counting proxies will\nalways report hits eventually.  You could replace usage limiting with\na max-time-to-wait-with-report mechanism, which would be easier to\ndefine and implement.\n\nI do not like the special treatment of varying resources, because of\nprivacy, efficiency, and complexity reasons.  If negotiation is done\nwith TCN, you will get good variant counts without  all this\ncomplexity, because each variant has its own URL in TCN.\n\nEven if cache busting turns out to be done mainly to get hit counts,\none could still make a case against your proposal.  If we don't\noptimize unwanted origin server behavior, the unwanted behavior will\ndisappear by itself eventually, because users will vote with their\nmouse-button and move on to faster sites.\n\nKoen.\n\n\n\n", "id": "lists-010-12264684"}, {"subject": "Re: Via Header Field (replaces Forwarded", "content": "This message reflects my understanding of consensus on the Via header\nfield as a replacement for the Forwarded header field present in draft 01.\n\n===================================================================\n\n10.xx  Via\n\n   The Via general-header field must be used by gateways and proxies to\n   indicate the intermediate protocols and recipients between the user\n   agent and the server on requests, and between the origin server and\n   the client on responses. It is analogous to the \"Received\" field of\n   RFC 822 [9] and is intended to be used for tracking message forwards,\n   avoiding request loops, and identifying the protocol capabilities of\n   all senders along the request/response chain.\n\n      Via   =   \"Via\" \":\" 1#( received-protocol received-by [ comment ] )\n\n      received-protocol = [ protocol-name \"/\" ] protocol-version\n      protocol-name     = token\n      protocol-version  = token\n\n      received-by       = ( host [ \":\" port ] ) | pseudonym )\n      pseudonym         = token\n\n   The received-protocol indicates the protocol version of the message\n   received by the server or client along each segment of the\n   request/response chain.  The received-protocol version is appended to\n   the Via field value when the message is forwarded so that information\n   about the protocol capabilities of upstream applications remains\n   visible to all recipients.\n\n   The protocol-name is optional if and only if it would be \"HTTP\".  The\n   received-by field is normally the host and optional port number of\n   a recipient server or client that subsequently forwarded the message.\n   However, if the real host is considered to be sensitive information,\n   it may be replaced by a pseudonym.  If the port is not given, it may\n   be assumed to be the default port of the received-protocol.\n\n   Multiple Via field values represent each proxy or gateway that has\n   forwarded the message.  Each recipient must append their information\n   such that the end result is ordered according to the sequence of\n   forwarding applications.\n\n   Comments may be used in the Via header field to identify the software\n   of the recipient proxy or gateway, analogous to the User-Agent and\n   Server header fields.  However, all comments in the Via field are\n   optional and may be removed by any recipient prior to forwarding the\n   message.\n\n   For example, a request message could be sent from an HTTP/1.0 user\n   agent to an internal proxy code-named \"fred\", which uses HTTP/1.1\n   to forward the request to a public proxy at nowhere.com, which\n   completes the request by forwarding it to the origin server at\n   www.ics.uci.edu.  The request received by www.ics.uci.edu would then\n   have the following Via header field:\n\n      Via: 1.0 fred, 1.1 nowhere.com (Apache/1.1)\n\n   Proxies and gateways used as a portal through a network firewall\n   should not, by default, forward the names and ports of hosts\n   within the firewall region. This information should only be\n   propagated if explicitly enabled. If not enabled, the received-by\n   host of any host behind the firewall should be replaced by an\n   appropriate pseudonym for that host.\n\n   For organizations that have strong privacy requirements for hiding\n   internal structures, a proxy may combine an ordered subsequence of\n   Via header field entries with identical received-protocol values into\n   a single such entry.  For example,\n\n      Via: 1.0 ricky, 1.1 ethel, 1.1 fred, 1.0 lucy\n\n   could be collapsed to\n\n        Via: 1.0 ricky, 1.1 mertz, 1.0 lucy\n\n   Applications should not combine multiple entries unless they are all\n   under the same organizational control and the hosts have already been\n   replaced by pseudoynms.  Applications must not combine entries which\n   have different received-protocol values.\n\n      Note: The Via header field replaces the Forwarded header field\n      which was present in earlier drafts of this specification.\n\n===================================================================\nPlus these additional changes in context:\n\n*** draft-ietf-http-v11-spec-01.txtMon Feb 12 16:37:14 1996\n--- tttMon Apr  1 20:36:56 1996\n***************\n*** 1491,1501 ****\n         General-Header = Cache-Control            ; Section 10.8\n                        | Connection               ; Section 10.9\n                        | Date                     ; Section 10.17\n-                       | Forwarded                ; Section 10.20\n                        | Keep-Alive               ; Section 10.24\n                        | MIME-Version             ; Section 10.28\n                        | Pragma                   ; Section 10.29\n                        | Upgrade                  ; Section 10.41\n  \n     General header field names can be extended reliably only in \n     combination with a change in the protocol version. However, new or \n--- 1491,1501 ----\n         General-Header = Cache-Control            ; Section 10.8\n                        | Connection               ; Section 10.9\n                        | Date                     ; Section 10.17\n                        | Keep-Alive               ; Section 10.24\n                        | MIME-Version             ; Section 10.28\n                        | Pragma                   ; Section 10.29\n                        | Upgrade                  ; Section 10.41\n+                       | Via                      ; Section 10.xx\n  \n     General header field names can be extended reliably only in \n     combination with a change in the protocol version. However, new or \n***************\n*** 3636,3668 ****\n  \n- 10.20  Forwarded\n- \n-    The Forwarded general-header field is to be used by gateways and \n-    proxies to indicate the intermediate steps between the user agent \n-    and the server on requests, and between the origin server and the \n-    client on responses. It is analogous to the \"Received\" field of RFC \n-    822 [9] and is intended to be used for tracing transport problems \n-    and avoiding request loops.\n- \n-        Forwarded      = \"Forwarded\" \":\" #( \"by\" URI [ \"(\" product \")\" ]\n-                         [ \"for\" FQDN ] )\n- \n-        FQDN           = <Fully-Qualified Domain Name>\n- \n-    For example, a message could be sent from a client on \n-    ptsun00.cern.ch to a server at www.ics.uci.edu port 80, via an \n-    intermediate HTTP proxy at info.cern.ch port 8000. The request \n-    received by the server at www.ics.uci.edu would then have the \n-    following Forwarded header field:\n- \n-        Forwarded: by http://info.cern.ch:8000/ for ptsun00.cern.ch\n- \n-    Multiple Forwarded header fields are allowed and should represent \n-    each proxy/gateway that has forwarded the message. It is strongly \n-    recommended that proxies/gateways used as a portal through a \n-    network firewall do not, by default, send out information about the \n-    internal hosts within the firewall region. This information should \n-    only be propagated if explicitly enabled. If not enabled, the for \n-    token and FQDN should not be included in the field value, and any \n-    Forwarded headers already present in the message (those added \n-    behind the firewall) should be removed.\n- \n--- 3636,3637 ----\n  \n***************\n*** 4060,4066 ****\n  \n     If the response is being forwarded through a proxy, the proxy \n     application must not add its data to the product list. Instead, it \n!    should include a Forwarded field (as described in Section 10.20).\n  \n         Note: Revealing the specific software version of the server \n         may allow the server machine to become more vulnerable to \n--- 4032,4038 ----\n  \n     If the response is being forwarded through a proxy, the proxy \n     application must not add its data to the product list. Instead, it \n!    should include a Via field (as described in Section 10.xx).\n  \n         Note: Revealing the specific software version of the server \n         may allow the server machine to become more vulnerable to \n***************\n*** 4232,4237 ****\n--- 4204,4213 ----\n  \n         User-Agent: CERN-LineMode/2.15 libwww/2.17b3\n  \n+ 10.xx  Via\n+ \n+    [new section above]\n+ \n  10.44  WWW-Authenticate\n  \n     The WWW-Authenticate response-header field must be included in 401 \n***************\n*** 4604,4610 ****\n     information within the context of any given request. Therefore, \n     applications should supply as much control over this information as \n     possible to the provider of that information. Four header fields \n!    are worth special mention in this context: Server, Forwarded, \n!    Referer and From.\n  \n     Revealing the specific software version of the server may allow the \n--- 4580,4586 ----\n     information within the context of any given request. Therefore, \n     applications should supply as much control over this information as \n     possible to the provider of that information. Four header fields \n!    are worth special mention in this context: Server, Via, Referer and\n!    From.\n  \n     Revealing the specific software version of the server may allow the \n***************\n*** 4615,4622 ****\n     Proxies which serve as a portal through a network firewall should \n     take special precautions regarding the transfer of header \n     information that identifies the hosts behind the firewall. In \n!    particular, they should remove, or replace with sanitized versions, \n!    any Forwarded fields generated behind the firewall.\n  \n     The Referer field allows reading patterns to be studied and reverse \n     links drawn. Although it can be very useful, its power can be \n--- 4591,4598 ----\n     Proxies which serve as a portal through a network firewall should \n     take special precautions regarding the transfer of header \n     information that identifies the hosts behind the firewall. In \n!    particular, they should replace any Via fields generated behind the\n!    firewall with sanitized versions, as described in Section 10.xx.\n  \n     The Referer field allows reading patterns to be studied and reverse \n     links drawn. Although it can be very useful, its power can be \n\n=========================================================================\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1226716"}, {"subject": "Re: Calculating Age Questio", "content": ">> > should be 13? Basically, at each cache, the Age value will   \n>> > be increased by the response_delay of that cache (plus   \n>> > resident_time if applicable).   \n>>   \n>> Regardless of what it says in the spec, the Age value is not touched  \n>> by the cache unless resident_time > 0.  In other words, a cache does  \n>> not age a response that it has never had in its possession.  It is only  \n>> when resident_time > 0 (the response is coming from the cache and  \n>> not from an upstream server) that the cache sets the Age value in  \n>> the message. \n>  \n> Are we making a distinction here between \"proxies\" and \"caches\" \n> then? It seems the wording in 14.6 still requires caches to \n> send the Age header, even if acting more as a proxy. Henrik's code \n> that he posted earlier seems to follow this too. \n\nI am, yes, because that is the only way to interpret the spec without\nbreaking the purpose of Age.  I couldn't tell whether Henrik's code\nactually set the Age header, or was just interpreting the age locally\n(you have to do that when you store the cached entity).\n\n> As an implementer, I take the wording to mean calculate the \n> Age value and send it everytime even if I've just received the \n> response and it's not really in the cache yet. \n\nThat's why I began with \"Regardless of what it says in the spec\" --\nthe spec is in error, grievously so, and there is no excuse for it.\n\n> Putting that aside though, my real question went back to the \n> discussion in July between you and Jeff Mogul over the algorithm. I \n> guess your point here still reflects that. It does seem the \n> algorithm (if all caches send Age) can easily inflate the value. \n> More so if there are multiple caches in a chain and the resource \n> is kept resident for any length of time in each. \n\nYep, that about sums it up.  More importantly, Age becomes less reliable\non average than just comparing the Date stamps between systems, which\nmakes it a waste of time (no pun intended).  That's silly, since it was\ndesigned to be a lower bound on the actual age time, not a\n\"conservative estimate\".\n\n.....Roy\n\n\n\n", "id": "lists-010-12276232"}, {"subject": "What is a specification for? [was Re: Calculating Age Question", "content": "    Regardless of what it says in the spec, the Age value is not touched\n    by the cache unless resident_time > 0.\n\nI'm sorry, Roy, but I simply cannot let this go by without comment.\n\nEveryone knows by now that you and I disagree over whether it is\nbetter to overestimate or underestimate the Age.\n\nEveryone also seems to agree on what the specification (i.e.,\ndraft-ietf-http-v11-spec-07.txt) actually says.  Even you seem\nto agree that what the specification SAYS is that the Age calculation\nis done in such a way as to tend to overestimate the Age, although\nyou think this is the technically wrong thing to do.\n\nIt may be that, after appropriate discussion in the HTTP working\ngroup, the consensus of the WG is that the next draft of the\nspecification will change to agree with your desires.  It's\nup to the WG chair(s) to declare the consensus on this kind of issue\nat the appropriate time.\n\nBut, in the absence of a resolved consensus that the latest draft of\nthe specification is wrong, it is entirely unsupportable to say \"I\ndon't like what is in the specification, so regardless of what the\nspecification says, I plan to do something different, and I'll tell\nother people to violate the language of the specification.\"\n\nLet's suppose, for example (to choose a MUST at random from \nthe HTTP/1.1 spec), that I decided that it is wrong to use\n\"GMT\" as a timezone, and I told people\n\nRegardless of what it says in the spec, the Date value\nis always represented as an RFC1123 date in Coordinated\nUniversal Time (UTC).\n\nYou'd presumably give me a hard time about this, and you would\nbe right to do so.  The standards process has no meaning if people\nchoose to ignore the plain meaning of the words in a standard.\n\nHaving said this, because this specification is a Proposed\nStandard, it is perfectly reasonable to experiment with\nalternatives, and I assume that both Roy and I would appreciate\nseeing the results of a side-by-side comparison of the performance\nof several algorithms for computing Age.  But let's not confuse\nthis with changing the meaning of a specification by fiat.\n\n-Jeff\n\nP.S.: assuming a distinction between \"caches\" and \"proxies\"\nwhere the specification makes no such distinction, in order\nto find a way to interpret the wording of the specification\naccording to your desires, doesn't help.  If you think the\nspec is wrong, please try to change it, not to create an\nexternal understanding that contradicts the plain language\nof the spec.\n\n\n\n", "id": "lists-010-12285359"}, {"subject": "Comment on current HTTP/1.1 draft (v11/7", "content": "Hello\n\nI have a query on the intended behaviour of a 1.1-compliant proxy.  \nIf the proxy receives an HTTP/0.9 response from the proxied query, \nshould it create a 1.1 response for its client?  If so, what headers \nwould be required.  In particular, if a file is retrieved with an \nunknown type, what file type should be specified?\n\nThe draft spec doesn't seem to specify the desired/required \nbehaviour.  The nearest I could find is section 13.5.2, however this \ndoes not really address this issue.  Many other sections specify \nappropriate behaviour of proxy/cache servers, so this seems as though \nit could be an oversight.\n\nCheers\n----------------------------------------------------------------\nJeremy Laidman                          JPLaidman@ACSLink.net.au\nNetworking Consultant                            +61 0416 290866\nCanberra Institute of Technology                  +61 6 207 4272\n\n\n\n", "id": "lists-010-12296027"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age Question", "content": "Well said, Jeff.\n\nI think an RFC on 'Calculation of Age in HTTP proxies' that layed out\nthe alternatives and issues would be one way to raise a counter\nopinion.\n\nLarry\n\n\n\n", "id": "lists-010-12303784"}, {"subject": "Re: Hit Metering  report of 0/", "content": "In a previous episode Jeffrey Mogul said...\n:: \n::     In the absence of a specific server request for any minimum return\n::     count, the proxy cannot know if the report is worth sending. Some\n::     servers may not believe that a return connection is worth the\n::     overhead of receiving a 1/0 report, just as some servers may insist\n::     on seeing a 0/0 report to know that the proxy's 'best-effort' is\n::     succeeding.\n[..]\n:: \n:: It would not complicate things too much to add a Meter response-directive\n:: along the lines of\n:: Meter: want-report= MIN/MAX\n:: as long as the default was to NOT send this, and that the default\n:: MIN is equal to 1 and the default MAX is equal to infinity.\n:: \n\nThis satisfies my interest in the matter. The only point I might\nquestion in the fact that we have lost the use/re-use granularity in\nthis response-directive that is maintained elsewhere throughout the\nproposal. This breaks that pattern relying on aggregates instead, but\nI can't see an alternative short of 3 different directives\n(report-use-count, report-reuse-count, report-aggregate-count) and I\ndon't really like that as an option.\n\n:: I'm really reluctant to add something from the proxy to the server\n:: to indicate the proxy's \"willingness to report\" limits, since it's\n:: hard to imagine that a proxy cache with non-infinite disk space\n:: could actually guarantee a minimum. \n\nI'm convinced on this issue. The above mechanism should be sufficient\nanyhow.\n\n-Patrick\n--\nPatrick R. McManus - Applied Theory Communications -Software Engineering\nhttp://pat.appliedtheory.com/~mcmanusProgrammer Analyst\nmcmanus@AppliedTheory.com'Prince of Pollywood'Standards, today!\n*** - You Kill Nostalgia, Xenophobic Fears. It's Now or Neverland. - ***\n\n\n\n", "id": "lists-010-12312158"}, {"subject": "Draft 07 Edit Commen", "content": "A small edit in draft 07 here. \n \nAccept-Ranges is defined to be a response header but \nis not listed in section 6.2 as one. It should \nprobably be added when convenient and allowable. \n \nPaul \n \n \n\n\nPaul Hethmon \nphethmon@utk.edu \nphethmon@hethmon.com \n---------------------------------------------------------- \nInet.Mail for OS/2 -- Internet Mail Server \n---------------------------------------------------------- \nwww.hethmon.com -- ftp.hethmon.com \n----------------------------------------------------------\n\n\n\n", "id": "lists-010-12321486"}, {"subject": "revised agenda for Dec IET", "content": "I've revised the agenda for the HTTP working group meeting at the\nDecember IETF at\n\n   ftp://ftp.parc.xerox.com/pub/masinter/http-wg-agenda.html\n\nI'll update it as I get corrections & suggestions. If you sent me mail\nabout it and I missed it, please mail me again.\n\n================================================================\nDraft HTTP working group Agenda for December 1996 Meeting\nSee 37th IETF Agenda for details of the full schedule.\n--------------------------------------------------------------------------\nMonday, December 9, 10:00-11:30\n30 min\n    Discussion of HTTP/1.1 Implementation Experiences\n    Development of HTTP/1.1 issues list\n    Report from people trying to implement HTTP/1.1 to talk about their\n    experiences, difficulties with the spec, questions. Topics identified\n    so far:\n       * Content-disposition\n60 min\n    Content negotiationDiscussion led by Andy Mutz:\n       * draft-holtman-http-negotiation-04.txt\n       * draft-mutz-http-attributes-02.txt\n       * draft-ietf-http-feature-reg-00.txt\n       * Yaron Goland draft on display attributes\n\nTuesday, December 10, 9:00-10:50\n30 min\n    Safe POST / GET-with-body\ndraft-holtman-http-safe-01.txt\n30 min\n    Hit Metering\ndraft-mogul-http-hit-metering-00.txt\n30 min\n    HTTP-related groups\n       * HTTP NG\n       * HTTP Mib\n       * HTTP Security\n           o draft-petke-http-auth-scheme-00.txt\n       * WEBDAV\n       * IPP\n       * draft-pritchard-http-links-00.txt (?)\n       * draft-aboba-rtp-http-01.txt (?)\n20 min\n    Group scheduling, charter\n--------------------------------------------------------------------------\nComments, suggestions to Larry Masinter\n\n\n\n", "id": "lists-010-12328544"}, {"subject": "Some data related to the frequency of cachebustin", "content": "Larry Masinter suggested that the case for hit-metering might\nbe strengthened by actual data on the frequency with which\napparently cache-busted responses are seen today.  (Note that\nthis does not address how the world might change once HTTP/1.1\nis deployed.)\n\nLarry's suggestion was to analyze the logs from a large number\nof proxies, but I only have access to one (albeit very busy)\nproxy.  Anyway, I do not believe that the standard log format\ncontains enough information to decide if a response has been\nmarked uncachable.\n\nHowever, it so happens that I had collected some detailed traces\nfor the main proxy between Digital and the Internet, including\nfull request and response headers for a subset of the queries.\n\nThese traces were made over a period of about 5 hours last\nI collected them as a trial run of a much longer set of traces\nthat I plan to obtain, for an unrelated research project, but\nthey have sufficient information to make a crude estimate of\nthe frequency of cache-busting.  What they don't and can't tell\nus is whether the observed cache-busting is done for hit-metering\npurposes, or for other reasons, but I don't think this is possible\nwithout a detailed examination of the URLs and resources, and\nI don't have time for that.\n\nAnd, lest anyone be tempted to ask: we will not, under any\ncircumstances, release logs or traces from our proxy, for obvious\nreasons of privacy.  So please don't ask.\n\nI said that these were a \"subset of the queries\", because they\nwere collected for another purpose and I wasn't interested in\nthings like GIF and JPEG.  URLs with the following filename\nextensions were not collected:\n    \"jpeg\", \"jpg\", \"gif\", \"exe\", \"z\", \"gz\", \"mpeg\", \"mpg\", \"au\", \"snd\",\n    \"aif\", \"aiff\", \"aifc\", \"wav\", \"ief\", \"tiff\", \"xwd\", \"mpe\", \"qt\",\n    \"mov\", \"avi\", \"movie\", \"gl\", \"dl\", \"fli\", \"flc\"\nThe code that did the pre-filtering was pretty simplistic, so\na few references to hostnames in the .au domain were also omitted.\n\nMy analysis tools do not count references which were terminated\nprematurely (i.e., the user hit \"Stop\").\n\nFor the references that I did collect, the totals are\n61108 references\n1406 unique client hosts\n19141921 request bytes[headers + body]\n323409455 response bytes [headers + body]\n342551376 total bytes [requests + responses]\n\n16365 of those references had status codes not defined as\ncachable in section 13.4 of the HTTP/1.1 spec, so I did\nnot look any further at those.\n\n11154 of the remaining references (18% of the total) had URLs with \"?\"\nor \"cgi-bin\", so I assumed that these are uncachable and did not\nanalyze them for cache-busting.  (In fact, a moderate fraction of the\nqueries did have explicit expiration and last-modified times, which is\na subject for another study.)  For the queries, the byte-counts were:\n4335438 req-bytes, 48814692 resp-bytes, 53150130 total bytes\n\nThis left 33589 non-query possibly-cachable references (55% of the total\ncollected).  The byte-counts for these were:\n10402610 req-bytes, 270683010 resp-bytes, 281085620 total bytes\n(which is a mean request size of 309 bytes, and a mean response size\nof 8059 bytes).\n\nI categorization two kinds of non-query, possibly-cachable responses as\n\"cache-busted\":\n(1) those with no Last-Modified time given (which makes\nGET If-Modified-Since impossible).\n(2) those with both Expired and Last-Modified, and whose\nexpiration time was less than or equal to their Last-Modified\ntime (I called these \"pre-expired\" responses).\nI probably should also have counted as \"cache-busted\" those responses\nwith an expiration time less than or equal to the value of their Date\nresponse-header, if any, but I'll have to modify my tools to get that\ninformation.\n\nAnyway, the results are\nResponses with no last-modified time: 10401\nResponses pre-expired: 28\nfor a total of 10429 cache-busted refs, with these byte-counts:\n3932702 req-bytes, 81597623 resp-bytes, 85530325 bytes\n\nAs a fraction of all 61108 references, this is\n17% of the references\n21% of the req-bytes, 25% of the resp-bytes, 25% of the total bytes\n\nAs a fraction of the 33589 non-query possibly-cachable references:\n31% of the references\n38% of the req-bytes, 30% of the resp-bytes, 30% of the total bytes\n\nSummary: while it is certainly debatable whether my categorization\nof no-Last-Modified responses as \"cache-busted\" is appropriate or not,\nif one accepts this categorization, then the frequency of cache-busting\nseems to be pretty high.  One could also debate how much this would\nbe reduced by our hit-metering proposal, but there does seem to be\nsome potential here.\n\nClearly it would be a good idea to do a deeper analysis, including\nmore references, more than one proxy site, and a more careful\ncategorization of the references.  Maybe the major suppliers of\nproxy software ought to provide support for gathering such statistics.\n\n-Jeff\n\n\n\n", "id": "lists-010-12336676"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Jeffrey Mogul wrote:\n\n> things like GIF and JPEG.  URLs with the following filename\n> extensions were not collected:\n>     \"jpeg\", \"jpg\", \"gif\", \"exe\", \"z\", \"gz\", \"mpeg\", \"mpg\", \"au\", \"snd\",\n>     \"aif\", \"aiff\", \"aifc\", \"wav\", \"ief\", \"tiff\", \"xwd\", \"mpe\", \"qt\",\n>     \"mov\", \"avi\", \"movie\", \"gl\", \"dl\", \"fli\", \"flc\"\n\nSo you collected \"shtml\" which is very likely to go out without\nlast-modified header.\n\nOTOH, those that you did not collect are very likely to be larger than\nHTML files and more stable. Except in case of \"advanced usage\" of ad banners.\n\n-- \nLife is a sexually transmitted disease.\n\ndave@fly.cc.fer.hr\ndave@zemris.fer.hr\n\n\n\n", "id": "lists-010-12349137"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age Question", "content": "As an author of the specification, I have a right to point out those\nparts of the specification which are known to be in error.  It is foolish\nto insist that implementers should implement HTTP/1.1 incorrectly just\nbecause the specification was accepted by the IESG with a known error\ninside of it.  This is not a matter of opinion, it is a fact based on\nboth the original intent of the Age header field and the intent of the\ncaching subgroup.  I have provided sufficient documentation of that fact,\nand your opinion that the age calculation should be \"conservative\" is\nirrelevant to whether or not the Age header field should be created by\na proxy that never used its cache in forwarding the response message.\n\nEncouraging people to implement Age as you suggest would promulgate\nincorrect implementations and make Age useless as a real lower-bound\nfor the already-conservative age calculation.  If it becomes useless,\nthen you have defeated its purpose entirely.  I cannot wait until the\nnext round of drafts if I am to prevent Age from becoming useless,\nnor do I have the time to generate another special draft just to point\nout an error which should be obvious to anyone who actually looks at\nthe problems caused by promulgating erroneous Age values.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-12358203"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age Question", "content": "Roy T. Fielding:\n>\n>As an author of the specification, I have a right to point out those\n>parts of the specification which are known to be in error.\n[....]\n> This is not a matter of opinion, it is a fact based on\n>both the original intent of the Age header field and the intent of the\n>caching subgroup.\n\nOriginal intent is irrelevant.  What matters is if the current\nspecification is buggy.\n\nI agree with you that the age calculation specification is ugly and\nwasteful, but I do not agree that it is `in error', i.e. that it has a\nbug that must be fixed for 1.1 to work.  You have not convinced me\nthat something really bad will happen if the age calculations in the\ncurrent spec are used.\n\nNow, I would applaud it if the obvious changes are made to the age\ncalculation text in the spec.  Not because the changes would fix an\nerror, but because the changes would make the spec less ugly.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-12368165"}, {"subject": "build a web grap", "content": "hello,\nHas anybody written a program which build the hypertext graph for a web site?\nBefore doing a c program, I want to know if that exists\nthank you\nbl doan \n\n*        Bich-Lien DOAN                                             *\n*        Ecole Nationale des Mines de Saint-Etienne                 *\n*        Centre SIMADE                                              *\n*        158, cours Fauriel  42023 Saint-Etienne Cedex 2, France    *\n\n\n\n", "id": "lists-010-12377211"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "On Wed, 27 Nov 1996, Jeffrey Mogul wrote:\n\n> Anyway, the results are\n> Responses with no last-modified time: 10401\n> Responses pre-expired: 28\n> for a total of 10429 cache-busted refs, with these byte-counts:\n> 3932702 req-bytes, 81597623 resp-bytes, 85530325 bytes\n> \n> As a fraction of all 61108 references, this is\n> 17% of the references\n> 21% of the req-bytes, 25% of the resp-bytes, 25% of the total bytes\n> \n> As a fraction of the 33589 non-query possibly-cachable references:\n> 31% of the references\n> 38% of the req-bytes, 30% of the resp-bytes, 30% of the total bytes\n> \n> Summary: while it is certainly debatable whether my categorization\n> of no-Last-Modified responses as \"cache-busted\" is appropriate or not,\n> if one accepts this categorization, then the frequency of cache-busting\n> seems to be pretty high.  One could also debate how much this would\n> be reduced by our hit-metering proposal, but there does seem to be\n> some potential here.\n\nYou pegged my primary objection to your methodology. It is entirely\nunsupportable to label having no last-modified as being deliberate\ncache-busting (the only kind of cache busting this proposal could affect).\nPretty much all CGI does this (no last-modified) by default:\n\nfrom www.netimages.com:\n\nGET /ni-cgi-bin/fetch HTTP/1.0\n\nHTTP/1.0 200 OK\nDate: Thu, 28 Nov 1996 15:03:19 GMT\nServer: Apache/1.1.1\nContent-type: text/html\nSet-Cookie: Apache=19830833849193398884; path=/\n\nI made absolutely no effort to intentionally cache bust the response (the\ndata served is static - but from a huge database of Usenet articles). In\nfact - I wrote it well before I knew *how* to deliberately cache bust. \n\nrom a customer of mine who is using an off-the-shelf database frontend\n(and who doesn't have the slightest idea that cache busting is even\n*possible* - never mind doing it deliberately):\n\nrom www.trcnet.com.\n\nGET / HTTP/1.0\n\nHTTP/1.0 200 OK\nServer: Domino/1.0\nDate: Thursday, 28-Nov-96 15:20:01 GMT\nContent-Type: text/html\nContent-Length: 2946\n\nI would say the only *confirmable* deliberate cache busting done are the\n28 pre-expired responses. And they are an insignificant (almost\nunmeasurable) percentage of the responses. \n\nAs you noted - much more study is needed. This one is utterly\ninconclusive. You conclude from your numbers that significant savings can\nbe found. I conclude from the same numbers that the extra overhead of the\nhit metering in fact is *higher* than the loses to deliberate cache\nbusting. You would have more network traffic querying for hit meter\nresults than the savings for such a tiny number of cache busted responses.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-12383934"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age Question", "content": "On Wed, 27 Nov 1996, Roy T. Fielding wrote:\n\n> As an author of the specification, I have a right to point out those\n> parts of the specification which are known to be in error.  It is foolish\n> to insist that implementers should implement HTTP/1.1 incorrectly just\n> because the specification was accepted by the IESG with a known error\n> inside of it.  This is not a matter of opinion, it is a fact based on\n> both the original intent of the Age header field and the intent of the\n> caching subgroup.  I have provided sufficient documentation of that fact,\n> and your opinion that the age calculation should be \"conservative\" is\n> irrelevant to whether or not the Age header field should be created by\n> a proxy that never used its cache in forwarding the response message.\n\nGive it up Roy. You have been trying to push your personal belief on this\nfor months. The most that can be *accurately* said is that you object to\nthe clear statement in the specification. You have no special 'rights' -\nthe spec is the *consensus* of the HTTP WG, not your personal spec.  That\nyou do not *LIKE* the final spec is not a problem with the spec but with\nyour relationship to it.\n\n> \n> Encouraging people to implement Age as you suggest would promulgate\n> incorrect implementations and make Age useless as a real lower-bound\n> for the already-conservative age calculation.  If it becomes useless,\n> then you have defeated its purpose entirely.  I cannot wait until the\n> next round of drafts if I am to prevent Age from becoming useless,\n> nor do I have the time to generate another special draft just to point\n> out an error which should be obvious to anyone who actually looks at\n> the problems caused by promulgating erroneous Age values.\n\nRoy - false cachable hits are MUCH worse than false uncachable hits. Your\n'doomsday scenario' of network meltdown because of the extra hits due to\npre-maturely expired objects has NOT been demonstrated and some people\npointed out that your prefered scheme potentially results in stale objects\nbeing served as fresh - which is utterly unacceptable under any\ncircumstances. Caches *should* be ultra-conservative with regard to Age.\n\nThe published proposed standard is correct. Your position is not. Please\nquit trying to express an ownership of the standard - it is not \"Roy\nFielding's Proposed HTTP Standard\". It is the IETF's proposed standard\nand the result of the consenus of the WG. \n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-12394265"}, {"subject": "Pointer to draft about web measuremen", "content": "In my comments on the hit metering draft, I said\n\n>   We could also try to eliminate cache busting by\n>eliminating the need to measure hits at all.  Suppose we could\n>convince web advertisers that they ought to stop paying for hits and\n>start paying for _clicks_, with clicks a newly defined measure which\n>has a much better correlation to actual ad exposure.\n\nTo add a reference: Rohit Khare pointed me to an interesting draft of\nan article which discusses payment models for web advertising and the\nneed for standardization:\n\n http://www2000.ogsm.vanderbilt.edu/novak/web.standards/webstand.html\n\nKoen.\n\n\n\n", "id": "lists-010-12404612"}, {"subject": "hit metering in HTT", "content": "The HTTP Working group is creating standards for new versions of\nHTTP. We are considering a proposal for a standard for 'hit metering':\nallowing proxy caches to send back hit counts to origin servers.\n\nThis proposal can be found at:\n\nftp://ds.internic.net/internet-drafts/draft-mogul-http-hit-metering-00.txt\n\nSome of the debate in the standards group is over the efficacy of this\nmethod of gathering statistics, whether the data gathered would be\nsufficient to reduce the 'cache busting' techniques used on the web by\nthose who wish to otherwise gather demographic information.\n\nWe have noticed your paper on \"net metrics for new media\".\nhttp://www2000.ogsm.vanderbilt.edu/novak/web.standards/webstand.html\n\nIt is important to make sure that proposals get adequate review. Since\nthe main question before us is not quite a technical question but a\nprojection of the future behavior of the marketplace, it's reasonable\nto consult with those who profess to be experts in the field.\n\nI believe that it is important that the technical standards for\ngathering demographic data match the marketing standards for the kind\nof demographic data that is wanted. Do they?\n\nYour comments on this issue would be welcome; if there are others who\nwould also be suitable reviewers on this topic, please forward this\nmessage.\n\nThanks,\n\nLarry Masinter\nChair, HTTP working group\n\n\n\n", "id": "lists-010-12412076"}, {"subject": "Re: DRAFT Agenda for HTTPW", "content": "I have stored\n   ftp://ftp.parc.xerox.com/pub/masinter/draft-goland-http-headers-00.txt\n\nand linked it into the http-wg-agenda in the same directory.\n\n\n\n", "id": "lists-010-12421376"}, {"subject": "HTTP and network managemen", "content": "I've been informed that a recent reorganization of IETF areas has\nmerged applications and application-specific network management, and\nthus that HTTP-WG is responsible for reviewing proposals for managing\nHTTP agents.\n\nWe don't have a particular agenda item for\ndraft-hazewinkel-http-mib-00.txt at the next HTTP working group, but\nperhaps you could update the working group as to the status of this\nwork?\n\nI do have a few minutes on the agenda for \"HTTP MIB\".\n\nLarry Masinter\nChair, HTTP-WG\n\n\n\n", "id": "lists-010-12428643"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age Question", "content": "Roy claims that there is an egregious error in the HTTP/1.1\nspecification over the calculation of Age, and yet he does not have\ntime to generate another special draft which corrects this error.\n\nHowever, it is unacceptable to claim that the draft means what it\ndoesn't say and what we (as a group) intended it to mean.\n\nThus, I'd like to ask that if ANYONE ELSE in the WHOLE WORKING GROUP\nagrees with Roy that the proposal for calculation of Age in the\nHTTP/1.1 specification is incorrect, would you please at least send me\n(if not the whole group) a message indicating that you think this is\nsomething that should be addressed.\n\nCreating an Internet Draft on \"The Calculation of Age in HTTP/1.1\" is\nactually easier (less effort) than composing impassioned messages\nabout it, or even reading all of the mail it generates, so clearly\nwe'll be able to find _someone_ to write it up if, in fact, it needs\nwriting up.\n\nHappy Thanksgiving,\n\n(Giving thanks that the holidays is letting me catch up on mail).\n\nLarry\n\n\n\n", "id": "lists-010-12436844"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age Question", "content": ">Thus, I'd like to ask that if ANYONE ELSE in the WHOLE WORKING GROUP\n>agrees with Roy that the proposal for calculation of Age in the\n>HTTP/1.1 specification is incorrect, would you please at least send me\n>(if not the whole group) a message indicating that you think this is\n>something that should be addressed.\n\nFWIW, I think the age calculation is conservative to a fault and would\nprefer to see it change, but I don't share Roy's passion.\n\nI consider the issue of whether or not to add an Age: header (with any\nvalue) to a response that was NOT served from cache to be a different issue\nentirely, and feel more strongly that a proxy should not add an Age: to a\nfresh response.\n\nIf a client makes a must-revalidate request to a proxy and sees the Age:\nheader (worse - one miscaluclated with a time larger than the initial\nresposne time minus request time) how can it distinguish between whether 1)\nit got a fresh response, or 2) that the proxy ignored it's wishes and gave\nit a stale copy from a few seconds prior?\n\nNow that I reference the spec again, I'm confused in the ascertation made on\nthis list that all proxies must add an Age: header.\n\nIn 14.6:  \"HTTP/1.1 caches MUST send an Age header in every response.\"\n\nThat's caches -- not proxies.... ?\n\n-----\nDaniel DuBois, Traveling Coderman        www.spyglass.com/~ddubois\n   o  The alpha of Heroes of Might and Magic II Bible is here!\n      http://www.spyglass.com/~ddubois/HOMM2.html\n   o  Free Trial: Now you can play Meridian 59 FREE for two days.\n      Get all the details at http://meridian.3do.com/trial/\n\n\n\n", "id": "lists-010-12445405"}, {"subject": "suggested wording concerning Hos", "content": "I've noticed a cumbersome locution spreading through the HTTP/1.1\ndraft that I would like to cut off.  Here's an example:\n\n    ... and the Host request header (present if the request-URI is not\n    an absoluteURI) ...\n\nHow about if we define a term, \"request-host\", and use that term instead.\nThe above phrase would then become\n    ... and the request-host ...\n\nHere are some words:\n\n[Section 1.3, Terminology?]\n\nrequest-host\n    The request-host of an HTTP request is the user agent's notion of\n    the host domain name or IP address of the origin server as provided\n    in the absoluteURI of a request (if that form is used) or the Host\n    request header.\n\nDave Kristol\n\n\n\n", "id": "lists-010-1244622"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age Question", "content": "> FWIW, I think the age calculation is conservative to a fault and would\n> prefer to see it change, but I don't share Roy's passion.\n> \n> I consider the issue of whether or not to add an Age: header (with any\n> value) to a response that was NOT served from cache to be a different issue\n> entirely, and feel more strongly that a proxy should not add an Age: to a\n> fresh response.\n\nJust to clarify, that is the only objection I have related to the age\ncalculation.  The rest of the algorithm is fine, as I said before, and\nI have yet to see a technical argument from *anybody* which would indicate\nthat the part I object to is not in error.  The only responses I have\nreceived are churlish suppositions about my philosophy of HTTP caching\nbeing different from others, and statements that it wasn't \"significant\nenough\" to justify generation of a new draft before IESG approval last\nsummer.  In fact, our last discussion on this subject ended with (what\nappeared to me) a clear consensus that it was an error that would be\nplaced on the Issues list for the next revision, and thus it was\nappropriate to steer implementers away from the error.\n\nIf someone has such an argument that doesn't involve childish assumptions\nabout my intentions, please do let us know.  I would rather be proven wrong\nthan be left in doubt, since this does have a significant impact on the\ncachability of HTTP and thus the core of what I spent two years of my life\nworking towards.  Please forgive me if I am passionate about it.\n\n......Roy\n\n\n\n", "id": "lists-010-12455380"}, {"subject": "Adding Ag", "content": "Roy T. Fielding:\n>\n  [Daniel DuBois:]\n>> I consider the issue of whether or not to add an Age: header (with any\n>> value) to a response that was NOT served from cache to be a different issue\n>> entirely, and feel more strongly that a proxy should not add an Age: to a\n>> fresh response.\n>\n>Just to clarify, that is the only objection I have related to the age\n>calculation.\n\nOK, let me try to sum up where we stand.\n\nIn the 1.1 draft, a cache is defined as:\n\n| cache\n|   A program's local store of response messages and the subsystem that\n|   controls its message storage, retrieval, and deletion. [...]\n|   Any client or server may include a cache, [...]\n\nSection 14.6 of the 1.1 draft spec says:\n\n|   HTTP/1.1 caches MUST send an Age header in every response.\n\nThis is a bit ambiguous, it can mean\n\na) HTTP/1.1 proxy caches MUST send an Age header in every response.\n\nor\n\nb) HTTP/1.1 caches MUST include an Age header in every response which\n   is retrieved.\n\nSeen from the proxy, b) means that\n\n    HTTP/1.1 proxies MUST send an Age header in every response which\n    was retrieved from the cache subsystem.\n\nMany people (Daniel, Roy, Me) would like to see the spec clarified to\nuse the b) version.  \n\nI propose to add\n  \n   `clarification on when to send an Age header'\n\nas a topic for the 1.1 discussion at the IETF.\n\n>......Roy\n\nKoen.\n\n\n\n", "id": "lists-010-12464305"}, {"subject": "Re: Adding Ag", "content": "Is someone willing to write up a separate internet draft which lays\nout the issues? \"The Calculation of Age in HTTP proxies\" or some such.\n\nIf there's a draft, I can bring copies & pass them out and we can\ndiscuss what the draft says. I don't want to schedule too much time in\nthe meeting to just repeat what was said on the mailing list.\n\nLarry\n\n\n\n", "id": "lists-010-12473011"}, {"subject": "Re: hit metering in HTT", "content": "I re-read my mail from yesterday, and realized that I never defined\n'cache-busting'. Here's an attempt:\n\n================================================================\ncache busting:\n  any of a variety of techniques employed by origin servers to\n  prevent an intermediate cache from supplying the same data twice (to\n  the same or different users) merely for the purpose of gathering\n  accurate 'hit' data.\n\n  Cache-busting reduces the value of the techniques the HTTP working\n  group is developing to make caching in the Internet more effective,\n  which might in turn result in greater efficiency of the network and\n  more responsive Internet access, especially across saturated\n  trans-ocean internet links.\n\n================================================================\nLarry\n\n\n\n", "id": "lists-010-12480634"}, {"subject": "Some data related to the frequency of cachebustin", "content": "Jeff,\n\nThere's another category of cache-busting that you did not mention in\nthe statistics you reported.  This is the use of unique URL\ncomponents, which may be \"once-only\" URLs, or are at least unique for\na single user.  These are typically dynamically generated and inserted\nas components of hyperlinks in HTML documents, so that following these\nlinks makes a uniquely identifiable request to the server.  This is\ndone not so much to make sure the response is not cached, but to make\nsure that the response is not shared. (It is of course also done\nsometimes to identify the client to the server -- much as cookies are\nsometimes used).\n\nThis type of response is even less cache-friendly than \"ordinary\"\ncache-busting, because the responses may well be cachable, and it is\npretty well guaranteed that nobody else will ever request the same\nURL.\n\nNot to beat a dead horse or anything, but the reason people use these\ntechniques is because they are the only way to guarantee some degree\nof control over the user experience.  To really beat a _thoroughly_\ndead horse, this is the case because caches and history mechanisms are\nimproperly conflated in most browsers.  The \"correct\" methods of\ncontrolling cachability, with HTTP headers, are interpreted by\nbrowsers in ways that screw up the user experience as well as\ncontrolling the cache.  Until/unless this ever gets properly\naddressed, people will have to use cache-unfriendly workarounds.\n\n--Shel\n\n\n\n", "id": "lists-010-12488691"}, {"subject": "Re: hit metering in HTT", "content": "Larry Masinter writes:\n > I re-read my mail from yesterday, and realized that I never defined\n > 'cache-busting'. Here's an attempt:\n > \n > ================================================================\n > cache busting:\n >   any of a variety of techniques employed by origin servers to\n >   prevent an intermediate cache from supplying the same data twice (to\n >   the same or different users) merely for the purpose of gathering\n >   accurate 'hit' data.\n > \n >   Cache-busting reduces the value of the techniques the HTTP working\n >   group is developing to make caching in the Internet more effective,\n >   which might in turn result in greater efficiency of the network and\n >   more responsive Internet access, especially across saturated\n >   trans-ocean internet links.\n > \n > ================================================================\n > Larry\n > \n > \n\nFor the etymologists among us, does anyone know where this term originated?  \n--Shel\n\n\n\n", "id": "lists-010-12498128"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Shel,\n\nAre you saying that none of the current proposals for hit metering\nwill help with unique-URL cache-busting because they don't actually\naddress the problem of browser behavior?\n\n# Not to beat a dead horse or anything, but the reason people use these\n# techniques is because they are the only way to guarantee some degree\n# of control over the user experience.\n\nDon't cookies do the right thing?\n\n# To really beat a _thoroughly_ dead horse, this is the case because\n# caches and history mechanisms are improperly conflated in most\n# browsers.\n\nDidn't we 'fix' this with HTTP/1.1?\n\nLarry\n\n\n\n", "id": "lists-010-12506551"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Larry Masinter writes:\n > Shel,\n > \n > Are you saying that none of the current proposals for hit metering\n > will help with unique-URL cache-busting because they don't actually\n > address the problem of browser behavior?\n > \n\nI am not entirely up to speed on the proposals, so I can't really\ncomment on them, but I don't have to be that familiar with them to\nmake the point, because what I _am_ saying is that cache-busting will\ncontinue to be practiced for reasons other than hit-metering, using\ntechniques even more evil (from the cache abuse perspective) than\npre-expiring documents.\n\nIt's friendlier to caches to tell them _not_ to cache documents that\nare requested using unique URLs, since you know they'll never be\nrequested again, but since pre-expiration is one of those things that\nmakes browsers behave weirdly (within the spec, I might add), it is\navoided for that reason.\n\n\n > # Not to beat a dead horse or anything, but the reason people use these\n > # techniques is because they are the only way to guarantee some degree\n > # of control over the user experience.\n > \n > Don't cookies do the right thing?\n > \n\nIn their domain they do the right thing (unless they get cached in a\n1.0 cache that doesn't know about cookies!)  But they are not\nuniversally supported (yet), and they don't really address any of the\ndocument expiration-related problems that browsers have.  Also, as\nlong as there is any doubt about the correctness of caches, and right\nnow there is plenty of doubt, unique URLs are _much_ more reliable\nthan any other means to make sure that the client gets the requested\ndocument and not some out of date version, or one intended for someone\nelse.  Of course, 1.1 and future versions will improve this whole story,\neventually, but if there are protocol features that remain too tricky\nto use because of things browsers do, the situation isn't going to\nimprove much.  Service providers care more about correctness than\ncache and bandwidth friendliness.\n\n > # To really beat a _thoroughly_ dead horse, this is the case because\n > # caches and history mechanisms are improperly conflated in most\n > # browsers.\n > \n > Didn't we 'fix' this with HTTP/1.1?\n > \n > Larry\n > \n > \n\nIf the browser guys read the spec carefully enough, and follow the\nrecommendations as well as the requirements, the situation will be\n_improved_ (eventually), but to really get this one nailed, I don't\nreally believe that the small amount of verbiage we threw at it is going\nto be enough. To get it \"right\", we'd need to allow for the kind of\nbrowser controls in HTTP that nobody wants to contemplate because it\nseems to many people to be a mixing of levels that makes people\nuncomfortable.  I can understand that concern and even sympathize with\nit, but that doesn't make the issues go away.\n\nIf you read the spec, already understanding the issue, you'll see that\nthe issue is addressed, but if, as an implementer, you don't already\ngrok the architectural significance, you are unlikely to be\nsufficiently guided by what is in the spec.  (There's only so much\nnetwork specs can do).\n\nTo my mind the problem is that there seems to be no forum in which it\nis appropriate and effective to discuss the interaction of HTTP and\nthe subtleties of browser behavior.  In this working group we just\nwant to stick to the \"bits on the wire\", which is certainly\nunderstandable.  But I think with HTTP we have a new kind of beast,\nand the problems that I'd like to be able to work on can't be solved\nby thinking only about bits on the wire.\n\nThat said, I'm kind of resigned to the fact that this isn't going to\nbe dealt with any time soon at the protocol level, and that, as a\npractical matter, one does what one has to do, which is of course very\nsad for the scalability of the internet, improved bandwidth usage, and\nflexibility of new kinds of web-based services.  People will find\nanswers but they may not be very pretty ones.\n\n--Shel\n\n\n\n", "id": "lists-010-12515532"}, {"subject": "Re: Via Header Field (replaces Forwarded", "content": "Argh, missed one...\n\n>       received-by       = ( host [ \":\" port ] ) | pseudonym )\n\nshould be\n\n        received-by       = ( host [ \":\" port ] ) | pseudonym\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1252300"}, {"subject": "cachebustin", "content": "I noticed the recent thread on cache busting. The last item mentioned\nby Shel Kaplan about the use of unique URLs can be probably a good\nmotivation to use a different replacement policy than LRU. Lorenzo\nViciano and I have a report on this at\n\nhttp://www.iet.unipi.it/~luigi/research.html/\n\nwhich suggests a different policy called LRV which uses -- among\nother parameters -- the number of previous accesses to determine\nhow valuable is a document. The nice thing about LRV is that the\nweight associated with the number of previous accesses to a document\nis evaluated dynamically by a proxy: if the percentage of\ndocuments with a single access increases, their weight decreases\nand they are purged earlier.\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n", "id": "lists-010-12527556"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "On Fri, 29 Nov 1996, Larry Masinter wrote:\n\n> # Not to beat a dead horse or anything, but the reason people use these\n> # techniques is because they are the only way to guarantee some degree\n> # of control over the user experience.\n> \n> Don't cookies do the right thing?\n\nCookies don't keep browsers from intertwining history with the local \ncache.\n\n> \n> # To really beat a _thoroughly_ dead horse, this is the case because\n> # caches and history mechanisms are improperly conflated in most\n> # browsers.\n> \n> Didn't we 'fix' this with HTTP/1.1?\n\nOther than carefully defining the difference between a History mechanism\nand Caching, we did NOTHING! A protocol mechanism is needed so that \nthe server (applications) can influence browser history presentation.\nThe caching subgroup explicitly chose to defer this issue. \n\nDave Morris\n\n\n\n", "id": "lists-010-12535485"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "On Sat, 30 Nov 1996, David W. Morris wrote:\n\n\n> Other than carefully defining the difference between a History mechanism\n> and Caching, we did NOTHING! A protocol mechanism is needed so that \n> the server (applications) can influence browser history presentation.\n> The caching subgroup explicitly chose to defer this issue. \n\nYup. This problem will not clear until the author has the ability to\ndistinguish between 'This page *MUST NOT* ever be displayed from a\nhistory', 'This page *MAY* be redisplayed from a history but *MUST NOT* be\nrefetched when displayed from history', 'This page *MAY* be redisplayed\nfrom a history, but *MUST* be refetched first', and 'This page *MAY* be\nredisplayed from history, unconditionally.' THe last two cases can be\nhandled by properly implementing the existing Expires and cache control\ndirectives. But the first two cases cannot be done at all right now - and\nare both quite important.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-12545389"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "You said of\n\n\n1 'This page *MUST NOT* ever be displayed from a history'\n2 'This page *MAY* be redisplayed from a history but *MUST NOT* be\n   refetched when displayed from history'\n3 'This page *MAY* be redisplayed from a history, but *MUST* be\n   refetched first'\n4 'This page *MAY* be redisplayed from history, unconditionally.' \n \nthat\n\n# The last two cases can be handled by properly implementing the\n# existing Expires and cache control directives.\n\nbut I don't believe there are ANY http directives that place any\nrequirements on the handling of history lists, to the point where HTTP\n_only_ requires 4.\n\nIn fact, there are some browsers where doing much of anything else\ndoesn't make much sense. For example, there was a two-dimensional\ninfinite-plane browser where the 'history' was always completely\nvisible, albeit in perspective.\n\nHowever, I'm a little fuzzy on why lack-of-controls of history makes\n'cache-busting' more of a problem, or lessens the value of hit\nmetering.\n\nLarry\n\n\n\n", "id": "lists-010-12554043"}, {"subject": "Question Digest Draft Sec 2.1.", "content": "In section 2.1.1 of draft-ietf-http-digest-aa-05.txt, the \nsyntax for WWW-Authenticate is given as: \n \nWWW-Authenticate    = \"WWW-Authenticate\" \":\" \"Digest\" \n                         digest-challenge \n \ndigest-challenge    = 1#( realm | [ domain ] | nonce | \n                     [ digest-opaque ] |[ stale ] | [ algorithm ] ) \n \nrealm               = \"realm\" \"=\" realm-value \nrealm-value         = quoted-string \ndomain              = \"domain\" \"=\" <\"> 1#URI <\"> \nnonce               = \"nonce\" \"=\" nonce-value \nnonce-value         = quoted-string \nopaque              = \"opaque\" \"=\" quoted-string \nstale               = \"stale\" \"=\" ( \"true\" | \"false\" ) \nalgorithm           = \"algorithm\" \"=\" ( \"MD5\" | token ) \n \n \nMy question is about \"digest-opaque\". Is the term \"opaque\" \nsupposed to be \"digest-opaque\"? This seems to be the intention \nhere, but the syntax does not reflect it. Am I missing something \nfrom somewhere else? The term \"digest-opaque\" does not  \nappear anywhere else in the draft nor in the HTTP draft 07. \n \nthanks, \n \nPaul \n \n\n\nPaul Hethmon \nphethmon@utk.edu \nphethmon@hethmon.com \n---------------------------------------------------------- \nInet.Mail for OS/2 -- Internet Mail Server \n---------------------------------------------------------- \nwww.hethmon.com -- ftp.hethmon.com \n----------------------------------------------------------\n\n\n\n", "id": "lists-010-12563117"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Larry Masinter writes:\n > You said of\n > \n > \n > 1 'This page *MUST NOT* ever be displayed from a history'\n > 2 'This page *MAY* be redisplayed from a history but *MUST NOT* be\n >    refetched when displayed from history'\n > 3 'This page *MAY* be redisplayed from a history, but *MUST* be\n >    refetched first'\n > 4 'This page *MAY* be redisplayed from history, unconditionally.' \n >  \n > that\n > \n > # The last two cases can be handled by properly implementing the\n > # existing Expires and cache control directives.\n > \n > but I don't believe there are ANY http directives that place any\n > requirements on the handling of history lists, to the point where HTTP\n > _only_ requires 4.\n > \n > In fact, there are some browsers where doing much of anything else\n > doesn't make much sense. For example, there was a two-dimensional\n > infinite-plane browser where the 'history' was always completely\n > visible, albeit in perspective.\n > \n\n(I know I'm repeating myself here, so bear with me):\n\nWe just need to define the difference between a cache, which is used\nexclusively for performance improvements and is supposed to be\nsemantically transparent, and __any other client-local storage of\nfetched results__, which may be used for whatever purpose desired by the\nclient (this includes \"history\").  This was done, to an extent, for\n1.1.  The issue is that the rules for controlling the cache should not\nbe mixed up with the rules for the other local storage.  The design\nproblem is that nobody wants to constrain browser design more than\nnecessary to make services predictable and reliable, and that to even\ntalk about this kind of thing we have to go beyond \"bits on the wire\".\n\n > However, I'm a little fuzzy on why lack-of-controls of history makes\n > 'cache-busting' more of a problem, or lessens the value of hit\n > metering.\n > \n > Larry\n > \n > \n\nUse of extra-protocol solutions like unique URLs are a problem for\ncaching, especially if they can't be combined with appropriate cache\ncontrols for fear of making browsers act badly.  These types of\nsolutions may not be a problem for hit metering, except they make\naccumulating statistics more complex, because now many different URLs\nas seen by clients are actually \"the same\" URL from the server\nstatistics point of view.\n\nSince caches and other local storage are typically mixed up, certain\nuses of certain HTTP headers will have unintended consequences.  So,\npeople resort to solutions that are outside the protocol, e.g. unique\nURLs.\n\nTo repeat again the oft-repeated example, let's say a service author\nwants to send out a document that must always be refetched on \"new\"\nrequests, but should be displayed from a locally stored copy if\nsomeone wants to view previous results.  You set it up to expire\nimmediately, or you set it up so that it is not cachable.  That's\nfine, but what happens when someone hits the BACK button in their\nbrowser to go to this page?  If the history buffer and cache system\nare mixed up, hitting BACK will result in the page being re-fetched,\nwhen the service author's goal was to have it be redisplayed from\nlocal storage.  Some browsers can be a bit nasty about it, depending\nhow the page was generated, and may display results like \"DATA\nMISSING\".  This is no good from a UI perspective, and it will really\nfreak out naive users, to the point that authors such as myself will\nsimply avoid using the headers that cause this, and find other ways,\noutside the protocol, to approximate the desired result of causing new\nrequests to get new pages but allow local browser history functions to\nwork, too.  The problem with this is that using these techniques is\neven worse than avoiding caching altogether -- it can cause pages that\nshould never be cached in the first place to be cached, possibly\ndisplacing usefully cached pages.\n\nIt's also a pain from the service design perspective, since you have\nto think about all kinds of weird interactions in browsers before\nusing seemingly obvious and straightforward controls like Expires and\nCache-control.  In the long run this may be worse than a little\ncaching inefficiency.\n\n--Shel\n\n\n\n", "id": "lists-010-12571465"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age  Question", "content": "Benjamin Franz writes:\n>Roy - false cachable hits are MUCH worse than false uncachable hits. Your\n>'doomsday scenario' of network meltdown because of the extra hits due to\n>pre-maturely expired objects has NOT been demonstrated and some people\n>pointed out that your preferred scheme potentially results in stale objects\n>being served as fresh - which is utterly unacceptable under any\n>circumstances. Caches *should* be ultra-conservative with regard to Age.\n\nrom my perspective as one involved in Intranet and Extranet applications \n(defined as \"inside the company WAN\" and \"inside the company WAN as tunneled \nover the Internet\"), it must be remembered that HTTP is going to be used for \nsome very time-critical, nay, even near real-time, applications, where \nserving false data could cost thousands or millions of (insert local \ncurrency unit here).  If the Age spec is actually broken (as it appears to \nbe, though I haven't followed the arguments that closely), it should be \nfixed.  But it is never acceptable to serve stale data as fresh.  As a \n(possibly bad) analogy, if serving stale data as fresh were OK, relational \ndatabase vendors would not have taken the pains they have to make sure data \nis accurate in the face of concurrent updates to that data.  Instead, \nrelational DBs go through all sorts of contortions to ensure that the data \npresented is accurate as of the time of the database query.\n\nAlthough I can see the possibility inherent in Roy's scenario of network \nmeltdowns, any engineer should know that you don't just engineer for the \naverage case -- you prepare, in one way or another, for the extreme cases. \n (It is not acceptable, for example, for a television set to explode or \nbreak down if there is no signal on a channel or the TV is located in a \nhouse 1 door away from the television transmitter.)  If (once the Age spec \nis fixed), the New Zealand proxy \"melts down\" because of some (relatively \nsmall) fraction of HTTP responses are false uncachable hits, then it is not \nengineered well enough -- certainly not to handle the increased load that \nwill be placed on it as more New Zealanders (sp?) go on-line to the Web, or \nas more New Zealand businesses use the Web more.\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-12583549"}, {"subject": "Re: What is a specification for? [was Re: Calculating Age  Question", "content": "We're going in circles we've visited before.\n\nI would like to shut down the topic of \"computation of Age by HTTP\nProxies\" unless someone volunteers to edit an Internet Draft on the\ntopic that proposes alternative(s) to what the draft says and\ndiscusses the pros and cons.\n\n\n\n", "id": "lists-010-12593523"}, {"subject": "Re: suggested wording concerning Hos", "content": "> I've noticed a cumbersome locution spreading through the HTTP/1.1\n> draft that I would like to cut off.  Here's an example:\n> \n>     ... and the Host request header (present if the request-URI is not\n>     an absoluteURI) ...\n\nArgh, where did that come from?  The Host header is ALWAYS present in\nHTTP/1.1.  It is never removed, not even when the full-URI is present.\nIt will not be removed until HTTP/2.0, which is a different specification.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1259847"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Larry Masinter:\n>\n>However, I'm a little fuzzy on why lack-of-controls of history makes\n>'cache-busting' more of a problem,\n\nWith some current browsers, if you use caching directives, this has weird\nside-effects on how the history mechanism works.  HTTP/1.1 states that there\nshould be no side effects on the history buffer, but not every current\nbrowser conforms to that.\n\nAs long as these side-effects on the history mechanism remain, service\nauthors which do not want the side effects (and there are many reasons for\nnot wanting them) cannot use the caching directives.  So these service\nauthors will have to resort to one-time-URL cache busting techniques if they\nwant to prevent the users from seeing stale data.\n\nCache busting will remain with us to some extent until this unwanted\ncoupling between history buffers and caches goes away.\n\nI have some hope that the language in 1.1 will make the coupling go away.\nIf not, introducing explicit history control headers is my best bet on\ngetting browsers to offer at least the option of not coupling between cache\nand history.  Even though history control headers would not affect the bytes\non the wire, they would affect the caching options for these bytes, so I\nfeel that I could make a strong case for the http-wg getting involved in\nthis area.\n\n>Larry\n\nKoen.\n\n\n\n", "id": "lists-010-12601387"}, {"subject": "Protocol Action: Proposed HTTP State Management Mechanism to Proposed Standar", "content": "  The IESG has approved the Internet-Draft \"Proposed HTTP State Management\n  Mechanism\" <draft-ietf-http-state-mgmt-05.txt, .ps> as a Proposed\n  Standard. This document is the product of the HyperText Transfer Protocol\n  Working Group. The IESG contact persons are Keith Moore and Harald\n  Alvestrand.\n\n\nTechnical Summary\n\n  This protocol extension defines a way for HTTP servers to ask clients\n  to maintain \"per-session\" state for them.  This is accomplished by\n  having the server encode state information in a \"cookie\" which is\n  given to the client on an initial transaction, and which the client\n  includes along with future transaction requests for a particular set\n  of URIs (not necessarily to the same server as issued the original\n  cookie).  Having clients keep state, especially across server\n  boundaries, is somewhat controversial, since it can violate users'\n  expectations of privacy.  However, state management can be\n  accomplished even with vanilla HTTP by encoding \"cookies\" in URLs.\n  Explicit HTTP support for state management is preferable to that\n  alternative.\n\n  The document attempts to explicitly address users' security and\n  privacy concerns by: requiring clients to ignore server-supplied\n  cookies in certain situations; insisting that (in certain\n  circumstances) users be made aware of, and have control over, whether\n  a cookie is sent to a server with a different domain than the server\n  that provided the current page; and requiring that clients provide\n  the user with certain mechanisms to know when a stateful session is\n  in progress and/or to control whether and under what conditions\n  cookies are being stored by the client.\n\n  The document also defines mechanisms which allow a server to specify\n  the behavior of HTTP caches with respect to state management\n  information.\n\n  The extension defined here is similar to the Netscape HTTP state\n  management mechanism which is already in wide use; thus, the\n  implications of using this extension are believed to be well\n  understood.  The document includes advice for servers on how to\n  interoperate with with user agents that use Netscape's method.\n\nWorking Group Summary\n\n  There was significant working group discussion of both the protocol\n  and the provisions for user privacy, but the group reached consensus\n  on the current docuemnt.\n\nProtocol Quality\n\n  Keith Moore reviewed the spec for IESG.\n\n\n\n", "id": "lists-010-12610753"}, {"subject": "hit metering as 'Proposed Standard", "content": "There's been some private mail on the topic of Hit Metering as\nProposed Standard, but I thought I should respond publicly:\n\nIt is not useful or relevant for the \"major vendors\" to give\ntestimonials as to the number or intensity of desire on the part of\ntheir customers to have Hit Metering.  I think everyone believes that\ncustomers want help in gathering demographic information without\ninterfering with caching. We all believe that there are many many\ncustomers who want the problem solved. We don't need testimonials.\n\nI don't actually think these customers would be well served if we push\nthrough a Proposed Standard that doesn't actually solve the problem\nthey want solved.\n\nThe IETF process (RFC 1602) says that a Proposed Standard has resolved\nall the known design choices. There's an obvious design choice for how\nwe go about letting people gather data: explicit hit metering, or\nstatistical sampling. I'm not seeing 'rough consensus' in the group\nthat we've resolved this design choice.\n\n# It is because of the above that I don't think Experimental is the right\n# path. Experimental is for when there are doubts about the soundness, and\n# expreimental use is required to test it. And Experimental explictly\n# forbids deployment in operational use -- but it is exactly operational\n# use that we need to permit in order to decide the question of utility.\n \nYes, exactly. There are doubts about the soundness. I think the doubts\nhave been expressed clearly, and I don't think that sufficient\nevidence has been presented to remove those doubts.\n\nThis is on the agenda, and we can at least see from the folks in the\nroom whether we have consensus on whether Hit Metering should go\nforward as Proposed Standard.\n\nIf there are others who have not sent mail on the topic who have a\nstrong opinion about moving Hit Metering forward, you can mail either\nme privately or the group as a whole.\n\nLarry\n\n\n\n", "id": "lists-010-12620803"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Benjamin Franz points out several ways in which my simplistic trace\nanalysis might have overestimated the number of possibly cache-busted\nresponses seen at our proxy.\n\nIn particular, he suggests that some of the non-query\npossibly-cachable references that I counted might actually\nhave been CGI output, which should not have been included\nin the set of \"possibly cache-busted responses\".  (I will\nnote, however, that one of the examples he gave would NOT\nhave been counted as such by my analysis, because that URL\nincluded the string \"cgi-bin\".  I explicitly did not count\nsuch URLs.)\n\nIf someone would like to propose a *feasible* filter on URLs\nand/or response headers (i.e., something that I could implement\nin a few dozen lines of C) that would exclude other CGI\noutput (i.e., besides URLs containing \"?\" or \"cgi-bin\", which\nI already exclude), then I am happy to re-run my analysis.\n\nDrazen Kacar pointed out that I should probably have\nexcluded .shtml URLs from this category, as well, because\nthey are essentially the same thing as CGI output.  I checked\nand found that 354 of the references in the trace were to .shtml\nURLs, and hence 10075, instead of 10429, of the references\nshould have been categorized as possibly cache-busted.  (This\nis a net change of less than 4%.)\n\n    I would say the only *confirmable* deliberate cache busting done\n    are the 28 pre-expired responses. And they are an insignificant\n    (almost unmeasurable) percentage of the responses.\n\nIf I was writing a scientific paper whose thesis was that a\nsignificant fraction of the responses are cache-busted, then\nyou are right that I would not have a rigorous proof regarding\nanything but these 28 pre-expired responses.  And, no matter\nhow much more filtering I do on the data, I would not expect\nto be able to construct a rigorous proof based on such a trace.\n\nOn the other hand, I don't believe that this trace could provide\na rigorous proof of the converse hypothesis, that no deliberate\ncache-busting is done.  Nor do I believe that any trace-based\nanalysis could prove this, given the frequency with which I\nfound responses that leave the question ambiguous.\n\nIn short, if we are looking for a rigorous, scientific *proof*\nthat cache-busting is either prevalent or negligible, I don't\nthink we are going to find it in traces, and I can't think of\nwhere else one might look.\n\nBut we are engaged in what fundamentally is an *engineering*\nprocess, rather than a scientific one.  This means that, from\ntime to time, we are going to have to infer future reality from\nan imprecise view of current reality, and that the future is\nin large part determined by the result of our engineering, not\nindependent of it.\n\nI welcome other sources of data that might help make this inference\nmore reliable.  Certainly we should not base everything on five\nhours of trace data from one site.  On the other hand, it's\nfoolish to dismiss the implications of the data simply because\nit fails to rigorously prove a particular hypothesis (pace the\nTobacco Institute, which has taken about 30 years to admit that\nthere might in fact be a connection between smoking and cancer.)\n\n    As you noted - much more study is needed. This one is utterly\n    inconclusive. You conclude from your numbers that significant\n    savings can be found.\n\nI wouldn't say I concluded that.  I said \"there does seem to\nbe some potential here.\"\n\n    I conclude from the same numbers that the extra overhead of the hit\n    metering in fact is *higher* than the loses to deliberate cache\n    busting. You would have more network traffic querying for hit meter\n    results than the savings for such a tiny number of cache busted\n    responses.\n\nThis mystifies me.  What overhead of hit-metering are you talking about?\n\nThere are three kinds of overhead in our proposed scheme:\n\n(1) additional bytes of request headers\n(a) for agreeing to hit-meter\n(b) for reporting usage-counts\n(2) additional bytes of response headers\n(3) additional HEAD request/response transactions for\n\"final reports\"\n\nOverheads of types #1(b), #2, and #3 are *only* invoked if the origin\nserver wants a response to be hit-metered (or usage-limited,\nbut that's not relevant to this analysis).  This means that\nif hit-metering were not useful to the origin-server, it would\nnot be requested, and so these overheads would not be seen.\n(I'm assuming a semi-rational configuration of the server!)\n\nNote that #3 can *only* happen instead of a full request\non the resource, and is likely to elicit a smaller (no-body)\nresponse, so it's not really clear that this should be\ncounted as an \"overhead\".\n\nWhat remains is the overhead (type #1(a)) of a proxy telling\na server that it is willing to meter.  I'll ignore the obvious\nchoice that a proxy owner could make, which is to disable this\nfunction if statistics showed that hit-metering increases overheads\nin reality, and assume that the proxy is run by someone of less\nthan complete understanding of the tradeoffs.\n\nSo, once per connection, the proxy would send\nConnection: meter\nwhich is 19 bytes, by my count.  If each connection carried just\none request, then (assuming that the mean request size stays\nat about 309 bytes, which is what I found for all of the requests\nI traced, and this does not include any IP or TCP headers!), then\nthis is about a 6% overhead. (But at one request/connection,\nand with a mean request size smaller than 576 bytes, there would\nprobably be almost no increase in packet count.)\n\nHowever, since hit-metering can only be used with HTTP/1.1 or\nhigher, and persistent connections are the default in HTTP/1.1,\nand because we defined this aspect of a connection to be \"sticky\"\nin our proposal, one has to divide the calculated overhead by\nthe expected number of requests per connection.  As far as I know,\nnobody has done any quantitative study of this since my SIGCOMM '95\npaper, which is presumably somewhat out of date, but (using simulations\nbased on traces of real servers) I was expecting on the order of 10\nrequests/connection.  It might even be higher, given the growing\ntendency to scatter little bits of pixels throughout every web page.\n\nAnyway, I wouldn't presume to put a specific number on this, because\nI'm already basing things on several layers of speculation.  But I\nwould appreciate seeing an analysis based on real data that supports\nyour contention, that \"the extra overhead of the hit metering in fact\nis *higher* than the loses to deliberate cache busting.\"\n\n-Jeff\n\n\n\n", "id": "lists-010-12629245"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Jeffrey Mogul wrote:\n> \n> If someone would like to propose a *feasible* filter on URLs\n> and/or response headers (i.e., something that I could implement\n> in a few dozen lines of C) that would exclude other CGI\n> output (i.e., besides URLs containing \"?\" or \"cgi-bin\", which\n> I already exclude), then I am happy to re-run my analysis.\n\nYou can check for everything that ends with \".cgi\" and \".nph\" as well\nas everything that starts with \"nph-\". Don't forget that CGIs can\nhave trailing path info.\n\n> Drazen Kacar pointed out that I should probably have\n> excluded .shtml URLs from this category, as well, because\n> they are essentially the same thing as CGI output.  I checked\n> and found that 354 of the references in the trace were to .shtml\n> URLs, and hence 10075, instead of 10429, of the references\n> should have been categorized as possibly cache-busted.  (This\n> is a net change of less than 4%.)\n\nThere is a short (3 char) extension as well. I don't know which one.\nI think it's \".shm\", but I'm not sure. You'll get additional percent\nor two if you inlude all of these.\n\n>     I would say the only *confirmable* deliberate cache busting done\n>     are the 28 pre-expired responses. And they are an insignificant\n>     (almost unmeasurable) percentage of the responses.\n\nSome of them are probably due to HTTP 1.0 protocol and could have been\ncacheable if the server could count on vary header being recognized by\nthe client.\n\n> In short, if we are looking for a rigorous, scientific *proof*\n> that cache-busting is either prevalent or negligible, I don't\n> think we are going to find it in traces, and I can't think of\n> where else one might look.\n\nI can. On-line advertising mailing lists. I'm subscribed to one of those\nnot because it's my job, but to stay in touch with the web things. I'm\njust a lurker there (OK, I'm a lurker here as well, but not because I\nwant to. I can't find time to read the drafts and I'm at least two\nversions behind with those I did read.)\n\nPeople on the list are professionals and experts in their field, but\nnot in HTML or HTTP. A month ago somebody posted \"a neat trick\" which\nhad these constructs in HTML source:\n\n<FONT FACE=\"New Times Roman\" \"Times Roman\" \"Times\" SIZE=-1>...</FONT>\n<A HREF=...><TABLE>...</TABLE></A>\n\nThan somebody else pointed out that Netscape won't make the whole table\nclickable if it's contained in anchor. The answer from the original author\nstarted with \"For some reason (and I don't know why) it seems that\nNetscape can't...\". I let that one pass to see if anyone would mention\nDTDs, syntax, validators or anything at all. No one did. This is viewed\nas lack of functionality in NSN, and not as trully horrible HTML.\nTo be fair, I must mention that most of them know a thing or two about\nALT attributes and are actively fighting for its usage. They probably\ndon't know it's required in AREA, but IMG is a start. My ethernal\ngratitude to people who are fighting on comp.infosystems.www.html. I stopped\nyears ago.\n\nAnother example is HTTP related. There was talk about search engines and\none person posted that cheating them is called \"hard working\". Than there\nwas a rush of posts saying that is not ethical and that pages text that\ncontains repeating of key words could come up on the top of the list, but\nit would look horrible when the customer really requests the page. No one\nmentioned that you can deliver one thing to the search engine and another\nto the browser.\n\nTo conclude, marketing people are clueless about HTML and (even more) HTTP\nand they can't participate on this list. It's not that they would not\nwant to. They have some needs and if those are not met with HTTP, responses\nwill be made uncacheable as soon as they find out how to do it.\nI'm doing the same thing because of charset problems. It's much more\nimportant for the information provider that users get the right code page\nthan to let proxy cache the wrong one. OK, I'm checking for HTTP 1.1 things\nwhich indicate that I can let the entity body be cacheable, but those are\nnot coming right now and (reading the wording in HTTP 1.1 spec) I doubt\nthey will.\n\nA few examples of what's needed...\n\nSuppose I need high quality graphics for the page, but it's not mandatory.\nI'll make two versions of pictures, one will have small files and the\nother will (can't do anything about it) have big files. I can conclude\nvie feature negotiation if the user's hardware and software can display\nhigh quality pictures, but not if the user wants it, ie. if the bandwidth\nis big enough or if the user is prepared to wait.\nSo, I'll display low res pictures by default and put a link to the same\npage with high res graphics. User's preference will be sent back to him in\nthe cookie. It's really, really hard and painfull to maintain two versions\nof pages just for this and I'd want my server to select appropriate picture\nbased on URL and the particular cookie. What happens with the proxy?\nI can send \"Vary: set-cookie\", but this is not enough. There'll be other\ncookies. On a really comercial site there'll be one cookie for each user.\nPeople are trying to gather information about their visitors. I can't\nblame them, although I have some ideas about preventing this. (Will have\nto read state management draft, it seems). Anyway, this must be made\nnon cacheable. Counting on LOWSRC is not good enough. \n\nAnother thing are ad banners. Some people are trying not to display the\nsame banner more than 5 or 6 time to a particular user. The information\nabout visits is stored in (surprise, surprise) cookie. The same thing, again.\n\nI think that technical experts should ask the masses what's needed. Don't\nexpect the response in the form of Internet draft, though.\n\n-- \nLife is a sexually transmitted disease.\n\ndave@fly.cc.fer.hr\ndave@zemris.fer.hr\n\n\n\n", "id": "lists-010-12643690"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "On Tue, 3 Dec 1996, Drazen Kacar wrote:\n\n> Jeffrey Mogul wrote:\n> > \n> > If someone would like to propose a *feasible* filter on URLs\n> > and/or response headers (i.e., something that I could implement\n> > in a few dozen lines of C) that would exclude other CGI\n> > output (i.e., besides URLs containing \"?\" or \"cgi-bin\", which\n> > I already exclude), then I am happy to re-run my analysis.\n> \n> You can check for everything that ends with \".cgi\" and \".nph\" as well\n> as everything that starts with \"nph-\". Don't forget that CGIs can\n> have trailing path info.\n> \n> > Drazen Kacar pointed out that I should probably have\n> > excluded .shtml URLs from this category, as well, because\n> > they are essentially the same thing as CGI output.  I checked\n> > and found that 354 of the references in the trace were to .shtml\n> > URLs, and hence 10075, instead of 10429, of the references\n> > should have been categorized as possibly cache-busted.  (This\n> > is a net change of less than 4%.)\n> \n> There is a short (3 char) extension as well. I don't know which one.\n> I think it's \".shm\", but I'm not sure. You'll get additional percent\n> or two if you inlude all of these.\n\nIts worse than that. The world has started using many different TLA\nextensions for CGI type stuff. .dll is used on the Microsoft site with a\npath segment of 'isapi'. I have also seen .ast, .asm, .asp, .nsf, .exe,\n.phtml, and of course .pl, .cgi and even .tcl. TO add to the problems,\nsome people configure Apache to do server-side parsing on .html files. One\ngeneral rule of thumb is 'anything except a widely known file extension in\nthe standard mime.conf file plus some common others (.mpg, .mov, .fli,\n.avi, .wav, .mp2, .mp3, .png, .htm, .pdf, .java, .class) is probably being\ngenerated dynamically'. \n\n> I can send \"Vary: set-cookie\", but this is not enough. There'll be other\n> cookies. On a really comercial site there'll be one cookie for each user.\n\nYep.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-12658332"}, {"subject": "[Fwd: fwd: web graph query", "content": "You can use the web version of CIAO.  Visit either the demo site at\n\nhttp://www.research.att.com/~chen/web-demo\n\nor the home page of CIAO\n\nhttp://www.research.att.com/~chen/ciao\n\n\n-- \nYih-Farn Chen   (Robin)     AT&T Research\n--------------------------------------------------------------------\n2B-138, 600 Mountain Avenue, Murray Hill, NJ 07974-0636\nemail: chen@research.att.com  phone: (908)-582-7483  fax: (908)-582-3063\n\n\nattached mail follows:\nSaw this on the http-wg list -- completely wrong place for it -- and no reply. \n Thought you might want to contact the author.\n\n\n\nReturn-Path: cuckoo.hpl.hp.com!http-wg-request\nReceived: by toucan; Thu Nov 28 10:11:01 EST 1996\nReceived: from hplb.hpl.hp.com by research; Thu Nov 28 10:08:21 EST 1996\nReceived: from cuckoo.hpl.hp.com by hplb.hpl.hp.com; Thu, 28 Nov 1996 15:00:22 GMT\nReceived: by cuckoo.hpl.hp.com\n(1.37.109.16/15.6+ISC) id AA251613198; Thu, 28 Nov 1996 14:59:58 GMT\nResent-Date: Thu, 28 Nov 1996 14:59:58 GMT\nDate: Thu, 28 Nov 1996 15:56:55 +0100\nFrom: doan@cambur.emse.fr (Bich-lien Doan)\nMessage-Id: <199611281456.PAA12198@groseille.chic>\nTo: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nSubject: build a web graph\nMime-Version: 1.0\nContent-Type: text/plain; charset=us-ascii\nContent-Transfer-Encoding: 7bit\nContent-Md5: Vq/udqxot8rjgK6wXPzTcg==\nResent-Message-Id: <\"Sc9NZ.0.p86.OZQdo\"@cuckoo>\nResent-From: http-wg@cuckoo.hpl.hp.com\nX-Mailing-List: <http-wg@cuckoo.hpl.hp.com> archive/latest/1981\nX-Loop: http-wg@cuckoo.hpl.hp.com\nPrecedence: list\nResent-Sender: http-wg-request@cuckoo.hpl.hp.com\n\nhello,\nHas anybody written a program which build the hypertext graph for a web site?\nBefore doing a c program, I want to know if that exists\nthank you\nbl doan \n\n*        Bich-Lien DOAN                                             *\n*        Ecole Nationale des Mines de Saint-Etienne                 *\n*        Centre SIMADE                                              *\n*        158, cours Fauriel  42023 Saint-Etienne Cedex 2, France    *\n\n\n\n", "id": "lists-010-12669167"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Jeffrey Mogul writes:\n\n > Drazen Kacar pointed out that I should probably have\n > excluded .shtml URLs from this category, as well, because\n > they are essentially the same thing as CGI output.  I checked\n > and found that 354 of the references in the trace were to .shtml\n > URLs, and hence 10075, instead of 10429, of the references\n > should have been categorized as possibly cache-busted.  (This\n > is a net change of less than 4%.)\n\nUnfortunately, use of server-side-include type schemes (what .shtml is\ntypically meant to invoke) is not always so easy to detect --- the\nApache web server, for instance, has hooks which allow the same sort\nof processing to be applied to *.html files with certain unusual Unix\npermission bit settings (XBitHack), and there are people who run the\nserver configured to treat *all* *.html files as (potentially)\ncontaining server-side includes.  Deliberate cache-busting (e.g., to\nenable collection of better metrics) may not be the intent of these\nsetups, but they currently have something of that effect...\n\nrst\n\n\n\n", "id": "lists-010-12679220"}, {"subject": "INTEGOK: updated wordin", "content": "Several comments have come in that are incorporated into the revised\nwording attached. A synopsis of the reasons for the changes:\n\n1. Content-MD5 is not proof against malicious attacks\n2. In both MIME and HTTP, the digest is computed on what would be sent\n(modulo Transfer-Encoding or Content-Transfer-Encoding) AND (in\npractice) the canonical form (modulo Content-Encoding); for text, the\ncanonical form differs slightly between the two. See the HTTP 1.0 spec,\nappendix C, for the relationship of MIME media types and HTTP media\ntypes.\n3. Typos and awkward wording have been fixed.\n4. Binary media types can specify their own transmission byte orders, so\nnetwork byte order can't be mandated in the digest.\n5. The explanation on application of RFC 1864 in the HTTP context has\nbeen changed to a note. The note at the end has been made definitive.\n\nThanks to Dave Kristol, Jim Gettys, Roy Fielding and Phill Hallam-Baker.\n\n---------------------------------------------\n\n10.13Content-MD5\n\nThe Content-MD5 entity-header field is an MD5 digest of the entity-body,\nas defined in RFC 1864 [xx], for the purpose of providing an end-to-end\nmessage integrity check (MIC) of the entity-body. (Note: an MIC is good\nfor detecting accidental modification of the entity-body in transit, but\nis not proof against malicious attacks.)\n\nContentMD5= \"Content-MD5\" \":\" md5-digest\nmd5-digest= <base64 of 128 bit MD5 digest as per RFC 1864>\n\nThe Content-MD5 header may be generated by an origin server to function\nas an integrity check of the entity-body. Only origin-servers may\ngenerate the Content-MD5 header field; proxies and gateways MUST NOT\ngenerate it, as this would defeat its value as an end-to-end integrity\ncheck. Any recipient of the entity-body, including gateways and proxies,\nMAY check that the digest value in this header field matches that of the\nentity-body as received. \n\nThe MD5 digest is computed based on the content of the entity body,\nincluding any Content-Encoding that has been applied, but not including\nany Transfer-Encoding.  If the entity is received with a\nTransfer-Encoding, that encoding must be removed prior to checking the\nContent-MD5 value against the received entity.\n\nThis has the result that the digest is computed on the octets of the\nentity body exactly as, and in the order that, they would be sent if no\ntransfer coding were being applied.\n\n   Note: there are several ways in which the application of\n   Content-MD5 to HTTP entity-bodies differs from its\n   application to MIME entity-bodies. One is that HTTP,\n   unlike MIME, does not use Content-Transfer-Encoding,\n   and does use Transfer-Encoding and Content-Encoding.\n   Another is that, unlike MIME, the digest is computed\n   over the entire entity-body, even if it happens to be\n   a MIME \"multipart\" content-type. (Note that the multipart\n   bodies may themselves have Content-MD5 headers.) Another\n   is that HTTP more frequently uses binary content types\n   than MIME, so it is worth noting that in such cases,\n   the byte order used to compute the digest is the\n   transmission byte order defined for the type. Lastly,\n   the canonical form of text types in HTTP includes several\n   line break conventions, so conversion of all line breaks\n   to CR-LF is not required before computing or checking\n   the digest: any acceptable convention should be left\n   unaltered for inclusion in the digest.\n\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-1267945"}, {"subject": "issue:  what version", "content": "I don't recall whether the following issue was resolved on the mailing list:\n\nWhat protocol version number should an HTTP/1.1-compliant origin server\nsend for an HTTP/1.0 request?\n\nThere seemed to be two camps:\n1) Send HTTP/1.0 as the response to HTTP/1.0 requests (and HTTP/1.1 as the\nresponse to HTTP/1.1 requests).\n\nPro:HTTP/1.0 clients may only understand HTTP/1.0 responses\nCon:a client would never be able to determine whether a server\nunderstands HTTP/1.1\n\n2) Send HTTP/1.1 responses always.\n\nPro:the server advertises its capability\nCon:    because the response (headers) must be HTTP/1.0\ncompatible, the server is \"lying\" about the kind of\nresponse and may mislead or confuse the client.\n\nMy preference is (1).\nDave Kristol\n\n\n\n", "id": "lists-010-12688264"}, {"subject": "Re: issue:  what version", "content": "At 12:05 PM 12/3/96 EST, Dave Kristol wrote:\n\n>My preference is (1).\n\nMe too - let the client do the advertizing! The experiences from libwww\nshows that very few 1.0 server applications break if presented with a 1.1\nrequest. I have summarized the problems that I know of at\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Forum/Conformance.html\n\nIf you know of other problems then let me know!\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-12696456"}, {"subject": "Re: issue:  what version", "content": "Addressed to: dmk@research.bell-labs.com (Dave Kristol)\n              HTTP Working Group <http-wg@cuckoo.hpl.hp.com>\n\n** Reply to note from dmk@research.bell-labs.com (Dave Kristol) Tue, 3 Dec 96 12:05:56 EST \n \n \n> What protocol version number should an HTTP/1.1-compliant origin server  \n> send for an HTTP/1.0 request?  \n>   \n> There seemed to be two camps:  \n> 1) Send HTTP/1.0 as the response to HTTP/1.0 requests (and HTTP/1.1 as the  \n> response to HTTP/1.1 requests).  \n>   \n> Pro:HTTP/1.0 clients may only understand HTTP/1.0 responses  \n> Con:a client would never be able to determine whether a server  \n> understands HTTP/1.1  \n>   \n> 2) Send HTTP/1.1 responses always.  \n>   \n> Pro:the server advertises its capability  \n> Con:    because the response (headers) must be HTTP/1.0  \n> compatible, the server is \"lying\" about the kind of  \n> response and may mislead or confuse the client. \n \nMy understanding is that (2) is the prefered approach. The HTTP \nversion number is to indicate the capabilities of the agent versus \nthe level of the message. \n \nPaul \n \n\n\nPaul Hethmon \nphethmon@hethmon.com -- phethmon@utk.edu \n------------------------------------------------------------ \nInet.Mail Internet Mail Server \n------------------------------------------------------------ \nwww.hethmon.com -- ftp.hethmon.com \n------------------------------------------------------------\n\n\n\n", "id": "lists-010-12704645"}, {"subject": "Re: issue:  what version", "content": "At 12:05 PM 12/3/96 EST, Dave Kristol wrote:\n>I don't recall whether the following issue was resolved on the mailing list:\n>\n>What protocol version number should an HTTP/1.1-compliant origin server\n>send for an HTTP/1.0 request?\n>\n>There seemed to be two camps:\n>1) Send HTTP/1.0 as the response to HTTP/1.0 requests (and HTTP/1.1 as the\n>response to HTTP/1.1 requests).\n>\n>Pro:HTTP/1.0 clients may only understand HTTP/1.0 responses\n>Con:a client would never be able to determine whether a server\n>understands HTTP/1.1\n>\n>2) Send HTTP/1.1 responses always.\n>\n>Pro:the server advertises its capability\n>Con:    because the response (headers) must be HTTP/1.0\n>compatible, the server is \"lying\" about the kind of\n>response and may mislead or confuse the client.\n>\n>My preference is (1).\n>Dave Kristol\n>\n\nWe've been working with implementation #2 (taking care not to use\nany 1.1 mechanisms that would cause problems to a 1.0 client when\nthe request indicates HTTP/1.0), and have not encountered any \ninteroperability issues yet.  I've noticed that www.apache.org is running\na preliminary version of Apache v1.2 that returns HTTP/1.1 in its\nresponses -- Robert, have you folks gotten any complaints from \nany users?\n\nHas anybody else done any \"experimentation\"?  \n\n--\nSteve Wingardswingard@spyglass.com\nSpyglass, Inc.\n\n\n\n", "id": "lists-010-12714197"}, {"subject": "Re: issue:  what version", "content": "Steve Wingard writes:\n > At 12:05 PM 12/3/96 EST, Dave Kristol wrote:\n > >I don't recall whether the following issue was resolved on the mailing list:\n > >\n > >What protocol version number should an HTTP/1.1-compliant origin server\n > >send for an HTTP/1.0 request?\n > >\n > >There seemed to be two camps:\n > >1) Send HTTP/1.0 as the response to HTTP/1.0 requests (and HTTP/1.1 as the\n > >response to HTTP/1.1 requests).\n > >\n > >Pro:HTTP/1.0 clients may only understand HTTP/1.0 responses\n > >Con:a client would never be able to determine whether a server\n > >understands HTTP/1.1\n > >\n > >2) Send HTTP/1.1 responses always.\n > >\n > >Pro:the server advertises its capability\n > >Con:    because the response (headers) must be HTTP/1.0\n > >compatible, the server is \"lying\" about the kind of\n > >response and may mislead or confuse the client.\n > >\n > >My preference is (1).\n > >Dave Kristol\n > >\n > \n > We've been working with implementation #2 (taking care not to use\n > any 1.1 mechanisms that would cause problems to a 1.0 client when\n > the request indicates HTTP/1.0), and have not encountered any \n > interoperability issues yet.  I've noticed that www.apache.org is running\n > a preliminary version of Apache v1.2 that returns HTTP/1.1 in its\n > responses -- Robert, have you folks gotten any complaints from \n > any users?\n > \n > Has anybody else done any \"experimentation\"?  \n\nJigsaw uses (2) to; The proxy had some problems (some old NCSA\nservers), but nothing really ennoying. My idea was to send the highest\nminor number within the match of the major number of the client. If\nmajor number changes, I would probably have to use (1), though\n(another story, anyway, since Upgrade would probably be involved too).\n\nAnselm.\n\n\n\n", "id": "lists-010-12723739"}, {"subject": "Re: issue: what version", "content": "    2) Send HTTP/1.1 responses always.\n\nPro:the server advertises its capability\nCon:    because the response (headers) must be HTTP/1.0\ncompatible, the server is \"lying\" about the kind of\nresponse and may mislead or confuse the client.\n\nI don't understand this \"Con\".  As far as I remember, we have always\ninsisted that HTTP/1.1 responses be completely usable by HTTP/1.0\nclients, providing that these clients follow the long-standing rule\nthat they should ignore headers that are not defined in the version\nthey purport to implement.  I don't think we changed the syntax or\nsemantics of any HTTP/1.0 response header (or request header, for\nthat matter).  If we did, it's possibly a bug in the spec.\n\nSo an HTTP/1.1 server ought to be able to send to an HTTP/1.0\nclient a response which is both fully compliant with the HTTP/1.1\nspec, and also fully comprehensible to an HTTP/1.0 client.  (Of\ncourse, it is possible to botch the server implementation, but\nwe usually design a spec on the assumption that its implementations\nwill actually implement it.)\n\nPerhaps if someone has a specific example of a case where sending\nan HTTP/1.1 response to an HTTP/1.0 client will cause a problem,\nthey should share it with the working group, before we attempt\nto drag HTTP/1.1 from \"Proposed\" to \"Draft\".\n\n-Jeff\n\n\n\n", "id": "lists-010-12733197"}, {"subject": "Re: issue: what version", "content": ">Perhaps if someone has a specific example of a case where sending\n>an HTTP/1.1 response to an HTTP/1.0 client will cause a problem,\n\nTransfer-Encoding\n\nBut that doesn't mean #2 isn't the right thing to do.\n\n-----\nDaniel DuBois, Traveling Coderman        www.spyglass.com/~ddubois\n   o  The alpha of Heroes of Might and Magic II Bible is here!\n      http://www.spyglass.com/~ddubois/HOMM2.html\n   o  Free Trial: Now you can play Meridian 59 FREE for two days.\n      Get all the details at http://meridian.3do.com/trial/\n\n\n\n", "id": "lists-010-12741807"}, {"subject": "Re: issue: what version", "content": ">>2) Send HTTP/1.1 responses always.\n>>\n>>Pro:the server advertises its capability\n>>Con:    because the response (headers) must be HTTP/1.0\n>>compatible, the server is \"lying\" about the kind of\n>>response and may mislead or confuse the client.\n>>\n>>My preference is (1).\n>>Dave Kristol\n\nI don't believe this Con.  There is no lie in using a portion of the\nHTTP/1.1 protocol.  There is no need to use every feature on every\nresponse.\n\nSteve asked:\n> We've been working with implementation #2 (taking care not to use\n> any 1.1 mechanisms that would cause problems to a 1.0 client when\n> the request indicates HTTP/1.0), and have not encountered any \n> interoperability issues yet.  I've noticed that www.apache.org is running\n> a preliminary version of Apache v1.2 that returns HTTP/1.1 in its\n> responses -- Robert, have you folks gotten any complaints from \n> any users?\n\nWe had one mention from a user of an obscure Fresco browser that it\npuked on receipt of \"HTTP/1.1 200 ...\".  It was (and is) my opinion that\nit is better to kill a client that is that poorly implemented now rather\nthan let it fester.  We have had no other complaints.\n\nThe only real problem we have encountered is with the poor user interface\ngiven to browsers with settings for the language tag.  It seems that\nseveral major browsers encourage the user to send \"en-US\" instead of\njust \"en\" (or \"en, en-US\").  This is causing havoc with the language\nselection algorithm.  However, this is not specific to HTTP/1.1.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-12749598"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Last time I look, Pointcast objects (when you use point cast behind a http\nproxy) are cachable by header and url but every url is unique.\nI guess we can catagorize them as cache busting too.\n\nAnawat\n\n\n\n", "id": "lists-010-12759017"}, {"subject": "Re: issue: what version", "content": "Jeffrey Mogul <mogul@pa.dec.com> wrote:\n  > [DMK:]\n  >     2) Send HTTP/1.1 responses always.\n  > \n  > Pro:the server advertises its capability\n  > Con:    because the response (headers) must be HTTP/1.0\n  > compatible, the server is \"lying\" about the kind of\n  > response and may mislead or confuse the client.\n  > \n  > I don't understand this \"Con\".  As far as I remember, we have always\n  > insisted that HTTP/1.1 responses be completely usable by HTTP/1.0\n  > clients, providing that these clients follow the long-standing rule\n  > that they should ignore headers that are not defined in the version\n  > they purport to implement.  I don't think we changed the syntax or\n  > semantics of any HTTP/1.0 response header (or request header, for\n  > that matter).  If we did, it's possibly a bug in the spec.\n  > \n  > So an HTTP/1.1 server ought to be able to send to an HTTP/1.0\n  > client a response which is both fully compliant with the HTTP/1.1\n  > spec, and also fully comprehensible to an HTTP/1.0 client.  (Of\n  > course, it is possible to botch the server implementation, but\n  > we usually design a spec on the assumption that its implementations\n  > will actually implement it.)\n  > \n  > Perhaps if someone has a specific example of a case where sending\n  > an HTTP/1.1 response to an HTTP/1.0 client will cause a problem,\n  > they should share it with the working group, before we attempt\n  > to drag HTTP/1.1 from \"Proposed\" to \"Draft\".\n\nMy point about \"lying\" (which I knew to be a loaded word) is that,\nbecause the response is an HTTP/1.0 response in every way, it's false\nto call it HTTP/1.1.  (But then, we have to decide whether the version\nrefers to the message or the agent's capabilities, as Paul Hethmon\npointed out.)\n\nOn that point, the spec. is ambiguous (3.1):\nHTTP uses a \"<major>.<minor>\" numbering scheme to indicate versions of\nthe protocol. The protocol versioning policy is intended to allow the\nsender to indicate the format of a message and its capacity for\nunderstanding further HTTP communication, rather than the features\nobtained via that communication.\n\nIn the current case, I claim the \"format\" is HTTP/1.0, but the\n\"capacity for understanding\" is HTTP/1.1.\n\nAnyway, here's what I think is an example of a possible problem:\nSuppose an HTTP/1.0 UA sends Connection: keep-alive, and an\nunderstanding HTTP/1.1 server responds:\n\nHTTP/1.1 200 OK\nConnection: keep-alive\n...\n\nNow, \"keep-alive\" is not a defined HTTP/1.1 connection token, and,\nindeed, the HTTP/1.1 server is not keeping the connection alive in the\nHTTP/1.1 way.  The server (as part of an HTTP/1.1 response) should have\nno Connection response header.  (Yes, okay, the Connection: keep-alive\nheader the functional equivalent in an HTTP/1.1 response.)\n\nAnother example.  Suppose I have an HTTP/1.1 client that, for its own\nreasons, sends an HTTP/1.0 request.  It gets back an HTTP/1.1 response\nthat contains no Connection response header.  Should it interpret the\nHTTP/1.1 response to mean the connection is being kept open?\n\nDave Kristol\n\n\n\n", "id": "lists-010-12767111"}, {"subject": "Re: issue:  what version", "content": "On Tue, 3 Dec 1996, Dave Kristol wrote:\n\n> I don't recall whether the following issue was resolved on the mailing list:\n\nAs I recall, it was discussed, but not resolved.\n\n> What protocol version number should an HTTP/1.1-compliant origin server\n> send for an HTTP/1.0 request?\n> \n> There seemed to be two camps:\n> 1) Send HTTP/1.0 as the response to HTTP/1.0 requests (and HTTP/1.1 as the\n> response to HTTP/1.1 requests).\n> \n> Pro:HTTP/1.0 clients may only understand HTTP/1.0 responses\n\nThere are very few clients that do this - probably about the same as\nthe number of servers that can't understand HTTP/1.1 requests.\n\n> Con:a client would never be able to determine whether a server\n> understands HTTP/1.1\n> \n> 2) Send HTTP/1.1 responses always.\n> \n> Pro:the server advertises its capability\n> Con:    because the response (headers) must be HTTP/1.0\n> compatible, the server is \"lying\" about the kind of\n> response and may mislead or confuse the client.\n\nIMHO, HTTP/1.0 and HTTP/1.1 are close enough in form that it is quite\npossible to craft a HTTP/1.1 response that is completely compatible\nwith HTTP/1.0. Apache 1.2 (a public beta of which was just released)\nfollows this approach.\n\n-- \n________________________________________________________________________\nAlexei Kosut <akosut@nueva.pvt.k12.ca.us>      The Apache HTTP Server\nURL: http://www.nueva.pvt.k12.ca.us/~akosut/   http://www.apache.org/\n\n\n\n", "id": "lists-010-12777312"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "    There's another category of cache-busting that you did not mention in\n    the statistics you reported.  This is the use of unique URL\n    components, which may be \"once-only\" URLs, or are at least unique for\n    a single user.\n\nRight you are.  I should have been more explicit in the title of\nmy message, and I didn't explain it clearly enough in the body\nof the message, but this analysis was only aimed at finding instances\nof cache-busting that might easily be avoided through use of our\nhit-metering proposal.  I thought it would be more realistic to\nlook for cache-busting that is done without using the unique-URL\ntechnique. \n\nIt's not clear to me whether the users of once-only URLs would\nswitch to a more cache-friendly approach if our hit-metering\nproposal were available.  (Clearly, anyone that requires\ncache-busting to provide usable results in the face of broken\nhistory mechanisms is not going to switch, at least not until\nvirtually all browsers have fixed their history support.)  So\nI therefore assumed that non of the once-only URLs would be\namenable to hit-metering, and so I did not try to include these\nURLs in my category of \"possibly cache-busted responses.\"\n\nOn the other hand, it's not clear that I could have identified them\nfrom their names.  If they were pre-expired or had no last-modified\ndate, and they did not match my CGI filter, I would have included\nthem in my category of \"possibly cache-busted responses\" by mistake.\n\nWhen I am ready to re-do the analysis, I'll try a version that is\nlimited to URLs for which the trace contains at least two status-200\nresponses.  Presumably this will avoid any once-only URLs, right?\nHowever, it will decrease the sample size by a large factor, which\nmeans that the significance of the results may be weakened.\n\n-Jeff\n\n\n\n", "id": "lists-010-12786636"}, {"subject": "Re: suggested wording concerning Hos", "content": "  > > DMK:\n  > > I've noticed a cumbersome locution spreading through the HTTP/1.1\n  > > draft that I would like to cut off.  Here's an example:\n  > > \n  > >     ... and the Host request header (present if the request-URI is not\n  > >     an absoluteURI) ...\n  > \n  > Roy:\n  > Argh, where did that come from?  The Host header is ALWAYS present in\n  > HTTP/1.1.  It is never removed, not even when the full-URI is present.\n  > It will not be removed until HTTP/2.0, which is a different specification.\n\nThe words were in Koen Holtmann's mailing on content negotiation.  I\nagree with your point that Host is always required, so that's an error\nin Koen's wording.\n\nBut it's the intent of the wording I'm trying to get at.  There will be\nother places where someone is trying to say, \"the host that appears in\nabsoluteURI, or, if there is no absoluteURI, the host that appears in\nthe Host request header.\"  I want to shorten that locution to\n\"request-host\".\n\nDave Kristol\n\n\n\n", "id": "lists-010-1279413"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Jeffrey Mogul writes:\n >     There's another category of cache-busting that you did not mention in\n >     the statistics you reported.  This is the use of unique URL\n >     components, which may be \"once-only\" URLs, or are at least unique for\n >     a single user.\n > \n > Right you are.  I should have been more explicit in the title of\n > my message, and I didn't explain it clearly enough in the body\n > of the message, but this analysis was only aimed at finding instances\n > of cache-busting that might easily be avoided through use of our\n > hit-metering proposal.  I thought it would be more realistic to\n > look for cache-busting that is done without using the unique-URL\n > technique. \n > \n\nYes, sure.  You'd have to resort to unreliable heuristic techniques to\npick out such URLs.  In fact, you're likely to have already considered\nthem in one of your other categories, since they are more likely to\nshow up as invocations of CGI programs and the like, rather than\nstatic \".html\" URLs -- *something* on the server end has to interpret\nor strip off the unique part of the URL.  Unless the http server\nitself has been hacked, it will be a CGI program or the moral\nequivalent.\n\n > It's not clear to me whether the users of once-only URLs would\n > switch to a more cache-friendly approach if our hit-metering\n > proposal were available.  (Clearly, anyone that requires\n > cache-busting to provide usable results in the face of broken\n > history mechanisms is not going to switch, at least not until\n > virtually all browsers have fixed their history support.)  So\n > I therefore assumed that non of the once-only URLs would be\n > amenable to hit-metering, and so I did not try to include these\n > URLs in my category of \"possibly cache-busted responses.\"\n > \n\nThey're mainly not amenable to hit metering because it's impossible to\nalgorithmically determine the \"equivalence class\" of once-only URLs --\nall the superficially distinct URLs that fetch \"the same\" resource\nlook like different URLs. Anyway I'd have to guess that the\noverwhelming majority of servers that work using unique URLs do it\nmore for semantics than explicitly for cache-busting.\nOne question that must be asked about this:  is this technique\nprevalent enough to be worth worrying much about?  I see it a lot, but\nthen, I pay attention to sites that do stuff like this.\n\n > On the other hand, it's not clear that I could have identified them\n > from their names.  If they were pre-expired or had no last-modified\n > date, and they did not match my CGI filter, I would have included\n > them in my category of \"possibly cache-busted responses\" by mistake.\n > \nbut that \"mistake\" is actually OK, right?\n\n > When I am ready to re-do the analysis, I'll try a version that is\n > limited to URLs for which the trace contains at least two status-200\n > responses.  Presumably this will avoid any once-only URLs, right?\n\nIt will avoid true \"once-only\" URLs,  but you still might see\nsome matches on \"per-session\" URLs -- ones that track a user through a\nsession.  These per-session URLs are also fairly pointless to cache in a\nshared cache, since they're only relevant to one user, but that user\nmight ask for the same thing more than once.  Based purely\non anecdotal evidence I think per-session URLs are a lot more common than\ntrue once-only URLs.\n\n > However, it will decrease the sample size by a large factor, which\n > means that the significance of the results may be weakened.\n > \n > -Jeff\n > \n > \n\n\n--Shel\n\n\n\n", "id": "lists-010-12796085"}, {"subject": "Re: issue:  what version", "content": "At 12:05 PM -0500 1996-12-03, Dave Kristol wrote:\n>I don't recall whether the following issue was resolved on the mailing list:\n>\n>What protocol version number should an HTTP/1.1-compliant origin server\n>send for an HTTP/1.0 request?\n\nMy understanding was that this *was* resolved to be 1. \n\nCL-HTTP was patched from 2 to 1 as a result of the discussions, and\nsubsequent releases have continued to use 1.\n\nThe 1.0 proxy was the critical argument.  Read Henrik's list.\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Forum/Conformance.html\n\nI find it somewhat strange that this issue is still floating around and\nthat some people have switched positions (!).\n\nAnyway, we need a definitive statement or we're going to continue to see lots of\nconfusion.\n\n\n\n", "id": "lists-010-12807482"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Shel Kaphan wrote:\n\n> Yes, sure.  You'd have to resort to unreliable heuristic techniques to\n> pick out such URLs.  In fact, you're likely to have already considered\n> them in one of your other categories, since they are more likely to\n> show up as invocations of CGI programs and the like, rather than\n> static \".html\" URLs -- *something* on the server end has to interpret\n> or strip off the unique part of the URL.  Unless the http server\n> itself has been hacked, it will be a CGI program or the moral\n> equivalent.\n\nThere are servers you don't have to hack. They were written by hackers.\nPhttpd, for example, has this nice thing called URL rewriting. Basicaly,\nthe first thing the server does is checking if the requested URL is\nmatched by one of rewriting pattern and if so, it changes it according to\nrewriting rule. For example, I can have this pair:\n\n/*/xexe/*.html        /cgi-bin/script/%{-}\n\nwhich will translate http://my.host/~dave/xexe/cacheme.html to\nhttp://my.host/cgi-bin/script/~dave/xexe/cacheme.html\nUser agent will always see the URL before rewriting and server will invoke\nCGI which will receive original URL via PATH_INFO (and PHTTPD_ORIG_URL as\nwell :) env variable(s). Phttpd is being run on a little more than 100\nhosts, so I suppose you won't encounter this often, but I think that\nApache 1.1 can do crippled version of the magick with Action directive.\n\n-- \nLife is uncacheable sexually transmitted disease.\n\ndave@fly.cc.fer.hr\ndave@zemris.fer.hr\n\n\n\n", "id": "lists-010-12815446"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "On Tue, 3 Dec 1996, Robert S. Thau wrote:\n\n> Unfortunately, use of server-side-include type schemes (what .shtml is\n> typically meant to invoke) is not always so easy to detect --- the\n> Apache web server, for instance, has hooks which allow the same sort\n> of processing to be applied to *.html files with certain unusual Unix\n> permission bit settings (XBitHack), and there are people who run the\n> server configured to treat *all* *.html files as (potentially)\n> containing server-side includes.  Deliberate cache-busting (e.g., to\n> enable collection of better metrics) may not be the intent of these\n> setups, but they currently have something of that effect...\n\nXBitHack is set to allow cacheing of SHTML. If the server has this \nenabled, and the file has the group excute bit set, then the page is \nserved with the last-modified date of the container file. It's up to the\nwebmaster to update the modification date to the container file when\nany included files change. I use this extensively (user-configurable\npages where the included file grows, setting PICS headers, common header\nand footer information on thousands of pages, etc.). \n\nAnawat has recently pointed out to me, though, that Content-Length does \nnot get set, which will cause a persistant connection to be dropped at\nthe end of the document.\nI confess that persistant connection is something I haven't really looked \nat - it's not supported in many of the browsers and agents I've used.\n\nRe. action - in Apache one can define a new suffix and pseudo-mime-type\nwhich will redirect to a CGI script. I made one so that \n/some/doc.lang would go to /cgi-bin/redirect-lang/doc.lang and thence\nto /some/english.html, /some/french.html etc. as a semi-cacheable\nalternative to the Apache content-negotiation using  .var suffix, which\nplain doesn't work with cache in HTTP 1.0\n\nAndrew\n\n\n\n", "id": "lists-010-12825085"}, {"subject": "Re: Some data related to the frequency of cachebustin", "content": "Why can't (shouldn't) one cache a CGI response ? It seems to me more\nrational to flush cache based on the frequency of hits. For example, the \n\"help\" page at altavista is CGI-generated from a query, but as far as I \nknow it's static. It's perfectly reasonable to generate static pages\nfrom a database using CGI or otherwise, and it's quite possible to\nset all the headers Last-Modified, Expires, Content-Length etc. in\nan appropriate manner. I use  a Squid cache set to reject \"/imagemap\"\nand not much else (though not to pass cgi-bin or ? to the parent).\nPerhaps 5% of queries are cache hits, compared to around 16% of images.\n\nIf someone looks up \"Soccer in Latvia\" in a search engine, is it really\ngoing to change in ten minutes? A day? More so than \nhttp://www.obscure.org/some/really/obscure/page.html ?\n\nRe. charsets, content negotiation, etc in HTTP 1.0 - I decided\nas a compromise that using a redirect CGI is \"mostly harmless\".\nTrue, if the origin server can't be reached you're stuck, but the\nbig text files can be cached. I think Microsoft's doing something like \nthis, but as someone pointed out to me, they use set-cookie with a path\nof / which strictly speaking makes the whole site uncacheable.\n\nAndrew Daviel         mailto:advax@triumf.ca \nhttp://vancouver-webpages.com/CacheNow/ - campaign for Proxy Cache\n\n\n\n", "id": "lists-010-12834830"}, {"subject": "Revised AGENDA for HTTP-WG Dec 91", "content": "ftp://ftp.parc.xerox.com/pub/masinter/http-wg-agenda.html\n\nPlease check over carefully, and make sure that I've gotten all the\ntopics & speakers correctly. Also note plea for minutes-takers at end.\n================================================================\nDraft HTTP working group Agenda for December 1996 Meeting\nSee 37th IETF Agenda for details of the full schedule.\nMonday, December 9, 10:00-11:30\n\n30 min\n Discussion of HTTP/1.1 Implementation Experiences\n  Development of HTTP/1.1 issues list\n   Report from people trying to implement HTTP/1.1 to talk about their\n   experiences, difficulties with the spec, questions. Topics identified so\n   far:\n       * Version number of response to HTTP/1.0 request\n       * Content-disposition (give filename for downloaded files)\n       * Warning headers and revalidation\n60 min\n Content negotiation\n  Andy Mutz: discussion of drafts.\n       * draft-holtman-http-negotiation-04.txt\n       * draft-mutz-http-attributes-02.txt\n       * draft-ietf-http-feature-reg-00.txt\n       * Yaron Goland draft on display attributes\n\nTuesday, December 10, 9:00-10:50\n20 min\n Hit Metering\n  Jeff Mogul: draft-mogul-http-hit-metering-00.txt. Issues:\n      * Cache-Control: \"end-clients can cache for N>0 but proxies must\n        revalidate\", e.g., proxy-max-age\n      * Applicability statement\n      * Use of connection header\n\n10 min\n Safe POST / GET-with-body\n  draft-holtman-http-safe-01.txt\n\n30 min\n Reports from other HTTP-related working groups\n     * Mike Spreitzer: HTTP-NG report\n     * Harrie Hazewinkel: Definitions of Managed Objects for WWW Servers\n       (draft-ietf-applmib-wwwmib-00.txt), being processesed by APPLMIB on\n       Wednesday, Dec 11, 1300-1500\n     * Rich Petke: new authentication (draft-petke-http-auth-scheme-00.txt)\n     * Eric Whitehead: Distributed Authoring and Versioning (WEBDAV), to be\n       discussed Wednesday, Dec 11, 0900-1130\n     * Carl-Uno Manros: Internet Printing Protocol, to be discussed\n       Thursday, Dec 12, 1300-1500\n\n20 min\n Other issues\n    * HTTP over multicast (draft-aboba-rtp-http-01.txt)?\n    * Use of HTTP for network management (draft-mellquist-web-sys-01.txt)\n    * HTTP security (Whither SHTTP?)\n\n20 min\n Status and Plans\n  Assess group status, charter milestones, and solicit opinions about where we\n  should go from here.\n\n------------------------------------------------------------------------------\n\nComments, suggestions to Larry Masinter\n\n\n================================================================\nPlease volunteer to take minutes. (We do better when we have\ntwo people taking notes who then combine them.)\n\n  Minutes need to include the topics discussed, considerations\n  raised, and any conclusions, but not a blow-by-blow of who said\n  what. If you volunteer to take minutes, I can forward you\n  the requirements.\n\n\n\n", "id": "lists-010-12844222"}, {"subject": "Re: Revised AGENDA for HTTP-WG Dec 91", "content": "Bill Janssen sais he intends to attend the WG mtg, and give the HTTP-NG report.\n\n\n\n", "id": "lists-010-12854537"}, {"subject": "HTTP 1.1 spec (page 15", "content": "I wonder what the specification says about ``\\)'' within a comment and\nabout ``\\\"'' within a quoted-string. To my interpretation of the specs\nthe character will be \"quoted\" but according to the grammar will\nterminate the element anyhow.\n\nI think some clarification would be good there.\n\n-- \nUlrich Windl <Ulrich.Windl@rz.uni-regensburg.de>\n\n\n\n", "id": "lists-010-12862046"}, {"subject": "origin of the term &quot;cache busting&quot", "content": "Shel asked:\n   For the etymologists among us, does anyone know where this\n   term originated?\n\nThe winner (as far as this mailing list goes) appears to be Koen Holtman.\n\nI keep complete logs of my incoming mail, and this was the\nearliest reference I found:\n\n    From: koen@win.tue.nl (Koen Holtman)\n    Message-Id: <199508191558.RAA17041@wswiop05.win.tue.nl>\n    Subject: Re: Proposal: Pragma min-age (Was:Re: A modest proposal)\n    Date: Sat, 19 Aug 1995 17:58:57 +0200 (MET DST)\n\n    [....]\n\n    A wasteful provider responding to these warnings by generating\n    pages with `one-time-urls' or other cache busters would immediately\n    show up on the `wastefulness detector' you assume present: a proxy\n    administrator could then decide to stop talking to the provider\n    altogether.\n\n    [....]\n    \nThe full message is at\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1995q3/0400.html\n\n-Jeff\n\nP.S.: I didn't find another use of the term until your own message\nof Fri, 5 Jan 1996 12:56:00.\n\n\n\n", "id": "lists-010-12869490"}, {"subject": "caching CGI response", "content": "Andrew Daviel writes:\n    Why can't (shouldn't) one cache a CGI response ? It seems to me\n    more rational to flush cache based on the frequency of hits.\n\nThe HTTP/1.1 specification, in fact, does not specifically say\nthat proxies and clients should not cache the results of a CGI\nresponse.  In fact, section 13.4 (Response Cachability) says\n\n    Unless specifically constrained by a Cache-Control\n    directive, a caching system may always store a successful response \n    as a cache entry, may return it without validation if it\n    is fresh, and may return it after successful validation. If there is\n    neither a cache validator nor an explicit expiration time associated\n    with a response, we do not expect it to be cached [...]\n\nIn other words, if the server supplies a response with either a\nLast-Modified header, or an Expires header (or \"Cache-control: max-age\")\nthat gives an expiration time in the future, then the response\n*should* be cached.\n\nHowever, because most existing caches were designed before HTTP/1.1,\nand do not expect servers to generate Expires headers (most servers\napparently do not), they often cache responses that have neither\na Last-Modified header or an Expires header.  This is not really\nsuch a great idea, but it \"usually\" works.  The two well-known cases\nthat it often does not work in are those where the URL includes a \"?\"\nand those where it includes \"cgi-bin\" (or a few similar strings).\nSo it's normal practice for proxies to not cache responses to such\nURLs.\n\nNote that section 13.9 says, regarding URLs with \"?\" in them,\n    caches MUST NOT treat responses to such URLs as fresh unless\n    the server provides an explicit expiration time.\nThere is a general consensus (but not unanimity) that it is better\nto err on the side of caution in this case.  I.e., since there are\nmany such URLs for which caching would cause seriously wrong results,\nit's better to not cache any of these responses (and thus give up\nthe ability to cache certain responses that are cachable), rather\nthan to risk occasionally returning wrong answers.\n\nHowever, I think everyone agrees with you that it's both possible and\ndesirable for origin servers to explictly mark all responses\nas either non-cachable or cachable, since then the proxies don't\nhave to play guessing games based on the URL.  E.g., if you are\nwriting a server that uses CGI or \"?\" URLs, and you know that\nsome of these are cachable, if you simply add a Last-Modified\nor Expires (in the future) header to the response, then a well\ndesigned proxy will cache the response.  Conversely, if you\nmark the response as Expires \"in the past\", then no well designed\ncache should cache it (without at least sending you a conditional\nGET to see if the value has changed).\n\nAs to why the AltaVista people haven't done this: I don't know.\nSome of them work in our building, but I don't have much to\ndo with their design decisions (and they didn't invite me\nfor a ride in the blimp!).\n\nIt's probably too hard to decide automatically that a response\non a query for \"Soccer in Latvia\" would be more stable than\na query for \"Cool Site of the day\", but it should certainly\nbe possible to set an expiration time reflecting the expected\ntime between database updates.\n\n-Jeff\n\n\n\n", "id": "lists-010-12877988"}, {"subject": "Re: (ACCEPT*) Draft text for Accept header", "content": "Koen Holtman writes:\n\n> 10.2  Accept-Charset\n> \n>    The Accept-Charset request-header field can be used to indicate what\n>    character sets are acceptable for the response. This field allows\n>    clients capable of understanding more comprehensive or\n>    special-purpose character sets to signal that capability to a server\n>    which is capable of representing documents in those character\n>    sets. The US-ASCII character set can be assumed to be acceptable to\n>    all user agents.\n> \n> |  [##QUESTION TO BE RESOLVED: Apparently, the latest HTML spec says\n> |  that iso-8859-1 can be assumed to be acceptable to all user agents.\n> |  Should the above US-ASCII be changed to iso-8859-1??  There has\n> |  been lots of discussion on the list, but I have not been able to\n> |  detect a consensus opinion.##]\n\nMy 2 cents: it soyld be iso-8859-1 that is the default, referring\nto previous discussion for that view.\n\nAnother suggestion: there should be a quality parameter also with\naccept-charset,  a q<1 meaning that the browser may be able to display\nin a less readable fashion, for example in mnemonic or 10646 fallback,\nor that it need to shift fonts or load a special programme to\ndispaly the charset or other impeded things.\n\nKeld\n\n\n\n", "id": "lists-010-1288176"}, {"subject": "Re: caching CGI response", "content": "I have talked with Louis Monier (and I think I mentioned\nthis to Mike Burrows as well) about doing stuff for marking\nCGI responses cachable in AltaVista.  I told them that\nit didn't make any real difference to do it before 1.1 clients\nand proxies were available, which hasn't quite happened yet.\n(And they have had more than enough things to do in the first\nplace that asking them to do so before it would actually help anyone \nseemed pointless.). So Louis at least has it on his list of things to \ndo for A.V. in finite time (unless he has forgotten).\n\nI'll poke at them again next week if I can.\n\nBTW, we should have interesting HTTP/1.1 performance data available\nby next week (the IETF meeting); we're scrambling here to take\nthe data now we have implementations we actually think work correctly\nthat do pipelining (not just persistent connections, which we've\nhad running for a long time).  When the write up is done, we'll\npost a note to the working group (with luck, you might see it\nover the weekend).\n\nPreliminary results look very nice :-).  Saves lots of packets,\nand runs faster.  :-).  But then again, that's what we expected....\n- Jim Gettys\n\n\n\n", "id": "lists-010-12888813"}, {"subject": "Re: caching CGI response", "content": "> However, I think everyone agrees with you that it's both possible and\n> desirable for origin servers to explictly mark all responses\n> as either non-cachable or cachable, since then the proxies don't\n> have to play guessing games based on the URL.  E.g., if you are\n> writing a server that uses CGI or \"?\" URLs, and you know that\n> some of these are cachable, if you simply add a Last-Modified\n> or Expires (in the future) header to the response, then a well\n> designed proxy will cache the response.  Conversely, if you\n> mark the response as Expires \"in the past\", then no well designed\n> cache should cache it (without at least sending you a conditional\n> GET to see if the value has changed).\n\nDo you know offhand which proxies currently interpret the\nLast-Modified header in a way that would cache CGI/\"?\" URLs?\n\nThe interactive imaging protocol we're currently developing\n*should* be cacheable, as the image tiles being sent don't\nchange, and the initial image view will generally be the same\neach time it's requested. It would be nice if we could get\n*some* systems to cache something while we work out a better\nscheme for making the rest of the tile information cacheable\nby proxies.\n\n--hjl\n\n\n\n", "id": "lists-010-12897012"}, {"subject": "Re: caching CGI response", "content": "Jeffrey Mogul:\n[...]\n>However, because most existing caches were designed before HTTP/1.1,\n>and do not expect servers to generate Expires headers (most servers\n>apparently do not), they often cache responses that have neither\n                          ^^^^^\n>a Last-Modified header or an Expires header.\n\nI think there are very few existing 1.0 proxies that cache responses without\na Last-Modified header.  Doing would cause problems with a large fraction of\nall CGI-based stuff, and this would get noticed very quickly by the cache\nmaintainer.\n\nI believe the AOL cache does (or did at some point) cache everything for a\nfew minutes at least, no matter what the headers, but proxies on the `real'\ninternet generally tend to err on the conservative side.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-12905248"}, {"subject": "Re: caching CGI response", "content": "If the only issue is one of including a Last-Modified or Expires header,\nperhaps an informational RFC would be appropriate. It should a) make the\npoint that not all CGI responses should be cachable (one time only pages)\nbut that many should, and b) describe what should be included in the\nheader.\n\n\n---\nGregory Woodhouse     gjw@wnetc.com\nhome page:            http://www.wnetc.com/home.html\nresource page:        http://www.wnetc.com/resource/\n\n\n\n", "id": "lists-010-12913581"}, {"subject": "Re: AcceptCharset suppor", "content": "> # If there is a need for a client to express \"I can understand UTF-8,\n> # but can only display some of the 10646 characters: ...\" - and I \n> # definitely think there is such a need - I don not see a way to implement\n> # this cleanly.\n \nOn Sat, 7 Dec 1996, Larry Masinter wrote:\n> I think this kind of communication is along the same lines as: \"I can\n> implement all of HTML 3.5 tables, except I don't know anything about\n> the 'border' parameter\".\n> \n> That is, there may be a need to communicate special subset\n> capabilities, but usually those limitations are transient and too\n> fine-grained to actually matter in real communication.\n\nThat does not look like a fair comparison.  Whatever HTML 3.5 tables\nare, understanding the border parameter looks like a minor thing, as\nyou say.  But not being able to say what characters I can understand\nwould matter a lot in real communication.\n\nSaying \"I can understand 10646\" or \"I can understand UTF-8\" practically\njust means that I can decode that character encoding.  That is on the\nsame level as saying \"I can understand 8-bit character sets\" without\nspecifying which.  If anything more detailed is too fine-grained to\nreally matter then I don't see why anybody should currently bother \nto use Accept-Charset: ISO-8859-2 etc.\n \n> In general, in the web, we've avoided catering to fine-grained\n> differentiation of client capabilities. Yes, you can say \"I speak\n> postscript\" or not, but there's no good way to say \"I can take\n> postscript files but don't give me any that won't look good on little\n> pieces of paper\".\n\nBut whether that text is readable for me or appears as complete garbage\n(because I couldn't tell the server about my character repertoire)\nis a bit more significant than whether something looks good or bad.\n\nIf I move from sending (say) Accept-Charset: iso-8859-3 to \nAccept-Charset: utf-8 (because my browser now understands that character\nencoding), then I *lose* the capability to express what is more important\nfor the human user: what characters I can actually see.  And the\noverloading of Accept-Language with character repertoire meaning seems\nto show that there is a perceived need to express character repertoire\ncapabilities.\n\nWith the given structure of the MIME \"charset\" parameter (and therefore\nthe Accept-Charset header), the logical thing to at least preserve\nwhat currently can be expressed w.r.t. repertoire would be to register\nlots of additional charsets: we'd then have ISO-10646-Unicode-Latin2,\nISO-10646-Unicode-Latin3, ISO-10646-Unicode-Latin4, and so on.  Well\nI can see why that isn't very inviting, looks like a big can of worms...    \nWhat I cannot understand is how the loss of existing expressive \ncapability for negotiation (of something *essential*) can be seen as \na step forward.  \n\n> There _is_ a proposal for allowing profiles of capabilities to be\n> expressed and negotiated, and the proposal is elaborated in internet\n> drafts:\n> draft-holtman-http-negotiation-04.txt\n> draft-ietf-http-feature-reg-00.txt\n> and related topics in:\n> draft-mutz-http-attributes-02.txt\n> draft-goland-http-headers-00.txt\n> from your nearby internet drafts directory. Perhaps 'support for\n> particular subsets of ISO-10646' might fit into this category.\n\nI am rather thinking about 'need for..' than 'support for..'.\n\nMaybe it is the most practical way.  But no mechanism is in place yet,\nwhile overloading the language header (and associated inventiveness with\nnew HTML tags) can be done now... \n\nCome to think of it, putting 'particular subsets of ISO-10646' under\nfeature tag registration wouldn't work.  Other protocols like mail\npresumably will also need a way to say \"this is Latin42 characters\nencoded with UTF-8'.  I don't think that a HTTP/HTML/Web specific\nfeature tag registration can take over the IANA charset registry's\nfunction.\n\nBTW It seems those drafts specifically exclude \"MIME type, charset, \nand language\" from the new feature tags.  Probably because they are\ntoo essential.  For all practical purposes Hebrew characters encoded\nas UTF-8 (or raw 16-bit) *is* a different charset fro Greek characters\nencoded the same way.\n\n  Klaus\n\n\n\n", "id": "lists-010-12922244"}, {"subject": "Re: AcceptCharset suppor", "content": "It's important to look at content negotiation from the point of view\nof the content PROVIDERS, and then design the protocol so that clients\ncan ask for things that providers are willing to provide. In general,\nit does us no good to let clients ask for things that information\nsources aren't really willing to deal with anyway.\n\nIn general, content is available in one language and one charset and\nnon-negotiation, and in general, this is a good idea. Sometimes,\nif content is negotiable, content providers might want to provide a\nFEW alternatives, but not many, and certainly not lots of\nsubvariants. \n\nGiven this situation, \"expressivity of desires\", although nice in the\nabstract, doesn't really do us much good, if the desires we're\nexpressing aren't really things that are variable. So, while 'feature\nnegotiation' might include things like 'screen size', it's hard to\nimagine a content provider doing anything USEFUL with the factoid that\n'this client can render latin42 characters encoded with UTF8' in\ncomparison with a client that doesn't say anything like that.\n\nPerhaps one of the requirements for 'feature registration' should be a\nthat the registration form contain N different URLs where the content\nactually differs depending on whether the feature is or is not\npresent.\n\nAt least then others would have some test cases.\n\nLarry\n\n\n\n", "id": "lists-010-12935334"}, {"subject": "Re: AcceptCharset suppor", "content": "Klaus Weide:\n>\n[...using feature negotiation to negotiate on UTF-8....]\n\n>Maybe it is the most practical way.  But no mechanism is in place yet,\n>while overloading the language header (and associated inventiveness with\n>new HTML tags) can be done now... \n\nOverloading a HTTP header and adding HTML tags will take _much_ more time\nthan waiting for feature negotiation to be in place.\n\nBut skimming the UTF-8 specification, I gather that UTF-8 is an encoding\nmechanism, not a character set.  HTTP offers the\nAccept-Encoding/Content-encoding headers to negotiate on this.  Or does\nusing Accept-Encoding only shift the problem to negotiating which part\nof UCS you can render?\n\nWhen we reviewed the Accept-* header definitions for HTTP/1.1 early this\nyear, we did not discuss the particular problem of character sets which\ncould only be partially rendered, as would often be the case with unicode\nstuff.  It is certainly possible that HTTP/1.1 cannot solve this problem,\nand maybe HTTP/1.1 + feature negotation also can't solve it.\n\nHowever, in the http-wg, we are very reluctant to do things like overload the\nlanguage header; it is felt that adding more special-purpose complexity will\ndecrease the useful lifetime of the HTTP/1.x protocols.  The feature\nnegotiation framework exists to keep negotiation complexity out of the main\nprotocol, so if the choice is between overloading headers and using feature\nnegotiation, we will want to use feature negotiation, even if the feature\ntags look a bit strange.\n\n>Come to think of it, putting 'particular subsets of ISO-10646' under\n>feature tag registration wouldn't work.  Other protocols like mail\n>presumably will also need a way to say \"this is Latin42 characters\n>encoded with UTF-8'.\n\nOther protocols can use registered feature tags if they need to say the same\nthings.  HTTP borrowed media types from MIME mail, and MIME mail can borrow\nfeature tags from HTTP.  It has already been recognised that feature tags\ncould be useful for other protocols (and for conditional HTML).\n\n>  I don't think that a HTTP/HTML/Web specific\n>feature tag registration can take over the IANA charset registry's\n                              ^^^^^^^^^\n>function.\n\nWe are not aiming to take over any existing IANA registry.\n\n>BTW It seems those drafts specifically exclude \"MIME type, charset, \n>and language\" from the new feature tags.  Probably because they are\n>too essential.\n\nI don't know what you mean by `too essential', but \"MIME type, charset, and\nlanguage\" were excluded because we don't want to duplicate existing IANA\nregistries.  The registration draft does allow you to use feature tags to\nnegotiate on (new) charset-type things _if_ these new things cannot be\nhandled by the existing mechanisms.\n\n  For all practical purposes Hebrew characters encoded\n>as UTF-8 (or raw 16-bit) *is* a different charset fro Greek characters\n>encoded the same way.\n\nSo you could say:\n\n Content-Type: text/html;charset=<hebrew>\n Content-Encoding: utf-8\n\nand if you have a mixed language document:\n\n Content-Type: text/html;charset=<hebrew>;charset=<latin-x>\n Content-Encoding: utf-8\n\nOn the other hand, using feature tags, you could say:\n\n Content-Type: text/html;charset=utf-8\n Content-Features: utf-8-cs=\"<hebrew>\" utf-8-cs=\"<latin-x>\"\n\n\n>  Klaus\n\nKoen.\n\n\n\n", "id": "lists-010-12944339"}, {"subject": "Re: AcceptCharset suppor", "content": "# Content-Type: text/html;charset=utf-8\n# Content-Features: utf-8-cs=\"<hebrew>\" utf-8-cs=\"<latin-x>\"\n\nThere's no real point to this, though. The text/html;charset=utf8\nis enough to tell you how to interpret the body, and the body itself\nwill tell you which repertoire(s) are used.\n\n# 9.3 Content-Features\n#   The Content-Features response header can be used by a server to\n#   indicate how the presence or absence of particular features in the\n#   user agent affects the overall quality of the response.\n#       Content-Features = \"Content-Features\" \":\" feature-list\n\nhow does the 'content-features' actually indicate this? This is\nconfusing to me. Is content-features actually useful at all?\nIn general, you don't want to list ALL the features that a given piece\nof content might exhibit, you only want those that are used to\ndistinguish one content from another for the purpose of transparent\nfeature negotiation.\n\nLarry\n\n\n\n", "id": "lists-010-12955854"}, {"subject": "Re: AcceptCharset suppor", "content": "I think we should be very cautious about overloading the MIME charset\nor Accept-charset to mean more than just a character encoding.\n\n(I'll like to put in a word for reviving \"Charset considered\nHarmful as an informational RFC\" ;)\n\nIn the context of HTML, the issues of what characters can be usefully used\nor rendered as gypths seem more closely allied to SGML than\nthe MIME transport layer. (Though I don't know any SGML mechanism i\nto solve it all.)\n\nRemember that I could send all these funky characters using numeric\ncharacter references, and a charset of US-ASCII.\n\nA true multi-lingual document is likely to have a mix of languages\nand scripts. That's why we have LANG attributes, to help the\nsoftware out in choosing appropriate fonts, hyphnation methods, spelling\ndictionaries, etc. We haven't really addressed all the possible cases \nwhere one language uses multiple scripts/writing systems.\n\n(Hindi/Urdu _may_ be one such example I'm just guessing here.)\n\nI'd like to see more people implement the i18n spec before we fight\nabout what comes next, however...\n\n\n\n", "id": "lists-010-12964735"}, {"subject": "Re: suggested wording concerning Hos", "content": "Dave Kristol:\n>  > > DMK:\n>  > > I've noticed a cumbersome locution spreading through the HTTP/1.1\n>  > > draft that I would like to cut off.  Here's an example:\n>  > > \n>  > >     ... and the Host request header (present if the request-URI is not\n>  > >     an absoluteURI) ...\n>  > \n>  > Roy:\n>  > Argh, where did that come from?  The Host header is ALWAYS present in\n>  > HTTP/1.1.  It is never removed, not even when the full-URI is present.\n> > It will not be removed until HTTP/2.0, which is a different specification.\n>\n>The words were in Koen Holtmann's mailing on content negotiation.  I\n>agree with your point that Host is always required, so that's an error\n>in Koen's wording.\n\nYes, the addition of `(present if...)' seems to be an error in my\nwording.  I must be slightly out of sync on the host issue: I thought\nthat a user agent could omit the Host header when talking to proxies.\n\n>But it's the intent of the wording I'm trying to get at.  There will be\n>other places where someone is trying to say, \"the host that appears in\n>absoluteURI, or, if there is no absoluteURI, the host that appears in\n>the Host request header.\"  I want to shorten that locution to\n>\"request-host\".\n\nI just went through the old 1.1 draft, and there are plenty of other\nplaces where someone is trying to say this.  The current language used\nis `the resource identified by the request-URI'.\n\nThe root of the problem seems to be that the language `the resource\nidentified by the request-URI' is inaccurate.  It has always been\ninaccurate (the resources identified by http://a.com/index and\nhttp://b.com/index are different, but have the same request-URI in\nrequest messages to origin servers), but this inaccuracy has become\ntoo great to endure in a web where we have vanity hostnames mapping to\nthe same server.\n\nI do not think that changing\n\n `the resource identified by the request-URI'\n\nto\n\n `the resource identified by the request-URI and request-host'\n\nis radical enough.  It has some problems, because sometimes, the\nrequest-host must be ignored.  After a close look at the current 1.1\ndraft, I have concluded that we can solve the problem by changing all\nexisting occurrences of\n\n `the resource identified by the request-URI'\n\nand my new\n\n `the resource identified by the request-URI and the Host\n  request header'\n\nto\n\n `the resource identified by the request'.\n\nIt would also be good to add explicit language about how a request\nmessage identifies a resource to Section 5.1.2.  I have found plenty\nof language there on how to make a request message if you have the\nfull resource URI, but not much about deriving the full resource URI\nfrom an incoming request message.\n\nI would be willing to write language for section 5.1.2, but I think\nthe Host issue owner is better qualified to do so.\n\n>Dave Kristol\n\nKoen.\n\n\n\n", "id": "lists-010-1296839"}, {"subject": "Re: AcceptCharset suppor", "content": "Larry Masinter:\n>\n># Content-Type: text/html;charset=utf-8\n># Content-Features: utf-8-cs=\"<hebrew>\" utf-8-cs=\"<latin-x>\"\n>\n>There's no real point to this, though. The text/html;charset=utf8\n>is enough to tell you how to interpret the body, and the body itself\n>will tell you which repertoire(s) are used.\n\nYes.  Consider the above a bad example.  I should have written:\n\n Accept-Charset: utf-8\n Accept-Features: utf-8-cs=\"<hebrew>\", utf-8-cs=\"<latin-x>\", *\n\nbecause we are really talking about how the user agent can make its\ncapabilities known to the server. \n\nThe content-features header is not really useful.  It is only there for\nsymmetry with Accept-Features.  Even if it is present in a response, it is\nnot supposed to list all features used by the content, but only the features\nthat were negotiated on.  You should be able to know which features to use\nby looking at the content itself.\n\n>Larry\n\nSorry for the confusion,\n\nKoen.\n\n\n\n", "id": "lists-010-12972798"}, {"subject": "Re: AcceptCharset suppor", "content": "Larry Masinter:\n[...]\n>Perhaps one of the requirements for 'feature registration' should be a\n>that the registration form contain N different URLs where the content\n>actually differs depending on whether the feature is or is not\n>present.\n>\n>At least then others would have some test cases.\n\nWe certainly thought about such a requirement when we were writing the\ndocument, but we could not figure out how to put it in.\n\nThe problem is that, if these N different URLs already exist, this would be\nproof that negotiation on the feature can already be done without\nregistering a new feature tag.\n\nIn general, it is hard to add an `actual usefullness' requirement without\nhaving a review board in the loop, and we don't want to have a review board.\n\nWe do have the following field in the registration form:\n\n\n   Applications or sites which will use this feature tag:       [optional]\n\n    | For applications, also specify the number of the first version\n    | which will use the tag.\n\n\nthe idea is that, if the author leaves this blank, this would be a good\nwarning sign that you should not take the tag too seriously.\n\n\n>Larry\n\nKoen.\n\n\n\n", "id": "lists-010-12981877"}, {"subject": "Re: AcceptCharset suppor", "content": "Koen Holtman writes:\n\n> But skimming the UTF-8 specification, I gather that UTF-8 is an encoding\n> mechanism, not a character set.\n\nWell, no. UTF8 is an encoding of characters. It implies the character\nrepertoire of ISO 10646. So it is a charset in MIME sense, including\nthe specific character definitions of 10646. You cannot use UTF8\nto encode Japanese X0208 for example.\n\nKeld\n\n\n\n", "id": "lists-010-12990868"}, {"subject": "Re: AcceptCharset suppor", "content": "You know, you can't register a DNS name until you have a DNS server\nthat already serves the name. So someone could put up a web server\nthat does feature negotiation using an unregistered tag just as part\nof the registration process, to let those who are reviewing the\nregistration see just what it is that's being registered.\n\nI wouldn't suggest people run it as their main server on their home\npage, but just _somewhere_ that's publicly available. And if you can't\nsupply URLs, perhaps you could supply just the varying content.\n\nI think the 'mail' context for feature negotiation might be as\nmultipart/alternative, and inside multipart/alternative,\ncontent-features might make sense.\n\nIt might be that we'd want to separate out \"features & feature\nregistration\" into one draft outside of the \"content negotation\"\ndraft, just in order to support additional applications.\n\nI hope to have a good discussion about features & feature registration\nnext week at IETF.\n\nLarry\n\n\n\n", "id": "lists-010-12998826"}, {"subject": "Re: FW: please get me off!!!", "content": "Sorry to bother so many people but I do not know the\nemail of the list manager.  I wish to be taken off the\nmailing list.  Thank you.\n\nchua\nlchua@netcom.com\n\n\n\n", "id": "lists-010-13006974"}, {"subject": "Re: AcceptCharset suppor", "content": "On Sun, 8 Dec 1996, Keld J|rn Simonsen wrote:\n\n> Koen Holtman writes:\n> \n> > But skimming the UTF-8 specification, I gather that UTF-8 is an encoding\n> > mechanism, not a character set.\n> \n> Well, no. UTF8 is an encoding of characters. It implies the character\n> repertoire of ISO 10646. So it is a charset in MIME sense, including\n> the specific character definitions of 10646. You cannot use UTF8\n> to encode Japanese X0208 for example.\n\nExactly. Just see what UTF is standing for:\nUCS Transfer Format. Now UCS is Universal Character Set, which\nis the character set defined by ISO 10646.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13014299"}, {"subject": "please get me off!!!", "content": "-- Fabien BERNARD\n\nCoPlaNet\n50-54 rue de Silly\n92100 BOULOGNE BILLANCOURT\nTel: 01 49 09 85 78\nFax: 01 49 09 85 82\nEmail: fabien@coplanet.fr\n\n-------------------------------------------------------------------\nYou look like you're pretty groovy, or if you want something visual,\nThat's not too abysmal, we could take in an old Steve Reeves movie.\n\n-- Frank'n Furter.\n-------------------------------------------------------------------\n\n\n\n", "id": "lists-010-13023206"}, {"subject": "Re: AcceptCharset suppor", "content": "# Yes, it will; but the whole point of entity header fields seems to \n# be to have essential metainformation available without/before peeking\n# into the body.\n\n# Attempt to define \"essential\":  Essential metainformation is\n# metainformation that enables a client to make decisions about what to do\n# with the content which have to be made (or should be made) before looking\n# at the content.  Examples for a Web browser: whether to render, or start\n# a file save dialog, or invoke an external viewer (and which one).\n\nI agree that the issue is over what constitutes \"essential\". However,\nthe process hasn't been taken to its logical limit. If you have two\ndifferent postscript viewers, one of which does a good job with Type 1\nfonts and another of which does a good job with Truetype fonts, you\nmight wish to claim that it is \"essential\" that you know which class\nof fonts are in a postscript document before deciding which external\nviewer to invoke. If both of these viewers only deal with Postscript\nlevel 1, but you have a Postscript level 2 printer, then you might\nwant to claim that it is \"essential\" to know the postscript level\nbefore deciding whether to attempt to view the document or to save it\nto a file and then print it.\n\nIn these cases, though, the nature of the fonts used in the document\nand the level of Postscript used are not considered \"essential\",\npresumably because the decision is based on a limitation on the\nexternal viewer rather than something intrinsic about the data stream.\n\nWith the now freely available Bitstream unicode font and other pieces\nof technology being developed for dynamic delivery of fonts, perhaps\nwe might want to view the inability to view sub-repertoires of 10646\nas limitations of particular implementations rather than an intrinsic\nproperty. \n\n# The example given earlier by Larry Masinter, about a browser understanding\n# HTML 3.5 tables but not the \"border\" parameter, would not be about\n# essential information; it is unlikely that a client has different\n# rendering processes available to choose from, of which one understands \n# \"border\" and the other does not.\n\nThe problem is that in general it is impossible to list all of the\nfeatures that _might_ be useful. It's for that reason that\n'content-features' as a general entity header field, to be sent with\neach transmission, doesn't make a lot of sense. I can imagine limiting\nits use to exactly those features that were the subject of\nnegotiation, and only in those cases where there were multiple\nrenderings available and the features in question characterized the\ndifferences between the renderings.\n\n# Ok but that character (sub-)repertoire would also be useful (\"essential\"\n# in many cases) for non-nogotiating clients.  [Of course you may think\n# there shouldn't be any non-negotiating clients left, but that probably\n# will take a while.]\n\nIf there's no negotiation, then there's really no decision to be made:\nattempt to render the document. Expect the rendering program to fail\ngracefully.\n\n# I think charset (sub-)repertoire information should be available without\n# looking at the content.  That may be less of a concern for monolithic\n# Web browsers prevalent today.  But the protocol shouldn't be restricted\n# to that paradigm.\n\nYou need to look at the protocol from the point of view of the\ninformation providers, not the consumers. If providers have multiple\nrenderings of a single document, then they'll want to convey the\ninformation about how to distinguish the renderings. From this comes\nfeature negotiation.  However, if a provider has a single rendering,\nit's unreasonable to expect that they label the rendering with\nanything other than the information that's necessary to interpret the\nrendering. That's what is _essential_. Since most documents won't be\nlabelled with non-essential information (character subrepertoires),\nclients will HAVE to be able to cope with documents that don't have\nthat labelling (look in the body).\n\nI'm not a fan of the 'everything not mandatory is forbidden' school:\nwe might ALLOW a content-features tag anywhere, and if sub-repertoires\nof 10646 are useful feature designations, so be it. I just wouldn't\nwant to REQUIRE it, or even encourage it.\n\nLarry\n\n\n\n", "id": "lists-010-13030513"}, {"subject": "Re: AcceptCharset suppor", "content": "On Sat, 7 Dec 1996, Klaus Weide wrote:\n\n> Saying \"I can understand 10646\" or \"I can understand UTF-8\" practically\n> just means that I can decode that character encoding.  That is on the\n> same level as saying \"I can understand 8-bit character sets\" without\n> specifying which.  If anything more detailed is too fine-grained to\n> really matter then I don't see why anybody should currently bother \n> to use Accept-Charset: ISO-8859-2 etc.\n\nOh no, definitely not. Saying you understand UTF-8, or anything else,\nmeans that you can decode and you know which characters you can\nrender and which not. Just taking arbitrary glyphs for arbitrary\ncharacters, as you would when e.g. renderig EBCDIC with a\nISO-8859-2 font, is absolutely forbidden.\nThe HTML I18N spec does not exactly specify what you have to\ndo with charcters you cannot render, but it is clear enough\nto say that this should be done in a way that allows the\nreader to distinguish between correctly rendered characters\nand characters that couldn't be rendered.\n\n\n> With the given structure of the MIME \"charset\" parameter (and therefore\n> the Accept-Charset header), the logical thing to at least preserve\n> what currently can be expressed w.r.t. repertoire would be to register\n> lots of additional charsets: we'd then have ISO-10646-Unicode-Latin2,\n> ISO-10646-Unicode-Latin3, ISO-10646-Unicode-Latin4, and so on.  Well\n> I can see why that isn't very inviting, looks like a big can of worms...    \n\nIt definitely is! Look at RFC 1815 (ignored by everybody) to\nsee how this can be brought to extremes.\n\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13042763"}, {"subject": "Re: AcceptCharset suppor", "content": "> I think charset (sub-)repertoire information should be available without\n> looking at the content.  That may be less of a concern for monolithic\n> Web browsers prevalent today.  But the protocol shouldn't be restricted\n> to that paradigm.\n\nAs I've already noted, for HTML, the character repertoire is a function of the\nSGML \"document character set\" *NOT* the character encoding (aka the\nMIME charset). So restrictions on the character repertoire (or its\nrendering or usage) need to be somehow expressed at the SGML level,\nnot confused with the charset.\n\nThe problem of Unicode/ISO character standard versioning is a bit perplexing,\nbut we need to remember that ISO 10646 is playing two roles when we talk \nabout UTF8 as a encoding and a document character set (though maybe not \nin the same version)  \n\nTo get numeric refences to work right we need to treat this in a way that\nworks for other charsets besides UTF8 and friends, (i.e. ISO-8859-2\nor ISO-2022-JP with numeric references to Korean Hangul codes from\nISO 10646)\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-010-13052191"}, {"subject": "Re: AcceptCharset suppor", "content": "On Sun, 8 Dec 1996, Koen Holtman wrote:\n> Klaus Weide:\n> >\n> [...using feature negotiation to negotiate on UTF-8....]\n> \n> >Maybe it is the most practical way.  But no mechanism is in place yet,\n> >while overloading the language header (and associated inventiveness with\n> >new HTML tags) can be done now... \n> \n> Overloading a HTTP header and adding HTML tags will take _much_ more time\n> than waiting for feature negotiation to be in place.\n\nLet's hope so :).  However, with overloading I meant treating \n{Content,Accept}-Language headers (and related HTML tags or attributes)\nas carrying character repertoire meaning - which is happening now. \n\n> But skimming the UTF-8 specification, I gather that UTF-8 is an encoding\n> mechanism, not a character set.  HTTP offers the\n> Accept-Encoding/Content-encoding headers to negotiate on this.  Or does\n> using Accept-Encoding only shift the problem to negotiating which part\n> of UCS you can render?\n\nIt would be *nice* if UTF-8 could be treated that way, like a C-E (or\nC-T-E, for mail).  It could then be used for labelling and negotiation\nof character encoding orthogonal to the question of repertoire.\nBut that isn't the case.\n\n> When we reviewed the Accept-* header definitions for HTTP/1.1 early this\n> year, we did not discuss the particular problem of character sets which\n> could only be partially rendered, as would often be the case with unicode\n> stuff.  It is certainly possible that HTTP/1.1 cannot solve this problem,\n> and maybe HTTP/1.1 + feature negotation also can't solve it.\n\n[...] \n> >  I don't think that a HTTP/HTML/Web specific\n> >feature tag registration can take over the IANA charset registry's\n>                               ^^^^^^^^^\n> >function.\n> \n> We are not aiming to take over any existing IANA registry.\n\nI didn't mean that you were trying to do that.  But using feature tags\nfor negotiating (labelling) sub-repertoire, i.e. \"which characters can\nbe (are) used\" with utf-8, would effectively amount to using them for\na function that could up to recently be done using IANA registered\ncharsets alone.\n\n  Klaus\n\n\n\n", "id": "lists-010-13060534"}, {"subject": "Re: AcceptCharset suppor", "content": "On Sun, 8 Dec 1996, Koen Holtman wrote:\n\n> Larry Masinter:\n> >\n> ># Content-Type: text/html;charset=utf-8\n> ># Content-Features: utf-8-cs=\"<hebrew>\" utf-8-cs=\"<latin-x>\"\n> >\n> >There's no real point to this, though. The text/html;charset=utf8\n> >is enough to tell you how to interpret the body, and the body itself\n> >will tell you which repertoire(s) are used.          ^^^^^^^^^^^^^^^\n   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nYes, it will; but the whole point of entity header fields seems to \nbe to have essential metainformation available without/before peeking\ninto the body.\n\nAttempt to define \"essential\":  Essential metainformation is\nmetainformation that enables a client to make decisions about what to do\nwith the content which have to be made (or should be made) before looking\nat the content.  Examples for a Web browser: whether to render, or start\na file save dialog, or invoke an external viewer (and which one).\n\nThe example given earlier by Larry Masinter, about a browser understanding\nHTML 3.5 tables but not the \"border\" parameter, would not be about\nessential information; it is unlikely that a client has different\nrendering processes available to choose from, of which one understands \n\"border\" and the other does not.\n\n> Yes.  Consider the above a bad example.  I should have written:\n> \n>  Accept-Charset: utf-8\n>  Accept-Features: utf-8-cs=\"<hebrew>\", utf-8-cs=\"<latin-x>\", *\n> \n> because we are really talking about how the user agent can make its\n> capabilities known to the server. \n\nOk but that character (sub-)repertoire would also be useful (\"essential\"\nin many cases) for non-nogotiating clients.  [Of course you may think\nthere shouldn't be any non-negotiating clients left, but that probably\nwill take a while.]\n\n> The content-features header is not really useful.  It is only there for\n> symmetry with Accept-Features.  Even if it is present in a response, it is\n> not supposed to list all features used by the content, but only the features\n> that were negotiated on.  \n\nThat would make Content-Features less useful for carrying additional\ninformation in other protocols, e.g. mail.\n\n> You should be able to know which features to use\n> by looking at the content itself.\n\nI think charset (sub-)repertoire information should be available without\nlooking at the content.  That may be less of a concern for monolithic\nWeb browsers prevalent today.  But the protocol shouldn't be restricted\nto that paradigm.\n\n  Klaus\n\n\n\n", "id": "lists-010-13070255"}, {"subject": "Distributed interactive sims on the We", "content": "If one were to create a distributed interactive simulation on the Web, \nusing Netscape or some other Web browser as an interface, what current methods \nand request headers (GET, POST, etc.) are available for interacting with \na server?  For example, is it possible to create a SimCity type game \nwith an image map as an interface, and clicking different regions of the \nimage, representing buildings, countries, or whatever, will send different \nrequests to a server, maybe for new images.  Is this possible with current \nHTTP browsers and servers, using HTTP methods and fields?  If not, then is \nJava or some kind of scripting language needed to do this?  Any \nsuggestions anyone may have would be appreciated.  \n\n\n\n", "id": "lists-010-1307410"}, {"subject": "Re: AcceptCharset suppor", "content": "On Mon, 9 Dec 1996, Klaus Weide wrote:\n\n> On Sun, 8 Dec 1996, Keld J&o/rn Simonsen wrote:\n> > Koen Holtman writes:\n> > \n> > > But skimming the UTF-8 specification, I gather that UTF-8 is an encoding\n> > > mechanism, not a character set.\n> > \n> > Well, no. UTF8 is an encoding of characters. It implies the character\n>                                                ^^^^^^^^^^^^^^^^^^^^^^^^\n> > repertoire of ISO 10646. So it is a charset in MIME sense, including\n>   ^^^^^^^^^^^^^^^^^^^^^^^\n> > the specific character definitions of 10646.\n> \n> If that is taken seriously, then \"Accept-Charset: utf-8\" cannot be used\n> to just send information about what character encoding a client can\n> decode.  It implies that (at least when sent in the encoding of utf-8)\n> all characters from the 10646 repertoire are acceptable.\n\nYes, all characters are acceptable up to the level of acceptability\nthat the HTML I18N spec requires. Which is not very much.\n\n\n> It seems predictable that e.g. \"Accept-Charset: koi8-r,iso-8859-1,utf-8\"\n> will be used to indicate \"documents containing characters which are \n> also in koi8-r and latin-1 characters are acceptable in utf-8 encoding\", \n> because there is currently no better way to express that (other than\n> maybe with language tags, which has other problems already mentioned:\n> e.g. transliteration/transcription, languages that do not imply exactly\n> one character repertoire).\n\nThere is no real need to express subrepertoiries.\n\n\n> This is of course not specific to HTTP or the Web, protocols without\n> negotiation like mail need charset labelling.  A simple MIME compliant\n> MUA should have sufficient information from message headers to dispatch \n> to the appropriate viewer.  In the pre-UTF era this was reliably possible \n> e.g. with metamail (given the correct charset parameter and availability of\n> appropriate codepage).  With messages labelled \"utf-8\", heuristics have to \n> be involved.\n\nThe concept of having a different viewer for every \"charset\" is still\nwidespread, but rather outdated and doomed. For a network computer\nrunning Java, being able to display all of Unicode/ISO 10646 is\njust a must. It might be a pain for older and more expensive\ntechnology to follow, but that's their problem, not ours.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13080715"}, {"subject": "Re: AcceptCharset suppor", "content": "On Tue, 10 Dec 1996, Larry Masinter wrote:\n\n> # Yes, it will; but the whole point of entity header fields seems to \n> # be to have essential metainformation available without/before peeking\n> # into the body.\n> \n> # Attempt to define \"essential\":  Essential metainformation is\n> # metainformation that enables a client to make decisions about what to do\n> # with the content which have to be made (or should be made) before looking\n> # at the content.  Examples for a Web browser: whether to render, or start\n> # a file save dialog, or invoke an external viewer (and which one).\n> \n\n> With the now freely available Bitstream unicode font and other pieces\n> of technology being developed for dynamic delivery of fonts, perhaps\n> we might want to view the inability to view sub-repertoires of 10646\n> as limitations of particular implementations rather than an intrinsic\n> property. \n\n> # Ok but that character (sub-)repertoire would also be useful (\"essential\"\n> # in many cases) for non-nogotiating clients.  [Of course you may think\n> # there shouldn't be any non-negotiating clients left, but that probably\n> # will take a while.]\n> \n> If there's no negotiation, then there's really no decision to be made:\n> attempt to render the document. Expect the rendering program to fail\n> gracefully.\n\nVery well put. The HTML I18N spec does not require to render any\ncharacter with an appropriate glyph. It gives even some clues\nabout what \"fail gracefully\" might mean or not. Strictly speaking,\nif a browser says Accept-Charset: ISO-8859-2, this only means\nthat it understands how to may these characters to Unicode. It\ndoes not mean that it will be able to render them. It might\nhave a look at the document, find a lot of e.g. Chinese\nnumerical character references, and choose a Chinese codepage\ninstead of the ISO-8859-2 codepage (if it is as old-fashioned\nas to need a single codepage for the whole document).\nOf course, you are usually safe to assume that ISO-8859-2\ncan and will indeed be displayed, but the is no absolute\nguarantee. The only thing the recipient (browser) should\nguarantee is that the reader understands to what extent\nthe display is a faithful representation of the document.\n\n \n> # I think charset (sub-)repertoire information should be available without\n> # looking at the content.  That may be less of a concern for monolithic\n> # Web browsers prevalent today.  But the protocol shouldn't be restricted\n> # to that paradigm.\n\nOne big problem advocates of subrepertoire information usually don't\nconsider in enough detail is how to express all the different\nsubrepertoires. For a set of N things, there are 2**N subrepertoires.\nFor Unicode 2.0, N is 38,885. Even if not all of these subrepertoires\nare relevant, it is very difficult to design a system that will\nallow to designate all useful subrepertoires without either\nsending around too much information or having to keep all this\ninformation available (and updated every time a new subrepertoire\nis defined) on each client.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13090671"}, {"subject": "Proxy authentication draf", "content": "Here is the first cut t the auth draft\n\n> Network Working Group                                         Josh Cohen\n> Internet-Draft                             Netscape Communications Corp.\n>                                                          5 December 1996\n\n\n>              HTTP Proxy Authorization Header Modifications\n>                      <draft-cohen-httpauth-00.txt>\n\n> Status of this Memo\n\n>    This document is an Internet-Draft.  Internet-Drafts are working\n>    documents of the Internet Engineering Task Force (IETF), its areas,\n>    and its working groups.  Note that other groups may also distribute\n>    working documents as Internet-Drafts.\n\n>    Internet-Drafts are draft documents valid for a maximum of six months\n>    and may be updated, replaced, or obsoleted by other documents at any\n>    time.  It is inappropriate to use Internet- Drafts as reference\n>    material or to cite them other than as ``work in progress.''\n\n>    To learn the current status of any Internet-Draft, please check the\n>    ``1id-abstracts.txt'' listing contained in the Internet- Drafts\n>    Shadow Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n>    munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n>    ftp.isi.edu (US West Coast).\n\n> Abstract\n\n>    Currently, the HTTP specification allows for client request proxy\n>    authorization via the Proxy-Authenticate headers. Servers can use\n>    'Proxy-Authenticate:' to prompt a client to send 'Proxy-\n>    Authorization:' including appropriate credentials for use of the\n>    proxy.\n\n>    While the 'realm', a parameter to the authentication scheme, allows\n>    usage of different credentials based on the URL being processed,\n>    there is no specific way to link different credentials with different\n>    proxies a chain.  This makes is impossible to have more than one\n>    proxy in a chain which requires authentication based on different\n>    credential databases.\n\n\n\n\n\n\n\n\n\n\n\n> J. Cohen                                                        [Page 1]\n\n\n\n\n\n> INTERNET-DRAFTHTTP Proxy Authorization Header Modifications5 December 1996\n\n\n> Header Modifications\n\n>  WWW-Authenticate:\n\n>    In section 14.46 of the HTTP/1.1 Internet Draft, it reads\n\n>         WWW-Authenticate  = \"WWW-Authenticate\" \":\" 1#challenge\n\n>    this draft proposes to change that to:\n\n>         WWW-Authenticate  = \"WWW-Authenticate\" \":\" 1#(realm-id\n>    challenge)\n\n>  Authorization:\n\n>    In section 14.8 of the HTTP/1.1 Internet Draft, it reads\n\n>        Authorization = \"Authorization\" \":\" credentials\n\n>    this draft proposes to change that to:\n\n>        Authorization = \"Authorization\" \":\" realm-id \";\" credentials\n\n>  Proxy-Authenticate:\n\n>    In section 14.33 of the HTTP/1.1 Internet Draft, it reads\n\n>         Proxy-Authenticate = \"Proxy-Authenticate\" \":\" challenge\n\n>    this draft proposes to change that to:\n\n>         Proxy-Authenticate = \"Proxy-Authenticate\" \":\" realm-id challenge\n\n>  Proxy-Authorization:\n\n>    In section 14.34 of the HTTP/1.1 Internet Draft, it reads\n\n>        Proxy-Authorization = \"Proxy-Authorization\" \":\" credentials\n\n>    this draft proposes to change that to:\n\n>        Proxy-Authorization = \"Proxy-Authorization\" \":\" realm-id \";\"\n>    credentials\n\n\n\n\n\n\n\n\n> J. Cohen                                                        [Page 2]\n\n\n\n\n\n> INTERNET-DRAFTHTTP Proxy Authorization Header Modifications5 December 1996\n\n\n>  challenge Definition\n\n>    In section 11 of the HTTP/1.1 Internet Draft, it reads\n\n>         auth-scheme = token\n\n>    this draft proposes to change that to:\n\n>         auth-scheme = \"auth-scheme\" \"=\" token\n\n>    and where it reads:\n\n>        challenge = auth-scheme 1*SP realm *( \",\" auth-param )\n\n>    this draft proposes to change that to:\n\n>        challenge = auth-scheme 1*SP realm *( \";\" auth-param )\n\n\n> realm-id Field Format\n\n>        realm-id = \"realm-id\" \"=\" ( token | quoted-string )\n\n> Discussion\n\n>            While the original intentions of the draft meant to provide a\n>    way to authenticate multiple proxies in a chain, the solution has\n>    become more involved.  To implement the change, and satisfy\n>    consistency rules in HTTP/1.1, specifically rules about header\n>    collapsing, syntactical changes are required to the definitions of\n>    the 'challenge'.  This can affect both WWW-Authenticate as well as\n>    Proxy-Authenticate, as well as add to the backward incompatibility of\n>    version 1.1 with previous versions.\n\n>            In addition to the Proxy-Authenticate headers, we propose to\n>    modify the WWW-Authenticate: and Authorization: headers as well.\n>    This is to maintain syntactic consistency.  With the modification of\n>    the challenge definition, and the inclusion of the 'realm-id' field,\n>    the challenge becomes a set of paramters to the value of 'realm-id'.\n>    We feel this is correct since the challenge is unique to the server\n>    requesting the credentials.  Unfortunately, this has ill effects for\n>    the current definition of the WWW-Authenticate: and Authorization:\n>    headers in the HTTP/1.1 Draft.  Without the 'realm-id' field, the\n>    challenge becomes a set of parameters with no associated header field\n>    value.  While contemplating this it became apparent that there would\n>    be benefits to having the 'realm-id' field in those headers as well.\n\n>            Currently, the 'realm' parameter serves as both a user prompt\n\n\n\n> J. Cohen                                                        [Page 3]\n\n\n\n\n\n> INTERNET-DRAFTHTTP Proxy Authorization Header Modifications5 December 1996\n\n\n>    as well as an identifier to manage the scope of authorization\n>    credentials.  This is problematic at best since while the user\n>    prefers a verbose message, the protocol would seem to prefer a short,\n>    unique identification to define the scope of the credentials.  By\n>    adding the 'realm-id' field, we now have two different parameters to\n>    represent the two different functions which had been previously\n>    overloaded into a single parameter.\n\n>            Despite the extra effort required, we feel that it is a\n>    worthwhile change.  While it complicates implementations in the short\n>    term, it makes the authentication headers comply with HTTP/1.1's own\n>    rules about header collapsing, which it previously did not.  In\n>    addition to the compliance issues, this allows multiple proxies to\n>    exist in a chain which require separate authorization credentials.\n>    This type of proxy usage is becoming more widespread as caches play a\n>    more important role in the protocol itself and as organizations are\n>    deploying larger and more complex cache hierarchies.  In addition to\n>    the benefits in proxies, it allows authorization scopes to have\n>    unique identifiers separate from a user prompt which can be prone to\n>    name collisions.\n\n\n> Security Considerations\n\n>    This draft does not address any security considerations.\n\n> Author's Address\n\n>    Josh Cohen\n>    Netscape Communications Corporation\n>    501 E. Middlefield Rd\n>    Mountain View, CA 94043\n\n>    Phone (415) 937-4157\n>    EMail: josh@netscape.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> J. Cohen                                                        [Page 4]\n\n\n\n> -- \n> -----------------------------------------------------------------------------\n> Josh Cohen        Netscape Communications Corp.\n> Netscape Fire Department       \n> Server Engineering\n> josh@netscape.com                       http://home.netscape.com/people/josh/\n> -----------------------------------------------------------------------------\n\n\n-- \n-----------------------------------------------------------------------------\nJosh R Cohen /Server Engineer        josh@early.com\nNetscape Communications Corp. \n(This message is sent from my private email account to reach me for \nbusiness related issues, mailto:josh@netscape.com )\n-----------------------------------------------------------------------------\n\n\n\n", "id": "lists-010-13101516"}, {"subject": "Re: AcceptCharset suppor", "content": "Klaus Weide:\n>\n[...]\n>That would make Content-Features less useful for carrying additional\n>information in other protocols, e.g. mail.\n\nContent-Features was not designed to be useful for carrying additional\ninformation, neither in HTTP nor in other protocols, so it isn't.\n\nIf you want a header that is useful for such things, just invent one.  You\ncould even make it a PEP response header.\n\n>  Klaus\n\nKoen.\n\n\n\n", "id": "lists-010-13117383"}, {"subject": "Re: Proxy authentication draf", "content": "That's all fine and good, but it can't be deployed because it is\na completely incompatible change.  The WG has discussed the issues\ninvolved in changing the authentication syntax and decided that\nthere was no way to do so in HTTP/1.x.  A far better plan would be\nto determine what you need in the abstract and make sure that it is\ndefined that way for the next *major* version change of HTTP.\n\n.....Roy\n\n\n\n", "id": "lists-010-13125394"}, {"subject": "Re: HTTP 1.1 spec (page 15", "content": "Ulrich Windl <Ulrich.Windl@rz.uni-regensburg.de> writes:\n> I wonder what the specification says about ``\\)'' within a comment and\n> about ``\\\"'' within a quoted-string. To my interpretation of the specs\n> the character will be \"quoted\" but according to the grammar will\n> terminate the element anyhow.\n\nHmmm, that's odd -- it was supposed to say\n\n    comment        = \"(\" *( ctext | quoted-pair | comment ) \")\"\n\n    quoted-string  = ( <\"> *( qdtext | quoted-pair ) <\"> )\n\nI guess that should be put on the errata list.\n\n.....Roy\n\n\n\n", "id": "lists-010-13133017"}, {"subject": "PLEASE GET ME OFF!!!!", "content": "     Sorry to bother so many people but I do not know the email of the list\nmanager. I wish to be taken off the mailing list.  Thank you.\n\n      always;\n    Beng Kiam\n    ---------\n\n\n\n", "id": "lists-010-13141110"}, {"subject": "Reload page... without disconnect ", "content": "Hi,\n\nI was searching through www.metacrawler.com\n\nand was surprised that it reloaded a page without javascript\nand that to with different data.\n\nWhen one preses the search button.. we get a CGI response of the\npresent status of the search... and when all is done it loads \na new html file... without any java script.\n\nInfact... I had to struggle to see whether it had javascript \nbecause the cache always showed me the latest page not the\npre-reload page.\n\nI tried to telnet to port 80 and see what its doing... but it\nshuts me out before I send the http request.\n\nAny ideas of how its being done.... BTW OS/2's webexplorer refused\nto show me that intermediatory page... I use NETSCAPE other wise to\nwatch that effect.\n\nI am building a http search server and want help to use this if\nhttp protocol supports this process.\nBye and thanks.\n---RKT---\n\n    _____________________________________\n   /\\                                    \\\n   \\_|         Royans K Tharakan          |\n     |                                    |\n     |          rkt@poboxes.com           |\n     |                                    |\n     |        Tel: +91 11 4636689         |\n     |   _________________________________|_\n      \\_/__________________________________/\n\n\n\n", "id": "lists-010-13149011"}, {"subject": "Re: (ACCEPT*) Draft text for Accept header", "content": "Keld J|rn Simonsen:\n>Koen Holtman writes:\n>\n>> 10.2  Accept-Charset\n>> \n>>    The Accept-Charset request-header field can be used to indicate what\n>>    character sets are acceptable for the response. This field allows\n>>    clients capable of understanding more comprehensive or\n>>    special-purpose character sets to signal that capability to a server\n>>    which is capable of representing documents in those character\n>>    sets. The US-ASCII character set can be assumed to be acceptable to\n>>    all user agents.\n>> \n>> |  [##QUESTION TO BE RESOLVED: Apparently, the latest HTML spec says\n>> |  that iso-8859-1 can be assumed to be acceptable to all user agents.\n>> |  Should the above US-ASCII be changed to iso-8859-1??  There has\n>> |  been lots of discussion on the list, but I have not been able to\n>> |  detect a consensus opinion.##]\n>\n>My 2 cents: it soyld be iso-8859-1 that is the default, referring\n>to previous discussion for that view.\n\nBased on your comments and those by Albert Lunde, I think the\nstrongest case is for a change to iso-8859-1.\n\nI will change the draft to say iso-8859-1.  \n\nIf anyone disagrees with the change: tell me so in private e-mail\n(just say `I do not agree to iso-8859-1', there is no need to supply a\nreason).  I will report to the list on the mail I get.  If I get no\ncomments on this issue in two days, I will do a last call to establish\nthat we have consensus.\n\n>Another suggestion: there should be a quality parameter also with\n>accept-charset,  a q<1 meaning that the browser may be able to display\n>in a less readable fashion, for example in mnemonic or 10646 fallback,\n>or that it need to shift fonts or load a special programme to\n>dispaly the charset or other impeded things.\n\nThere has been some discussion that just a quality parameter on\naccept-charset does not solve all problems, because different MIME\ntypes will be rendered by different helper apps which have different\nbuilt-in charset capabilities.  You may be accepting unicode in html\ntext, but not in postscript text.\n\nBut it is clear to me that some people think that a q=.. parameter on\naccept-charset would solve at least their problems.  Adding a\nq=.. parameter won't introduce any technical problems as far as\ncontent negotiation is concerned, it could even be seen as an obvious\ngeneralization.  One could object to the addition of a q=.. parameter\non the grounds that the HTTP protocol should be kept simple.\n\nHere is what I propose: anyone who wants to have a q= parameter on\naccept-charset, sent me private mail saying that you do.  Anyone who\ndoes not want it, send me private mail saying that you do not.  I will\nreport a count in two days, and will change text depending on the\nresults.  I then will do a last call to establish that we have\nconsensus on this issue.\n\n>Keld\n\nKoen.\n\n\n\n", "id": "lists-010-1315322"}, {"subject": "help.. contentlength: 0 ", "content": "The http specs mention that content-length should not be zero... though\npeople are using it... and netscape takes it to mean \nthat 'download until disconnect'\n\nIs there a way to tell the browser without using this unpredictable\nmethod that, the data size is dynamic.\n\n\nThanx in anticipation\n---RKt---\n______________________________________________________________________\n   _     _              \n  | |   //              Royans K Tharakan       E-Mail:rkt@poboxes.com\n  | |  //                                             :    rkt@usa.net\n  | | //                Internet Consultant                           \n  | |//                 Web-Page Hosting/Designing      +91-11-4636689\n  |__/ I R T U A L      Java & JavaScript Consultant                  \nC o n s u l t a n c y  \nvirtual@TheOffice.net   When the going gets tough,the tough gets going\n______________________________________________________________________\n\n\n\n", "id": "lists-010-13157417"}, {"subject": "Re: help.. contentlength: 0 ", "content": "> The http specs mention that content-length should not be zero... though\n> people are using it... and netscape takes it to mean \n> that 'download until disconnect'\n\nThat is a bug in Navigator.  \"Content-length: 0\" means no message body.\n\n> Is there a way to tell the browser without using this unpredictable\n> method that, the data size is dynamic.\n\nNot sending a Content-Length in HTTP/1.x.  Using Transfer-Encoding: chunked\nin HTTP/1.1.\n\n....Roy\n\n\n\n", "id": "lists-010-13165887"}, {"subject": "Re: Reload page... without disconnect ", "content": ">I was searching through www.metacrawler.com\n>\n>and was surprised that it reloaded a page without javascript\n>and that to with different data.\n>\n>When one preses the search button.. we get a CGI response of the\n>present status of the search... and when all is done it loads \n>a new html file... without any java script.\n>\n>Infact... I had to struggle to see whether it had javascript \n>because the cache always showed me the latest page not the\n>pre-reload page.\n>\n>I tried to telnet to port 80 and see what its doing... but it\n>shuts me out before I send the http request.\n>\n>Any ideas of how its being done.... BTW OS/2's webexplorer refused\n>to show me that intermediatory page... I use NETSCAPE other wise to\n>watch that effect.\n>\n>I am building a http search server and want help to use this if\n>http protocol supports this process.\n\nWhat you are experimenting is probably (definitely) server push. It is a \nfeature of Netscape Navigator that is not part of HTTP 1.x. What it does \nis send a multipart content-type telling the client to replace each part \nwith the next one (thus the page changes). Basically, this forces the \nconnection to stay open (though it has nothing to do with persistant \nconnection in HTTP 1.1). Servers usually push images (to create some sort \nof animation), but some also push html text (for the same purpose that \nyou experimented).\n\nUnfortunately, all servers (including some of Netscape's !) do not \nimplement this consistently and the specs \n(http://partner.netscape.com/assist/net_sites/pushpull.html) are somewhat \nunclear. Server push has the drawback of forcing the client to keep the \nconnection open by pretending that more data is to come. It also requires \nthat the client nkows how to deal with this feature...\n\nExplorer and Navigator both implement server push...\n\nHope this helps.\n\n\n---------------------------------------------------------------\nFrederic Artru          artru@apple.com          (408) 974-5410\nSenior Engineer                                 1 Infinite Loop\nInternet Platform                           Cupertino, CA 95014\n---------------------------------------------------------------\n\n\n\n", "id": "lists-010-13173189"}, {"subject": "Re: HTTP 1.1 spec (page 15", "content": "Roy T. Fielding:\n>\n>Ulrich Windl <Ulrich.Windl@rz.uni-regensburg.de> writes:\n>> I wonder what the specification says about ``\\)'' within a comment and\n>> about ``\\\"'' within a quoted-string. To my interpretation of the specs\n>> the character will be \"quoted\" but according to the grammar will\n>> terminate the element anyhow.\n>\n>Hmmm, that's odd -- it was supposed to say\n>\n>    comment        = \"(\" *( ctext | quoted-pair | comment ) \")\"\n>\n>    quoted-string  = ( <\"> *( qdtext | quoted-pair ) <\"> )\n>\n>I guess that should be put on the errata list.\n\nGack!  HTTP/1.0 says:\n\n|   A string of text is parsed as a single word if it is quoted using\n|   double-quote marks.\n|\n|       quoted-string  = ( <\"> *(qdtext) <\"> )\n|\n|       qdtext         = <any CHAR except <\"> and CTLs,\n|                        but including LWS>\n|\n|   Single-character quoting using the backslash (\"\\\") character is not\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ \n|   permitted in HTTP/1.0.\n    ^^^^^^^^^^^^^^^^^^^^^^\n\nAnd I guess that most depoyed header parsers conform to this.  You would get\nsubtle but ugly incompatibility problems if you introduce single-character\nquoting in 1.1.  In my opinion, we can only change the parsing of quoted\nstrings if we also increase the major version number.  The 1.1 grammar is\nfine, the text is in error.\n\n>.....Roy\n\nKoen.\n\n\n\n", "id": "lists-010-13183141"}, {"subject": "Please Get Me Off!!!..", "content": "Hello...\n     \nSorry to bother so many people but I do not know the email of the list\nmanager. I wish to be taken off the mailing list.  \nThank you.\n\n      always;\n    Beng Kiam\n    ---------\n\n\n\n", "id": "lists-010-13198956"}, {"subject": "Re: Proxy authentication draf", "content": "I don't see how this proposal is completely incompatible. The\nchallenge is extended, and a new kind of authorization is\nexpected. Old clients will fail to recognize the new challenge, and so\nwon't have access. New clients will recognize both.\n\nOld clients will send the old authorization header, which new servers\nwill either recognize or not.\n\nSo what's the breakage?\n\nLarry\n\n\n\n", "id": "lists-010-13206127"}, {"subject": "Tip of the Day: 9.9c Long Distance  http://www.superhot.com/phone", "content": "Privacy Alert:  Did you know your Email address is publicly available?\n\nIf you're aware of this fact, kindly disregard this message; otherwise,\nplease note that it's been published on an unprotected, openly-linked\nweb-page, that any person or program can access at:\n\n   http://www.ics.uci.edu/~ejw/authoring/draft-whitehead-http-distreq.html\n\nThis public service announcement was brought to you by ActiveAgent,\na shareware Java webcrawler, whose development has been sponsored by:\n\nTip of the Day: 9.9c Long Distance - http://www.superhot.com/phone/\n\n\n\n", "id": "lists-010-13214336"}, {"subject": "HTTPWG summar", "content": "Here is the two-paragraph summary of the status of the HTTP working\ngroup. Minutes forthcoming:\n\nThe HTTP working group met twice. We heard about (good) performance\nresults from HTTP/1.1 pipelining, and discussed several issues that\nhave arisen as implementations of HTTP/1.1 is proceeding. Some of\nthese may result in either clarifications or revisions of the spec as\nit moves to Draft Standard.  There are a few major issues still left:\ne.g., hit metering, content negotiation, and the working group will\nkeep going for at least until the next IETF.\n\nThere is confusion in the standards arena for security-related HTTP\nprotocols: SHTTP will be revised, there are proposals for new\nauthentication methods, and to revise the SSL Tunnelling\ndraft. Somehow these all need to get sorted out.  There are many\ngroups that want to use HTTP or something like it, but with\nextensions. Some of those seem like they're appropriate (web\nauthoring) and others are more questionable. We're awaiting a new PEP\ndraft, and perhaps that will help.\n\nLarry Masinter\nChair, HTTP-WG\n\n\n\n", "id": "lists-010-13221557"}, {"subject": "Re: HTTP/1.1 Contradictio", "content": ">19.4.6 Introduction of Transfer-Encoding\n>... Proxies/gateways MUST remove any transfer coding prior to forwarding a\n>message via a MIME-compliant protocol.\n>\n>(Sounds good, not only can we remove it [per 4.3], we MUST remove it, but)\n\nIIRC, HTTP is not a MIME compliant protocol.  The spec writers are talking\nhere about proxies that forward messages to a mail gateway.  (And yeah, a\nproxy that has that feature would need to remove Trasnfer-Encoding, since\nsuch a beast will choke any mail software out there.)\n\n>13.5.2 Non-modifiable Headers\n>... A cache or non-caching proxy MUST NOT modify or add any of the following\n>fields in a response that contains  the no-transform Cache-Control\n>directive, or in any request:\n>Content-Length\n>\n>Now if we MUST remove Transfer coding, seems like we MUST add a\n>Content-Length header?\n\nSeems to me if you remove the transfer encoding, you would have to add a\nContent-Length.  I don't understand why they included that header amongst\nthose that couldn't be added/modified couldn't be added.  Let's ask them....\n\nHey HTTP-WG, why can't proxies modify/change Content-Length on no-transform\nresponses?\n\n-----\nDaniel DuBois, Traveling Coderman        www.spyglass.com/~ddubois\n   o  The Heroes of Might and Magic II Bible is here!\n      http://www.spyglass.com/~ddubois/HOMM2.html\n\n\n\n", "id": "lists-010-13229531"}, {"subject": "Re: HTTP/1.1 Contradictio", "content": "    Hey HTTP-WG, why can't proxies modify/change Content-Length on\n    no-transform responses?\n\nI believe this is because the Digest-Authentication people needed\nthat.  See \n   http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-digest-aa-05.txt\n(but you'll have to ask one of them for a definitive answer).\n\n-Jeff\n\n\n\n", "id": "lists-010-13239415"}, {"subject": "RE: HTTP/1.1 Contradictio", "content": "Imagine that you've computed an MD5 (or other) checksum over the content\nbody and various of the headers. If you change any of them, the digest\nwon't check thereafter. When the digest is used for authentication and\nas a secure integrity check, the failure of the digest to check would\nlead to some kind of security fault.  The \"no-transform\" directive was\nadded to tell proxies that changing the body or certain headers would\nlead to some kind of failure.\n\nIn the case in question (an HTTP to mail gateway), it may not be\nimportant that the recipient of the Mime-body be able to verify its\norigins and that it wasn't tampered with in transit -- an ordinary mail\nclient wouldn't know how to do so anyway. Hence, adding a Content-Length\nin the gateway might not matter.\n\n>----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Sent: Friday, December 13, 1996 3:48 PM\n>To: Daniel DuBois\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: HTTP/1.1 Contradiction \n>\n>    Hey HTTP-WG, why can't proxies modify/change Content-Length on\n>    no-transform responses?\n>\n>I believe this is because the Digest-Authentication people needed\n>that.  See \n>   http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-digest-aa-05.txt\n>(but you'll have to ask one of them for a definitive answer).\n>\n>-Jeff\n>\n>\n\n\n\n", "id": "lists-010-13246482"}, {"subject": "RE: HTTP/1.1 Contradictio", "content": "At 04:18 PM 12/13/96 -0800, Paul Leach wrote:\n>Imagine that you've computed an MD5 (or other) checksum over the content\n>body and various of the headers. If you change any of them, the digest\n>won't check thereafter. When the digest is used for authentication and\n>as a secure integrity check, the failure of the digest to check would\n>lead to some kind of security fault.  The \"no-transform\" directive was\n>added to tell proxies that changing the body or certain headers would\n>lead to some kind of failure.\n>\n>In the case in question (an HTTP to mail gateway), it may not be\n>important that the recipient of the Mime-body be able to verify its\n\nFirst let me say I'm not concerned with mail gateways, so I don't want to\nget too lost thinking about the implications of such. :)\n\nI understand what you are saying in the first paragraph, and it sounds like\nwhat you are saying applies to Transfer-Encoding as well.  Or are we\npresuming that whatever the digest-computation algorithms are in the browser\n(and the server as well!) they are both intelligent/aware enough to ignore\nthe Transfer-Encoding headers in their computations?\n\nIf we can't make that assumption (it sounds like a tenuous one), then we\nshould state somewhere in the HTTP/1.1 spec that proxies are *not* allowed\nto remove Transform-Encodings on responses/messages that have a no-transform\nCache-Control directive.\n\nAre there any other implications of this?\n\nNow I think some more about it.... Why are we specifying headers at all?\nWhy aren't they ALL un-modifiable in face of no-transform and the\npossibility of a some-headers-digested situation?\n\n-----\nDaniel DuBois, Traveling Coderman        www.spyglass.com/~ddubois\n   o  The Heroes of Might and Magic II Bible is here!\n      http://www.spyglass.com/~ddubois/HOMM2.html\n\n\n\n", "id": "lists-010-13257080"}, {"subject": "Re: (DNS) consensus wordin", "content": "You are not supposed to do negative caching with a time larger than the \nzone SOA MINIMUM field.  See section 4.3.4 of RFC 1034.\n\nDonald\n\n On Sun, 31 Mar 1996, David W. Morris wrote: \n\n> Date: Sun, 31 Mar 1996 18:43:00 -0800 (PST)\n> From: David W. Morris <dwm@shell.portal.com>\n> To: Anawat Chankhunthod <chankhun@catarina.usc.edu>\n> Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: (DNS) consensus wording \n> \n> \n> \n> On Sat, 30 Mar 1996, Anawat Chankhunthod wrote:\n> \n> > I have some additional ideas. Danzig., et.al (SIGCOMM92 ??) shows that \n> > NEGATIVE caching DNS will help to reduce DNS traffic quite a bit. \n> > Negative cache means when DNS lookup fail (may be after 2-3 trails) for\n> > that particular host, we should cache the result (the failure) for a short\n> > time to avoid subsequent look up (which likely to fail again). Say 2 minutes.\n> \n> While this is a useful suggestion, it doesn't fall in the domain of being\n> a security issue. It might be considered for an HTTP implementation \n> recommendations document.\n> \n> Dave Morris\n> \n> \n\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\nhttp://www.cybercash.com           http://www.eff.org/blueribbon.html\n\n\n\n", "id": "lists-010-1325911"}, {"subject": "RE: HTTP/1.1 Contradictio", "content": "Because the transfer-encoding is added and stripped off on a hop-by-hop\nbasis, that header (and its effects on the entity-body) were\ndeliberately left out of the digest.\n\nMore precisely, only certain fields that we felt were strongly related\nto the integrity of the entity body are _included_ in the digest.  Here\nis what header-related info the digest-auth spec says is included in the\ndigest:\n    entity-info = H(\n              digest-uri-value \":\"\n              media-type \":\"         ; Content-type, see section 3.7 of\n[2]\n              *DIGIT \":\"             ; Content length, see 10.12 of [2]\n              content-coding \":\"     ; Content-encoding, see 3.5 of [2]\n              last-modified \":\"      ; last modified date, see 10.25 of\n[2]\n              expires                ; expiration date; see 10.19 of [2]\n              )\n\n    last-modified   = rfc1123-date  ; see section 3.3.1 of [2]\n    expires         = rfc1123-date\n\nIt goes on to say that if a header is missing, then the null string is\nincluded in the digest. Note that this is mandatory for all the headers\n_except_ Content-Length, whose value can be computed even when the\nheader is missing.\n\nNote also that in order to digest a chunked body, the digest has to go\nin the footer, after the end of the chunk-encoded body. However, the\nHTTP spec says that only headers that state that they are allowed in\nfooters are allowed in footers, and the relevant digest-auth header does\nnot so state.\n\nSo, there is no semantic reason that a proxy couldn't convert from a\ncontent-length that is implicit in the chunked encoding to one that is\nexplicit in the Content-Length header, but we'd have to change a couple\nof things that essentially needlessly disallow it:\n\nCHANGES NEED TO THE HTTP/1.1 SPEC to make the above work:\n1. Allow proxies to add Content-Length header if not present.\n\nCHANGES NEEDED TO THE DIGEST SPEC:\n\n1. Change section 2.1.3 \"The AuthenticationInfo Header\" to say that it\nis allowed in the footer of a chunked encoded HTTP message.\n2. Change section 2.1.2 to specifically state that content length is\nalways to be included. The HTTP/1.1 spec requires that content length is\nwell defined in all messages, whether or not there is a Content-Length\nheader.\n\nPaul\n\n\n>----------\n>From: Daniel DuBois[SMTP:dan@spyglass.com]\n>Sent: Friday, December 13, 1996 4:37 PM\n>To: Paul Leach\n>Cc: 'http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com'; pmontelo@spyglass.com\n>Subject: RE: HTTP/1.1 Contradiction \n>\n>At 04:18 PM 12/13/96 -0800, Paul Leach wrote:\n>>Imagine that you've computed an MD5 (or other) checksum over the content\n>>body and various of the headers. If you change any of them, the digest\n>>won't check thereafter. When the digest is used for authentication and\n>>as a secure integrity check, the failure of the digest to check would\n>>lead to some kind of security fault.  The \"no-transform\" directive was\n>>added to tell proxies that changing the body or certain headers would\n>>lead to some kind of failure.\n>>\n>>In the case in question (an HTTP to mail gateway), it may not be\n>>important that the recipient of the Mime-body be able to verify its\n>\n>First let me say I'm not concerned with mail gateways, so I don't want to\n>get too lost thinking about the implications of such. :)\n>\n>I understand what you are saying in the first paragraph, and it sounds like\n>what you are saying applies to Transfer-Encoding as well.  Or are we\n>presuming that whatever the digest-computation algorithms are in the browser\n>(and the server as well!) they are both intelligent/aware enough to ignore\n>the Transfer-Encoding headers in their computations?\n>\n>If we can't make that assumption (it sounds like a tenuous one), then we\n>should state somewhere in the HTTP/1.1 spec that proxies are *not* allowed\n>to remove Transform-Encodings on responses/messages that have a no-transform\n>Cache-Control directive.\n>\n>Are there any other implications of this?\n>\n>Now I think some more about it.... Why are we specifying headers at all?\n>Why aren't they ALL un-modifiable in face of no-transform and the\n>possibility of a some-headers-digested situation?\n>\n>-----\n>Daniel DuBois, Traveling Coderman        www.spyglass.com/~ddubois\n>   o  The Heroes of Might and Magic II Bible is here!\n>      http://www.spyglass.com/~ddubois/HOMM2.html\n>\n>\n\n\n\n", "id": "lists-010-13266712"}, {"subject": "RE: HTTP/1.1 Contradictio", "content": "At 04:37 PM 12/13/96 -0800, Daniel DuBois wrote:\n>If we can't make that assumption (it sounds like a tenuous one), then we\n>should state somewhere in the HTTP/1.1 spec that proxies are *not* allowed\n>to remove Transform-Encodings on responses/messages that have a no-transform\n>Cache-Control directive.\n>\n>Are there any other implications of this?\n\nThe spec says:\n\nRequests with a version lower than that of the proxy/gateway's version MAY\nbe upgraded before being forwarded; the proxy/gateway's response to that\nrequest MUST be in the same major version as the request.\n\nHow about the following:\n\n1.0 Client - 1.1 Proxy - 1.1 Origin\n\nClient sends request to proxy, proxy forms 1.1 request to origin, origin\nstarts CGI program, CGI program generates response sans Content-Length,\norigin chunkifies entity body, origin sends\nchunked/no-transform/digest/whatever response to 1.1 proxy, proxy MUST\nremove the chunking before sending it to the 1.0 client.\n\nPat\n\n\n\n", "id": "lists-010-13281340"}, {"subject": "Re: AcceptCharset suppor", "content": "Klaus Weide writes:\n\n> On Sun, 8 Dec 1996, Keld J&o/rn Simonsen wrote:\n> > Koen Holtman writes:\n> > \n> > > But skimming the UTF-8 specification, I gather that UTF-8 is an encoding\n> > > mechanism, not a character set.\n> > \n> > Well, no. UTF8 is an encoding of characters. It implies the character\n>                                                ^^^^^^^^^^^^^^^^^^^^^^^^\n> > repertoire of ISO 10646. So it is a charset in MIME sense, including\n>   ^^^^^^^^^^^^^^^^^^^^^^^\n> > the specific character definitions of 10646.\n> \n> If that is taken seriously, then \"Accept-Charset: utf-8\" cannot be used\n> to just send information about what character encoding a client can\n> decode.  It implies that (at least when sent in the encoding of utf-8)\n> all characters from the 10646 repertoire are acceptable.\n> \n> It seems predictable that e.g. \"Accept-Charset: koi8-r,iso-8859-1,utf-8\"\n> will be used to indicate \"documents containing characters which are \n> also in koi8-r and latin-1 characters are acceptable in utf-8 encoding\", \n> because there is currently no better way to express that (other than\n> maybe with language tags, which has other problems already mentioned:\n> e.g. transliteration/transcription, languages that do not imply exactly\n> one character repertoire).\n\nI have suggested that we introduce a repertoire identification\nin IP protocols, to address that issue. \n\nkeld\n\n\n\n", "id": "lists-010-13289338"}, {"subject": "Re: AcceptCharset suppor", "content": "I'll ask again: could you demonstrate two equivalent documents that\nyou might want to use content negotiation to distinguish between,\nwhere accept-charset (as an expression of capability) and\naccept-language (as an expression of language) are inadequate to\ndistinguish between them?\n\nIf no one has any realistic examples, perhaps the issue is moot?\n\nLarry\n\n\n\n", "id": "lists-010-13298673"}, {"subject": "Re: AcceptCharset suppor", "content": "Klaus Weide:\n>\n>On Sun, 8 Dec 1996, Koen Holtman wrote:\n>> Klaus Weide:\n>> >\n>> [...using feature negotiation to negotiate on UTF-8....]\n>> \n>> >Maybe it is the most practical way.  But no mechanism is in place yet,\n>> >while overloading the language header (and associated inventiveness with\n>> >new HTML tags) can be done now... \n>> \n>> Overloading a HTTP header and adding HTML tags will take _much_ more time\n>> than waiting for feature negotiation to be in place.\n>\n>Let's hope so :).  However, with overloading I meant treating \n>{Content,Accept}-Language headers (and related HTML tags or attributes)\n>as carrying character repertoire meaning - which is happening now. \n                                            ^^^^^^^^^^^^^^^^^^^^^^\n\nInteresting.  That seems like a very strange thing to do.\n\nWho is doing this and why?  Could you give a pointer?\n\nKoen.\n\n\n\n", "id": "lists-010-13306845"}, {"subject": "Please get me off !!", "content": "Nothing personal but please can you possibly get me off this list!\nPleeeeeeeeeeeease !!!!!!!\n\n\nAndre Aylestock\naaylesto@vbcs.awinc.com\n\nThank you so much, who ever you are  XXXXX\n\n\n\n", "id": "lists-010-13315603"}, {"subject": "Re: AcceptCharset suppor", "content": "> I'll ask again: could you demonstrate two equivalent documents that\n> you might want to use content negotiation to distinguish between,\n> where accept-charset (as an expression of capability) and\n> accept-language (as an expression of language) are inadequate to\n> distinguish between them?\n\nusing URIs for agent interaction there's every reason why i would want to\nuse media types to negotiate formats or interfaces.  \n\n\n -john\n\n\n\n", "id": "lists-010-13322590"}, {"subject": "Warnings, RFC 1522, and ISO-8859", "content": "Dear HTTP 1.1 specialists,\n\nAs a specialist in I18N (coauthor of the HTML I18N spec),\nI am extremely busy trying to have a look at all the many\ninternet drafts in the working, to help to find viable and\nlong-lasting I18N solutions. As virtually all applications\narea drafts affect I18N, this takes a lot of time (besides\nmy normal work!). It is therefore only lately that I have become\naware of some details in the HTTP 1.1 spec that I have difficulties\nunderstanding and that I would propose to change.\nDiscussions in private, and on another list, have not given\nany serious explanations for why HTTP 1.1 solves these issues\nthe way it does, and have suggested that I address them directly\nto this list.\n\n\nThe areas concerned I am concerned with are the TEXT rule and\nits explanation in Section 2.2 on page 16, and the warnings in\nSection 14.45 on pages 128/9 of draft-ietf-http-v11-07.txt.\n\nThe main concern is the choice of \"ISO-8859-1 OR RFC 1522\"\nfor the encoding of TEXT and warnings. I will expand on this\nbelow.\n\nA second point is the question of whether it is okay for\na server/proxy not to send any warning text, but only\nthe number, (correctly!) assuming that the client is in\nbetter shape to decide on language and wording of the text.\nOn the other side, the draft gives the strong impression\nthat the warning has to come with text, so that client\nimplementors will just try to display it, and the user\nmay end up without any warning text.\n\nA third point is the question of explicitly specifying English\nas a default for warnings. Every HTTP implementor should\nbe expected to have as much knowledge of internet practice\nto be able to conclude that given no other indication of\nlanguage preference, English is the best choice anyway.\nSo saying \"The default Language is English\" is a dummy\nstatement. On the other hand, it seems to be offensive\nto some people (not me!) that worry about the dominance\nof English in the internet. It can very well be argued\nthat English as a default should not be written in stone.\nTherefore, it might be silently removed if any other changes\nin the \"Warning\" text are necessary.\n\n\nNow back to the MAIN POINT: Can anybody explain to me why\nISO-8859-1 was choosen as a default for TEXT in headers\nand warnings? Given the recommendations of the IAB\ncharset workshop (draft-weider-iab-char-wrkshop-00.txt),\nwhich repeatedly mentionnes UTF-8, this seems like a\nrather antiquated choice. On the other side, UTF-8\nis extremely suited for the purpose: It covers all the\ncharacters of the world, is reasonably compact, and\nworks together smoothlessly with ASCII. It is clear\nthat 7-bit octets are reserved for ASCII; the 8th bit,\na precious resource, should be used as carefully as\npossible. Using it for UTF-8 is definitely better\nthan using it for ISO-8859-1.\n\nTo improve the situation, I propose four variants for a\nbetter solution. The choice of variant may depend on\nvarious factors I am not fully knowledgeable about,\nsuch as the installed base of servers, proxies, and\nclients that support ISO-8859-1 and/or RFC 1522.\nWhoever knows anything on this topic is wellcome\nto share his/her knowledge.\n\n\nSolution 1: UTF-8 only\n----------------------\n\nAdvantages: Very easy to implement on server. For those\nthat doubt it, I offer to transcode lists of warnings from\nISO 8859-1 (and quite a few other encodings) to UTF-8.\nA file with a list of warnings in various languages\ncan be edited directly if it is in UTF-8, whereas this is\nnot possible for an RFC1522-based solution. This applies\nalso to the other solutions that are based on UTF-8, as the\nserver can choose to use any of the allowed encodings.\nEasy to implement on client (does not need RFC 1522 code).\nUTF-8 support will be in the major web clients next year,\nand in anything that is serious about Java, anyway.\nDisplay support for exotic scripts such as Tibetan is\nnot an issue, as RFC 1522 has the same problems.\n\n\nSolution 2: UTF-8 and RFC 1522\n------------------------------\n\nThe main advantage for this is that some scripts, in\nparticular Indic scripts and Georgian, expand by\na factor of 3 from native encoding to UTF-8.\nOtherwise, there is no good reason for keeping RFC\n1522 with UTF-8 except maybe for installed base.\n\n\nSolution 3: UTF-8 and ISO-8859-1\n--------------------------------\n\nAt first sight, this may seem very dangerous and bad\ndesign, because how should one find out whether something\nis ISO-8859-1 or UTF-8? Indeed, \"guessing\" is needed.\nBut guessing is tremendously simplified, to the extent\nwhere it is really difficult to speak about guessing,\nby the following facts:\n\nISO-8859-1 8-bit characters can be divided into three\nareas: A0-BF, C0-DF, and E0-FF. A0-BF contains all kinds\nof symbols, such as 1/4, copyright, superscript 2,...\nC0-DF contains upper case accented characters, E0-FF\ncontains lower-case accented characters. The range\n80-9F is not defined in ISO-8859-1, it's reserved for\ncontrol characters (C1), but not used in internet context.\nIn ISO-8859-1 strings, E0-FF will be relatively frequent,\nC0-DF considerably less frequent, and A0-BF even less.\nSequences of two characters with the 8th bit set also\nare very rare.\n\nIn UTF-8, the range 80-FF is divided into leading\ncharacters (L: C0-FF) and trailing characters (T: 80-BF).\nThe following sequences are legal UTF-8:\nL1 (C0-DF) T\nL2 (E0-EF) T T\nL3 (F0-F7) T T T\nL4 (F8-FB) T T T T\nL5 (FC-FD) T T T T T\n\nSo to find an octet sequence that is both legal UTF-8 and\nreasonable ISO-885-1, the best chance is to find a reasonable\ncombination of an uppercase accented letter followed by a\nspecial sign such as copyrigth. Can a warning, or any other\nTEXT, reasonably be expected to contain such a combination\n(and no other 8-bit characters that don't conform to UTF-8)?\nCode to test an octet string for UTF-8 compliance is avaliable\non request. The \"guessing\" solution was also accepted in ftp-wg\nto provide a reasonable upgrade path for existing implementations\nthat use arbitrary unlabeled \"charset\"s in their filenames.\n\n\nSolution 4: RFC 1522 only\n-------------------------\n\nNot really the best solution, but at least fair to everybody,\nand leaving the 8th bit open for the future.\n\n\n\nSome readers may argue that HTTP 1.0 already specifies\nISO-8859-1 as the default for TEXT. This is not exactly\ntrue. HTTP 1.0 says:\n\n   Recipients of header field TEXT containing octets outside the US-\n   ASCII character set may assume that they represent ISO-8859-1\n   characters.\n\nVery obviously, this is just a suggestion, not a default. It does\nnot make sense to thighten this default in the wrong direction.\nIt may even be that at some places, based on this not-so-tight\nspecification, implementors may have used any encoding for\nsuch fields.\nIt may also be worth contemplating what happens when an UTF-8\nstring sent out by a server happens to be displayed on a\nclient that is assuming that it can be nothing else than\nISO-8859-1. If the UTF-8 string is something else than a\nstring that could have been represented in ISO-8859-1,\nthen it would have been impossible to reliably send it with\nHTTP 1.0 anyway. Otherwise, accidental accented characters\nwill appear as two octets with the 8th bit set, and display\nas one or two characters in the ISO-8859-1 range. While\nthis is of course very unfortunate, it does not preclude\nreadability. It is a phenomenon that most computer users\ndealing with such languages are actually only too familliar\nwith.\n\n\nSome additional comments for people concerned about these\nissues:\n\nPrevious Discussions\n--------------------\nTo those of you to whom I give the impression of reopening\na point that has already been beaten to death, please note\nthat this is not true. \"charset\" issues and defaults for\nentity content have been discussed repeatedly and vigurously\non this list, and a reasonable solution, considering all the\nbackwards compatibility issues, has been found.\nHowever, ever after scanning the list archives of this year\nin great detail, I have not found any serious discussion\nof I18N issues in headers and warnings.\n\n\nProcedural Concerns\n-------------------\nThe current HTTP 1.1 draft is beyond last call, waiting for\nbecomming an RFC. I do not know whether last minute changes\ncan or should be made, but I have to say I don't care.\nWhether the issues I mentionned above are solved by a last\nminute change, a separate RFC, a mutual understanding in\nthis group, or whatever, is of minor concern if they are\nsolved at all. [The reference to RFC1522 has to be changed\nanyway to its superseding RFC 2047.]\n\n\nMany thanks in advance for you consideration.\n\nRegards,Martin Du\"rst.\n\n----\nDr.sc.  Martin J. Du\"rst    ' , . p y f g c R l / =\nInstitut fu\"r Informatik     a o e U i D h T n S -\nder Universita\"t Zu\"rich      ; q j k x b m w v z\nWinterthurerstrasse  190     (the Dvorak keyboard)\nCH-8057   Zu\"rich-Irchel   Tel: +41 1 257 43 16\n S w i t z e r l a n d   Fax: +41 1 363 00 35   Email: mduerst@ifi.unizh.ch\n----\n\n\n\n", "id": "lists-010-13330651"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "Martin J. Duerst:\n>\n[...]\n>Now back to the MAIN POINT: Can anybody explain to me why\n>ISO-8859-1 was choosen as a default for TEXT in headers\n>and warnings? \n\nThe TEXT encoding was US-ASCII in HTTP/1.0 (RFC1945), but it got changed\ninto ISO-8859-1 for HTTP/1.1 because HTML uses ISO-8859-1.  \n\n>Given the recommendations of the IAB\n>charset workshop (draft-weider-iab-char-wrkshop-00.txt),\n>which repeatedly mentionnes UTF-8, this seems like a\n>rather antiquated choice.\n\nThe basic choice to replace US-ASCII by ISO-8859-1 was made in April.  See\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1996q2/0062.html .  The idea\nwas to sync HTTP with the defaults in HTML, we did not have any i18n\nconsiderations in mind.\n\nAs for the Warning header: we did not spend days discussing how to\ninternationalise the warning text field, this was just a micro-decision made\nby one of the editors along the way.  Maybe it was not an optimal decision,\nbut we did not have the time to spend days optimising every micro-decision.\n\n> On the other side, UTF-8\n>is extremely suited for the purpose: It covers all the\n>characters of the world, is reasonably compact, and\n>works together smoothlessly with ASCII.\n\nSounds good, but you should have told us in April/May, when we were\nfinishing the draft.  Maybe we could use UTF-8 in HTTP/1.2 or HTTP/2.0.\nThere is always a next version.\n\n[...]\n\n>Procedural Concerns\n>-------------------\n>The current HTTP 1.1 draft is beyond last call, waiting for\n>becomming an RFC. I do not know whether last minute changes\n>can or should be made, \n\nIf I understand the IETF process correctly, only very serious bugs can be\nfixed at this point.\n\nWe cannot reverse decisions like the default charset without doing a last\ncall again, which would delay the draft by many months.  And we don't want\nany delay, it is generally thought that 1.1 is dangerously late already.\n\n>Regards,Martin Du\"rst.\n\nKoen.\n\n\n\n", "id": "lists-010-13347856"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "Hello Koen,\n\nMany thanks for your information.\n\n> Martin J. Duerst:\n> >\n> [...]\n> >Now back to the MAIN POINT: Can anybody explain to me why\n> >ISO-8859-1 was choosen as a default for TEXT in headers\n> >and warnings? \n> \n> The TEXT encoding was US-ASCII in HTTP/1.0 (RFC1945),\n\nNot true. RFC1945 explicitly allows octets from character sets\nother than US-ASCII (which means octets with the 8th bit set).\nIt allows the recipient to assume that these represent\nISO-8859-1 characters, but it leaves the possibility open for\nit to be something else. Here is the full quote:\n\n   The TEXT rule is only used for descriptive field contents and values\n   that are not intended to be interpreted by the message parser. Words\n   of *TEXT may contain octets from character sets other than US-ASCII.\n\n       TEXT           = <any OCTET except CTLs,\n                        but including LWS>\n\n   Recipients of header field TEXT containing octets outside the US-\n   ASCII character set may assume that they represent ISO-8859-1\n   characters.\n\n> but it got changed\n> into ISO-8859-1 for HTTP/1.1 because HTML uses ISO-8859-1.  \n\nHTML 2.0, as of Nov. 1995 (RFC1866) already contained very\nclear language that HTML will move to ISO-10646. Also, there\nis a big difference between entity bodies (where the agreement\nis that \"charset\" should be labelled as far as legacy browsers\ndon't prevent that) and headers (where labeling only makes\nsense for 7-bit email, but is not necessary with UTF-8).\n\n\n> >Given the recommendations of the IAB\n> >charset workshop (draft-weider-iab-char-wrkshop-00.txt),\n> >which repeatedly mentionnes UTF-8, this seems like a\n> >rather antiquated choice.\n> \n> The basic choice to replace US-ASCII by ISO-8859-1 was made in April.  See\n> http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q2/0062.html .\n\nSorry, but 1996q2/0062.html is about Accept-Charset. The question there\nwas whether it could be assumed that clients in general would be able\nto render ISO-8859-1, so that clients would not be required to\ninclude ISO-8859-1 in their Accept-Charset line. There was more\ndiscussion on this later, but I think the decision taken was reasonable.\n\nThe above message does not mention TEXT at all.\n\n\n>The idea\n> was to sync HTTP with the defaults in HTML, we did not have any i18n\n> considerations in mind.\n\nThis sounds to me as if somebody were saying \"We were discussing\npasswords - We did not have any security considerations in mind\".\n\n\n> As for the Warning header: we did not spend days discussing how to\n> internationalise the warning text field, this was just a micro-decision made\n> by one of the editors along the way. Maybe it was not an optimal decision,\n> but we did not have the time to spend days optimising every micro-decision.\n\nThere is really no need to discuss such things for days. The only\nrequirement is to make the right decision.\n\n\n> > On the other side, UTF-8\n> >is extremely suited for the purpose: It covers all the\n> >characters of the world, is reasonably compact, and\n> >works together smoothlessly with ASCII.\n> \n> Sounds good,\n\nI can assure you: It is as good as it sounds.\n\n\n>but you should have told us in April/May, when we were\n> finishing the draft.  Maybe we could use UTF-8 in HTTP/1.2 or HTTP/2.0.\n> There is always a next version.\n\nThe sooner we change, the better. The longer we wait, the more\nprograms there will be that depend on it, and the more difficult\nit will be to change it.\n\n\n> [...]\n> \n> >Procedural Concerns\n> >-------------------\n> >The current HTTP 1.1 draft is beyond last call, waiting for\n> >becomming an RFC. I do not know whether last minute changes\n> >can or should be made, \n> \n> If I understand the IETF process correctly, only very serious bugs can be\n> fixed at this point.\n> \n> We cannot reverse decisions like the default charset without doing a last\n> call again, which would delay the draft by many months.  And we don't want\n> any delay, it is generally thought that 1.1 is dangerously late already.\n\nI definitely don't want to delay the draft. But if we agree on\nthe direction to go in this issue, we can issue a small draft\n(e.g. Encoding of Headers in HTTP) to clear up the issue.\nThis should neither delay the IETF process, nor will it delay\nimplementations to wait for HTTP 1.2 to do the right thing.\n\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13357649"}, {"subject": "Re: Document Action: Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.0 toInformationa", "content": "> > The IESG has reviewed the Internet-Draft \"Hypertext Transfer Protocol\n> > -- HTTP/1.0\" <draft-ietf-http-v10-spec-05.txt, .ps> and recommends that\n> > it be published by the RFC Editor as an Informational RFC, but with the\n> > following IESG Note:\n\n> >The IESG has concerns about this protocol, and expects this\n> >document to be replaced relatively soon by a standards track\n> >document.\n\n> Maybe I'm being a bit picky here, but it should be made clear that there\n> are no known plans in the HTTP WG to replace the HTTP/1.0 document, nor to\n> replace the HTTP/1.0 protocol described in the document in question. There\n> certainly are plans to come out soon with a *different document*, and that\n> document will described a *different protocol*, namely HTTP/1.1.\n\nFirst of all, it's important to view this statement in light of how the IETF\nstandards process works. Protocols are normally defined and either declared to\nbe informational or else proceed down the standards track: proposed standard,\ndraft standard, and finally standard.\n\nA updated protocol can be defined at some point. When complete it normally\nobsoletes the older protocol, and if the old version was a standard it is\nusually moved to historical status. (The move to historical seems somewhat\nhaphazard in its application -- for example, RFC1113-1115 (PEM) have been\nmoved, but RFC1425-1427, which have also been obsoleted by newer documents of\nthe same or higher grade, have not. I don't pretend to understand the nuances\nhere.) \n\nIn any case, there is never any attempt to replace or withdraw the document per\nse -- once issued, an RFC there forever.\n\nProtocol version numbers offer an interesting twist, however. The usual intent\nis for the new version of the protocol to obsolete the old protocol eventually.\nI believe this is the intent for, say, RIPv2, SNMPv2, and IPv6. But there seem\nto be no hard-and-fast rules for this. As far as I know it would be perfectly\npermissible to have two standard protocols, Pv1 and Pv2, with overlapping\nfunctionality and no intent to ever obsolete the earlier version.\n\nAnother thing to keep in mind is that the history of protocol versioning in the\nIETF doesn't present a pretty picture. We have cases where things were clearly\nbotched, like MIME-version. We have cases of near-botches, such as IMAP3.  The\ninescapable conclusion is that the IETF really doesn't do versioning very well.\nWhether this is a problem intrinsic to the IETF or simply a protocol with\nprotocol versions in general is a matter of opinion.\n\nIn any case, I view the IESG statement as effectively saying that \"this\nprotocol will be marked as obsolete the minute there is something to obsolete\nit with\". This is a strong statement and one quite different from the implicit\ndefault, which I would characterize as \"this protocol, like any other, may be\nobsoleted by something else in the future\". I do think the use of the term\n\"replace\" in this context was unforunate, since it is not the term that is\nnormally used, nor does it accurately convey a sense of what actually happens.\nI recommend that the IESG change the wording to use the term \"obsolete\"\nexplicitly.\n\nWording aside, this is a strong statement for the IESG to make, and I would\nalso characterize it as a measure of just how uncomfortable the IESG is with\nHTTP/1.0. (Please note that I'm simply making an observation here -- I am not\noffering an opinion of whether or not I think the IESG is correct in its\nassessment.)\n\n> Of course, there are also plans to try to get as much of the world to adopt\n> the new protocol as soon as we can. However, this is an implementation\n> issue.\n\nI'm a bit unclear as to the nature of your discomfort. If you were concerned\nthat the document will simply disappear, that just doesn't happen. Obsoleting\none RFC with another, however, is another matter -- this *is* the mechanism\nthe IETF uses to indicate to the world that people should adopt a new\nprotocol instead of the old one. \n\nIf you're still uncomfortable with this document being obsoleted at some point,\nthen I think we may have a problem that deserves further discussion.\n\nNed\n\n\n\n", "id": "lists-010-1336273"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "Martin J. Duerst:\n>\n>Hello Koen,\n>\n>Many thanks for your information.\n>\n>> Martin J. Duerst:\n>> >\n>> [...]\n>> >Now back to the MAIN POINT: Can anybody explain to me why\n>> >ISO-8859-1 was choosen as a default for TEXT in headers\n>> >and warnings? \n>> \n>> The TEXT encoding was US-ASCII in HTTP/1.0 (RFC1945),\n>\n>Not true. RFC1945 explicitly allows octets from character sets\n>other than US-ASCII (which means octets with the 8th bit set).\n\nYes, but it does not tell you how to interpret octets with the 8th bit set.\nSo the bottom line is that you can depend on US-ASCII and nothing else.\n\n[...]\n>> but it got changed\n>> into ISO-8859-1 for HTTP/1.1 because HTML uses ISO-8859-1.  \n>\n>HTML 2.0, as of Nov. 1995 (RFC1866) already contained very\n>clear language that HTML will move to ISO-10646.\n\nRFC1866 also contains very clear language about HTML user agents supporting\nISO-8859-1 by default, so this is what we took.\n\nI'm sure that, if more i18n specalists had reviewed an commented on the\nHTTP/1.1 draft before last call, we could have written a draft better\nprepared for i18n.  But there weren't and we didn't.\n\n> Also, there\n>is a big difference between entity bodies (where the agreement\n>is that \"charset\" should be labelled as far as legacy browsers\n>don't prevent that) and headers (where labeling only makes\n>sense for 7-bit email, but is not necessary with UTF-8).\n\nI don't remember that we were aware of a difference between entity bodies and\nheaders at the time.  What basically happened is that we changed US-ASCII to\nISO-8859-1 for Accept-Charset first.  Then, the other places where a US-ASCII\ndefault was used got edited to reflect this change.  I did not edit the other\nplaces, so you'l have to ask someone else (Jim Gettys?) for the complete\nhistory.\n\n[...]\n>>The idea\n>> was to sync HTTP with the defaults in HTML, we did not have any i18n\n>> considerations in mind.\n>\n>This sounds to me as if somebody were saying \"We were discussing\n>passwords - We did not have any security considerations in mind\".\n\nWell, you are an i18n specialist, I'm not.  Those who were involved in\ncreating 1.1 all know that 1.1 is not perfect.  It is as good as we could\nmake it in the time we had, with the resources we had.\n\n>> As for the Warning header: we did not spend days discussing how to\n>> internationalise the warning text field, this was just a micro-decision made\n>> by one of the editors along the way. Maybe it was not an optimal decision,\n>> but we did not have the time to spend days optimising every micro-decision.\n>\n>There is really no need to discuss such things for days. The only\n>requirement is to make the right decision.\n\nSorry, the only requirement is rough consensus and running code.\n\n[....]\n>I definitely don't want to delay the draft. But if we agree on\n>the direction to go in this issue, we can issue a small draft\n>(e.g. Encoding of Headers in HTTP) to clear up the issue.\n>This should neither delay the IETF process, nor will it delay\n>implementations to wait for HTTP 1.2 to do the right thing.\n\nI'm not an expert on this small draft business, but I think it will be\ndifficult to have a small draft which changes HTTP/1.1 semantics (instead of\njust adding to semantics or clarifying semantics) without also having a\nprocedural delay.  I believe it is ultimately up to the IESG, though.\n\nAs for the direction on this issue: I'm not convinced that there is anything\nthat needs fixing.  I gather that the Warning header definition is extremely\nyucky from a i18n standpoint, but that does not justify changing it.  There\nare plenty of yucky headers in the protocol, but the headers are not meant\nfor human consumption, so who cares about taste?  Stability is more\nimportant.\n\n>Regards,Martin.\n\nKoen.\n\n\n\n", "id": "lists-010-13369895"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "This was an excellent message, by the way, and raised the issue in a\nway that made the alternatives clear. I don't know whether any of the\nothers in the WG want to (re)visit the topic of changing the charset\ndefault for messages in HTTP/1.1 warnings, and would appreciate WG\nmembers responding directly as to their interest in pursuing this,\nbased on your analysis.\n\nLarry\n\n\n\n", "id": "lists-010-13381699"}, {"subject": "Re: AcceptCharset suppor", "content": "On Sat, 14 Dec 1996, Koen Holtman wrote:\n> Klaus Weide:\n> >\n> >On Sun, 8 Dec 1996, Koen Holtman wrote:\n> >> \n> >> Overloading a HTTP header and adding HTML tags will take _much_ more time\n> >> than waiting for feature negotiation to be in place.\n> >\n> >Let's hope so :).  However, with overloading I meant treating \n> >{Content,Accept}-Language headers (and related HTML tags or attributes)\n> >as carrying character repertoire meaning - which is happening now. \n>                                             ^^^^^^^^^^^^^^^^^^^^^^\n> \n> Interesting.  That seems like a very strange thing to do.\n> \n> Who is doing this and why?  Could you give a pointer?\n\nExamples where \"Language\" is treated as carrying charset meaning\n(not just repertoire, but \"charset\" including encoding):\n\nPages that do the poor-man's negotiation of letting the user select\na \"language\" manually, than return a page whose charset may vary \ndepending on the language choice.\n   <URL: http://www.alis.com/internet_products/language.en.html>\n   <URL: http://www.accentsoft.com/>\n   <URL: http://www.dkuug.dk/maits/>   \n\nAnother example, which does \"real\" (automatic) negotiation:\n   <URL: http://www.dkuug.dk:81/maits/summary>\n(For example, with \"accept-language: el, en\" you get Greek in iso-8859-7\n- even when also sending an \"accept-charset\" which excludes iso-8859-7.)\n\nAs for cases where *-Language (or <LANG> etc.) would be used to \ndistinguish between sub-repertoires of Unicode - well I tried to find\nsome examples, but couldn't.  Possible reasons are (1) my search was\nnot extensive (or systematic) enough, (2) they don't exist [yet],\n(3) there aren't many UTF (or 10646) pages now, (4) there aren't many\ntruly multilingual pages now (with more than one language requiring\nmore-than-USASCII).  Also the UTF and multilingual pages I found are\nexperimental or for demonstration purposes, so they don't really bother\nright now about supporting browsers which might be less endowed - the\nintentions rather seems to be to demonstrate \"You need _this_ browser\n[from us] to see this!\".  \n\nMy impression that informal overloading of *-Language with charset\nmeaning is (for some) regarded as an acceptable practice derives from\nrecent messages to the www-international list, were it was argued that\nthis is OK because it covers the \"common\" and \"regualar\" case - see e.g.\n\n   <http://lists.w3.org/Archives/Public/www-international/msg00405.html>,\n   <http://lists.w3.org/Archives/Public/www-international/msg00412.html>,\nand more generally\n   <http://lists.w3.org/Archives/Public/www-international/threads.html>.\n\n  Klaus\n\n\n\n", "id": "lists-010-13389459"}, {"subject": "Re: AcceptCharset suppor", "content": "> > because there is currently no better way to express that (other than\n> > maybe with language tags, which has other problems already mentioned:\n> > e.g. transliteration/transcription, languages that do not imply exactly\n> > one character repertoire).\n> \n> I have suggested that we introduce a repertoire identification\n> in IP protocols, to address that issue. \n\nSince this goes beyond HTTP or the www, I would be interested to know\nwhere (in which forum) you have made that proposal; and whether it is \nmore likely to be considered there. :)\n\nThis may be more useful for other protocols (MIME for mail, news etc.)\nthan HTTP and other formats than HTML, because HTML already implies\nthat all 10646 characters can occur (according to the i18n draft;\nnumeric character references to all 10646 characters are valid,\nindependend from the character encoding used for transfer of a\ndocument; as has recently been pointed out).  But text/plain is also\nstill part of the Web, IMHO...\n\nHaving a way to externally identify character repertoire may lead to\nfaster acceptance of UTF-8 as character encoding, since one can then\nuse UTF-8 without losing the repertoire information implied by the\ncurrenlt used charsets.\n\n  Klaus\n\n\n\n", "id": "lists-010-13400710"}, {"subject": "Re: AcceptCharset suppor", "content": "On Sat, 14 Dec 1996, Larry Masinter wrote:\n> \n> I'll ask again: could you demonstrate two equivalent documents that\n> you might want to use content negotiation to distinguish between,\n> where accept-charset (as an expression of capability) and\n> accept-language (as an expression of language) are inadequate to\n> distinguish between them?\n> \n> If no one has any realistic examples, perhaps the issue is moot?\n\nCurrently accept-charset is de facto used as an expression of _two_\ncapabilities: (1) to decode a character encoding, (2) to be able to\ndisplay (or take responsibility for) a certain character repertoire.\nThe second aspect is not new, is has been around as long as MIME had\na charset parameter.  It will be lost when/if everything moves to UTF-8\n(and starts using accept-charset: utf-8 / charset=utf-8).\n\nExample of a site where documents are provided in several charsets\n(all for the same language):\nsee <URL: http://www.fee.vutbr.cz/htbin/codepage>.\n\nIt is of course not desirable that a server would have to deal with\nsuch a variety of codepages/charsets, but rather the result of an\nunfortunate state of nonstandardization.  Which seems to be a big\nproblem in that part of the world, and no doubt also in other places,\nand it may last for a while.  Anyway you asked for an example where\naccept-charset and accept-language together are inadequate to\ndistinguish between versions a server is willing to provide.\nCurrently they are adequate for this site; they would not if the site\nchose to always use UTF-8 while still supporting the current set of\nclients with their varying codepages.\n\nIt is certainly much easier to make a Web clients able to decode UTF-8\nto locally available character sets, than to upgrade all client\nmachines so that they have fonts available to display all of the 10646\ncharacters.  I assume the former will be done much sooner, and that\nuse of UTF-8 should be encouraged before all the fonts (or knowledge\nto choose culturally correct replacement representations) are\navailable to everyone.\n\n  Klaus\n   \n\n\n\n", "id": "lists-010-13409795"}, {"subject": "Re: AcceptCharset suppor", "content": "On Mon, 16 Dec 1996, Klaus Weide wrote:\n\n> On Sat, 14 Dec 1996, Keld J?rn Simonsen wrote:\n> > I have suggested that we introduce a repertoire identification\n> > in IP protocols, to address that issue. \n> \n> Since this goes beyond HTTP or the www, I would be interested to know\n> where (in which forum) you have made that proposal; and whether it is \n> more likely to be considered there. :)\n> \n> This may be more useful for other protocols (MIME for mail, news etc.)\n> than HTTP and other formats than HTML, because HTML already implies\n> that all 10646 characters can occur (according to the i18n draft;\n> numeric character references to all 10646 characters are valid,\n> independend from the character encoding used for transfer of a\n> document; as has recently been pointed out).  But text/plain is also\n> still part of the Web, IMHO...\n> \n> Having a way to externally identify character repertoire may lead to\n> faster acceptance of UTF-8 as character encoding, since one can then\n> use UTF-8 without losing the repertoire information implied by the\n> currenlt used charsets.\n\nI don't think so. Adding repertoire information will complicate\nthings, and slow down use of UTF-8. It also detracts from the basic\nidea of Unicode/ISO 10646, which is to remove repertoire restrictions.\n\nTo have the client care about how to represent all the characters\nin ISO10646 is much easier than to have the server care about\nhow to present a document in an arbitrary repertiore. With N\ncharacters in total, on the client you only need a list of length\nN, giving the representation of each of these characters. In many\nimplementations, this list will be trivial for many parts of the\nlist. There are also many very creative solutions available, such\nas making each \"undisplayable\" character a little box with a link\nto a page describing that character.\nIf you want to deal with things on the server, you need in principle\n2**N different solutions, each for all N characters. In practice\nit's still a lot of different solutions you have to care for.\n\nSo we really don't need protocol extensions for something that\nnobody will implement because it's easier to deal with on the\nclient side. Those few cases where it's really relevant,\nsuch as Japanese transliterated to romaji, can be dealt with\nthe existing mechanisms.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13419792"}, {"subject": "Re: AcceptCharset suppor", "content": "On Mon, 16 Dec 1996, Klaus Weide wrote:\n\n> Currently accept-charset is de facto used as an expression of _two_\n> capabilities: (1) to decode a character encoding, (2) to be able to\n> display (or take responsibility for) a certain character repertoire.\n\nThis is just because current clients don't actually do much of a\ndecoding, they just switch to a font they have available.\n\n> The second aspect is not new, is has been around as long as MIME had\n> a charset parameter.  It will be lost when/if everything moves to UTF-8\n> (and starts using accept-charset: utf-8 / charset=utf-8).\n> \n> Example of a site where documents are provided in several charsets\n> (all for the same language):\n> see <URL: http://www.fee.vutbr.cz/htbin/codepage>.\n\nThe list is impressive. It becomes less impressive if you realize\nthat all (as far as I have checked) the English pages and some\nof the Check pages (MS Cyrillic/MS Greek/MS Hebrew,...) are just\nplain ASCII, and don't need a separate URL nor should be labeled\nas such in the HTTP header. They could add a long list of other\nencodings, and duplicate their documents, to e.g. serve them in\nShift-JIS or so :-). Only the US-ASCII version contains the comment\n\"bez diakritiky\" (without diactritics), but that applies to many\nmore pages.\n\n> It is certainly much easier to make a Web clients able to decode UTF-8\n> to locally available character sets, than to upgrade all client\n> machines so that they have fonts available to display all of the 10646\n> characters.\n\nThe big problem is not fonts. A single font covering all current ISO\n10646 characters can easily be bundeled with a browser. The main\nproblem is display logic, for Arabic and Indic languages in particular.\n\n>I assume the former will be done much sooner, and that\n> use of UTF-8 should be encouraged before all the fonts (or knowledge\n> to choose culturally correct replacement representations) are\n> available to everyone.\n\nDefinitely UTF-8 should be encouraged. But that's not done by\nintroducing new protocol complications and requiring the servers\nto deal with unpredictable transliteration issues that can be\ndealt with more easily on the client side.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13429951"}, {"subject": "Re: AcceptCharset suppor", "content": "On Mon, 16 Dec 1996, Klaus Weide wrote:\n\n> On Sat, 14 Dec 1996, Koen Holtman wrote:\n> > Klaus Weide:\n> > >\n> > >Let's hope so :).  However, with overloading I meant treating \n> > >{Content,Accept}-Language headers (and related HTML tags or attributes)\n> > >as carrying character repertoire meaning - which is happening now. \n> >                                             ^^^^^^^^^^^^^^^^^^^^^^\n> > \n> > Interesting.  That seems like a very strange thing to do.\n> > \n> > Who is doing this and why?  Could you give a pointer?\n> \n> Examples where \"Language\" is treated as carrying charset meaning\n> (not just repertoire, but \"charset\" including encoding):\n> \n> Pages that do the poor-man's negotiation of letting the user select\n> a \"language\" manually, than return a page whose charset may vary \n> depending on the language choice.\n>    <URL: http://www.alis.com/internet_products/language.en.html>\n>    <URL: http://www.accentsoft.com/>\n>    <URL: http://www.dkuug.dk/maits/>   \n> \n> Another example, which does \"real\" (automatic) negotiation:\n>    <URL: http://www.dkuug.dk:81/maits/summary>\n> (For example, with \"accept-language: el, en\" you get Greek in iso-8859-7\n> - even when also sending an \"accept-charset\" which excludes iso-8859-7.)\n\nWhat you perceive as overloading and automatic negotiation is just\na side effect of your exagerated assumptions about the server.\nYou think the server is somehow being intelligent and saying\n\"well, this guy wants Greek, so I am assuming he will be able to\ndisplay iso-8859-7, even if he doesn't say so\".\nWhat's probably happening is that the program starts checking languages,\nfinds that it can meet the request for Greek, then checks what\nencodings it has for this document, finds that it only has iso-8859-7,\nand sends it out.\nSo the only thing the server does is giving priority to languages\nover encodings. This results from the fact that there is no\nspecification about relative priority of Accept headers, or\nhow to combine them. Another server could start with \"charset\"s,\nfind that it has some document available in one of the\ncharsets you specified, and serve it even if it is not in a language\nyou specified. This does not work with your example, but assume\nyou specify Polish and Czech and ISO-8859-2, and the server\nhas the document in Russian and Hungarian, you might get the\ndocument in Hungarian (because it is ISO-8859-2) even if you\nwill understand more in Russian.\n\nThe problem here is not overloading or \"automatic\" negotiations,\nthe \"problem\" is that the Accept headers were designed for\nthe general case (i.e. you know Greek and have reasonable equipment\nto view it), whereas you need transparent negotiation for\nthe special cases you are considering.\n\n\n> As for cases where *-Language (or <LANG> etc.) would be used to \n> distinguish between sub-repertoires of Unicode - well I tried to find\n> some examples, but couldn't.  Possible reasons are (1) my search was\n> not extensive (or systematic) enough, (2) they don't exist [yet],\n> (3) there aren't many UTF (or 10646) pages now, (4) there aren't many\n> truly multilingual pages now (with more than one language requiring\n> more-than-USASCII).  Also the UTF and multilingual pages I found are\n> experimental or for demonstration purposes, so they don't really bother\n> right now about supporting browsers which might be less endowed - the\n> intentions rather seems to be to demonstrate \"You need _this_ browser\n> [from us] to see this!\".  \n\n> My impression that informal overloading of *-Language with charset\n> meaning is (for some) regarded as an acceptable practice derives from\n> recent messages to the www-international list, were it was argued that\n> this is OK because it covers the \"common\" and \"regualar\" case - see e.g.\n> \n>    <http://lists.w3.org/Archives/Public/www-international/msg00405.html>,\n>    <http://lists.w3.org/Archives/Public/www-international/msg00412.html>,\n> and more generally\n>    <http://lists.w3.org/Archives/Public/www-international/threads.html>.\n\nIt's informal, and in many cases just a side-effect.\nIt is always possible for the server to make some guesses.\nNo server is required to honor your requests re. language or\n\"charset\". For both, the model is: Check if you have it, if\nnot, serve something else. For \"charset\", some servers do\nstraightforward transcoding, but servers that do language\nconversion (translation) or transliteration are a big\nexception.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13440438"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "On Mon, 16 Dec 1996, Koen Holtman wrote:\n\n> Martin J. Duerst:\n> >\n> >Hello Koen,\n\n> >> The TEXT encoding was US-ASCII in HTTP/1.0 (RFC1945),\n> >\n> >Not true. RFC1945 explicitly allows octets from character sets\n> >other than US-ASCII (which means octets with the 8th bit set).\n> \n> Yes, but it does not tell you how to interpret octets with the 8th bit set.\n> So the bottom line is that you can depend on US-ASCII and nothing else.\n\nIf that's the bottom line, why was the ISO-8859-1 default introduced?\nIt's quite a lot more difficult to distinguish ISO-8859-1 from\nISO-8859-2, -3, -4, -9, and -10, than distinguishing UTF-8 from\nany legacy coding whatsoever.\n\n> [...]\n> >> but it got changed\n> >> into ISO-8859-1 for HTTP/1.1 because HTML uses ISO-8859-1.  \n> >\n> >HTML 2.0, as of Nov. 1995 (RFC1866) already contained very\n> >clear language that HTML will move to ISO-10646.\n> \n> RFC1866 also contains very clear language about HTML user agents supporting\n> ISO-8859-1 by default, so this is what we took.\n\nThere is a big difference between what *set* of characters a client is\nrequired to support and what kind of *encoding* should be choosen as\na default.\n \n\n> I'm not an expert on this small draft business, but I think it will be\n> difficult to have a small draft which changes HTTP/1.1 semantics (instead of\n> just adding to semantics or clarifying semantics) without also having a\n> procedural delay.  I believe it is ultimately up to the IESG, though.\n\nIf that is a problem, there is an easy way out: We just don't tell\nthe IESG about the changes we want to make until after the big\nHTTP/1.1 is out. Anybody know any more details?\n\n\n> As for the direction on this issue: I'm not convinced that there is anything\n> that needs fixing.  I gather that the Warning header definition is extremely\n> yucky from a i18n standpoint, but that does not justify changing it.  There\n> are plenty of yucky headers in the protocol, but the headers are not meant\n> for human consumption, so who cares about taste?  Stability is more\n> important.\n\nAre there any 1.1 server implementations that send warnings?\nAre there any 1.1 server implementations that send warnings\nin anything else than English? If the warning headers are not\nintended for human consumption, then why are they there in the\nfirst place? If headers are not intended for human consumption,\nwhy bother with a small distortion of ISO-8859-1 characters\nthat are sent out as UTF-8 but displayed directly as ISO-8859-1,\nif we can get on the right path for the future?\nIn an earlier mail, you have mentionned that there is always\na new version. Now you say that stability is more important.\nIf you cite stability already at this point, won't you stress\nit much more when the issue will be raised again when going\nto a new version? Isn't it better to change as fast as possible,\nbefore existing implementations make it too difficult to change\nanything?\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13453996"}, {"subject": "HTTP &quot;Warning&quot; (was: Charset support", "content": "It was not meant as a proposal, but rather a prediction what might\nhappen.  Proxies continue sending English text, clients take over\nlocalization of those well-defined messages.\n\n> I mainly don't know how big the chance is that we will have\n> other warnings than the 6 now defined.\n\nThere are five where \"the number is the message\".  The description\nfor 99 reads \"The warning text may include arbitrary information to be\npresented to a human user, or logged\", so in that case the real\ninformation would be in the warning text - not just the words \"Misc.\nWarning.\"\n \n> > # As for \"who is the standard\", it's definitely http-wg. But not having\n> > # to implement RFC1522 will be a relief to most implementors, don't\n> > # you think so?\n> > \n> > RFC1522 is inescapable for those clients that want to use charsets\n> > other than 8859-1 and UTF8. In the current setup, RFC1522 is\n> > avoidable for those clients that only implement 8859-1. And RFC1522\n> > code must be there anywhere for any software package that includes\n> > both a web browser and a mail client. (I think this includes most\n> > client vendors). So I'm not sure which implementors you're claiming\n> > will be relieved.\n> \n> It's mainly the server implementors that I am thinking about.\n> They don't have the RFC1522 code around. And my guess is that\n> as long as they don't send anything, clients won't do much\n> to receive it. RFC1522 is not inescapable.\n\nRFC1522 encoding is yucky.  There are length restrictions that seem\narbitrary for HTTP and make no sense there - from RFC2047: \"An\n'encoded-word' may not be more than 75 characters long...\"  \"... each\nline of a header field that contains one or more 'encoded-word's is\nlimited to 76 characters.\"  There is a rule that whitespace between \nencoded-words is insignificant, but not between an encoded-word and\na normal word.\n\nThe HTTP 1.1 sentence \"If a character set other than ISO-8859-1 is used, \nit MUST be encoded in the warn-text using the method described in \nRFC 1522.\"  doesn't talk about *how* the RFC 1522 method has to be applied,\nso it seems all the arcane rules from (now) RCF2047 have to be applied.\n\nThere is also an inconsistency between HTTP 1.1 and RFC 1522/2047 which\nmakes HTTP's use of the method not-really-1522 - see\n <http://www-uk.hpl.hp.com/people/ange/archives/http-wg-archive/1671.html>\nand\n <http://www-uk.hpl.hp.com/people/ange/archives/http-wg-archive/1676.html>.\n(I didn't follow up on that exchange then.)  Does the \"moving target of \nMIME\" (from Roy Fieldings's response) still apply?\n\nAnother insufficiency of the current warning header hasn't been mentioned:\nthe suggested heuristics for a clients selecting between several warning\nheaders include (14.45)\n  o  Warnings in the user's preferred character set take priority over\n     warnings in other character sets but with identical warn-codes and\n     warn-agents.\nBut there is not way to flag the language.  So a client program in general\nhas no way to know whether the text of a given message is comprehensible\nby the user.\n \nMy conclusion:  It is very likely that nobody will implement Warning+1522\nanytime soon.  It has been stated (severl time, I believe) that when\nHTTP advances to Draft Standard, every feature which doesn't have two\ninteroperable implementations will have to be dropped.  So maybe that\nis the way Warning+1522 will be resolved...  \n\n   Klaus\n\nPS. A precedent for iso-8859-1 in TEXT:\n    www.alis.com returns response \"404 Pas trouv?\".\n\n\n\n", "id": "lists-010-13465018"}, {"subject": "sub-repertoires (was: AcceptCharset support", "content": "On Tue, 17 Dec 1996, Martin J. Duerst wrote:\n> On Mon, 16 Dec 1996, Klaus Weide wrote:\n> \n> > Currently accept-charset is de facto used as an expression of _two_\n> > capabilities: (1) to decode a character encoding, (2) to be able to\n> > display (or take responsibility for) a certain character repertoire.\n> \n> This is just because current clients don't actually do much of a\n> decoding, they just switch to a font they have available.\n\n1) I don't think that is tru for all current clients,\n2) the effect is the same.\n\n[...]\n> > Example of a site where documents are provided in several charsets\n> > (all for the same language):\n> > see <URL: http://www.fee.vutbr.cz/htbin/codepage>.\n> \n> The list is impressive. It becomes less impressive if you realize\n> that all (as far as I have checked) the English pages and some\n> of the Check pages (MS Cyrillic/MS Greek/MS Hebrew,...) are just\n> plain ASCII, and don't need a separate URL nor should be labeled\n> as such in the HTTP header. They could add a long list of other\n> encodings, and duplicate their documents, [...]\n\nRight.  There are still at least 3 different (in a relevant way)\nrepertoires.\n\nIf those could be labelled (and negotiated) as charset=UTF-8 + \nrepertoire indication,  less duplication would be needed.\n\n> > It is certainly much easier to make a Web clients able to decode UTF-8\n> > to locally available character sets, than to upgrade all client\n> > machines so that they have fonts available to display all of the 10646\n> > characters.\n> \n> The big problem is not fonts. A single font covering all current ISO\n> 10646 characters can easily be bundeled with a browser. The main\n> problem is display logic, for Arabic and Indic languages in particular.\n\nI am interested in having something that could work well with UTF-8\neven on a vtxxx terminal or a Linux console (in cases where that makes\nsense).\n\n> Definitely UTF-8 should be encouraged. But that's not done by\n> introducing new protocol complications and requiring the servers\n> to deal with unpredictable transliteration issues that can be\n> dealt with more easily on the client side.\n\nI am not thinking of requiring a server to do anything.  Just being\nable to say \n Content-type: text/plain;charset=utf-8; charrep=\"latin-1,latin-2,koi8\"\n(made-up syntax, may be fatally flawed) for those who wish to do so;\nand something equivalent for the \"accept-*\" side.  Nothing mandatory, let\neverybody who doesn't care default to the currently implied full 10646 \nrepertoire.  I think the examples show that people are doing the\nequivalent now (whether accidentally or not).\n\nNo client would be forced to do anything with that additional info (they\ncan ignore it, or treat as advisory).  No server would be required to \nsend it, or react to a \"accept-charrep/accept-features:...\" (or whatever\nthe syntax might be).\n\n  Klaus\n\n\n\n", "id": "lists-010-13476374"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "I'm not much of an expert on i18n, so I can't really comment on\nwhat is \"right\" or \"wrong\".  But I can comment on this:\n\n    As for the Warning header: we did not spend days discussing how to\n    internationalise the warning text field, this was just a\n    micro-decision made by one of the editors along the way.  Maybe it\n    was not an optimal decision, but we did not have the time to spend\n    days optimising every micro-decision.\n\nActually, one of my early drafts for the Warning header proposed\nthis syntax:\n\n   Warning headers are sent with responses using:\n\n       Warning = \"Warning\" \":\" warn-code SP warn-agent SP warn-text\n               [SP language-tag [SP charset]]\n\n       warn-code = 2DIGIT\n\n       warn-agent = ( host [ \":\" port ] ) | pseudonym\n                       ; the name or pseudonym of the server adding\n                       ; the Warning header, for use in debugging\n\n       warn-text = quoted-string\n\nI can't remember exactly why or when the language-tag and charset fields\nwere removed.  However, I do remember that we had a moderately long\ndiscussion of this issue at the Montreal IETF meeting; it was NOT\nremoved by an editor's micro-decision.\n\nSomeone else may be able to reconstruct the argument that was made\nin Montreal, but I believe we basically agreed that the charset\nfield was unnecessary because of availability of the RFC1522 method.\n\nAnd I think it may be irrelevant that the spec says \"the default language\nis English\" (section 14.45), since there is no way for a user\nto explicitly negotiate what language the Warning will arrive in.\nThe actual default is whatever the implementor wants it to be.\n\nPresumably, either the user understands the language, or the user\ndoesn't understand the language, but either way, would a language\ntag make it any easier for the browser to display the Warning in\na readable form?\n\nAgain, I know very little about i18n; is it actually the case that you\ncannot always render the text with knowledge of the character set\nalone, but must also know the language as well?\n\nIf it is, in fact, not possible to correctly render the Warning\ntext without a language tag, then perhaps this is a serious enough\nbug to warrant changing the HTTP/1.1 spec.  (This is, after all,\nwhat the 6-month delay before Draft Standard is meant to allow!)\nOtherwise, what actual problem is left unsolved?\n\n-Jeff\n\n\n\n", "id": "lists-010-13487668"}, {"subject": "Re: (INTEGOK) rough consensu", "content": "        To compute an MD5 signature for textual material,\nI would like to see the following applied:\n\n            o   trailing white space stripped\n\n            o   remaining whitespace reduced to single blanks\n\n            o   CR/LF ignored\n\n        The justification for this is that something cut-n-pasted\nshould still pass.   No,  we're not going to splice the TCP stream\nand insert cut-n-pasted text,  but the above will make digests work on\nthe widest range of interoperable platforms.  What I'm saying is that\ncut-n-paste-ability isn't the goal,  it represents a sample of the\nkind of mangling that might happen when the text crosses some boundaries.\nInteroperability is the goal.   Make MD5 verification work cross-platform.\n\n--\nRick Troth <troth@casita.houston.tx.us>, Houston, Texas, USA\nhttp://casita.houston.tx.us/~troth/\n\n\n\n", "id": "lists-010-1348839"}, {"subject": "Re: AcceptCharset suppor", "content": "On Dec 18, 12:03am, Keld J|rn Simonsen wrote:\n\n[in amongst generally good stuff ]\n\n> Of cause it complicates matters\n> with yet another parameter, but it could help in chosing an\n> appropiate font, and then it is the right concept.\n\nSequences of bytes, sequences of characters, and sequences of glyphs\nare not the same thing. The charset does not, necessarily, map 1:1 to\nthe font encoding vector. The characters might not even be displayed\nvisually - they might be spoken for example.\n\n> I note that a MIME charset identifies a repertoire,\n\nNot necessarily. It identifies a mapping from  (one or more)\nbytes to characters. In the particular case of HTML it does not identify\na repertoire at all. It is possible to write a document that contains the\nentire character repertoire of 10646, and have it correctly labelled as\nUS-ASCII - just using numeric character references.\n\n\n\n\n\n-- \nChris Lilley, W3C                          [ http://www.w3.org/ ]\nGraphics and Fonts Guy            The World Wide Web Consortium\nhttp://www.w3.org/people/chris/              INRIA,  Projet W3C\nchris@w3.org                       2004 Rt des Lucioles / BP 93\n+33 (0)4 93 65 79 87       06902 Sophia Antipolis Cedex, France\n\n\n\n", "id": "lists-010-13497605"}, {"subject": "Re: AcceptCharset suppor", "content": "Klaus Weide writes:\n\n> On Sat, 14 Dec 1996, Keld J=F8rn Simonsen wrote:\n> > I have suggested that we introduce a repertoire identification\n> > in IP protocols, to address that issue.=20\n> \n> Since this goes beyond HTTP or the www, I would be interested to know\n> where (in which forum) you have made that proposal; and whether it is=20\n> more likely to be considered there. :)\n\nIt was for the IAB character set report - the one recommending 10646\nas the character set for all internet protocols.\n\n> This may be more useful for other protocols (MIME for mail, news etc.)\n> than HTTP and other formats than HTML, because HTML already implies\n> that all 10646 characters can occur (according to the i18n draft;\n> numeric character references to all 10646 characters are valid,\n> independend from the character encoding used for transfer of a\n> document; as has recently been pointed out).  But text/plain is also\n> still part of the Web, IMHO...\n> \n> Having a way to externally identify character repertoire may lead to\n> faster acceptance of UTF-8 as character encoding, since one can then\n> use UTF-8 without losing the repertoire information implied by the\n> currenlt used charsets.\n\nI also saw Martin's remark here, and I am not so sure about it.\nMy main purpose for proposing a repertoire identifier/header\nwas from an architectural  point of view, to align with the\nconcept from ISO on character sets. Of cause it complicates matters\nwith yet another parameter, but it could help in chosing an\nappropiate font, and then it is the right concept.\n\nI note that a MIME charset identifies a repertoire, and you could then\nuse the mime charsets as also parameters here. From a practical\nview there are only a limited set of repertoires, (I don't think\nthe N*N or maybe N! sets are feasible). Included scould also be\nthe subrepertoires identified in 10646. Repertoires are also a\nkey concept for transliteration, which are needed especially when\npeople want to make something out of a text, but does not understand\nthe script, eg cyrillic, or indic.\n\nKeld\n\n\n\n", "id": "lists-010-13506380"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "The Montreal IETF took place *after* the end of the last call.\n\nSee for instance the thread about the \"charset flap\" in\n><http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q2/thread.html>, with\n>the end in Q3\n><http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q3/thread.html> as well\n>as another thread \"proposed HTTP changes for charset\".\n\nInteresting reading.  I was not present in Montreal, and I did not remember\nthe resulting thread.  I guess I'm not very qualified as a historian of this\nissue.\n\n[...]\n>>I'm not an expert on this small draft business, but I think it will be\n>>difficult to have a small draft which changes HTTP/1.1 semantics (instead of\n>>just adding to semantics or clarifying semantics) without also having a\n>>procedural delay.  I believe it is ultimately up to the IESG, though.\n>\n>RFC 2026 (The Internet Standards Process -- Revision 3) contains this text\n>in section 6.2:\n>\n>   [...] the standards track.  However, deferral of changes to the next\n>   standards action on the specification will not always be possible or\n>   desirable; for example, an important typographical error, or a\n>   technical error that does not represent a change in overall function\n>   of the specification, may need to be corrected immediately.  In such\n>   cases, the IESG or RFC Editor may be asked to republish the RFC (with\n>   a new number) with corrections, and this will not reset the minimum\n>   time-at-level clock.\n>\n>IMHO, the following four points in HTTP/1.1 are in error and could (and\n>should!) be corrected by taking advantage of that language.\n\nI don't think you can take advantage of the language above.  The magic words\nare \"that does not represent a change in overall function of the\nspecification\".  Changing the charset defaults _would_ represent a change in\nthe overall function of the specification.\n\n[...]\n>>  I gather that the Warning header definition is extremely\n>>yucky from a i18n standpoint, but that does not justify changing it.  There\n>>are plenty of yucky headers in the protocol, but the headers are not meant\n>>for human consumption, so who cares about taste?  Stability is more\n>>important.\n>\n>The text in Warning: headers is meant for human consumption, especially in\n>the case of a 99 warning.\n\nAh, but that text only gets consumed by humans _after_ the user agent has\nstripped off the yucky bits.  The complete warning header is never seen by\nend users, even if it has the 99 code.\n\n>Fran?ois Yergeau <yergeau@alis.com>\n\nKoen.\n\n\n\n", "id": "lists-010-13515926"}, {"subject": "Re: AcceptCharset suppor", "content": "Klaus Weide writes:\n\n> Pages that do the poor-man's negotiation of letting the user select\n> a \"language\" manually, than return a page whose charset may vary \n> depending on the language choice.\n>    <URL: http://www.alis.com/internet_products/language.en.html>\n>    <URL: http://www.accentsoft.com/>\n>    <URL: http://www.dkuug.dk/maits/>   \n\nWell, the maits page should also do language negotiation, but then\nthere is an explicit language selection available too.\n\nKeld\n\n\n\n", "id": "lists-010-13526817"}, {"subject": "Re: AcceptCharset suppor", "content": "Martin J. Duerst wrote:\n> \n> On Mon, 16 Dec 1996, Klaus Weide wrote:\n> \n> \n> > Example of a site where documents are provided in several charsets\n> > (all for the same language):\n> > see <URL: http://www.fee.vutbr.cz/htbin/codepage>.\n> \n> The list is impressive. It becomes less impressive if you realize\n> that all (as far as I have checked) the English pages and some\n> of the Check pages (MS Cyrillic/MS Greek/MS Hebrew,...) are just\n> plain ASCII,\n\nAnd even less impressive when one finds out that\nnot a single page Displays the 'diacritics' correctly,\neven when one selects Latin-2 encoding in the Netscape 3.\n \n  Why is that? Are the experts building an Edsel?\n\n> > It is certainly much easier to make a Web clients able to decode UTF-8\n> > to locally available character sets, than to upgrade all client\n> > machines so that they have fonts available to display all of the 10646\n> > characters.\n\n      Besides, character set is not function of a language,\n   math, APL, music, all have different need for character sets\n   and character sets mixtures which can exceed 10646\n> \n> Definitely UTF-8 should be encouraged. But that's not done by\n> introducing new protocol complications and requiring the servers\n> to deal with unpredictable transliteration issues that can be\n> dealt with more easily on the client side.\n> \n> Regards,        Martin.\n\n-- \n Peter O.B. Mikes      pom@llnl.gov     \nhttp://edprog.llnl.gov/team/pom.html\n\n\n\n", "id": "lists-010-13534951"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "Martin J. Duerst:\n>\n>On Mon, 16 Dec 1996, Koen Holtman wrote:\n>\n>> Martin J. Duerst:\n>> >\n>> >Hello Koen,\n>\n>> >> The TEXT encoding was US-ASCII in HTTP/1.0 (RFC1945),\n>> >\n>> >Not true. RFC1945 explicitly allows octets from character sets\n>> >other than US-ASCII (which means octets with the 8th bit set).\n>> \n>> Yes, but it does not tell you how to interpret octets with the 8th bit set.\n>> So the bottom line is that you can depend on US-ASCII and nothing else.\n>\n>If that's the bottom line, why was the ISO-8859-1 default introduced?\n\nUS-ASCII is the bottom line for HTTP/1.0.  I told you before that we\nintroduced ISO-8859-1 in HTTP/1.1.  Different protocols, different defaults.\n\n[...]\n>> I'm not an expert on this small draft business, but I think it will be\n>> difficult to have a small draft which changes HTTP/1.1 semantics (instead of\n>> just adding to semantics or clarifying semantics) without also having a\n>> procedural delay.  I believe it is ultimately up to the IESG, though.\n>\n>If that is a problem, there is an easy way out: We just don't tell\n>the IESG about the changes we want to make until after the big\n>HTTP/1.1 is out.\n\nI don't think we are allowed to do that.  It would be kind of dishonest\nanyway.\n\nLook, it all boils down to procedural matters.  HTTP/1.1 is at a procedural\nstage where we do not want to change it anymore.  It does not really matter\nthat the change would be trivial.  Trivial changes which reverse design\nchoices are not permitted if delays are to be avoided:  only huge bugs may\nbe fixed.\n\nWe may also add notes like \"This Warning header is a very bad example of how\nto do i18n; please avoid constructs like this in future protocols, and do X\ninstead\", but we cannot change the mechanisms in the spec itself.\n\n> Anybody know any more details?\n\nSee RFC1602.\n\n[...]\n>In an earlier mail, you have mentionned that there is always\n>a new version. Now you say that stability is more important.\n\nThe stability of the 1.1 document is important at the current stage.  People\nare implementing it now, and we don't want these people to tear up code they\nhave already written because the document changed.  This is why the\nprocedure does not allow changes.\n\n>If you cite stability already at this point, won't you stress\n>it much more when the issue will be raised again when going\n>to a new version?\n\nNo, absolutely not.  I'm against it for 1.1 because of procedural issues.\nIf people want it in 1.2 or 2.0 or NG or whatever, that would be fine with\nme.\n\n>Regards,Martin.\n\nKoen.\n\n\n\n", "id": "lists-010-13544571"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": ">Martin Duerst:\n>>I definitely don't want to delay the draft. But if we agree on\n>>the direction to go in this issue, we can issue a small draft\n>>(e.g. Encoding of Headers in HTTP) to clear up the issue.\n>>This should neither delay the IETF process, nor will it delay\n>>implementations to wait for HTTP 1.2 to do the right thing.\n>\n>I'm not an expert on this small draft business, but I think it will be\n>difficult to have a small draft which changes HTTP/1.1 semantics (instead of\n>just adding to semantics or clarifying semantics) without also having a\n>procedural delay.  I believe it is ultimately up to the IESG, though.\n\nRFC 2026 (The Internet Standards Process -- Revision 3) contains this text\nin section 6.2:\n\n   [...] the standards track.  However, deferral of changes to the next\n   standards action on the specification will not always be possible or\n   desirable; for example, an important typographical error, or a\n   technical error that does not represent a change in overall function\n   of the specification, may need to be corrected immediately.  In such\n   cases, the IESG or RFC Editor may be asked to republish the RFC (with\n   a new number) with corrections, and this will not reset the minimum\n   time-at-level clock.\n\nIMHO, the following four points in HTTP/1.1 are in error and could (and\nshould!) be corrected by taking advantage of that language.  Especially\nsince RFC publication has not yet occurred.\n\n1) The default charset for text entities is ISO 8859-1.\n\n2) All clients can be assumed to support ISO 8859-1.\n\n3) The default charset for the Warning: header is ISO 8859-1.\n\n4) The default language for the Warning: header is English.\n\nThe first two are statements of fact that are demonstarbly wrong on today's\nWeb.  The last two are obstacles to i18n that are not justified by any\ntechnical requirements.  All four should be modified or deleted.\n\n>As for the direction on this issue: I'm not convinced that there is anything\n>that needs fixing.  I gather that the Warning header definition is extremely\n>yucky from a i18n standpoint, but that does not justify changing it.  There\n>are plenty of yucky headers in the protocol, but the headers are not meant\n>for human consumption, so who cares about taste?  Stability is more\n>important.\n\nThe text in Warning: headers is meant for human consumption, especially in\nthe case of a 99 warning.\n\nRegards,\n\n\n-- \nFran?ois Yergeau <yergeau@alis.com>\nAlis Technologies Inc., Montr?al\nT?l : +1 (514) 747-2547\nFax : +1 (514) 747-2561\n\n\n\n", "id": "lists-010-13554859"}, {"subject": "PLEASE GET ME OFF!!!!", "content": "     Sorry to bother so many people but I do not know the email of the list\nmanager. I wish to be taken off the mailing list.  Please get me off... .\nThank you.\n\n      always;\n    Beng Kiam\n    ---------\n\n\n\n", "id": "lists-010-13565597"}, {"subject": "Re: (INTEGOK) rough consensu", "content": ">      Note: the net result of the above is that the digest is\n>      computed on the content that would be sent over-the-wire, in\n>      network byte order, but prior to any transfer coding being\n>      applied.\n\nI basically agree with Roy about this section -- the simpler the prose is on\nthis point, the better.\n\nHowever, there is still one problem remaining: The use of the term \"network\nbyte order\". It is wrong to use this term in this context. Network byte\norder refers quite specifically to the ordering of bytes in network address\ninformation. It is not only nonsensical to talk about a generic sequence\nof octets being in network byte order, it is outright incorrect, as there\nmay be media types defined with embedded network address information in them\nthat isn't presented in network byte order.\n\nThe bytes ungoing checksumming are in whatever order the media type puts them\nin for transfer of the content across the wire.\n\nNed\n\n\n\n", "id": "lists-010-1356720"}, {"subject": "PLEASE GET ME OFF!!!!! Repl", "content": "It's a bit kafkaesque, try\nwww-international-request@w3.org\nor\nwww-international@www10.w3.org\nor\ncarrasco@innet.lu\n\nBe happy in the new year,\nA. Almeida\n\n\n\n", "id": "lists-010-13573562"}, {"subject": "Re: AcceptCharset suppor", "content": "On Tue, 17 Dec 1996, Peter O.B. Mikes wrote:\n\n>       Besides, character set is not function of a language,\n>    math, APL, music, all have different need for character sets\n>    and character sets mixtures which can exceed 10646\n\nMath is a different issue. It was in the HTML 3.0 proposal, and some\npeople I guess are still working at it. It needs its own special\ndescription. The same applies to music of course. Thinking about a\nscore as a text doesn't make sense in technical terms.\nAs for APL, there has been ample work to make sure it is covered.\nIf you know about something that is not covered, please submit\nthat to Unicode and your national standard body.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13581929"}, {"subject": "Re: AcceptCharset suppor", "content": "On Wed, 18 Dec 1996, Keld J|rn Simonsen wrote:\n\n> I also saw Martin's remark here, and I am not so sure about it.\n> My main purpose for proposing a repertoire identifier/header\n> was from an architectural  point of view, to align with the\n> concept from ISO on character sets. Of cause it complicates matters\n> with yet another parameter, but it could help in chosing an\n> appropiate font, and then it is the right concept.\n\nChoosing the right font (or in many cases the right fontS)\nis a matter that can be highly complex. For some clues to\nwhat issues are involved, please see\nftp://ftp.ifi.unizh.ch/pub/multilingual/FontComposition.ps.Z\nA repertoire is not sufficient for these cases, while having\na look at the document itself will help a lot (and will also\nhelp in the simple cases, as has been pointed out before).\n\n\n> I note that a MIME charset identifies a repertoire, and you could then\n> use the mime charsets as also parameters here. From a practical\n> view there are only a limited set of repertoires, (I don't think\n> the N*N or maybe N! sets are feasible).\n\nI agree that it's not the worst case of 2**N (much smaller than N!,\nbut bigger than N*N). \n\n>Included scould also be\n> the subrepertoires identified in 10646.\n\nHow many are there? And how many \"charset\"s do we have? My guess\nis that altogether, there might be around 500. Because repertories\ncan be combined, you end up with 2**500, which is around 1E150.\nAs you can see, we get into pretty large numbers rather quickly.\nWe can start to try to separate useful and less usefull combinations,\nbut that will be more work than assuring that all of ISO 10646 can\nbe displayed somehow.\n\n\n> Repertoires are also a\n> key concept for transliteration, which are needed especially when\n> people want to make something out of a text, but does not understand\n> the script, eg cyrillic, or indic.\n\nIf you move transliteration to the client side, repertories are\nmuch less of an issue. If you are going for \"graphic transliteration\",\nsuch as replacing u-umlaut by u\", they are not needed. For language-\ndependent transliteration (using ue in the above case), language\ninformation is needed, but again no repertoiry.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13590958"}, {"subject": "Re: sub-repertoires (was: AcceptCharset support", "content": "On Tue, 17 Dec 1996, Klaus Weide wrote:\n\n> On Tue, 17 Dec 1996, Martin J. Duerst wrote:\n> > On Mon, 16 Dec 1996, Klaus Weide wrote:\n> > \n> > > Currently accept-charset is de facto used as an expression of _two_\n> > > capabilities: (1) to decode a character encoding, (2) to be able to\n> > > display (or take responsibility for) a certain character repertoire.\n> > \n> > This is just because current clients don't actually do much of a\n> > decoding, they just switch to a font they have available.\n> \n> 1) I don't think that is tru for all current clients,\n> 2) the effect is the same.\n> \n> [...]\n> > > Example of a site where documents are provided in several charsets\n> > > (all for the same language):\n> > > see <URL: http://www.fee.vutbr.cz/htbin/codepage>.\n> > \n> > The list is impressive. It becomes less impressive if you realize\n> > that all (as far as I have checked) the English pages and some\n> > of the Check pages (MS Cyrillic/MS Greek/MS Hebrew,...) are just\n> > plain ASCII, and don't need a separate URL nor should be labeled\n> > as such in the HTTP header. They could add a long list of other\n> > encodings, and duplicate their documents, [...]\n> \n> Right.  There are still at least 3 different (in a relevant way)\n> repertoires.\n> \n> If those could be labelled (and negotiated) as charset=UTF-8 + \n> repertoire indication,  less duplication would be needed.\n> \n> > > It is certainly much easier to make a Web clients able to decode UTF-8\n> > > to locally available character sets, than to upgrade all client\n> > > machines so that they have fonts available to display all of the 10646\n> > > characters.\n> > \n> > The big problem is not fonts. A single font covering all current ISO\n> > 10646 characters can easily be bundeled with a browser. The main\n> > problem is display logic, for Arabic and Indic languages in particular.\n> \n> I am interested in having something that could work well with UTF-8\n> even on a vtxxx terminal or a Linux console (in cases where that makes\n> sense).\n\nIf such a beast has the abilities to:\n\n- Display a large set of glyphs\n- Not assume one character == one display cell\n- Have the possibility to insert some code between a charater string\nand a glyph sequence for appropriate conversion\n\nthen it is mainly a matter of time and effort.\n\n\n> > Definitely UTF-8 should be encouraged. But that's not done by\n> > introducing new protocol complications and requiring the servers\n> > to deal with unpredictable transliteration issues that can be\n> > dealt with more easily on the client side.\n> \n> I am not thinking of requiring a server to do anything.  Just being\n> able to say \n>  Content-type: text/plain;charset=utf-8; charrep=\"latin-1,latin-2,koi8\"\n> (made-up syntax, may be fatally flawed) for those who wish to do so;\n> and something equivalent for the \"accept-*\" side.  Nothing mandatory, let\n> everybody who doesn't care default to the currently implied full 10646 \n> repertoire.  I think the examples show that people are doing the\n> equivalent now (whether accidentally or not).\n\nI showed that simple transliteration is much better handled on the\nclient than on the server. For accept-*, your main justification\nseems transliteration. I think it is futile to invest in protocol\nmechanisms that will never take on because no server will be ready\nto do something that the client can do much easier.\nAs for charrep itself, the document says it all. No need for anything\nelse.\n\nRegards,Martin.\n\n\n\n", "id": "lists-010-13601410"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "I wrote:\n    I can't remember exactly why or when the language-tag and charset\n    fields were removed.  However, I do remember that we had a\n    moderately long discussion of this issue at the Montreal IETF\n    meeting; it was NOT removed by an editor's micro-decision.\n\nActually, I slightly mis-remembered that.  We did have a discussion\nof I18N issues at Montreal, but the discussion that led to the\nsimplification of the Warning header syntax was held at the\n(earlier) Los Angeles IETF.\n\n-Jeff\n\n\n\n", "id": "lists-010-13613032"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "? 23:58 17-12-96 +0100, Koen Holtman a ?crit :\n>The Montreal IETF took place *after* the end of the last call.\n\nI'm afraid not. The deadline was July 5th, see\n<http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q2/thread.html>.  It\nwas late, very late (mea culpa, I was too busy before) but not too late.\n\n>>RFC 2026 (The Internet Standards Process -- Revision 3) contains ...\n>\n>I don't think you can take advantage of the language above.  The magic words\n>are \"that does not represent a change in overall function of the\n>specification\".  Changing the charset defaults _would_ represent a change in\n>the overall function of the specification.\n\nLet's review my 4 points in this light:\n\n1) (Default entity charset is Latin-1) This is plain wrong, various clients\nassume other defaults depending on their users needs.  Netscape, Microsoft\nand others will not stop letting their users set their own default because\nthe spec says Latin-1.  Dropping that language would not change the\nprotocol, it would align the spec with reality.\n\n2) (All clients support Latin-1) This is not true, implementers will have\nto live with it, and again dropping the false claim would align the spec\nwith reality, not change the functionning of anything.  I just don't\nbelieve all low-end browsers will automagically start supporting Latin-1 on\nnon-Latin-1 platforms because the HTTP/1.1 says so. Implementers relying on\nthat statement will be misled.\n\n3&4) (Latin-1 and English defaults for Warning:) Since the text is not\nprocessed by the protocol (only the numeric code), the functionning would\nnot be impacted by changing the defaults to something sensible (like UTF-8\nand None).\n\n>>The text in Warning: headers is meant for human consumption, especially in\n>>the case of a 99 warning.\n>\n>Ah, but that text only gets consumed by humans _after_ the user agent has\n>stripped off the yucky bits.  The complete warning header is never seen by\n>end users, even if it has the 99 code.\n\nUTF-8 bits are no more yucky than ASCII; both are just encodings for\ncharacters.\n\n\nThis is not an issue for HTTP 1.2, but with 1.1.  If Latin-1 goes as the\nWarning header default, I'm sure arguments will be made when designing 1.2\nthat compatibility with 1.1 must be preserved at all cost, just as this\nargument was made repeatedly for 1.1 vs 1.0.  There is no coming back.\n\nI think RFC 2026 allows the WG to fix those problems without delaying\nHTTP/1.1 at all, without endangering its status as Proposed Standard and\nwithout resetting the clock for Draft Standard.\n\nRegards,\n-- \nFran?ois Yergeau <yergeau@alis.com>\nAlis Technologies Inc., Montr?al\nT?l : +1 (514) 747-2547\nFax : +1 (514) 747-2561\n\n\n\n", "id": "lists-010-13620405"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "Francois Yergeau:\n>\n>? 23:58 17-12-96 +0100, Koen Holtman a ?crit :\n>>The Montreal IETF took place *after* the end of the last call.\n>\n>I'm afraid not. The deadline was July 5th, see\n><http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q2/thread.html>.\n\nOops, seems you are right.  I distincly remember the IESG itself planning to\nend the last call before the Montreal IETF, so I don't know where that date\ncame from.\n\n[...]\n>>I don't think you can take advantage of the language above.  The magic words\n>>are \"that does not represent a change in overall function of the\n>>specification\".  Changing the charset defaults _would_ represent a change in\n>>the overall function of the specification.\n>\n>Let's review my 4 points in this light:\n>\n>1) (Default entity charset is Latin-1) This is plain wrong, various clients\n>assume other defaults depending on their users needs.\n\nThese various clients are 1.0 clients.  \n\nThe definition of the 1.1 default can't be `plain wrong' because some 1.0\nclients use other defaults.  We are dealing with a normative statement for\n1.1 here, not with a description of best current practice.  Such a normative\nstatement can only be wrong if it leads to an internal contradiction in the\ndraft.\n\n>2) (All clients support Latin-1) This is not true,\n\nSame reply as 1).  Note that 1.1 does not _require_ all 1.1 clients to\nimplement Latin-1, it only requires them to find it acceptable even if they\nonly partially implement it.\n\n>3&4) (Latin-1 and English defaults for Warning:) Since the text is not\n>processed by the protocol (only the numeric code), the functionning would\n>not be impacted by changing the defaults to something sensible (like UTF-8\n>and None).\n\nThe text is processed when the 99 code is used, and clients are free to\nprocess the text when seeing other codes as well.  So changing the defaults\ndoes have an impact.\n\nOne final remark:  I have the feeling this thread is going in circles.  I'll\ntry to stop sending responses.\n\n>Fran?ois Yergeau <yergeau@alis.com>\n\nKoen.\n\n\n\n", "id": "lists-010-13631085"}, {"subject": "Re: AcceptCharset suppor", "content": "Klaus Weide:\n>\n[...]\n>Examples where \"Language\" is treated as carrying charset meaning\n>(not just repertoire, but \"charset\" including encoding):\n\nFirst, thanks for collecting these examples.\n\n>Pages that do the poor-man's negotiation of letting the user select\n>a \"language\" manually, than return a page whose charset may vary \n>depending on the language choice.\n>   <URL: http://www.alis.com/internet_products/language.en.html>\n>   <URL: http://www.accentsoft.com/>\n>   <URL: http://www.dkuug.dk/maits/>   \n\nHm, I would not call this `language carrying implied charset meaning'.  If I\nwere to make a page like the ones above, I would leave out all mention of\nrequiring support for the appropriate charset too, because of stylistic\nreasons.\n\n>Another example, which does \"real\" (automatic) negotiation:\n>   <URL: http://www.dkuug.dk:81/maits/summary>\n>(For example, with \"accept-language: el, en\" you get Greek in iso-8859-7\n>- even when also sending an \"accept-charset\" which excludes iso-8859-7.)\n\nI guess this is a case of a CGI programmer making something that is good\nenough in the usual case instead of 100% optimal.  How many current user\nagents send a (reliable) Accept-Charser header anyway?\n\nSo I don't interpret your examples as the Accept-Language/Content-language\nheaders carrying charset meaning.  Which is good, actually, because if they\nhad carried charset meaning, that would have created a nasty negotiation\nproblem.\n\n>  Klaus\n\nKoen.\n\n\n\n", "id": "lists-010-13641100"}, {"subject": "TRACE and MaxForward", "content": "Here are the proposed changes to the TRACE method and operation.\nIt is a context diff of the changes necessary to complete the TRACE\nmethod for HTTP/1.1.\n\nThis is actually an old topic (discussed last November) but this is\nthe first time that the changes have been put into real words.\n\n.....Roy\n===================================================================\n*** draft-ietf-http-v11-spec-01.txtMon Feb 12 16:37:14 1996\n--- trace.txtTue Apr  2 18:03:35 1996\n***************\n*** 1657,1660 ****\n--- 1657,1661 ----\n                        | Host                     ; Section 10.22\n                        | If-Modified-Since        ; Section 10.23\n+                       | Max-Forwards             ; Section 10.yy\n                        | Proxy-Authorization      ; Section 10.31\n                        | Range                    ; Section 10.33\n***************\n*** 2397,2409 ****\n  8.12  TRACE\n  \n!    The TRACE method requests that the server identified by the \n!    Request-URI reflect whatever is received back to the client as the \n!    entity body of the response. In this way, the client can see what \n!    is being received at the other end of the request chain, and may \n!    use this data for testing or diagnostic information.\n  \n!    If successful, the response should contain the entire, unedited \n!    request message in the entity body, with a Content-Type of \n!    \"message/http\", \"application/http\", or \"text/plain\". Responses to \n!    this method are not cachable.\n  \n--- 2398,2422 ----\n  8.12  TRACE\n  \n!    The TRACE method is used to invoke a remote, application-layer\n!    loopback of the request message.  The final recipient of the request\n!    should reflect the message received back to the client as the\n!    entity body of a 200 (OK) response.  The final recipient is either\n!    the origin server or the first proxy or gateway to receive a\n!    Max-Forwards value of zero (0) in the request (see Section 10.yy).\n  \n!    A TRACE request must not include an entity body and must include a\n!    Content-Length header field with a value of zero (0).\n  \n+    TRACE allows the client to see what is being received at the other\n+    end of the request chain and use that data for testing or diagnostic\n+    information.  The value of the Via header field (Section 10.xx) is of\n+    particular interest, since it acts as a trace of the request chain.\n+    Use of the Max-Forwards header field allows the client to limit the\n+    length of the request chain, which is useful for testing a chain of\n+    proxies forwarding messages in an infinite loop.\n+ \n+    If successful, the response should contain the entire request message\n+    in the entity body, with a Content-Type of \"message/http\",\n+    \"application/http\", or \"text/plain\".  Responses to this method must\n+    not be cached.\n+ \n***************\n*** 3891,3893 ****\n--- 3904,3931 ----\n  \n+ 10.yy  Max-Forwards\n+ \n+    The Max-Forwards general-header field may be used with the TRACE\n+    method (Section 8.12) to limit the number of times that a proxy or\n+    gateway can forward the request to the next inbound server.  This can\n+    be useful when the client is attempting to trace a request chain\n+    which appears to be failing or looping in mid-chain.\n+ \n+        Max-Forwards   = \"Max-Forwards\" \":\" 1*DIGIT\n+ \n+    The Max-Forwards value is a decimal integer indicating the remaining\n+    number of times this request message may be forwarded.\n+ \n+    Each proxy or gateway recipient of a TRACE request containing a\n+    Max-Forwards header field should check and update its value prior to\n+    forwarding the request.  If the received value is zero (0), the\n+    recipient should not forward the request; instead, it should respond\n+    as the final recipient with a 200 response containing the received\n+    request message as the response entity body (as described in\n+    Section 8.12).  If the received Max-Forwards value is greater than\n+    zero, then the forwarded message should contain an updated\n+    Max-Forwards field with a value decremented by one (1).\n+ \n+    The Max-Forwards header field should be ignored for all other methods\n+    defined by this specification and for any extension methods for which\n+    it is not explicitly referred to as part of that method definition.\n+  \n\n\n\n", "id": "lists-010-1364789"}, {"subject": "Re: AcceptCharset suppor", "content": "Martin J. Duerst:\n[...]\n> This results from the fact that there is no\n>specification about relative priority of Accept headers, or\n>how to combine them.\n\nThe HTTP/1.1 specification does not define `dimension X always takes\npriority over Y', but it *does* allow such priorities to be expressed if\nthey exist for a user agent.\n\nUsing quality factors, a user agent can express which consideration takes\npriority.  For example, with\n\nAccept-Language: en;q=1.0, he;q=0.2\nAccept-Charset: <latin-1>;q=0.9, <latin-5>;q=1.0\n\ngetting the english language would have priority over getting <latin-5>.\nAnother user agent could give priority to getting <latin-5> by sending\n\nAccept-Language: en;q=1.0, he;q=0.9\nAccept-Charset: <latin-1>;q=0.2, <latin-5>;q=1.0 .\n\nNote that, both historically and in the upcoming transparent content\nnegotiation specification, quality factors are combined by multiplying them.\n\n[...]\n>This does not work with your example, but assume\n>you specify Polish and Czech and ISO-8859-2, and the server\n>has the document in Russian and Hungarian, you might get the\n>document in Hungarian (because it is ISO-8859-2) even if you\n>will understand more in Russian.\n\nNote that under transparent content negotiation, you will *never* get a\nHungarian document if you specify Polish and Czech and ISO-8859-2.  You will\nget a list of available documents instead.  Under plain HTTP/1.x, you might\nindeed get the Hungarian version.\n\n>Regards,Martin.\n\nKoen.\n\n\n\n", "id": "lists-010-13650511"}, {"subject": "New MIMEtype to ask for HTML HEADs? Other suggestions", "content": "I would to raise a issue I want to thing about during Xmas, sure\nyou can make some valuable suggestion.\n\nWe are beginning to have some valuable head part in HTML documents;\nMETA, PICS (?), TITLE, LINK,...  so this part is beggining to\nbe a document on his own.\n\nNow, I wonder which would be the adequate method to retrieve all\nthis information. Clearly the old proposal of traslating META\nto www-*: and then sending it in a HTTP HEAD is impractical,\nboth by size and consistency considerations.\n\nSo I see two alternatives: To ask for other\nfile type, say text/htmlhead, or to ask for \nsome specific feature list whose answers results to be\nonly the head of the html.\nThe first alternative has the adventage of compatibily with 1.0,\nso it could be done with current servers.\n\nI can not think of other ways. A new HTTP method, by example,\nwould be excesive, and would introduce an spureous dependence\nbetween html and http.  \n\n\nAlejandro Rivero\n\n         \n\n\n\n", "id": "lists-010-13660241"}, {"subject": "Re: New MIMEtype to ask for HTML HEADs? Other suggestions", "content": "    We are beginning to have some valuable head part in HTML documents;\n    META, PICS (?), TITLE, LINK,...  so this part is beggining to be a\n    document on his own.\n\n    Now, I wonder which would be the adequate method to retrieve all\n    this information. Clearly the old proposal of traslating META to\n    www-*: and then sending it in a HTTP HEAD is impractical, both by\n    size and consistency considerations.\n\n    So I see two alternatives: To ask for other file type, say\n    text/htmlhead, or to ask for some specific feature list whose\n    answers results to be only the head of the html.  The first\n    alternative has the adventage of compatibily with 1.0, so it could\n    be done with current servers.\n\n    I can not think of other ways. A new HTTP method, by example, would\n    be excesive, and would introduce an spureous dependence between\n    html and http.\n\nHTTP/1.1 introduces the ability for a client to ask for a subrange\nof the bytes of a resource.  For example,\nGET /foo.html HTTP/1.1\nRange: bytes=0-1023\nwould retrieve the first Kbyte of the file (or to its end, whichever\nis shorter).  Later on, the client could do\nGET /foo.html HTTP/1.1\nRange: bytes=1023-\nto get the rest of the file.  (Note that this works for any file\ntype, not just HTML.)\n\nIf most HTML authors (or authoring tools) put the useful information\nat the very front of the file, then a client that wants to get the\nhead information, but not the entire file, can do this and succeed\nwith high probability.  If the useful information is longer than\nthe requested subrange, it's not a big problem; just request some\nmore.  If it's shorter, that's also not a problem; just ignore the\nextra stuff.\n\nNote that this is fully compatible with HTTP/1.0 servers, in the\nsense that the client will always receive at least as much data\nas it wants (although it may receive more), because HTTP/1.0 servers\nsimply ignore \"Range:\".\n\n-Jeff\n\n\n\n", "id": "lists-010-13668567"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "Jeffrey Mogul wrote:\n> Again, I know very little about i18n; is it actually the case that you\n> cannot always render the text with knowledge of the character set\n> alone, but must also know the language as well?\n\n(I also know very little about i18n.  These are free thoughts from me.)\nI don't think you *must* know it, but it can help.  I think it would\nbe beneficial if all texts are marked with natural language.  Here are\nsome ways a program might use the information on what language a text\nis in:\n\n * Hyphenate correctly\n * Transliterate the text into another character set before showing\nit.  How to transliterate might depend on what language the text is in.\n * Deciding which dictionary to use if the user can look up words in\nthe text in a dictionary from the rendering program.\n * Do an automatic translation into another language.  (A bit far-fetched...)\n\nSpecific for warning messages like those:\n * Decide if the warning text sent should be shown, or a stock\nmessage associated with that error number instead.\n\n-- \nPer Starback  <starback@minsk.docs.uu.se>  http://www.update.uu.se/~starback\n \"Life is but a gamble!  Let flipism chart your ramble!\"\n\n\n\n", "id": "lists-010-13677987"}, {"subject": "RE: New MIMEtype to ask for HTML HEADs? Other suggestions", "content": "An equally backwards-compatible way is to define a new header:\nAccept-Meta: header1, header2,...\nwhich could go along with GET, HEAD, etc. requests, and ask that the\ninfo corresponding to those headers be returned, but would be a hint not\nto include too much more.\n\nHTTP/1.0 and 1.1 servers that didn't understand Accept-Meta would send\nall the entity-headers, but others would returns a more manageable\nnumber.\n\n>----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Sent: Thursday, December 19, 1996 10:40 AM\n>To: Alejandro Rivero\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: New MIMEtype to ask for HTML HEADs? Other suggestions? \n>\n>    We are beginning to have some valuable head part in HTML documents;\n>    META, PICS (?), TITLE, LINK,...  so this part is beggining to be a\n>    document on his own.\n>\n>    Now, I wonder which would be the adequate method to retrieve all\n>    this information. Clearly the old proposal of traslating META to\n>    www-*: and then sending it in a HTTP HEAD is impractical, both by\n>    size and consistency considerations.\n>\n>    So I see two alternatives: To ask for other file type, say\n>    text/htmlhead, or to ask for some specific feature list whose\n>    answers results to be only the head of the html.  The first\n>    alternative has the adventage of compatibily with 1.0, so it could\n>    be done with current servers.\n>\n>    I can not think of other ways. A new HTTP method, by example, would\n>    be excesive, and would introduce an spureous dependence between\n>    html and http.\n>\n>HTTP/1.1 introduces the ability for a client to ask for a subrange\n>of the bytes of a resource.  For example,\n>GET /foo.html HTTP/1.1\n>Range: bytes=0-1023\n>would retrieve the first Kbyte of the file (or to its end, whichever\n>is shorter).  Later on, the client could do\n>GET /foo.html HTTP/1.1\n>Range: bytes=1023-\n>to get the rest of the file.  (Note that this works for any file\n>type, not just HTML.)\n>\n>If most HTML authors (or authoring tools) put the useful information\n>at the very front of the file, then a client that wants to get the\n>head information, but not the entire file, can do this and succeed\n>with high probability.  If the useful information is longer than\n>the requested subrange, it's not a big problem; just request some\n>more.  If it's shorter, that's also not a problem; just ignore the\n>extra stuff.\n>\n>Note that this is fully compatible with HTTP/1.0 servers, in the\n>sense that the client will always receive at least as much data\n>as it wants (although it may receive more), because HTTP/1.0 servers\n>simply ignore \"Range:\".\n>\n>-Jeff\n>\n>\n\n\n\n", "id": "lists-010-13686452"}, {"subject": "Authorization in chunked encodin", "content": ">>>>> \"PL\" == Paul Leach <paulle@microsoft.com>\n\n  ... writes re the interation between transfer-encoding and\n  content digest in authentication information.\n\nPL> Note also that in order to digest a chunked body, the digest has to go\nPL> in the footer, after the end of the chunk-encoded body. However, the\nPL> HTTP spec says that only headers that state that they are allowed in\nPL> footers are allowed in footers, and the relevant digest-auth header does\nPL> not so state.\n\nPL> CHANGES NEEDED TO THE DIGEST SPEC:\n\nPL> 1. Change section 2.1.3 \"The AuthenticationInfo Header\" to say that it\nPL> is allowed in the footer of a chunked encoded HTTP message.\n\n  This issue is also important in the context of servers for embedded\n  systems (such as our EmWeb product http://www.agranat.com/emweb.html ).\n\n  Because much of the data we serve is generated dynamically and\n  because in an embedded system it is not practical to buffer all the\n  data in order to compute a content digest before sending it, we\n  would very much like to see the change Paul suggests above to allow\n  the Authentication-info header to appear in the final chunk of a\n  chunk-encoded response.\n\n--\nScott Lawrence             Principal Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n", "id": "lists-010-13698817"}, {"subject": "HTTP 1.1 errata Internet Draft", "content": "PL> 1. Change section 2.1.3 \"The AuthenticationInfo Header\" to say that it\nPL> is allowed in the footer of a chunked encoded HTTP message.\n\nI was going to maintain a web page of errata for HTTP/1.1, but I was\nwondering if it might be better to document this in the Internet\nstandard way. We're asking for separate Internet Drafts for major\nchanges.\n\nVolunteers?\n\n\n\n", "id": "lists-010-13707685"}, {"subject": "Re: New MIMEtype to ask for HTML HEADs? Other suggestions", "content": "The WEBDAV group is considering the issues of associating metadata\nwith revisable documents, and there are some serious proposals.\n\nYou might want to review Yaron's draft to see if the mechanisms there\nare sufficient for these purposes.\n\n>    We are beginning to have some valuable head part in HTML documents;\n>    META, PICS (?), TITLE, LINK,...  so this part is beggining to be a\n>    document on his own.\n\n\n\n", "id": "lists-010-13714540"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "# RFC 2026 (The Internet Standards Process -- Revision 3) contains this text\n# in section 6.2:\n\n#   [...] the standards track.  However, deferral of changes to the next\n#   standards action on the specification will not always be possible or\n#   desirable; for example, an important typographical error, or a\n#   technical error that does not represent a change in overall function\n#   of the specification, may need to be corrected immediately.  In such\n#   cases, the IESG or RFC Editor may be asked to republish the RFC (with\n#   a new number) with corrections, and this will not reset the minimum\n#   time-at-level clock.\n\n# IMHO, the following four points in HTTP/1.1 are in error and could (and\n# should!) be corrected by taking advantage of that language.  Especially\n# since RFC publication has not yet occurred.\n\n# 1) The default charset for text entities is ISO 8859-1.\n# 2) All clients can be assumed to support ISO 8859-1.\n# 3) The default charset for the Warning: header is ISO 8859-1.\n# 4) The default language for the Warning: header is English.\n\n- These are not typographical errors.\n- These are not technical 'errors', in the sense that they were\n  conscious deliberate choices made by the HTTP working group after\n  lengthy discussion.\n\nIn addition, \n\n- Changing them WOULD result in a change in the function of the\n  specification.\n- They do not need to be corrected immediately.\n\n# The first two are statements of fact that are demonstarbly wrong on\n# today's Web. \n\nThey are not statements of fact, they are descriptions of the protocol\nbeing defined, HTTP/1.1, which did not exist before.\n\n# The last two are obstacles to i18n that are not justified by any\n# technical requirements.  All four should be modified or deleted.\n\nThey do not represent obstacles to i18n. I18N is perfectly possible\nand supported by HTTP/1.1. None of the proposed changes would actually\nIMPROVE i18n. Eliminating any other charset than UTF-8 would seriously\nhamper I18N, and if other charsets are to be allowed, RFC1522 encoding\nor some other equivalent is mandatory.\n\nI believe we have heard clearly from a number of working group members\nthat they would like to see this change made urgently. However, I also\nbelieve we have heard clearly from other working group members that\nthey would not like to see this change made. In some cases, the\ncurrent scheme is supported as is, and no change is desired. In some\nother cases, working group members believe that the issue could be\naddressed in some future specification, but not in HTTP/1.1.\n\nIn any case, I don't intend to act further on suggestions that we try\nto halt the progression of draft-ietf-http-v11-spec-07.* to RFC, and I\nwill continue to encourage those who wish to see changes to HTTP to\nwrite them in other Internet Drafts.\n\nLarry\n\n\n\n", "id": "lists-010-13723041"}, {"subject": "The problem with proxyrevalidate, and a proposed solutio", "content": "In the HTTP-WG session at the IETF meeting last week, we briefly\ndiscussed the problem proxy-revalidate.  Paul Leach and I volunteered\nto take this as an action item.\n\nHere are two excerpts from the current HTTP/1.1 spec:\n    When the must-revalidate directive is present in a response\n    received by a cache, that cache MUST NOT use the entry after it\n    becomes stale to respond to a subsequent request without first\n    revalidating it with the origin server. (I.e., the cache must do an\n    end- to-end revalidation every time, if, based solely on the origin\n    server's Expires or max-age value, the cached response is stale.)\n\n    [...]\n\n    The proxy-revalidate directive has the same meaning as the must-\n    revalidate directive, except that it does not apply to non-shared\n    user agent caches. It can be used on a response to an authenticated\n    request to permit the user's cache to store and later return the\n    response without needing to revalidate it (since it has already\n    been authenticated once by that user), while still requiring\n    proxies that service many users to revalidate each time (in order\n    to make sure that each user has been authenticated).\n\nNote that the only reasonable reading of this definition of\nproxy-revalidate is that\n\nif (acting as as shared cache) then\n    if (proxy-revalidate) and (entry age < max-age) then\nrevalidate before using this entry\n    else\nuse this entry without revalidating\n    end\nend\n\nHowever, the rest of the second paragraph doesn't make sense\nwith this reading, if one assumes that most end-client caches will\nobey the max-age directive.  That's because if the origin server\nwants such a client to be able to use an authenticated response\nfrom its cache without revalidating it, but wants a shared proxy\ncache to revalidate the response (to make sure that the new client\nhas been authenticated), then there is no way to express this.\nI.e., the origin server would like to set max-age > 0 to allow\nthe end-client to cache without revalidating, but would have to set\nmax-age=0 to force the proxy to revalidate.\n\nThis is my fault, I suspect, because I wrote this part of the\nspec.  I was trying to solve two problems with one feature, and\nI don't think that was possible.  The first problem was the\none described above, which was described to me by the authentication\npeople (primarily Paul Leach), and which Paul has now made me\nunderstand again.  The second problem was that we introduced\n\"must-revalidate\" to deal with caches that might be loose about\nobserving max-age, and several people observed that this is somewhat\ntoo strict for many applications, where it's OK for the end-client\ncache to be \"loose\", but it might not be OK for a proxy cache to be\n\"loose\".  So the current specification of proxy-revalidate actually\ndoes solve that problem.\n\nNote that \"Cache-control: private\" is very inefficient, because\nit forces a proxy to fully reload the entity body each time; it\ncannot do a conditional GET in this case.  So that's not really\na solution.\n\nThe problem with proxy-revalidate also makes it much harder to\ndo efficient hit-counting, *with or without* our proposed hit-metering\nextensions.\n\nAfter thinking about it for a while, and making up a table of all\nthe possible combinations, I've come to the conclusion that there\nis no way to express all of the possibly desirable policies with\nthe current set of cache-control directives, even if we tried to\nredefine the meaning of proxy-revalidate.  In order to make\nauthentication work without defeating caching entirely, we either\nneed to add at least one cache-control directive, or we would have\nto redefine proxy-revalidate in a way that would make it much\nless efficient.\n\nPaul and I believe that adding one more directive is the right\nsolution.  There are several possibilities that would work.\nOne would be something like\nCache-control: proxy-mustcheck\nwhich would be ignored by non-shared caches, but would mean that\na shared cache would have to revalidate entry regardless of its\nage (i.e., even if apparently \"fresh\").  In typical use, one might\nsee\nCache-control: max-age=30, proxy-mustcheck\nwhich allows all caches to store the response, and an end-client\nto use its cache entry for ages up to 30 seconds, but requires\na proxy to always revalidate.\n\nThe other approach, which Paul favors (and I think I do, too) is\ndefine something like\nCache-control: proxy-maxage=NNN\nwhich supplies a separate age limit that applies only to shared\ncaches.  For example, a server doing authentication or hit-metering\nmight send:\nCache-control: max-age=30, proxy-maxage=0\nwhich means the same thing as the previous example (but actually\ntakes one less byte to encode!).\n\nIt probably makes sense to define proxy-maxage as implying\n\"proxy-revalidate\", since otherwise one would have to send both\ndirectives, and it's likely that this would be the normal case.\n\nThe nice thing about proxy-maxage is that it allows other things,\ntoo, such as\nCache-control: max-age=30, proxy-maxage=10\nor even\nCache-control: max-age=10, proxy-maxage=30\nto provide finer control over the action of caches.  It's not\nimmediately clear to me that this is useful, but it seems to\nbe no more expensive to implement than \"proxy-mustcheck\", and\nit might turn out to be useful later on.\n\nI believe that Larry wants us to issue an Internet-Draft on\nthis topic, and not to discuss it to death on the mailing list.\nBut because there are several possible choices, I wanted to\nsee what people think before getting started on the Internet-Draft.\n\n-Jeff\n\nP.S.: Regarding IETF process: we discussed this at the meeting\nin San Jose, with the Area Director present.  If the consensus\nof the working group is that (1) this is truly a bug, and (2)\nthat the proposed fix is both small and not controversial, then\nthe IESG will probably allow us to make this kind of change\nwithout restarting at Proposed Standard.  So please don't get\nstarted now on a discussion about process; we can have that\nlater on, if there is still disagreement on the technical issues\nonce specific proposal(s) have been documented.\n\n\n\n", "id": "lists-010-13733023"}, {"subject": "Administrivia: getting off the lis", "content": "Folks,\n\nIf you wish to be removed from the http-wg mailing list, send a message to:\n\n  http-wg-request@cuckoo.hpl.hp.com\n          ^^^^^^^\nwith a Subject: line of:\n\n  unsubscribe\n\nPlease don't send unsubscribe messages to the http-wg mailing list itself.\n--\n(http-wg list owner)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-010-13746210"}, {"subject": "Re: INTEGOK: updated wordin", "content": "> Several comments have come in that are incorporated into the revised\n> wording attached. A synopsis of the reasons for the changes:\n\n> 1. Content-MD5 is not proof against malicious attacks\n\nI hope we all know this, but it never hurts to point it out.\n\n> 2. In both MIME and HTTP, the digest is computed on what would be sent\n> (modulo Transfer-Encoding or Content-Transfer-Encoding) AND (in\n> practice) the canonical form (modulo Content-Encoding); for text, the\n> canonical form differs slightly between the two. See the HTTP 1.0 spec,\n> appendix C, for the relationship of MIME media types and HTTP media\n> types.\n\nWell put. However, I have something of a problem with referring to this as a\ndifference between \"MIME and HTTP\". This is confusing in that the MIME\nspecification doesn't define Content-MD5, either in general or in some\nMIME-specific profile. RFC1864 does, it's the only other specification of\nContent-MD5, and it does so specifically for email transports, which doesn't\nnecessarily cover everything that uses MIME in some way.\n\nSomeone looking for the \"MIME specific definition of Content-MD5\" in MIME\nproper isn't going to find it, and even if we do eventually fold the definition\nof Content-MD5 into MIME proper it will be done in such a way that the\ntransports it is defined for are clearly specified.\n\nAs such, I think it would be better to refer to this a a difference between\n\"email usage as specified by RFC1864\" and HTTP usage.\n\n> 4. Binary media types can specify their own transmission byte orders, so\n> network byte order can't be mandated in the digest.\n\nGood. I just sent a message to the effect that this wording needed to be\nchanged -- please ignore it.\n\n> The Content-MD5 entity-header field is an MD5 digest of the entity-body,\n> as defined in RFC 1864 [xx], for the purpose of providing an end-to-end\n> message integrity check (MIC) of the entity-body. (Note: an MIC is good\n> for detecting accidental modification of the entity-body in transit, but\n> is not proof against malicious attacks.)\n\n> ContentMD5= \"Content-MD5\" \":\" md5-digest\n> md5-digest= <base64 of 128 bit MD5 digest as per RFC 1864>\n\n> The Content-MD5 header may be generated by an origin server to function\n> as an integrity check of the entity-body. Only origin-servers may\n> generate the Content-MD5 header field; proxies and gateways MUST NOT\n> generate it, as this would defeat its value as an end-to-end integrity\n> check. Any recipient of the entity-body, including gateways and proxies,\n> MAY check that the digest value in this header field matches that of the\n> entity-body as received.\n\n> The MD5 digest is computed based on the content of the entity body,\n> including any Content-Encoding that has been applied, but not including\n> any Transfer-Encoding.  If the entity is received with a\n> Transfer-Encoding, that encoding must be removed prior to checking the\n> Content-MD5 value against the received entity.\n\n> This has the result that the digest is computed on the octets of the\n> entity body exactly as, and in the order that, they would be sent if no\n> transfer coding were being applied.\n\n>    Note: there are several ways in which the application of\n>    Content-MD5 to HTTP entity-bodies differs from its\n>    application to MIME entity-bodies. One is that HTTP,\n>    unlike MIME, does not use Content-Transfer-Encoding,\n>    and does use Transfer-Encoding and Content-Encoding.\n>    Another is that, unlike MIME, the digest is computed\n>    over the entire entity-body, even if it happens to be\n>    a MIME \"multipart\" content-type. (Note that the multipart\n>    bodies may themselves have Content-MD5 headers.) Another\n>    is that HTTP more frequently uses binary content types\n>    than MIME, so it is worth noting that in such cases,\n>    the byte order used to compute the digest is the\n>    transmission byte order defined for the type. Lastly,\n>    the canonical form of text types in HTTP includes several\n>    line break conventions, so conversion of all line breaks\n>    to CR-LF is not required before computing or checking\n>    the digest: any acceptable convention should be left\n>    unaltered for inclusion in the digest.\n\nThis is much better but still not quite right. For one thing, it isn't that\nRFC1864 defines the way in which Content-MD5 is done over a multipart\ndifferently, it's that Content-MD5 isn't allowed over composite objects by\nRFC1864, period. There is no definition for how to do Content-MD5 over either a\nmultipart or a message/rfc822 for this prose to differ from.\n\nA more important point is that the calculation of Content-MD5 over multipart\n(or message/rfc822) is inadequately specified in this document. It seems clear\nthat any transfer encoding is removed before the checksum is computed. But a\nmultipart (or message/rfc822) can contain many different transfer encodings.\nAre they all removed? (I assume the answer is \"yes\".) If they are removed what\nhappens to the header fields specifying the transfer encoding? Are they\nincluded in the MIC calculation, or are they ignored since they have\neffectively been removed? (I have no idea which way this should be done.)\n\nDefining Content-MD5 in the case of a composite MIME object is a real can of\nworms. There were good reasons why RFC1864 didn't get into it. At a minimum you\nhave to deal with all sorts of composite objects, not just multipart, and you\nalso need to specify how transfer encoding fields are handled under the\nchecksum.\n\nFinally, as the computation of Content-MD5 is done over the content with\ntransfer encodings removed, the note about binary being more or less prevalent\non different transports is pointless. Once transfer encodings are removed\neverything that isn't textual in form is binary by definition. This note makes\nit sound like the calculation is different depending on whether or not a\ntransfer encoding was used. We have substantive proof that implementors get\nconfused by such things (e.g. the implementors of Microsoft Exchange believing\nthat transfer encodings are some sort of presentation device for MIME agents)\nand as such I strongly recommend that this be reworded.\n\nNed\n\n\n\n", "id": "lists-010-1375243"}, {"subject": "Administrivia: mailing list downtim", "content": "Folks,\n\nDue to a complete site powerdown, the machine running the http-wg mailing list\nwill almost certainly be off-line on the 28th and 29th of December.\n--\n(http-wg mailing list owner)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-010-13753210"}, {"subject": "The problem with proxyrevalidate, and a proposed solutio", "content": "Jeffrey Mogul writes:\n > After thinking about it for a while, and making up a table of all\n > the possible combinations....\n\nI think it would help a lot to have this table, before actually going\ninto deeper discussions. I know my imlementation of 1.1 proxy is wrong\nin several places, and before touching the code, I would like to make\nsure I understand the semantics of all the possible combinations. BTW\nit would be nice if this table also contained the warnings to be\nemitted for each cases along with the conditions under which these\nwarnings should be emitted.\n\nI now realize that writing down this table is not a small task, I will\nbe glad to provide, if this helps, the table that Jigsaw currently\nimplements as a starting point (even though, as I said, I know it's\nwrong).\n\nAnselm.\n\n\n\n", "id": "lists-010-13759866"}, {"subject": "Re: The problem with proxyrevalidate, and a proposed solutio", "content": "    Jeffrey Mogul writes:\n     > After thinking about it for a while, and making up a table of all\n     > the possible combinations....\n    \n    Anselm Baird-Smith writes:\n    I think it would help a lot to have this table, before actually going\n    into deeper discussions.\n    \nActually, I did create the table before sending that message, but I\nthought the message was already pretty long without it.  Also, the\ntable needs some explanatory notes, which I have now added.  See\nbelow.\n\n    BTW it would be nice if this table also contained the warnings to be\n    emitted for each cases along with the conditions under which these\n    warnings should be emitted.\n\nI'll take you up on your offer to help ... if you can figure out\nhow to add these to the table, please do so.  It would be nice if\nit still fits in a 72-column fixed-width format, because then I\ncan stick it into an Internet-Draft without any glitches.\n\n-Jeff\n\nExplanatory notes:\n\nThe leftmost column shows the Cache-control headers emitted by the\norigin server.  In all cases, I assume that the response is marked\nas \"revalidatable\" in some way, such as the presence of an entity-tag\nor last-modified date.\n\nThe rest of the table is divided into two major columns, depending\non whether the cache involved is at an end-client system, or a\nshared proxy cache.  (We can probably treat a non-shared proxy\ncache as being a sort of \"distributed\" implementation of an end-client\ncache.)\n\nEach of the major columns is divided into three minor columns, showing\nthe action to take\n(1) if the response is fresh\n(2) if the response is stale, and the cache is being strict\nabout revalidating stale responses\n(3) if the response is stale, and the cache is not being strict\nabout revalidating stale responses\n\nThe table assumes that there is only one kind of \"max age\" for a response,\nset by a max-age directive (or, equivalently, by an Expires header).\nIf we were to introduce a proxy-maxage directive, the table would\nneed some more rows, and in the Proxy Cache columns, the test for\n\"freshness\" would have to account for this new kind of \"max age\".\n\nIn the table entries,\nNA   means \"not applicable\" (i.e., can't happen)\nuse   means \"use entry without revalidating\"\ncheck   means \"use entry after revalidating\"\nignore   means \"use entry without revalidating, even if stale\"\n\n\nSite:    Client cache    Proxy Cache\nentry state:freshstalestalefreshstalestale\npolicy:strictloosestrictloose\n-----------------------------------------------------------------------\nIn current HTTP/1.1 spec:\n-----------------------------------------------------------------------\nAge limit\nfrom server\n===========\nNo max-ageuseNANAuseNANA\n\nmax-age > 0usecheckignoreusecheckignore\n\nmax-age = 0NAcheckignoreNAcheckignore\n\nmust-revalusecheckcheckusecheckcheck\n+max-age > 0\n\nmust-revalNAcheckcheckNAcheckcheck\n+max-age = 0\n\nproxy-revalusecheckignoreusecheckcheck\n+max-age > 0\n\nproxy-revalNAcheckignoreNAcheckcheck\n+max-age = 0\n\n-----------------------------------------------------------------------\nMissing from HTTP/1.1 spec:\n-----------------------------------------------------------------------\nAge limit\nfrom server\n===========\nNo max-ageuseNANAcheckNANA\n\nmax-age > 0usecheckignorecheckcheckcheck\n\nmax-age = 0NAcheckignoreNAcheckcheck\n\n-----------------------------------------------------------------------\nIf we added a proxy-mustcheck directive, those three rows would become:\n-----------------------------------------------------------------------\nAge limit\nfrom server\n===========\nNo max-ageuseNANAcheckNANA\n+proxy-\nmustcheck\n\nmax-age > 0usecheckignorecheckcheckcheck\n+proxy-\nmustcheck\n\nmax-age = 0NAcheckignoreNAcheckcheck\n+proxy-\nmustcheck\n\n-----------------------------------------------------------------------\nMissing from HTTP/1.1 spec, but probably not useful:\n-----------------------------------------------------------------------\nAge limit\nfrom server\n===========\nNo max-agecheckNANAuseNANA\n\nmax-age > 0checkcheckcheckusecheckignore\n\nmax-age = 0NAcheckcheckNAcheckignore\n\n\n\n", "id": "lists-010-13768478"}, {"subject": "Re: Please get me off this thin", "content": "Please remove me from this message service!!! PLEASE!\n\n\n\n", "id": "lists-010-13780706"}, {"subject": "Re: The problem with proxyrevalidate, and a proposed solutio", "content": "On Fri, 20 Dec 1996, Jeffrey Mogul wrote:\n\n> on whether the cache involved is at an end-client system, or a\n> shared proxy cache.  (We can probably treat a non-shared proxy\n> cache as being a sort of \"distributed\" implementation of an end-client\n> cache.)\n\nGood point ... actually the right wording, I think, would describe \nnot the implementation (end-client vs shared proxy) but more like\n\"on behalf of a single identified user\". It should be considered legal\nfor a proxy to be shared but be 'end-client' under the current wording\nas long as the proxy has identified the user and interprets the caching\nrules on a per individual user basis just as if the proxy were not\nshared. There are some interesting application and economies of scale\npossibilities which I wouldn't want precluded by the letter of the 'law'.\n\nI think the intent is that an individual 'human' receive reliable\ndelivery of information. I believe automated clients are a subset\nof what a human might do in a global information space and such \nclients can be modeled as humans. It would be OK for a single\nhuman to use two different UAs against the same cache for example but\nthe automated client is probably not going to do that.\n\nDave Morris\n\n\n\n", "id": "lists-010-13787196"}, {"subject": "HTTP response version, agai", "content": "I still consider the question unresolved as to what version an HTTP/1.x\nserver should return for an HTTP/1.0 request.  I claim the draft\n(section 6.1) does not specify it:\n    \"The first line of a Response message is the Status-Line,\n    consisting of the protocol version followed by ...\"\n\nTo review (minus the pros and cons), the issue is whether the server\nshould return (1) HTTP/1.0 (the version of the request) or (2) HTTP/1.1\n(the version the server software understands).  The concern people have\nexpressed is that in the first case you could never determine whether a\nserver understands HTTP/1.1.\n\nI favor case one.  Henrik Frystyk Nielsen has noted that clients should\nbegin to send HTTP/1.1 in their requests.  HTTP/1.1 servers would\nrespond in kind.  HTTP/1.0 servers (with two known exceptions)\ngenerally respond to HTTP/1.1 requests with a response of HTTP/1.0.\nWe could also [hack] recommend as a convention that servers include\nthe protocol version that the server understands as a comment in the\nServer: header.\n\nMy opinion notwithstanding, I offer wording for the two cases for\nsection 6.1, to follow the Status-Line syntax:\n\nCase 1 (return HTTP/1.0 to HTTP/1.0 request):\nThe protocol version in the response MUST be the lesser of HTTP/1.1 and\nthe protocol version in the request.  The headers in the response MUST\nbe consistent with the protocol version in the response.\n\nCase 2 (return HTTP/1.1 to HTTP/1.0 request):\nThe protocol version in the response MUST be the greater of HTTP/1.1 and\nthe protocol version in the request.  The headers in the response MUST\nbe consistent with the protocol version in the request.\n\nWe need to agree on one case or the other and (I believe) to choose words\nto add to the draft.\n\nDave Kristol\n\n\n\n", "id": "lists-010-13796960"}, {"subject": "Re: HTTP response version, agai", "content": "I agree that we need to resolve this, but I think that we need to think\nabout the proxy/cache issue as well.  A server which responds with a\n1.1 version advertises its capabilities to both the requesting user agent\nand any intervening proxies.  Since many of the 1.1 features have substantial\nimpact on how a proxy can interact with the origin server, I worry that\nby limiting the response we will significantly slow the use of those\nfeatures.\n\nI believe that the major/minor numbering scheme has been clear for a\nlong time.  If it breaks a browser or a script to get a protocol message\nwhich advertises a higher minor version, we need to fix the browsers\nand scripts, or we will face this problem every time the minor version\ngets rev'ved.\nTed Hardie\nNASA NIC\n\n\nOn Dec 20,  5:31pm, Dave Kristol wrote:\n> Subject: HTTP response version, again\n> I still consider the question unresolved as to what version an HTTP/1.x\n> server should return for an HTTP/1.0 request.  I claim the draft\n> (section 6.1) does not specify it:\n>     \"The first line of a Response message is the Status-Line,\n>     consisting of the protocol version followed by ...\"\n>\n> To review (minus the pros and cons), the issue is whether the server\n> should return (1) HTTP/1.0 (the version of the request) or (2) HTTP/1.1\n> (the version the server software understands).  The concern people have\n> expressed is that in the first case you could never determine whether a\n> server understands HTTP/1.1.\n>\n> I favor case one.  Henrik Frystyk Nielsen has noted that clients should\n> begin to send HTTP/1.1 in their requests.  HTTP/1.1 servers would\n> respond in kind.  HTTP/1.0 servers (with two known exceptions)\n> generally respond to HTTP/1.1 requests with a response of HTTP/1.0.\n> We could also [hack] recommend as a convention that servers include\n> the protocol version that the server understands as a comment in the\n> Server: header.\n>\n> My opinion notwithstanding, I offer wording for the two cases for\n> section 6.1, to follow the Status-Line syntax:\n>\n> Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n> The protocol version in the response MUST be the lesser of HTTP/1.1 and\n> the protocol version in the request.  The headers in the response MUST\n> be consistent with the protocol version in the response.\n>\n> Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n> The protocol version in the response MUST be the greater of HTTP/1.1 and\n> the protocol version in the request.  The headers in the response MUST\n> be consistent with the protocol version in the request.\n>\n> We need to agree on one case or the other and (I believe) to choose words\n> to add to the draft.\n>\n> Dave Kristol\n>-- End of excerpt from Dave Kristol\n\n\n\n-- \nTed Hardie\n\n\n\n", "id": "lists-010-13805851"}, {"subject": "Re: HTTP response version, agai", "content": "Dave, what about:\n\n# The protocol version in the response MAY be either HTTP/1.1 or\n# HTTP/1.0. The headers in the response MUST be consistent with BOTH the\n# protocol version of the response AND the protocol version in the\n# request.\n\nI don't know why we have to nail this down. \"We MAY not always have to\nMUST if we can MAY.\"\n\nLarry\n\n\n\n", "id": "lists-010-13816093"}, {"subject": "Re: HTTP response version, agai", "content": "On Fri, 20 Dec 1996, Dave Kristol wrote:\n> I still consider the question unresolved as to what version an HTTP/1.x\n> server should return for an HTTP/1.0 request.\n[...]\n> Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n> Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n\nI agree with Dave that Case 1 is preferable.  AOL's proxies apparently\nstarted giving users errors this week when a new version of Apache was\nreleased, which responded to 1.0 requests with 1.1 responses (Case 2).  \nWhile this instance will likely be fixed next week, it does indicate how an\nHTTP/1.0 client can be confused by an HTTP/1.1 response.\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-13823928"}, {"subject": "Re: HTTP response version, agai", "content": ">I agree with Dave that Case 1 is preferable.  AOL's proxies apparently\n>started giving users errors this week when a new version of Apache was\n>released, which responded to 1.0 requests with 1.1 responses (Case 2).  \n>While this instance will likely be fixed next week, it does indicate how an\n>HTTP/1.0 client can be confused by an HTTP/1.1 response.\n\nWell, I agree with Ted.  Do #2 and fix the broken clients/proxies.\n\nThe logic case #1 follows doesn't seem any different to me than saying\nclients should send 1.0 by default when we find a server that gets confused\nby something other than 1.0.\n\nAnd the capabilities of the server to support 1.1 should be important to\nothers along the request chain, not just the 1.0 client on the end.\n\n-----\nDaniel DuBois, Traveling Coderman        www.spyglass.com/~ddubois\n   o  The Heroes of Might and Magic II Bible is here!\n      http://www.spyglass.com/~ddubois/HOMM2.html\n\n\n\n", "id": "lists-010-13832449"}, {"subject": "Re: HTTP response version, agai", "content": "As I said in my reply to Dave, any resolution of this issue should be\nconsistent with section 3.1 of RFC 1945:\n\n3.1  HTTP Version\n\n   HTTP uses a \"<major>.<minor>\" numbering scheme to indicate versions\n   of the protocol. The protocol versioning policy is intended to allow\n   the sender to indicate the format of a message and its capacity for\n   understanding further HTTP communication, rather than the features\n   obtained via that communication. No change is made to the version\n   number for the addition of message components which do not affect\n   communication behavior or which only add to extensible field values.\n   The <minor> number is incremented when the changes made to the\n   protocol add features which do not change the general message parsing\n   algorithm, but which may add to the message semantics and imply\n   additional capabilities of the sender. The <major> number is\n   incremented when the format of a message within the protocol is\n   changed.\n\nThis seems to favor choice 2) below.\n\nDave Kristol liltingly intones:\n> \n> I still consider the question unresolved as to what version an HTTP/1.x\n> server should return for an HTTP/1.0 request.  I claim the draft\n> (section 6.1) does not specify it:\n>     \"The first line of a Response message is the Status-Line,\n>     consisting of the protocol version followed by ...\"\n> \n> To review (minus the pros and cons), the issue is whether the server\n> should return (1) HTTP/1.0 (the version of the request) or (2) HTTP/1.1\n> (the version the server software understands).  The concern people have\n> expressed is that in the first case you could never determine whether a\n> server understands HTTP/1.1.\n> \n> ...edited for brevity...\n>\n\nchuck\nChuck MurckoN2K Inc.Wayne PAchuck@telebase.com\nAnd now, on a lighter note:\nShaw's Principle:\nBuild a system that even a fool can use, and only a fool will\nwant to use it.\n\n\n\n", "id": "lists-010-13840613"}, {"subject": "HTTP response version, agai", "content": "Dave Kristol writes:\n...\n\n > Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n > The protocol version in the response MUST be the lesser of HTTP/1.1 and\n > the protocol version in the request.  The headers in the response MUST\n > be consistent with the protocol version in the response.\n > \n > Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n > The protocol version in the response MUST be the greater of HTTP/1.1 and\n > the protocol version in the request.  The headers in the response MUST\n > be consistent with the protocol version in the request.\n > \n > We need to agree on one case or the other and (I believe) to choose words\n > to add to the draft.\n > \n > Dave Kristol\n > \n > \n\n\nThere are multiple possible interpretations of \"The headers in the\nresponse MUST be consistent with the protocol version in the\n(response|request)\".\nInterpretation A:  the headers in the response must be _defined_ in the\nprotocol version in the (response|request), or\nInterpretation B:  the headers in the response must _not be inconsistent_\nwith the protocol version in the (response|request). \n\nThe way in which the major/minor version scheme works is involved with\nthis.  It's always supposed to be possible for a 1.n client to receive\n1.m responses from a 1.m server and behave reasonably according to the\ncapabilities of version min(1.n,1.m).  If m<=n, the headers will\ndescribe how a version 1.m client should behave, and the 1.n version\nshould be able to do that.  If m>n, the response should still have a\nreasonable interpretation in 1.n.  If this can't work, the major/minor\nversion system isn't going to work.\n\nIf we believe interpretation A above, then a HTTP 1.0 proxy anywhere\nbetween a HTTP 1.1 client and a HTTP 1.1 server would prevent the\nserver from ever being able to pass any 1.1-only headers through the\nproxy to the client in either case (1) or (2).  Furthermore this would\nrequire proxies to remove 1.1 headers when responding to 1.0 clients.\nSo, this is pretty bad.\n\nIf we believe interpretation A above, then, since 1.1 is supposed to\nbe backward compatible with 1.0, if headers defined in 1.1 are left\nuninterpreted (and this is the crux of the major/minor version\nscheme), then it is always OK to return a normal 1.1 response to a 1.0\nrequest.  This seems to suggest it makes sense to return a 1.1 version\nnumber as well.\n\n--Shel\n\n\n\n", "id": "lists-010-13849675"}, {"subject": "Re: HTTP response version, agai", "content": "# I think it's important to be precise about what the version number\n# means.  Given that there are two views, I would say the spec. is\n# imprecise.\n\nThe spec is precise in that it says what the version numbers mean. The\nspec just doesn't proscribe behavior.\n\nThere are not two views on what the version numbers mean. And there\nare more than two views on what servers SHOULD do. You supplied two:\n\n> Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n> Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n\nI noted, though, that since we don't outlaw Case 1 (a HTTP/1.0 server\nmay return HTTP/1.0 to a HTTP/1.0 request!), supporting Case 2 is\nreally supporting MAY rather than MUST.\n\nA server MAY return HTTP/1.0 to a HTTP/1.0 request\nOtherwise:\nA server MAY return HTTP/1.1 to a HTTP/1.0 request\nOtherwise:\nA server MAY return HTTP/1.2 to a HTTP/1.0 request\n...\nad infinitum\n\n\n\n\n\n  \n\n\n\n", "id": "lists-010-13859334"}, {"subject": "Re: HTTP response version, agai", "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n  > Dave, what about:\n  > \n  > # The protocol version in the response MAY be either HTTP/1.1 or\n  > # HTTP/1.0. The headers in the response MUST be consistent with BOTH the\n  > # protocol version of the response AND the protocol version in the\n  > # request.\n  > \n  > I don't know why we have to nail this down. \"We MAY not always have to\n  > MUST if we can MAY.\"\n\nI think it's important to be precise about what the version number\nmeans.  Given that there are two views, I would say the spec. is\nimprecise.\n\nDave\n\n\n\n", "id": "lists-010-13867722"}, {"subject": "Re: HTTP response version, agai", "content": "On Fri, 20 Dec 1996, Daniel DuBois wrote:\n> Well, I agree with Ted.  Do #2 and fix the broken clients/proxies.\n\nIf it were only proxies, I would agree, but fixing broken clients is a lot\nharder (that is, getting client users to upgrade is harder than getting\nproxy maintainers to do so).  It seems imminently desirable to ease\nprotocol upgrades, and case #1 would do so.\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-13875616"}, {"subject": "Re: HTTP response version, agai", "content": ">If it were only proxies, I would agree, but fixing broken clients is a lot\n>harder (that is, getting client users to upgrade is harder than getting\n>proxy maintainers to do so).  It seems imminently desirable to ease\n>protocol upgrades, and case #1 would do so.\n\nGetting users to upgrade their clients is incredibly easy if said clients no\nlonger function.\n\n-----\nDaniel DuBois, Traveling Coderman        www.spyglass.com/~ddubois\n   o  The Heroes of Might and Magic II Bible is here!\n      http://www.spyglass.com/~ddubois/HOMM2.html\n\n\n\n", "id": "lists-010-13883671"}, {"subject": "505 HTTP Version Not Supporte", "content": "For the sake of completeness and the future of HTTP, I would like to\npropose a new server error code for indicating that the server does\nnot support the HTTP protocol version received in the request message.\n\n   505 HTTP Version Not Supported\n\n   The server does not support, or refuses to support, the HTTP protocol\n   version that was used in the request message.  The server is\n   indicating that it is unable or unwilling to complete the request\n   using the same major version as the client, as described in\n   Section 3.1, other than with this error message.  The response should\n   contain an entity describing why that version is not supported and\n   what other protocols are supported by that server.\n\n\nPlease note that, among other things, this allows future applications\nto cease support for older protocols and yet do so in a way that is\nmeaningful to older clients.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1388687"}, {"subject": "Re: HTTP response version, agai", "content": "On Fri, 20 Dec 1996, M. Hedlund wrote:\n> On Fri, 20 Dec 1996, Dave Kristol wrote:\n> > I still consider the question unresolved as to what version an HTTP/1.x\n> > server should return for an HTTP/1.0 request.\n> [...]\n> > Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n> > Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n> \n> I agree with Dave that Case 1 is preferable.  AOL's proxies apparently\n> started giving users errors this week when a new version of Apache was\n> released, which responded to 1.0 requests with 1.1 responses (Case 2).  \n> While this instance will likely be fixed next week, it does indicate how an\n> HTTP/1.0 client can be confused by an HTTP/1.1 response.\n\nNo, it indicates how a company with little concern for standards can dictate\nimplementations in other products through technological inertia.  There's\nnothing in the 1.1 response which should cause problems with the 1.0 proxy or\n1.0 client - section 3.1 of both the 1.0 and 1.1 specs promise this, and (as\nbest this group can tell) 1.1 fulfills this promise.  \n\nIf it becomes common acceptance that 1.0 and 1.1 are incompatible, then no one\nwill ever upgrade to 1.1.  This is exactly the perception this wg labored long\nand hard to prevent.\n\nThe big question is, what will happen first: will AOL fix their proxies, or\nwill Apache users \"fix\" [hack] their servers?  Client service dictates that\nwe at Organic hack our servers, but the Apache development group has no such\nrequirements.  \n\nA document describing the situation is available at\n\n  http://www.apache.org/info/aol-http.html\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  www.apache.org  hyperreal.com  http://www.organic.com/JOBS\n\n\n\n", "id": "lists-010-13892083"}, {"subject": "Re: HTTP response version, agai", "content": "On Fri, 20 Dec 1996, Larry Masinter wrote:\n\n> Dave, what about:\n> \n> # The protocol version in the response MAY be either HTTP/1.1 or\n> # HTTP/1.0. The headers in the response MUST be consistent with BOTH the\n> # protocol version of the response AND the protocol version in the\n> # request.\n> \n> I don't know why we have to nail this down. \"We MAY not always have to\n> MUST if we can MAY.\"\n\nAnd the content of the response MUST be compatible with the request\nversion when the version is lower than the response version ...\nthis I suppose is a side effect of TRANSFER-ENCODING: as a header\nnot allowed by the above rule but is will surely break a 1.0 client\nif it is presumed to ignore unknown headers and is sent a content\nbody with chunked encoding.\n\nI think that the real issue here is we are using a single value to\naccomplish two objectives:\n\n   a.  Label the level of the response\n   b.  Declare the capabilities of the server\n\n\nPerhaps the 'bug' fix is to add a way for the server to declare its\ncapabilities ? And in the status, label the level of the response.\n\nDave Morris\n\n\n\n", "id": "lists-010-13902545"}, {"subject": "Re: HTTP response version, agai", "content": "On Fri, 20 Dec 1996, Daniel DuBois wrote:\n\n> Getting users to upgrade their clients is incredibly easy if said clients no\n> longer function.\n\nThat is only true if the user has the choice AND the problem is general.\nIf the user is using a UA built into an environment, then may not have\nthe ability to plug in a replacement. \n\nMore important, if the user gets breakage visiting a few sites (and most\nusers will not understand anything more complex than it didn't work and\nthat happens a lot for many reasons) they will just consider it a \nfeature of the web and will have now reason to upgrade.\n\nDave Morris\n\n\n\n", "id": "lists-010-13911975"}, {"subject": "Re: HTTP response version, agai", "content": "It seems to me that the purpose of the version number in the reponse is not\nto advertise the capabilities of the server but to indicate which protocol\nversion forms the basis of its response. The appropriate response to a\nrequest may well depend upon the version. For example, a chunked reply\nappropriate in response to a 1.1 request but not a 1.0, and the HTTP/1.1 in\nthe reponse header indicates which protocol version forms the basis of a\nresponse. Similarly, for error reponses, if the version number is 1.1, then\nthe response indicates that the request, considered as a 1.1 request,\nresulted in an error condition. If the version in the reponse were 1.0,\nthen tht would indicate that the request was treated as a 1.0 request.\n\nSo, I would say the version in the reponse MUST be the lesser of the requet\nversion and the server version. If the server wishes to advertise that it\nsupports a higher protocol version, then this should be done in the rponse\nheader, either using Server: (which I don't think is *that* much of a\nhack), or perhaps by overloading Upgrade:.\n \n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nto come up with a better one.\n\n\n\n", "id": "lists-010-13920954"}, {"subject": "question/observation about HTTP/1.", "content": "I am working with a group of printer vendors to define a print protocol \nbased on MIME and HTTP.\n\nAfter reading draft-ietf-http-v11-spec-07, I have a question about\ndetermining lengths of body-parts of a Multipart entity. \n\nIf Content-length is present for a body-part of a Multipart, does it\ntake precedent over a boundary string?  The section 4.4 'Message Length'\nperhaps implies that it does.\n\nCan a 'Transfer-Encoding: chunked' appear in the header of a body-part\nof a Multipart entity? \n\nIf the answer to both these questions is yes, then I would like more\nexplicit wording that the rules for messages applies to nested messages\nin Content-types that have nested entities, such as Multipart content-\ntype.\n\nIf the answer is no, then I would like to know if we can change your mind.\n\nThanks for any help.\n\nBob Herriot\n\n\n\n", "id": "lists-010-13930675"}, {"subject": "Re: HTTP response version, agai", "content": "    I agree that we need to resolve this, but I think that we need to\n    think about the proxy/cache issue as well.  A server which responds\n    with a 1.1 version advertises its capabilities to both the\n    requesting user agent and any intervening proxies.\n\nActually, because the response version number is NOT end-to-end\n(it only has meaning for a specific hop), it does advertise\nthe server's capabilities to the next-hop client, but not to any\nsubsequent client.\n\nE.g., in this case\n\norigin-server -----> Proxy1 ------> Proxy2 -----> end-client\n\nIf Proxy1 sends a response to Proxy2 with\nHTTP/1.1 200 OK\nthen Proxy2 knows that Proxy1 supports HTTP/1.1, but it cannot\ninfer anything from this about the origin-server, and the end-client\ncannot discover the version implemented by Proxy1.\n\nThis is my recollection of Roy Fielding's explanation from many\nmonths ago, and I believe that this is the understanding under\nwhich the HTTP/1.1 spec was written.  It may need to be documented,\nbut I think it's too late to change.\n\n-Jeff\n\nP.S.: not that I disagree with your overall position, Ted.\n\n\n\n", "id": "lists-010-13938280"}, {"subject": "Re: HTTP response version, agai", "content": "    The appropriate response to a request may well depend upon the\n    version.\n\nThe request version, yes.\n\n    For example, a chunked reply appropriate in response to a 1.1\n    request but not a 1.0,\n\nTrue\n\n    and the HTTP/1.1 in the response header indicates which protocol\n    version forms the basis of a response.\n\nNot exactly.  You shouldn't send a response that looks like\nHTTP/1.0 206 Partial Content\nbecause HTTP/1.0 doesn't define this code, although a reasonable\nimplementation (of *either* HTTP/1.1 or HTTP/1.0) would accept\nthis message anyway.  But you can send something that is fully\nacceptable to an HTTP/1.0 client (one which isn't buggy, anyway)\nwith an HTTP/1.1 response version.  That's the way that the HTTP/1.1\nprotocol was designed, and if there are any cases where this is not\ntrue, someone should describe the specifics.\n\nThe problem with chunked responses is solved not by playing games\nwith the response version number.  An HTTP/1.1 server should not\nsend a chunked response to an HTTP/1.0 request, but this is 100%\nindependent of the response version number.\n\n    Similarly, for error reponses, if the version number is 1.1, then\n    the response indicates that the request, considered as a 1.1\n    request, resulted in an error condition. If the version in the\n    reponse were 1.0, then tht would indicate that the request was\n    treated as a 1.0 request.\n    \nNo, the HTTP/1.1 specification was designed carefully to make this\nunnecessary.  If a properly formed HTTP/1.0 request would lead to an\nerroneous status code, this is true independent of whether the server\nimplements HTTP/1.0 or HTTP/1.1.\n\nThere is only one reason that one might want an HTTP/1.1 server\nto respond to an HTTP/1.0 request with a version number of HTTP/1.0.\nThis would only be necessary if the client implementation is\nexcessively strict with respect to version numbers.  (I was going\nto write \"if the client implementation was buggy\", but since all\nHTTP/1.0 clients were written without the benefit of a formal\nstandard, I don't want to get into a pointless argument about what\nconstitutes a \"bug\").  In *all* other cases, there is no possible\nharm if an HTTP/1.1 server always responds with that version.\n\n-Jeff\n\n\n\n", "id": "lists-010-13946452"}, {"subject": "HTTP response version, agai", "content": "Shel Kaphan writes:\n > If we believe interpretation A above, then a HTTP 1.0 proxy anywhere\n > between a HTTP 1.1 client and a HTTP 1.1 server would prevent the\n > server from ever being able to pass any 1.1-only headers through the\n > proxy to the client in either case (1) or (2).  Furthermore this would\n > require proxies to remove 1.1 headers when responding to 1.0 clients.\n > So, this is pretty bad.\n > \n > If we believe interpretation A above, then, since 1.1 is supposed to\n^\nI meant \"B\" here.\n\n > be backward compatible with 1.0, if headers defined in 1.1 are left\n > uninterpreted (and this is the crux of the major/minor version\n > scheme), then it is always OK to return a normal 1.1 response to a 1.0\n > request.  This seems to suggest it makes sense to return a 1.1 version\n > number as well.\n > \n > --Shel\n > \n > \n\n--Shel\n\n\n\n", "id": "lists-010-13955771"}, {"subject": "Re: HTTP response version, agai", "content": "The version number in the response indicates the capabilities of the\nserver. However, a server should not respond with a chunked transfer\nencoding to a client that doesn't indicate it supports it. The\nversion number in the response can be ANY HTTP/1.x, but it would be\nincorrect to send back a response that assumed HTTP/1.1 if the request\nsaid HTTP/1.0.\n\n# I think that the real issue here is we are using a single value to\n# accomplish two objectives:\n#   a.  Label the level of the response\n#   b.  Declare the capabilities of the server\n\nThere is nothing wrong with this. In addition, we declare that 'a <=\nb', that is, a response should be less than or equal to the\ncapabilities of the server. A response should also be less than or\nequal to the capabilities of the request.\n\nSince HTTP/1.x versions are upward compatible, you can tell the level\nof 'a' by looking at the headers themselves.\n\nLarry\n\n\n\n", "id": "lists-010-13964759"}, {"subject": "Please get me off!", "content": "Please get me off the list. \n\n\n\n", "id": "lists-010-13973570"}, {"subject": "Re: (DNS) consensus wordin", "content": "% From dwm@shell.portal.com Mon Apr  1 18:04:05 1996\n\n% It is my understanding that MUST and SHOULD are defined terms and\n% strongly encouraged is not as far as RFCs are concerned. Thus, I\n% offer the following editorial alternative to Koen's suggestion (which\n% I endorse):\n% \n%   If a client caches the result of a DNS lookups, it should observe the\n%   TTL (Time To Live) reported by the DNS server. If the TTL value is\n%   not available, the client must not cache the result of a DNS lookup\n%   for longer than XX minutes. In either case, the client must immediately \n%   discard a name lookup result if a network error occurs when using the \n%   result to initiate a connection.\n% \n% Rationale for other changes:\n% 1. I believe this paragraph is about DNS name lookups and should be\n%    specific\n% 2. We don't care what the motivation is for the caching\n% 3. I'm not sure that 10 minutes is the right number ... my IPSs tell me\n%    that 24 hours must be allowed for DNS change propigation. Given\n%    rational expectation for rate of change of the value, I would prefer\n%    a larger number ... or if we have a DNS expert, perhaps there is\n%    a DNS defined default TTL for cases where not is specified.\n%    I can live with the 10 minutes but it was a detail which I felt should\n%    surface for expert comment.\n\nRFC 1034 does not say anything about intended time of propagation - \nit rather specifies that \"if a change can be anticipated, the TTL can be\nreduced prior to the change\", and it can even be set up to 0, meaning that\nit must not be cached. Of course this should be done only in \nparticular cases (prior to a major change), but I think it is \nexploited by multi-IP servers to share workload (IMO a bad move - a TTL\nof 10 minutes would have been ok)\n\nMy own understanding of TTL is that *every* request has a TTL - if not \nexplicitly, it is set up from the MINIMUM field in the Zone Authority data.\nIf DNS lookup is considered a threat for the Net, I would suggest to set\nthe maximum caching at 30 minutes - I am not sure if is a Good Thing,\nhowever. I'd rely to DNS data.\n\n.mau.\n\n\n\n", "id": "lists-010-1397649"}, {"subject": "DRAFT Minutes, HTTP-WG Dec 91", "content": "1. HTTP/1.1 implementation issues\n- Performance Evaluation of HTTP/1.1 pipelining (Henrik Frystyk\nNielsen and Jim Gettys) (Slides submitted independently.)  In\nexperimentation, HTTP pipelining resulted in a factor of 5 improvement\nas measured by number of packets. (A paper is also available at\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Pipeline.html)\n\n- What should a server respond to an HTTP/1.0 request?\nThere are both clients and servers that choke on HTTP/1.1.  It's too\nlate to fix existing implementations.  If we don't say that we can do\nHTTP/1.1 then a client may never find out that a HTTP server can speak\n1.1 and we can't make any progress.  It doesn't seem to be headers but\nmerely that broken 1.0 apps look for the string \"HTTP/1.0\" in order to\ndistinguish it from HTTP/0.9. The issue was not resolved at the\nmeeting, but was to be (has been) continued on the mailing list.\n\n- Content-Disposition?\nWe'd discussed on the list a means to give a proposed filename for\ndownloaded files. Since Content-Disposition is an experimental MIME\nheader, this may be added in a non-normative appendix.  There is a\ngeneric security warning in the equivalent MIME document.  Alex\nHopmann volunteered to write up a note about Content Disposition.\n\n- Warning Headers\nIf you have cascaded proxies then warning headers may be cached and\npassed to the client even though the document has been revalidated and\nis valid. The rules for how and when to strip warning headers should\nbe made more explicit. Simon Spero has a pointer. [??]\n\n- 305 Response code Underdefined?\nThe current draft does not define 305 well. If you are behind a\nfirewall then a 305 response will fail on all future requests. That is\nit has to be a hop by hop and not end to end return code.  There\nshould be a proxy configuration mechanism so that it can be handled\nwith proxies. Ari volunteered to write up a spec about how to use\n305.\n\n- Proxy Authentication Underdefined?\nThe issuse is that if you have cascaded proxies then the \"collapsing\"\nof proxy authentication may fail. It is a different model than\noriginally intended where proxy authentication was a hop-by-hop\nmexchanism.  Josh Cohen proposed a change to the current draft. He\nwould like to add a realm id. He committed to send a draft to the\nmailing list.  This would also have an impact on www-authenticate.  We\nagreed to review Josh's draft and decide whether to do this inside\nHTTP/1.1 or outside of it.\n\n- Should IMS and IUS be \"==\" or \"<=\"\nIf-modified-since and if-unmodified-since are defined as\ninequalities. But there may be race conditions where a client can get\na stale document. There are also problems in broken client and\ndaylight saving time and clock skew. There had been some belief that\nan equality check instead of the current less than or equal would be\nsafer.  We could recommend that date stamps are not used at all but\nthis will not change existing clients.  Clients may reformat the\ndate/time stamp when revalidating.\n\n- Who should close the Connection?\nIt is not clear at this point in time and we need more implementation\nexperience. We may want to come up with an implementation advice.\n\n2. Content negotiation\n- Transparent Content Negotiation\n\nAndy Mutz gave an overview of the current draft: How to handle\nmultiple variants is an important issue in i18n.  How does this\ninteract with proxies and caches?  Having the clients telling its\npreferences does not scale. Instead the server can ennumerate the\nchoices.  There are at least two implementations of TCN that can\ninteroperate.  Request for moving this to proposed standard in January\nand hook up to HTTP/1.1? What should we do?\n\nImplementation problems: charset, MIME types are not orthogonal!\nIt is still not clear how useful the current spec is in practice.\nIt is problematic to specify the algorithm for how to do content\nnegotiation, q values should be relative and not absolute.\n\nIt was proposed to isolate the alternate part from the rest of the\ncurrent spec, and progress the part of transparent negotiation that\nhas alternatives, but not the multiplication of q factors.\n\nThere are still some open issues: Server rendering machanisms?,\nQuality values on feature tags?\n\n- Feature Tag Registration\nAndy Mutz gave an overview. We need more review from the working\ngroup. Larry committed to make sure that the draft is added to the wg\nhome page.\n\n- User Agent Display Attributes\nWhere should this go? It could be part of a transparent content\nnegotiotaion or it can be part of a feature tag negotiation.  Yaron\nGoland referred to the draft to describe the feature tags that\nMicrosoft would like to have. It is not a proposal for a core set.\nYaron will work with rest of the draft authors to converge the\ndocuments.\n\n3. Hit Metering\nJeff Mogul presented the current status of the Hit Metering proposal.\nAn important note is that the current definition of proxy validate in\nHTTP/1.1 needs to be changed. Jeff proposed a fix \"Proxy-must-check\".\nIt was noted that cache-busting may not be a growing problem and a hit\nmetering scheme may make it worse.  The proposal adds additional\nrequirements on a proxy: It has to know where a document comes from,\nfor example. It was discussed whether the extra overload was a\nproblem. The general feeling was no.  The issue of statistical\nsampling was been brought up several times but nobody have provided a\ndraft.  The limitations of the current proposal should be made clear\nin the draft. If this is met then the proposal can move forward as\nproposed standard.\n\n4. Safe POST / GET-with-body\nIt was discussed whether HTTP should have more properties for handling\nthe user-agent. There was a general feeling that HTTP should be kept\northogonal to what goes on in the user agent. [??]\n\n5. Other groups with work related to HTTP\n\n- HTTP-NG\nJim Gettys talked about the status of the work. Neither of the current\ndistributed RPC systems IIOP from CORBA nor DCOM from Microsoft is\nreally suitable for the Web. One of the main problems is scalability.\nHTTP-NG is being worked on, but the work is still research.\n\n- Web Server Management\nHarrie Haxewinkel reported on definitions of managed objects for WWW\nservers. (Look for the APPLMIB working group.)\n\n- SHHTP\nCharlie Kaufman (chair of WTS wg) reported on SHTTP.  The wg has not\nmet for the last two IETF meetings.  SHTTP Was based on HTTP/1.0 and\none of the problems why it has been held up was due to undefined\ninteractions with HTTP/1.1.  There will be an updated draft coming out\nand it will be cross mailed to the HTTP-wg list for review.\n\n- Remote pass-phrase Authentication\nRich Petke gave an overview of the ?the \"Remote pass-phrase\nauthentication\" which has no passwords in clear.  There are four\ndrafts available as draft-petke-* The system is currently in\nproduction.  Look for \"Virtual Key\" which is the slogan. There was not\nan intention to move this forward in standards track.\n\n- SLL Tunnelling\nThe current draft for SLL tunnelling through proxies has expired but\nAri would like to resubmit it and make it an RFC.  There are\nimplementations in NS and MSIE proxies and clients and also in Apache.\nIt was the overall feeling that the draft should move forward as\nstandards track.\n\n- Web Distributed Authoring  and Versioning (Webdav)\nJim Whitehead reported. There is a BOF Wednesday.  Web DAV is based on\nPUT and adding on new features for locking, link management,\nrelationship, attributes, etc. The proposals will interact with\ncontainers and webmaps, and include access control and versioning.\n\n- Internet Printing Protocol\nCarl-Uno Manros reported. The work started with the same group which\ndid the printer MIB. The goal is to make printing available over the\ninternet with more features than are in lpr. An initial proposal looks\nlike it uses HTTP.\n\n- MMUSIC\nThere is ongoing work in MMUSIC on RTSP and SIP which looks something\nlike HTTP. They want to use HTTP but don't want to step on HTTP\nversion numbers: is there a registry?  As HTTP is getting more complex\nit is difficult to role your own stuff anymore. This will make other\npeople have to reinvent their own protocol. Maybe a layering of HTTP\nwould solve this.\n\n6. PEP\nDan Connolly report that no new draft has been issued and there was\nnothing to discuss at this meeting. People are eagerly awaiting the\nnext draft. Paul Leach noted that PEP didn't allow extensions for\nerror messages as MMUSIC noted.\n\n7. Working Group Plans\nThere a couple of months left before the HTTP/1.1 can go to draft\nstandard. We should work towards that date!  There are some minor\ndetails that should be incorporated into the current\nversion. Hopefully it will not have to be recycled as Proposed.  The\nalternate stuff should be taken out of the current content negotiation\nto be put into a separate draft and moved forward.  Things don't have\nto be folded into the current HTTP/1.1 - they can be merged later.\nAnything that relies on the version number should be added and stuff\nthat doesn't should be kept separate.\n\nProposed milestones:\n   o 2/97 Hit Metering to IESG\n   o 2/1/97 New drafts on remaining isses and problems in HTTP/1.1\n   o 2/1/97 PEP draft\n   o 2/1/97 Content Negotiation draft\n   o 3/1/97 New 1.1 suite of documents going to IESG\n\n\n\n", "id": "lists-010-13979899"}, {"subject": "Re: HTTP response version, agai", "content": "David W. Morris wrote:\n> On Sat, 21 Dec 1996 S.N.Brodie@ecs.soton.ac.uk wrote:\n> > I read the message that the AOL proxy has been issuing that blames the\n> > remote site for the failure (in Apache Week).  It would seem that their\n> > proxy does not implement HTTP/1.0 correctly if it does not accept a\n> > response in the same major version (which is all servers have to provide)\n> \n> Perhaps they don't implement the non-standard HTTP/1.0 RFC 'correctly'\n> which may not have even existed when they implemented...\n\nThis is not the case. AOL implemented this change in the last few weeks, in\nresponse, they say, to \"broken\" servers issuing HTTP/1.1 responses. It seems\nunfortunate to me that they elected to do this without discussion, either with\nthe authors of the \"broken\" servers, or with HTTP-WG.\n\nIt also seems to me that the spec is not clear on this issue. There is clear\nintent in HTTP/1.1, in that the word \"major\" was added to the version of\nthe response, but, AFAICS, no clear requirement to respond either 1.0 or 1.1 to\na 1.0 request. Since a requirement of the spec is to be liberal in what is\naccepted, it seems to me that the correct interpretation of the spec is that\na 1.1 reponse to a 1.0 request is permitted.\n\nI forget where we are wrt modifications to the spec. It seems to me that a\nmodification should be made to clarify this point, whichever way it goes.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n", "id": "lists-010-13995950"}, {"subject": "Re: HTTP response version, agai", "content": "On Sat, 21 Dec 1996, Ben Laurie wrote:\n\n> It also seems to me that the spec is not clear on this issue. There is clear\n> intent in HTTP/1.1, in that the word \"major\" was added to the version of\n> the response, but, AFAICS, no clear requirement to respond either 1.0 or 1.1 to\n> a 1.0 request. Since a requirement of the spec is to be liberal in what is\n> accepted, it seems to me that the correct interpretation of the spec is that\n> a 1.1 reponse to a 1.0 request is permitted.\n> \n> I forget where we are wrt modifications to the spec. It seems to me that a\n> modification should be made to clarify this point, whichever way it goes.\n\nIMHO, I don't think it's particuarly neccessary for the spec to\nspecify how a HTTP/1.1 server has to respond to HTTP/1.0 requests. As\nwas pointed out, even if the spec did say that it had to respond with\nHTTP/1.1, that doesn't mean anything, because in fact when a HTTP/1.0\nrequest came in, the server could just use HTTP/1.0 semantics and\nwould be perfectly within its rights to respond with HTTP/1.0. I don't\nthink it's neccessary to specific one over the other - both can\ncoexist perfectly well. (that being said, I do favor responding to\nHTTP/1.0 with HTTP/1.1).\n\nHowever, it seems certainly desirable to add some language to the spec\nalong the lines of \"if a message of a version with a known major\nnumber, but an unknown minor number is received (e.g., if a HTTP/1.0\nclient recieves a HTTP/1.1 response, or a HTTP/1.2 server receives a\nHTTP/1.7 request), it should be treated as if it was a request\nequivilent to the highest minor number in that major version that is\nsupported.\"\n\n-- \n________________________________________________________________________\nAlexei Kosut <akosut@nueva.pvt.k12.ca.us>      The Apache HTTP Server\nURL: http://www.nueva.pvt.k12.ca.us/~akosut/   http://www.apache.org/\n\n\n\n", "id": "lists-010-14007104"}, {"subject": "Re: HTTP response version, agai", "content": "On Sat, 21 Dec 1996 S.N.Brodie@ecs.soton.ac.uk wrote:\n\n> I agree with Brian here.  Case 2 is the preferable solution.  For a\n> start it's the only easy way I can see of allowing my (HTTP/1.1\n> compliant) browser can establish persistent connections with 1.1 proxies\n> without requiring an extra request just to test it out.  I am not willing\n> to rely on some of the more obscure methods (such as OPTIONS) not being\n> implemented in a 1.0 proxy either.\n\nNot true ... your browser would send the HTTP/1.1 request and a 1.1 server\nwould respond in kind in either case. A 1.0 server would be in error to\nreturn 1.1 in the status response with a 1.0 response (which was all it\ncould generate).  In terms of pipelining being used before the server\nresponds with HTTP/1.1, while that is currently allowed by the spec.,\nI am not convinced that it won't seriously confuse some existing 1.0\nservers to receive data following the initial request so be wary and if\nyou try it let us and test with a wide variety of servers, let us know\nwhat happens.\n\n> My browser keeps a list of sites recently visited and their HTTP version\n> precisely so it can avoid confusing them.\n\nWhat will it do in the future when an HTTP/1.2 site declares itself?\n\n> \n> I read the message that the AOL proxy has been issuing that blames the\n> remote site for the failure (in Apache Week).  It would seem that their\n> proxy does not implement HTTP/1.0 correctly if it does not accept a\n> response in the same major version (which is all servers have to provide)\n\nPerhaps they don't implement the non-standard HTTP/1.0 RFC 'correctly'\nwhich may not have even existed when they implemented...\n\nUntil this discussion started, my interpretation was that n HTTP/1.0\nrequest should always receive HTTP/1.0 in the status line.\n\nDave Morris\n\n\n\n", "id": "lists-010-14018979"}, {"subject": "Re: Spontaneous unsubscription? (fwd", "content": "Nice one! I just tried to send the message below, and look what happened.\nCertain strings have been modified to protect me from a recurrence!\n\nCheers,\n\nBen.\n\nhttp-wg-request@cuckoo.hpl.hp.com wrote:\n> From heap.ben.algroup.co.uk!arachnet-fw.algroup.co.uk!hplb.hpl.hp.com!cuckoo.hpl.hp.com!http-wg-request Sat Dec 21 12:47:59 1996\n> Date: Sat, 21 Dec 1996 13:53:02 GMT\n> From: http-wg-request@cuckoo.hpl.hp.com\n> Message-Id: <199612211353.AA123526382@cuckoo.hpl.hp.com>\n> To: ben@algroup.co.uk\n> Subject: Re: Spontaneous uns*bscription?\n> References:  <9612211247.aa24605@gonzo.ben.algroup.co.uk>\n> In-Reply-To:  <9612211247.aa24605@gonzo.ben.algroup.co.uk>\n> X-Loop: http-wg@cuckoo.hpl.hp.com\n> Precedence: junk\n> \n> WARNING:\n> Please try to use 'http-wg-request@cuckoo.hpl.hp.com'\n> the next time when issuing (un)subscribe requests.\n> \n>                  32752 ben@algroup.co.uk\n> ben@algroup.co.uk\n> \n> You have been removed from the list.\n> \n> If this wasn't your intention or you are having problems getting yourself\n> uns*bscribed, reply to this mail now (quoting it entirely (for diagnostic\n> purposes), and of course adding any comments you see fit).\n> \n> Transcript of uns*bscription request follows:\n> -- \n> >From ben%gonzo.ben.algroup.co.uk@heap.ben.algroup.co.uk Sat Dec 21 13:53:02 1996\n> >Received: from otter.hpl.hp.com by cuckoo.hpl.hp.com with ESMTP\n> >(1.37.109.16/15.6+ISC) id AA123416381; Sat, 21 Dec 1996 13:53:01 GMT\n> >Return-Path: <ben%gonzo.ben.algroup.co.uk@heap.ben.algroup.co.uk>\n> >Received: from hplb.hpl.hp.com by otter.hpl.hp.com with ESMTP\n> >(1.37.109.16/15.6+ISC) id AA070786380; Sat, 21 Dec 1996 13:53:00 GMT\n> >Received: from arachnet.algroup.co.uk by hplb.hpl.hp.com; Sat, 21 Dec 1996 13:52:58 GMT\n> >Received: from heap.ben.algroup.co.uk by arachnet.algroup.co.uk id aa14828;\n> >          21 Dec 96 13:52 GMT\n> >Received: from gonzo.ben.algroup.co.uk by heap.ben.algroup.co.uk id aa24906;\n> >          21 Dec 96 12:56 GMT\n> >Subject: Spontaneous uns*bscription?\n> >To: HTTP Working Group <http-wg@cuckoo.hpl.hp.com>\n> >Date: Sat, 21 Dec 1996 12:47:37 +0000 (GMT)\n> >From: Ben Laurie <ben@gonzo.ben.algroup.co.uk>\n> >Reply-To: ben@algroup.co.uk\n> >X-Mailer: ELM [version 2.4 PL24 PGP2]\n> >Mime-Version: 1.0\n> >Content-Type: text/plain; charset=US-ASCII\n> >Content-Transfer-Encoding: 7bit\n> >Content-Length:        591\n> >Message-Id:  <9612211247.aa24605@gonzo.ben.algroup.co.uk>\n> >\n> >I've just discovered that I've been spontaneously uns*bscribed from HTTP-WG.\n> >I've just resubscribed, but I'd be grateful if someone would:\n> >\n> >a) tell me where the archives are\n> >\n> >b) take a look to see why I was uns*bscribed so I can avoid it happening again\n> >\n> >TIA,\n> >\n> >Cheers,\n> >\n> >Ben.\n> >\n> >-- \n> >Ben Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\n> >Freelance Consultant and  Fax:   +44 (181) 994 6472\n> >Technical Director        URL: http://www.algroup.co.uk/Apache-SSL\n> >A.L. Digital Ltd,         Apache Group member (http://www.apache.org)\n> >London, England.          Apache-SSL author\n> >\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n", "id": "lists-010-14030382"}, {"subject": "Re: HTTP response version, agai", "content": "Brian Behlendorf wrote:\n> \n> On Fri, 20 Dec 1996, M. Hedlund wrote:\n> > On Fri, 20 Dec 1996, Dave Kristol wrote:\n> > > I still consider the question unresolved as to what version an HTTP/1.x\n> > > server should return for an HTTP/1.0 request.\n> > [...]\n> > > Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n> > > Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n> > \n> > I agree with Dave that Case 1 is preferable.  AOL's proxies apparently\n> > started giving users errors this week when a new version of Apache was\n> > released, which responded to 1.0 requests with 1.1 responses (Case 2).  \n> > While this instance will likely be fixed next week, it does indicate how an\n> > HTTP/1.0 client can be confused by an HTTP/1.1 response.\n> \n> No, it indicates how a company with little concern for standards can dictate\n> implementations in other products through technological inertia.  There's\n> nothing in the 1.1 response which should cause problems with the 1.0 proxy or\n> 1.0 client - section 3.1 of both the 1.0 and 1.1 specs promise this, and (as\n> best this group can tell) 1.1 fulfills this promise.  \n\nI agree with Brian here.  Case 2 is the preferable solution.  For a\nstart it's the only easy way I can see of allowing my (HTTP/1.1\ncompliant) browser can establish persistent connections with 1.1 proxies\nwithout requiring an extra request just to test it out.  I am not willing\nto rely on some of the more obscure methods (such as OPTIONS) not being\nimplemented in a 1.0 proxy either.\n\nMy browser keeps a list of sites recently visited and their HTTP version\nprecisely so it can avoid confusing them.\n\nI read the message that the AOL proxy has been issuing that blames the\nremote site for the failure (in Apache Week).  It would seem that their\nproxy does not implement HTTP/1.0 correctly if it does not accept a\nresponse in the same major version (which is all servers have to provide)\n\n\n-- \nStewart Brodie, Electronics & Computer Science, Southampton University.\nhttp://www.ecs.soton.ac.uk/~snb94r/      http://delenn.ecs.soton.ac.uk/\n\n\n\n", "id": "lists-010-14045320"}, {"subject": "Re: question/observation about HTTP/1.", "content": "# If Content-length is present for a body-part of a Multipart, does it\n# take precedent over a boundary string?  The section 4.4 'Message Length'\n# perhaps implies that it does.\n\nIt shouldn't. Content-length is optional, and (on many systems)\nunreliable.\n\n# Can a 'Transfer-Encoding: chunked' appear in the header of a body-part\n# of a Multipart entity? \n\nI think so.\n\n# If the answer to both these questions is yes, then I would like more\n# explicit wording that the rules for messages applies to nested messages\n# in Content-types that have nested entities, such as Multipart content-\n# type.\n\nWhich rules?\n\n# If the answer is no, then I would like to know if we can change your mind.\n\nNot without supplying convincing reasons.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-14055785"}, {"subject": "Re: HTTP response version, agai", "content": "Brian Behlendorf:\n>On Fri, 20 Dec 1996, M. Hedlund wrote:\n>> \n>> [...]  AOL's proxies apparently\n>> started giving users errors this week when a new version of Apache was\n>> released, which responded to 1.0 requests with 1.1 responses (Case 2).  \n>> While this instance will likely be fixed next week, it does indicate how an\n>> HTTP/1.0 client can be confused by an HTTP/1.1 response.\n>\n>No, it indicates how a company with little concern for standards can dictate\n>implementations in other products through technological inertia.\n\n[....]\n\n>If it becomes common acceptance that 1.0 and 1.1 are incompatible, then no one\n>will ever upgrade to 1.1.  This is exactly the perception this wg labored long\n>and hard to prevent.\n>\n>The big question is, what will happen first: will AOL fix their proxies, or\n>will Apache users \"fix\" [hack] their servers?  Client service dictates that\n>we at Organic hack our servers, but the Apache development group has no such\n>requirements.  \n>\n>A document describing the situation is available at\n>\n>  http://www.apache.org/info/aol-http.html\n\nEek!  From the looks of it, this could easily devolve into trench\nwarfare.  And the perception that 1.0 and 1.1 are compatible would be\nthe first victim.\n\nI think that AOL's decision is silly, but they _are_ allowed to let\ntheir 1.0 clients do silly things when getting a 1.1 response.  The\n1.0 spec is a `best current practice' spec, so you can call your\nclients 1.0 even if they disagree with some of it.  It would make AOL\nclients `inferior current practice', but they are allowed to be.\n\nHowever, AOL has no business trying to drive internet standards by\nmaking their clients barf on certain responses they deem incorrect.\n\nI advise the Apache development group to work around AOL's inferior\n1.0 clients, rather than adjusting the default to them.  This means\nextending the standard configuration files of the next Apache release\nwith some BrowserMatch directive that causes AOL's clients to receive\nthe 1.0 version number they expect.  This will take AOL out of the\nloop as far as driving internet standards is concerned.\n\nWe can then discuss in peace what to do about response version numbers\nin the 1.1 spec.  I think that we at least need to add more clarifying\nlanguage.\n\n>        Brian\n\nKoen.\n\n\n\n", "id": "lists-010-14063656"}, {"subject": "Re: 505 HTTP Version Not Supporte", "content": "At 6:36 PM 4/2/96, Roy T. Fielding wrote:\n>Please note that, among other things, this allows future applications\n>to cease support for older protocols and yet do so in a way that is\n>meaningful to older clients.\n\nI think \"meaningful\" is a relative term. It isn't going to be very useful\nfor a 0.9 client to receive a 505 message, for example. It isn't going to\nbe much more useful for a 1.0 client to receive a 505 error if it simply\ntreats it as an unknown failure. There are other, existing error codes that\nare interpreted as meaning a particular requested service or method is\nunavailable. If the intent is to let a client know WHY the service or\nmethod is unavailable (i.e., because of a protocol version mismatch), why\ndon't we just add an entity to the existing error code(s) that clients\nalready support?\n\nThis would allow new clients to provide more info to the user and/or switch\nprotocol versions to accomodate the server. Existing older clients would\ncontinue to process the error code(s) that existed when they were written\nand respond accordingly.\n\nI propose using whichever of the 400, 501, or 503 error codes that makes\nsense. It seems that 501 might be the most appropriate, and that adding a\nheader field like \"Failure-Reason:\" would allow newer clients to elaborate\non why the request failed without confusing older clients.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                      \"What? Me? WebSTAR?\"\n\n\n\n", "id": "lists-010-1406885"}, {"subject": "Re: HTTP response version, stil", "content": "Arnoud Engelfriet writes:\n > \n > Wouldn't it be better to *only* give 1.1 responses if the *request* was\n > also done in HTTP/1.1?\n > \n > Galactus\n > \n\nThe question seems to boil down to what the significance of the\nversion number in the header is.\n\nSince it should be possible for (most) HTTP 1.1 headers to exist without\nharm in a message labelled HTTP/1.0, then it matters very little what\nversion is in the label, so long as the other headers are interpreted\nindependently of the (minor) version number in the label.\n\nA 1.1 server can't use aspects of the protocol in its communication\nwith a 1.0 client that cannot be understood in 1.0, such as chunked\ntransfer encoding.  However, a 1.1 server should be allowed to add\nsuch headers to the response as should not confuse a 1.0 client.\n\nSo, one important function of the version number in a _request_ is to\nlimit the response to that part of the protocol which is at least\nbackward compatible with the request, according to our rules of\nengagement (that unknown headers be ignored (clients) and passed\nthrough (proxies)).\n\nThe version number in the request identifies the capabilities the\nclient wishes to expose, but since that client may be a proxy, and may\nbe a proxy for another client with a higher version number, it would\nbe nice if the version number of the proxy did not limit a server to\nresponses without anything in them pertinent to higher versions than\nthat of the immediately adjacent client.  (The protocol version number\nin a message is a point-to-point aspect of the procotol, not\nend-to-end).\n\nIn other words, it would be nice if a 1.1 client could get a message\nwith 1.1 headers (and semantics) even if speaking to the server\nthrough a 1.0 proxy.\n\nThe question then becomes, how should a message from a 1.1 server to a\n1.0 client be labelled?  We know that it may have backward compatible\n1.1-level headers in it, but is that enough reason to label the entire\nmessage as 1.1?   Intuitively it makes sense, but on the other\nhand, a 1.0 proxy as in the above example would return 1.1 headers if they\nwere given it by a 1.1 server, and the 1.0 proxy would be unlikely to\nidentify its responses as HTTP/1.1.\n\nMy conclusion is that the version number in the response doesn't\nmatter much.  The important point is elsewhere: HTTP 1.1 servers have\nto provide backward compatible responses when they receive requests\nlabelled HTTP 1.0.  However, they should not be restricted from adding\nheaders only defined within HTTP 1.1 in those responses.  Similarly,\nHTTP 1.1 clients that do not know the version of a server should give\nbackward compatible requests, or be prepared for anomolous results.\n\nSince there _are_ some aspects of HTTP 1.1 that are incompatible with\n1.0 (chunked transfer encoding for example) it is important to\nclarify which aspects of 1.1 are backward compatible and which are\nnot.\n\nIt is unfortunate that there seems to be some software around that \nwon't work simply due to the version number in the response header.\n\n--Shel\n\n\n\n", "id": "lists-010-14074304"}, {"subject": "Re: HTTP response version, agai", "content": "Alexei Kosut wrote:\n> \n> On Sat, 21 Dec 1996, Ben Laurie wrote:\n> \n> > It also seems to me that the spec is not clear on this issue. There is clear\n> > intent in HTTP/1.1, in that the word \"major\" was added to the version of\n> > the response, but, AFAICS, no clear requirement to respond either 1.0 or 1.1 to\n> > a 1.0 request. Since a requirement of the spec is to be liberal in what is\n> > accepted, it seems to me that the correct interpretation of the spec is that\n> > a 1.1 reponse to a 1.0 request is permitted.\n> > \n> > I forget where we are wrt modifications to the spec. It seems to me that a\n> > modification should be made to clarify this point, whichever way it goes.\n> \n> IMHO, I don't think it's particuarly neccessary for the spec to\n> specify how a HTTP/1.1 server has to respond to HTTP/1.0 requests. As\n> was pointed out, even if the spec did say that it had to respond with\n> HTTP/1.1, that doesn't mean anything, because in fact when a HTTP/1.0\n> request came in, the server could just use HTTP/1.0 semantics and\n> would be perfectly within its rights to respond with HTTP/1.0. I don't\n> think it's neccessary to specific one over the other - both can\n> coexist perfectly well. (that being said, I do favor responding to\n> HTTP/1.0 with HTTP/1.1).\n> \n> However, it seems certainly desirable to add some language to the spec\n> along the lines of \"if a message of a version with a known major\n> number, but an unknown minor number is received (e.g., if a HTTP/1.0\n> client recieves a HTTP/1.1 response, or a HTTP/1.2 server receives a\n> HTTP/1.7 request), it should be treated as if it was a request\n> equivilent to the highest minor number in that major version that is\n> supported.\"\n\nAgreed. Except we should spell equivalent correctly ;-)\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n", "id": "lists-010-14084174"}, {"subject": "Re: HTTP response version, agai", "content": "> I think that AOL's decision is silly, but they _are_ allowed to let\n> their 1.0 clients do silly things when getting a 1.1 response.  The\n> 1.0 spec is a `best current practice' spec, so you can call your\n> clients 1.0 even if they disagree with some of it.  It would make AOL\n> clients `inferior current practice', but they are allowed to be.\n\nPlease stop spreading this nonsense.  There is only one definition\nof HTTP/1.0 and that is found in RFC 1945.  Whether or not it is a\nstandard is simply irrelevant.  Any application which fails to\nimplement HTTP/1.0 as specified in RFC 1945 is not implementing\nHTTP/1.0, period.  That is the difference between a \"bug\" and a \"feature\"\nfor any protocol, regardless of its status in the IETF.\n\nAOL's problem is that their proxy does not correctly implement HTTP/1.0.\nThat isn't too surprising -- many applications don't.  Their problems\nwon't go away until they do implement HTTP/1.x correctly.  This situation\nhas absolutely nothing to do with HTTP/1.1 aside from the singular fact\nthat it was one or more features of an HTTP/1.1 response that triggered\nthis particular bug in AOL's proxy.  I could easily trigger the same\nbug with a valid HTTP/1.0 response.\n\nIt is very important that implementers understand the source of their\nproblems.  We didn't spend 18 months writing and reviewing RFC 1945\njust because it seemed like a good idea at the time.  We needed a\nprotocol definition that was not subject to the variance of every\nsingle hacker under the sun.\n\nOn a related subject, the reason why HTTP/1.1 does not say when a\nserver should send an HTTP/1.0 response is because then it would\nbe an HTTP/1.0 server and not subject to the draft-07 specification.\nThere are some things better left for writers of books.\n\nCheers,\n\n ...Roy T. Fielding    (still on vacation, and enjoying it)\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-14095854"}, {"subject": "Re: HTTP response version, agai", "content": "David W. Morris wrote:\n> On Sat, 21 Dec 1996 S.N.Brodie@ecs.soton.ac.uk wrote:\n> \n> > My browser keeps a list of sites recently visited and their HTTP version\n> > precisely so it can avoid confusing them.\n> \n> What will it do in the future when an HTTP/1.2 site declares itself?\n\nTreat it as a 1.1 server and send it 1.1 requests, since the major\nversion matches (it is 1) and the minor version is >= 1.  Obviously, if\na process starts to define HTTP/1.2 I'll look at adding an\nimplementation of it.\n\n> Until this discussion started, my interpretation was that n HTTP/1.0\n> request should always receive HTTP/1.0 in the status line.\n\nI don't agree - but I don't have the documents here to back myself up.\n\n\n-- \nStewart Brodie, Electronics & Computer Science, Southampton University.\nhttp://www.ecs.soton.ac.uk/~snb94r/      http://delenn.ecs.soton.ac.uk/\n\n\n\n", "id": "lists-010-14105217"}, {"subject": "Re: HTTP response version, agai", "content": "Roy T. Fielding:\n>\n   [Koen Holtman:]\n>> I think that AOL's decision is silly, but they _are_ allowed to let\n>> their 1.0 clients do silly things when getting a 1.1 response.  The\n>> 1.0 spec is a `best current practice' spec, so you can call your\n                  ^^^^^^^^^^^^^^^^^^^^^ \n>> clients 1.0 even if they disagree with some of it.  \n\nCorrecting an error in my own message: RFC1945 is not even `best\ncurrent practice', it ended up being `informational'.  This gives it\neven less legislative power.  To quote RFC1602:\n\n              An \"Informational\" specification is published for the\n              general information of the Internet community, and does\n              not represent an Internet community consensus or\n              recommendation.\n\n>>                                             It would make AOL\n>> clients `inferior current practice', but they are allowed to be.\n>\n>Please stop spreading this nonsense.  There is only one definition\n>of HTTP/1.0 and that is found in RFC 1945.\n\nTrue.\n\n>  Whether or not it is a\n>standard is simply irrelevant.\n\nWe've had this argument before.  All I pointed out above was that the\nIETF as a whole has no opinion on the relevance of RFC1945.  You and I\nhave an opinion about it, and it happens to be the same opinion, but\nit is a _personal_ opinion.\n\nThis is not an AOL vs. IETF conflict.  This is an AOL vs. as bunch of\nindividuals conflict.  If people want to go to war with AOL about\nthis, they should know that they are not fighting for the IETF, and\nthat the IETF would frown on them using the prestige of the RFC series\nas a weapon.\n\nI feel that interoperability, which is the main thing the IETF is\nworking for, would not be served by making the default configuration\nof Apache incompatible with AOL's 1.0 clients.  Interoperability would\nnot even be served in the long run.  This would just cause a lot of\npain for unsuspecting AOL users and unsuspecting Apache site\nmaintainers.  Not to mention the mass media fallout.\n\nSure, I would like to see AOL change their proxies to match RFC1945.\nBut if this cannot be achieved by talking to them (maybe after putting\nsome clarifications in the 1.1 draft spec), so be it.  Just take them\nout of the loop.\n\nI would probably have had another opinion if this had been any other\nclient author, but AOL is a fairly big company.  I would be pleasantly\nsurprised if they were able to reverse their version numbering\ndecision within two months.\n\n> ...Roy T. Fielding    (still on vacation, and enjoying it)\n\nKoen.\n\n\n\n", "id": "lists-010-14115258"}, {"subject": "Re: HTTP response version, agai", "content": "Koen Holtman wrote:\n\n>I feel that interoperability, which is the main thing the IETF is\n>working for, would not be served by making the default configuration\n>of Apache incompatible with AOL's 1.0 clients.  Interoperability would\n>not even be served in the long run.  This would just cause a lot of\n>pain for unsuspecting AOL users and unsuspecting Apache site\n>maintainers.  Not to mention the mass media fallout.\n\nI think you're perhaps missing or forgetting the point. AOL went out\nof its way to cause this problem and has admitted that they did it to\ntry to FORCE http/1.1 servers (whih means Apache 1.2b to them)\nto change their behaviour, such actions should not be tolerated.\n\nIt is not unreasonable for AOL to go out of their way to undo the change\nnow that they have been told that they have nothing to fear from http/1.1\n\nAt this stage I will use my veto on the Apache list to prevent the\ndefault Apache configuration being compatible with AOL's sabotage.\nThat would leave us with tens of thousands of Apache 1.2 servers unable\nto use http/1.1 features for 8 million AOL users for some time to come.\nIt is in AOL's best interest that Apache 1.2 identifies itself correctly.\n\n\nrob\n\n\n\n", "id": "lists-010-14125476"}, {"subject": "Re: HTTP response version, agai", "content": "Dave is right.\n\nAn individual HTTP message should be interpretable by the version\nnumber in the request/response line.  Additionally, http 1.1 should\nminimize the proxy traffic that does not use persistent connections.\n\nThe following suggestions achieve these aims.\n.\nTo make proxies work right, one needs to record the version of\nthe client or the origin server in header.\n\n* HTTP-Version MUST be used by clients and servers to advertise\ntheir maximum protocol version. Too bad the server and client headers\ndidn't include maximumhttp version.\n\n* The via header takes care of the proxy versions.\n\nThe following ensures that the most traffic will use 1.1 persistent connections\nand caching features.\n\n\n* HTTP 1.1 proxies MUST upgrade 1.0 requests from clients when talking\nto 1.1 proxies or servers downstream. They also need to downgrade\nresponses.\n\n* HTTP 1.1 MUST downgrade 1.1 requests from clients when talking to\n1.0 proxies or servers downstream and upgrade responses coming back.\n\n\nOne way to minimize work in proxies and prevent tampering with an http\nmessage is to encapsulate it with the different protocol version. This\nkind of tunnelling is needed for any digested, digitally-signed, or\nencrypted message.\n\nFor present purposes, the last 1.1 proxy in the request chain would need to\nunpack a tunnelled 1.0 message before delivering it to a 1.0 origin server.\nSimilarly, the last proxy in the respopnse chain would need to unpack\nthe response for the 1.0 client.\n\nOnce the old servers and clients are gone, the encapsulation facility\nwill be used for transmitting cryptographically checked messages.\n\n\nAt 4:11 PM -0800 1996-12-20, David W. Morris wrote:\n>On Fri, 20 Dec 1996, Larry Masinter wrote:\n>\n>> Dave, what about:\n>> \n>> # The protocol version in the response MAY be either HTTP/1.1 or\n>> # HTTP/1.0. The headers in the response MUST be consistent with BOTH the\n>> # protocol version of the response AND the protocol version in the\n>> # request.\n>> \n>> I don't know why we have to nail this down. \"We MAY not always have to\n>> MUST if we can MAY.\"\n>\n>And the content of the response MUST be compatible with the request\n>version when the version is lower than the response version ...\n>this I suppose is a side effect of TRANSFER-ENCODING: as a header\n>not allowed by the above rule but is will surely break a 1.0 client\n>if it is presumed to ignore unknown headers and is sent a content\n>body with chunked encoding.\n>\n>I think that the real issue here is we are using a single value to\n>accomplish two objectives:\n>\n>   a.  Label the level of the response\n>   b.  Declare the capabilities of the server\n>\n>\n>Perhaps the 'bug' fix is to add a way for the server to declare its\n>capabilities ? And in the status, label the level of the response.\n>\n>Dave Morris\n\n\n\n", "id": "lists-010-14133455"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "They are technical errors, in the sense that either they will result in\nwidespread disregard for the spec (esp. 1 & 2) or they are simply poor\ntechnical choices, or both.\n\n>- They do not need to be corrected immediately.\n\nIMHO, they need to be corrected before going to Draft Standard. See below.\n\n># The first two are statements of fact that are demonstarbly wrong on\n># today's Web. \n>\n>They are not statements of fact, they are descriptions of the protocol\n>being defined, HTTP/1.1, which did not exist before.\n\nIt is amusing that you are making this argument now.  The exact opposite\nwas made at the Montreal IETF, when I cited the IAB charset committee's\nrecommendation of using UTF-8 in new protocols: even though the Warning\nheader was new, HTTP was not a new protocol so the recommendation did not\napply.\n\nWhether 1.1 is a new protocol is indeed debatable.  Take for example the\ncurrent thread about response version numbers: the arguments revolve about\na larger model for HTTP, with for instance claims that all 1.x clients\nshould accept responses of the same major version number.  In that larger\nmodel, 1.1 is not a new, distinct protocol, although it certainly has new\nfeatures.\n\nAlso, HTTP servers will for a long time have to talk to 1.0 proxies and\nclients (and vice versa). In fact, it was a very basic design goal of 1.1\nthat this can work smoothly.  Yet conforming 1.1 servers can simply assume\n(wrongly) that all 1.0 clients support 8859-1 (#2 above), resulting in bad\ninteroperability. Why is that basic design goal disregarded in this case,\nwhen the fix is so simple? Probably because the 1.0 \"spec\" had the same\n(false) statement; 1.1 carried it over, in which it did not behave as a new\nprotocol.  1.1 bought compatibility with the 1.0 spec, at the expense of\nreal interoperability.\n\n># The last two are obstacles to i18n that are not justified by any\n># technical requirements.  All four should be modified or deleted.\n>\n>They do not represent obstacles to i18n.\n\nYes, #3 does, and #4 does not even deserve further comment.  Cluttering the\n8-bit channel with 8859-1, with no justification whatsoever, is an obstacle\nto i18n: it becomes impossible to use 8-bit octets with any other charset,\nand 8859-1 is not a proper i18n solution.\n\n>None of the proposed changes would actually\n>IMPROVE i18n. \n\nTongue in cheek?  Having UTF-8 instead of 8859-1 in the 8-bit channel of\nthe Warning header is not an improvement?  Stopping the lies about the\ncharset of entities is not an improvement?  Come on!\n\n>Eliminating any other charset than UTF-8 would seriously\n>hamper I18N, and if other charsets are to be allowed, RFC1522 encoding\n>or some other equivalent is mandatory.\n\nNobody is asking for the elimination of other charsets. It's 8859-1 that's\nin the way as the default in Warning, just change that to UTF-8.  All other\ncharsets remain as before, under the RFC 1522 umbrella.\n\n>I believe we have heard clearly from a number of working group members\n>that they would like to see this change made urgently. However, I also\n>believe we have heard clearly from other working group members that\n>they would not like to see this change made. In some cases, the\n>current scheme is supported as is, and no change is desired. In some\n>other cases, working group members believe that the issue could be\n>addressed in some future specification, but not in HTTP/1.1.\n\nIf 1.1 is to move to Draft, it will have to address this.  Let's look at\nthe near future, the few-month horizon when 1.1 is due to progress.\nInteroperable implementations are needed for that, clients and servers.\n\nDo you think that browser makers will suddenly abandon their international\nmarkets and force their products to always assume 8859-1 as the default\ncharset for entities?  Not a chance!  Non-western users *need* other\ndefaults, since servers do not generally label charset, a direct\nconsequence of the \"official\" 8859-1 default.\n\nDo you think that substantially all browsers will start supporting 8859-1\non all platforms (incl. code page) on which they might run?  Very unlikely.\n\nThus, even in the brave new 1.1 world, the #1 prescription will be widely\ndisregarded, and the #2 assumption will remain very wrong.  The spec will\nnot be in sync with implementations, for very good reasons, and will not be\nsuitable for progression to Draft Standard.  Better fix it ASAP.\n\n>In any case, I don't intend to act further on suggestions that we try\n>to halt the progression of draft-ietf-http-v11-spec-07.* to RFC,\n\nNo such suggestions have been made.  The language I cited earlier from RFC\n2026 allows those changes *without* halting publication of draft-07, and\nwithout resetting the clock to Draft Standard. The idea is republication\nwith a new RFC number and no impact on time-at-level.\n\nRegards,\n-- \nFran?ois Yergeau <yergeau@alis.com>\nAlis Technologies Inc., Montr?al\nT?l : +1 (514) 747-2547\nFax : +1 (514) 747-2561\n\n\n\n", "id": "lists-010-14144093"}, {"subject": "Re: Warnings, RFC 1522, and ISO-8859", "content": "Despite all of the mail sent on this topic, there won't be any action\non it until someone has written up an alternative and there is the\nappearance of consensus on the mailing list that the alternative is\nsupported as a replacement for the current text.\n\nInternet drafts can be sent to the Internet Drafts editor. I suggest\nthe document name:\n\n  draft-ietf-http-charset-flap-00.txt\n\nPlease be very clear about what it is you propose changing.\n\nFrankly, I don't believe there is anything close to consensus on this\nissue and I have yet to see anyone actually change their mind on this\nissue, but I don't want to stop you if you think this a good use of\nyour time.\n\nLarry\n\n\n\n", "id": "lists-010-14156756"}, {"subject": "Re: 505 HTTP Version Not Supporte", "content": "At 07:17 AM 4/3/96 -0600, Chuck Shotton wrote:\n>At 6:36 PM 4/2/96, Roy T. Fielding wrote:\n>>Please note that, among other things, this allows future applications\n>>to cease support for older protocols and yet do so in a way that is\n>>meaningful to older clients.\n>\n>I think \"meaningful\" is a relative term. It isn't going to be very useful\n>for a 0.9 client to receive a 505 message, for example. It isn't going to\n\nBut a 0.9 client won't ever see the error status code, so what's the point.\n(Haven't people pretty much given up on 0.9 doing anything useful in the\nfuture?  There's too many features you can't have when you don't have headers.)\n\n>be much more useful for a 1.0 client to receive a 505 error if it simply\n>treats it as an unknown failure.  There are other, existing error codes that\n>are interpreted as meaning a particular requested service or method is\n>unavailable. If the intent is to let a client know WHY the service or\n\nNeither 501 nor 503 are appropriate.  501 indicates the method is bad, 503\nindicates the server is overloaded.  Neither one of those codes should be\noverloaded to indicate a protocol version rejection.\n\n>method is unavailable (i.e., because of a protocol version mismatch), why\n>don't we just add an entity to the existing error code(s) that clients\n>already support?\n>This would allow new clients to provide more info to the user and/or switch\n>protocol versions to accomodate the server.\n\nIf they don't understand 1.1, how would they be able to switch protocol\nversions?  There's no 1.05 they could be theoretically upgrading to from 1.0.\n\n>Existing older clients would\n>continue to process the error code(s) that existed when they were written\n>and respond accordingly.\n\nThe appropriate response to a 505 for a 1.0 client is the same as for a 500:\ntotal failure, report the entity body to the user.  Following the logic of\nyour line of arguement, we should just get rid of all 5xx codes and have one\n500 code.\n\n-----\nthe Programmer formerly known as Dan          \n                                     http://www.spyglass.com/~ddubois/\n                                     New direct dial phone: 708-245-6577\n\n\n\n", "id": "lists-010-1415844"}, {"subject": "CacheControl directive, semantic", "content": "Jeffrey Mogul writes:\n > I'll take you up on your offer to help ... if you can figure out\n > how to add these to the table, please do so.  It would be nice if\n > it still fits in a 72-column fixed-width format, because then I\n > can stick it into an Internet-Draft without any glitches.\n\nNo wonder my implementation of HTTP/1.1 caching proxy is wrong. I had\na real hard time trying figuring out the semantics and interactions\nof all the caching rules express in draft 07 (I should have checked\nthis before). \n\nFor my own sanity, I have tried to build up a richer table (it may be a\nmistake). The most difficult part in the exercise was to find an\nappropriate layout for the tables (I really don't want to miss some\ncases). I would rather have taken a top down approach (define the\ncases, and see how they are expressed through HTTP/1.1), but for the\ntime being, I tried the bottom up approach (given cache-control\ndirectives, define the semantics).\n\nIf you find any cases missing, or if they are any mistakes, please let\nme know so that I can fix my implementation before it's too late. \n\nThanks for your attention,\nAnselm.\n\n---------- \nMy first table defines a \"fresh\" function, which returns a boolean,\nand which takes as parameters:\n\nAr: The max-age value specified in the request\nSr: The max-stale value specified in the request\nFr: The min-fresh value specified in the request\n\nAp: The max age value of the reply (expressed either through the\n    max-age directive, or the expires header)\n\nAe: The current entity age, computed as defined in section 13.2.3\n\nThis table defines the \"fresh\" function, along with any warnings to be\nemitted when needed. Warnings accumulate: if, further on, this\ndocument define some cases where warnings are required, then the\nwarnings should be aggregated, and all of them should be emitted.\n\n+-----------------------------------------------------------------------+\n|         Request     |     Reply      | fresh           | warnings     |\n| max | max   | min   |    max-age     |                 |              |\n| age | stale | fresh |(or)expires     |                 |              |\n+-----+-------+-------+----------------+-----------------+--------------+\n+  *  |   *   |   *   |       Ap       | Ae<Ap           |   13?        |\n+-----+-------+-------+----------------+-----------------+--------------+\n+ Ar  |   *   |   *   |       Ap       | Ae<Ar && Ae<Ap  |   13?        |\n+-----+-------+-------+----------------+-----------------+--------------+\n+  *  |   Sr  |  NA   |       Ap       | Ae<Ap+Sr        |Ae<Ap=>10,13? |\n+-----+-------+-------+----------------+-----------------+--------------+\n+  *  |   *   |  Fr   |       Ap       | Ae+Fr<Ap        |  13?         |\n+-----+-------+-------+----------------+-----------------+--------------+\n+  *  |   *   |   *   |  (undefined)   | Ae < Ap         |  13?         |\n+-----+-------+-------+----------------+-----------------+--------------+\n+ Ar  |  Sr   |  NA   |       Ap       |Ae<Ap+Sr && Ae<Ar|Ae<Ap=>10,13? |\n+-----+-------+-------+----------------+-----------------+--------------+\n+ Ar  |  NA   |  Fr   |       Ap       |Ae+Fr<Ap && Ae<Ap|  13?         |\n+-----+-------+-------+----------------+-----------------+--------------+\n+  0  |  NA   |  NA   |       Ap       | false           |  13?         |\n+-----+-------+-------+----------------+-----------------+--------------+\n\n* : undefined (has no value)\nNA: Not applicable\n\nThe \"loose\" function is true when \"fresh\" is true and a warning is\ndefined.\n\nGiven that definition of the fresh and loose functions, I then tried\nto rebuild Jeff's table, inverting the layout, though (to make sure\nall cases where available at a glance). This now defines the \"valid\"\nfunction:\n\nThe valid row returns a boolean, indicating wether the cached reply\ncan be used to answer the given request. It also adds one or more of\nthe following comments in parenthesis:\n\nNC: not cached (in that situation, the reply should not even be cached)\nii: The class of the warning to be emitted.\n\nThe \"Cache site\" is overloaded: it can be either the type of cache, or\nthe number of a section in the HTTP/1.1 spec (draft v07), that defines\nthat configuration.\n\nF: is the result of the \"fresh\" function as defined above\nL: is the result of the \"loose\" function as defined above\n\nC: means that a check (revalidation or more) must be performed to the\n   origin server\nU: means that the cached reply can be used as is.\n\n+--------+\n| Cache  |\n| site   | public | private | must-reval | proxy-reval | hasAuth | valid\n+--------+--------+---------+------------+-------------+---------+-----------\n| shared |   *    |    *    |     *      |     *       |   *     | F?U:C\n+--------+--------+---------+------------+-------------+---------+-----------\n| 14.9.4 |   *    |    *    |     *      |     X       |   *     | F&&!L?U:C\n+--------+--------+---------+------------+-------------+---------+-----------\n| 14.9.4 |   *    |    *    |     X      |     *       |   *     | F&&!L?U:C\n+--------+--------+---------+------------+-------------+---------+-----------\n| 14.8   |   *    |    *    |     *      |     *       |   X     | (NC)\n+--------+--------+---------+------------+-------------+---------+-----------\n| 14.8/1 |   *    |    *    |     *      |     X       |   X     | C\n+--------+--------+---------+------------+-------------+---------+-----------\n| 14.8/3 |   X    |    *    |     *      |     NA      |   X     | F?U:C\n+--------+--------+---------+------------+-------------+---------+-----------\n| 14.9.1 |   NA   |    X    |     ?      |     NA      |   ?     | (NC)\n+--------+--------+---------+------------+-------------+---------+-----------\n| 14.9.1 |   X    |    *    |     *      |     NA      |   ?     | F?U:C\n+--------+--------+---------+------------+-------------+---------+-----------\n| priv   |\n+--------+--------+---------+------------+-------------+---------+-----------\n| 14.8/2 |   *    |    *    |     *      |     X       |   X     | C\n+--------+--------+---------+------------+-------------+---------+-----------\n| All remaining cases obtained by swapping <public>, <private> and \n| <must-reval>, <proxy-reval>, keeping valid constant.\n+--------+--------+---------+------------+-------------+---------+-----------\n| discon |                                                         (12)\n <to be specififed>\n\n\nSome comments on the spec:\n\n- 14.8/1 changes the general semantics of proxy-revalidate, when an\n  authorization field is present. It also probably assumes that a proxy\n  is a shared cache, while a \"cache\" is a single client cache.\n- [a question] 14.9.4 says under \"End to end reload\" that \"... No\n  field names may be included with the no-cache directive in a\n  request\". I assume an \"In that case, \" is implicitly assumed at the\n  beginning of the sentence ? [I can see good reasons for using\n  no-cache with field names in a request, as does]\n- In general, understanding HTTP/1.1 caching rules requires tricky\n  navigation between sections 13 and 14.8 and 14.9; I often feel\n  uncomfortable because when I read one, I am afraid that the other\n  changes the semantics (not to mention interactions with\n  negotiation).\n- BTW State management also got \"proxy-revalide\" wrong (I guess):\n\n   --- state management, draft, v05; section 4.2.3 -----\n   The origin server should send the following additional HTTP/1.1 response\n   headers, depending on circumstances:\n   \n      * To allow caching of a document and require that it be validated\nbefore returning it to the client: Cache-control: must-revalidate.\n   \n      * To allow caching of a document, but to require that proxy caches\n(not user agent caches) validate it before returning it to the\nclient: Cache-control: proxy-revalidate.\n   \n   ---\n\nThe first point here, seems to rely on the fact that\nmust-revalidate/proxy-revalidate is interpreted as \"*always*\nrevalidate the document before returning it\"; which was my\ninterpretation when I implemented Jigsaw proxy.\n\n\n\n", "id": "lists-010-14164551"}, {"subject": "Re: HTTP response version, agai", "content": "Rob Hartill:\n>\n>Koen Holtman wrote:\n>\n>>I feel that interoperability, which is the main thing the IETF is\n>>working for, would not be served by making the default configuration\n>>of Apache incompatible with AOL's 1.0 clients.  Interoperability would\n>>not even be served in the long run.  This would just cause a lot of\n>>pain for unsuspecting AOL users and unsuspecting Apache site\n>>maintainers.  Not to mention the mass media fallout.\n>\n>I think you're perhaps missing or forgetting the point.\n\nNope.\n\n> AOL went out\n>of its way to cause this problem and has admitted that they did it to\n>try to FORCE http/1.1 servers (whih means Apache 1.2b to them)\n>to change their behaviour,\n\nI'm fully aware of this, I read the aol-http web page.\n\n> such actions should not be tolerated.\n\nYour conclusion, not mine.  I interpret their action as a mistake, not\nas an act of war.  You forget that about half the people on this list\n(mis?)interpret the version number requirements of HTTP/1.x in the\nsame way as AOL does.\n\n>It is not unreasonable for AOL to go out of their way to undo the change\n>now that they have been told that they have nothing to fear from http/1.1\n\nSure, but give them some time.  Do not expect a big organization to be\nable to correct its mistakes quickly.\n\nI don't think you should go to war with AOL.  I recommend that you\ntreat AOL's proxies as inferior 1.0 implementations and work around\ntheir breakage in the Apache configuration files.  I'm not asking you\nto change the response version number in the default case, and I'm not\nasking you to ever omit HTTP/1.1-specific response headers. Just set\nthings up so that you can ignore AOL's actions.\n\n>At this stage I will use my veto on the Apache list to prevent the\n>default Apache configuration being compatible with AOL's sabotage.\n\nIf you want to send your users to the trenches over this, that is your\nchoice.  But don't expect me to stand by the side and cheer.  I worked\nvery hard in the last 1.5 years to allow for the _smooth_ deployment\nof HTTP/1.1.\n\n>rob\n\nKoen.\n\nPS: I just read Apache Week 46, and it reports that you _are_ going to\nwork around AOL's breakage.  So did I misunderstand your comments or\nis Apache Week wrong?\n\n\n\n", "id": "lists-010-14180387"}, {"subject": "Re: HTTP response version, agai", "content": "Koen Holtman wrote:\n> PS: I just read Apache Week 46, and it reports that you _are_ going to\n> work around AOL's breakage.  So did I misunderstand your comments or\n> is Apache Week wrong?\n\nThere are various patches under consideration to make AOL's breakage\nconfigurably fixable. Unfortunately, the neat way to do it does not necessarily\nwork with modules (I'm feeling slightly smug about this, because I already\nadvocated an API change to cure this very problem, but was not permitted to\ninclude it in 1.2).\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n", "id": "lists-010-14190212"}, {"subject": "Re: HTTP response version, agai", "content": "In the discussion so far the proponents of \"send HTTP/1.1 in response\nto an HTTP/1.0 request\" emphasize the desirability of advertising the\nserver's capabilities.  But that shouldn't be necessary.  If a client\nunderstands HTTP/1.1, it should send an HTTP/1.1 request (as Henrik\nfirst noted).  An HTTP/1.0 server (with two known exceptions) will\nrespond with HTTP/1.0.  If a client sends an HTTP/1.0 request, then\nsending an HTTP/1.1 response advertises something that's apparently of\nno use to that client.\n\nI agree with Dave Morris that we're trying to overload the meaning of\nthe protocol version in the status line.  If it's truly desirable to\nadvertise capability (and I'm not convinced), then I think the version\nof the response and the capability version should be separated.  The\nextra information would only be necessary when the two versions\ndiffer.\n\nDave Kristol\n\n\n\n", "id": "lists-010-14198877"}, {"subject": "Re: HTTP response version, agai", "content": "Koen Holtman wrote:\n\n>> such actions should not be tolerated.\n>\n>Your conclusion, not mine.  I interpret their action as a mistake, not\n>as an act of war.  You forget that about half the people on this list\n>(mis?)interpret the version number requirements of HTTP/1.x in the\n>same way as AOL does.\n\nbut did any of them set out to deliberately sabotage the work of others\nby silently blocking access ?. AOL's mistake was not approaching Apache\nor anyone else to discuss their so called \"problem\". Instead they chose\nto take on Apache and force it to play by AOL rules. They've admitted\nthat much.\n\n>>It is not unreasonable for AOL to go out of their way to undo the change\n>>now that they have been told that they have nothing to fear from http/1.1\n>\n>Sure, but give them some time.  Do not expect a big organization to be\n>able to correct its mistakes quickly.\n\nPeople running Apache 1.2 servers didn't have the luxury of giving AOL\ntime to make good especially when they showed no signs of doing so, quite\nthe opposite in fact.\n\n>I don't think you should go to war with AOL.\n\nThe \"war\" is over before it started. AOL have now surrendered. An AOL\nnetwork director has told me that AOL will undo their HTTP/1.1 blockade.\n\n>I recommend that you treat AOL's proxies as inferior 1.0 implementations\n\nwhat new ?  :-)\n\n>PS: I just read Apache Week 46, and it reports that you _are_ going to\n>work around AOL's breakage.  So did I misunderstand your comments or\n>is Apache Week wrong?\n\nI haven't looked to see what they wrote but it sounds wrong. We had no\nintention of changing Apache to workaround AOL's blockade. We did come\nup with some optional and unreleased patches which were being considered\nfor distribution outside of the main source code.\n\nrob\n\n\n\n", "id": "lists-010-14206363"}, {"subject": "Re: HTTP response version, agai", "content": "Rob Hartill:\n>Koen Holtman wrote:\n>\n    [Rob Hartill:]\n>>> such actions should not be tolerated.\n>>\n>>Your conclusion, not mine.  I interpret their action as a mistake, not\n>>as an act of war.  You forget that about half the people on this list\n>>(mis?)interpret the version number requirements of HTTP/1.x in the\n>>same way as AOL does.\n>\n>but did any of them set out to deliberately sabotage the work of others\n>by silently blocking access ?.\n\nGeez.  Try to look at it from their side.  AOL thought it was\ndetecting a new type of protocol error, and decided to let their\nproxies report it.  At least that is what I read from their response\nto you which is quoted on http://www.apache.org/info/aol-http.html .\nI would hardly call what they did deliberate sabotage.  Aggressive\ndiagnostics maybe.  And I wouldn't call their error message silent.\n\nAlso, they did respond to your mail about it, didn't they?\n\n[...]\n>>I don't think you should go to war with AOL.\n>\n>The \"war\" is over before it started. AOL have now surrendered. An AOL\n>network director has told me that AOL will undo their HTTP/1.1\n>blockade.\n\nWell, count me pleasantly surprised at their speed.\n\n>>I recommend that you treat AOL's proxies as inferior 1.0 implementations\n>\n>what new ?  :-)\n\nI did not say `start to treat as inferior', I said `treat as\ninferior'.\n\n>rob\n\nKoen.\n\n\n\n", "id": "lists-010-14214953"}, {"subject": "Hostile webserver attack!!!", "content": "Dear readers,\n\nHere is a summary of an article published in our local nigh newspaper\nregarding a security breach:\n\n\"Webcom Webserver has suffered a major attack on its web site.  This\nattack took off thier main webserver off the air for 40 hours!!!!\n\nThe attacking method is the \"SYN-flood\" which allows bombing of the site\nwith messages rating up to 200 messages per second.  Sending messages\nusing \"SYN-flood\" the user does not send a real IP address and the web\nserver keep on searching for the remote user to send the answer to. \nOverloading the web server with so many false messages didnot allow the\n\"real\" messages to get through and overloaded the Machine's memory.\n\nIt seems that two Hackers magazines have published the source code and\nnow any webserver in the world is opened to such and attack.\"\n\n(Summarised from \"Globes\"  http://www.globes.co.il Israel financial\nmagazine, Hi-Tech section, tuesday edition).\n\nIs any of you guys familiar with this \"SYN-flood\" bombimg method?  does\nanyone know how you can located this suspects and place them under a\n\"black list\" of forbidden sites?\n\n-- \nRegards\n-----------------------------------------------------------------\nErez Levin\nR&D manager\n\n\nDDDDDD   IIIIII NN      NN    GGGGG       OOO\nD     D    II   NNNN    NN   GG         OO   OO\nD      D   II   NN NN   NN  GG         OO     OO\nD      D   II   NN  NN  NN  GG  GGGG   OO     OO  Infosystems\nD     D    II   NN   NN NN   GG  GG     OO   OO\nDDDDDD   IIIIII NN    NNNN    GGGG        OOO\n\nEmail: erezl@dingo.co.il\nOur site:http://www.dingo.co.il\n-------------------------------------------------------------------\n\n\n\n", "id": "lists-010-14223610"}, {"subject": "Re: Hostile webserver attack!!!", "content": "At 22:55 +0100 24/12/96, Erez Levin wrote:\n[blah blah about SYN-flodd attack...]\n\n>Is any of you guys familiar with this \"SYN-flood\" bombimg method?  does\n>anyone know how you can located this suspects and place them under a\n>\"black list\" of forbidden sites?\n\n1. The SYN-flood attack has been a well-known bombing method for quite a\nfew weeks (months?) now.\n\n2. There is no way of locating the originator. The inherent principle of\nthe method consists of sending TCP SYN packets (the first packet in a TCP\nconnection, used to initiate it) with a false source address, so that the\ndestination cannot send the SYN_ACK back, and thus gets its table of\nconnection in \"opening\" (SYN_RCVD) state overflowed.\n\n3. Most major OSes have been patched to resist SYN flooding.\n\n4. To prevent your site, and downstream sites from yours, if you're an ISP,\nfrom being a source of SYN-flood attacks, you should set up access-lists on\nyour border routers discarding packets with a source that does not match\nthe corresponding network(s).\n\nNote that this is absolutely not linked to HTTP only, but to all TCP services.\n\nJacques.\n\n--- Jacques Caron - Pressicom - jcaron@pressicom.fr\n    Mail:   5/7 rue Raspail - 93108 Montreuil Cedex - France\n    Tel:    +33 (0)1 49 88 63 93 - Fax: +33 (0)1 49 88 75 15\n    TAMTAM: +33 (0)6 06 51 02 37 <- ca a encore change.\n    Planete.net: Angouleme, Bordeaux, Lille, Lyon, Marseille, Montreuil,\n    Montpellier, Nancy, Nantes, Rouen et Toulouse - http://www.planete.net\n\n\n\n", "id": "lists-010-14232165"}, {"subject": "Re: HTTP response version, agai", "content": "It appreciate all the help that i have been receiving,and i know i would have\ngiven up long ago if it were not for all the teams and their pain stakeing\neffords to help me to learn.\nI am not verry on computers as we all know,but i am trying to learn . Merry\nChristmas to all and a Happy New Year.\n                              Pixieo2@aol.com\nP.S. I know i must be the pilot program for having to envent the wheel\n\n\n\n", "id": "lists-010-14240811"}, {"subject": "Announce: Authentication Test Serve", "content": "  Agranat Systems, Inc, producer of the EmWeb Embedded Web server, has\n  established a free online interoperability test for browser digest\n  authentication at:\n\n    http://digest-test.agranat.com/\n\n  This server provides a series of tests of browser authentication,\n  both 'basic' and 'digest' schemes, and of authentication to multiple\n  realms (a feature poorly supported in most current browsers).\n\n  In the event of a test failure, the server provides detailed traces\n  of the messages between browser and server to help browser authors\n  see what has occured.\n\n--\nScott Lawrence             Principal Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n", "id": "lists-010-14248873"}, {"subject": "Serve", "content": "I hope this helps.\nPixieo2@aol.com\n\n\n\n\n\n\napplication/applefile attachment: Untitled_1\n\napplication/octet-stream attachment: Untitled_1\n\n\n\n\n", "id": "lists-010-14256172"}, {"subject": "Re: 505 HTTP Version Not Supporte", "content": "At 10:04 AM 4/3/96, Daniel DuBois wrote:\n>At 07:17 AM 4/3/96 -0600, Chuck Shotton wrote:\n\n>>method is unavailable (i.e., because of a protocol version mismatch), why\n>>don't we just add an entity to the existing error code(s) that clients\n>>already support?\n>>This would allow new clients to provide more info to the user and/or switch\n>>protocol versions to accomodate the server.\n>\n>If they don't understand 1.1, how would they be able to switch protocol\n>versions?  There's no 1.05 they could be theoretically upgrading to from 1.0.\n\nNo kidding. You're missing the whole point of my message. Existing clients\nalready know how to do something reasonably smart with existing error\nnumbers. Existing clients haven't a clue about 505. Adding a header field\nto an existing error code would allow existing clients to retain their\ncurrent behavior (a good thing) while enabling new clients to determine\nfailure reasons and perhaps renegotiate a different HTTP version (another\ngood thing).\n\nAdding a new, unknown error code that isn't supported by existing clients\nwould elicit an undetermined response from a large variety of the client\nbase (a bad thing) and would mean that only new clients would be forward\ncompatible with the 505 error code (an OK thing.) As usual, the solution\nseems to be to make the HTTP protocol bigger instead of smarter. All I\nadvocated was using an existing error code that means \"the server cannot\nhonor that request\" and expounding on it in some header fields. This seems\na much less fragile alternative to assuming all clients will understand and\ndo something smart with a 505 error code that they've never heard of\nbefore.\n\n>>Existing older clients would\n>>continue to process the error code(s) that existed when they were written\n>>and respond accordingly.\n>\n>The appropriate response to a 505 for a 1.0 client is the same as for a 500:\n>total failure, report the entity body to the user.  Following the logic of\n>your line of arguement, we should just get rid of all 5xx codes and have one\n>500 code.\n\nI'd certainly prefer that to a proliferation of them if that's the only\nchoice. Especially since the 500 series has been misdocumented and/or\nmisimplement since their inception.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                      \"What? Me? WebSTAR?\"\n\n\n\n", "id": "lists-010-1425800"}, {"subject": "Re: HTTP response version, agai", "content": "[Koen Holtman:]\n> Correcting an error in my own message: RFC1945 is not even `best\n> current practice', it ended up being `informational'.  This gives it\n> even less legislative power.  To quote RFC1602:\n> \n>               An \"Informational\" specification is published for the\n>               general information of the Internet community, and does\n>               not represent an Internet community consensus or\n>               recommendation.\n\nAnd, as I said before, that is irrelevant.  There is no legislative\npower in any IETF specs, even full standards.  RFC 1945 is not a\nproposed standard because we wanted HTTP/1.1 to be the proposed standard\nfor HTTP, and the IETF has no process for recognizing protocol families.\nNevertheless, the IETF did decide, via both the recommendation of this WG\nand the IESG, that RFC 1945 does define HTTP/1.0.  Aside from \"Obsolete\"\nor \"Updated\", the document status is not relevant to its meaning.\n\n.....Roy\n\n\n\n", "id": "lists-010-14262729"}, {"subject": "Re: Proxy authentication draf", "content": "> I don't see how this proposal is completely incompatible. The\n> challenge is extended, and a new kind of authorization is\n> expected. Old clients will fail to recognize the new challenge, and so\n> won't have access. New clients will recognize both.\n> \n> Old clients will send the old authorization header, which new servers\n> will either recognize or not.\n> \n> So what's the breakage?\n\nI don't have access to the document right now, so this is just from\nmemory.  The document proposes a change to the challenge syntax\n(right?) and that is an incompatible change in the sense that some\nclients may not even recognize it as a challenge, may puke, may die, etc.\nIf you change the challenge syntax, you will also need to change the\nfield-name of the header carrying the challenge.\n\nSince this is the same change discussed in mid-summer 95 (from memory)\nand rejected at that time, it is unlikely to succeed now.  At the very\nleast, it should be accompanied with extensive compatibility tests.\n\n.....Roy\n\n\n\n", "id": "lists-010-14270468"}, {"subject": "origin server vs. proxy with full UR", "content": "Okay, this may be a dumb question....  (Not my first!)\n\nHTTP/1.1 requires a server to accept an absoluteURI, like\nhttp://www.business.com/foo/bar.\n\nQuestion:  how does a server that acts both as a proxy and as an origin\nserver determine whether to service a request (as origin server) or\nforward it (as a proxy)?  The requests look the same.\n\nPossible answers:\n1) That's an implementation issue.  Not germane.\n2) The server examines the net_loc and decides whether the addressed\nhost corresponds to the origin server's name.  If not, the request is\na proxy request.  (The server must also check aliases, IP addresses, etc.)\n\nDave Kristol\n\n\n\n", "id": "lists-010-14278569"}, {"subject": "Re: origin server vs. proxy with full UR", "content": "On Fri, 27 Dec 1996, Dave Kristol wrote:\n\n> Okay, this may be a dumb question....  (Not my first!)\n> \n\nActually Dave you ask extremely good questions.  The problem seems to\nbe that they don't always get answered.\n\nI am still waiting for consensus on the response header from an \nHTTP/1.1 server to an HTTP/1.0 request.  Did I miss it?\n\nThe options were:\n\n1) Send HTTP/1.0 because that is the protocol the server is actually\nusing.\n\n2) Send HTTP/1.1 to advertise the servers capability to use 1.1 even\nthough what it is using is the 1.0 protocol.\n\n3) This is an implementation issue and up to the server.  The browser/proxy\nmust be able to accomodate either.\n\n\nI believe Dave opted for 1) and that seems the most reasonable to me.\nThe ambiguity has already created the notorious AOL flap.  Apparently\nthe Apache writers decided on option 2) (eminently reasonable) and at\nleast some of them interpreted the spec to mean that option 2) is a\nMUST (this seems hard to defend especially in light of the fact the WG\ncan't agree on the meaning of the spec).  At the same time AOL proxy\ndevelopers interpreted the HTTP/1.1 header to mean that HTTP/1.1\nprotocol and headers were being used.  This is not really consistent\nwith the current spec, but is a \"common sense\" interpretation one\nmight arrive at if the spec was not carefully studied.  Yes, everyone\nSHOULD study the spec, but option 2) is somewhat counter-intuitive \nand even the WG does not agree that this is what the spec says.\n\nThe bottom line is that 2) is consistent with the spec and may even\nhave been the intention of some spec authors.  But it is sufficiently\ncounter-intuitive that it will continue to cause problems unless the\nspec spells out explicitly that this is what is required.  The fact\nthat the WG can't agree on what the current spec says about this is\nprima facie evidence that ambiguity needs to be removed.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-14285885"}, {"subject": "Re: HTTP response version, agai", "content": "You, Steve Wingard, wrote:\n++ \n++ At 10:14 AM 12/30/96 -0500, Abigail wrote:\n++ >\n++ >I think the problem is more fundamental. If we force HTTP/1.0 clients\n++ >to accept HTTP/1.1 reponses, they also have to accept HTTP/1.2,\n++ >HTTP/1.7, etc responses. That of course means no HTTP/1.x header can\n++ >ever contain something which causes problems with HTTP/1.0 clients.\n++ \n++ Your conclusion doesn't follow.  An HTTP/1.x server should be aware of the\n++ version number of the client that it is speaking to, and refrain from using\n++ header information or mechanisms that are not \"understood\" by that client.\n++ A response from an HTTP/1.1 server can be constructed to be acceptable\n++ to an HTTP/1.0 client, and still be a \"legal\" HTTP/1.1 response.\n\n\nIn that case, it would be easy to put \"HTTP/1.0\" in the response as\nwell, wouldn't it? After all, if you 'downgrade' it to be HTTP/1.0, you\nmight as well label it as HTTP/1.0.\n\nThe problem is that if the server takes out headers that cannot be\nunderstood by the client, but keeps HTTP/1.1 headers which can safely\nbe ignored by the client, but still labels it as HTTP/1.1, the client\ndoesn't know it can ignore those unknown headers.\n\nIn my opinion, the server should not include HTTP/1.1 headers which are\nnot part of HTTP/1.0 when responding to a HTTP/1.0 request, and label\nthe response as being HTTP/1.0.\n\n\nAbigail\n\n\n\n", "id": "lists-010-14295400"}, {"subject": "Re: HTTP response version, agai", "content": "You, Brian Behlendorf, wrote:\n++ \n++ On Fri, 20 Dec 1996, M. Hedlund wrote:\n++ > On Fri, 20 Dec 1996, Dave Kristol wrote:\n++ > > I still consider the question unresolved as to what version an HTTP/1.x\n++ > > server should return for an HTTP/1.0 request.\n++ > [...]\n++ > > Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n++ > > Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n++ > \n++ > I agree with Dave that Case 1 is preferable.  AOL's proxies apparently\n++ > started giving users errors this week when a new version of Apache was\n++ > released, which responded to 1.0 requests with 1.1 responses (Case 2).  \n++ > While this instance will likely be fixed next week, it does indicate how an\n++ > HTTP/1.0 client can be confused by an HTTP/1.1 response.\n++ \n++ No, it indicates how a company with little concern for standards can dictate\n++ implementations in other products through technological inertia.  There's\n++ nothing in the 1.1 response which should cause problems with the 1.0 proxy or\n++ 1.0 client - section 3.1 of both the 1.0 and 1.1 specs promise this, and (as\n++ best this group can tell) 1.1 fulfills this promise.  \n\nBut that wasn't known when HTTP/1.0 was made. It also isn't known\nwhether HTTP/1.2 response headers won't contain anything that causes\nproblems for HTTP/1.1 clients.\n\n++ If it becomes common acceptance that 1.0 and 1.1 are incompatible, then no one\n++ will ever upgrade to 1.1.  This is exactly the perception this wg labored long\n++ and hard to prevent.\n\nI don't follow this logic. Even if HTTP/1.1 and HTTP/1.0 would be\ncompletely imcompatible, as long as HTTP/1.1 servers talk HTTP/1.0 to\nHTTP/1.0 clients, there won't be a problem.\n\n++ The big question is, what will happen first: will AOL fix their proxies, or\n++ will Apache users \"fix\" [hack] their servers?  Client service dictates that\n++ we at Organic hack our servers, but the Apache development group has no such\n++ requirements.  \n\nI think the problem is more fundamental. If we force HTTP/1.0 clients\nto accept HTTP/1.1 reponses, they also have to accept HTTP/1.2,\nHTTP/1.7, etc responses. That of course means no HTTP/1.x header can\never contain something which causes problems with HTTP/1.0 clients.\n\n\n\nAbigail\n\n\n\n", "id": "lists-010-14304383"}, {"subject": "Re: HTTP response version, agai", "content": "At 10:14 AM 12/30/96 -0500, Abigail wrote:\n>You, Brian Behlendorf, wrote:\n>++ \n>++ On Fri, 20 Dec 1996, M. Hedlund wrote:\n>++ > On Fri, 20 Dec 1996, Dave Kristol wrote:\n>++ > > I still consider the question unresolved as to what version an\nHTTP/1.x\n>++ > > server should return for an HTTP/1.0 request.\n>++ > [...]\n>++ > > Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n>++ > > Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n>++ > \n>++ > I agree with Dave that Case 1 is preferable.  AOL's proxies apparently\n>++ > started giving users errors this week when a new version of Apache was\n>++ > released, which responded to 1.0 requests with 1.1 responses (Case 2).  \n>++ > While this instance will likely be fixed next week, it does indicate\nhow an\n>++ > HTTP/1.0 client can be confused by an HTTP/1.1 response.\n>++ \n>++ No, it indicates how a company with little concern for standards can\ndictate\n>++ implementations in other products through technological inertia.  There's\n>++ nothing in the 1.1 response which should cause problems with the 1.0\nproxy or\n>++ 1.0 client - section 3.1 of both the 1.0 and 1.1 specs promise this,\nand (as\n>++ best this group can tell) 1.1 fulfills this promise.  \n>\n>But that wasn't known when HTTP/1.0 was made. It also isn't known\n>whether HTTP/1.2 response headers won't contain anything that causes\n>problems for HTTP/1.1 clients.\n>\n>++ If it becomes common acceptance that 1.0 and 1.1 are incompatible, then\nno one\n>++ will ever upgrade to 1.1.  This is exactly the perception this wg\nlabored long\n>++ and hard to prevent.\n>\n>I don't follow this logic. Even if HTTP/1.1 and HTTP/1.0 would be\n>completely imcompatible, as long as HTTP/1.1 servers talk HTTP/1.0 to\n>HTTP/1.0 clients, there won't be a problem.\n>\n>++ The big question is, what will happen first: will AOL fix their\nproxies, or\n>++ will Apache users \"fix\" [hack] their servers?  Client service dictates\nthat\n>++ we at Organic hack our servers, but the Apache development group has no\nsuch\n>++ requirements.  \n>\n>I think the problem is more fundamental. If we force HTTP/1.0 clients\n>to accept HTTP/1.1 reponses, they also have to accept HTTP/1.2,\n>HTTP/1.7, etc responses. That of course means no HTTP/1.x header can\n>ever contain something which causes problems with HTTP/1.0 clients.\n\nYour conclusion doesn't follow.  An HTTP/1.x server should be aware of the\nversion number of the client that it is speaking to, and refrain from using\nheader information or mechanisms that are not \"understood\" by that client.\nA response from an HTTP/1.1 server can be constructed to be acceptable\nto an HTTP/1.0 client, and still be a \"legal\" HTTP/1.1 response.\n\n\n\n--\nSteve Wingardswingard@spyglass.com\nSpyglass, Inc.\n\n\n\n", "id": "lists-010-14314829"}, {"subject": "Re: HTTP response version, agai", "content": "> \n> You, Brian Behlendorf, wrote:\n> ++ \n> ++ On Fri, 20 Dec 1996, M. Hedlund wrote:\n> ++ > On Fri, 20 Dec 1996, Dave Kristol wrote:\n> ++ > > I still consider the question unresolved as to what version an HTTP/1.x\n> ++ > > server should return for an HTTP/1.0 request.\n> ++ > [...]\n> ++ > > Case 1 (return HTTP/1.0 to HTTP/1.0 request):\n> ++ > > Case 2 (return HTTP/1.1 to HTTP/1.0 request):\n> ++ > \n> ++ > I agree with Dave that Case 1 is preferable.  AOL's proxies apparently\n> ++ > started giving users errors this week when a new version of Apache was\n> ++ > released, which responded to 1.0 requests with 1.1 responses (Case 2).  \n> ++ > While this instance will likely be fixed next week, it does indicate how an\n> ++ > HTTP/1.0 client can be confused by an HTTP/1.1 response.\n> ++ \n> ++ No, it indicates how a company with little concern for standards can dictate\n> ++ implementations in other products through technological inertia.  There's\n> ++ nothing in the 1.1 response which should cause problems with the 1.0 proxy or\n> ++ 1.0 client - section 3.1 of both the 1.0 and 1.1 specs promise this, and (as\n> ++ best this group can tell) 1.1 fulfills this promise.  \n> \n> But that wasn't known when HTTP/1.0 was made. It also isn't known\n> whether HTTP/1.2 response headers won't contain anything that causes\n> problems for HTTP/1.1 clients.\n\nThe theory of the major.minor version numbering scheme is that\nif we do have to release a new version that causes problems for\nHTTP/1.1, we will call it 2.0 not 1.2\n\nBut it looks like all parties don't have the same understanding\nof what's supposed to be going on.\n\n\n\n", "id": "lists-010-14325487"}, {"subject": "Re: HTTP response version, agai", "content": "> In my opinion, the server should not include HTTP/1.1 headers which are\n> not part of HTTP/1.0 when responding to a HTTP/1.0 request, and label\n> the response as being HTTP/1.0.\n> \n> Abigail\n> \nIt's not the response that being labeled by the HTTP/1.1 header, it's\nthe server's capability.  There would be no reason to send an HTTP\nheader if it only had to match the client's request.\n\nbob\n\n\n\n", "id": "lists-010-14334041"}, {"subject": "Re: HTTP response version, agai", "content": "You, Bob Jernigan, wrote:\n++ \n++ > In my opinion, the server should not include HTTP/1.1 headers which are\n++ > not part of HTTP/1.0 when responding to a HTTP/1.0 request, and label\n++ > the response as being HTTP/1.0.\n++ > \n++ > Abigail\n++ > \n++ It's not the response that being labeled by the HTTP/1.1 header, it's\n++ the server's capability.  There would be no reason to send an HTTP\n++ header if it only had to match the client's request.\n\nIt doesn't have to match. A server could respond with an HTTP header\n*less* than the request.\n\n\n\nAbigail\n\n\n\n", "id": "lists-010-14342197"}, {"subject": "Re: HTTP response version, agai", "content": "At 11:27 AM 12/30/96 -0500, Abigail wrote:\n>You, Steve Wingard, wrote:\n>++ \n>++ At 10:14 AM 12/30/96 -0500, Abigail wrote:\n>++ >\n>++ >I think the problem is more fundamental. If we force HTTP/1.0 clients\n>++ >to accept HTTP/1.1 reponses, they also have to accept HTTP/1.2,\n>++ >HTTP/1.7, etc responses. That of course means no HTTP/1.x header can\n>++ >ever contain something which causes problems with HTTP/1.0 clients.\n>++ \n>++ Your conclusion doesn't follow.  An HTTP/1.x server should be aware of the\n>++ version number of the client that it is speaking to, and refrain from\nusing\n>++ header information or mechanisms that are not \"understood\" by that client.\n>++ A response from an HTTP/1.1 server can be constructed to be acceptable\n>++ to an HTTP/1.0 client, and still be a \"legal\" HTTP/1.1 response.\n>\n>\n>In that case, it would be easy to put \"HTTP/1.0\" in the response as\n>well, wouldn't it? After all, if you 'downgrade' it to be HTTP/1.0, you\n>might as well label it as HTTP/1.0.\n\nThat's the whole crux of the argument:   In my opinion, it's NOT an\n\"HTTP/1.0\" response.  It is a legal HTTP/1.1 response that happens\nto be sent to an HTTP/1.0 client.   \n\nThere seems to be a notion behind the arguments to \"label it as 1.0\"\nthat all HTTP/1.1 responses MUST contain header fields that are new\nand specific to HTTP/1.1 and will cause problems to an HTTP/1.0 client;\notherwise it's not \"really\" HTTP/1.1.  I think that notion is incorrect.\nThe spec is very careful to note things that should be done only when\ncommunicating with HTTP/1.1 clients (Persistent connections,\nPipelining, Transfer codings, etc.), but the absence of those\nfeatures in a response to an HTTP/1.0 client does NOT reduce the\n\"1.1-ness\" of the response, IMO.   If there are specific cases where\nthe capabilities of a 1.0 client would require the creation of a\nresponse that is in violation of the 1.1 specification, I would agree\nwith the \"label it as 1.0\" argument.  So far, I have not found any\nof those cases on my own.  If others have found them, I welcome\nlearning about them.\n\n>The problem is that if the server takes out headers that cannot be\n>understood by the client, but keeps HTTP/1.1 headers which can safely\n>be ignored by the client, but still labels it as HTTP/1.1, the client\n>doesn't know it can ignore those unknown headers.\n\nI don't agree.   The 1.0 spec has the same language as 1.1 draft 7 when\nit comes to unrecognized response headers:  \"Unrecognized response\nheader fields are treated as Entity-Header fields\" and \"Unrecognized\nEntity-Header fields should be ignored by the recipient\".    I think\nyou're presuming a level of sophistication on the part of existing 1.0\nclients that does not exist.  And if a client developer IS putting time\nand effort today into making an HTTP/1.0 client aware of responses\nlabeled \"HTTP/1.1\" and assuming things based upon that, I'd suggest\nthat the developer's time would be better served by implementing\nHTTP/1.1...\n\n>In my opinion, the server should not include HTTP/1.1 headers which are\n>not part of HTTP/1.0 when responding to a HTTP/1.0 request, and label\n>the response as being HTTP/1.0.\n\nI agree with the first part of the statement, and disagree with the second.\n(But you knew that already.)\n\n\n\n", "id": "lists-010-14350572"}, {"subject": "Re: 505 HTTP Version Not Supporte", "content": "> \n> But a 0.9 client won't ever see the error status code, so what's the point.\n> (Haven't people pretty much given up on 0.9 doing anything useful in the\n> future?  There's too many features you can't have when you don't have headers.)\n\nI.e., \"if I don't need it, no-one does.\"  But 0.9 is useful and will continue\nto be useful in certain circumstances.  About half my hits are 0.9 but\nthey do return <1% of the data.  It all depends on what you think http\nis useful for.\n\nbob\n\n\n\n", "id": "lists-010-1435957"}, {"subject": "Re: HTTP response version, agai", "content": "At 12:05 PM 12/30/96 -0500, you, Abigail, wrote:\n>Bob Jernigan wrote:\n>++ > In my opinion, the server should not include HTTP/1.1 headers which are\n>++ > not part of HTTP/1.0 when responding to a HTTP/1.0 request, and label\n>++ > the response as being HTTP/1.0.\n>++ It's not the response that being labeled by the HTTP/1.1 header, it's\n>++ the server's capability.  There would be no reason to send an HTTP\n>++ header if it only had to match the client's request.\n>It doesn't have to match. A server could respond with an HTTP header\n>*less* than the request.\n\nPardon my newness,but why should it not be allowed to respond with an HTTP\nheader *greater* than the request?  From a system log point of view, the\nextra information might be nice to know.  If I see that all my users are\nconnecting to HTTP/1.1 sites, then I might think about upgrading the browsers\nthat we use (if I had any users, that is)...\n\nAnd why is the reverse problem not being discussed?\nI guess it must be fairly clear what should happen when a 1.1 client sends\na 1.1 header to a 1.0 server, so why not just follow that procedure for this\nsituation?\n\nBlake.\n\n\n\n", "id": "lists-010-14360990"}, {"subject": "Re: HTTP response version, agai", "content": "On Mon, 30 Dec 1996, Bob Jernigan wrote:\n\n> It's not the response that being labeled by the HTTP/1.1 header, it's\n> the server's capability.  There would be no reason to send an HTTP\n> header if it only had to match the client's request.\n> \n> bob\n> \n\nA lot of people have said this, but I don't see where it is spelled out.\nThis particular interpretation of the version number in the response is\nvery counter-intuitive and so I think calls for some discussion in the\ndocument. Yes, I realize that versions with the same major version number\nare supposexd to be backwardly compatible, but this is an entirely\ndifferent issue.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nto come up with a better one.\n\n\n\n", "id": "lists-010-14369604"}, {"subject": "Re: HTTP response version, agai", "content": "At 09:29 AM 12/30/96 -0800, Gregory Woodhouse wrote:\n>> It's not the response that being labeled by the HTTP/1.1 header, it's\n>> the server's capability.  There would be no reason to send an HTTP\n>> header if it only had to match the client's request.\n>A lot of people have said this, but I don't see where it is spelled out.\n\nWell, in\nhttp://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v11-spec-07.txt\n(which I realize is just a draft, but still)...\n\n  3.1 HTTP Version\n\n  [snipped for brevity...]\n\n  Applications sending Request or Response messages, as defined by this\n  specification, MUST include an HTTP-Version of \"HTTP/1.1\". Use of this\n  version number indicates that the sending application is at least\n  conditionally compliant with this specification.\n\nIt seems to be spelled out pretty clearly...\n\nOf course 3.1 also says\n\n  Since the\n  protocol version indicates the protocol capability of the sender, a\n  proxy/gateway MUST never send a message with a version indicator which\n  is greater than its actual version; if a higher version request is\n  received, the proxy/gateway MUST either downgrade the request version,\n  respond with an error, or switch to tunnel behavior.\n\nSo AOL's behaviour is to spec (respond with an error), although it would\nhave been nicer for them to go with option 1 or 3.\n\n>This particular interpretation of the version number in the response is\n>very counter-intuitive and so I think calls for some discussion in the\n>document. Yes, I realize that versions with the same major version number\n>are supposexd to be backwardly compatible, but this is an entirely\n>different issue.\n\nI agree that it is somewhat counter intuitive, but it also makes sense\nwhen you consider that\n\n  The HTTP version of an application is the highest HTTP version for which\n  the application is at least conditionally compliant.\n\nGiven this, a server should _not_ be returning HTTP/1.0 to a 1.0 request,\nsince that is not the highest version for which it's compliant.\nIt does make life a little harder on browser and proxy writers, though.\n\nJust my thoughts,\nBlake.\n\n\n\n", "id": "lists-010-14378764"}, {"subject": "Re: HTTP response version, agai", "content": "You, Blake Winton, wrote:\n++ \n++ At 12:05 PM 12/30/96 -0500, you, Abigail, wrote:\n++ >Bob Jernigan wrote:\n++ >++ > In my opinion, the server should not include HTTP/1.1 headers which are\n++ >++ > not part of HTTP/1.0 when responding to a HTTP/1.0 request, and label\n++ >++ > the response as being HTTP/1.0.\n++ >++ It's not the response that being labeled by the HTTP/1.1 header, it's\n++ >++ the server's capability.  There would be no reason to send an HTTP\n++ >++ header if it only had to match the client's request.\n++ >It doesn't have to match. A server could respond with an HTTP header\n++ >*less* than the request.\n++ \n++ Pardon my newness,but why should it not be allowed to respond with an HTTP\n++ header *greater* than the request?  From a system log point of view, the\n++ extra information might be nice to know.  If I see that all my users are\n++ connecting to HTTP/1.1 sites, then I might think about upgrading the browsers\n++ that we use (if I had any users, that is)...\n\nThat's very interesting... you log and study all the requests your\nusers do?\n\n++ And why is the reverse problem not being discussed?\n++ I guess it must be fairly clear what should happen when a 1.1 client sends\n++ a 1.1 header to a 1.0 server, so why not just follow that procedure for this\n++ situation?\n\nI don't think you can reverse the situation. A client doesn't know\nwhich HTTP version the server uses; a server does know which version\nthe client uses.\n\n\nAbigail\n\n\n\n", "id": "lists-010-14388486"}, {"subject": "Re: HTTP response version, agai", "content": "Abigail wrote:\n> \n> You, Bob Jernigan, wrote:\n> ++ \n> ++ > In my opinion, the server should not include HTTP/1.1 headers which are\n> ++ > not part of HTTP/1.0 when responding to a HTTP/1.0 request, and label\n> ++ > the response as being HTTP/1.0.\n> ++ > \n> ++ > Abigail\n> ++ > \n> ++ It's not the response that being labeled by the HTTP/1.1 header, it's\n> ++ the server's capability.  There would be no reason to send an HTTP\n> ++ header if it only had to match the client's request.\n> \n> It doesn't have to match. A server could respond with an HTTP header\n> *less* than the request.\n> \n> Abigail\n> \nYes, the server would do that if couldn't serve at the level requested,\nit which case it is advertizing its capability level.  Of course, in\nthis case the browser is burdened with handling the downgraded response,\ni.e., it must not expect the headers that aren't there.\n\nbob\n\n\n\n", "id": "lists-010-14397526"}, {"subject": "Re: HTTP response version, agai", "content": "Abigail wrote:\n> \n> That's very interesting... you log and study all the requests your\n> users do?\n\nQuite often, yes.\n> \n> ++ And why is the reverse problem not being discussed?\n> ++ I guess it must be fairly clear what should happen when a 1.1 client sends\n> ++ a 1.1 header to a 1.0 server, so why not just follow that procedure for this\n> ++ situation?\n> \n> I don't think you can reverse the situation. A client doesn't know\n> which HTTP version the server uses; a server does know which version\n> the client uses.\n> \nNo, the server knows which version was in the request.  Our client\nissues both 0.9 and 1.0 requests, and soon 1.1, as appropriate to\nthe situation.\n> \n> Abigail\n> \nbob\n\n\n\n", "id": "lists-010-14406133"}, {"subject": "Re: HTTP response version, agai", "content": "On Mon, 30 Dec 1996, Blake Winton wrote:\n\n> At 09:29 AM 12/30/96 -0800, Gregory Woodhouse wrote:\n> >> It's not the response that being labeled by the HTTP/1.1 header, it's\n> >> the server's capability.  There would be no reason to send an HTTP\n> >> header if it only had to match the client's request.\n> >A lot of people have said this, but I don't see where it is spelled out.\n> \n> Well, in\n> http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v11-spec-07.txt\n> (which I realize is just a draft, but still)...\n> \n>   3.1 HTTP Version\n> \n>   [snipped for brevity...]\n> \n>   Applications sending Request or Response messages, as defined by this\n>   specification, MUST include an HTTP-Version of \"HTTP/1.1\". Use of this\n>   version number indicates that the sending application is at least\n>   conditionally compliant with this specification.\n> \n> It seems to be spelled out pretty clearly...\n>\n\nI'm not sure I agree with your intepretation. My take on the above is that\n\na) The message (request or response) complies with HTTP/1.1\n\nAND\n\nb) An application is not allowed to claim a version number for a message it\nsends unless it is at least conditionally compliant with that version of\nthe protocol.\n\nIt seems to me tht you are interpreting the above paragraph as if though it\nsaid:\n\nApplications MUST send the highest version number with which they are at\nleast conditionally compliant in each message.\n\nThe difference is tht I see this paragraph as a protective measure to\nprevent applications from claiming to support a version number with which\nthey are not at least conditionally compliant, not a requirement that\napplications advertise the highest version number with which they are\ncompliant.\n \n> Of course 3.1 also says\n> \n>   Since the\n>   protocol version indicates the protocol capability of the sender, a\n>   proxy/gateway MUST never send a message with a version indicator which\n>   is greater than its actual version; if a higher version request is\n>   received, the proxy/gateway MUST either downgrade the request version,\n>   respond with an error, or switch to tunnel behavior.\n> \n\nGood example.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nto come up with a better one.\n\n\n\n", "id": "lists-010-14414283"}, {"subject": "Re: HTTP response version, agai", "content": "At 10:13 AM 12/30/96 -0800, Gregory J. Woodhouse wrote:\n>On Mon, 30 Dec 1996, Blake Winton wrote:\n>> >> It's not the response that being labeled by the HTTP/1.1 header, it's\n>> >> the server's capability.\n>> >A lot of people have said this, but I don't see where it is spelled out.\n>> http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v11-spec-07.txt\n>>   3.1 HTTP Version\n>>   Applications sending Request or Response messages, as defined by this\n>>   specification, MUST include an HTTP-Version of \"HTTP/1.1\". Use of this\n>>   version number indicates that the sending application is at least\n>>   conditionally compliant with this specification.\n>I'm not sure I agree with your intepretation. My take on the above is that\n>a) The message (request or response) complies with HTTP/1.1\n>AND\n>b) An application is not allowed to claim a version number for a message it\n>sends unless it is at least conditionally compliant with that version of\n>the protocol.\n\nIt most definately says that, but I feel that it says something more...\n\n>It seems to me tht you are interpreting the above paragraph as if though it\n>said:\n>\n>Applications MUST send the highest version number with which they are at\n>least conditionally compliant in each message.\n\nWhich would seem to be bourne out in a later paragraph of section 3.1\n\n  The HTTP version of an application is the highest HTTP version for which\n  the application is at least conditionally compliant.\n\n>The difference is tht I see this paragraph as a protective measure to\n>prevent applications from claiming to support a version number with which\n>they are not at least conditionally compliant, not a requirement that\n>applications advertise the highest version number with which they are\n>compliant.\n\nI see your point, and I agree that there is a measure of protectiveness\nabout it, but I believe that it can and should serve both purposes.  Why\nshould we throw away a bit of information which may become useful?\n \n>> Of course 3.1 also says\n>>   Since the\n>>   protocol version indicates the protocol capability of the sender, a\n>>   proxy/gateway MUST never send a message with a version indicator which\n>>   is greater than its actual version; if a higher version request is\n>>   received, the proxy/gateway MUST either downgrade the request version,\n>>   respond with an error, or switch to tunnel behavior.\n>Good example.\n\nThanks.  I'd just like to point out a sentence I missed.\n\n  Since the\n  protocol version indicates the protocol capability of the sender, ...\n\nThis indicates to me that the only proper response a 1.1 compilant server\nshould send back is HTTP/1.1\n\nBlake.\n\nP.S.  I'm reading the mailing list, and it seems to get here before e-mail\n      which is sent directly to me, so I would be as happy not to get two\n      copies of any given message, if the rest of you don't mind.  :)\n\n\n\n", "id": "lists-010-14424216"}, {"subject": "Re: HTTP response version, agai", "content": "On Mon, 30 Dec 1996, Blake Winton wrote:\n\n> >I'm not sure I agree with your intepretation. My take on the above is that\n> >a) The message (request or response) complies with HTTP/1.1\n> >AND\n> >b) An application is not allowed to claim a version number for a message it\n> >sends unless it is at least conditionally compliant with that version of\n> >the protocol.\n> \n> It most definately says that, but I feel that it says something more...\n> \n> >It seems to me tht you are interpreting the above paragraph as if though it\n> >said:\n> >\n> >Applications MUST send the highest version number with which they are at\n> >least conditionally compliant in each message.\n> \n> Which would seem to be bourne out in a later paragraph of section 3.1\n> \n>   The HTTP version of an application is the highest HTTP version for which\n>   the application is at least conditionally compliant.\n>\n\nBut what is being defined here is the protocol version of an application.\nApplications and messages are different things.\n \n> >The difference is tht I see this paragraph as a protective measure to\n> >prevent applications from claiming to support a version number with which\n> >they are not at least conditionally compliant, not a requirement that\n> >applications advertise the highest version number with which they are\n> >compliant.\n> \n> I see your point, and I agree that there is a measure of protectiveness\n> about it, but I believe that it can and should serve both purposes.  Why\n> should we throw away a bit of information which may become useful?\n>  \n> >> Of course 3.1 also says\n> >>   Since the\n> >>   protocol version indicates the protocol capability of the sender, a\n> >>   proxy/gateway MUST never send a message with a version indicator which\n> >>   is greater than its actual version; if a higher version request is\n> >>   received, the proxy/gateway MUST either downgrade the request version,\n> >>   respond with an error, or switch to tunnel behavior.\n> >Good example.\n> \n> Thanks.  I'd just like to point out a sentence I missed.\n> \n>   Since the\n>   protocol version indicates the protocol capability of the sender, ...\n> \n> This indicates to me that the only proper response a 1.1 compilant server\n> should send back is HTTP/1.1\n> \n> Blake.\n> \n\nYes, but this is a bit ambiguous. I interpret it to mean that a sender\nusing a protocl version of 1.x claims conditional compliance with HTTP/1.x.\nIn other words, the version number indicates the capability of the sencder\nin the sense that it guarantees a minimum capability on the part of the\nsender. You seem to interpret it as meaning tht the sender is asserting tht\n1.x is its maximum capability. Both are valid accorcing to the rules of\nEnglish grammar. \n\n\nFor the record, my understanding is that applications should use the\nhighest version number they understand unless there is some reason they\nhave to downgrade (e.g., in a proxy situation), so much of this must sound\nrather academic. \n\nMy point is simply that the version number in the response indicates the\nversion number of the response (not the server). As someone else has\npointed out, it is irrelevant whether a 1.1 response would be legal as a\n1.0 response, it is still a 1.1 responsed and properly labeled as such.\nServers should not have to try to determine the minimum protocol level for\nwhich their responses are valid. \n\n> P.S.  I'm reading the mailing list, and it seems to get here before e-mail\n>       which is sent directly to me, so I would be as happy not to get two\n>       copies of any given message, if the rest of you don't mind.  :)\n> \n> \n\nSorry, I was being lazy. When I reply, your address shows up on the To: \nline and the list on the Cc: line. To reply to just the list I need to\neither cut and paste the address to the To: line or enter it by hand. I'll\ndo this is in the future. \n\nSorry about the multiple typos. I'm using a telnet proxy that can introduce\ndelays of multiple seconds at unpredictable times.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nto come up with a better one.\n\n\n\n", "id": "lists-010-14434856"}, {"subject": "RE: 505 HTTP Version Not Supporte", "content": ">----------\n>From: cshotton@biap.com[SMTP:cshotton@biap.com]\n>Subject: Re: 505 HTTP Version Not Supported\n>\n>\n>>The appropriate response to a 505 for a 1.0 client is the same as for a 500:\n>>total failure, report the entity body to the user.  Following the logic of\n>>your line of arguement, we should just get rid of all 5xx codes and have one\n>>500 code.\n>\n>I'd certainly prefer that to a proliferation of them if that's the only\n>choice. Especially since the 500 series has been misdocumented and/or\n>misimplement since their inception.\n\nThis is a red herring. Adding 505 will not confuse correct (and probably\nmost incorrect) existing clients any more than overloading an existing\ncode into meaninglessness -- they'll display the entity body.\n\nBut making a new error code allows new clients to try and be clever\nabout how they react. (Ditto with getting people to implement to other\n500 series correctly.)\n\nI'm for 505. And meaningful status codes that convey information in a\nway that allows programmatic response when appropriate, in general.\n\nPaul\n\n\n\n", "id": "lists-010-1444402"}, {"subject": "Re: HTTP response version, agai", "content": "> I think the problem is more fundamental. If we force HTTP/1.0 clients\n> to accept HTTP/1.1 reponses, they also have to accept HTTP/1.2,\n> HTTP/1.7, etc responses. That of course means no HTTP/1.x header can\n> ever contain something which causes problems with HTTP/1.0 clients.\n\nThat's correct. That *is* why it's called HTTP/1.x, and not\nHTTP/2.x\n\n-- \nSameer ParekhVoice:   510-986-8770\nPresidentFAX:     510-986-8777\nC2Net     C2Net is having a party: http://www.c2.net/party/\nhttp://www.c2.net/sameer@c2.net\n\n\n\n", "id": "lists-010-14446030"}, {"subject": "Call for Closure   HTTP response versio", "content": "On Mon, 30 Dec 1996, sameer wrote:\n\n> > HTTP/1.7, etc responses. That of course means no HTTP/1.x header can\n> > ever contain something which causes problems with HTTP/1.0 clients.\n> That's correct. That *is* why it's called HTTP/1.x, and not\n> HTTP/2.x\n\nThis is indeed the design goal; if there are any situations which \nviolate this constraint in such a way as to return incorrect results \nwithout signalling an error,  the specification is in error, and must be \ncorrected before being advanced.\n\nIf there are such cases, then there needs to be some emergency repairs; \nif there are no such cases, the following is always safe, and will always \nuse 1.1 when available:\n\n1) clients which support HTTP/1.1 SHOULD  send 1.1 requests\n\n2) servers should echo the lesser of the request version and the \n   supported protocol version.\n\nOtherwise, I call for a coin flip. \n\nSimon\n\n\n\n", "id": "lists-010-14454965"}, {"subject": "Problems with ..-state-mgmt05.tx", "content": "draft-ietf-http-state-mgmt-05.txt (HTTP State Management Mechanism)\nhas been a Proposed Standard for about a month now.  This message\ndiscusses two problems with draft-ietf-http-state-mgmt-05.txt I have\nseen so far.  I feel that these problems need to be resolved in some\nway as soon as possible.\n\nThese problems may or may not have an impact on the standards track\nprogress of the draft.  I will not discuss that issue in this message,\nas others know more about such procedural matters.\n\nProblem 1. Advice about sending Cache-Control: must-revalidate\n==========\n\n(Problem first identified by Anselm Baird-Smith)\n\nSection 4.2.3 (Controlling Caching) says:\n\n|The origin server should send the following additional HTTP/1.1 response\n|headers, depending on circumstances:\n[...]\n|   * To allow caching of a document and require that it be validated\n|     before returning it to the client: Cache-control: must-revalidate.\n                                                        ^^^^^^^^^^^^^^^^ \n\nWe recently found out that `must-revalidate' in HTTP/1.1 means `must\nrevalidate if stale', not `must always revalidate'.  This  means that\nthe advice above is wrong, it should be:\n\n   * To allow caching of a document and require that it be validated\n     before returning it to the client: \n     Cache-control: max-age=0, must-revalidate.\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nEssentially the same problem occurs in the next item of the spec text:\n\n|   * To allow caching of a document, but to require that proxy caches\n|     (not user agent caches) validate it before returning it to the\n|     client: Cache-control: proxy-revalidate.\n\nand the same fix should be applied.\n\n\nProblem 2. Downwards compatibility with existing cookie implementations\n==========\n\n(problem first identified by Martijn Koster)\n\nIf the following header (without the line breaks) is sent to Microsoft\nInternet Explorer V3:\n\nSet-cookie: xx=\"1=2&3-4\";\n    Comment=\"blah\";\n    Version=1; Max-Age=15552000; Path=/;\n    Expires=Sun, 27 Apr 1997 01:16:23 GMT\n\nthen MSIE v3 will return the cookie\n\nCookie: Max-Age=15552000\n\nand not xx=\"1=2&3-4\", which would be the most reasonable thing to do\naccording to Netscape's cookie specification.  (By the way, Microsoft\nwill probably fix this problem in MSIE v4.)\n\nThe downwards compatibility scheme in the state management draft,\nhowever, _relies_ on all existing browsers sending back xx=\"1=2&3-4\"\nwhen getting the Set-Cookie header above.  It therefore seems that\ndownwards compatibility, and smooth deployment, is impossible for the\ncurrent specification.\n\nThe state management draft assumes that existing clients are tolerant\nwhen parsing Set-Cookie headers.  As is shown by MSIE v3, this\nassumption is wrong.  A downwards compatibility scheme, which in my\nopinion is crucially important if this specification is to be deployed\nat all, will therefore have to rely on some other assumption.  \n\nI propose using the assumption that unknown response headers are\nignored by all clients.  This leads to the following fix: the state\nmanagement specification defines a _new_ header for setting cookies,\nfor example \"Cookie-Control\", together with a rule that if a\nCookie-Control header is present in the response message, a compliant\nclient must ignore all Set-Cookie headers.  This would allow downwards\ncompatibility by sending two headers:\n\nCookie-Control: xx=\"1=2&3-4\";\n    Comment=\"blah\";\n    Version=1; Max-Age=15552000; Path=/;\nSet-Cookie: xx=\"1=2&3-4\"; Path=/; Expires=Sun, 27 Apr 1997 01:16:23 GMT\n\nThe \"Version=1\" directive in the Cookie-Control header above could\npossibly also be removed.\n\n\nKoen.\n\n\n\n", "id": "lists-010-14465543"}, {"subject": "Re: Call for Closure   HTTP response versio", "content": "Simon Spero <ses@tipper.oit.unc.edu> wrote\n  > [regarding HTTP/1.x headers that break HTTP/1.0 clients]\n\n  > If there are such cases, then there needs to be some emergency repairs; \n  > if there are no such cases, the following is always safe, and will always \n  > use 1.1 when available:\n  > \n  > 1) clients which support HTTP/1.1 SHOULD  send 1.1 requests\n  > \n  > 2) servers should echo the lesser of the request version and the \n  >    supported protocol version.\n  >\n  > Otherwise, I call for a coin flip. \n\nI agree with Simon's proposal.\n\nLet me make some assumptions.  They may be controversial, but I haven't\nseen substantial contradictory evidence:\n\n1) The HTTP/1.1 draft is clear about which HTTP/1.1 headers cannot be\nsent to HTTP/1.0 clients.\n\n2) If an HTTP/1.1 server sends a response labeled as HTTP/1.1, but with\nonly HTTP/1.0-compatible headers, HTTP/1.0 clients will understand it.\n(There are a few known exceptions.)\n\n3) Based on their good-faith readings of the HTTP/1.1 spec., people in\nthe HTTP community are divided about what response version an HTTP/1.1\nserver should send in response to an HTTP/1.0 request.  This difference\nalone demands clearer wording in the specification.\n\nSince no one has described a \"show-stopper\" scenario, I stipulate that\nthe choice of response version is not a threat to interoperation.  We\nare left then with a choice based on protocol aesthetics or taste.\n\nI have stated several times my preference:  responses must be labeled\nwith the same version as the request.\n\nProponents of the other view assert that my approach will slow\ndeployment of HTTP/1.1.  I don't see why.  Henryk and I have both asked\nfor a direct response to the question, Why can't clients just send\nHTTP/1.1 requests when they're HTTP/1.1 capable?  An HTTP/1.1 server\nwill respond in kind, and both will reap the benefits of the newer\nprotocol.  An HTTP/1.0 server will respond as HTTP/1.0, and both will\nuse that.  Having a server send an HTTP/1.1 response to a user agent or\nproxy that isn't HTTP/1.1 capable does nothing that I can see to speed\nHTTP/1.1 deployment or interoperation.\n\nAs the one who has launched this discussion (twice), I'm open to\ncompelling *technical* arguments for the other side.  If there aren't\nany, I'd like someone with the opposing view to admit we're arguing\ntaste (\"religion\", if you like).  Then we can stop going around in\ncircles and flip a coin, as Simon suggests.\n\nDave Kristol\n\n\n\n", "id": "lists-010-14475957"}, {"subject": "Re: Call for Closure   HTTP response versio", "content": "Simon Spero: \n> On Mon, 30 Dec 1996, sameer wrote:\n> \n> > > HTTP/1.7, etc responses. That of course means no HTTP/1.x header can\n> > > ever contain something which causes problems with HTTP/1.0 clients.\n> > That's correct. That *is* why it's called HTTP/1.x, and not\n> > HTTP/2.x\n> \n> This is indeed the design goal; if there are any situations which \n> violate this constraint in such a way as to return incorrect results \n> without signalling an error,  the specification is in error, and must be \n> corrected before being advanced.\n> \n> If there are such cases, then there needs to be some emergency repairs; \n> if there are no such cases, the following is always safe, and will always \n> use 1.1 when available:\n> \n> 1) clients which support HTTP/1.1 SHOULD  send 1.1 requests\n> \n> 2) servers should echo the lesser of the request version and the \n>    supported protocol version.\n> \n> Otherwise, I call for a coin flip. \nIf the client doesn't unterstands some parts of the response, that doesn't\nmean, that others (proxies) doesn't understand them.\n(Think of expires/max-age interference in the spec. 1.0 proxies shouldn't\ncache pre-expired responses while 1.1 proxies can cache them according to the\nmax-age.)\nI my interpretation\n1) true\n2) completely wrong. Response headers not understood by older (currently 1.0)\nclients/proxies will be treated as entity headers and ignored/passed untouched.\nThe spec is right, we can add some additional clarifications, if the WG thinks\nAOL proxy implementors misinterpreted the spec properly - I mean it isn't\nenough clear. (For me it is enough clean. Maybe because I read proxy related\nparts with more attention and neglected some other parts.)\n\nI think the advertising function of the native server version in resposes will be\nliked by the browser implementors: another possible cause to say 'please upgrade\nto the latest version' (I mean when the AVERAGE difference between the server and\nclient version is  higher than some threshold.) and will somewhat promote technical\nprogress (replacing historycal software versions).\n\nAndrew. (Endre \"Balint\" Nagy) <bne@CareNet.hu>\n\n\n\n", "id": "lists-010-14486151"}, {"subject": "Re: Call for Closure   HTTP response versio", "content": "On Tue, 31 Dec 1996, Simon Spero wrote:\n\n> On Mon, 30 Dec 1996, sameer wrote:\n> \n> > > HTTP/1.7, etc responses. That of course means no HTTP/1.x header can\n> > > ever contain something which causes problems with HTTP/1.0 clients.\n> > That's correct. That *is* why it's called HTTP/1.x, and not\n> > HTTP/2.x\n> \n> This is indeed the design goal; if there are any situations which \n> violate this constraint in such a way as to return incorrect results \n> without signalling an error,  the specification is in error, and must be \n> corrected before being advanced.\n>\n\nAnd I don't think anyone is questioning that this behavior is correct.\n \n> If there are such cases, then there needs to be some emergency repairs; \n> if there are no such cases, the following is always safe, and will always \n> use 1.1 when available:\n> \n> 1) clients which support HTTP/1.1 SHOULD  send 1.1 requests\n> \nAgreed.\n\n> 2) servers should echo the lesser of the request version and the \n>    supported protocol version.\n> \n> Otherwise, I call for a coin flip. \n> \n> Simon\n> \nI'm a little unsure about this one, but I wouldn't object.\n\n\nAnyway, my only point has been that the document is ambiguous about the\nfunction of the version number in the response. I also have my doubts as to\nwhether it is good design to use this version number to advertize the\nservers capabilities--and as I've indicated, this was not my reading of the\nspec, but I do admit this is one possible interpretation.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nto come up with a better one.\n\n\n\n", "id": "lists-010-14497129"}, {"subject": "Re: CacheControl directive, semantic", "content": "Thanks for going through the effort to put together those two\ntables.  I have to admit that I have not yet had the time to\ngo through them carefully to see if our understandings mesh,\npartly because you've had to compromise between readability\nand fitting onto a standard page width.\n\nI think it might be possible to make the individual tables\nmore readable by factoring them into a larger number of simpler\ntables.\n\nFor example, your first table, which you call the \"fresh\" function,\nreally combines two factors.  The first is what I would prefer\nto call the \"fresh function\", which determines whether a response\nis \"fresh\" independent of the request parameters.  This would\ntake only these parameters as input:\n\n    Ap: The max age value of the reply (expressed either through the\nmax-age directive, or the expires header)\n[or infinity if unspecified]\n    \n    Ae: The current entity age, computed as defined in section 13.2.3\n\nand yield a boolean as output.  This corresponds precisely to the\ndefinition of \"fresh\" in section 1.3 of the HTTP/1.1 spec, which\nis\nAe <= Ap\n\nOne could then define a \"sufficiently fresh\" function, which\ndoes depend on request parameters, by a transformation of the\ninput parameters to this table.  I think this is right:\n\n    Ar: The max-age value specified in the request\n[or infinity if unspecified]\n    Sr: The max-stale value specified in the request\n[or zero if unspecified]\n    Fr: The min-fresh value specified in the request\n[or zero if unspecified]\n\n    Ap' = min(Ap, Ar)\n    Ae' = Ae + Fr - Sr\n\nSo the \"sufficiently fresh\" function would then simply be\n\n    Ae' <= Ap'\n\nPutting it all together, the \"sufficiently fresh\" function\nwould be\n\n    SuffFresh(Ae, Ap, Ar, Sr, Fr) := min(Ap, Ar) < (Ae + Fr - Sr)\n\nIf I have done the algebra correctly.\n\nRegarding your second table, which indicates whether a cache\nentry is usable with or without revalidation, I think we first\nneed to nail down the semantics of \"proxy-revalidate\".\n\nAs you point out, this language in 14.8:\n\n  1. If the response includes the \"proxy-revalidate\" Cache-Control\n     directive, the cache MAY use that response in replying to a\n     subsequent request, but a proxy cache MUST first revalidate it with\n     the origin server, using the request-headers from the new request\n     to allow the origin server to authenticate the new request.\n\neither contradicts or modifies this language in 14.9.4:\n\n    When the must-revalidate directive is present in a response\n    received by a cache, that cache MUST NOT use the entry after it\n    becomes stale to respond to a subsequent request without first\n    revalidating it with the origin server. [...] The proxy-revalidate\n    directive has the same meaning as the must- revalidate directive,\n    except that it does not apply to non-shared user agent caches.\n    \nI think we've agreed that this is a bug in the specification.  That\nis, we did not intend that the meaning of this cache-control directive\nshould change when another response header is present.  As you say,\nthe other interpretation would require \"tricky navigation between\nsections 13 and 14.8 and 14.9\", and that seems like a bad idea.\n\nBoth you and Koen have also recently pointed out that the\nstate-management draft runs into the same confusion over\nproxy-revalidate.   Clear evidence that we need to fix this!\n\n-Jeff\n\n\n\n", "id": "lists-010-14506834"}, {"subject": "question on 14.9.4, nocache directive in request", "content": "Anselm asks\n    - [a question] 14.9.4 says under \"End to end reload\" that \"... No\n      field names may be included with the no-cache directive in a\n      request\". I assume an \"In that case, \" is implicitly assumed at the\n      beginning of the sentence ? [I can see good reasons for using\n      no-cache with field names in a request, as does]\n\nThe complete text of this paragraph is:\n    End-to-end reload\n      The request includes a \"no-cache\" Cache-Control directive or, for\n      compatibility with HTTP/1.0 clients, \"Pragma: no-cache\". No field\n      names may be included with the no-cache directive in a request. The\n      server MUST NOT use a cached copy when responding to such a request.\n\nActually, our intention (as far as I understand it) for this\nrequest directive was that it totally prevents the use of a cached\nresponse in reply to the request.  I.e., we did not intend that\nthere be an \"In that case\" at the beginning of the second sentence.\nI'm not sure I understand what it would mean to specify a field name\nin the request.  E.g., what does this mean:\n\nGET /foo.html HTTP/1.1\nCache-control: no-cache=\"Expires\"\n\nIf you have an example that does make sense, that would be helpful.\n\nIn a *response* message, we allowe the no-cache directive to carry one\nor more field names, e.g.,\n\nHTTP/1.1 200 OK\nServer: CERN/3.0 libwww/2.17\nCache-control: no-cache=\"Server\"\n\nwould mean (according to the somewhat sketchy language in\nsection 14.9.1) that an HTTP/1.1 client or proxy could cache\nmost of the response, but could not cache the \"Server\" response header.\n\nI believe that this was intended as a way for certain applications\nto allow caching while preventing the storage of certain response\nfields that might have privacy implications, although I'm not\nsure I can come up with a good example.  (However, I would\nexpect that the \"private\" response directive would serve that\npurpose well enough.)\n\n-Jeff\n\n\n\n", "id": "lists-010-14517502"}, {"subject": "Re: Call for Closure   HTTP response versio", "content": "Dave Kristol:\n>\n>Let me make some assumptions.  They may be controversial, but I haven't\n>seen substantial contradictory evidence:\n>\n>1) The HTTP/1.1 draft is clear about which HTTP/1.1 headers cannot be\n>sent to HTTP/1.0 clients.\n>\n>2) If an HTTP/1.1 server sends a response labeled as HTTP/1.1, but with\n>only HTTP/1.0-compatible headers, HTTP/1.0 clients will understand\n>it.\n\nUgh.  I don't know what twist in this thread caused you to make those\nassumptions, but they paint a completely wrong picture of the actual\nsituation.\n\n*ALL* headers are compatible, in a sense, with HTTP/1.0 clients,\nbecause HTTP/1.x clients ignore all headers they do not understand.\nTherefore, a 1.1 server can send as many HTTP/1.1 headers it wants to\na 1.0 client, as long as it also includes enough 1.0 headers to let\nthe client interpret the response correctly.\n\nNow, it is true that some HTTP/1.1 protocol *mechanisms*, like chunked\nencoding and 1xx responses, cannot be used with HTTP/1.0 clients.  But\nmere headers can never break a 1.0 client.\n\nExample of a response which is compatible with both HTTP/1.0 and\nHTTP/1.1 clients:\n\n     HTTP/1.1 200 OK\n     Date: Tue, 11 Jun 1996 20:05:31 GMT      <- 1.0 header\n     Content-Type: text/html                  <- 1.0 header\n     Last-Modified: Mon, 10 Jun 1996 10:01:14 GMT <- 1.0 header\n     Content-Length: 5327                     <- 1.0 header\n     Cache-control: max-age=604800            <- 1.1 header\n     Content-Location: paper.html.en          <- 1.1 header\n     Alternates: {\"paper.html.en\" 0.9 {type text/html} {language en}},\n                 {\"paper.html.fr\" 0.7 {type text/html} {language fr}},\n                 {\"paper.ps.en\"   1.0 {type application/postscript}\n                     {language en}}  <- not 1.0, not 1.1, but TCN header\n     Etag: \"gonkyyyy;1234\"                    <- 1.1 header\n     Vary: negotiate, accept, accept-language <- 1.1 header\n     Expires: Thu, 01 Jan 1980 00:00:00 GMT   <- 1.0 header\n\nThe HTTP/1.0 specification tells you which headers you must include to\nlet a HTTP/1.0 client correctly interpret your response.\n\nThe HTTP/1.1 specification tells you which headers you must include to\nlet a HTTP/1.1 client correctly interpret your response.\n\nIf a HTTP/1.0 client gets a response with a HTTP/1.1 version number,\nit can safely discard all headers it does not understand and interpret\nthe rest as HTTP/1.0.  If the end result is not what the web site\nauthor intended, it is always the fault of the server, and never the\nfault of the client.\n\n[...]\n>Since no one has described a \"show-stopper\" scenario, I stipulate that\n>the choice of response version is not a threat to interoperation. \n\nCorrect.\n\n> We\n>are left then with a choice based on protocol aesthetics or taste.\n\nCorrect.  Also, the 1.1 draft leaves this choice to the taste of the\nauthor of the server.  I see no reason to change the draft to\nprescribe one particular choice.  Though I completely agree with the\nsentiment that the draft needs more text explaining this issue.\n\n>Dave Kristol\n\nKoen.\n\n\n\n", "id": "lists-010-14526558"}, {"subject": "Re: Call for Closure  HTTP response versio", "content": "On Tue, 31 Dec 1996, Dave Kristol wrote:\n\n> 1) The HTTP/1.1 draft is clear about which HTTP/1.1 headers cannot be\n> sent to HTTP/1.0 clients.\n> \n> 2) If an HTTP/1.1 server sends a response labeled as HTTP/1.1, but with\n> only HTTP/1.0-compatible headers, HTTP/1.0 clients will understand it.\n> (There are a few known exceptions.)\n\nThe way GNU E-scape is written right now, it will send an HTTP/1.0 request.\nIf the first line it gets back contains HTTP/1.0, it treats the response\nas an HTTP/1.0 response.  Otherwise, it assumes that it's talking to\nan HTTP/0.9 server, and retries the request as HTTP/0.9\n\nI will change it to react correctly to HTTP/1.x responses if that's\nwhat is decided is the correct answer.  I haven't even made a prerelease\navailable to others yet, so there's plenty of time for the decission\nto be made.\n\n> I have stated several times my preference:  responses must be labeled\n> with the same version as the request.\n> \n> Proponents of the other view assert that my approach will slow\n> deployment of HTTP/1.1.  I don't see why.  Henryk and I have both asked\n> for a direct response to the question, Why can't clients just send\n> HTTP/1.1 requests when they're HTTP/1.1 capable?  An HTTP/1.1 server\n> will respond in kind, and both will reap the benefits of the newer\n> protocol.  An HTTP/1.0 server will respond as HTTP/1.0, and both will\n> use that.  Having a server send an HTTP/1.1 response to a user agent or\n> proxy that isn't HTTP/1.1 capable does nothing that I can see to speed\n> HTTP/1.1 deployment or interoperation.\n\nSending HTTP/1.1 responses to HTTP/1.0 requests might speed deployment,\nbecause it might hurt interoperation, and it will be nessisary to\nupgrade browsers to keep them interoperable.\n\nThere's no reason I see that it's nessisary to send HTTP/1.1 headers\nin response to HTTP/1.0 requests.\n\n\nnemo\n                                                 http://www.cyclic.com/~nemo\n<nemo@koa.iolani.honolulu.hi.us>                    <devnull@gnu.ai.mit.edu>\n----------------------------------------------------------------------------\n\"...For I have not come to call the righteous, but sinners.\"  -- Mathew 9:13\n\n\n\n", "id": "lists-010-14537718"}, {"subject": "Re: Call for Closure  HTTP response versio", "content": "  > From dmk Tue Dec 31 15:53:12 1996\n  > Resent-Date: Tue, 31 Dec 1996 20:48:33 GMT\n  > Date: Tue, 31 Dec 1996 10:45:13 -1000 (HST)\n\"Joel N. Weber II\" <nemo@koa.iolani.honolulu.hi.us> writes:\n  > On Tue, 31 Dec 1996, Dave Kristol wrote:\n  > \n  > > 1) The HTTP/1.1 draft is clear about which HTTP/1.1 headers cannot be\n  > > sent to HTTP/1.0 clients.\n  > > \n  > > 2) If an HTTP/1.1 server sends a response labeled as HTTP/1.1, but with\n  > > only HTTP/1.0-compatible headers, HTTP/1.0 clients will understand it.\n  > > (There are a few known exceptions.)\n  > \n  > The way GNU E-scape is written right now, it will send an HTTP/1.0 request.\n  > If the first line it gets back contains HTTP/1.0, it treats the response\n  > as an HTTP/1.0 response.  Otherwise, it assumes that it's talking to\n  > an HTTP/0.9 server, and retries the request as HTTP/0.9\n  > \n  > I will change it to react correctly to HTTP/1.x responses if that's\n  > what is decided is the correct answer.  I haven't even made a prerelease\n  > available to others yet, so there's plenty of time for the decission\n  > to be made.\n\nThe decision your code makes should not be between \"HTTP/1.0\" and\nother, but between \"HTTP/1.*\" and other.  Regardless of the choice\narrived at by the working group concerning version number, what you\nhave now is wrong.\n\nDave Kristol\n\n\n\n", "id": "lists-010-14549157"}, {"subject": "Re: 505 HTTP Version Not Supporte", "content": "At 01:42 PM 4/3/96 -0500, Bob Jernigan wrote:\n>I.e., \"if I don't need it, no-one does.\"\n\nI don't think it's a matter of me shrugging off the little people.  We've\nbeen moving a long time towards increased HTTP complexity.  We're now at the\npoint where to be compliant with the latest HTTP there are headers you are\n*required* to send.  The issue of how  virtual servers are supposed to\nhandle 0.9 requests can be a sticky one (as long as 1.0 and 0.9 are around\nwill vendors continue to suck up multiple IP addresses for one machine?).\n\nIMO, this pseudo-backward compatibilty can only remain feasible for so long.\nI just think the world (wide web) would be a better place if no one made 0.9\nrequests anymore.  And I'll probably feel the same way about 1.0 requests\nsoon enough.\n\n>But 0.9 is useful and will continue\n>to be useful in certain circumstances.  About half my hits are 0.9 but\n>they do return <1% of the data.  It all depends on what you think http\n>is useful for.\n\n-----\nthe Programmer formerly known as Dan          \n                                     http://www.spyglass.com/~ddubois/\n                                     New direct dial phone: 708-245-6577\n\n\n\n", "id": "lists-010-1455344"}, {"subject": "Re: Call for Closure  HTTP response versio", "content": "\"Joel N. Weber II\" <nemo@koa.iolani.honolulu.hi.us> wrote:\n>On Tue, 31 Dec 1996, Dave Kristol wrote:\n>\n>> 1) The HTTP/1.1 draft is clear about which HTTP/1.1 headers cannot be\n>> sent to HTTP/1.0 clients.\n>> \n>> 2) If an HTTP/1.1 server sends a response labeled as HTTP/1.1, but with\n>> only HTTP/1.0-compatible headers, HTTP/1.0 clients will understand it.\n>> (There are a few known exceptions.)\n>\n>The way GNU E-scape is written right now, it will send an HTTP/1.0 request.\n>If the first line it gets back contains HTTP/1.0, it treats the response\n>as an HTTP/1.0 response.  Otherwise, it assumes that it's talking to\n>an HTTP/0.9 server, and retries the request as HTTP/0.9\n>\n>I will change it to react correctly to HTTP/1.x responses if that's\n>what is decided is the correct answer.  I haven't even made a prerelease\n>available to others yet, so there's plenty of time for the decission\n>to be made.\n\nBut that was decided years ago when HTTP/1.0 was first born.\nClients should be testing for \"HTTP/1.\", not \"HTTP/1.0\", and until\nsomeday there's a 2.x and it's an issue, retrying as HTTP/0.9.\n\n\n>> I have stated several times my preference:  responses must be labeled\n>> with the same version as the request.\n>> \n>> Proponents of the other view assert that my approach will slow\n>> deployment of HTTP/1.1.  I don't see why.  Henryk and I have both asked\n>> for a direct response to the question, Why can't clients just send\n>> HTTP/1.1 requests when they're HTTP/1.1 capable?  An HTTP/1.1 server\n>> will respond in kind, and both will reap the benefits of the newer\n>> protocol.  An HTTP/1.0 server will respond as HTTP/1.0, and both will\n>> use that.  Having a server send an HTTP/1.1 response to a user agent or\n>> proxy that isn't HTTP/1.1 capable does nothing that I can see to speed\n>> HTTP/1.1 deployment or interoperation.\n>\n>Sending HTTP/1.1 responses to HTTP/1.0 requests might speed deployment,\n>because it might hurt interoperation, and it will be nessisary to\n>upgrade browsers to keep them interoperable.\n>\n>There's no reason I see that it's nessisary to send HTTP/1.1 headers\n>in response to HTTP/1.0 requests.\n\nClients may be (in fact, are) implementing HTTP/1.1 incrementally,\nand thus may support some HTTP/1.1 features, but still be declaring\nthemselves as HTTP/1.0.  It most certainly is helpful for their\ndevelopment to receive the HTTP/1.1 headers.  Such clients also may be\nsending some of the HTTP/1.1 headers, if they're relevant to the\nfeatures they handle, and don't result in things they can't yet handle.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-14558226"}, {"subject": "Re: Call for Closure   HTTP response versio", "content": "On Tue, 31 Dec 1996, Koen Holtman wrote:\n\n> Dave Kristol:\n> >\n> >Let me make some assumptions.  They may be controversial, but I haven't\n> >seen substantial contradictory evidence:\n> >\n> >1) The HTTP/1.1 draft is clear about which HTTP/1.1 headers cannot be\n> >sent to HTTP/1.0 clients.\n> >\n> >2) If an HTTP/1.1 server sends a response labeled as HTTP/1.1, but with\n> >only HTTP/1.0-compatible headers, HTTP/1.0 clients will understand\n> >it.\n> \n> Ugh.  I don't know what twist in this thread caused you to make those\n> assumptions, but they paint a completely wrong picture of the actual\n> situation.\n\nNo, I don't think that's wrong. Dave's point 2) refers more to the\n\"HTTP/1.1\" label than the headers, and point 1) is correct. There are\nsome headers that don't work with HTTP/1.0. I guess they *could* be\nsent... but page 23 does say \"A server MUST NOT send transfer-codings\nto an HTTP/1.0 client,\" for example.\n\nNow, the second point is not 100% true, as there are some clients that\ndo not function this way (AOL's recent proxy mishap is a good\nexample). If there were many clients that were misfunctional in this\nmanner, it would make sense for a server to send a response in the\nversion of the request, since the server's user would probably want it\nto work.\n\nOn the flip side, it is not true either that 100% of HTTP/1.0 servers\nwill respond correctly to a HTTP/1.1 request. Again, this number is\nvery small. If it were not, it would make sense for a HTTP/1.1 client\nto send HTTP/1.0 requests, and switch to HTTP/1.1 when it finds a\ncompatible server. In this scenario, it would be neccessary for\nHTTP/1.1 servers to always use HTTP/1.1 in the response.\n\nIn a situation where there were many (say 10% or more) \"broken\"\n(though they aren't really) clients and many broken servers, this\nwould be a real problem - we'd probably have to invent a header field\nthat says \"I understand HTTP/1.1, but can't label the response as such\nbecause otherwise I'd block out a whole lot of requests/responses.\"\n\nLuckily, the current situation features few \"broken\" clients and few\nbroken servers. So we can (and possibly will) have all\nHTTP/1.1-compliant programs always send HTTP/1.1, both clients and\nservers.\n\nThe problem, as I see it, with not making a decision on this issue is\nthis: if a server decides to respond to HTTP/1.0 requests with\nHTTP/1.0, and HTTP/1.1 requests with HTTP/1.1, and a browser decides\nto issue HTTP/1.0 requests first, then use HTTP/1.1 if the server\nsupports it, on the assumption that the server will always return\nHTTP/1.1 if it is compliant, these two devices will end up using\nHTTP/1.0, even though they both support HTTP/1.1.\n\nNow, we could certainly simply reccomend that all HTTP/1.1-compliant\nbrowsers/proxies always send requests with HTTP/1.1, and servers can\neither send HTTP/1.0 to HTTP/1.1 responses or HTTP/1.1 to HTTP/1.1\nresponses. This would seem to be the nearest we've come to a\nconsensus.\n\n-- \n________________________________________________________________________\nAlexei Kosut <akosut@nueva.pvt.k12.ca.us>      The Apache HTTP Server\nURL: http://www.nueva.pvt.k12.ca.us/~akosut/   http://www.apache.org/\n\n\n\n", "id": "lists-010-14568819"}, {"subject": "RE: 505 HTTP Version Not Supporte", "content": ">----------\n>From: Daniel DuBois[SMTP:ddubois@spyglass.com]\n>Subject: Re: 505 HTTP Version Not Supported\n>\n>At 01:42 PM 4/3/96 -0500, Bob Jernigan wrote:\n>>I.e., \"if I don't need it, no-one does.\"\n>\n>I don't think it's a matter of me shrugging off the little people. \n>We've\n>been moving a long time towards increased HTTP complexity.  We're now\n>at the\n>point where to be compliant with the latest HTTP there are headers you\n>are\n>*required* to send.  The issue of how  virtual servers are supposed to\n>handle 0.9 requests can be a sticky one (as long as 1.0 and 0.9 are\n>around\n>will vendors continue to suck up multiple IP addresses for one\n>machine?).\n>\n>IMO, this pseudo-backward compatibilty can only remain feasible for so\n>long.\n>I just think the world (wide web) would be a better place if no one\n>made 0.9\n>requests anymore.  And I'll probably feel the same way about 1.0\n>requests\n>soon enough.\n>\n>>But 0.9 is useful and will continue\n>>to be useful in certain circumstances.  About half my hits are 0.9 but\n>>they do return <1% of the data.  It all depends on what you think http\n>>is useful for.\n\n<soapbox>\nI think Dan is being too kind. My reading of how people in charge of big\nchunks of the Interent backbone feel is that if we don't expunge 0.9\n_and_ 1.0 ASAP then the whole net will collapse. So, while it may be\nconvenient to not have to change, and the old way may deliver value to\nthose using it, they are creating disvalue for everyone else.\n\nThe minimum pieces of 1.1 that have to be implemented by clients and\nservers to \"save the Internet\" aren't that hard: send Host:, and use\npersistent connections. (Proxy caches have additional requirements.) The\npeople who have \"20 line HTTP clients and 200 line HTTP servers\" in PERL\nmight see their code grow by 20 lines in order to do this. Real clients\nand servers might add 100-1000 lines (mostly for persistent\nconnections).  In return, the Internet will be there for us all to use.\n</soapbox>\n\nPaul\n\n\n\n", "id": "lists-010-1463465"}, {"subject": "REWRITE: updated consensus wordin", "content": "Due to comments from the WG, here is an update. The changes are as\nfollows:\n\n1. This isn't really a replacement for all of 5.1.2; Jim Gettys already\nposted another change for \nissues FULLURL and HOST that apply to this section. The real change is\nthe addition of the last \nparagraph; the rest of 5.1.2 is just here to give context for the\nchanges. (I've attempted to incorporate \nhis changes for your reading pleasure, but Jim's placement of the text\nhe posted may differ from mine.)\n\n2. The sentence that read\n\n\"(Proxies may canonicalize the Request-URI, according to the\ncanonicalization rules in section 3.2.2, for internal processing\npuposes, e.g., for comparison of cache keys when doing cache lookups or\nupdates, but should not use this form in forwarded requests.)\"\n\nhas been changed to\n\n>\"(Proxies may transform the Request-URI for internal processing\n>puposes, but should not send such a transformed Request-URI  in\nforwarded requests. Transformations for use in cache updates and lookups\n>   are subject to additional requirements; see section TBD on caching.)\"\n\nin order to make it clear that what this section says is decoupled from\ncache processing rules.\n>\n>This issue has been discussed on the list, so it is believed that this\n>represents the consensus on this issue. If you disagree, please let me\n>know; otherwise we will close this issue.\n>-------------------------\n>5.1.2 Request-URI\n>\n>   The Request-URI is a Uniform Resource Identifier (Section 3.2) and \n>   identifies the resource upon which to apply the request.\n>\n>       Request-URI    = \"*\" | absoluteURI | abs_path\n>\n>   The three options for Request-URI are dependent on the nature of \n>   the request. The asterisk \"*\" means that the request does not apply \n>   to a particular resource, but to the server itself, and is only \n>   allowed when the Method used does not necessarily apply to a \n>   resource. One example would be\n>\n>       OPTIONS * HTTP/1.1\n>\n>   To allow for transition to absoluteURIs in all requests in future\n>versions\n>   of HTTP, HTTP/1.1 servers must accept the absoluteURI form in\n>requests, \n>   even though HTTP/1.1 clients will not normally generate them.\n>   Versions of HTTP after HTTP/1.1 may require absoluteURIs everywhere,\n>   after HTTP/1.1 or later have become the dominant implementations.\n\n>   The absoluteURI form is required when the request is being made to\n>   a proxy.  The absoluteURI form is only allowed to an origin server\n>   if the client knows the server supports HTTP/1.1 or later.\n>   If the absoluteURI form is used, any Host request-header included\n>   with the request must be ignored. The proxy is requested to forward\n>the request and \n>   return the response. If the request is GET or HEAD and a prior \n>   response is cached, the proxy may use the cached message if it \n>   passes any restrictions in the Cache-Control and Expires header \n>   fields. Note that the proxy may forward the request on to another \n>   proxy or directly to the server specified by the absoluteURI. In \n>   order to avoid request loops, a proxy must be able to recognize all \n>   of its server names, including any aliases, local variations, and \n>   the numeric IP address. An example Request-Line would be:\n>\n>       GET http://www.w3.org/pub/WWW/TheProject.html HTTP/1.1\n>\n>   The most common form of Request-URI is that used to identify a \n>   resource on an origin server or gateway. In this case, only the \n>   absolute path of the URI is transmitted (see Section 3.2.1, \n>   abs_path). For example, a client wishing to retrieve the resource \n>   above directly from the origin server would create a TCP connection \n>   to port 80 of the host \"www.w3.org\" and send the line:\n>\n>       GET /pub/WWW/TheProject.html HTTP/1.1\n>\n>   followed by the remainder of the Full-Request. Note that the \n>   absolute path cannot be empty; if none is present in the original \n>   URI, it must be given as \"/\" (the server root).\n>\n>   If a proxy receives a request without any path in the Request-URI \n>   and the method used is capable of supporting the asterisk form of \n>   request, then the last proxy on the request chain must forward the \n>   request with \"*\" as the final Request-URI. For example, the request\n>\n>       OPTIONS http://www.ics.uci.edu:8001 HTTP/1.1\n>\n>   would be forwarded by the proxy as\n>\n>       OPTIONS * HTTP/1.1\n>\n>   after connecting to port 8001 of host \"www.ics.uci.edu\".\n>\n>   The Request-URI is transmitted as an encoded string, where some \n>   characters may be escaped using the \"% hex hex\" encoding defined by \n>   RFC 1738 [4]. The origin server must decode the Request-URI in \n>   order to properly interpret the request.\n>\n>|   In requests that they forward, proxies MUST NOT rewrite the\n>|   \"abs_path\" part of a Request-URI in any way except as noted\n>|   above to replace a null abs_path with \"*\". Illegal Request-URIs\n>|   should be responded to with an appropriate status code. (Proxies\n|   may transform the Request-URI for internal processing puposes,\n>|   but should not send such a transformed Request-URI  in forwarded\n|   requests. Transformations for use in cache updates and lookups\n>|   are subject to additional requirements; see section TBD on caching.)\n>|   The main reason for this rule is to make sure that the form of\n>Request-URIs\n>|   is well specified, to enable future extensions without fear that\n>they will\n>|   break in the face of some rewritings. Another is that one\n>consequence of\n>|   rewriting the Request-URI is that integrity or authentication\n>checks by the\n>|   server may fail; since rewriting must be avoided in this case, it\n>may as\n>|   well be proscribed in general. Note: servers writers should be\n>aware that\n>|   some existing proxies do some rewriting.\n>\n\n\n\n", "id": "lists-010-1474230"}, {"subject": "Re: 505 HTTP Version Not Supporte", "content": "Paul Leach wrote:\n> \n> \n> \n> >----------\n> >From: Daniel DuBois[SMTP:ddubois@spyglass.com]\n> >Subject: Re: 505 HTTP Version Not Supported\n> >\n> >At 01:42 PM 4/3/96 -0500, Bob Jernigan wrote:\n> >>I.e., \"if I don't need it, no-one does.\"\n> >\n> >I don't think it's a matter of me shrugging off the little people. \n> >We've\n> >been moving a long time towards increased HTTP complexity.  We're now\n> >at the\n> >point where to be compliant with the latest HTTP there are headers you\n> >are\n> >*required* to send.  The issue of how  virtual servers are supposed to\n> >handle 0.9 requests can be a sticky one (as long as 1.0 and 0.9 are\n> >around\n> >will vendors continue to suck up multiple IP addresses for one\n> >machine?).\n> >\n> >IMO, this pseudo-backward compatibilty can only remain feasible for so\n> >long.\n> >I just think the world (wide web) would be a better place if no one\n> >made 0.9\n> >requests anymore.  And I'll probably feel the same way about 1.0\n> >requests\n> >soon enough.\n> >\n> >>But 0.9 is useful and will continue\n> >>to be useful in certain circumstances.  About half my hits are 0.9 but\n> >>they do return <1% of the data.  It all depends on what you think http\n> >>is useful for.\n> \n> <soapbox>\n> I think Dan is being too kind. My reading of how people in charge of big\n> chunks of the Interent backbone feel is that if we don't expunge 0.9\n> _and_ 1.0 ASAP then the whole net will collapse. So, while it may be\n> convenient to not have to change, and the old way may deliver value to\n> those using it, they are creating disvalue for everyone else.\n> \n> The minimum pieces of 1.1 that have to be implemented by clients and\n> servers to \"save the Internet\" aren't that hard: send Host:, and use\n> persistent connections. (Proxy caches have additional requirements.) The\n> people who have \"20 line HTTP clients and 200 line HTTP servers\" in PERL\n> might see their code grow by 20 lines in order to do this. Real clients\n> and servers might add 100-1000 lines (mostly for persistent\n> connections).  In return, the Internet will be there for us all to use.\n> </soapbox>\n> \n> Paul\n> \nThen perhaps it's time to be explicit about retiring 0.9.  I find\nit useful in my intranet environment but I see no reason this\nshould burden the Internet.  We could say that \"If you use 0.9 in\nan Intranet environment, then this environment should be shielded\nfrom the Internet.\"  I can't believe I'm the only one still using\n0.9, but if so, I'll happily withdraw any request to preserve 0.9\nand stuff them in a tiny server behind my firewall and drop any\nclaim about HTTP.  The same thing might apply to anyone who wants\nto stay with HTTP/1.0.  \n\nbob\n\n\n\n", "id": "lists-010-1487621"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "> I know there are plans to make proxies report this kind of info in the\n> future, but we're not there yet and sites that are dependant on ad revenue\n> cannot afford to be good net citizens w.r.t caching ads.\n\nJust to clarify, the proposals are to standardize a method\nto *allow* proxies to report this kind of data.  Nothing in the\nproposals *makes* anyone do anything.  Jeff and Paul\nwere very clear about that from the beginning, and it\nkeeps the hit-metering draft out of the scary\n\"big-brother\" category.\n\nregards,\nTed Hardie\n\n\n\n\n\n\n-- \n\n\n\n", "id": "lists-010-14960748"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "I'd like to bring up one another (small) point.\n\nVirtually all banner advertising on the web relies on redirects for\nclick-throughs so that click rates can be measured.  This means that the\nredirection to the advertiser's web site will always be an unverifiable\ntransaction.\n\nSo, when a user visits an advertiser's site directly, cookie assignment on\nthe home/jump page is possible, whereas when the user visits the page via\nan advertisement, it will not be possible.\n\nDesigners of web sites (at least the large percentage who will advertise on\nthe web) will have to take into account that cookie assignments on their\nhome page may fail a large percentage of the time.  If they wish to measure\nnumber of unique visitors to their site, they will get a highly inaccurate\nreading since often multiple cookies will be assignied to a single user\nbefore one \"sticks\".\n\nThis is not a major flaw, but it is inelegant, and I just want to make sure\neveryone has considered this.\n\nDwight\n\n\n\n", "id": "lists-010-14968882"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "On Mon, 17 Mar 1997, Dwight Merriman wrote:\n> Designers of web sites (at least the large percentage who will advertise on\n> the web) will have to take into account that cookie assignments on their\n> home page may fail a large percentage of the time.  If they wish to measure\n> number of unique visitors to their site, they will get a highly inaccurate\n> reading since often multiple cookies will be assignied to a single user\n> before one \"sticks\".\n\nThis is reportedly already the case to due to faulty proxies.  I believe\nHotwired keeps an extensive database of which proxies fail to properly\npass-through cookie assignments as a result.\n\nMy point is simply that other system faults already require the complexity\nyou mention.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-14978008"}, {"subject": "Re: 505 HTTP Version Not Supporte", "content": "I really don't understand the arguments concerning 0.9. It really \ndoes have very little use. Consider the simplest possible HTTP 1.1\nrequest\n\nGET /foo/bar.html HTTP/1.1\nHost: www.foobar.com:80\n\n\n\nI don't think that this is very much more complex!\n\nThen we have the simplest reply parser:-\n\n1) Check for HTTP/x.x\n\n2) If not present we have hit one of the very few remaining 0.9 servers.\nact accordingly\n\n3) Otherwise gobble up the status code and headers. If the first digit\nof the status code was a 2 we have some content. Otherwise we don't.\n\n\nIf a language makes it difficult to implement such code easily then \nthe solution is to try a new language. Perl and TK implementations\nare not madatory for IETF acceptance. There are decent libraries arround\nin any case. \n\nI would like to see a statement to the effect that 0.9 will be phased out in \nfuture versions of the protocol. For the entire period of use of 0.9 it has been \nsubject to future revision. Its existence has never been guarnteed. When 0.9 was \nsuperceeded there were fewer than 100 Web servers in operation. I would be \nsuprized if any of those servers was still in operation (bar possibly the REXX \nbased servers on CERNVM and DESY-NEWLIB which are in any case condemned \nmachines).\n\nIf one considers some of the problems occurring becuase of unread input etc. I \nvery much doubt that many platforms could support a 0.9 server which is unaware \nof 1.0 with any great reliability. Remember that in the days of 0.9 the Web was \nin any case pretty ropey which was part of the need for a change. If a 1.0 \nclient connects to a 0.9 server the headers part of the 1.0 request will be \nunread by the server. This will in turn mean that the server will close the \nsocket with unread data which in many stacks causes a reset command to be sent \nwhich is likely to result in unread data. This type of effect was a common \nproblem in the 0.9 to 1.0 upgrade period.\n\nThe simplicity of implementing 0.9 should be set against the complexity of \nsupporting 0.9 in a 1.1 framework.\n\nI don't think we should continue to discuss the issue at this time. The best we \ncan do in 1.1 is to warn people that 0.9 may not be required in future and that \nclients must not assume that it will be avaliable.\n\n\nPhill\n\n\n\n", "id": "lists-010-1498316"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "Agreed.  But I think if one looks at the long run, which is what is most\nimportant for a standard, all the proxies will work properly.\n\nI just find it kind of weird that the RFC specs nondeterministic behavior\n-- sometimes cookies work, sometimes they don't, and you can't always know\nwhen they will without a lot of complexity.\n\nDwight\n\n----------\n> From: M. Hedlund <hedlund@best.com>\n> To: Dwight Merriman <dmerriman@doubleclick.net>\n> Cc: http <http-wg@cuckoo.hpl.hp.com>; dmk@allegra.att.com;\nmontulli@netscape.com; yarong@microsoft.com\n> Subject: Re: Unverifiable Transactions / Cookie draft\n> Date: Monday, March 17, 1997 12:34 PM\n> \n> \n> On Mon, 17 Mar 1997, Dwight Merriman wrote:\n> > Designers of web sites (at least the large percentage who will\nadvertise on\n> > the web) will have to take into account that cookie assignments on\ntheir\n> > home page may fail a large percentage of the time.  If they wish to\nmeasure\n> > number of unique visitors to their site, they will get a highly\ninaccurate\n> > reading since often multiple cookies will be assignied to a single user\n> > before one \"sticks\".\n> \n> This is reportedly already the case to due to faulty proxies.  I believe\n> Hotwired keeps an extensive database of which proxies fail to properly\n> pass-through cookie assignments as a result.\n> \n> My point is simply that other system faults already require the\ncomplexity\n> you mention.\n> \n> M. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-14987560"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "On Mon, 17 Mar 1997, Dwight Merriman wrote:\n\n> Agreed.  But I think if one looks at the long run, which is what is most\n> important for a standard, all the proxies will work properly.\n\nSure, but then soon to come are cookie monsters (well actually I'm\naware of at least one already).  And all proxies I know of can easily\nblock sites converting the banner ad to a small broken image icon.\nI know of one personal web tool based on proxy technology which has this\nability as well.\n\nThe more people who are offended by hidden profiling, the wider the \ndeployment of such tools.\n\n> \n> I just find it kind of weird that the RFC specs nondeterministic behavior\n> -- sometimes cookies work, sometimes they don't, and you can't always know\n> when they will without a lot of complexity.\n\nI think the question is from whose perspective the behavior is\nnon-deterministic. The behavior is quite deterministic from my perspective\nas a user.\n\nIn the single very limited involvement I had with advertising, the revenue\nwas associated with the click thru on the banner ad, not the bannner ad.\nThat would, as I understand the DoubleCLick model would be a click to \nthe DoubleClick site from which a set-cookie wouldn't be considered an\nunverifiable transaction.\n\nIf I were advertising, knowing when a user actually asked to see more of\nmy message would have tremendous value compared with having my ad\nwhere they might have seen it.\n\nThe bottom line is that WWW advertising is based on a business model which\ndidn't even exist a couple of years ago. It is based on leveraging\ntechnology which is new and evolving. I am confident that it will evolve\nin response to improvements to the technology..\n\nDavid Morris\n\n\n\n", "id": "lists-010-14998965"}, {"subject": "Re: Confused about persistent connection for old client", "content": "    Section 8.1.3 says:\n    \"A proxy server MUST NOT establish a persistent connection with an\n    HTTP/1.0 client\".\n    \n    Excerpt from section 19.7.1:\n    \"An HTTP/1.1 server may also establish persistent connections with\n    HTTP/1.0 clients upon receipt of a Keep-Alive connection token.\"\n    \n    Why is there difference in handling persistent connections for proxy\n    servers and for HTTP/1.1 servers?\n\nBecause it was recognized during the HTTP/1.1 design process that\nthe uncontrolled forwarding of HTTP/1.0 + Keep-Alive requests\nthrough proxies (especially multiple levels of proxies) could lead\nto confusing communication failures.  See the first paragraph\nof 19.7.1, which explains this.\n\n19.7.1 should perhaps say \"An HTTP/1.1 origin server\" where\nthe part that you quoted says \"An HTTP/1.1 server\".\n\n-Jeff\n\n\n\n", "id": "lists-010-15008190"}, {"subject": "Re: Fact-checking: do any inservice proxy caches ever ignore Expires", "content": "I wrote:\n    Koen and I have been discussing offline whether it is possible\n    to send\n    Expires: Sun, 06 Nov 1994 08:49:37 GMT\n    (or some similar ancient date) to ensure that *every* pre-HTTP/1.1\n    proxy cache will not, under *any* circumstances, cache the response.\n\nThis wasn't a very good way of asking the question (I should have been\na lot more careful about using the verb \"to cache\").\n\nI should have asked whether sending an old http-date in an Expires header\nwill ensure that *every* pre-HTTP/1.1 proxy cache will not, under\nany* circumstances, use a cached copy of the response in reply to a\nsubsequent request without first checking with the origin server,\nusing an \"If-Modified-Since\" request.\n\nI.e., in HTTP/1.1 terms, will the proxy cache ever knowingly return\nan explictly stale response?\n\nI got some responses, not all of which were forwarded to the mailing\nlist.\n\nHere's what I have been told:\n\nAri Luotonen\n    All various CERN proxy 3.0 versions and all versions of Netscape Proxy\n    (1.1, 2.0, 2.5) handle the Expires: header correctly, and understand\n    it to mean \"do not use without revalidation after this time\".\n    \nVinod Valloppillil <vinodv@microsoft.com>\n    We [Microsoft] always honor expires: in our proxy server\n\nDave Ladd (dladd@spyglass.com)\n    [SurfWatch ProServer from Sypglass] will cache responses with an \n    explicit Expires, but if the date is in the past, we'll end up\n    always trying a conditional get. (Expires only affects the \n    freshness lifetime).\n\nPeter Danzig\n    To the best of my recollection, no version of the\n    Harvest cache or its derivitives ignored an Expires\n    header.\n\nbut ...\nrom Anthony Baxter <arb@connect.com.au>\n    squid allows you to force certain pages to not be expired for certain\n    amount of time. This is useful for, eg www.realaudio.com, who set all\n    their pages to expire 'now' (well, they used to, anyway).\n\n\"Kolics Bertold, University of Veszprem\" <bertold@tohotom.vein.hu>\n    It *is* possible in Squid-1.1.x.\n    Excerpt from the Release-Notes:\n\"Squid 1.1 switched from a Time-To-Live based expiration model\nto a Refresh-Rate model.  Objects are no longer purged from the\ncache when they expire.  Instead of assigning TTL's when the\nobject enters the cache, we now check freshness requirements\nwhen objects are requested.\"\n    Minimum age can be given for specific URL patterns. By default it is set\n    to zero for all URL patterns, but for cache-unfriendly sites it is usually\n    changed...\n\nSo I am still a little confused about whether Squid simply lets stuff\nstay in the cache past the Expires time, or whether it always checks\npast-Expires responses with the origin server before providing them.\nKolics Bertold's quote from the Squid 1.1 release notes implies (but\ndoes not specifically state) that Squid *always* checks freshness;\nAnthony Baxter's comments implies (but not specifically state) the\nopposite.\n\nSorry about the confusing language the first time I sent this.\n\n-Jeff\n\n\n\n", "id": "lists-010-15016607"}, {"subject": "Agenda for HTTP working group April ", "content": "There's been an enormous amount of activity, a large number of\ninternet drafts that have not yet been submitted. However, here's\nmy second cut at an agenda for the Memphis IETF meeting. Please\nsend any corrections or suggestions to me or to the working group.\n\nLarry\n-------\nApril 7, 9:30-11:30\n 30 minutes Agenda review & WG status overview\n 90 minutes HTTP/1.1 issue list\n     Moving RFC 2068 and 2069 from Proposed to Draft Standard.\n     List of issues:\n        http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/\n     and internet drafts linked therein. In particular:\nftp://ietf.org/internet-drafts/draft-ietf-http-versions-01.txt\n       \nftp://ietf.org/internet-drafts/draft-mogul-http-revalidate-00.txt\n\n        ftp://ietf.org/internet-drafts/draft-holtman-safe-01.txt (?)\n\n\nApril 7, 19:30-22:00\n 30 minutes Content negotiation status & last calls\n       \nftp://ietf.org/internet-drafts/draft-ietf-http-negotiation-01.txt\nready?\n        ftp://ietf.org/internet-drafts/draft-ietf-http-rvsa-v10-00.txt\nftp://ietf.org/internet-drafts/draft-ietf-http-feature-reg-00.txt\n\n 30 minutes Revising state\n       \nftp://ietf.org/internet-drafts/draft-ietf-http-state-mgmt-errata-00.txt\n         and proposed revision to RFC 2109(?)\n 30 minutes PEP\n        New draft not yet submitted\n        http://www.w3.org/pub/WWW/Protocols/PEP/\n 10 minutes hit metering\n         ftp://ietf.org/internet-drafts/draft-mogul-hit-metering-02.txt\n 20 minutes\n      revisit issues from issue list: closure? \n 30 minutes WG future: new WGs, charter, close by summer?\n\n\n\n", "id": "lists-010-15028298"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "On Mon, 17 Mar 1997, David W. Morris wrote:\n> In the single very limited involvement I had with advertising, the revenue\n> was associated with the click thru on the banner ad, not the bannner ad.\n> That would, as I understand the DoubleCLick model would be a click to \n> the DoubleClick site from which a set-cookie wouldn't be considered an\n> unverifiable transaction.\n> \n> If I were advertising, knowing when a user actually asked to see more of\n> my message would have tremendous value compared with having my ad\n> where they might have seen it.\n\nFor some models this makes sense - I even know ad models where content sites\nare compensated only as a percentage of online sales generated!  But there is a\ncompelling argument that compensation models based on clickthroughs or more are\nflawed because it encourages the content developer to focus exclusively on\npushing people through the ad, rather than actually providing useful content.\nI'm sure many content sites would refuse to partake in this.  \n\n> The bottom line is that WWW advertising is based on a business model which\n> didn't even exist a couple of years ago. It is based on leveraging\n> technology which is new and evolving. I am confident that it will evolve\n> in response to improvements to the technology..\n\nThe business model of sponsored content has been around forever.  So have these\nprivacy issues - how do you know that donation you made to the Sierra Club last\nyear didn't lead to that Democratic National Committee soliciation you got\nyesterday?  Looking to technology for a solution to the privacy problems is\nas misguided as blaming technology for causing them.  \n\nI have come to believe over the last year that this is not a problem for which\ntechnology can provide a complete solution.  Intent is everything - and even\nbrowsers that obey this spec and folks who don't use cookies are still ripe for\nabuse by parties with intent.  Fortunately abuse is not a requirement to make a\nprofit in this model - doubleclick doesn't have to be able to do a credit check\non you or know your email address to do what they do.\n\nTwo companies which have similar privacy issues - FireFly and Narrowline -\nbacked up their claims of consumer protection by paying Coopers & Lybrand\n(one of the Big 6 accounting firms) to perform a personal information security\naudit.  Perhaps DoubleClick should do the same.\n\nI think in the long run a solution like one suggested by\n\"Jaye, Dan\" <DJaye@engagetech.com> in \n<c=US%a=_%p=CMG%l=ANDEXC01-970314233918Z-22988@wilexc01.cmgi.com>\nis the one which most closely semantically matches what's going on here.  The\nissue is one of trust - and trust brokered by a certificate authority (be it\nCoopers&Lybrand, ETrust, Better Business Bureau, an organization of the EC,\nwhatever) makes sense.\n\nBut in the short term... let's not kid ourselves.  This will not significantly\nincrease the privacy of users on the web.  This is not similar to the\n\"opt-in/opt-out\" debate amongst direct marketers - of which I am firmly in the\nland of \"opt-in\".  This is a small hurdle - even just using the IP number and\nUser-Agent as the key to the profiling database would be sufficient for many\nuses, and slightly more elaborate schemes can be used to make it much more\nprecise. That it is a small hurdle to work around is *not* a justification for\nputting it in the spec. \n\nI am somewhat loath to enter this conversation as I was periphery to the\ndiscussions early on which led to this, and I now have a slightly different\nopinion, and we know this RFC has been in development for a long time.  The\nfact that my opinion can be different today suggests it may be too early to try\nand coerce the protocols into implementing policy... just a thought.\n\nI think Dwight's proposal might be a little overcomplicated - something like\nthe below might address most concerns out there?\n\n1) By default, user agents are configured to prompt for confirmation on \n     receipt of all cookies - unlike today's defaults which is to accept\n     all cookies.\n2) Upon receipt of a cookie, UI must have the options:\n     Accept this cookie\n     Do not accept this cookie\n     Accept all cookies from this domain\n     Never accept cookies from this domain\n3) UI allows for some indication of pages with content inlined from other\n     servers.  Perhaps specially flag cookie requests for inlined content too.\n4) Remove the restriction on unverified transactions.\n\nShow me this leads to /less/ privacy, particularly in combination with all the\nother existing provisions of the current spec.\n\nI really wish this was a more clear-cut situation - that cookies really did\nmake the difference between being \"database-able\" or not.  It doesn't. \n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  www.apache.org  hyperreal.com  http://www.organic.com/JOBS\n\n\n\n", "id": "lists-010-15037210"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "There is an interesting assumption being made that protocols have the\nright to dictate user interface to software makers. Am I the only one\nwho finds this development disturbing? Not because I am overly concerned\nabout protocols dictating UI, the protocol will be roundly ignored and\ncompliance will be coincidental at best, but rather that by dictating\nrequirements in areas clearly beyond the scope of a wire protocol, the\nauthority of the protocol group is lessened.\n\nYaron\n\n> -----Original Message-----\n> From:Brian Behlendorf [SMTP:brian@organic.com]\n> Sent:Tuesday, March 18, 1997 12:19 AM\n> To:David W. Morris\n> Cc:http working group\n> Subject:Re: Unverifiable Transactions / Cookie draft\n> \n> On Mon, 17 Mar 1997, David W. Morris wrote:\n> > In the single very limited involvement I had with advertising, the\n> revenue\n> > was associated with the click thru on the banner ad, not the bannner\n> ad.\n> > That would, as I understand the DoubleCLick model would be a click\n> to \n> > the DoubleClick site from which a set-cookie wouldn't be considered\n> an\n> > unverifiable transaction.\n> > \n> > If I were advertising, knowing when a user actually asked to see\n> more of\n> > my message would have tremendous value compared with having my ad\n> > where they might have seen it.\n> \n> For some models this makes sense - I even know ad models where content\n> sites\n> are compensated only as a percentage of online sales generated!  But\n> there is a\n> compelling argument that compensation models based on clickthroughs or\n> more are\n> flawed because it encourages the content developer to focus\n> exclusively on\n> pushing people through the ad, rather than actually providing useful\n> content.\n> I'm sure many content sites would refuse to partake in this.  \n> \n> > The bottom line is that WWW advertising is based on a business model\n> which\n> > didn't even exist a couple of years ago. It is based on leveraging\n> > technology which is new and evolving. I am confident that it will\n> evolve\n> > in response to improvements to the technology..\n> \n> The business model of sponsored content has been around forever.  So\n> have these\n> privacy issues - how do you know that donation you made to the Sierra\n> Club last\n> year didn't lead to that Democratic National Committee soliciation you\n> got\n> yesterday?  Looking to technology for a solution to the privacy\n> problems is\n> as misguided as blaming technology for causing them.  \n> \n> I have come to believe over the last year that this is not a problem\n> for which\n> technology can provide a complete solution.  Intent is everything -\n> and even\n> browsers that obey this spec and folks who don't use cookies are still\n> ripe for\n> abuse by parties with intent.  Fortunately abuse is not a requirement\n> to make a\n> profit in this model - doubleclick doesn't have to be able to do a\n> credit check\n> on you or know your email address to do what they do.\n> \n> Two companies which have similar privacy issues - FireFly and\n> Narrowline -\n> backed up their claims of consumer protection by paying Coopers &\n> Lybrand\n> (one of the Big 6 accounting firms) to perform a personal information\n> security\n> audit.  Perhaps DoubleClick should do the same.\n> \n> I think in the long run a solution like one suggested by\n> \"Jaye, Dan\" <DJaye@engagetech.com> in \n> <c=US%a=_%p=CMG%l=ANDEXC01-970314233918Z-22988@wilexc01.cmgi.com>\n> is the one which most closely semantically matches what's going on\n> here.  The\n> issue is one of trust - and trust brokered by a certificate authority\n> (be it\n> Coopers&Lybrand, ETrust, Better Business Bureau, an organization of\n> the EC,\n> whatever) makes sense.\n> \n> But in the short term... let's not kid ourselves.  This will not\n> significantly\n> increase the privacy of users on the web.  This is not similar to the\n> \"opt-in/opt-out\" debate amongst direct marketers - of which I am\n> firmly in the\n> land of \"opt-in\".  This is a small hurdle - even just using the IP\n> number and\n> User-Agent as the key to the profiling database would be sufficient\n> for many\n> uses, and slightly more elaborate schemes can be used to make it much\n> more\n> precise. That it is a small hurdle to work around is *not* a\n> justification for\n> putting it in the spec. \n> \n> I am somewhat loath to enter this conversation as I was periphery to\n> the\n> discussions early on which led to this, and I now have a slightly\n> different\n> opinion, and we know this RFC has been in development for a long time.\n> The\n> fact that my opinion can be different today suggests it may be too\n> early to try\n> and coerce the protocols into implementing policy... just a thought.\n> \n> I think Dwight's proposal might be a little overcomplicated -\n> something like\n> the below might address most concerns out there?\n> \n> 1) By default, user agents are configured to prompt for confirmation\n> on \n>      receipt of all cookies - unlike today's defaults which is to\n> accept\n>      all cookies.\n> 2) Upon receipt of a cookie, UI must have the options:\n>      Accept this cookie\n>      Do not accept this cookie\n>      Accept all cookies from this domain\n>      Never accept cookies from this domain\n> 3) UI allows for some indication of pages with content inlined from\n> other\n>      servers.  Perhaps specially flag cookie requests for inlined\n> content too.\n> 4) Remove the restriction on unverified transactions.\n> \n> Show me this leads to /less/ privacy, particularly in combination with\n> all the\n> other existing provisions of the current spec.\n> \n> I really wish this was a more clear-cut situation - that cookies\n> really did\n> make the difference between being \"database-able\" or not.  It doesn't.\n> \n> \n> Brian\n> \n> --=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n> =-=-=--\n> brian@organic.com  www.apache.org  hyperreal.com\n> http://www.organic.com/JOBS\n> \n\n\n\n", "id": "lists-010-15050797"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "BTW I should clarify that I meant that implementers will generally not\nfeel bound by sections of a wire protocol which specify UI, not that an\nentire protocol would be ignored because it provides UI requirements.\n\nYaron\n\n> -----Original Message-----\n> From:Yaron Goland \n> Sent:Tuesday, March 18, 1997 1:17 AM\n> To:http working group\n> Subject:RE: Unverifiable Transactions / Cookie draft\n> \n> There is an interesting assumption being made that protocols have the\n> right to dictate user interface to software makers. Am I the only one\n> who finds this development disturbing? Not because I am overly\n> concerned about protocols dictating UI, the protocol will be roundly\n> ignored and compliance will be coincidental at best, but rather that\n> by dictating requirements in areas clearly beyond the scope of a wire\n> protocol, the authority of the protocol group is lessened.\n> \n> Yaron\n> \n> -----Original Message-----\n> From:Brian Behlendorf [SMTP:brian@organic.com]\n> Sent:Tuesday, March 18, 1997 12:19 AM\n> To:David W. Morris\n> Cc:http working group\n> Subject:Re: Unverifiable Transactions / Cookie draft\n> \n> On Mon, 17 Mar 1997, David W. Morris wrote:\n> > In the single very limited involvement I had with advertising,\n> the revenue\n> > was associated with the click thru on the banner ad, not the\n> bannner ad.\n> > That would, as I understand the DoubleCLick model would be a\n> click to \n> > the DoubleClick site from which a set-cookie wouldn't be\n> considered an\n> > unverifiable transaction.\n> > \n> > If I were advertising, knowing when a user actually asked to\n> see more of\n> > my message would have tremendous value compared with having my\n> ad\n> > where they might have seen it.\n> \n> For some models this makes sense - I even know ad models where\n> content sites\n> are compensated only as a percentage of online sales generated!\n> But there is a\n> compelling argument that compensation models based on\n> clickthroughs or more are\n> flawed because it encourages the content developer to focus\n> exclusively on\n> pushing people through the ad, rather than actually providing\n> useful content.\n> I'm sure many content sites would refuse to partake in this.  \n> \n> > The bottom line is that WWW advertising is based on a business\n> model which\n> > didn't even exist a couple of years ago. It is based on\n> leveraging\n> > technology which is new and evolving. I am confident that it\n> will evolve\n> > in response to improvements to the technology..\n> \n> The business model of sponsored content has been around forever.\n> So have these\n> privacy issues - how do you know that donation you made to the\n> Sierra Club last\n> year didn't lead to that Democratic National Committee\n> soliciation you got\n> yesterday?  Looking to technology for a solution to the privacy\n> problems is\n> as misguided as blaming technology for causing them.  \n> \n> I have come to believe over the last year that this is not a\n> problem for which\n> technology can provide a complete solution.  Intent is\n> everything - and even\n> browsers that obey this spec and folks who don't use cookies are\n> still ripe for\n> abuse by parties with intent.  Fortunately abuse is not a\n> requirement to make a\n> profit in this model - doubleclick doesn't have to be able to do\n> a credit check\n> on you or know your email address to do what they do.\n> \n> Two companies which have similar privacy issues - FireFly and\n> Narrowline -\n> backed up their claims of consumer protection by paying Coopers\n> & Lybrand\n> (one of the Big 6 accounting firms) to perform a personal\n> information security\n> audit.  Perhaps DoubleClick should do the same.\n> \n> I think in the long run a solution like one suggested by\n> \"Jaye, Dan\" <DJaye@engagetech.com> in \n> \n> <c=US%a=_%p=CMG%l=ANDEXC01-970314233918Z-22988@wilexc01.cmgi.com>\n> is the one which most closely semantically matches what's going\n> on here.  The\n> issue is one of trust - and trust brokered by a certificate\n> authority (be it\n> Coopers&Lybrand, ETrust, Better Business Bureau, an organization\n> of the EC,\n> whatever) makes sense.\n> \n> But in the short term... let's not kid ourselves.  This will not\n> significantly\n> increase the privacy of users on the web.  This is not similar\n> to the\n> \"opt-in/opt-out\" debate amongst direct marketers - of which I am\n> firmly in the\n> land of \"opt-in\".  This is a small hurdle - even just using the\n> IP number and\n> User-Agent as the key to the profiling database would be\n> sufficient for many\n> uses, and slightly more elaborate schemes can be used to make it\n> much more\n> precise. That it is a small hurdle to work around is *not* a\n> justification for\n> putting it in the spec. \n> \n> I am somewhat loath to enter this conversation as I was\n> periphery to the\n> discussions early on which led to this, and I now have a\n> slightly different\n> opinion, and we know this RFC has been in development for a long\n> time.  The\n> fact that my opinion can be different today suggests it may be\n> too early to try\n> and coerce the protocols into implementing policy... just a\n> thought.\n> \n> I think Dwight's proposal might be a little overcomplicated -\n> something like\n> the below might address most concerns out there?\n> \n> 1) By default, user agents are configured to prompt for\n> confirmation on \n>      receipt of all cookies - unlike today's defaults which is\n> to accept\n>      all cookies.\n> 2) Upon receipt of a cookie, UI must have the options:\n>      Accept this cookie\n>      Do not accept this cookie\n>      Accept all cookies from this domain\n>      Never accept cookies from this domain\n> 3) UI allows for some indication of pages with content inlined\n> from other\n>      servers.  Perhaps specially flag cookie requests for\n> inlined content too.\n> 4) Remove the restriction on unverified transactions.\n> \n> Show me this leads to /less/ privacy, particularly in\n> combination with all the\n> other existing provisions of the current spec.\n> \n> I really wish this was a more clear-cut situation - that cookies\n> really did\n> make the difference between being \"database-able\" or not.  It\n> doesn't. \n> \n> Brian\n> \n> \n> --=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n> =-=-=--\n> brian@organic.com  www.apache.org  hyperreal.com\n> http://www.organic.com/JOBS\n> \n\n\n\n", "id": "lists-010-15066111"}, {"subject": "Re: 505 HTTP Version Not Supporte", "content": "Regarding support for HTTP 0.9, I'd note that another little thing that\nkeeps it \"alive\" at a very low level is hacks to interoperate with gopher.\n\nIn particular, a gopher server can serve up a link to a text HTML page,\nthat some WWW clients are able to follow. This has helped some sites do\na gopher/web transition.\n\nBut this is hardly a compelling reason to keep 0.9 alive in the long term.\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-010-1507799"}, {"subject": "Re: Confused about persistent connection for old client", "content": "Classification:\nPrologue: MPRESLER@IBMUSM20 (inside IBM)\nmpresler@us.ibm.com (outsite IBM)\nEpilogue: \"Be excellent to each other.\" -- A. Lincoln\n\n>Because it was recognized during the HTTP/1.1 design process that\n>the uncontrolled forwarding of HTTP/1.0 + Keep-Alive requests\n>through proxies (especially multiple levels of proxies) could lead\n>to confusing communication failures.  See the first paragraph\n>of 19.7.1, which explains this.\n\n OK, I can understand this; I can certainly see problems with this when using\nolder proxies like CERN httpd. To ask the question differently, though, what's\ncurrent practice? What do existing proxy servers do? If you'd like, e-mail to\nmpresler@us.ibm.com, and I'll summarize.\n\n IBM's proxy product does not support Keep-Alives from HTTP/1.0 clients. Our\ncontent (origin) server does support them.\n\n -- Martin\n\n\n\n", "id": "lists-010-15081634"}, {"subject": "State Mgmt/RFC 2109 plan", "content": "Concerning the State Mgmt RFC (2109), my plans are:\n\n- to withdraw draft-ietf-http-state-mgmt-errata-00.txt\n- to submit draft-ietf-http-state-mgmt-06.txt\n\nThe second is a version of RFC 2109 that incorporates the renaming of\nSet-Cookie to Set-Cookie2, details of how to handle Set-Cookie and\nSet-Cookie2 compatibly, and some minor editorial changes.  I expect to\nsubmit the I-D (which subsumes the errata I-D) today or tomorrow.\n\nDave Kristol\n\n\n\n", "id": "lists-010-15089726"}, {"subject": "&quot;DoubleClick Tries to Force Hand into Cookie Jar&quot", "content": "http://www.wired.com/news/technology/story/2615.html\n\n\n\n", "id": "lists-010-15096661"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "In a previous episode Ted Hardie said...\n:: \n:: \n:: > I know there are plans to make proxies report this kind of info in the\n:: > future, but we're not there yet and sites that are dependant on ad revenue\n:: > cannot afford to be good net citizens w.r.t caching ads.\n:: \n:: Just to clarify, the proposals are to standardize a method\n:: to *allow* proxies to report this kind of data.  Nothing in the\n:: proposals *makes* anyone do anything.  Jeff and Paul\n:: were very clear about that from the beginning, and it\n:: keeps the hit-metering draft out of the scary\n:: \"big-brother\" category.\n\nRight on.. and to clarify a little further when serving to a proxy the\norigin server is told whether or not the proxy pledges to return this\ninformation at a later date.. if it doesn't they can cache bust.. the\nweakest point of the hit-metering draft IMHO is that it doesn't try\nand provide any other methods of determing proxy reliability wrt this\npledge to base the \"to cache or to bust\" decision on..\n\n-P\n\n\n\n", "id": "lists-010-15103129"}, {"subject": "draft-holtman-http-safe01.tx", "content": "Let's preface this with a little background.. I do a lot of\napplication design dealing with HTTP.. we have a huge need for some\ntype of GET-with-body or Post with No side effect type of\nfunctionality in HTTP.. but I think there's a problem with the\ndraft-holtman-http-safe-01.txt approach.\n\nThe draft introduces Safe as a response header which is of course not\ninitiated in any way by the client.. this leaves no method for the\nclient to send a request to the server (with a body) that Mandates\nthat they consent to no side effects.. leading to some particularly\ngruesome scenarios:\n\n* Client gets a page via post.. it's marked Safe\n* Client reloads page page.. no UA confirmation is\nasked.. this time a side effect does occur (do to some application\nlogic.. time of day perhaps) and the response is marked Safe: no..\n* User doesn't reload again.. has no idea that the last load\nof page had a different impact than previous loads..\n\nIn addition, there needs to be some way for the UA to send a request\nthat doesn't allow side effects to occur (the current semantics of\nGET) for safety's safe, instead of just determining whether or not\nthey have caused side effects. Holtman does a nice job in section\n2 of presenting the reasons why that method must also accomodate a\nbody.\n\nI'm not sure that there is a better way than a new method.\n\nThe recently mentioned draft-ietf-http-uahint-00.txt suffers the same\nlimitation. \n\n-P\n\n\n\n", "id": "lists-010-15111794"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "To:http-wg@cuckoo.hpl.hp.com\n\n\n\nOn Tue, 18 Mar 1997, Yaron Goland wrote:\n\n> There is an interesting assumption being made that protocols have the\n> right to dictate user interface to software makers. Am I the only one\n> who finds this development disturbing? Not because I am overly concerned\n> about protocols dictating UI, the protocol will be roundly ignored and\n> compliance will be coincidental at best, but rather that by dictating\n> requirements in areas clearly beyond the scope of a wire protocol, the\n> authority of the protocol group is lessened.\n\nWhen the purpose of the protocol is to allow for reliable communication\nbetween a user and an application, there is sometimes no other choice than\nto specify the content of the UI. There is a large difference in my mind\nbetween describing what choices the user must be presented with and\ntelling the UI designer how to present those choices. The UI choices I've\nseen have all been related to specification of the communication features\nrequired.\n\nIf you object to a specific statement, please be specific. I have seen no\ngeneral trend to take over the UI.\n\nDave Morris\n\n\n\n", "id": "lists-010-15120006"}, {"subject": "Re: draft-holtman-http-safe01.tx", "content": "On Tue, 18 Mar 1997, Patrick McManus wrote:\n\n> The draft introduces Safe as a response header which is of course not\n> initiated in any way by the client.. this leaves no method for the\n> client to send a request to the server (with a body) that Mandates\n> that they consent to no side effects.. leading to some particularly\n> gruesome scenarios:\n> \n> * Client gets a page via post.. it's marked Safe\n> * Client reloads page page.. no UA confirmation is\n> asked.. this time a side effect does occur (do to some application\n> logic.. time of day perhaps) and the response is marked Safe: no..\n> * User doesn't reload again.. has no idea that the last load\n> of page had a different impact than previous loads..\n\nThe safe: yes (and uahint variation) response header mean that any future\nresubmit of the request will be safe. It says nothing about the request\nwhich was just made which may have had a side effect.  When designing the\napplication, the author of the HTML must know if POST would be\nappropriate. \n\n> \n> In addition, there needs to be some way for the UA to send a request\n> that doesn't allow side effects to occur (the current semantics of\n> GET) for safety's safe, instead of just determining whether or not\n\nThe type of request sent by a client is determined by the HTML the user\nis responding to. It is the responsiblity of the application designer\nto match the method with its characteristics.  Are you anticipating\nthat future clients will offer the user a choice other then GET for\nmanually typed URLs? \n\n> The recently mentioned draft-ietf-http-uahint-00.txt suffers the same\n> limitation. \n\nAt this stage, vis a vis 'safe/notsafe', the uahint proposal is only\nsupposed to be a syntax change from Koen's draft to what ever value or\nflaws exist should be the same.\n\nDave MOrris\n\n\n\n", "id": "lists-010-15128713"}, {"subject": "Re: Fact-checking: do any inservice proxy caches ever ignore Expires", "content": "> but ...\n> >From Anthony Baxter <arb@connect.com.au>\n>     squid allows you to force certain pages to not be expired for certain\n>     amount of time. This is useful for, eg www.realaudio.com, who set all\n>     their pages to expire 'now' (well, they used to, anyway).\n>     \n> \"Kolics Bertold, University of Veszprem\" <bertold@tohotom.vein.hu>\n>     It *is* possible in Squid-1.1.x.\n>     Excerpt from the Release-Notes:\n> \"Squid 1.1 switched from a Time-To-Live based expiration model\n> to a Refresh-Rate model.  Objects are no longer purged from the\n> cache when they expire.  Instead of assigning TTL's when the\n> object enters the cache, we now check freshness requirements\n> when objects are requested.\"\n>     Minimum age can be given for specific URL patterns. By default it is set\n>     to zero for all URL patterns, but for cache-unfriendly sites it is usually\n>     changed...\n> \n> So I am still a little confused about whether Squid simply lets stuff\n> stay in the cache past the Expires time, or whether it always checks\n> past-Expires responses with the origin server before providing them.\n> Kolics Bertold's quote from the Squid 1.1 release notes implies (but\n> does not specifically state) that Squid *always* checks freshness;\n> Anthony Baxter's comments implies (but not specifically state) the\n> opposite.\nSquids refresh_pattern directive overrides expiration from the response,\nthat's squid never contact origin servers before the min-age parameter.\nIf someone specifies a non-zero min age, squid violates the spec.\nI would be satisfied, if min-age and max-age specified in refresh_pattern\nhave effect only if Expires and cache-control max-age is absent in the\nresponse - in sync with the spec.\nHowever, an overriding version of refresh_pattern would be useful too,\nbecause of human lousiness we still see too much pre-expired pages, or\npages lacking Last-modified and content-length. I really hate to see such pages,\nbut how current proxies handle Cache-Control: must-revalidate? \nSquid honors Cache-Control: must-revalidate - the proper (http/1.1)\nway to count hits and do dynamic html etc.\n> Sorry about the confusing language the first time I sent this.\n> \n> -Jeff\n> \nAndrew. (Endre \"Balint\" Nagy) <bne@CareNet.hu> <bne@bne.ind.eunet.hu>\n\n\n\n", "id": "lists-010-15138318"}, {"subject": "Re: draft-holtman-http-safe01.tx", "content": "In a previous episode David W. Morris said...\n:: \n:: On Tue, 18 Mar 1997, Patrick McManus wrote:\n:: \n:: > The draft introduces Safe as a response header which is of course not\n:: > initiated in any way by the client.. this leaves no method for the\n:: > client to send a request to the server (with a body) that Mandates\n:: > that they consent to no side effects.. leading to some particularly\n:: > gruesome scenarios:\n[..]\n:: The safe: yes (and uahint variation) response header mean that any future\n:: resubmit of the request will be safe. It says nothing about the request\n:: which was just made which may have had a side effect.\n\n\nThat's not how I read section 3:\n\n--\n3 The Safe response header\n\n This header is proposed as an addition to the HTTP/1.x suite.\n\n The Safe response header field indicates whether the corresponding\n request is safe in the sense of Section 9.1.1 (Safe Methods) of the\n HTTP/1.1 draft specification [1].\n--\n\nIt speaks very specifically that the safe response header field talks\nabout the corresponding (i.e. current) request.. I think making any\nassumptions about future request/response pairs requires clairvoyance\n(and the Note further in section three claims that it can do this\nclairvoyance)\n\n::  When designing the\n:: application, the author of the HTML must know if POST would be\n:: appropriate. \n:: \n\nwho said anything about html?\n\nIt is not unusual for us to release a c/s pair  and\nhave other folks independently develop their own client side\ninterfaces without our knowledge.. it'd be extremely kind if they\ncould have a method that would guarantee no side effects for them\nshould we upgrade the server side to change it's behavior without\ntelling them.. (which is sort of hard, as we don't know they are there!)\n\nIn short if we've got a CGI that makes side effects we make damn sure\nit's input comes from POST not from GET before doing it.. even if\nwe've coded the client side to use POST.. because someone else out\nthere experimenting against our service and using GET shouldn't be\nable to impact the environment.. this whole system works fine, we just\nneed a GET with a message body.\n\n:: > \n:: > In addition, there needs to be some way for the UA to send a request\n:: > that doesn't allow side effects to occur (the current semantics of\n:: > GET) for safety's safe, instead of just determining whether or not\n:: \n:: The type of request sent by a client is determined by the HTML the user\n:: is responding to. It is the responsiblity of the application designer\n:: to match the method with its characteristics.  Are you anticipating\n:: that future clients will offer the user a choice other then GET for\n:: manually typed URLs? \n\nIt seems like a much more flexible choice than designing the system so\nthey can't. Tthe whole UA world isn't html oriented browsers that can\ndraw distinctions between \"typed it in\" and \"clicked on it\"..\n\n\n\n-Patrick\n--\nPatrick R. McManus - Applied Theory Communications -Software Engineering\nhttp://pat.appliedtheory.com/~mcmanusProgrammer Analyst\nmcmanus@AppliedTheory.com'Prince of Pollywood'Standards, today!\n*** - You Kill Nostalgia, Xenophobic Fears. It's Now or Neverland. - ***\n\n\n\n", "id": "lists-010-15148962"}, {"subject": "CHUNKED: close", "content": "It appears that all questions on this issue have been answered without\nrequiring any changes, so this issue is hereby declared closed for the\nupcoming draft.\n\n>----------\n>From: Paul Leach\n>Sent: Sunday, March 31, 1996 4:53 PM\n>To: 'http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com'\n>Subject: CHUNKED: draft changes\n>\n>Following is draft wording changes to section 3.6 of the draft HTTP 1.1\n>spec to allow future extensions to chunked transfer encoding, such as\n>digital signatures, etc. Changes are marked with change bars (manually\n>generated, so 100% guarantees can not be made...)\n>\n>If there are objections, please let me know; otherwise, we will\n>conclude that sufficient consensus exists to close out this issue.\n>--------\n>\n>3.6  Transfer Codings\n>\n>   Transfer coding values are used to indicate an encoding \n>   transformation that has been, can be, or may need to be applied to \n>   an Entity-Body in order to ensure safe transport through the \n>   network. This differs from a content coding in that the transfer \n>   coding is a property of the message, not of the original resource.\n>\n>|      transfer-coding         = \"chunked\" | transfer-extension\n>|      transfer-extension      = token\n>\n>   All transfer-coding values are case-insensitive. HTTP/1.1 uses \n>   transfer coding values in the Transfer-Encoding header field \n>   (Section 10.39).\n>\n>   Transfer codings are analogous to the Content-Transfer-Encoding \n>   values of MIME [7], which were designed to enable safe transport of \n>   binary data over a 7-bit transport service. However, \"safe \n>   transport\" has a different focus for an 8bit-clean transfer \n>   protocol. In HTTP, the only unsafe characteristic of message bodies \n>   is the difficulty in determining the exact body length \n>   (Section 7.2.2), or the desire to encrypt data over a shared \n>   transport.\n>\n> | All HTTP/1.1 applications must be able to receive and decode the \n> | \"chunked\" transfer coding, and ignore chunked extensions they do\n> | not understand. The chunked encoding modifies the body \n>   of a message in order to transfer it as a series of chunks, each \n>   with its own size indicator, followed by an optional footer \n>   containing entity-header fields. This allows dynamically-produced \n>   content to be transferred along with the information necessary for \n>   the recipient to verify that it has received the full message.\n>\n>       Chunked-Body   = *chunk\n>                        \"0\" CRLF\n>                        footer\n>                        CRLF\n>\n>|      chunk          = chunk-size [ chunk-ext ] CRLF\n>                        chunk-data CRLF\n>\n>       chunk-size     = hex-no-zero *HEX\n>|      chunk-ext      = *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] )\n>|      chunk-ext-name = token\n>|      chunk-ext-val  = token | quoted-string\n>       chunk-data     = chunk-size(OCTET)\n>\n>   |      footer         = *<Content-MD5 and future headers that\n>specify\n>|                         they are allowed in footer>\n>\n>       hex-no-zero    = <HEX excluding \"0\">\n>\n>   Note that the chunks are ended by a zero-sized chunk, followed by \n>   the footer and terminated by an empty line. An example process for \n>   decoding a Chunked-Body is presented in Appendix C.5.\n>\n>----------------------------------------------------\n>Paul J. Leach            Email: paulle@microsoft.com\n>Microsoft                Phone: 1-206-882-8080\n>1 Microsoft Way          Fax:   1-206-936-7329\n>Redmond, WA 98052\n>\n>\n\n\n\n", "id": "lists-010-1515090"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "On Mar 18,  1:17am, Yaron Goland wrote:\n> Subject: RE: Unverifiable Transactions / Cookie draft\n> There is an interesting assumption being made that protocols have the\n> right to dictate user interface to software makers. Am I the only one\n> who finds this development disturbing? Not because I am overly concerned\n> about protocols dictating UI, the protocol will be roundly ignored and\n> compliance will be coincidental at best, but rather that by dictating\n> requirements in areas clearly beyond the scope of a wire protocol, the\n> authority of the protocol group is lessened.\n>\n> Yaron\n\nIETF working groups are not limited to wire protocols, and the\nclose relationship in the past between the User Services Directorate\nand the Applications Directorate reflects the idea that the use\nof the protocols must be taken into account in their design.  Ultimately\nit is up to the Area Directors and the IESG to make a call on whether\na particular requirement belongs in a protocol.  By advancing this\nproposal, I believe that they have agreed that the requirement belongs.\n\nIt is possible, of course, that had they heard this discussion earlier\nthat they would have had different opinions, but the ability to\ntime travel is not yet a requirement to become an Area Director.\n\nregards,\nTed Hardie\nNASA NIC\n\nNB:  I am not in this message speaking for NASA.\n\n\n-- \n\n\n\n", "id": "lists-010-15160372"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "I think it is important to remember that what DoubleClick, FocalLink,\nand GlobalTrack use cookies for is to deliver controllable advertising.\n\nAdvertising is what will pay for most of the useful services on the\nweb.  I think most people recognize this now.  It is important to \nadvertisers to be able to know the number of unique individuals who\nsee their message and to be able to control it.  (eg:  show this ad\nthree times to each person)\n\nOne does not have to know who the user is to accomplish this.  All one \nneeds to know is that they are the same person that was already shown\nthis ad three times so we should show another one now.\n\nThere is no need to violate anybody's privacy to achieve this goal.  This\nis in fact exactly what is achieved with a serial-number cookie.  Now, \nif you take away the auto-cookie capability, sites will be forced to \nrequire users to register and \"login\" to get this kind of control.\n\nThe \"login\" model is a serious step back in privacy.  Suddenly, we not \nonly know it is the same person that was here earlier, we know it is a \nparticular person with a particular email address etc.\n\nThe cookie method is more likely to remain anonymous since it is \nactually easier to administer anonymously than with a known identity\nfor each user.  The \"login\" method on the other hand is easier to administer\nif you require the users to identify themselves.  Given that \"more information \nis always better\" to an advertiser, most sites using the \"login\" method will \nfall to the temptation of requiring all kinds of personal information \nfrom their users to grant access.  (eg: income, address, etc.)\n\nThe inherint convenience in the \"anonymous cookie\" method has driven the\nmarket so far toward a much more anonyous method of controlling advertising\ndelivery.  If you take that away, get ready to register at every \nuseful site and give up all semblence of privacy.\n\nDo you actually think all of these sites will continue to provide these\nextremely valuable and *expensive to operate* services if they can't\nprovide highly controllable and measurable advertising?  If you respond\nthat sites will simply \"revert to a pay-per-view subscription model\" you\nare really missing the point.  The pay-per-view folks always get a ton\nof personal information on you and then *sell it directly to other\npeople*.  Have you ever noticed that every time you order something from\na catalog, new catalogs from 5 other companies suddenly arrive at your\ndoor two weeks later?  \n\nSwitching to a user registration model always cedes more privacy than \n\"anonymous tagging\".\n\nmore comments in the text below...\n\nOn Fri, 14 Mar 1997, M Hedlund wrote:\n> On Fri, 14 Mar 1997, Yaron Goland wrote:\n> \n> > Rather my point is that I do not believe that you have helped protect\n> > user privacy [...]\n> \n> Okay, well we disagree on this.  Besides, if you are right and there is no\n> privacy protection, then why make any changes?  Doubleclick can simply use\n> something other than cookies!  If we afford no privacy protection in this\n> draft, then we do no harm to Doubleclick (and any other similar\n> businesses).\n> \n> > [...]  but I do believe that you have hurt a lot of smaller web\n> > sites who are trying to make a living on the web and thus contributed to\n> > the reduction of diversity on the web. I believe that the outcome is\n> > undesirable.\n> \n> I suspect that the number of businesses who have based their whole revenue\n> model on cookie sharing is extremely low, and that no such outcome will\n> occur.\n> \n\nAny site that lives off of advertising will soon depend heavily on cookies\nfor their whole revenue model.  In case you haven't noticed, this includes\nbasically *all* of the most useful resources on the web.\n\nNow, it is true that a really large site can afford put in an ad management\nsystem of their own and you'll still have the \"anonymous cookie\" method\nin wide use (but *only* at large well-funded sites).\n\nSites that cannot afford to create their own ad management systems would\ngo out of business in the face of competion of larger sites with much\nbetter advertising control.  Their only choice is to join a network\nof centralized advertising delivery because it is too expensive for them\nto sell their advertising directly themselves.\n\nNuking centralized ad management is indeed nuking smaller advertising\nsupported websites and only those sites.\n\nBut take heart, they can always switch to the subscription model and\nthen sell other people your address and annual income.\n\nSteve Madere\n\n> \n> M. hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15169129"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "On Tue, 18 Mar 1997, Yaron Goland wrote:\n> There is an interesting assumption being made that protocols have the\n> right to dictate user interface to software makers. Am I the only one\n> who finds this development disturbing?\n\nI agree with you that UI should not be dictated by this spec.  Lou Montulli\nfrom Netscape agreed with you too (as I understood his position)  and as a\nresult we tried to remove such dicta from the spec.  Lou's comments\ndirectly resulted in the wording of section 7.1:\n\n> 7.1  User Agent Control\n>\n>   [...] This state\n>   management specification therefore requires that a user agent give\n>   the user control over such a possible intrusion, although the\n>   interface through which the user is given this control is left\n>   unspecified.\n\nWe felt it important to give the user control of their cookie database, but\nat Lou's behest we did not try to specify a particular UI for this control. \nInstead, we tried to specify requirements for browser capabilities, while\nleaving the UI up to the browser vendor. \n\nIf you are arguing that we should not have tried to specify requirements\nfor browser capabilities at all, on that point I disagree.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15181846"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "On Sun, 16 Mar 1997, Larry Masinter wrote (re: rationale for privacy\nrequirements):\n> Given how often this issue seems to be raised, do you think it would\n> help to have a more explicit rationale for this design beyond the mail\n> messages and and the blow-by-blow commentary of the minutes?\n\nI've thought about this a fair amount since receiving this message, but I\nam satisfied with the language of section 7.1 (Privacy: User Agent\nControl).  It doesn't seem to me that the dispute arises from confusion\nover the intent of the draft; instead, we seem to be arguing over the\nappropriateness of that intent.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15190945"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "On Mar 18, 11:32am, Steve Madere wrote:\n> Subject: RE: Unverifiable Transactions / Cookie draft\n>\n> I think it is important to remember that what DoubleClick, FocalLink,\n> and GlobalTrack use cookies for is to deliver controllable advertising.\n>\n> Advertising is what will pay for most of the useful services on the\n> web.  I think most people recognize this now.  It is important to\n> advertisers to be able to know the number of unique individuals who\n> see their message and to be able to control it.  (eg:  show this ad\n> three times to each person)\n>\n> One does not have to know who the user is to accomplish this.  All one\n> needs to know is that they are the same person that was already shown\n> this ad three times so we should show another one now.\n>\n> There is no need to violate anybody's privacy to achieve this goal.  This\n> is in fact exactly what is achieved with a serial-number cookie.  Now,\n> if you take away the auto-cookie capability, sites will be forced to\n> require users to register and \"login\" to get this kind of control.\n>\n\nThere is nothing in the spec which forces you back to a \"login\" method\nto track unique users.  The spec requires that you make sure that the\nsame cookies are not used across domains; it allows you to do what you\nwant *within a domain*.  If you want to make sure that a dejanews user\nsees an advert three times and then gets moved to a different ad, you\nare welcome to use cookies to do so.\n\nIf you wish to use cookies to make sure that a user sees an advert\nthree times *at any site* before moving on to a different ad, sorry.\nBecause the use of the same cookie across sites allows for the\ncreation of very invasive clicktrail analysis, that has been ruled out.\nthere is\nsomething which says that if you are using cookies, you need to manage\nthem within your domain.\n\n\n\n> Any site that lives off of advertising will soon depend heavily on cookies\n> for their whole revenue model.  In case you haven't noticed, this includes\n> basically *all* of the most useful resources on the web.\n\nI might dispute whether the most resources on the web are advertising\nsupported or not, but I won't.  What I will dispute is the idea that\ncookie-based advertising *requires* third-party management of the\ncookies.  It doesn't.  There are several other ways of doing this,\nmost of which have already been given by others in this thread.\nYou presume in what follows this section that sites which do not\nuse third-party management systems will have to homegrow a system,\nsell their own advertisements, and die of the increased costs.\n\nThis is one heck of a presumption.  I will counter presume that\na development effort to provide a workable and auditable management\nsystem for small sites will produce a reasonably-priced or\nfree management system within a year, and that small scale sites\nwill be able to create an electronic market for their own goods,\nthus avoiding ceding part of their profits to third-party management\nsystems.\n\nBoth scenarios are possible--and your participation in the development\nefforts could significantly increase the likelihood of one or the\nother outcome.  You choose.\n\nregards,\nTed Hardie\nNASA NIC\n\n\n\n\n-- \n\n\n\n", "id": "lists-010-15199733"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "Will talk to you soon about this subject\n\n\n\n", "id": "lists-010-15210360"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "On Tue, 18 Mar 1997, Steve Madere wrote:\n> if you take away the auto-cookie capability, sites will be forced to \n> require users to register and \"login\" to get this kind of control.\n\nAbsolutely not.  I refuse to write a first draft business plan for the\nother methods that would prevent this from being true, but others have\nalready provided the general idea.  Your assertion is not supportable.\n\n> Do you actually think all of these sites will continue to provide these\n> extremely valuable and *expensive to operate* services if they can't\n> provide highly controllable and measurable advertising?\n\nIt seems to have worked for television, radio, and many forms of print\nmedia for quite a while.  Somehow, I have confidence that the Web and its\nrevenue models will survive this RFC.\n\n> Nuking centralized ad management is indeed nuking smaller advertising\n> supported websites and only those sites.\n\nAgain, I think you underestimate the resourcefulness of the marketing\ncommunity (of which I am a part!).  This provision of the RFC is trying to\nmaintain the privacy protections users have come to expect, and alert them\nto collection that occurs above and beyond their expectations. \n\n[Okay, enough from me on this topic.  Further general discussions of the\none-to-one future seem to be out of the scope of this WG and should\nprobably be redirected to www-talk, unless there really is consensus that\nthe draft needs revision.  If so, I would suggest we focus on Dan Jaye's\nsuggestion and my response.]\n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15217716"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": ">>>>> \"SM\" == Steve Madere <madere@dejanews.com> writes:\n\nSM> I think it is important to remember that what DoubleClick, FocalLink,\nSM> and GlobalTrack use cookies for is to deliver controllable advertising.\n\n  I believe that the archives of this list show that there is ample\n  awareness of this - what is in dispute is whether or not that is a\n  good thing.\n\nSM> Advertising is what will pay for most of the useful services on the\nSM> web.  I think most people recognize this now.\n\n  I certainly recognize no such thing.  The web is so new that almost\n  nothing can be said about its future with certainty.  There are a\n  good many people putting a lot of work into micro-payment\n  technologies, for example, who clearly believe that other revenue\n  models are viable.\n\n  Even if it were true, the concerns expressed by contributors to this\n  discussion are every bit as legitimate an element of this discussion\n  as any commercial interest.  I dare say that the perception of\n  reasonable privacy and the potential for control of personal\n  information will have as much to do with the continued acceptance of\n  the web as the revenue expectations of advertisers.  Would you like\n  the list of all the videos you've looked at on store shelves to be\n  available?  How about the list of all the videos Bill Clinton has\n  ever looked at?\n\nSM> It is important to advertisers to be able to know the number of\nSM> unique individuals who see their message and to be able to control\nSM> it.  (eg: show this ad three times to each person)\n\n  I find this claim amazing.\n\n  First, advertisers have never had this ability before.  No other\n  major advertising medium: newspapers, magazines, billboards, or\n  television (all of which have traditionally been paid for primarily\n  by advertising) provide any such information; they provide at best\n  very rough estimates.  I certainly wish that I could ensure that I\n  would never see certain television advertisements again.\n\n  Second, the claim that agencies have this capability with current\n  technology has been amply debunked here and elsewhere.  If any\n  business is making claims to advertisers today that they are\n  providing any such count without a large margin of error, then they\n  are either mistaken or not being completely honest.\n\nSM> One does not have to know who the user is to accomplish this.  All one\nSM> needs to know is that they are the same person that was already shown\nSM> this ad three times so we should show another one now.\n\n  To assume that advertisers will not correlate identity information\n  received as a part of some transaction with the cookie serial number\n  is, IMHO, naive.\n\nSM> [...] The \"login\" method on the other hand is easier to administer\nSM> if you require the users to identify themselves.  Given that \"more\nSM> information is always better\" to an advertiser, most sites using\nSM> the \"login\" method will fall to the temptation of requiring all\nSM> kinds of personal information from their users to grant access.\nSM> (eg: income, address, etc.) ... get ready to register at every\nSM> useful site and give up all semblance of privacy.\n\n  At least then the user will be aware of who is collecting what\n  information and be able to make their choices accordingly - I know\n  which institutions will get my business.\n\n  I believe that those who are advocating this change are doing so\n  simply to reduce their own costs of doing business; as at least one\n  person has pointed out, the agencies can continue to collect cookie\n  data by providing the sites using their ad stream with programs to\n  forward the appropriate information.  In such a case I would at\n  least know to whom I have given data and can make my choices\n  accordingly.\n\n--\nScott Lawrence             Principal Engineer        <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n", "id": "lists-010-15227110"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "[I believe my earlier comments make clear that I disagree with some of\nBrian's positions.  I will try to limit these comments to new or\nparticularly murky points.]\n\nOn Tue, 18 Mar 1997, Brian Behlendorf wrote:\n> Looking to technology for a solution to the privacy problems is\n> as misguided as blaming technology for causing them.  \n\nBrian, I would (in all seriousness) like to have a long discussion with you\non this assertion sometime in the future, but I don't believe it is\nrelevant to the draft we are discussing.  I don't think anyone is trying to\nsolve the Web's privacy problems with this draft; and if we were, I would\nagree with Yaron that such a goal would be way out of the scope of this WG.\nInstead, I believe we are trying to avoid _creating_ privacy problems in\nthe HTTP spec. \n\n> The fact that my opinion can be different today suggests it may be too\n> early to try and coerce the protocols into implementing policy...\n\nWhat's \"policy\"?  I don't consider this \"policy\" discussion to be all that\ndifferent in _scope_ from the discussions we had last year, in which many\nWG members asserted that creating a scalable caching mechanism was the most\nimportant work before us.  Is the HTTP spec implementable between one\nbrowser and one server without a solid caching mechanism?  Sure -- but to\nleave it at that would ignore the realities of bandwidth depletion and\nnetwork congestion that HTTP's implementation has, in part, caused.  So the\ngroup let a \"policy\" question -- in this case, resource depletion -- guide\nits work. \n\nMy point is not that caching (bandwidth conservation) and cookies (privacy)\nare indistinguishable -- obviously, there are many dinstinctions.  Instead,\nI am arguing that it is specious to dismiss privacy protections as\n\"implementing policy.\"  We should not retreat from a privacy issue simply\nbecause it seems a broader concern than the specification of working code.  \n\n[proposal:] \n> 1) By default, user agents are configured to prompt for confirmation on \n>      receipt of all cookies - unlike today's defaults which is to accept\n>      all cookies.\n> 2) Upon receipt of a cookie, UI must have the options:\n>      Accept this cookie\n>      Do not accept this cookie\n>      Accept all cookies from this domain\n>      Never accept cookies from this domain\n> 3) UI allows for some indication of pages with content inlined from other\n>      servers.  Perhaps specially flag cookie requests for inlined content too.\n\nI think in the above you are doing what Yaron is asking that we avoid:\nspecifying a UI in a wire protocol.  I would agree with him that the above\nis too restrictive. \n\n> 4) Remove the restriction on unverified transactions.\n> \n> Show me this leads to /less/ privacy, particularly in combination with all the\n> other existing provisions of the current spec.\n\nLess privacy than what?  The current implementations from the Netscape\ncookie proposal?  Perhaps not. The implementations that would arise from\ncompliance with the draft RFC?  I would argue that (4) above would\nsubstantially reduce privacy protection for the user if compared to the\ncurrent draft.  You are arguing for an \"after-the-fact\" notification (3),\nwhich I don't think is sufficient.\n\n> I really wish this was a more clear-cut situation - that cookies really did\n> make the difference between being \"database-able\" or not.  It doesn't. \n\nI agree that it doesn't _solve_ the problem of \"databasability.\"  I\ndisagree that anyone is trying to do that. \n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15238913"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "On Fri, 14 Mar 1997, Jaye, Dan wrote:\n> I would like to suggest that we provide a mechanism, similar to a\n> Certificate Authority, that would allow for a \"unverifiable transaction\"\n> to be verified against a list of valid site certificates.  These\n> certificates could be assigned levels, perhaps using the E-TRUST\n> trustmarks, and users could select their privacy level according to\n> those trustmarks.  The default behavior could be for the cookies to be\n> rejected from all non-verifiable transactions except for ETrust Level 3\n> (i.e., anonymous) site certificates.\n\nI agree that this is a fine suggestion.  How about changing section 4.3.5,\nparagraph 1, sentence 4, from:\n\n> A transaction is verifiable if the user has the option to review the\n> request-URI prior to its use in the transaction. \n\nto:\n\n> A transaction is verifiable if the user _or a user-designated agent_\n> has the option to review the request-URI prior to its use in the\n> transaction. \n\n(emphasis for review purposes).\n\nWould that give the specification sufficient flexibility for your\nrecommendation to be implemented?\n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15250738"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": ">The \"login\" model is a serious step back in privacy.  Suddenly, we not \n>only know it is the same person that was here earlier, we know it is a \n>particular person with a particular email address etc.\n\nI disagree. The user is at least aware that they are revealing information\nabout themselves. What is so offensive about cookies is that no steps\nwere taken to inform the user of their implications. Indeed little effort\nappears to have gone into thinking through those implications.\n\nAttempting to bludgeon the user into accepting cookies by putting up\na noisy dismiss box each time it is recieved is a fraudulent method\nof providing \"choice\". The user is forced to \"pay\" by dismissing the box\neach time. There should either be a switch to turn them off entirely\nor some means of selecting which to allow and which not to.\n\nI regard the various promisses from the vendors on this as sophistry.\nIf they were worried about user privacy then they would have implemented\nthis long ago.\n\nI'll just point out that Microsoft voluntarily entered into an agreement to\nabide by the terms of the European data privacy laws. I don't believe that\ncookies meet those laws. \n\nPhill\n\n\n\n", "id": "lists-010-15261061"}, {"subject": "Re: (ACCEPT*) Draft text for Accept header", "content": "Koen wrote:\n\n% If you have comments on this text, now is the time to comment.  I\n% intend to close this issue at the end of the week.  This means that I\n% will send a last call for disagreement with perceived consensus,\n% together with a possibly improved version of the text below, in a few\n% days.\n\n%    If no Accept header is present, then it is assumed that the client\n% |  accepts all media types.  If Accept headers are present, and if the\n% |  resource cannot send a response which is acceptable according to\n% |  the Accept headers, then the server should send an error response\n% |  with the 413 (not acceptable) status code, though the sending of an\n% |  un-acceptable response is also allowed.\n\nI don't agree with this: let's suppose I want an audio file, but I can\nread only .wav and no .au . Should I get the whole .au file and then notice\nthat it is unreadable for me? It does not seem sensitive, especially\nsince there is already a default to get any type (namely, no Accept line\nat all)\n\nIs it possible at least to state things in such a way that in 1.2 the \nsituation will change (I don't know, maybe adding an Accept-Only\nheader if people believe that my view is stupid)? After all, we have\nalready a SHOULD, so it should not be that difficult to promote it to\na MUST in the next version...\n\n.mau.\n\n\n\n", "id": "lists-010-1526481"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": ">On Tue, 18 Mar 1997, Steve Madere wrote:\n>> if you take away the auto-cookie capability, sites will be forced to \n>> require users to register and \"login\" to get this kind of control.\n\n>Absolutely not.  I refuse to write a first draft business plan for the\n>other methods that would prevent this from being true, but others have\n>already provided the general idea.  Your assertion is not supportable.\n\nI propose that we do away with auto cookies altogether. The idea is\nboth obnoxious and unnecessary. They are a very unsatisfatory solution\nto the demographics problem in any case. The advertisers latched onto\nthem because that was all they had.\n\nI proposed an alternative mechanism over two years ago, an limited\nlinkability session identifier which would permit a site administrator\nto track users within a particular site but not across sites.\n\nI believe that this is the best compromise between personal privacy and \nsite administration. The disconnect between each page download\nis an artifact of the HTTP protocol. There are many legitimate reasons\nwhy a server needs to be able to track a user - generation of personalised\ncontent being one.\n\nThe privacy question could be settled in extremis by allowing the user\nto select which sites to send session IDs to and which to not send them \nto.\n\nNote that the implementation of the session ID requires no more than fifty\nlines of code if you are already using MD5 or SHA.\n\nI know that the vendors have wedged themselves into another idea but the\nfact we can't agree on it indicates to my view that it was a lousy idea in\nthe first place. But as with most IETF ideas the argument keeps being\nrepeated \"you should have brought this up months before\". I did, I\nwas fobbed off with the response \"we are only a few months from \nagrement\". Two years later I see no prospect of consensus. Let us\nface it cookies are a busted flush. They are considered so obnoxious \nthat one country (Germany) has passed privacy legislation in response \nto them. How anyone could imagine that a working group could\nreach consensus in this situation is beyone me.\n\nProposed: The cookie draft is written to reflect the privacy issues \nirrespective of the desires of demographic tracking companies.\n\nThe session ID draft is at:\n\nhttp://www.w3.org/pub/WWW/TR/WD-session-id.html\n\nI *REALLY* will be pissed if people again try to claim that the light is\nat the end of the tunnel and that its too late to change. The train has\ncrashed inside the tunnel and its too damn late to bore another. Lets\nat least consider taking the alternative route.\n\nPhill\n\n\n\n", "id": "lists-010-15270093"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "-----BEGIN PGP SIGNED MESSAGE-----\n\n[BTW, sorry to turn this into a religious debate, but this is an \nextremely important issue.]\n\nI am not against cookies. I am not against advertising using cookies.\nNothing in the draft makes it difficult to implement either. What I\nam against is facilitaing the violation of my privacy. Specifically,\nI should not be able to visit site X and be unwittingly tracked\nby site Y at the same time, unless I have explicitly chosen to allow \nsuch. I believe the current draft covers this nicely.\n\nSteve Madere wrote:\n> \n> I think it is important to remember that what DoubleClick, FocalLink,\n> and GlobalTrack use cookies for is to deliver controllable advertising.\n> \n> Advertising is what will pay for most of the useful services on the\n> web.  I think most people recognize this now.  It is important to\n> advertisers to be able to know the number of unique individuals who\n> see their message and to be able to control it.  (eg:  show this ad\n> three times to each person)\n\n- From what basis do you make this claim? I certainly do not recognize\nthat. This group is not here to cater to the interests of advertisers\nbecause they whine about not being able to implement privacy-crippling\napplications simply and easily. The potential misuse of advertising\ndata is astronomical in proportion, and I as a user should not be\nrequired to be the guinea pig of the ad agency simply by browsing the\nweb. If I choose to give such data away, that's _my_ choice. We should\nnot make life difficult for the privacy-concerned user.\n\n> \n> One does not have to know who the user is to accomplish this.  All one\n> needs to know is that they are the same person that was already shown\n> this ad three times so we should show another one now.\n> \n> There is no need to violate anybody's privacy to achieve this goal.  \n> This is in fact exactly what is achieved with a serial-number cookie.  \n> Now, if you take away the auto-cookie capability, sites will be forced \n> to require users to register and \"login\" to get this kind of control.\n\nHow do you define \"a violation of privacy\"? I would _absolutely_ include\nin that definition any knowledge of my activities without my consent.\nWhether the consent be given explicitly or by choosing a policy which\nallows my user agent to give consent for me. Now obviously by\nvisiting a site I am in essence giving consent to _that_ site to\nknow my IP address. I have not agreed to have some other site obtain\nthat data.\n\nThe idea that tracking cookies doesn't give away information is\nwrong. I have an IP address. You have logs of my IP address at various\nsites around the web. You can quickly begin to identify patterns\nof behavior and tendencies without knowing my name. Usually,\nlearning more about someone is fairly easy based on IP address.\n\n> \n> The \"login\" model is a serious step back in privacy.  Suddenly, we not\n> only know it is the same person that was here earlier, we know it is a\n> particular person with a particular email address etc.\n\n\"Logins\" are explicitly consented to by the user. They type their\nusername, password, or whatever. Their personal information is\ngiven away by choice, not by default.\n\n> \n> The cookie method is more likely to remain anonymous since it is\n> actually easier to administer anonymously than with a known identity\n> for each user.  The \"login\" method on the other hand is easier to\n\nYour definition of anonymous is lacking.\n\n> administer if you require the users to identify themselves.  Given that \n> \"more information is always better\" to an advertiser, most sites using \n> the \"login\" method will fall to the temptation of requiring all kinds of \n> personal information from their users to grant access.  (eg: income, \n> address, etc.)\n> \n> The inherint convenience in the \"anonymous cookie\" method has driven the\n> market so far toward a much more anonyous method of controlling \n> advertising delivery.  If you take that away, get ready to register at \n> every useful site and give up all semblence of privacy.\n\nI have no problem giving my information to some sites. If they require\nthat information, and I choose not to give it, then I can't get in.\nBig deal. Again, my choice.\n\n> Sites that cannot afford to create their own ad management systems would\n> go out of business in the face of competion of larger sites with much\n> better advertising control.  Their only choice is to join a network\n> of centralized advertising delivery because it is too expensive for them\n> to sell their advertising directly themselves.\n\nNot so. They can purchase off-the-shelf ad management systems. They\njust can't participate easily in giving their ad data away \nautomagically without anyone's knowledge. The ad system has to\nactually go and give it away behind everyone's back by sending logs\nor whatever. At least then it's clear what is being done.\n\n> \n> Nuking centralized ad management is indeed nuking smaller advertising\n> supported websites and only those sites.\n> \n> But take heart, they can always switch to the subscription model and\n> then sell other people your address and annual income.\n\nThen I won't subscribe. The bottom line is it's _my_ choice, not theirs.\n\nJeremey.\n\n- -- \n=-----------------------------------------------------------------------= \nJeremey Barrett                                  VeriWeb Internet Corp.\nCrypto, Ecash, Commerce Systems                  http://www.veriweb.com/\n\nPGP Key fingerprint =  3B 42 1E D4 4B 17 0D 80  DC 59 6F 59 04 C3 83 64\n=-----------------------------------------------------------------------=\n\n-----BEGIN PGP SIGNATURE-----\nVersion: 2.6.2\n\niQCVAwUBMy7vxS/fy+vkqMxNAQFYnwQAuBXyOQQ+I/kAegjV25UPTWVzVXDwScAZ\n8piMME0WxRPPaI8V3CCOVFVl5Eyiti2iWmUcp1w7AZLVWXrQgZ5bn3VvRlGMUKuD\nj2E6K7r6U94qnzD7mO+n7nXl21gxib/ZTphkslrRSosnJJVqbtC/XlWqPcXBrjzC\nSxr/6y7khsg=\n=zB+4\n-----END PGP SIGNATURE-----\n\n\n\n", "id": "lists-010-15280960"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "Phillip M. Hallam-Baker wrote:\n> I'll just point out that Microsoft voluntarily entered into an agreement to\n> abide by the terms of the European data privacy laws. I don't believe that\n> cookies meet those laws. \n\nIf you are referring to the laws implemented by the Data Protection Act (DPA)\nin the UK, then I'd be surprised. The requirements of this law, in my limited\nunderstanding, are that, if one keeps \"personal data\" on a computer, then one\nmust register with the Data Protection Registrar, and be prepared to reveal, on\npayment of a fee, the contents of the relevant entry of that database to an\nindividual.\n\nI don't see that this has any impact on cookies (beyond requiring the collector\nof data to do things).\n\nI'm not aware of any other relevant legislation, current or forthcoming, in the\nUK, and, although I am not qualified to give an opinion, I do have good reason\nto be aware of such issues.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n", "id": "lists-010-15294619"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "You guys should really ease up on the conspiracy theories. Whenever \nyou call Pizza -Hut Dominoes or any big food delivery place your are \nsent (technically) a cookie. They know exactly what phone number you \nare calling from. They will not say that because it is against the \nlaw for them to tell you that they have your number, but it is legal \nfor them to have it.\n\nSecond whenever you get a driver's license you are sent \n(technically) a cookie. Where do you think Reader's Digest gets your \naddress to send you you entries to win $10,000,000. No one complains \nabout that. The truth is that it is very expensive to keep a site up, \nand advertising is the only way to go for bigger sites. So stop being \nparanoid, you have been receiving cookies way before you knew what a \nmodem looked like\n\n\n\n", "id": "lists-010-15303971"}, {"subject": "Cookies and cach", "content": "OK, so I haven't read every piece of mail on this list ... \n\nCan someone answer this simple question:\n\nIf I make a request for a page with a Cookie header, is the result \ncacheable in a public cache ?\n\ne.g. I pick up a cookie with path=/ from some org, then\nget a GIF (which has no Set-Cookie, Expires now, Cache-control, etc.), \ncan it be cached ?\n\nI think I had misunderstood the mechanism somewhat; if I want to use\nthe cookie in a page, should I make that page uncacheable with Expires, \nCache-Control, etc. ??\n\nAndrew\n\n\n\n", "id": "lists-010-15312417"}, {"subject": "Re[2]: Unverifiable Transactions / Cookie draf", "content": "Perhaps I am misunderstanding this issue, but it seems to me that these are the\nfacts:\n\n1) DoubleClick etc. use cookies to track users across domains\n2) This allows small sites to participate in selling ad space by just including\ninline image URLs that point to DoubleClick's ad server\n3) THIS permits small sites to generate ad revenue by merely adding some HTML to\ntheir pages\n4) The current cookie spec prevents this scheme\n5) DoubleClick, et al can continue to do what they do now, BUT they will need\ntheir member sites to run a CGI script that they would provide to track users.\n\nEffects:\n1) no increase in privacy - there's just a CGI-based mechanism instead of a\ncookie-based mechanism\n2) small sites have problems because they need more technical know-how, and more\nCPU power to run the DoubleClick ad tracking CGI on each ad view\n3) sites that actually reside on ISPs which don't allow CGIs also have problems\nand can no longer particpate\n\nSummary:\nYou have not helped privacy, you have just redistibuted income from small sites\nto big sites. Selfishly speaking, this to my advantage because I have a big\nsite, but for the general health and future of the web, it seems unfortunate.\n\nThere are a lot of small sites that make a few $100 a month from banner ads (NOT\nDoubleClick, but ad barter networks and small volume networks) and this revenue\nhelps support small specialty sites that otherwise would not exist. This\npromotes diversity on the web, which is perhaps beyond the scope of the goals of\nthe spec, but which we might agree is a \"good thing\".\n\n\n____________________Reply Separator____________________\nSubject:    Re: Unverifiable Transactions / Cookie draft\nAuthor: hardie@thornhill.arc.nasa.gov (Ted Hardie)\nDate:       3/18/97 9:58 AM\n\nOn Mar 18, 11:32am, Steve Madere wrote:\n> Subject: RE: Unverifiable Transactions / Cookie draft\n>\n> I think it is important to remember that what DoubleClick, FocalLink,\n> and GlobalTrack use cookies for is to deliver controllable advertising.\n>\n> Advertising is what will pay for most of the useful services on the\n> web.  I think most people recognize this now.  It is important to\n> advertisers to be able to know the number of unique individuals who\n> see their message and to be able to control it.  (eg:  show this ad\n> three times to each person)\n>\n> One does not have to know who the user is to accomplish this.  All one\n> needs to know is that they are the same person that was already shown\n> this ad three times so we should show another one now.\n>\n> There is no need to violate anybody's privacy to achieve this goal.  This\n> is in fact exactly what is achieved with a serial-number cookie.  Now,\n> if you take away the auto-cookie capability, sites will be forced to\n> require users to register and \"login\" to get this kind of control.\n>\n\nThere is nothing in the spec which forces you back to a \"login\" method\nto track unique users.  The spec requires that you make sure that the\nsame cookies are not used across domains; it allows you to do what you\nwant *within a domain*.  If you want to make sure that a dejanews user\nsees an advert three times and then gets moved to a different ad, you\nare welcome to use cookies to do so.\n\nIf you wish to use cookies to make sure that a user sees an advert\nthree times *at any site* before moving on to a different ad, sorry.\nBecause the use of the same cookie across sites allows for the\ncreation of very invasive clicktrail analysis, that has been ruled out.\nthere is\nsomething which says that if you are using cookies, you need to manage\nthem within your domain.\n\n\n\n> Any site that lives off of advertising will soon depend heavily on cookies\n> for their whole revenue model.  In case you haven't noticed, this includes\n> basically *all* of the most useful resources on the web.\n\nI might dispute whether the most resources on the web are advertising\nsupported or not, but I won't.  What I will dispute is the idea that\ncookie-based advertising *requires* third-party management of the\ncookies.  It doesn't.  There are several other ways of doing this,\nmost of which have already been given by others in this thread.\nYou presume in what follows this section that sites which do not\nuse third-party management systems will have to homegrow a system,\nsell their own advertisements, and die of the increased costs.\n\nThis is one heck of a presumption.  I will counter presume that\na development effort to provide a workable and auditable management\nsystem for small sites will produce a reasonably-priced or\nfree management system within a year, and that small scale sites\nwill be able to create an electronic market for their own goods,\nthus avoiding ceding part of their profits to third-party management\nsystems.\n\nBoth scenarios are possible--and your participation in the development\nefforts could significantly increase the likelihood of one or the\nother outcome.  You choose.\n\n                                regards,\n                                        Ted Hardie\n                                        NASA NIC\n\n\n\n\n-- \n\n\n\n", "id": "lists-010-15319923"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "You guys should really ease up on the conspiracy theories. Whenever \nyou call Pizza -Hut Dominoes or any big food delivery place your are \nsent (technically) a cookie. They know exactly what phone number you \nare calling from. They will not say that because it is against the \nlaw for them to tell you that they have your number, but it is legal \nfor them to have it.\n\nSecond whenever you get a driver's license you are sent \n(technically) a cookie. Where do you think Reader's Digest gets your \naddress to send you you entries to win $10,000,000. No one complains \nabout that. The truth is that it is very expensive to keep a site up, \nand advertising is the only way to go for bigger sites. So stop being \nparanoid, you have been receiving cookies way before you knew what a \nmodem looked like\n\n\n\n", "id": "lists-010-15332168"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "-----BEGIN PGP SIGNED MESSAGE-----\n\nSteven Sajous wrote:\n> \n> Second whenever you get a driver's license you are sent\n> (technically) a cookie. Where do you think Reader's Digest gets your\n> address to send you you entries to win $10,000,000. No one complains\n> about that. The truth is that it is very expensive to keep a site up,\n> and advertising is the only way to go for bigger sites. So stop being\n> paranoid, you have been receiving cookies way before you knew what a\n> modem looked like\n\nIt is obvious to me that the spec does not preclude advertising. \nRather than rewrite the same thing in other words, I will paste\nfrom one of Ted Hardie's messages, which is quite clear:\n\nTed Hardie wrote:\n> There is nothing in the spec which forces you back to a \"login\" method\n> to track unique users.  The spec requires that you make sure that the\n> same cookies are not used across domains; it allows you to do what you\n> want *within a domain*.  If you want to make sure that a dejanews user\n> sees an advert three times and then gets moved to a different ad, you\n> are welcome to use cookies to do so.\n> \n> If you wish to use cookies to make sure that a user sees an advert\n> three times *at any site* before moving on to a different ad, sorry.\n> Because the use of the same cookie across sites allows for the\n> creation of very invasive clicktrail analysis, that has been ruled out.\n> there is\n> something which says that if you are using cookies, you need to manage\n> them within your domain.\n> \n\nJeremey.\n\n- -- \nJeremey Barrett                                  VeriWeb Internet Corp.\nCrypto, Ecash, Commerce Systems                 http://www.veriweb.com/\nPGP key fingerprint =  3B 42 1E D4 4B 17 0D 80  DC 59 6F 59 04 C3 83 64\n\n-----BEGIN PGP SIGNATURE-----\nVersion: 2.6.2\n\niQCVAwUBMy8Gfy/fy+vkqMxNAQHKuAQAzrzcHJdg7wt8BspuutccYOsBbf/sE8u2\n5QeB3xk8yKtY6AOLzZRa1p7g5IRi0Hvv3qHROK019+mjCknx/gyofbJyeFDU1eli\n0VFL1ksj8pxoBrtv1Oo9HygzZHitJ6c1zisnH+tRyigDAZtECfvBiEJKdoWP7QGy\nMhWHiNvQkUo=\n=teIi\n-----END PGP SIGNATURE-----\n\n\n\n", "id": "lists-010-15340283"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "-----BEGIN PGP SIGNED MESSAGE-----\n\nSteven Sajous wrote:\n> \n> You guys should really ease up on the conspiracy theories. Whenever\n> you call Pizza -Hut Dominoes or any big food delivery place your are\n> sent (technically) a cookie. They know exactly what phone number you\n> are calling from. They will not say that because it is against the\n> law for them to tell you that they have your number, but it is legal\n> for them to have it.\n\nIt's not about conspiracies. Just because existing systems do not\npromote privacy is no reason to continue them, especially in a medium\nwhich _could_ be very conducive to privacy. I'm not willing to\nthrow privacy out the window because \"it's not out there now\". We\nshould be focused on improving matters not maintaining the status quo.\n\n> \n> Second whenever you get a driver's license you are sent\n> (technically) a cookie. Where do you think Reader's Digest gets your\n> address to send you you entries to win $10,000,000. No one complains\n> about that. The truth is that it is very expensive to keep a site up,\n> and advertising is the only way to go for bigger sites. So stop being\n> paranoid, you have been receiving cookies way before you knew what a\n> modem looked like\n\nGreat. Some systems sacrifice my privacy, so why not let the rest of\nthem do it too? I do not oppose advertising. I oppose _facilitating\nviolating a user's privacy without his/her consent_. Period. If they\nchoose to allow cookies from anywhere at anytime, so be it, I think\nthat should be an option in user agents. \n\nJeremey.\n\n- -- \nJeremey Barrett                                  VeriWeb Internet Corp.\nCrypto, Ecash, Commerce Systems                 http://www.veriweb.com/\nPGP key fingerprint =  3B 42 1E D4 4B 17 0D 80  DC 59 6F 59 04 C3 83 64\n\n-----BEGIN PGP SIGNATURE-----\nVersion: 2.6.2\n\niQCVAwUBMy8Exy/fy+vkqMxNAQGXcgQAw/KNCKvOZfMAS0eFmstoVchWoY0GXTon\n7CV+0+0eu361psx4+86OL0SPpjFw+tscb+6gHAtY4E3IUBtp3mXWg1HDQzoa5o/I\nld6h4Y449nyFW6eKKqKa6p9u2WPHu/WfQqYNsQokKzEa1bu/AnFN79CWLvRPbZe3\ncM1f6R48f0U=\n=/028\n-----END PGP SIGNATURE-----\n\n\n\n", "id": "lists-010-15349546"}, {"subject": "Consensus version of security considerations for basic authenticatio", "content": "It appears that there is consensus on the attached version of\nsecurity consdiderations section concerning basic authentication.\nThis was posted for comment on March 24 and there have been no \nsuggestions for alteration.  \n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n--------------------------------------------------------------------\n\n\n\n14.1 Authentication of Clients\n\nAs mentioned in Section 11.1, the Basic authentication scheme is not a\nsecure method of user authentication, nor does it in any way protect\nthe Entity-Body, which is transmitted in clear text across the\nphysical network used as the carrier. HTTP does not prevent additional\nauthentication schemes and encryption mechanisms from being employed\nto increase security or the addition of enhancements (such as schemes\nto use one-time passwords) to Basic authentication.\n\nThe most serious flaw in Basic authentication is that it results in\nthe essentially clear text transmission of the user's password over\nthe physical network.  It is this problem which Digest Authentication\nattempts to address.\n\nBecause Basic authentication involves the clear text transmission of\npasswords it should never be used (without enhancements) to protect\nsensitive or valuable information.\n\nA common use of Basic authentication is for identification purposes --\nrequiring the user to provide a username and password as a means of\nidentification, for example, for purposes of gathering accurate usage\nstatistics on a server.  When used in this way it is tempting to think\nthat there is no danger in its use if illicit access to the protected\ndocuments is not a major concern.  This is only correct if the server\nissues both username and password to the users and in particular does\nnot allow the user to choose his or her own password.  The danger\narises because naive users frequently reuse a single password to avoid\nthe task of maintaining multiple passwords.\n\nIf a server permits users to select their own passwords, then the\nthreat is not only illicit access to documents on the server but also\nillicit access to the accounts of all users who have chosen to use\ntheir account password.  If users are allowed to choose their own\npassword that also means the server must maintain files containing the\n(presumably encrypted) passwords.  Many of these may be the account\npasswords of users perhaps at distant sites.  The owner or\nadministrator of such a system could conceivably incur liability if\nthis information is not maintained in a secure fashion.\n\nBasic Authentication is also vulnerable to spoofing by counterfeit\nservers.  If a user can be led to believe that he is connecting to a\nhost containing information protected by basic authentication when in\nfact he is connecting to a hostile server or gateway then the attacker\ncan request a password, store it for later use, and feign an error.\nThis type of attack is not possible with Digest Authentication.\nServer implementors should guard against the possibility of this sort\nof counterfeiting by gateways or CGI scripts.  In particular it is\nvery dangerous for a server to simply turn over a connection to a\ngateway since that gateway can then use the persistant connection\nmechanism to engage in multiple transactions with the client while\nimpersonating the original server in a way that is not detectable by\nthe client.\n\n\n\n", "id": "lists-010-1535452"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "I think that this debate has been revisited sufficiently\nthat we're no longer making good progress. I am looking \nfor ways of wrapping up the discussion, rather than repeating\n(endlessly) arguments made and remade again.\n\nThere is a significant divergence of views, and many remain\nsteadfast that the original tradeoff in 2109 between privacy\nand capabilities are well considered.\n\nI think there's also a significant counter-opinion\ndeveloping. I would suggest -- as an alternative to\ncontinuing on the mailing list -- that we ask those\nwho would prefer to see a different resolution on the\nissue of unverifiable transactions to write a separate\ninternet draft, outlining what the protocol should say\ninstead and addressing the issue of user privacy to the\nsame detail as RFC 2109. Perhaps if we can see a complete\ndesign which adequately protects user privacy, we can\nconsider the alternative with sufficient technical detail.\n\nIs this OK? Can we close down discussion on this point until\nwe have a fully worked out alternative from someone?\n\nThanks,\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-010-15358812"}, {"subject": "Re:  Re[2]: Unverifiable Transactions / Cookie draf", "content": "Eric_Holstege@broder.com (Eric Holstege) wrote:\n  > Perhaps I am misunderstanding this issue, but it seems to me that these are the\n  > facts:\n  > \n  > 1) DoubleClick etc. use cookies to track users across domains\n  > 2) This allows small sites to participate in selling ad space by just including\n  > inline image URLs that point to DoubleClick's ad server\n  > 3) THIS permits small sites to generate ad revenue by merely adding some HTML to\n  > their pages\n  > 4) The current cookie spec prevents this scheme\n  > 5) DoubleClick, et al can continue to do what they do now, BUT they will need\n  > their member sites to run a CGI script that they would provide to track users.\n  > [...]\n\nJust to be clear about point (4):  the cookie spec does not *prevent*\nunverifiable transactions (the kind of transaction DoubleClick uses).\nThe spec says (4.3.5):\n\n    This restriction prevents a malicious service author from using\n    unverifiable transactions to induce a user agent to start or continue a\n    session with a server in a different domain.  The starting or\n    continuation of such sessions could be contrary to the privacy\n    expectations of the user, and could also be a security problem.\n\n    User agents may offer configurable options that allow the user agent, or\n    any autonomous programs that the user agent executes, to ignore the\n    above rule, so long as these override options default to ``off.''\n\nIn other words, unverifiable transactions are allowed, but a user has\nto take an active step to enable them.\n\nDave Kristol\n\n\n\n", "id": "lists-010-15366968"}, {"subject": "determining proxy reliabilit", "content": "(This was Re: Unverifiable Transactions / Cookie draft, but\nI think the topic has drifted far enough to merit a new Subject).\n\n    From: Patrick McManus <mcmanus@appliedtheory.com>\n    In a previous episode Ted Hardie said...\n    \n    :: Just to clarify, the proposals are to standardize a method\n    :: to *allow* proxies to report this kind of data.  Nothing in the\n    :: proposals *makes* anyone do anything.  Jeff and Paul\n    :: were very clear about that from the beginning, and it\n    :: keeps the hit-metering draft out of the scary\n    :: \"big-brother\" category.\n    \n    Right on.. and to clarify a little further when serving to a proxy\n    the origin server is told whether or not the proxy pledges to\n    return this information at a later date.. if it doesn't they can\n    cache bust.. the weakest point of the hit-metering draft IMHO is\n    that it doesn't try and provide any other methods of determing\n    proxy reliability wrt this pledge to base the \"to cache or to bust\"\n    decision on..\n    \nIt is true that there is no technical mechanism in the hit-metering\nproposal to prevent a proxy from agreeing to hit-meter a response,\nand then not doing so.  The proposal states MUST-level requirements,\nbut provides no means to verify that they are always observed.\n\nBut this is not any different from any other HTTP protocol requirement.\nFor example, an origin server can send\nCache-control: no-store\nto a proxy that identifies itself (in the request header) as\ncompliant with HTTP/1.1, but there is no way for the origin server\nto verify that the proxy actually obeys this directive.\n\nIf anyone can suggest \"other methods of determining proxy\nreliability with respect to this pledge\" (or any other pledge\nimplied by an HTTP/1.1 version number), I'd be interested.  But\nin general, this reduces to the problem of copy-protection in\nfully digital representations.  The only way that I know how to\nsolve this, in a network that spans administrative boundaries,\nis to use both end-to-end encryption and tamper-resistant\ndecryption hardware at the client end.  But this doesn't seem\neither feasible or desirable.\n\nThere are non-technical means to verify compliance (auditing,\nplanting fake information to trick people into exposing\ncopyright violations, etc.), but these are beyond the scope\nof a protocol specification.\n\n-Jeff\n\n\n\n", "id": "lists-010-15375397"}, {"subject": "RE: Cookies and cach", "content": "Microsoft's proxy caches responses from requests with cookie headers if\nno cache-control / Expires / pragma: no-cache / etc. is present in the\nresponse.\n\nWe've been operating this way since October and have seen no problems\nwith it...\n\nVinod Valloppillil\n\n-----Original Message-----\nFrom:Andrew Daviel [SMTP:andrew@andrew.Triumf.CA]\nSent:Tuesday, March 18, 1997 12:48 PM\nTo:http-wg@cuckoo.hpl.hp.com\nSubject:Cookies and cache\n\nOK, so I haven't read every piece of mail on this list ... \n\nCan someone answer this simple question:\n\nIf I make a request for a page with a Cookie header, is the\nresult \ncacheable in a public cache ?\n\ne.g. I pick up a cookie with path=/ from some org, then\nget a GIF (which has no Set-Cookie, Expires now, Cache-control,\netc.), \ncan it be cached ?\n\nI think I had misunderstood the mechanism somewhat; if I want to\nuse\nthe cookie in a page, should I make that page uncacheable with\nExpires, \nCache-Control, etc. ??\n\nAndrew\n\n\n\n", "id": "lists-010-15384899"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "On Tue, 18 Mar 1997, Steve Madere wrote:\n\n> Any site that lives off of advertising will soon depend heavily on cookies\n> for their whole revenue model.\n\nWhile I agree with some of what you're saying, this statement isn't\ncorrect. cookies are not a requirement for sites wanting to use\nadvertising to pay for the service.\n\nCookies are (currently) useful for advertising models, but they are\ncertainly not a necessity.\n\n--\nRob Hartill   Internet Movie Database (Ltd)\nhttp://us.imdb.com/Oscars/oscars_1996 -  hype free Oscars (R) info.\nhttp://us.imdb.com/usr/sweepstake     -  Win a 56k X2 modem. Free draw.\n\n\n\n", "id": "lists-010-15393941"}, {"subject": "305 Use prox", "content": "Below are notes and discussion points on 305 use proxy,\nplease read at your convenience, and voice your opinions..\n\nThis is a work item for memphis\n\n------\nComments, flames, death threats welcome..\n305 Use Proxy Response\n\nAfter some discussions, here are some thoughts and suggestions\non the use and implementation of the 305 use proxy response.\n\nOverview of suggestion:\n\n(For the example, Ill assume set-proxy: as the header name)\n(no bnf, Im going for concept... that can be discussed later )\n\nset-proxy : proxy-url scope=scopePrefix \ntype= once\n| forever \n| hits count=hitcount\n| lifetime=seconds\n\nfor GET http://www.foo.com/services/index.html HTTP/1.1\n\nexample response:\nHTTP/1.1 305 Use Proxy\nset-proxy : http://proxy1.foo.com:8080/ scope=http://www.foo.com/services/\n\nSuggested rules:\nOrigin servers may NOT send 305, only proxies may send them.\nThe set-proxy header is HOP BY HOP, not end to end.\n\nOn scope:\n  If the returned scope is 'wider' that the request minus the part\nof the path to the right of the final slash, the header should be\nrejected or the user should be queried at the least.\n\nExample:\nfor the above request ( http://www.foo.com/index.html )\n scope=http://www.foo.com/services/ VALID\n scope=http://www.foo.com/         INVALID\n\nSo basically:\nfor a client: ( depending on level of paranoia )\n (A)  if the scope is 'wider' than the requested URL, the user\nis queried.\n (B) if the scope is wider than the requested URL, and the destination\nproxy is not in the same domain, query the user\n\nNotes on discussions:\n\nCases for use:\n1. An origin server wishes to redirect a client to use a proxy\nto access its resources\n\n2. A proxy server wishes to redirect a client to another proxy.\n(the client can be another proxy )\n\nThe Current spec:\n10.3.6 305 Use Proxy\n\nThe requested resource MUST be accessed through the proxy given by the\nLocation field. The Location field gives the URL of the proxy. The\nrecipient is expected to repeat the request via the proxy.\n\n\nIssues:\n1) scope:\nDoes this apply to only this exact URL or any others\n\n2) validity time:\nShould the client use this proxy for this or these resources\nforever, or for how long, or how many transactions?\n\n3) Loops / hop counts\nWhat happens if proxy A redirects the client to proxy B which\nredirects it back to A?\n\n4) is Location: header enough?\nDoes the location allow enough flexibility to express some or\nall of these?\n\nIm impartial on the new header / extend location header issue, \nHowever, the security implications make the new header a\nprobable choice, IMHO.\n\n5) Security\nIf the scope is 'wide' ie for all HTTP transactions or even\njust for one domain, how does the client know to trust that the\nresponse was not from a malicious server.\n\nPossible solutions:\n\nScope: \nThe redirecting host should be able to indicate a mask\nfor urls which are to be redirected to this proxy.\nNaturally, this has security implications, ie an origin server\nwhich tells a client to redirect to an evil proxy for ALL urls.\nA safe suggestion I think it that that scope can be a prefix\nwhich may only affect 'less significant URLS'..\nExample:\n  Client requests http://www.ups.com/services/index.html\n  Server redirects to proxy1.ups.com\nallowed scope: http://www.ups.com/services/*\n    not allowed scope: http://www.ups.com/*\n    not allowed scope: http://www.mcom.com/*\n\nValidity Lifetime:\nWe couldnt come to a unanimous consenses here, except for \nthe fact that the current spec doesnt state anything about it.\n\nThe main ideas are:\n1) use this redirection forever ( or until the client is restarted )\n2) use this redirection just once, and come back the next time\n3) use this redirection for a specified amount of time\n4) use this redirection for a specified amount of transactions\n\nWhile its hard to come up with useful examples for all the cases,\nI beleive that a format which is flexible enough to allow any of them\nto be expresses is smartest.\n\nWhile caching and proxies have been in use for a non trivial amount of time,\ncomplex, hierarchical cache systems are only starting to be deployed.  \nBecause of this early stage, I feel that its best to keep as many\noptions open as possible, and give an much flexibility to administrators\nas possible.\n  \nLoops:\nOverall, this should be a solveable problem.  Intelligent\nICP type protocols should be able to avoid loops, but the idea\nof some sort of 'redirected via' flag/signal/indicator isnt unreasonable.\n\nLocation: header.\nIf people feel that we need to express a scope and a lifetime,\nwe'll need to either extend the Location: header or create a new one.\n\nextending Location:\nPresently, Location is defined as\n\n    Location       = \"Location\" \":\" absoluteURI\n\nwhich leaves it open for extensions, it doesnt clobber\nanything else.  The question is, though, will existing servers and clients\nchoke on additional fields..\n\nA new header:\nA new header gives us the most flexibility, not having to worry\nas much about the installed base.\n\nSecurity Implementation:\nDepending on your security stance, the implementation of this\nresponse could be considered unimplementable.  Its clear that this needs\nto be a hop-by-hop header, but that alone doesnt make the security problems\ngo away.  Since most proxies will forward any header to the client,\nthe client has no way to discern where the set-proxy header came from.\n\nIf the 305 is used by proxies to load balance, distribute cache, or failover,\nwide scopes are most beneficial, ie for ALL HTTP URLs or ALL URLS \nuse xyz proxy.  Unfortunately security implications make scope restriction\nnecessary.  Even if the 1.1 spec is modified to make set-proxy a \nhop-by-hop header, its easy for a malicious server to take advantage.\n\nWithout the scope restrictions, a malicious server can simply reply\nwith a 1.1 header, including a wide scope.  A 1.1 compliant proxy\nshould reject or supress this header, but an existing 1.0 or older\nproxy will happily forward this header to the 1.1 client. \nThis client would have no way of knowing if the 305 came from the\nproxy or the malicious origin server.\n\nUnfortunately, this makes the scope restrictions necessary.  At the\nsame time, it makes large scale load balancing or failover difficult,\nsince the a proxy can use this response to redirect a scope wider\nthan one host to another proxy.\n\nReasons for expressing scope, type and lifetime.\n\nscope:\nIt is wise to allow 305 to affect more than one single resource.\nIf not, a redirect would have to be sent for every subsequent request\nto a site.\n\nlifetime:\nThe rationale for expressing a lifetime, expressed in hits or\ntime, for a redirect's validity is in dispute.  As stated earlier,\nlarge cache hierarchies arent widespread, so its difficult to come\nup with real world examples.  I welcome discussion on this.  \nHowever, I will cite an example where it is useful.\n\nImagine a large organization with a complicated, multi-level cache\nhierarchy.  Client A normally talks to proxy A which is geographically\nclose to it.  Proxy A has some intelligence, lets say ICP, to route up\nthe proxy hierarchy through a few other proxies, and out to the origin\nserver. \nProxy A finds itself unable to contact its parent, or has stopped operating\ndue to cache garbage collection, or is for some reason, unable to help\nthe client.  Rather than stop all web traffic in proxy A's 'jurisdiction',\nproxy A uses the 305 to redirect client A to proxy B, which is far away\nand across a slow pipe.  This allows client A to continue to access\nweb based resources, but at a slower rate.\n\nWithout a lifetime associated with the redirect, it would be either\none-shot, or forever.  If it was one-shot, every client talking to proxy A\nwould attempt to retreive via proxy A and get redirected every time.\nIf it was a 'forever' then those clients would continue to use\nthe slow link and proxy forever, or at least until they restarted their\nclient and reverted to defaults.  \n\nBy associating a lifetime, a proxy can say \"Im in trouble, go use this\nother area proxy for 10 minutes, then check back with me.  If Im still\nin trouble, Ill redirect you again\".\n\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department       \nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n", "id": "lists-010-15401990"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "Rob Hartill wrote:\n\n  Cookies are (currently) useful for advertising models, but they are\n\n  certainly not a necessity.\n\nDespite what is implied above,  many ad delivery implementations\ncurrently \"require\" cookies to function correctly (e.g., sequence,\nimpression link).\nLet's not trivialize the reality of the current situation.   This\nstandard\nwill force thousands of web sites to change to support some new\nad delivery mechanism.\n\nPhillip A. Lindsay\nplindsay@pacbell.net\n\n\n\n", "id": "lists-010-15417910"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "Phillip Lindsay wrote:\n> \n> Rob Hartill wrote:\n> \n>   Cookies are (currently) useful for advertising models, but they are\n> \n>   certainly not a necessity.\n> \n> Despite what is implied above,  many ad delivery implementations\n> currently \"require\" cookies to function correctly (e.g., sequence,\n> impression link).\n\nIf the major browser vendors support this new standard as well as \nthey support the old standard we don't have to worry about cookie\nsupport going away for a very long time ;->\n\nthank you drive through\n-- \nGregory A. Meinke   | If I don't meet you no more in this world\nIMGIS, Inc.         | Then I'll see you in the next one.\ngmeinke@imgis.com   | Don't be late.  --J. Hendrix\n\n\n\n", "id": "lists-010-15426128"}, {"subject": "cookie draf", "content": "Let me start by saying, so there's no misunderstanding, Im \nagainst modifying the cookie draft to accomodate cross-domain sharing\nof cookies.\n\nThe opinions expressed on what is right or wrong are my personal\nviews, not necessarily Netscape's.\n\n> \n> It is important to advertisers to be able to know the number of\n> unique individuals who see their message and to be able to control\n> it.  (eg: show this ad three times to each person)\n> I think it is important to remember that what DoubleClick, FocalLink,\n> and GlobalTrack use cookies for is to deliver controllable advertising.\nWhile that ability may be extremely desireable by advertising firms,\nsuch as the ones already mentioned, this WG is not here to advance\nadvertising methods and technology.\n\nThis cross-domain sharing which essentially is a loophole of the\ncookie technology, provides something to advertisers which they\ndont have in other ad mediums.  By removing that functionality from\nthe draft, we are simple keeping the web in line with other advertising\nmediums.\n\nThe question of whether the cross-domain privacy issues are ethical \nis to be decided elsewhere, not in the wg, IMHO.  Just because the \ntechnology is available to give advertisers what is, at the least,\ncontroversial, doesnt mean we should.  \n\nIf anyone pins an ethical or moral 'duty' on this WG with regard to\nthis privacy issue, I beleive our duty is to take a conservative stance.\nBy keeping the information grabbing options available as written in the\nspecs closer to other advertising mediums, we are doing a 'good thing'.\n\nIf ad firms want to write additional tools to share that information\nacross domains, it is their perogative.  If new ground is to be\nforged in the level of information gathering, it is better for it\nto be done by the ad firms, than a loophole in the spec. \n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department                \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n", "id": "lists-010-15434381"}, {"subject": "Issues with the cookie draf", "content": "I went through this same debate on the DAV group when I made a\nsuggestion similar to Larry's. I was told, in no uncertain terms, that\ntelling people to go off and write their own spec is not the IETF way.\nRather it is the responsibility of the document editor to ensure that\nall comments are addressed to the satisfaction of the group. It is clear\nthat this is not the case. In order to help the document editor out I\nwill recap my major problems with the current specification. I hope\nothers who have issues with the specification will do the same.\n\n1. General Points:\n\nWhy are names beginning w/$ still reserved? As we have now defined the\nposition of NAME=VALUE, this restriction is no longer necessary.\n\nComment should be a language tagged Unicode string not a quoted string.\nThe actual language used can be implicitly negotiated on by the\naccept-language headers with the request. This is clearly not a robust\nsolution but it is probably appropriate to this situation.\n\nDiscard is entering dangerous territory. When exactly does a user agent\nterminate? Both MSIE 4.0 and NS 4.0 are moving to desk top models where\nthe user agent is operational as long as the computer is on.\nFurthermore, why would you want to discard a cookie when the user agent\nterminates? It sounds like this is an attempt to solve the problem of\nshared cash behavior. If the cookie is sensitive and if the cache is\nshared, we don't want the cookie hanging around. I think we should\nchange Discard to Private. Private would indicate that the cookie SHOULD\nonly be recorded if a private cache is in use.\n\nVersion should be optional, if not included, it should default to V1.\n\nThe default for Max-Age has the same \"how long is a UA session\" problem\nas Discard. IMHO the most robust solution is to have the cookie kept\nindefinitely if no Max-Age is included.\n\n2. Quotes & Responses:\n\nQuote:\n\"When it sends a\n     \"secure\" cookie back to a server, the user agent should use no less\n     than the same level of security as was used when it received the\n     cookie from the server.\"\n\nResponse:\nWhat is greater or lesser security? Do we expect clients to record what\nsecurity they were using when they received the cookie and then, through\nsome as yet undefined mechanism, decide what \"greater\" or \"lesser\"\nsecurity than the original security mechanism means? This definition is\ntoo fuzzy to be useful, I believe it should be removed.\nQuote:\n\"If an attribute appears more than once in a cookie, the behavior is\nundefined.\"\n\nResponse:\nUndefined things have a nasty habit of defining themselves. I propose\nthe sentence read \"If an attribute appears more than once in a cookie,\nthen the cookie is illegal and MUST be ignored.\" \n\nQuote:\n\"HTTP/1.1 servers must send Expires: old-date (where old-date is a date\nlong in the past) on responses containing Set-Cookie2 response headers\n|\nunless they know for certain (by out of band means) that there are no\ndownsteam(sic) HTTP/1.0 proxies..\" \n\nResponse:\nI believe this sentence should be changed to read \"HTTP/1.1 servers MUST\nsend Expires: old-date (where old-date is a date long in the past) on\nresponses containing Set-Cookie2 response headers meant for single users\nunless...\". We allow caching of Set-Cookie2 headers intended for\nmultiple people.\n\nQuote:\n\"   * The request-host is a FQDN (not IP address) and has the form HD,\n     where D is the value of the Domain attribute, and H is a string\n     that contains one or more dots.\"\n\nResponse:\nThe company Blah Inc. has the web site blah.com. Blah sells many\nproducts, one of which is called bar. Bar has been released in several\nversions, the newest of which is Foo. Blah wants to be able to present\ninformation to its customer that it thinks the customer will be\ninterested in and it wants to present this information across all of its\nsites. So it sends a cookie whose domain is .blah.com. If a user is\nvisiting foo.bar.blah.com and receives this cookie they will have to\nreject it because it violates the above rule. It is totally appropriate\nfor Blah Inc. to want to hand out cookies that apply to all the sites it\nowns. However instead of doing it simply by having a single cookie, it\nnow has to clutter the user's hard drive with cookies for every\n*.blah.com site visited, not to mention complicating the server's\nimplementation. I believe this requirement is not reasonable, especially\nfor complicated sites.\n\nQuote:\n\"User agents should allow the user to control cookie destruction....\" \n\nResponse:\nIf a UA maker wants to never allow a customer access to cookie control\nmechanisms, that is the UA maker's business, not the standards. We can\nnot threaten companies by saying \"Well if you don't create your\ninterface the way we say then you aren't compliant\" and expect to remain\ncredible as a standards organization. This is not a wire protocol\nrelated issue. It is a feature issue and a matter of competitive\nadvantage for UAs.\n\nQuote:\n\"   * The value for the $Domain attribute must be the value from the\n|\n     Domain attribute, if any, of the corresponding Set-Cookie2 response\n|\n     header.  Otherwise the attribute should be omitted from the Cookie2\n|\n     request header.\n|\n\n   * The value for the $Path attribute must be the value from the Path\n|\n     attribute, if any, of the corresponding Set-Cookie2 response\n|\n     header.  Otherwise the attribute should be omitted from the Cookie2\n|\n     request header\"\n\nResponse:\nAll cookies have Domain and Path values. When not explicitly defined\nthey are implicitly defined. Thus a user agent will record these values,\nexplicit or not. The above requirements now dictate that the UA has to\nrecord extra information, an indication if the Domain and Path are\nimplicit or explicit. I can find no good reason to place this\nrequirement on the UA. Instead we should simply require that the Domain\nand Path, explicit or not, should always be returned with the cookie.\n\nQuote:\n\"Domain Selection\n     The origin server's fully-qualified host name must domain-match the\n     Domain attribute of the cookie.  The origin server's port number\n|\n     must equal the port number of the server that sent the cookie.\"\n\nResponse:\nWhy do we have the port number requirement? If Blah Inc. has an HTTP\nserver on ports 80 and 81, why would we want to prevent sharing between\ntwo ports on the same system?\n\nQuote:\n\"If multiple cookies satisfy the criteria above, they are ordered in the\n|\nCookie2 header such that those with more specific Path attributes\nprecede those with less specific.  Ordering with respect to other\nattributes (e.g., Domain) is unspecified.\"\n\nResponse:\nIf we leave domain ordering undefined doesn't that sort of destroy the\nutility of requiring path ordering?\n\nQuote:\n\"User agents may offer configurable options that allow the user agent,\nor\nany autonomous programs that the user agent executes, to ignore the\nabove rule, so long as these override options default to ``off.''\"\n\nResponse:\nAgain, I do not feel it is appropriate for this specification to dictate\nto UA makers how to build the parts of their product that do not go over\nthe wire. If a UA maker wants this to default to \"ON\", that is their\nbusiness. If the UA maker wants to default to \"ON\" and not allow the\nuser to change the value, that is also their business. The mission, I\nhope, is interoperability, not second guessing UA makers.\n\nQuote:\n\"This state\nmanagement specification therefore requires that a user agent give the\nuser control over such a possible intrusion, although the interface\nthrough which the user is given this control is left unspecified.\nHowever, the control mechanisms provided shall at least allow the user\n\n   * to completely disable the sending and saving of cookies.\n\n   * to determine whether a stateful session is in progress.\n\n   * to control the saving of a cookie on the basis of the cookie's\n     Domain attribute.\"\n\nResponse:\nWire protocols have a massive effect on the range of functions a client\ncan implement. In effect, they restrict products. Software companies\nhave decided that interoperability is such an important product feature\nthat it is worth having their functionality restrained. However there is\nanother reason behind the software maker's behavior, they know that the\nreal battle is UI not features. Features tend to be a check-list, so\nlong as everyone has the same check marks, the competitive field remains\nflat. The area of competition becomes primarily one of interface. When\nstandards step beyond the wire, beyond even functionality, and go into\nthe area that is the heart of computer software, they render themselves\nirrelevant. Companies are not going to give up their competitive\nadvantage in order to be compliant with a standard. Worse yet, due to\npress pressures, companies will be forced to look like they are\ncompliant, even when they are not. This reduces the ability of the IETF\nto be an effective standards setting organization. Once companies are\nforced to selectively ignore standards the goal of interoperability\nbecomes impossible.\n\nYaron\n\n> -----Original Message-----\n> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n> Sent:Tuesday, March 18, 1997 12:16 PM\n> To:http working group\n> Subject:Re: Unverifiable Transactions / Cookie draft\n> \n> I think that this debate has been revisited sufficiently\n> that we're no longer making good progress. I am looking \n> for ways of wrapping up the discussion, rather than repeating\n> (endlessly) arguments made and remade again.\n> \n> There is a significant divergence of views, and many remain\n> steadfast that the original tradeoff in 2109 between privacy\n> and capabilities are well considered.\n> \n> I think there's also a significant counter-opinion\n> developing. I would suggest -- as an alternative to\n> continuing on the mailing list -- that we ask those\n> who would prefer to see a different resolution on the\n> issue of unverifiable transactions to write a separate\n> internet draft, outlining what the protocol should say\n> instead and addressing the issue of user privacy to the\n> same detail as RFC 2109. Perhaps if we can see a complete\n> design which adequately protects user privacy, we can\n> consider the alternative with sufficient technical detail.\n> \n> Is this OK? Can we close down discussion on this point until\n> we have a fully worked out alternative from someone?\n> \n> Thanks,\n> \n> Larry\n> -- \n> http://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-010-15442715"}, {"subject": "(CONTENT NEGOTIATION,VARY,ACCEPT*) Consensus on strateg", "content": "On Monday, I posted text about strategic decisions on the relation\nbetween content negotiation and the `main' HTTP/1.1 protocol as it\nwill be specified in the new 1.1 draft.  These decisions were agreed\non by the the editorial group working on the new HTTP/1.1 draft.\n\nThere have been not comments about strong disagreement with this\nstrategy, so I now declare that there appears to be rough consensus on\nnegotiation strategy.  I include the strategy text that was posted on\nMonday below.  Unless further comment is heard, this will close out\nthe strategy issue.\n\nKoen.\n\n--snip---\n\nStrategy decisions on the the relation between content negotiation and\n----------------------------------------------------------------------\nthe `main' HTTP/1.1 protocol\n----------------------------\n\nThe main goals of the strategy are 1) to avoid the risk that\ndisagreement about content negotiation would delay consensus on the 1.1\ndraft until after May 1, while 2) not blocking a smooth introduction\nof content negotiation after May 1.\n\nThe plans are as follows:\n\n- as announced earlier in a message by Jim Gettys, (Proposed structure\nof HTTP 1.1 document(s), Mon, 18 Mar 96 15:03:18 -0500), the content\nnegotiation text in draft-holtman-http-content-negotiation-00.txt will\nnot be merged with the main HTTP/1.1 document.  An updated version of\ndraft-holtman-http-content-negotiation-00.txt will appear in April,\nand it is hoped that consensus on it can be reached in May.\n\n- Section 12 of the old 1.1 draft (which gave an incomplete definition\nof a content negotiation mechanism) will not be present in the new 1.1\ndraft.\n\n- There will be a Vary header in the 1.1 draft, so that HTTP/1.1 can\nsupport opaque negotiation on language in a reasonably efficient\nway.\n\n- There will be Accept* headers in the 1.1 draft.  With respect to the\nAccept* headers previous draft, numerous small improvements are made;\nthese improvements reflect the consensus of the content negotiation\nsubgroup.\n\n- There will be `hooks' in the 1.1 draft to ensure that all HTTP/1.1\ncaches will be compatible, though not in an optimally efficient way,\nwith a transparent content negotiation mechanisms like the mechanism\ndefined in draft-holtman.  Thus, transparent content negotiation\n(which is what Section 12 of the old 1.1 draft covered incompletely)\nwon't have to wait for HTTP/1.2 if HTTP/1.2 turns out to take too\nlong, it can be done on top of HTTP/1.1. \n\n- The `hooks' for transparent content negotiation consist mainly of an\nAlternates header definition which defines the Alternates header as\nsynonymous with a certain Vary header.  Also, some language in the\ndraft will announce that a negotiation mechanism using Alternates is\nplanned.\n\n\n\n", "id": "lists-010-1546002"}, {"subject": "Re: 305 Use prox", "content": ">set-proxy : proxy-url scope=scopePrefix \n>type= once\n>| forever \n>| hits count=hitcount\n>| lifetime=seconds\n>\n>for GET http://www.foo.com/services/index.html HTTP/1.1\n\nWell, for starters, please use a syntax that can be unambiguously parsed.\nThat means enclose all URLs in \"double-quotes\" or <angle-brackets> if\nyou intend them to be used alongside parameters.  I suggest using a\nsyntax similar to Link in the RFC 2068 appendix, since it overcame the\nsame set of problems.\n\n>example response:\n>HTTP/1.1 305 Use Proxy\n>set-proxy : http://proxy1.foo.com:8080/ scope=http://www.foo.com/services/\n>\n>Suggested rules:\n>Origin servers may NOT send 305, only proxies may send them.\n\nNope.  The original intended purpose of 305 is to allow an origin server\nto prevent access unless it goes through the appropriate proxy.\n\n>The set-proxy header is HOP BY HOP, not end to end.\n\nYou mean that the 305 response is HOP by HOP -- it doesn't make any\nsense to just drop the header field.\n\n>On scope:\n>  If the returned scope is 'wider' that the request minus the part\n>of the path to the right of the final slash, the header should be\n>rejected or the user should be queried at the least.\n\nHmmm, looks like a realm, smells like a realm, why not call it a realm?\n\n>Example:\n>for the above request ( http://www.foo.com/index.html )\n> scope=http://www.foo.com/services/ VALID\n> scope=http://www.foo.com/         INVALID\n>\n>So basically:\n>for a client: ( depending on level of paranoia )\n> (A)  if the scope is 'wider' than the requested URL, the user\n>is queried.\n> (B) if the scope is wider than the requested URL, and the destination\n>proxy is not in the same domain, query the user\n\nSounds reasonable.\n\n>Notes on discussions:\n>\n>Cases for use:\n>1. An origin server wishes to redirect a client to use a proxy\n>to access its resources\n>\n>2. A proxy server wishes to redirect a client to another proxy.\n>(the client can be another proxy )\n\nYep.\n\n>The Current spec:\n>10.3.6 305 Use Proxy\n>\n>The requested resource MUST be accessed through the proxy given by the\n>Location field. The Location field gives the URL of the proxy. The\n>recipient is expected to repeat the request via the proxy.\n>\n>\n>Issues:\n>1) scope:\n>Does this apply to only this exact URL or any others\n\nThe original description only applied to the exact URL.  I agree that\na realm would be more efficient, subject to a good set of security\nconsiderations.\n\n>2) validity time:\n>Should the client use this proxy for this or these resources\n>forever, or for how long, or how many transactions?\n\nGood question.\n\n>3) Loops / hop counts\n>What happens if proxy A redirects the client to proxy B which\n>redirects it back to A?\n\nThe same that happens on all auto-redirection loops.  It is actually\nnoted in the section on 3xx responses.\n\n>4) is Location: header enough?\n>Does the location allow enough flexibility to express some or\n>all of these?\n\nProbably not 1 and 2.  3 is not needed.\n\n>Im impartial on the new header / extend location header issue, \n>However, the security implications make the new header a\n>probable choice, IMHO.\n>\n>5) Security\n>If the scope is 'wide' ie for all HTTP transactions or even\n>just for one domain, how does the client know to trust that the\n>response was not from a malicious server.\n\nThat would be the new consideration if we introduced scope/realm.\n\n>Possible solutions:\n>\n>Scope: \n>The redirecting host should be able to indicate a mask\n>for urls which are to be redirected to this proxy.\n>Naturally, this has security implications, ie an origin server\n>which tells a client to redirect to an evil proxy for ALL urls.\n>A safe suggestion I think it that that scope can be a prefix\n>which may only affect 'less significant URLS'..\n>Example:\n>  Client requests http://www.ups.com/services/index.html\n>  Server redirects to proxy1.ups.com\n>allowed scope: http://www.ups.com/services/*\n>    not allowed scope: http://www.ups.com/*\n>    not allowed scope: http://www.mcom.com/*\n\nKeep in mind that \"*\" is allowed in URLs.\n\n>Validity Lifetime:\n>We couldnt come to a unanimous consenses here, except for \n>the fact that the current spec doesnt state anything about it.\n>\n>The main ideas are:\n>1) use this redirection forever ( or until the client is restarted )\n>2) use this redirection just once, and come back the next time\n>3) use this redirection for a specified amount of time\n>4) use this redirection for a specified amount of transactions\n>\n>While its hard to come up with useful examples for all the cases,\n>I beleive that a format which is flexible enough to allow any of them\n>to be expresses is smartest.\n\nThat would be a good idea, yes.\n\n>While caching and proxies have been in use for a non trivial amount of time,\n>complex, hierarchical cache systems are only starting to be deployed.  \n>Because of this early stage, I feel that its best to keep as many\n>options open as possible, and give an much flexibility to administrators\n>as possible.\n>  \n>Loops:\n>Overall, this should be a solveable problem.  Intelligent\n>ICP type protocols should be able to avoid loops, but the idea\n>of some sort of 'redirected via' flag/signal/indicator isnt unreasonable.\n\nUmmm, the client can see for itself that it is looping.\n\n>Location: header.\n>If people feel that we need to express a scope and a lifetime,\n>we'll need to either extend the Location: header or create a new one.\n\nCreate a new one.  The problem with using the Location header was that\nsome older client might follow the HTTP rules and treat the response\nas if it were a 300, which would entice it to perform the original\nrequest on the URL in the Location header, which in this case would\nbe the base proxy URL.  I suggest using \"Proxy\", since it seems more\nadvisory in nature than \"set-proxy\".  Also, you should consider allowing\nthe server to send multiple fields so that the client can be given a\n\"choose one\" option.\n\n>Without the scope restrictions, a malicious server can simply reply\n>with a 1.1 header, including a wide scope.  A 1.1 compliant proxy\n>should reject or supress this header, but an existing 1.0 or older\n>proxy will happily forward this header to the 1.1 client. \n>This client would have no way of knowing if the 305 came from the\n>proxy or the malicious origin server.\n>\n>Unfortunately, this makes the scope restrictions necessary.  At the\n>same time, it makes large scale load balancing or failover difficult,\n>since the a proxy can use this response to redirect a scope wider\n>than one host to another proxy.\n\nWhy not include the notion of \"trusted\" proxies?  It should be possible\nfor a user agent to collect a list of trusted proxies in much the same\nway it would collect a list of trusted cookie sites, and there won't\nbe that many proxies for any given user agent.  The user agent could\nthen be set to accept direction automatically from trusted proxies,\nand query for all others.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-15462620"}, {"subject": "Re: Issues with the cookie draf", "content": "Yaron Goland wrote:\n> \n> I went through this same debate on the DAV group when I made a\n> suggestion similar to Larry's. I was told, in no uncertain terms, that\n> telling people to go off and write their own spec is not the IETF way.\n> Rather it is the responsibility of the document editor to ensure that\n> all comments are addressed to the satisfaction of the group. It is clear\n> that this is not the case. In order to help the document editor out I\n> will recap my major problems with the current specification. I hope\n> others who have issues with the specification will do the same.\n\nThe circumstances are considerably different.\n\nFirst, we're discussing a revision to a Proposed Standard which we\npassed\nthrough working group consensus, last call, and IESG review, after\nconsiderable\ndiscussion of the very same points that are being re-raised. It is that\nthis issues were not previously considered, it was considered\nat great length.\n\nSecondly, I am not suggesting that you go off and write your own\nprotocol,\nI am suggesting that you explicate your own point of view in an\nauxiliary\ndraft which explains how this particular element of the protocol should\nwork,\nand what the privacy and security implications are for that alternative.\nWe certainly would need to justify any change in position on the issue\nof\nprivacy and cookies from the one we've promoted over the last year, and\nuntil\nthat justification is written and the privacy considerations explained,\nwe\nwon't get past the IESG, much less the press.\n\nPersonally, I am skeptical that it is possible to deal with the privacy\nissues. However, on the mailing list, various people (including you)\nhave made rather forthright assertions that there is an alternative\nwhich\nprovides adequate(? equivalent? different but just as important?)\nprivacy\nguarantees. However, these details have been floating by in the middle\nof mail messages that also allude to the business models of the various\ncompanies that are engaged in advertising. If a separate proposal is\nwritten,\nwe'll be able to evaluate the privacy concerns independently of the\nbusiness\nconsiderations.\n\nSo, I will continue to call for volunteer(s) to write up an alternative\nproposal to Dave Kristol's soon-to-be-issued internet draft, and ask\nthat\nwe defer discussion of that particular issue until we have at least an\ninterim draft of an alternative that is claimed by its authors\nto deal with the requirements credibly. \n\nRegards,\n\nLarry\n(as HTTP-WG chair)\n\n\n\n", "id": "lists-010-15477911"}, {"subject": "RE: Issues with the cookie draf", "content": "My alternative proposal is to remove section 7 of the current draft and\nto make the other alterations I have specifically suggested in the rest\nof the post you referred to below. Would you like me to do a little\ncutting and pasting and actually make it into an I-D or should we\ncontinue to discuss the basic issue of how far this group should be\ngoing in its protocols?\n\nYaron\n\n> -----Original Message-----\n> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n> Sent:Tuesday, March 18, 1997 8:08 PM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: Issues with the cookie draft\n> \n> Yaron Goland wrote:\n> > \n> > I went through this same debate on the DAV group when I made a\n> > suggestion similar to Larry's. I was told, in no uncertain terms,\n> that\n> > telling people to go off and write their own spec is not the IETF\n> way.\n> > Rather it is the responsibility of the document editor to ensure\n> that\n> > all comments are addressed to the satisfaction of the group. It is\n> clear\n> > that this is not the case. In order to help the document editor out\n> I\n> > will recap my major problems with the current specification. I hope\n> > others who have issues with the specification will do the same.\n> \n> The circumstances are considerably different.\n> \n> First, we're discussing a revision to a Proposed Standard which we\n> passed\n> through working group consensus, last call, and IESG review, after\n> considerable\n> discussion of the very same points that are being re-raised. It is\n> that\n> this issues were not previously considered, it was considered\n> at great length.\n> \n> Secondly, I am not suggesting that you go off and write your own\n> protocol,\n> I am suggesting that you explicate your own point of view in an\n> auxiliary\n> draft which explains how this particular element of the protocol\n> should\n> work,\n> and what the privacy and security implications are for that\n> alternative.\n> We certainly would need to justify any change in position on the issue\n> of\n> privacy and cookies from the one we've promoted over the last year,\n> and\n> until\n> that justification is written and the privacy considerations\n> explained,\n> we\n> won't get past the IESG, much less the press.\n> \n> Personally, I am skeptical that it is possible to deal with the\n> privacy\n> issues. However, on the mailing list, various people (including you)\n> have made rather forthright assertions that there is an alternative\n> which\n> provides adequate(? equivalent? different but just as important?)\n> privacy\n> guarantees. However, these details have been floating by in the middle\n> of mail messages that also allude to the business models of the\n> various\n> companies that are engaged in advertising. If a separate proposal is\n> written,\n> we'll be able to evaluate the privacy concerns independently of the\n> business\n> considerations.\n> \n> So, I will continue to call for volunteer(s) to write up an\n> alternative\n> proposal to Dave Kristol's soon-to-be-issued internet draft, and ask\n> that\n> we defer discussion of that particular issue until we have at least an\n> interim draft of an alternative that is claimed by its authors\n> to deal with the requirements credibly. \n> \n> Regards,\n> \n> Larry\n> (as HTTP-WG chair)\n\n\n\n", "id": "lists-010-15487451"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "To:http-wg@cuckoo.hpl.hp.com\n\n\n\nOn Tue, 18 Mar 1997, Phillip Lindsay wrote:\n\n> Despite what is implied above,  many ad delivery implementations\n> currently \"require\" cookies to function correctly (e.g., sequence,\n> impression link).\n> Let's not trivialize the reality of the current situation.   This\n> standard\n> will force thousands of web sites to change to support some new\n> ad delivery mechanism.\n\nBy this statement you prove my concern justified. The RFC only requires\nthat users be made aware of unverifiable transactions. If making the\ngeneral user aware results in general rejection of such cookies by the\nuser community, then I believe the sites using such cookies are today\ndoing objectionable things with cookies and the requirement that the RFC\nremain as written is very important. \n\nIf cross domain cookies are not objectionable to the general user\npopulation, then they won't be rejected and no changes will be required to\ncontinue using such cookies.\n\nJust like the argument is made that UAs should be allowed to differentiate\nthemselves by cookie control features, I would argue that\ncookie-dispensing sites can differentiate themselves by doing the right\nkind of documentation and marketing to end users of the value to the end\nuser of their use of cookies and how they use the information obtained via\ncookies.  Knowing that DoubleClick has passed the ETrust (or whatever)\naudit/certification might convince me that they will not disclose their\ndata and I might allow their cookies.\n\nBut as its been said ... make the user aware, give them the choice.\n\nDave Morris\n\n\n\n", "id": "lists-010-15499212"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "To quote from the current cookie draft:\n      * to determine whether a stateful session is in progress.\n\n      * to control the saving of a cookie on the basis of the cookie's\n        Domain attribute.\n\nThe average user doesn't know about cookies and frankly they don't want\nto know about cookies. However browser implementers are now forced to\nput UI in front of them to tell them more than they ever cared to know\nabout cookies. Since the user doesn't know what a cookie is, they are\ngoing to call up to find out what the heck this cookie stuff is about.\nWho pays for that call? The browser implementer. So a wire protocol is\ntaking away browser implementers right to control the experience users\nhave with the very program the browser implementer is selling.\n\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Tuesday, March 18, 1997 8:08 AM\n> To:http-wg@cuckoo.hpl.hp.com\n> Cc:http working group\n> Subject:RE: Unverifiable Transactions / Cookie draft\n> \n> \n> \n> On Tue, 18 Mar 1997, Yaron Goland wrote:\n> \n> > There is an interesting assumption being made that protocols have\n> the\n> > right to dictate user interface to software makers. Am I the only\n> one\n> > who finds this development disturbing? Not because I am overly\n> concerned\n> > about protocols dictating UI, the protocol will be roundly ignored\n> and\n> > compliance will be coincidental at best, but rather that by\n> dictating\n> > requirements in areas clearly beyond the scope of a wire protocol,\n> the\n> > authority of the protocol group is lessened.\n> \n> When the purpose of the protocol is to allow for reliable\n> communication\n> between a user and an application, there is sometimes no other choice\n> than\n> to specify the content of the UI. There is a large difference in my\n> mind\n> between describing what choices the user must be presented with and\n> telling the UI designer how to present those choices. The UI choices\n> I've\n> seen have all been related to specification of the communication\n> features\n> required.\n> \n> If you object to a specific statement, please be specific. I have seen\n> no\n> general trend to take over the UI.\n> \n> Dave Morris\n\n\n\n", "id": "lists-010-15508318"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "On Tue, 18 Mar 1997, Yaron Goland wrote:\n\n> The average user doesn't know about cookies and frankly they don't want\n> to know about cookies. However browser implementers are now forced to\n> put UI in front of them to tell them more than they ever cared to know\n> about cookies. Since the user doesn't know what a cookie is, they are\n> going to call up to find out what the heck this cookie stuff is about.\n> Who pays for that call? The browser implementer. So a wire protocol is\n> taking away browser implementers right to control the experience users\n> have with the very program the browser implementer is selling.\n\nFirst all, I reject your wire protocol conclusion.  HTTP is a protocol\nwhich allows delivery of an application between a server and a user. There\nhas always been much more to HTTP (and its cousin HTML) than what flows\nover the wire. For HTTP to be useful, both the server and the user must\nhave their expectations met. And in the case under consideration, the\nexpectation deals with the user's privacy concerns.\n\nSecondly, the standards bodies are one way users can forcefully indicate\ntheir preferences to UA vendors. The economics of UA implementation\nhaven't left much room for real differentiation. When one is free bundled\nwith the operating system and the other is free for a major segment of the\npopulation, to deliver an alternative is difficult. The UA publishers have\nbeen left with a great deal of freedom as to how they present the required\ninformation to the user. Good UI design should eliminate many of the \nquestions your average user might have ... good on screen text, help\nfiles, etc. For the remainder, just apply the standard for fee tech\nsupport and consider it a revenue opportunity. I suspect that many users\nwill ask what the heck this cookie stuff is and just click the don't\nbother me box.  So the real issue in UI design is how to fairly present\nthe story so that users might understand that it isn't in their best\ninterest to accept all cookies.\n\nYou are beating a dead horse in my opinion when you argue that the HTTP\nspecification shouldn't consider specifing the UI functional requirements\nin this area. You make a number of suggestions for clarification which\nmight be reasonable for clarification of the new cookie2 specification.\nYou are more likely to get your other comments noticed by the group if\nyou separate them from the not appropriate for a wire protocol argument.\n\nDave Morris\n\n\n\n", "id": "lists-010-15519490"}, {"subject": "Re: draft-holtman-http-safe01.tx", "content": "On Tue, 18 Mar 1997, Patrick McManus wrote:\n\n> In a previous episode David W. Morris said...\n> :: \n> :: The safe: yes (and uahint variation) response header mean that any future\n> :: resubmit of the request will be safe. It says nothing about the request\n> :: which was just made which may have had a side effect.\n> \n> \n> That's not how I read section 3:\n> \n> --\n> 3 The Safe response header\n> \n>  This header is proposed as an addition to the HTTP/1.x suite.\n> \n>  The Safe response header field indicates whether the corresponding\n>  request is safe in the sense of Section 9.1.1 (Safe Methods) of the\n>  HTTP/1.1 draft specification [1].\n> --\n> \n> It speaks very specifically that the safe response header field talks\n> about the corresponding (i.e. current) request.. I think making any\n> assumptions about future request/response pairs requires clairvoyance\n> (and the Note further in section three claims that it can do this\n> clairvoyance)\n\nOK, I see the point. I will update the next round of the UAHINT draft to\nattempt clarification. The safe-ness in the past has always been based on\nthe postulate that if the initial request is safe because it has no side\neffects that the user is responsible for then re-submission is also safe\nbut arguments have always cited the duplicate order issue (that infamous\nunexpected pizza).  Assuming that the first POST would have a side-effect\nwhich would be repeated if the POST is repeated is the unsafe-ness\nconcern. What I will say when edit this section will attempt to say that\nresubmission of the request is SAFE. It will not be my intent to say that\nthe initial POST had no side-effect which the user will be held\naccountable for only that the user's level of accountability won't change\nas a result of resubmission. In general, I suspect the initial POST would\nhave had no side-effects but that really doesn't matter as long as a\nsubsequent POST wouldn't be additive.\n\n> \n> ::  When designing the\n> :: application, the author of the HTML must know if POST would be\n> :: appropriate. \n> :: \n> \n> who said anything about html?\n\nSorry, that was a lazy of saying that the definition of the ua client\nshould be expected to understand the application and not misuse POST.\n\n> It is not unusual for us to release a c/s pair  and\n> have other folks independently develop their own client side\n> interfaces without our knowledge.. it'd be extremely kind if they\n> could have a method that would guarantee no side effects for them\n> should we upgrade the server side to change it's behavior without\n> telling them.. (which is sort of hard, as we don't know they are there!)\n> \n> In short if we've got a CGI that makes side effects we make damn sure\n> it's input comes from POST not from GET before doing it.. even if\n> we've coded the client side to use POST.. because someone else out\n> there experimenting against our service and using GET shouldn't be\n> able to impact the environment.. this whole system works fine, we just\n> need a GET with a message body.\n\nI guess I really don't feel strongly one way or the other about this\nparticular requirement. I don't recall it being raised last fall when the\nGET/BODY vs. new header approach was debated. The problem is, as was\ndiscussed by the WG and covered in Koen's draft (and mine by cut/paste),\nthat a new method either takes a kludge HTML hack or results in a lack of\ncompatiblity. (An a new method would be UNSAFE by HTTP/1.1 specification).\n\nWhat would your reaction be to including the safe/uahint header on the\nPOST request? If the header said safe and the server said not-so, then\nan error response would be required. How such a header were inserted\nwould be an equivalent hack at the UA level to adding a new method but\nwouldn't effect proxies or servers.  The CGI program would have to check\nfor the header value.\n\nDave Morris\n\n\n\n", "id": "lists-010-15529568"}, {"subject": "Re: Issues with the cookie draf", "content": "Yaron,\n> My alternative proposal is to remove section 7 of the current draft and\n> to make the other alterations I have specifically suggested in the rest\n> of the post you referred to below. Would you like me to do a little\n> cutting and pasting and actually make it into an I-D  \n\nI will respond to this as if you meant it seriously.\n\nI think my request was very clear: a separate Internet Draft which\nexplains the proposal, its justification, and the way in which it\naddresses the concerns about user privacy which led to the current\ndesign.\n\nIf your proposal is \"remove section 7 of the current draft\", then you\nstill have something to write, since I have yet to see anything that you\ncould cut and paste that would  explain the situation sufficiently to\nmollify the concerns expressed here and elsewhere how user's privacy is\npreserved.\n\n> should we\n> continue to discuss the basic issue of how far this group should be\n> going in its protocols? \n\nAbsolutely not.\n\nInsofar as the IESG accepted RFC 2109 as a valid IETF protocol\nspecification, we have had a clear reading of the validity of scope. You\nmay believe that we have so far come to an incorrect engineering choice\n(which is why you've been invited to propose an alternative), but not\nthat we have no right to make such a choice.\n  \nIt is true that vendors can ignore what we write. In general, IETF\nstandards actually have no enforcement clause. Vendors can implement\nwhatever they want, no matter how broken or divergent from proposals\nagreed to here. Ultimately, what the implementors implement and the\nusers use determines what the real standards are. From this point of\nview, if a group with sufficient marketing clout creates products that\ncountervale our best judgement here as to how to deal with user privacy\nissues, well, then they'll just do it. You needn't remind us of this. \n\nHowever, the alternative to a standard that addresses the percieved\nrequirements of protecting user privacy is \"no standard\". That is, we\ncannot merely \"remove section 7 of the current draft\" and be done with\nit. The requirement is real, what we're asking for (once more) is a\ncredible explanation of how the privacy issues are (or are not) dealt\nwith. \n\n\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-010-15541170"}, {"subject": "RE: Unverifiable Transactions / Cookie draft (Warning: Rant Enclo sed", "content": "<RANT>\nThose arguing for the IETF enacting social policy have said that one of\nthe problem with cookies as they exist now is that, sure, browsers allow\nyou to selectively not receive cookies but you get pounded by warnings\nfor every single cookie you don't receive. The social engineers don't\nlike this behavior so they have decided to use their IETF stick to beat\nbrowser makers into acting the way they want. \n\nI won't even sideline into the question of what happened to the free\nmarket and the right to use other products. I suppose we can all look\nforward to a small group of academics using standards to force products\nto work the way they want from now on. Say goodbye to GUI and hello to\ncommand lines! I can't wait until the requirement \"Your product must not\ncrash, EVER, ohh and yeah, it has to run on LINUX.\" Is it just me or is\nthis all getting way to close to a Dilbert cartoon?\n\nSo, now, with the mighty standard bashing into the heads of browser\nmakers, exactly what sort of dialog do the social engineers think will\ngo up to the average user the first time they hit an \"unverified\"\ntransaction? What response do you think 99% of the users out there will\nclick? Oh yeah, gee, I forget, we all win because the user has a choice.\nAfter all users are clearly being brainwashed by the Evil Media (TM) and\ntherefore must be saved from their own product choices.\n\nI figure I should just relax and let it ride. After all, the reality of\nthe spec was decided before the last call was ever taken.\n</RANT>\n\nOf course, that is just my opinion, I could be wrong,\n\nYaron\n\nPS The reason I let this rant get posted to the list is because I\nstrongly believe that it accurately reflects the feelings of a\nsignificant number of people. If the IETF would continue being relevant\nit had better at least consider what those people are feeling and why.\nThat doesn't necessarily mean that it has to change what it has done\nw/the cookie spec but it does mean that there is a problem and it is in\neverybody's interest that it be addressed.\n\n> -----Original Message-----\n> From:Gregory A. Meinke [SMTP:gmeinke@acm.org]\n> Sent:Tuesday, March 18, 1997 4:23 PM\n> To:Phillip Lindsay\n> Cc:Rob Hartill; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: Unverifiable Transactions / Cookie draft\n> \n> Phillip Lindsay wrote:\n> > \n> > Rob Hartill wrote:\n> > \n> >   Cookies are (currently) useful for advertising models, but they\n> are\n> > \n> >   certainly not a necessity.\n> > \n> > Despite what is implied above,  many ad delivery implementations\n> > currently \"require\" cookies to function correctly (e.g., sequence,\n> > impression link).\n> \n> If the major browser vendors support this new standard as well as \n> they support the old standard we don't have to worry about cookie\n> support going away for a very long time ;->\n> \n> thank you drive through\n> -- \n> Gregory A. Meinke   | If I don't meet you no more in this world\n> IMGIS, Inc.         | Then I'll see you in the next one.\n> gmeinke@imgis.com   | Don't be late.  --J. Hendrix\n\n\n\n", "id": "lists-010-15550673"}, {"subject": "(ACCEPT*) Last call on draft text for Accept header", "content": "I posted proposed draft text for the Accept headers on Monday.  Since\nthen, I received comments about US-ASCII vs. iso-8859-1 as the\ncharacter set can be assumed to be acceptable to all user agents,\nabout q= parameters on the Accept-Charset header, and on the rule that\nservers may always choose to ignore Accept headers.\n\nUsing some of these comments, I adjusted the text.  Updated text is\nincluded below.  If I get no new comments on the new text within 2\ndays, I will declare consensus on this text and close this issue.\nUnless further comment is heard after that, the issue will remain\nclosed.\n\nDetails on the comments received:\n\n* On US-ASCII vs. iso-8859-1, the general opinion seems to be that\niso-8859-1 is the character that set can be assumed to be acceptable\nto all user agents.  I have performed the necessary update to reflect\nthis opinion below.\n\n* On q= parameters on the Accept-Charset header: I added these\nparameters to the text below, after receiving 2 votes in favor and 1\nvote against.\n\n* On the rule that servers may always choose to ignore Accept headers:\nA message from someone who did not agree with this rule was just\nposted, I will send a reply with a more detailed explanation of the\nbackground of this rule tomorrow.\n\nUpdated text is included below. | change bars indicate changes with\nrespect to the previous 1.1 draft, + change bars indicated changes\nwith respect to the text posted on Monday.\n\nKoen.\n\n--snip--\n\n\n3  Protocol parameter descriptions\n\n3.10  Language Tags\n\n   [##Note: I moved the language tag matching discussion that used to\n   be in this Section to Section 10.4 (Accept-Language).  Some other\n   minor edits were made.##]\n\n   A language tag identifies a natural language spoken, written, or\n   otherwise conveyed by human beings for communication of information\n   to other human beings. Computer languages are explicitly excluded.\n   HTTP/1.1 uses language tags within the Accept-Language and\n|  Content-Language fields.\n\n   The syntax and registry of HTTP language tags is the same as that\n   defined by RFC 1766 [1]. In summary, a language tag is composed of 1\n   or more parts: A primary language tag and a possibly empty series of\n   subtags:\n\n        language-tag  = primary-tag *( \"-\" subtag )\n\n        primary-tag   = 1*8ALPHA\n        subtag        = 1*8ALPHA\n\n   Whitespace is not allowed within the tag and all tags are\n   case-insensitive. The namespace of language tags is administered by\n   the IANA. Example tags include:\n\n       en, en-US, en-cockney, i-cherokee, x-pig-latin\n\n   where any two-letter primary-tag is an ISO 639 language abbreviation\n   and any two-letter initial subtag is an ISO 3166 country code.\n\n|\n|\n\n9. Status Code Definitions\n\n413 Not Acceptable\n\n   [##Note: the new 413 is similar to the 406 response code in the old\n   draft.  406 cannot be used for content negotiation compatibility\n   reasons##]\n\n   [##Note added later: I believe 413 is already taken.  The draft\n   editor should change every 413 in this text to an unassigned 4xx\n   number.##]\n\n|  The resource identified by the Request-URI and Host request header\n|  (present if the request-URI is not an absoluteURI) is only capable\n|  of generating response entities which have content characteristics\n|  not acceptable according to the accept headers sent in the request.\n\n|  HTTP/1.1 servers are allowed to return responses which are not\n|  acceptable according to the accept headers sent in the request.  In\n|  some cases, this may even be preferable over sending a 408\n|  response.  User agents are encouraged to inspect the headers of an\n|  incoming response to determine if it is acceptable.  If this is not\n|  the case, user agents should interrupt the receipt of the response\n|  if doing so would save network resources.  If it it unknown whether\n|  an incoming response would be acceptable, a user agent should\n|  temporarily stop receipt of more data and query the user for a\n|  decision on further actions.\n\n   [## Note: the paragraph above could be moved to a more convenient\n   location in the 1.1 document if the editor finds one.  Note that\n   the above rule was discussed extensively on the content negotiation\n   mailing list.  A short summary of the main reason behind this rule:\n   20 line HTTP servers.##]\n\n\n10  Header field definitions\n\n10.1  Accept\n\n|  The Accept request-header field can be used to specify certain\n   media types which are acceptable for the response.  Accept headers\n   can be used to indicate that the request is specifically limited to\n   a small set of desired types, as in the case of a request for an\n   in-line image.\n\n   The field may be folded onto several lines and more than one\n   occurrence of the field is allowed, with the semantics being the same\n   as if all the entries had been in one field value.\n\n       Accept         = \"Accept\" \":\" #(\n                        ( media-range\n|                         [ ( \":\" | \";\" ) \n|                           range-parameter \n|                           *( \";\" range-parameter ) ] )\n|                       | extension-token )\n\n       media-range     = ( \"*/*\"\n                       |   ( type \"/\" \"*\" )\n                       |   ( type \"/\" subtype )\n                         ) *( \";\" parameter )\n\n|      range-parameter = ( \"q\" \"=\" qvalue ) \n|                      | extension-range-parameter\n|\n|      extension-range-parameter = ( token \"=\" token )\n|\n|      extension-token = token \n\n   The asterisk \"*\" character is used to group media types into ranges,\n   with \"*/*\" indicating all media types and \"type/*\" indicating all\n   subtypes of that type.\n\n|  The range-parameter q is used to indicate the media type quality\n|  factor for the range, which represents the user's preference for\n|  that range of media types.  The default value is q=1.  In Accept\n|  headers generated by HTTP/1.1 clients, the character separating\n|  media-ranges from range-parameters should be a \":\".  HTTP/1.1\n|  servers should be tolerant of use of the \";\" separator by HTTP/1.0\n|  clients.\n\n   The example\n\n       Accept: audio/*:q=0.2, audio/basic\n\n   should be interpreted as \"I prefer audio/basic, but send me any audio\n   type if it is the best available after an 80% mark-down in quality.\"\n\n   If no Accept header is present, then it is assumed that the client\n|  accepts all media types.  If Accept headers are present, and if the\n|  resource cannot send a response which is acceptable according to\n|  the Accept headers, then the server should send an error response\n|  with the 413 (not acceptable) status code, though the sending of an\n|  un-acceptable response is also allowed.\n\n   A more elaborate example is\n\n|      Accept: text/plain:q=0.5, text/html,\n|              text/x-dvi:q=0.8, text/x-c\n\n   Verbally, this would be interpreted as \"text/html and text/x-c are\n   the preferred media types, but if they do not exist, then send the\n|  text/x-dvi entity, and if that does not exist, send the text/plain\n   entity.\"\n\n   Media ranges can be overridden by more specific media ranges or\n   specific media types. If more than one media range applies to a given\n   type, the most specific reference has precedence. For example,\n\n|      Accept: text/*, text/html, text/html;level=1, */*\n\n   have the following precedence:\n\n|      1) text/html;level=1\n       2) text/html\n       3) text/*\n       4) */*\n\n|  The media type quality factor associated with a given type is\n   determined by finding the media range with the highest precedence\n   which matches that type.\n\n   For example,\n\n|      Accept: text/*:q=0.3, text/html:q=0.7, text/html;level=1,\n               */*:q=0.5\n\n   would cause the following type quality factors to be associated:\n\n|      text/html;level=1                          = 1\n       text/html                                  = 0.7\n       text/plain                                 = 0.3\n       image/jpeg                                 = 0.5\n|      text/html;level=3                          = 0.7\n\n|  \n\n       Note: A user agent may be provided with a default set of \n       quality values for certain media ranges. However, unless the \n       user agent is a closed system which cannot interact with \n       other rendering agents, this default set should be \n       configurable by the user.\n\n\n10.2  Accept-Charset\n\n   The Accept-Charset request-header field can be used to indicate what\n   character sets are acceptable for the response. This field allows\n   clients capable of understanding more comprehensive or\n   special-purpose character sets to signal that capability to a server\n   which is capable of representing documents in those character\n+  sets. The ISO-8859-1 character set can be assumed to be acceptable to\n   all user agents.\n\n       Accept-Charset = \"Accept-Charset\" \":\" \n+                       1#( charset [ \";\" \"q\" \"=\" qvalue ] )\n\n+  Character set values are described in Section 3.4.  Each charset\n+  may be given an associated quality value which represents the\n+  user's preference for that charset.  The default value is q=1.  An\n+  example is\n\n+      Accept-Charset: iso-8859-5, unicode-1-1;q=0.8\n\n|  If no Accept-Charset header is present, the default is that any\n|  character set is acceptable.  If an Accept-Charset header is\n|  present, and if the resource cannot send a response which is\n|  acceptable according to the Accept-Charset header, then the server\n|  should send an error response with the 413 (not acceptable) status\n|  code, though the sending of an un-acceptable response is also\n|  allowed.\n\n\n10.3  Accept-Encoding\n\n   The Accept-Encoding request-header field is similar to Accept, but\n   restricts the content-coding values (Section 3.5) which are\n   acceptable in the response.\n\n       Accept-Encoding         = \"Accept-Encoding\" \":\" \n                                 #( content-coding )\n\n   An example of its use is\n\n       Accept-Encoding: compress, gzip\n\n   If no Accept-Encoding header is present in a request, the server\n   may assume that the client will accept any content coding.  If an\n|  Accept-Encoding header is present, and if the resource cannot send\n|  a response which is acceptable according to the Accept-Encoding\n|  header, then the server should send an error response with the\n|  413 (not acceptable) status code.\n\n\n10.4  Accept-Language\n\n   The Accept-Language request-header field is similar to Accept, but\n   restricts the set of natural languages that are preferred as a\n   response to the request.\n\n       Accept-Language = \"Accept-Language\" \":\"\n|                        1#( language-range [ \";\" \"q\" \"=\" qvalue ] )\n\n|      language-range = ( ( 1*8ALPHA *( \"-\" 1*8ALPHA ) )\n|                       | \"*\" )\n\n|  Each language-range may be given an associated quality value which\n|  represents an estimate of the user's comprehension of the languages\n|  specified by that range.  The quality value defaults to \"q=1\" (100%\n   comprehension).  For example,\n\n|      Accept-Language: da, en-gb;q=0.8, en;q=0.7\n|  \n|  would mean: \"I prefer Danish, but will accept British English (with\n|  80% comprehension) and other types of English (with 70%\n|  comprehension).\"\n\n|  A language-range matches a language-tag if it exactly equals the tag,\n|  or if it is a prefix of the tag such that the first tag character\n|  following the prefix is \"-\".  The special range \"*\", if present in\n|  the Accept-Language field, matches every tag not matched by any other\n|  ranges present in the Accept-Language field.\n\n|      Note: This use of a prefix matching rule does not imply that\n|      language tags are assigned to languages in such a way that it is\n|      always true that if a user understands a language with a certain\n|      tag, then this user will also understand all languages with tags\n|      for which this tag is a prefix.  The prefix rule simply allows\n|      the use of prefix tags if this is the case.\n\n|  The language quality factor assigned to a language-tag by the\n|  Accept-Language field is the quality value of the longest\n|  language-range in the field that matches the language-tag.  If no\n|  language-range in the field matches the tag, the language quality\n|  factor assigned is 0.\n\n   If no Accept-Language header is present in a request, the server\n|  should assume that all languages are equally acceptable.  If an\n|  Accept-Language header is present, and the resource cannot send a\n|  response acceptable to an audience capable of understanding at\n|  least one language that is assigned a quality factor greater than 0\n|  by the Accept-Language header, it is acceptable to send a response\n|  that uses one or more un-accepted languages.\n\n|  It may be contrary to be privacy expectations of the user to send\n|  an Accept-Language header with the complete linguistic preferences\n|  of the user in every request.  For a discussion of this issue, see\n|  Section aa.bb [##see below##].\n\n       Note: As intelligibility is highly dependent on the \n       individual user, it is recommended that client applications \n       make the choice of linguistic preference available to the \n       user. If the choice is not made available, then the \n       Accept-Language header field must not be given in the \n       request.  \n\n14 Security Considerations\n\n14.x  Privacy issues connected to Accept headers\n\n   [## Note: I believe someone else (Brian Behlendorf?) was also\n   writing text about this, so I only include some concerns about\n   Accept-Language important from a European viewpoint.  The concern\n   of user tracking through Accept headers is not covered below, see\n   Section 6.2 of draft-holtman for a discussion of this concern##]\n\n|  Accept request headers can reveal information about the user to all\n|  servers which are accessed.  The Accept-Language header in\n|  particular can reveal information the user would consider to be of\n|  a private nature, because the understanding of particular languages\n|  is often strongly correlated to the membership of a particular\n|  ethnic group.  User agents which offer the option to configure the\n|  contents of an Accept-Language header to be sent in every request\n|  are strongly encouraged to let the configuration process include a\n|  message which makes the user aware of the loss of privacy involved.\n|  An approach that limits the loss of privacy would be for a user\n|  agent to omit the sending of Accept-Language headers by default,\n|  and to ask the user whether it should to start sending\n|  Accept-Language headers to a server if it detects, by looking for\n|  at any Vary or Alternates response headers generated by the server,\n|  that such sending could improve the quality of service.\n\n\n[End of document.]\n\n\n\n", "id": "lists-010-1555664"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "The IETF is not a social engineering organization and its purpose is not\nfor users to \"forcefully indicate their preferences to UA vendors.\" I\nalso wasn't aware that \"UA publishers have been left with a great deal\nof freedom as to how they present the required information to the user.\"\nwas a bad thing. I thought this was a free country where UA vendors are\nfree to produce the product they want and users are free to use it if\nthey want.\n\nI suppose I should thank you. At least you have publicly admitted that\nyou are engaged in nothing more than social engineering. You couldn't\nget what you wanted through the market so now you will get it through\nthe standards process. I am sure all the UA vendors will come out with\nringing endorsement of the cookie standard and will fight each other to\nprove just how \"privacy sensitive\" they are. This will be easy because\nthe cookie standard is, with noted exceptions, a good thing and believe\nit or not David, UA vendors really do care about user privacy. I work\nfor one of those vendors and I know I care. But I also care about the\nIETF and the critical role it has to play in the development of the\nInternet. What your end run around the market has accomplished is to\nconvince vendors that the IETF is a dangerous place to create standards.\nAfter all, vendors want to work with an organization where they are\npartners in the standards process, not its target.\n\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Tuesday, March 18, 1997 10:50 PM\n> To:Yaron Goland\n> Cc:http working group\n> Subject:RE: Unverifiable Transactions / Cookie draft\n> \n> \n> \n> On Tue, 18 Mar 1997, Yaron Goland wrote:\n> \n> > The average user doesn't know about cookies and frankly they don't\n> want\n> > to know about cookies. However browser implementers are now forced\n> to\n> > put UI in front of them to tell them more than they ever cared to\n> know\n> > about cookies. Since the user doesn't know what a cookie is, they\n> are\n> > going to call up to find out what the heck this cookie stuff is\n> about.\n> > Who pays for that call? The browser implementer. So a wire protocol\n> is\n> > taking away browser implementers right to control the experience\n> users\n> > have with the very program the browser implementer is selling.\n> \n> First all, I reject your wire protocol conclusion.  HTTP is a protocol\n> which allows delivery of an application between a server and a user.\n> There\n> has always been much more to HTTP (and its cousin HTML) than what\n> flows\n> over the wire. For HTTP to be useful, both the server and the user\n> must\n> have their expectations met. And in the case under consideration, the\n> expectation deals with the user's privacy concerns.\n> \n> Secondly, the standards bodies are one way users can forcefully\n> indicate\n> their preferences to UA vendors. The economics of UA implementation\n> haven't left much room for real differentiation. When one is free\n> bundled\n> with the operating system and the other is free for a major segment of\n> the\n> population, to deliver an alternative is difficult. The UA publishers\n> have\n> been left with a great deal of freedom as to how they present the\n> required\n> information to the user. Good UI design should eliminate many of the \n> questions your average user might have ... good on screen text, help\n> files, etc. For the remainder, just apply the standard for fee tech\n> support and consider it a revenue opportunity. I suspect that many\n> users\n> will ask what the heck this cookie stuff is and just click the don't\n> bother me box.  So the real issue in UI design is how to fairly\n> present\n> the story so that users might understand that it isn't in their best\n> interest to accept all cookies.\n> \n> You are beating a dead horse in my opinion when you argue that the\n> HTTP\n> specification shouldn't consider specifing the UI functional\n> requirements\n> in this area. You make a number of suggestions for clarification which\n> might be reasonable for clarification of the new cookie2\n> specification.\n> You are more likely to get your other comments noticed by the group if\n> you separate them from the not appropriate for a wire protocol\n> argument.\n> \n> Dave Morris\n\n\n\n", "id": "lists-010-15562537"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "BTW, I would like to make it clear that I speak for myself, not\nMicrosoft. The opinions I express are mine and mine alone. The postings\nI have been putting out are strictly a reflection of my deep concern\nabout the damage I believe the cookie spec is doing to the IETF.\n\nThanks,\nYaron\n\n> -----Original Message-----\n> From:Yaron Goland \n> Sent:Tuesday, March 18, 1997 11:21 PM\n> To:'David W. Morris'\n> Cc:http working group\n> Subject:RE: Unverifiable Transactions / Cookie draft\n> \n> The IETF is not a social engineering organization and its purpose is\n> not\n> for users to \"forcefully indicate their preferences to UA vendors.\" I\n> also wasn't aware that \"UA publishers have been left with a great deal\n> of freedom as to how they present the required information to the\n> user.\"\n> was a bad thing. I thought this was a free country where UA vendors\n> are\n> free to produce the product they want and users are free to use it if\n> they want.\n> \n> I suppose I should thank you. At least you have publicly admitted that\n> you are engaged in nothing more than social engineering. You couldn't\n> get what you wanted through the market so now you will get it through\n> the standards process. I am sure all the UA vendors will come out with\n> ringing endorsement of the cookie standard and will fight each other\n> to\n> prove just how \"privacy sensitive\" they are. This will be easy because\n> the cookie standard is, with noted exceptions, a good thing and\n> believe\n> it or not David, UA vendors really do care about user privacy. I work\n> for one of those vendors and I know I care. But I also care about the\n> IETF and the critical role it has to play in the development of the\n> Internet. What your end run around the market has accomplished is to\n> convince vendors that the IETF is a dangerous place to create\n> standards.\n> After all, vendors want to work with an organization where they are\n> partners in the standards process, not its target.\n> \n> Yaron\n> \n> > -----Original Message-----\n> > From:David W. Morris [SMTP:dwm@xpasc.com]\n> > Sent:Tuesday, March 18, 1997 10:50 PM\n> > To:Yaron Goland\n> > Cc:http working group\n> > Subject:RE: Unverifiable Transactions / Cookie draft\n> > \n> > \n> > \n> > On Tue, 18 Mar 1997, Yaron Goland wrote:\n> > \n> > > The average user doesn't know about cookies and frankly they don't\n> > want\n> > > to know about cookies. However browser implementers are now forced\n> > to\n> > > put UI in front of them to tell them more than they ever cared to\n> > know\n> > > about cookies. Since the user doesn't know what a cookie is, they\n> > are\n> > > going to call up to find out what the heck this cookie stuff is\n> > about.\n> > > Who pays for that call? The browser implementer. So a wire\n> protocol\n> > is\n> > > taking away browser implementers right to control the experience\n> > users\n> > > have with the very program the browser implementer is selling.\n> > \n> > First all, I reject your wire protocol conclusion.  HTTP is a\n> protocol\n> > which allows delivery of an application between a server and a user.\n> > There\n> > has always been much more to HTTP (and its cousin HTML) than what\n> > flows\n> > over the wire. For HTTP to be useful, both the server and the user\n> > must\n> > have their expectations met. And in the case under consideration,\n> the\n> > expectation deals with the user's privacy concerns.\n> > \n> > Secondly, the standards bodies are one way users can forcefully\n> > indicate\n> > their preferences to UA vendors. The economics of UA implementation\n> > haven't left much room for real differentiation. When one is free\n> > bundled\n> > with the operating system and the other is free for a major segment\n> of\n> > the\n> > population, to deliver an alternative is difficult. The UA\n> publishers\n> > have\n> > been left with a great deal of freedom as to how they present the\n> > required\n> > information to the user. Good UI design should eliminate many of the\n> \n> > questions your average user might have ... good on screen text, help\n> > files, etc. For the remainder, just apply the standard for fee tech\n> > support and consider it a revenue opportunity. I suspect that many\n> > users\n> > will ask what the heck this cookie stuff is and just click the don't\n> > bother me box.  So the real issue in UI design is how to fairly\n> > present\n> > the story so that users might understand that it isn't in their best\n> > interest to accept all cookies.\n> > \n> > You are beating a dead horse in my opinion when you argue that the\n> > HTTP\n> > specification shouldn't consider specifing the UI functional\n> > requirements\n> > in this area. You make a number of suggestions for clarification\n> which\n> > might be reasonable for clarification of the new cookie2\n> > specification.\n> > You are more likely to get your other comments noticed by the group\n> if\n> > you separate them from the not appropriate for a wire protocol\n> > argument.\n> > \n> > Dave Morris\n\n\n\n", "id": "lists-010-15575238"}, {"subject": "Re: Unverifiable Transactions / Cookie draft (Warning: Rant Enclosed", "content": "> PS The reason I let this rant get posted to the list is because I\n> strongly believe that it accurately reflects the feelings of a\n> significant number of people. If the IETF would continue being relevant\n> it had better at least consider what those people are feeling and why.\n> That doesn't necessarily mean that it has to change what it has done\n> w/the cookie spec but it does mean that there is a problem and it is in\n> everybody's interest that it be addressed.\nThe WG *does* make every *reasonable* attempt to hear all sides.\nThat is what is meant when the phrase 'rough consensus' is used.\nThe wg meetings and mailing lists are well known.  Interested parties\nare obligated to put forth their opinions.  The wg cannot seek out\nevery individual or organization who 'might' have an opinion.\nThe fact that up until right now, it seemed that the draft\nmight move forward with a consensus.\n\nWhy is it that right at the wire ( no pun intended ) this has\nblown up again.  Its the same issues that have been discussed at \nlength before, many times.\n\nObviously the rough in 'rough consensus' leads one to beleive that\nnot everyone will be happy with everything, thats not possible.\nBut the draft should move forward on consensus, not based on\nwho shouts the loudest.\n\nIn any event, time would be better spent writing a specific\nalternative proposal for review, then a self proclaimed rant.\nIf so many people feel that way, they should write a draft\nthat expresses their views.\n\nThe chair has graciously invited you to create one for consideration.\n\n> BTW, I would like to make it clear that I speak for myself, not\n> Microsoft. The opinions I express are mine and mine alone. The postings\n> I have been putting out are strictly a reflection of my deep concern\n> about the damage I believe the cookie spec is doing to the IETF.\n\nIll avoid restating my disclamer, but it applies equally.\nOn 'what the cookie spec is doing to the IETF', I dont see it\nas being quite so dramatic.  \n\nThe wg's charter is to generate and propose standards based on \na consensus of opinion from a wide base of representation.\nIt has done so with many drafts.  \n\nIn this case, IMHO, a few (though quite big), organizations have\nmisinterpreted the spirit of the draft.  Now the wg is modifying\nthe draft to better express the spirit of that consensus.\nI dont see that as damaging to the IETF.\n\nI beleive that the spirit or intent of the wg consensus is more\nimportant than the words on paper in a spec.\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department                \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n", "id": "lists-010-15588375"}, {"subject": "Re: Unverifiable Transactions / Cookie draft (Warning: Rant Enclo sed", "content": "Yaron,\n\nPlease save your ranting for alt.bash.evil-empire.\n\nIf we weren't engaged in social engineering, we wouldn't have\na requirement for a \"security considerations\" section. \n\nYou are completely free to implement whatever the market\nlets you get away with implementing, and the IETF has no\nway to control it. It's only if you want the IETF as whole\nto endorse the protocol that you want to implement as \nan \"internet standard\" that you have to put up with the pain\nof actually considering those things which are established\nas IETF priorities. This does not include \"Your product must not\ncrash, EVER, ohh and yeah, it has to run on LINUX\" (although\nneither of those would be bad ideas), but it does include\nbeing careful to consider the issues of security, privacy,\neffect on the overall operation of the internet, \ninternationalization, even when those are not clearly\nwithin your current short-term market horizin.\n\nNo one has anything with which to bash the head of any browser\nmaker. This is a consensus process. If the browser makers\ndon't go along, well, then we can have either a different\nstandard or no standard. It's really up to you. But this is\nnot a free-market economy model, winner-take-all. This is\na consensus model. Convince us that you're right. Ranting\nis not a useful method of convincing anyone.\n\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n", "id": "lists-010-15599079"}, {"subject": "Re: Suggestion: Add CacheControl extensibility to Vary heade", "content": "Henrik Frystyk Nielsen:\n>\n>At 07:21 PM 3/16/97 +0100, Koen Holtman wrote:\n>\n>>In stead of \n>>\n>> Vary: Protocol, \"http://some.org/a-required-extension\"\n>>\n>>you can just as well use\n>>\n>> Vary: Protocol\n>> Cache-control: vary-protocol=\"http://some.org/a-required-extension\"\n>>\n>>or something, so there is no need to revise the Vary syntax: just use the\n>>extensible cache-control syntax to convey your extra directive.\n>\n>This is not exactly what I am trying to achieve. It is not a question of\n>describing which parts (headers, body, or complete message) of a response\n>that are cacheble or not\n\nI know the above is not a case of cache-control saying something about the\ncachability of some part of the response.  But you are still allowed to use\nthe cache-control extensibility mechanism to get the extension you want.  If\nit controls caches, it can be put in cache-control.  \n\nYou can also define a completely new header if you want.  My point is that\nthere is no need to incompatibily extend the unextendable Vary header.\n\n[...]\n>>You can also use the following hack, which will get you what you want even\n>>under the current HTTP/1.1 spec.  If you have multiple protocol headers but\n>>only want caches to vary on one, include an extra header like\n>>\n>>Proto-Vary: {http://some.org/a-required-extension {str \"req\"}}\n>>\n>>in the response, and send \"Vary: proto-vary\" in stead of \"Vary: protocol\".\n         ^^^^^^^^\n                Oops! I should have written request here\n\n>>Strange, but fun.  Of course, you would usually want to have a shorter\n>>encoding in the above proto-vary header.\n>\n>Throwing in random new headers is exactly what PEP tries to avoid\n\nYes, you have a point.  The above is a hack: if this functionality would be\nneeded often, you are better off defining clean new mechanism.\n\n>Henrik\n\nKoen.\n\n\n\n", "id": "lists-010-15607912"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "Dwight Merriman:\n>\n>I'd like to bring up one another (small) point.\n>\n>Virtually all banner advertising on the web relies on redirects for\n>click-throughs so that click rates can be measured.  This means that the\n>redirection to the advertiser's web site will always be an unverifiable\n>transaction.\n\nThe redirection won't be unverifiable if the script which does the counting\nand redirection to the page is on the target site already.\n\n>So, when a user visits an advertiser's site directly, cookie assignment on\n>the home/jump page is possible, whereas when the user visits the page via\n>an advertisement, it will not be possible.\n\nYes, if the script is on the site which served the banner.  But I don't see\nthis as a big problem.\n\n>Designers of web sites (at least the large percentage who will advertise on\n>the web) will have to take into account that cookie assignments on their\n>home page may fail a large percentage of the time.  If they wish to measure\n>number of unique visitors to their site, they will get a highly inaccurate\n>reading since often multiple cookies will be assignied to a single user\n>before one \"sticks\".\n\nSome careful design can work around this: suppress assignment of cookie if\nthe referer field shows that the user comes from the click-through script.\n\nNote however that higly inaccurare readings are almost guaranteed anyway\nbecause many people disable cookies, or have browsers which do not support\nthem.\n\n>\n>This is not a major flaw, but it is inelegant, and I just want to make sure\n>everyone has considered this.\n>\n>Dwight\n\nKoen.\n\n\n\n", "id": "lists-010-15617923"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "Rob Hartill:\n> \n>On Sun, 16 Mar 1997, Koen Holtman wrote:\n>> \n>> Caching of ad images is a feature, not a bug.\n> \n>True. I only said it introduced 'problems' - not bugs. The 'problems'\n>will have a significant impact on click-through rates and ads served\n>per pageview.\n\nHmmm.  The idea is that you do the measuring in the click-though CGI\nscript which returns the 302 to the as image.  The 302 message should\nbe highly uncachable: the spec says it is not for one thing.  A nice\nway to make it even more uncachable for some legacy browser caches is\nto make the ad gif into a clickable map: this generates an additional\nsemi-random URI component.\n\n[....]\n>> Transactions in future:\n>> \n>> 1a) host: a.com  GET page\n>> 1b.1) host: a.com  GET inline image\n>> 1b.2) host: a.com  GET inline image\n>> 1d) host: a.com  GET ad image (returns 302 redirect to 1c)\n>>  .... \n>> 1c) host: doubleclick.com GET ad image\n>> \n>> The new 1d is uncachable, but 1d is a very small transaction.  1c becomes\n>> cachable in proxy caches, which is very good.\n> \n>This \"future\" method is what I'm using now. However, 1c is NOT cacheable\n>(for us) in practise; we've had to make it deliberately uncacheable because\n>we were getting screwed (financially) by caches that'd never inform the\n>ad server that it had served the same ad to dozens or hundreds of users.\n\nSo in your scheme, the ad server counts 1c) transactions, and this is\nwhat your revenue is based on?  Well in that case of course the 1c)\nsite would have to make 1c) highly uncachable.\n\nMy design above however has the counting on which the revenue is based\nin the 1d) transaction, not the 1c) transaction, so 1c) is cachable.\nOf course this requires some mutual trust between 1d) site and the\npeople who are paying.\n\n[Note: I looked at your site and I could not find any use of the 302\nad serving scheme above.  I did find some use of 302 for\nclickthroughs, so I'm not entirely sure we are talking about the same\nthing here.  Could you point to an example of what you are using now?]\n\n>I know there are plans to make proxies report this kind of info in the\n>future, but we're not there yet and sites that are dependant on ad revenue\n>cannot afford to be good net citizens w.r.t caching ads.\n\nThey can be good citizens if they measure their hits in a script which\nredirects to the ads.  Of course, they will need to cache-bust the\nscript, but this is cheaper for the net than cache-busting the ad.\n\n>> End result: higher degree of proxy caching on the web.\n> \n>*if* people don't deliberately cache bust to protect their revenues. I\n>think that's going to be a big 'if'.\n\nI do expect them to bust, but not on large images like they do now.\nThe design above will increase the degree of caching while people\nstill bust the revenue counter.\n\n>> In fact, in my\n>> opinion doubleclick should be using this scheme already, unverifiable\n>> transactions or not.\n>> \n>> End result for host a.com: this adds about the cost of 1 additional small\n>> inline image, and some database processing (database processing is very\n>> cheap).\n> \n>IMO, that's assuming a hell of a lot and I don't agree with the assumption\n>that it'll end up being cheap. I think it'll be expensive enough to\n>cause some casualties among small sites that rely on ad networks.\n\nI have good news: unverifiable transactions may add some overhead for\nthese sites, but HTTP/1.1 removes a lot of overhead in the same\ntimeframe.  So I don't expect that our protocols will cost them extra\nmoney on hardware.  Of course, they will be spending some money just\nto keep up with the growth of the web.  \n\nAlso, if the site does more processing itself, and allows savings\n(because of caching) on the ad network site, I would expect the ad\nnetwork to give them a some appropriate amount of extra money for\ntheir trouble.\n\n>> And note that this additional transaction this will soon happen on\n>> a HTTP/1.1 persistent connection.\n>> \n>> So again, things turn out better for the web as a whole.\n> \n>Now you've lost me. We end up with an extra trip to the content site\n>and because this will be persistent, this extra trip will save us something\n?\n\nAgain, the extra trip will make the ad image cachable, and ad images\nare typically some 8Kb large.  The extra trip generates an extra\nuncachable response of about 0.5 Kb, but this is tiny in comparison.\n\n>Whichever way you look at it, it's going to slow down the content sites\n>and slow down delivery times for ads.\n\nIt will speed delivery if there are proxy caches in the loop.  I agree\nthat for those who are not behind proxy caches, things may get\nslightly slower.  But the worsening would be 8.5Kb in stead of 8Kb, so\nyou'd not even notice the slow-down factor.\n\n[...]\n>What I want to contribute to this discussion are 3 points:\n>        1) the simple/cheap alternatives aren't half as cheap/simple as\n>                some might think.\n\nAs I explained above, by making their ads uncachable, in stead of a\nmaking a tiny redirection response which redirects to their ads\nuncachable, advertising sites are not as cheap as they can be *right\nnow*.  Unverifiable transactions may force ad sites to pay proper\nattention to this issue, so they are a good thing for overall\nefficiency.\n\n>        2) it's likely that other loopholes will be found.\n\nOh, sure, there is always Java and ActiveX to have loopholes which\ngenerate bad press.  But we will have closed at least this\nbad-press-generating loophole.\n\n>        3) the new loopholes could have a more negative effect on web\n>                traffic.\n> \n>> >I don't know if this is possible, but we might end up with something\n>> >crazy like the following as a way to set the cookie behind the user's\n>> >back..\n>> >\n>> >- user visits content site 'X'.\n>> >- X sees user hasn't got X's cookie so bounces user to ad site Y\n>> >- ad site Y sets its cookie and redirects user back to content site X\n>> \n>> This step is not possible: if you redirect to another server, all\n>> transactions on that server are unverifiable, so Y cannot set a cookie.\n> \n>What I meant was redirecting the main request (pageview) rather than\n>an inline image. If X redirects to Y before displaying the page, Y can\n>set a cookie \n\nNo, Y cannot set a cookie.  IF X redirects with a 302 to Y, the\ntransaction on Y is unverifiable.  It does not matter that it is a\npage request: what matters is if the user had the option of reviewing\nY's URL.  See the definition of `unverifiable' in the spec.\n\n>and redirect back to the page on X which then includes the\n>inline from Y. Kind of a front-door into the site... come back when you've\n>'registered' with the ad company. Now that might not be possible, but\n>it does look possible to me.\n\n>Rob Hartill   Internet Movie Database (Ltd)\n\nKoen.\n\n\n\n", "id": "lists-010-15627823"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "On Tue, 18 Mar 1997, Yaron Goland wrote:\n> The IETF is not a social engineering organization ...\n\nI went to the W3 home page <http://www.w3.org/> for other reasons today and\nfound three major areas of research listed there: 'User Interface',\n'Technology and Society', and 'Architecture'.  Under 'Technology and\nSociety' was a subheading for 'Privacy and Demographics' (the topic we have\nbeen primarily arguing).  Does that make the W3C a \"social engineering\norganization\"?  Isn't Microsoft a W3C member?  If those topics -- privacy,\ndemographics, and society, not to mention user interface -- are germane to\nthe W3C's work, why are they suddenly irrelevant and out of scope for the\nHTTP-wg?  Are you saying we can't discuss them simply because we haven't\npaid the W3C's membership fee?\n\n> ... and its purpose is not for users to \"forcefully indicate their\n> preferences to UA vendors.\"\n\nThe IETF has, through its open admission policies, made itself a forum for\nany interested party to express an opinion and argue a position.  This is\ntrue for the representative from DoubleClick just as much as it is true for\nme or any other user or implementor out there.  If the position is\npersuasive and the group is persuaded (and working implementations can be\nproduced), then the standard can be modified to the group's wishes.  This\nis not a matter of forcing any vendor to do anything.  Instead, it is an\nopportunity for a a standards document to benefit from public review and\ncomment.  If you or your company don't like the product of that review, the\nworst we can do to you is call you \"non-compliant.\"  No other force is\navailable for exercise.\n\n> I figure I should just relax and let it ride. After all, the reality of\n> the spec was decided before the last call was ever taken.\n\nOh, PLEASE.  I personally made several invitations to Microsoft for\nrepresentatives to attend the state-management subgroup, and the topic was\na matter of open debate on www-talk and http-wg for literally _YEARS_\nbefore the last call without the slightest comment from Microsoft.  The\nsubgroup was announced on the http-wg list and all interested parties were\ninvited.  Netscape managed to send not one but _two_ representatives to the\ngroup (Lou Montulli and Ari Luotonen).  If you or anyone else at your\ncompany feels that all decisions were made before last call, you could have\nSHOWN UP for the discussion and participated, which is _how the decisions\nget made_.  Don't whine about abrogation of process unless you are ready to\nproduce an alternative document and defend it under the same public review\nthe current draft is receiving. \n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15642393"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "On Mon, 17 Mar 1997, Koen Holtman wrote:\n\n> >> 1c) host: doubleclick.com GET ad image\n> >> \n> >> The new 1d is uncachable, but 1d is a very small transaction.  1c becomes\n> >> cachable in proxy caches, which is very good.\n> > \n> >This \"future\" method is what I'm using now. However, 1c is NOT cacheable\n> >(for us) in practise; we've had to make it deliberately uncacheable because\n> >we were getting screwed (financially) by caches that'd never inform the\n> >ad server that it had served the same ad to dozens or hundreds of users.\n> \n> So in your scheme, the ad server counts 1c) transactions, and this is\n> what your revenue is based on?\n\nYes, it's pay per impression as with most other advertising on the web.\n\n> Well in that case of course the 1c)\n> site would have to make 1c) highly uncachable.\n\nIndeed.\n\n> [Note: I looked at your site and I could not find any use of the 302\n> ad serving scheme above.\n\nThe DoubleClick ads represent only part of our overal advertising.\n\n> I did find some use of 302 for\n> clickthroughs, so I'm not entirely sure we are talking about the same\n> thing here.  Could you point to an example of what you are using now?]\n\nThe ads are spread out evenly over the day in 'random' places, so there's\nno place I can point you.\n\n> Also, if the site does more processing itself, and allows savings\n> (because of caching) on the ad network site, I would expect the ad\n> network to give them a some appropriate amount of extra money for\n> their trouble.\n\n:-)  now wouldn't that be nice.\n\n> >Now you've lost me. We end up with an extra trip to the content site\n> >and because this will be persistent, this extra trip will save us something\n> ?\n> \n> Again, the extra trip will make the ad image cachable,\n\nCaching ads is very bad news for people selling ads on a per impression\nbasis. If a content site has to redirect to an ad network then it will\neither have to cache bust everything or waive goodbye to 30-50% of their\nrevenue OR 30-50% of their ad serving capacity.\n\nI can't see network sites trusting the content sites to report accurate\nimpression counts, so if the network site does all the accounting then\nits the content sites that will lose or cache bust everything. Going\nfrom our content site to the DoubleClick network site we cache bust\neverything.\n\n--\nRob Hartill   Internet Movie Database (Ltd)\nhttp://us.imdb.com/Oscars/oscars_1996 -  hype free Oscars (R) info.\nhttp://us.imdb.com/usr/sweepstake     -  Win a 56k X2 modem. Free draw.\n\n\n\n", "id": "lists-010-15653017"}, {"subject": "Re: determining proxy reliabilit", "content": "In a previous episode Jeffrey Mogul said...\n::     \n:: It is true that there is no technical mechanism in the hit-metering\n:: proposal to prevent a proxy from agreeing to hit-meter a response,\n:: and then not doing so.  The proposal states MUST-level requirements,\n:: but provides no means to verify that they are always observed.\n:: \n:: But this is not any different from any other HTTP protocol requirement.\n\nJeff,\n\n  I'm starting to see this in a new light, your argument about\nprotocol trust is a good one. In summary non reliable worries me more\nthan non compliant, read on. What I'm still hesitant on is what I\nfeel will be a very strong content-provider hesitation to this\nproposal because it's accuracy is so unbounded. While solving that\nproblem precisely is extremely difficult I think that something needs\nto be done to reduce it.. consider this:\n\n  A content provider implements hit-metering instead of doing their\ncurrent cache busting technique.. they're most pleased as the server\nload drops by 50% (as does potentially WAN traffic) but they also take\na 10% drop in usage numbers (after aggregation of all the proxy\nreports).. if they get paid on a linear model they could lose 10% of\ntheir income.. on quota systems it could be even worse, so they\nimmediately revert back to cache busting and their numbers return to\ntheir expected values.\n\nIt's a shame really, because we should all win by them using hit\nmetering.. not only is net traffic reduced but I would think that\ntheir access numbers (if reported properly) could actually go up as\nsite availabilty isn't always at the whims of WAN reliability as local\ncaching enters the picture.\n\nWhat they'd like to do is not allow caching to non compliant or non\nreliable caches and do so for the rest.. I think that non reliable\nhere is more important than non compliant.. I'm not worried about the\nproxy's algorithm as much as I am machines that are perpetually\nrebooted or have their proxies restarted by itchy sys admins.. or even\nmachines that devote a fixed amount of resources to storing these\ncounts and when those resources are exhausted can't report them to the\norigin server because of network unreachability and drop the report on\nthe floor instead.. certain environs are prone to being chronic with\nthis sort of thing.\n\nI made a proposal months ago about being able to (at the origin\nservers option) force the return of 0/0 counts.. at least this would\nallow the construction of deterministic audit trails and therefore some\nnotion of reliability.. it doesn't account for outright fraud by the\nproxy of course (they could misreport the numbers) but it does close\nthe case of any open ended transactions.. I'm not sure that it is\nenough, but I do think it helps considerably in establishing 'good\nfaith and a reliable history' which is something to go on..\n\n-Pat, not feeling bad about bringing this back up when it's still in\nID and considering we can do 50 messages a day on cookies that are\nnearing last call..\n\n\n\n", "id": "lists-010-15663060"}, {"subject": "Re: Issues with the cookie draf", "content": "Yaron Goland wrote:\n\n> Why are names beginning w/$ still reserved? As we have now defined the\n> position of NAME=VALUE, this restriction is no longer necessary.\n\nUnfortunately, it is, given the compatibility rules for combining\nSet-Cookie and Set-Cookie2 headers.  When the origin server receives a\nCookie header, it doesn't know, a priori, whether it's an old or new\ncookie.  Because Netscape's original spec. separated distinct cookies by\n';', the positional placement isn't sufficient to find NAME=VALUE.  More to\nthe point, something like Version=1 could look like a NAME=VALUE.  So I\nretained the '$' reservation, to distinguish returned attribute/values from\ncookie values.\n\n> \n> Comment should be a language tagged Unicode string not a quoted string.\n> The actual language used can be implicitly negotiated on by the\n> accept-language headers with the request. This is clearly not a robust\n> solution but it is probably appropriate to this situation.\n\nI'd like to see a more detailed proposal of how the language gets chosen,\nwhat the syntax of the Comment attribute would be, and what the\nimplications would be for displaying Comment's contents to a user (sect.\n4.2.2).\n\n> \n> Discard is entering dangerous territory. When exactly does a user agent\n> terminate? Both MSIE 4.0 and NS 4.0 are moving to desk top models where\n> the user agent is operational as long as the computer is on.\n\nIn that model, the user agent terminates when you shut down the computer.\n\n> Furthermore, why would you want to discard a cookie when the user agent\n> terminates? It sounds like this is an attempt to solve the problem of\n> shared cash behavior. If the cookie is sensitive and if the cache is\n> shared, we don't want the cookie hanging around. I think we should\n> change Discard to Private. Private would indicate that the cookie SHOULD\n> only be recorded if a private cache is in use.\n\nWhat's your definition of a \"private cache\"?  Does a Wintel PC have a\nprivate cache?  If so, how about a Wintel PC that sits in a university lab,\nwhere it's shared by lots of students?\n\nHere's the problem to solve, from the origin server's perspective.  The\nserver sends a cookie to a user agent.  The lifetime is meant to be the\nshorter of, say, 3 hours or the end of a session.  If I say Max-Age=10800,\nUser1 (in a shared PC lab) might finish after one hour and exit the\nbrowser, thinking this will eliminate any context that had been\ninstantiated while s/he used the PC.  User2 comes along, starts up the PC,\ngoes to the same site, and inadvertantly starts using User1's cookie, which\nis \"a bad thing\".  So the origin server also wants to send Discard, so when\nthe user agent session ends (browser exits, user reboots Windows,\nwhatever), User1's cookie is gone.\n> \n> Version should be optional, if not included, it should default to V1.\n\nAgain, the S-C and S-C2 combining rules necessitate an explicit Version, so\na server can tell whether it's getting V0 or V1 cookies.  Remember, the\norigin server only sees a Cookie header (not Cookie2), which is ambiguous.\n\n> \n> The default for Max-Age has the same \"how long is a UA session\" problem\n> as Discard. IMHO the most robust solution is to have the cookie kept\n> indefinitely if no Max-Age is included.\n\nThat *is* the default, unless there's a Discard.  That's why Discard is\nneeded -- to override the default where an application needs to do so.\n\n> \n> 2. Quotes & Responses:\n> \n> Quote:\n> \"When it sends a\n>      \"secure\" cookie back to a server, the user agent should use no less\n>      than the same level of security as was used when it received the\n>      cookie from the server.\"\n> \n> Response:\n> What is greater or lesser security? Do we expect clients to record what\n> security they were using when they received the cookie and then, through\n> some as yet undefined mechanism, decide what \"greater\" or \"lesser\"\n> security than the original security mechanism means? This definition is\n> too fuzzy to be useful, I believe it should be removed.\n\nI agree it's fuzzy, but RFC 2068 says nothing about transport security, so\nit would be hard to be more specific.  I think we can agree, though, that\nencryption is more secure than no encryption.  I don't want a cookie that\nwas originally encrypted to be returned to the server as cleartext.  So we\n*have* to say something here, and removing the statement would be wrong.  I\ninvite alternative words that get the point across.\n\n> Quote:\n> \"If an attribute appears more than once in a cookie, the behavior is\n> undefined.\"\n> \n> Response:\n> Undefined things have a nasty habit of defining themselves. I propose\n> the sentence read \"If an attribute appears more than once in a cookie,\n> then the cookie is illegal and MUST be ignored.\"\n\nI was trying to be \"generous in what you accept\".  The HTTP spec., for\nexample, does not mandate that a request be ignored if a header is\nmalformed.  It's an implementation decision.  Returning to Set-Cookie,\nignoring a duplicate attribute is a valid behavior.  Ignoring the cookie is\na valid behavior.\n\n> \n> Quote:\n> \"HTTP/1.1 servers must send Expires: old-date (where old-date is a date\n> long in the past) on responses containing Set-Cookie2 response headers\n> |\n> unless they know for certain (by out of band means) that there are no\n> downsteam(sic) HTTP/1.0 proxies..\"\n> \n> Response:\n> I believe this sentence should be changed to read \"HTTP/1.1 servers MUST\n> send Expires: old-date (where old-date is a date long in the past) on\n> responses containing Set-Cookie2 response headers meant for single users\n> unless...\". We allow caching of Set-Cookie2 headers intended for\n> multiple people.\n\nThe point of the original paragraph (sect. 4.2.3; could you cite sections,\nplease?) was, I think, that HTTP1.0 caches are unreliable and can't be\ntrusted to honor caching directives correctly.  So stuff must be stored in\nthem pre-expired.  Even cookies intended for multiple users, because\nthere's no way to persuade such older caches that they must revalidate\ndocuments and cookies.  HTTP/1.1 caches will ignore Expires in favor of\nCache-Control and do the right thing.\n> \n> Quote:\n> \"   * The request-host is a FQDN (not IP address) and has the form HD,\n>      where D is the value of the Domain attribute, and H is a string\n>      that contains one or more dots.\"\n\n[4.3.2 Rejecting Cookies]\n> \n> Response:\n> The company Blah Inc. has the web site blah.com. Blah sells many\n> products, one of which is called bar. Bar has been released in several\n> versions, the newest of which is Foo. Blah wants to be able to present\n> information to its customer that it thinks the customer will be\n> interested in and it wants to present this information across all of its\n> sites. So it sends a cookie whose domain is .blah.com. If a user is\n> visiting foo.bar.blah.com and receives this cookie they will have to\n> reject it because it violates the above rule. It is totally appropriate\n> for Blah Inc. to want to hand out cookies that apply to all the sites it\n> owns. However instead of doing it simply by having a single cookie, it\n> now has to clutter the user's hard drive with cookies for every\n> *.blah.com site visited, not to mention complicating the server's\n> implementation. I believe this requirement is not reasonable, especially\n> for complicated sites.\n\nSorry, this piece has a long history of discussion, and I don't think we're\nwilling to change it, although I do understand your point.  The issue was\nhow to provide adequate flexibility to applications (and you don't think we\nhave) while preventing cookie-sharing abuses that might arise from sites\nthat send cookies with too-liberal Domain=.\n> \n> Quote:\n> \"User agents should allow the user to control cookie destruction....\"\n> \n> Response:\n> If a UA maker wants to never allow a customer access to cookie control\n> mechanisms, that is the UA maker's business, not the standards. We can\n> not threaten companies by saying \"Well if you don't create your\n> interface the way we say then you aren't compliant\" and expect to remain\n> credible as a standards organization. This is not a wire protocol\n> related issue. It is a feature issue and a matter of competitive\n> advantage for UAs.\n\nThis item, of course, is part of the broader discussion about what RFC 2109\ncan and cannot say about UA interfaces.  The members of the sub-group were\nquite firm in their belief that users should have control of cookies.\n\n> \n> Quote:\n> \"   * The value for the $Domain attribute must be the value from the\n> |\n>      Domain attribute, if any, of the corresponding Set-Cookie2 response\n> |\n>      header.  Otherwise the attribute should be omitted from the Cookie2\n> |\n>      request header.\n> |\n> \n>    * The value for the $Path attribute must be the value from the Path\n> |\n>      attribute, if any, of the corresponding Set-Cookie2 response\n> |\n>      header.  Otherwise the attribute should be omitted from the Cookie2\n> |\n>      request header\"\n> \n> Response:\n> All cookies have Domain and Path values. When not explicitly defined\n> they are implicitly defined. Thus a user agent will record these values,\n> explicit or not. The above requirements now dictate that the UA has to\n> record extra information, an indication if the Domain and Path are\n> implicit or explicit. I can find no good reason to place this\n> requirement on the UA. Instead we should simply require that the Domain\n> and Path, explicit or not, should always be returned with the cookie.\n\nYes, there we are requiring extra information.\n\nHere's why.  You cannot specify explicitly by Domain and Path the domain\nand path you get by default.  For example, suppose x.y.com sends a cookie.\nIf it leaves out Domain=, the default domain is x.y.com.  The cookie will\nbe returned *only* to that site.  However, if you say Domain=.y.com, the\ncookie gets sent to any site *.y.com, not just x.y.com.  Or you can say\n.x.y.com, which domain-matches *.x.y.com.\n\nThere's a similar behavior for Path regarding '/'.\n> \n> Quote:\n> \"Domain Selection\n>      The origin server's fully-qualified host name must domain-match the\n>      Domain attribute of the cookie.  The origin server's port number\n> |\n>      must equal the port number of the server that sent the cookie.\"\n> \n> Response:\n> Why do we have the port number requirement? If Blah Inc. has an HTTP\n> server on ports 80 and 81, why would we want to prevent sharing between\n> two ports on the same system?\n\nWell, for one thing the two servers may be administered separately for\ndifferent purposes, and letting them share cookies seems like a bad idea.\n\n> \n> Quote:\n> \"If multiple cookies satisfy the criteria above, they are ordered in the\n> |\n> Cookie2 header such that those with more specific Path attributes\n> precede those with less specific.  Ordering with respect to other\n> attributes (e.g., Domain) is unspecified.\"\n> \n> Response:\n> If we leave domain ordering undefined doesn't that sort of destroy the\n> utility of requiring path ordering?\n\nOkay, I was lazy, following Lou's example in the original spec.  I didn't\nwant to have to specify a multi-dimensional sorting algorithm.  Got any\nideas?  (I'm hoping that multiple cookies to the same site are rare, so it\nisn't that big a problem, but I do feel a little guilty it is so poorly\nspecified.)\n\n> \n> Quote:\n> \"User agents may offer configurable options that allow the user agent,\n> or\n> any autonomous programs that the user agent executes, to ignore the\n> above rule, so long as these override options default to ``off.''\"\n> \n> Response:\n> Again, I do not feel it is appropriate for this specification to dictate\n> to UA makers how to build the parts of their product that do not go over\n> the wire. If a UA maker wants this to default to \"ON\", that is their\n> business. If the UA maker wants to default to \"ON\" and not allow the\n> user to change the value, that is also their business. The mission, I\n> hope, is interoperability, not second guessing UA makers.\n\nSame user agent interface discussion as before.  And same user control\ndiscussion.\n> \n> Quote:\n> \"This state\n> management specification therefore requires that a user agent give the\n> user control over such a possible intrusion, although the interface\n> through which the user is given this control is left unspecified.\n> However, the control mechanisms provided shall at least allow the user\n> \n>    * to completely disable the sending and saving of cookies.\n> \n>    * to determine whether a stateful session is in progress.\n> \n>    * to control the saving of a cookie on the basis of the cookie's\n>      Domain attribute.\"\n> \n> Response:\n> Wire protocols have a massive effect on the range of functions a client\n> can implement. In effect, they restrict products. Software companies\n> have decided that interoperability is such an important product feature\n> that it is worth having their functionality restrained. However there is\n> another reason behind the software maker's behavior, they know that the\n> real battle is UI not features. Features tend to be a check-list, so\n> long as everyone has the same check marks, the competitive field remains\n> flat. The area of competition becomes primarily one of interface. When\n> standards step beyond the wire, beyond even functionality, and go into\n> the area that is the heart of computer software, they render themselves\n> irrelevant. Companies are not going to give up their competitive\n> advantage in order to be compliant with a standard. Worse yet, due to\n> press pressures, companies will be forced to look like they are\n> compliant, even when they are not. This reduces the ability of the IETF\n> to be an effective standards setting organization. Once companies are\n> forced to selectively ignore standards the goal of interoperability\n> becomes impossible.\n\nDitto.\n\nDave Kristol\n\n\n\n", "id": "lists-010-15673686"}, {"subject": "Re: draft-holtman-http-safe01.tx", "content": "In a previous episode David W. Morris said...\n\n:: What would your reaction be to including the safe/uahint header on the\n:: POST request? \n\nWouldn't that be a problem for 1.0 servers/cgis who would ignore the safe\nrequirement and process the non safe post anyhow? \n\nI do believe that control over the issue belongs on the request side\nto maintain symmetry with the way it is now with GET and POST.. I\nalmost don't see an alternative to a new method (back to\nGETWBODY). Generally what I'm getting at is the UA has a right to\ndictate whether changes may be made or \"I'm just browsing\".. (the\nserver of course can respond with \"get out of here!\" if it's process\nrequires mores than browsing type permission.)\n\nMy suspicion is that this really should be deferred to 1.2 so a new\nmethod can be added.\n\nThoughts?\n\n-Pat\n--\nPatrick R. McManus - Applied Theory Communications -Software Engineering\nhttp://pat.appliedtheory.com/~mcmanusProgrammer Analyst\n*** - You Kill Nostalgia, Xenophobic Fears. It's Now or Neverland. - ***\n\n\n\n", "id": "lists-010-15695316"}, {"subject": "Re: Cookies and cach", "content": "Andrew Daviel:\n>\n>OK, so I haven't read every piece of mail on this list ... \n>\n>Can someone answer this simple question:\n>\n>If I make a request for a page with a Cookie header, is the result \n>cacheable in a public cache ?\n\nAccording to both HTTP/1.1 and the state management spec: Yes, unless one of\nthe usual headers which prevent caching is added.  See the state management\nspec (RFC 2109) for details.\n\n>Andrew\n\nKoen.\n\n\n\n", "id": "lists-010-15704042"}, {"subject": "ID: wildcards in AcceptCharse", "content": "I believe I once promised to write this draft: it is about one of the issues\non the HTTP/1.1 issues list.  \n\nI just submitted it to the ID editor, but it is short enough to post it here\ntoo.\n\nKoen.\n\n---snip---\nHTTP Working Group                                     Koen Holtman, TUE\nInternet-Draft\nExpires: September 18, 1997                               March 18, 1997\n\n\n                   Wildcards in the Accept-Charset Header\n\n                    draft-holtman-http-wildcards-00.txt\n\n\nSTATUS OF THIS MEMO\n\n        This document is an Internet-Draft. Internet-Drafts are\n        working documents of the Internet Engineering Task Force\n        (IETF), its areas, and its working groups. Note that other\n        groups may also distribute working documents as\n        Internet-Drafts.\n\n        Internet-Drafts are draft documents valid for a maximum of\n        six months and may be updated, replaced, or obsoleted by\n        other documents at any time. It is inappropriate to use\n        Internet-Drafts as reference material or to cite them other\n        than as \"work in progress\".\n\n        To learn the current status of any Internet-Draft, please\n        check the \"1id-abstracts.txt\" listing contained in the\n        Internet-Drafts Shadow Directories on ftp.is.co.za\n        (Africa), nic.nordu.net (Europe), munnari.oz.au (Pacific\n        Rim), ds.internic.net (US East Coast), or ftp.isi.edu (US\n        West Coast).\n\n        Distribution of this document is unlimited.  Please send\n        comments to the HTTP working group at\n        <http-wg@cuckoo.hpl.hp.com>.  Discussions of the working\n        group are archived at\n        <URL:http://www.ics.uci.edu/pub/ietf/http/>.  General\n        discussions about HTTP and the applications which use HTTP\n        should take place on the <www-talk@w3.org> mailing list.\n\nABSTRACT\n\n   The HTTP/1.1 specification (RFC 2068) defines an Accept-Charset\n   header, but fails to define a wildcard \"*\" which could be used in\n   this header to match all character sets.  This proposal corrects\n   this omission.\n\n\n1  Introduction\n\n The HTTP/1.1 specification (RFC 2068) defines an Accept-Charset\n header, but fails to define a wildcard \"*\" which could be used in\n this header to match all character sets.  This proposal corrects this\n omission.\n\n A wildcard in the Accept-Charset header is considered important,\n because it allows a better specification of the acceptance of many\n character sets if it is used in combination with q values.  The\n support for many different character sets is one possible route (or\n transition path) for web internationalization.  The existence of this\n path, and the desirability of enabling it, was not properly\n recognized when he HTTP/1.1 specification [1] was written.\n\n A wildcard can only be used to give an inaccurate specification of\n the support levels for many character sets under HTTP/1.x-based\n server-driven negotiation [1], and this inaccuracy may lead to\n problems.  When used in HTTP transparent content negotiation [2]\n however, the wildcard does not cause inaccurate end results, and in\n fact can be used as a bandwidth-saving device (see section 4.2.1 of\n [3]).\n\n\n2 Proposed edits\n\n It is proposed to change the following text in section 14.2 of [1]:\n\n   The ISO-\n   8859-1 character set can be assumed to be acceptable to all user\n   agents.\n\n          Accept-Charset = \"Accept-Charset\" \":\"\n                    1#( charset [ \";\" \"q\" \"=\" qvalue ] )\n\n   Character set values are described in section 3.4. Each charset may\n   be given an associated quality value which represents the user's\n   preference for that charset. The default value is q=1. An example is\n\n          Accept-Charset: iso-8859-5, unicode-1-1;q=0.8\n\n   If no Accept-Charset header is present, the default is that any\n   character set is acceptable.\n\n to the text below:\n\n   The ISO-\n   8859-1 character set can be assumed to be acceptable to all user\n   agents.\n\n          Accept-Charset = \"Accept-Charset\" \":\"\n |                  1#( ( charset | \"*\" ) [ \";\" \"q\" \"=\" qvalue ] )\n\n   Character set values are described in section 3.4. Each charset may\n   be given an associated quality value which represents the user's\n   preference for that charset. The default value is q=1. An example is\n\n          Accept-Charset: iso-8859-5, unicode-1-1;q=0.8\n\n | The special value \"*\", if present in the Accept-Charset field,\n | matches every character set (including ISO-8859-1) which is not\n | mentioned elsewhere in the Accept-Charset field.  If no \"*\" is\n | present in an Accept-Charset field, then all character sets not\n | explicitly mentioned get a quality value of 0, except for\n | ISO-8859-1, which gets a quality value of 1 if not explicitly\n | mentioned. \n   If no Accept-Charset header is present, the default is that any character\n   set is acceptable.\n\n\n3 Compatibility considerations\n\n The syntax rules in the current version of the HTTP/1.1 specification\n [1] allow a charset value of \"*\" to be present in the Accept-Charset\n header.  Thus, servers which implement [1] will have no trouble\n parsing a header like\n\n      Accept-Charset: iso-8859-5;q=0.8, *;q=0.2\n\n According to [1], the \"*\" value should be interpreted as an unknown\n (unregistered) character set designator.  Thus, servers which\n implement [1] will simply ignore the wildcard if present.\n\n\n4 Security considerations\n\n This proposal adds no new HTTP security considerations.\n\n\n5 References\n\n   [1] R. Fielding, J. Gettys, J. C. Mogul, H. Frystyk, and\n       T. Berners-Lee.  Hypertext Transfer Protocol -- HTTP/1.1.  RFC\n       2068, HTTP Working Group, January, 1997.\n\n   [2] K. Holtman, A. Mutz.  Transparent Content Negotiation in HTTP.\n       Internet-Draft draft-ietf-http-negotiation-01.txt, HTTP Working\n       Group.\n\n   [3] K. Holtman, A. Mutz.  HTTP Remote Variant Selection Algorithm\n       -- RVSA/1.0.  Internet-Draft draft-ietf-http-rvsa-v10-00.txt,\n       HTTP Working Group.\n\n\n6 Author's address\n\n   Koen Holtman\n   Technische Universiteit Eindhoven\n   Postbus 513\n   Kamer HG 6.57\n   5600 MB Eindhoven (The Netherlands)\n   Email: koen@win.tue.nl\n\n\nExpires: September 18, 1997\n\n\n\n", "id": "lists-010-15711241"}, {"subject": "httpequiv and new http header", "content": "In HTML, <META HTTP-EQUIV=\"Blah\" is supposed to be equivalent to an\nHTTP header \"Blah:\", yes ?\n\nWhat is the position on creating new HTTP-EQUIV types (and presumeably\nequivalent HTTP headers) ?\n\nIn the Dublin Core metadata work a form <registry>.<name>[.<type>]\nseems to be accepted, e.g. \n<META NAME=\"DC.Author\" CONTENT=\"Joe Fish\">\nand perhaps\n<META HTTP-EQUIV=\"DC.Author.email\" CONTENT=\"jfish@pisces.org\">\nand the equivalent\nDC.Author.email: jfish@pisces.org\nas an HTTP header\n\nwhich might imply that the \"DC\" portion should be reserved\nfor the DC crowd, and registry-less names be reserved for the HTTP group.\n\nWhat I don't want to see, obviously, is people generating headers like\nExpires: 4/5/99\nLocation: Bournemouth\n\nAndrew Daviel\nTRIUMF &  Vancouver Webpages\n\n\n\n", "id": "lists-010-15724895"}, {"subject": "Re: Fact-checking: do any inservice proxy caches ever ignor", "content": "At 09:30 AM 19/03/97 +0100, Koen Holtman wrote:\n[...]\n>What I remember (though my memory may be faulty) about New Zealand\n>educational caches is this:  at the http BOF at www5, when we were discussing\n>the HTTP/1.1 caching design, someone connected to the New Zealand caches\n>wanted it to be possible for a compliant HTTP/1.1 cache to ignore all\n>attempts at a `reload' if the user was accessing some sites with a `low\n>educational value'.  The cache would just keep returning the old cached\n>object (with a warning) instead of revalidating with the origin server.\n>\n>I somehow assumed that this meant you would also ignore very short expires\n>times from from these sites.  Or are very short expires times from sites\n>with a `low educational value' not yet a problem?\n\n  The question to which my posting was addressed referred to current\npractice. I don't know of anyone now doing this, and the software I'm\nworking with now does not allow Expires headers to be ignored.\n  Yes, I would still wish to have the option of setting a cache to limit\nthe bandwidth expended on sites unconnected with the purposes of the\norganisation operating the cache.\n\n- Donald Neal\n\n\n\n", "id": "lists-010-15733309"}, {"subject": "RE: Unverifiable Transactions / Cookie draf", "content": "At 11:21 PM 18/03/97 -0800, Yaron Goland wrote:\n>The IETF is not a social engineering organization and its purpose is not\n>for users to \"forcefully indicate their preferences to UA vendors.\" I\n>also wasn't aware that \"UA publishers have been left with a great deal\n>of freedom as to how they present the required information to the user.\"\n>was a bad thing. I thought this was a free country where UA vendors are\n>free to produce the product they want and users are free to use it if\n>they want.\n\n  I hadn't realised either that this discussion was taking place in a\nparticular country or that it was illegal anywhere to produce non-compliant\nproducts.\n  Passing information about people between companies without the consent of\nthe person involved, on the other hand, is subject to legislative control,\nin the country I'm discussing this from as in other places. What's the\nstatus of a product sold in such a country which denies the user a means to\nprevent a potentially illegal practice, I wonder?\n\n- Donald Neal\n\n\n\n", "id": "lists-010-15741829"}, {"subject": "Re: draft-holtman-http-safe01.tx", "content": "On Wed, 19 Mar 1997, Patrick McManus wrote:\n\n> In a previous episode David W. Morris said...\n> \n> :: What would your reaction be to including the safe/uahint header on the\n> :: POST request? \n> \n> Wouldn't that be a problem for 1.0 servers/cgis who would ignore the safe\n> requirement and process the non safe post anyhow? \n\nThere are also servers/applications which use GET and violate the safe\nrule. Your postulate is that an unknown third party is going to reverse\nengineer your safe POST interface and you are going to change that\ninterface to be non-safe. It would be foolish of you to ignore the client\nrequirement that the request be safe.\n\n> \n> I do believe that control over the issue belongs on the request side\n> to maintain symmetry with the way it is now with GET and POST.. I\n> almost don't see an alternative to a new method (back to\n> GETWBODY). Generally what I'm getting at is the UA has a right to\n> dictate whether changes may be made or \"I'm just browsing\".. (the\n> server of course can respond with \"get out of here!\" if it's process\n> requires mores than browsing type permission.)\n\nI don't see that a new method is required. A request header can establish\na R/O requirement ... except it may not be known if the server cares to\nobey the reqwuirement.\n\n\n\n> My suspicion is the this really should be deferred to 1.2 so a new\n> method can be added.\n\nWaiting for a version of HTTP which isn't on any working groups schedule\nis a problem.  The requirement you are posing wasn't part of the\ndiscussion (that I can recall) that resulted in the safe: response header.\nImproving the usability of internationalized and other applications where\na BODY was required/desired was the motivation. Timely deployment was\nconsidered important.\n\nI think the addition of the UA-Hint response header which carries advice\na user agent can use to improve the application can be incorporated \nwithin existing HTTP protocols. My intent was that nothing serious breaks\nif the header is ignored. (To the extend that an application needs the\nimproved security from the more precise history list control, they might\nneed to key off of UA version, etc.)\n\nI don't believe either proposed syntax (safe or uahint) precludes addition\nof a new method for GETWBODY.\n\nDave MOrris\n\n\n\n", "id": "lists-010-15749841"}, {"subject": "Re: Issues with the cookie draf", "content": "Dave Kristol <dmk@bell-labs.com> wrote:\n>Yaron Goland wrote:\n>>[...]\n>> 2. Quotes & Responses:\n>> \n>> Quote:\n>> \"When it sends a\n>>      \"secure\" cookie back to a server, the user agent should use no less\n>>      than the same level of security as was used when it received the\n>>      cookie from the server.\"\n>> \n>> Response:\n>> What is greater or lesser security? Do we expect clients to record what\n>> security they were using when they received the cookie and then, through\n>> some as yet undefined mechanism, decide what \"greater\" or \"lesser\"\n>> security than the original security mechanism means? This definition is\n>> too fuzzy to be useful, I believe it should be removed.\n>\n>I agree it's fuzzy, but RFC 2068 says nothing about transport security, so\n>it would be hard to be more specific.  I think we can agree, though, that\n>encryption is more secure than no encryption.  I don't want a cookie that\n                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n>was originally encrypted to be returned to the server as cleartext.  So we\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>*have* to say something here, and removing the statement would be wrong.  I\n>invite alternative words that get the point across.\n\nWhen we were field testing the cookie implementation in Lynx,\nit appeared the rule rather than the exception that cookies received\nvia SSL connections, e.g., for banking transactions, did not include\nthe Secure attribute.  Based on the wording in the RFC, under such\ncircumstances the cookie could be sent as cleartext if the domain\nand path checks pass.   This didn't seem like a good thing to do\n(and you apparently agree 8-), so we arbitrarily tag such cookies\nas secure whether or not the Secure attribute was present in the\nSet-Cookie header.\n\nAs things presently stand on the Web, what that really means\nis that if a Set-Cookie header was received in reply to a request using\nthe https scheme, Lynx will include the cookie only in requests also\nusing the https scheme.  Furthermore, it seems unlikely that a server\nwould ever include the Secure attribute in a Set-Cookie header that was\nnot sent encrypted.  So, in effect, the presence or absence of the\nSecure attribute becomes irrelevant (purely \"advisory\", or \"confirmatory\"\n8-), and the actual variable is whether or not encryption was used.\n\nOne problem with this \"better safe than sorry\" implementation\nis that the site may be using the cookie solely for tracking, and may\nindeed want it included for both encrypted and unencrypted requests\nthat pass the domain and path checks.  It's another can of worms, like\nthe \"unverifiable transactions\" issue, that's ultimately attributable\nto lack of hard information to the UAs/users about the uses sites will\nmake of cookies.   Though the \"better safe than sorry\" implementation\nseems preferable, IMHO, a clear justification for it should be included\nin the revision.\n\nI also agree that if the wording of the revision requires UAs\nto make relative judgements about the \"level of security\" offered by\ndifferent encryption schemes without clear guidelines on how to make\nthem, you'll be creating an implementation nighmare.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-15759827"}, {"subject": "Re: Issues with the cookie draf", "content": "Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n  > [...]\n  > >[DMK]\n  > >I agree it's fuzzy, but RFC 2068 says nothing about transport security, so\n  > >it would be hard to be more specific.  I think we can agree, though, that\n  > >encryption is more secure than no encryption.  I don't want a cookie that\n  >                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  > >was originally encrypted to be returned to the server as cleartext.  So we\n  >  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  > >*have* to say something here, and removing the statement would be wrong.  I\n  > >invite alternative words that get the point across.\n  > \n  > When we were field testing the cookie implementation in Lynx,\n  > it appeared the rule rather than the exception that cookies received\n  > via SSL connections, e.g., for banking transactions, did not include\n  > the Secure attribute.  Based on the wording in the RFC, under such\n  > circumstances the cookie could be sent as cleartext if the domain\n  > and path checks pass.   This didn't seem like a good thing to do\n  > (and you apparently agree 8-), so we arbitrarily tag such cookies\n  > as secure whether or not the Secure attribute was present in the\n  > Set-Cookie header.\n\nWell, I mis-spoke, but your point is an interesting one.  Actually there\nare a couple of interesting issues.\n\n1) Is \"Secure\" implicit?\n2) If not, then for example sending a cookie for an http: request after\nreceiving it from an https: request means we're violating the constraint\nthat the cookie should only be sent back to the same port it came from.\n\nWhat we were *trying* to do was more or less document what Netscape did\noriginally.  So,\n\n1) \"Secure\" was a hint.  (I wonder if anyone actually uses it.)\n2) A cookie marked as \"Secure\" should only go out securely.  Although there\nwas no such requirement, one presumes it was received securely!\n3) A cookie not marked as \"Secure\" could go out in other requests.  This\nwould appear to conflict with the \"to same port\" constraint I added in\nresponse to someone's comment.\n\n  > [...]\n  > \n  > One problem with this \"better safe than sorry\" implementation\n  > is that the site may be using the cookie solely for tracking, and may\n  > indeed want it included for both encrypted and unencrypted requests\n  > that pass the domain and path checks.  It's another can of worms, like\n  > the \"unverifiable transactions\" issue, that's ultimately attributable\n  > to lack of hard information to the UAs/users about the uses sites will\n  > make of cookies.   Though the \"better safe than sorry\" implementation\n  > seems preferable, IMHO, a clear justification for it should be included\n  > in the revision.\n\nWhat I said in my email misrepresents what the spec. says.  I think your\n\"better safe than sorry\" approach, though interesting and justifiable,\ndoesn't match the spec.\n  > \n  > I also agree that if the wording of the revision requires UAs\n  > to make relative judgements about the \"level of security\" offered by\n  > different encryption schemes without clear guidelines on how to make\n  > them, you'll be creating an implementation nighmare.\n\nAs it is, it's a specification nightmare. :-)  It's probably a good bet\nthat no one in ietf-tls would care to rate the relative level of security\nof the various encryption schemes.  But we can all probably agree that\nDES is better than cleartext or, say, rot13 or base64.\n\nDave Kristol\n\n\n\n", "id": "lists-010-15771037"}, {"subject": "Re: Issues with the cookie draf", "content": "To:http-wg@cuckoo.hpl.hp.com\n\n\n\nOn Wed, 19 Mar 1997, Dave Kristol wrote:\n\n> Well, I mis-spoke, but your point is an interesting one.  Actually there\n> are a couple of interesting issues.\n> \n> 1) Is \"Secure\" implicit?\n> 2) If not, then for example sending a cookie for an http: request after\n> receiving it from an https: request means we're violating the constraint\n> that the cookie should only be sent back to the same port it came from.\n> \n> What we were *trying* to do was more or less document what Netscape did\n> originally.  So,\n> \n> 1) \"Secure\" was a hint.  (I wonder if anyone actually uses it.)\n> 2) A cookie marked as \"Secure\" should only go out securely.  Although there\n> was no such requirement, one presumes it was received securely!\n> 3) A cookie not marked as \"Secure\" could go out in other requests.  This\n> would appear to conflict with the \"to same port\" constraint I added in\n> response to someone's comment.\n\nI think the same port requirement makes no sense unless the set-cookie\nincluded a port specification.  After all, allowing a cookie to be\nshared between x.y.com and w.y.com which are likely to be two machines\nbut not between x.y.com:80 and x.y.com:8080 which most likely will be\none machine and under a tighter span of control seems like a misdirected\nconcern. Hence, I would propose removing the port match requirement.\n\nFollowing that, I would argue that sharing cookies between http and https\nis very desireable and as long as the other verifications requirements are\nmet, doesn't create additional exposures.\n\nMy basic premise is that SSL transactions are expensive in CPU and will\nalso impact raw wire transfer time because MODEM/router hop level\ncompression will be almost useless. Given that assumption, assume the\nfollowing application design:\n\na) SSL post of a login request\nb) Response includes a session ID cookie NOT marked secure\nc) The user's next several interactions accumulate a list of things to\n   purchase.\nd) The user clicks CHECKOUT which brings a response which includes\n   a secure POST of a credit card or PIN number to confirm the purchase.\n\nIn this example, the cookie is issued in a secure response, carried  in\nmultiple unsecure transactions and finally used in another secure\ntransaction.\n\nOR, perhaps step a isn't needed and the cookie is issued in an unsecure\nconnection, used to accumulate  the shopping basket and finally returned\non a secure connection to complete the transaction.  In either case,\nsubstantial system resources are saved by limiting the duration of the SSL\n'session'.\n\nDave Morris\n\n\n\n", "id": "lists-010-15781978"}, {"subject": "Re: (ACCEPT*) Draft text for Accept header", "content": "Maurizio Codogno:\n>\n>Koen wrote:\n>\n>% If you have comments on this text, now is the time to comment.  I\n>% intend to close this issue at the end of the week.  This means that I\n>% will send a last call for disagreement with perceived consensus,\n>% together with a possibly improved version of the text below, in a few\n>% days.\n>\n>%    If no Accept header is present, then it is assumed that the client\n>% |  accepts all media types.  If Accept headers are present, and if the\n>% |  resource cannot send a response which is acceptable according to\n>% |  the Accept headers, then the server should send an error response\n>% |  with the 413 (not acceptable) status code, though the sending of an\n>% |  un-acceptable response is also allowed.\n>\n>I don't agree with this: let's suppose I want an audio file, but I can\n>read only .wav and no .au . Should I get the whole .au file and then\n>notice that it is unreadable for me?\n\nNo, of course not.  The server _should_ send an 413 response if it\nonly has data you cannot accept, but it is not _required_ to.  We\nexpect all major HTTP/1.1 servers to implement checking of Accept\nheaders, so that you won't get an .au you can't handle if you contact\nan average web site.  We do not expect major servers to take the easy\nway out and ignore the `should send an error response' above.\n\nNote that most existing major HTTP/1.0 servers _do_ take the easy way\nout, they do not check the Accept headers to see if you can handle the\ndata.  This lack of checking is of course also inspired by some\nHTTP/1.0 user agents sending out incorrect Accept headers.\n\nThe text `though the sending of an un-acceptable response is also\nallowed' above is meant for the authors of minimal, specialized\nHTTP/1.1 server programs which are meant to only serve text/html or\nsimilar types that are always acceptable.  We agreed in the content\nnegotiation subgroup that we would not want the spec to require such\nauthors to add Accept header checking code.\n\nIf such a minimal HTTP/1.1 server (or an existing HTTP/1.0 server) is\nrude enough to send you an .au file you cannot handle, you should not\nget the whole .au file and then notice it is unreadable.  Your\nHTTP/1.1 user agent is encouraged to check the Content-Type header of\nthe response, and to abort the transfer if that header indicates that\nthe response is .au (audio/basic) data you can't handle.  This is\ncovered in the text about the 413 status code.\n\n> It does not seem sensitive, especially\n>since there is already a default to get any type (namely, no Accept line\n>at all)\n\nThe default if an accept header is present is _not_ that you can send\nany type, the default is that you should only send the types that are\naccepted.  But minimal servers are not required to use this default if\nan accept header is present.\n\n>Is it possible at least to state things in such a way that in 1.2 the\n>situation will change (I don't know, maybe adding an Accept-Only\n>header if people believe that my view is stupid)?\n\nThe language in the 1.1 accept header text already allows for a\nstrengthening of the requirements in 1.2,  there is nothing in there\nthat would block it.\n\n> After all, we have\n>already a SHOULD, so it should not be that difficult to promote it to\n>a MUST in the next version...\n\nYes, you are right.  If it turns out that almost all future HTTP/1.1\nservers indeed check the Accept headers, as they should, it is\ndefinitely possible that we can strengthen the `should check' to a\n`must check' in HTTP/1.2.  But with even major HTTP/1.0 servers not\ndoing any checking, it is too soon to go for an `all servers must\ncheck' in 1.1.\n\n>.mau.\n\nKoen.\n\n\n\n", "id": "lists-010-1578326"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "Koen Holtman wrote:\n> \n> The redirection won't be unverifiable if the script which does the counting\n> and redirection to the page is on the target site already.\n> ....\n> Yes, if the script is on the site which served the banner.  But I don't see\n> this as a big problem.\n> \n\nI see this as a big problem. Many, Many sites are using cookie\ntechnology to\nsimply rotate banners. (Not necessarily to track user activity, although\nthis\ncan be a result of the same technology.) Many of these sites do not have\naccess\nto place scripts on their servers. You will be depriving a huge number\nof people\na functionality that they have been using for a while. \n\nThe bigger entities will be able to place programs on their servers to\nimplement\ncentral ad serving and duplicate all of the features that cookies\nprovide. \n(including abusing the privacy issues) While the smaller entities will\nbe given\na burden of not being able to use central ad serving techniques.\n\nSome Network examples :\n  Networks such as Burst!, Doubleclick, Web Rep, etc\n\nSome Banner Exchange examples :\n  Link Exchange,Banner Central, Net On\n\nSome Banner Serving Services :\n  Focallink, GlobalTrak\n\nThanks,\nDave Stein\nBurst!, http://www.burstmedia.com\n\n\n\n", "id": "lists-010-15792197"}, {"subject": "Re: Issues with the cookie draf", "content": "On Wed, 19 Mar 1997, David W. Morris wrote:\n> I think the same port requirement makes no sense unless the set-cookie\n> included a port specification.  After all, allowing a cookie to be\n> shared between x.y.com and w.y.com which are likely to be two machines\n> but not between x.y.com:80 and x.y.com:8080 which most likely will be\n> one machine and under a tighter span of control seems like a misdirected\n> concern. Hence, I would propose removing the port match requirement.\n\nI agree with your point above.  I think the rationale was that high-port\nservers could be run by users and that any user could thereby capture\ncookies intended for a low-port (root-run) server.  Marginal case.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15800344"}, {"subject": "Re: 305 Use prox", "content": "> Roy said\n> > Josh said\n> \n> Well, for starters, please use a syntax that can be unambiguously parsed.\n> That means enclose all URLs in \"double-quotes\" or <angle-brackets> if\n> you intend them to be used alongside parameters.  I suggest using a\n> syntax similar to Link in the RFC 2068 appendix, since it overcame the\n> same set of problems.\n> \nI expected that :)\n\n> >Suggested rules:\n> >Origin servers may NOT send 305, only proxies may send them.\n> \n> Nope.  The original intended purpose of 305 is to allow an origin server\n> to prevent access unless it goes through the appropriate proxy.\n> \nI agree that an origin server based redirect is a good idea,\nand although I cant quickly come up with a case for it which\ncouldnt acheive the same results by other means, I think\nthis functionality is worthwhile.  However, from a security\nstandpoint I think its hard to implement.\n\nThe two cases are for different uses.\n\n> >The set-proxy header is HOP BY HOP, not end to end.\n> \n> You mean that the 305 response is HOP by HOP -- it doesn't make any\n> sense to just drop the header field.\nYes.\n> \n> >On scope:\n> \n> Hmmm, looks like a realm, smells like a realm, why not call it a realm?\n> \nRealm has been used before, but its meaning has been made unclear \nsince current definitions of Realm have turned into a text\ndescription rather than a specific identifier.\nTo call this realm might lead to confusion.\n\n> The original description only applied to the exact URL.  I agree that\n> a realm would be more efficient, subject to a good set of security\n> considerations.\nMaybe it is wise to separate proxy originated redirects ( as a means\nof load balancing ) from origin redirects.  \nMaybe they could share the set-proxy header but use 306 as well as 305.\n\n> >3) Loops / hop counts\n> >What happens if proxy A redirects the client to proxy B which\n> >redirects it back to A?\n> \n> The same that happens on all auto-redirection loops.  It is actually\n> noted in the section on 3xx responses.\nI think we should leave this to the network designers.\n\n> Create a new one.  The problem with using the Location header was that\n> some older client might follow the HTTP rules and treat the response\n> as if it were a 300, which would entice it to perform the original\n> request on the URL in the Location header, which in this case would\n> be the base proxy URL.  I suggest using \"Proxy\", since it seems more\n> advisory in nature than \"set-proxy\".  Also, you should consider allowing\n> the server to send multiple fields so that the client can be given a\n> \"choose one\" option.\nWell, I think set-proxy is clearer, since the header is really\nadvising an action which is 'set to this proxy' for this resource.\n> \n> Why not include the notion of \"trusted\" proxies?  It should be possible\n> for a user agent to collect a list of trusted proxies in much the same\n> way it would collect a list of trusted cookie sites, and there won't\n> be that many proxies for any given user agent.  The user agent could\n> then be set to accept direction automatically from trusted proxies,\n> and query for all others.\nThis is a reasonable idea, the list could be given to the client\nat the beginning of its session, maybe through the Proxy AutoConfig\nmethods.\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department            \"Mighty Morphin' Proxy\nRanger\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n", "id": "lists-010-15808706"}, {"subject": "Re: 305 Use prox", "content": "On Wed, 19 Mar 1997, Josh wrote:\n\n> > Roy said\n> > > Josh said\n> > \n> > >Suggested rules:\n> > >Origin servers may NOT send 305, only proxies may send them.\n> > \n> > Nope.  The original intended purpose of 305 is to allow an origin server\n> > to prevent access unless it goes through the appropriate proxy.\n> > \n> I agree that an origin server based redirect is a good idea,\n> and although I cant quickly come up with a case for it which\n> couldnt acheive the same results by other means, I think\n> this functionality is worthwhile.  However, from a security\n> standpoint I think its hard to implement.\n\nI'm missing a point somewhere ... why do you think there is a greater\nsecurity issue with an origin server specifing a proxy redirect than\na proxy doing it? My sense is that the converse is true. Since the\nredirect is hop-hop, it seems like the origin server would be at least\nas trusted as any proxy in terms of telling a user where to get resources\nlogically owned by the origin server.\n\nDave Morris\n\n\n\n", "id": "lists-010-15819627"}, {"subject": "Re: determining proxy reliabilit", "content": "Patrick McManus writes:\n    I'm starting to see this in a new light, your argument about\n    protocol trust is a good one. In summary non reliable worries me more\n    than non compliant, read on. What I'm still hesitant on is what I\n    feel will be a very strong content-provider hesitation to this\n    proposal because it's accuracy is so unbounded.\n\nIt's not clear that the accuracy is really that unbounded.  Assuming\nthat non-compliance is an orthogonal issue, the three things that\ncould lead to inaccuracies are\n(1) perturbations of access patterns, due (as Koen has\nargued) to the potential for more cache-busting outside\nthe metering subtree\n(2) failure (or reboot) of a proxy before a report is\ndelivered\n(3) loss of a report message before it reaches the origin\nserver (i.e., through network failure)\nIf there are other sources of inaccuracy that I've missed, please\nlet me know.\n\nItem #1 is, for now, unknowable.  Perturbation could just as easily\nimprove the situation, since, as you observe, if hit-metering increases\ncaching, then more users might be accommodated.\n\nItem #2 is addressed in the latest draft, by adding an optional\ntimeout to the Meter response-directive (i.e., to the server's\nrequest that the response be hit-metered).  This can't eliminate\nthe problem of proxy crashes or reboot, but it can bound the\nlikelihood of report-lost-due-to-proxy-failure.  E.g., if the\ntimeout is set to 10 minutes, and the mean time between reboots\nfor the \"average\" proxy is (say) 60 minutes, then there is a 1/6\nchance of report loss.  Since I suspect that MTBF for proxies is\nprobably on the order of days, not hours, the actual loss\nprobability is likely to be lower.\n\nThis leaves #3, loss-in-transit.  My experience is that the most\ncommon way for servers to lose HTTP requests is due to internal\ncongestion (i.e., the SYN_RCVD problem), so if hit-metering\nimproves caching, the reduction in congestion ought to help this.\nBut loss due to network partition is also a problem, and (according\nto Vern Paxson's SIGCOMM '96 paper) it's getting worse.  This\nhas inspired me to change the text in the next version of the\ndraft from \"The proxy is not required to retry the [report]\nif it fails\" to \"The proxy is not required to retry the [report]\nif it fails (but it should do so, subject to resource constraints).\"\nThis is still \"best-efforts\", but the specification now encourages\nmore effort.\n\nThe next draft will also say:\n   Note that if there is doubt about the validity of the results of\n   hit-metering a given set of resources, the server can employ\n   cache-busting techniques for short periods, to establish a baseline\n   for validating the hit-metering results.\n(with a citation to James Pitkow's WWW6 paper for more discussion\nof such sampling techniques).  Given that this gives each origin\nserver a way to answer the question \"is hit-metering making my\ncounts inaccurate?\", it seems to avoid the question of whether\nhit-metering is accurate in general.  (Clearly, a server that\ndiscovered this way that hit-metering is giving bad results would\nsimply stop using hit-metering, at least for a while.)\n\n    I made a proposal months ago about being able to (at the origin\n    servers option) force the return of 0/0 counts.. at least this would\n    allow the construction of deterministic audit trails and therefore some\n    notion of reliability.. it doesn't account for outright fraud by the\n    proxy of course (they could misreport the numbers) but it does close\n    the case of any open ended transactions.. I'm not sure that it is\n    enough, but I do think it helps considerably in establishing 'good\n    faith and a reliable history' which is something to go on..\n    \nI tried putting support for 0/0 counts in a version of the\nproposal, but I took it out in favor of the timeout mechanism.\nJames Pitkow's paper points out that the lack of a time-bound\non the reports was a serious flaw of the original proposal.\n\nI think if the origin server can say \"send a report within\nX minutes, if you have anything to report\" then this effectively\ndoes the same thing as a request for 0/0 reports, but without\nthe additional message overhead.  (Remember, lots of studies have\nshown that most cache entries are never used more than once.)\nA 0/0 report also doesn't solve the \"proxy rebooted before sending\na report\" problem, but the timeout \"solves\" it (probabilistically).\n\n    -Pat, not feeling bad about bringing this back up when it's still in\n    ID and considering we can do 50 messages a day on cookies that are\n    nearing last call..\n\nYour comments have been quite valuable.\n\n-Jeff\n\n\n\n", "id": "lists-010-15828117"}, {"subject": "Re: Issues with the cookie draf", "content": "dmk@research.bell-labs.com (Dave Kristol) wrote:\n>Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n>  > [...]\n>  > >[DMK]\n>  > >I agree it's fuzzy, but RFC 2068 says nothing about transport security, so\n>  > >it would be hard to be more specific.  I think we can agree, though, that\n>  > >encryption is more secure than no encryption.  I don't want a cookie that\n>  >                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n>  > >was originally encrypted to be returned to the server as cleartext.  So we\n>  >  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>  > >*have* to say something here, and removing the statement would be wrong.\n>  > >I invite alternative words that get the point across.\n>  > \n>  > When we were field testing the cookie implementation in Lynx,\n>  > it appeared the rule rather than the exception that cookies received\n>  > via SSL connections, e.g., for banking transactions, did not include\n>  > the Secure attribute.  Based on the wording in the RFC, under such\n>  > circumstances the cookie could be sent as cleartext if the domain\n>  > and path checks pass.   This didn't seem like a good thing to do\n>  > (and you apparently agree 8-), so we arbitrarily tag such cookies\n>  > as secure whether or not the Secure attribute was present in the\n>  > Set-Cookie header.\n>\n>Well, I mis-spoke, but your point is an interesting one.  Actually there\n>are a couple of interesting issues.\n>\n>1) Is \"Secure\" implicit?\n>2) If not, then for example sending a cookie for an http: request after\n>receiving it from an https: request means we're violating the constraint\n>that the cookie should only be sent back to the same port it came from.\n>\n>What we were *trying* to do was more or less document what Netscape did\n>originally.  So,\n>\n>1) \"Secure\" was a hint.  (I wonder if anyone actually uses it.)\n\nI've never seen it used.\n\n\n>2) A cookie marked as \"Secure\" should only go out securely.  Although there\n>was no such requirement, one presumes it was received securely!\n>3) A cookie not marked as \"Secure\" could go out in other requests.  This\n>would appear to conflict with the \"to same port\" constraint I added in\n>response to someone's comment.\n>\n>  > [...]\n>  > \n>  > One problem with this \"better safe than sorry\" implementation\n>  > is that the site may be using the cookie solely for tracking, and may\n>  > indeed want it included for both encrypted and unencrypted requests\n>  > that pass the domain and path checks.  It's another can of worms, like\n>  > the \"unverifiable transactions\" issue, that's ultimately attributable\n>  > to lack of hard information to the UAs/users about the uses sites will\n>  > make of cookies.   Though the \"better safe than sorry\" implementation\n>  > seems preferable, IMHO, a clear justification for it should be included\n>  > in the revision.\n>\n>What I said in my email misrepresents what the spec. says.  I think your\n>\"better safe than sorry\" approach, though interesting and justifiable,\n>doesn't match the spec.\n\nYes, the way we coded it does not \"technically\" match RFC2109.\nIt's presently irrelvant for that spec, because the port contraint\nyields the same result for https versus http requests.   But that\ncontraint was not part of the original cookie implementations, and\nwhat is actually encountered on today's Web.   My point is that,\nrightly or wrongly, we \"arbitrarily\" took the posture you stated in\nyour reply to Yaron:\n\n\"I don't want a cookie that was originally encrypted to be\nreturned to the server as cleartext.\"\n\ni.e., that Secure should be implicit for encrypted cookies.  It seemed\nbetter to let the cookie handling \"fail\" under some circumstances than\nrisk that happening (though it could \"fail\" anyway, at present, for\nUAs which adopted the RFC2109 port contraint when communicating with\nservers which do not expect to be contrained in this way).\n\nThe reasoning was that under some circumstances, sending an\noriginally encrypted cookie as cleartext might go beyond \"privacy\"\nrisks to an actual security risk, and providers might not understand\nthe issues adequately to include the Secure attribute in such cases,\nso don't *ever* take that risk.\n\n\n>  > I also agree that if the wording of the revision requires UAs\n>  > to make relative judgements about the \"level of security\" offered by\n>  > different encryption schemes without clear guidelines on how to make\n>  > them, you'll be creating an implementation nighmare.\n>\n>As it is, it's a specification nightmare. :-)  It's probably a good bet\n>that no one in ietf-tls would care to rate the relative level of security\n>of the various encryption schemes.  But we can all probably agree that\n>DES is better than cleartext or, say, rot13 or base64.\n\nThe point, though, is that no formal ranking of encryption schemes\nw.r.t. \"security level\" is likely to be forthcoming.  If a revision indicates\nexplicitly that encrypted cookies have an implied Secure, which we can see\nfrom the recent discussion is \"controversial\" itself, added a ranking\nrequirement with no formal ranking mechanism surely will create a \"no\npossibility of broad consensus\" dilemma.  Sigh...\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-15839718"}, {"subject": "RE: 305 Use prox", "content": "How about a way to indicate that the client should access the origin \nserver directly, i.e., should not use a proxy at all?\n\nThanks,\nKip\n\n-----Original Message-----\nFrom:Josh Cohen [SMTP:josh@netscape.com]\nSent:Tuesday, March 18, 1997 6:30 PM\nTo:http-wg@cuckoo.hpl.hp.com\nSubject:305 Use proxy\n\nBelow are notes and discussion points on 305 use proxy,\nplease read at your convenience, and voice your opinions..\n\nThis is a work item for memphis\n\n------\nComments, flames, death threats welcome..\n305 Use Proxy Response\n\nAfter some discussions, here are some thoughts and suggestions\non the use and implementation of the 305 use proxy response.\n\nOverview of suggestion:\n\n(For the example, Ill assume set-proxy: as the header name)\n(no bnf, Im going for concept... that can be discussed later )\n\nset-proxy : proxy-url scope=scopePrefix\ntype= once\n| forever\n| hits count=hitcount\n| lifetime=seconds\n\nfor GET http://www.foo.com/services/index.html HTTP/1.1\n\nexample response:\nHTTP/1.1 305 Use Proxy\nset-proxy : http://proxy1.foo.com:8080/ \nscope=http://www.foo.com/services/\n\nSuggested rules:\nOrigin servers may NOT send 305, only proxies may send them.\nThe set-proxy header is HOP BY HOP, not end to end.\n\nOn scope:\n  If the returned scope is 'wider' that the request minus the part\nof the path to the right of the final slash, the header should be\nrejected or the user should be queried at the least.\n\nExample:\nfor the above request ( http://www.foo.com/index.html )\n scope=http://www.foo.com/services/ VALID\n scope=http://www.foo.com/         INVALID\n\nSo basically:\nfor a client: ( depending on level of paranoia )\n (A)  if the scope is 'wider' than the requested URL, the user\nis queried.\n (B) if the scope is wider than the requested URL, and the \ndestination\nproxy is not in the same domain, query the user\n\nNotes on discussions:\n\nCases for use:\n1. An origin server wishes to redirect a client to use a proxy\nto access its resources\n\n2. A proxy server wishes to redirect a client to another proxy.\n(the client can be another proxy )\n\nThe Current spec:\n10.3.6 305 Use Proxy\n\nThe requested resource MUST be accessed through the proxy given by \nthe\nLocation field. The Location field gives the URL of the proxy. The\nrecipient is expected to repeat the request via the proxy.\n\n\nIssues:\n1) scope:\nDoes this apply to only this exact URL or any others\n\n2) validity time:\nShould the client use this proxy for this or these resources\nforever, or for how long, or how many transactions?\n\n3) Loops / hop counts\nWhat happens if proxy A redirects the client to proxy B which\nredirects it back to A?\n\n4) is Location: header enough?\nDoes the location allow enough flexibility to express some or\nall of these?\n\nIm impartial on the new header / extend location header issue,\nHowever, the security implications make the new header a\nprobable choice, IMHO.\n\n5) Security\nIf the scope is 'wide' ie for all HTTP transactions or even\njust for one domain, how does the client know to trust that the\nresponse was not from a malicious server.\n\nPossible solutions:\n\nScope:\nThe redirecting host should be able to indicate a mask\nfor urls which are to be redirected to this proxy.\nNaturally, this has security implications, ie an origin server\nwhich tells a client to redirect to an evil proxy for ALL urls.\nA safe suggestion I think it that that scope can be a prefix\nwhich may only affect 'less significant URLS'..\nExample:\n  Client requests http://www.ups.com/services/index.html\n  Server redirects to proxy1.ups.com\nallowed scope: http://www.ups.com/services/*\n    not allowed scope: http://www.ups.com/*\n    not allowed scope: http://www.mcom.com/*\n\nValidity Lifetime:\nWe couldnt come to a unanimous consenses here, except for\nthe fact that the current spec doesnt state anything about it.\n\nThe main ideas are:\n1) use this redirection forever ( or until the client is restarted )\n2) use this redirection just once, and come back the next time\n3) use this redirection for a specified amount of time\n4) use this redirection for a specified amount of transactions\n\nWhile its hard to come up with useful examples for all the cases,\nI beleive that a format which is flexible enough to allow any of them\nto be expresses is smartest.\n\nWhile caching and proxies have been in use for a non trivial amount of \ntime,\ncomplex, hierarchical cache systems are only starting to be deployed. \nBecause of this early stage, I feel that its best to keep as many\noptions open as possible, and give an much flexibility to \nadministrators\nas possible.\n\nLoops:\nOverall, this should be a solveable problem.  Intelligent\nICP type protocols should be able to avoid loops, but the idea\nof some sort of 'redirected via' flag/signal/indicator isnt \nunreasonable.\n\nLocation: header.\nIf people feel that we need to express a scope and a lifetime,\nwe'll need to either extend the Location: header or create a new one.\n\nextending Location:\nPresently, Location is defined as\n\n    Location       = \"Location\" \":\" absoluteURI\n\nwhich leaves it open for extensions, it doesnt clobber\nanything else.  The question is, though, will existing servers and \nclients\nchoke on additional fields..\n\nA new header:\nA new header gives us the most flexibility, not having to worry\nas much about the installed base.\n\nSecurity Implementation:\nDepending on your security stance, the implementation of this\nresponse could be considered unimplementable.  Its clear that this \nneeds\nto be a hop-by-hop header, but that alone doesnt make the security \nproblems\ngo away.  Since most proxies will forward any header to the client,\nthe client has no way to discern where the set-proxy header came \nfrom.\n\nIf the 305 is used by proxies to load balance, distribute cache, or \nfailover,\nwide scopes are most beneficial, ie for ALL HTTP URLs or ALL URLS\nuse xyz proxy.  Unfortunately security implications make scope \nrestriction\nnecessary.  Even if the 1.1 spec is modified to make set-proxy a\nhop-by-hop header, its easy for a malicious server to take advantage.\n\nWithout the scope restrictions, a malicious server can simply reply\nwith a 1.1 header, including a wide scope.  A 1.1 compliant proxy\nshould reject or supress this header, but an existing 1.0 or older\nproxy will happily forward this header to the 1.1 client.\nThis client would have no way of knowing if the 305 came from the\nproxy or the malicious origin server.\n\nUnfortunately, this makes the scope restrictions necessary.  At the\nsame time, it makes large scale load balancing or failover difficult,\nsince the a proxy can use this response to redirect a scope wider\nthan one host to another proxy.\n\nReasons for expressing scope, type and lifetime.\n\nscope:\nIt is wise to allow 305 to affect more than one single resource.\nIf not, a redirect would have to be sent for every subsequent request\nto a site.\n\nlifetime:\nThe rationale for expressing a lifetime, expressed in hits or\ntime, for a redirect's validity is in dispute.  As stated earlier,\nlarge cache hierarchies arent widespread, so its difficult to come\nup with real world examples.  I welcome discussion on this.\nHowever, I will cite an example where it is useful.\n\nImagine a large organization with a complicated, multi-level cache\nhierarchy.  Client A normally talks to proxy A which is \ngeographically\nclose to it.  Proxy A has some intelligence, lets say ICP, to route \nup\nthe proxy hierarchy through a few other proxies, and out to the \norigin\nserver.\nProxy A finds itself unable to contact its parent, or has stopped \noperating\ndue to cache garbage collection, or is for some reason, unable to \nhelp\nthe client.  Rather than stop all web traffic in proxy A's \n'jurisdiction',\nproxy A uses the 305 to redirect client A to proxy B, which is far \naway\nand across a slow pipe.  This allows client A to continue to access\nweb based resources, but at a slower rate.\n\nWithout a lifetime associated with the redirect, it would be either\none-shot, or forever.  If it was one-shot, every client talking to \nproxy A\nwould attempt to retreive via proxy A and get redirected every time.\nIf it was a 'forever' then those clients would continue to use\nthe slow link and proxy forever, or at least until they restarted \ntheir\nclient and reverted to defaults.\n\nBy associating a lifetime, a proxy can say \"Im in trouble, go use \nthis\nother area proxy for 10 minutes, then check back with me.  If Im \nstill\nin trouble, Ill redirect you again\".\n\n\n-----------------------------------------------------------------------  \n------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department\nServer Engineering\njosh@netscape.com \n                      http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------  \n------\n\n\n\n", "id": "lists-010-15853319"}, {"subject": "I-D ACTION:draft-ietf-http-state-man-mec00.txt, .p", "content": " A New Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : HTTP State Management Mechanism (Rev1)                  \n       Author(s) : D. Kristol, L. Montulli\n       Filename  : draft-ietf-http-state-man-mec-00.txt, .ps\n       Pages     : 21\n       Date      : 03/19/1997\n\nThis document specifies a way to create a stateful session with HTTP \nrequests and responses.  It describes two new headers, Cookie and \nSet-Cookie2, which carry state information between participating origin \nservers and user agents.  The method described here differs from Netscape's\nCookie proposal, but it can interoperate with HTTP/1.0 user agents that use\nNetscape's method.  (See the HISTORICAL section.)          \n\nThis document reflects implementation experience with RFC 2109 \nand obsoletes it.         \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-state-man-mec-00.txt\".\n Or \n     \"get draft-ietf-http-state-man-mec-00.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-man-mec-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-state-man-mec-00.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-state-man-mec-00.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-15871112"}, {"subject": "I-D ACTION:draft-ietf-http-warning00.tx", "content": " A New Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Problem with HTTP/1.1 Warning header, and proposed fix  \n       Author(s) : J. Mogul, A. Luotonen\n       Filename  : draft-ietf-http-warning-00.txt\n       Pages     : 13\n       Date      : 03/19/1997\n\nThe current HTTP/1.1 (RFC2068) specification introduces a new \"Warning\" \nheader, meant to carry status information about a request that cannot or \nshould not be carried by the response status code.  The existing \nspecification for the interaction between Warning and HTTP caches is \nfaulty, in that it may allow incorrect results after cache validation \noperations.  This document identifies two separate (but related) problems, \nand proposes revisions of the HTTP/1.1 specification to solve these \nproblems.                                                                  \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-warning-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-warning-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-warning-00.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-15880842"}, {"subject": "I-D ACTION:draft-ietf-http-uahint00.tx", "content": " A New Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : The User Agent Hint Response Header                     \n       Author(s) : D. Morris\n       Filename  : draft-ietf-http-uahint-00.txt\n       Pages     : 10\n       Date      : 03/19/1997\n\nThis document proposes a HTTP response header called UA-Hint, which can be \nused by applications (servers) to describe handling hints which will allow \nuser agents to more accurately reflect the intent of the web application \ndesigner for the handling and presentation of the response entity.  The \nUA-Hint header is intended to be an extensible general mechanism by which \nthe application can suggest user agent behaviors which alter or extend the \nbehaviors specified in HTTP/1.1 (RFC 2068) with the express purpose of \nimproving the usability of the resulting application.  Intended \nconsiderations include enablement of  safe POST and refined handling of the\ntraditional history buffer.                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-uahint-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-uahint-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-uahint-00.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-15890434"}, {"subject": "Re: (ACCEPT*) Last call on draft text for Accept header", "content": "I propose a minor change to the text in 10.4 and question the multiple definitions\nof matching.\n\nrom 10.4\n> |  A language-range matches a language-tag if it exactly equals the tag,\n> |  or if it is a prefix of the tag such that the first tag character\n> |  following the prefix is \"-\".  \n\nSection 3.10 defines a language-tag as\n\n        language-tag  = primary-tag *( \"-\" subtag )\n\n        primary-tag   = 1*8ALPHA\n        subtag        = 1*8ALPHA\n\nIt is preferable to reword the above section in 10.4  to\n\n\"A language-range matches a language-tag if it exactly equals the\ntag,  or if it equals the primary-tag (see 3.10)\"\n\n-------\n\nThe rules for matching language-range with language-tag are currently \naddressed in three sections of HTTP 1.1 - and they are not the same. \n\nSection 3.10 \n\"A server should consider that it has a match when a language tag\nreceived in an Accept-Language header matches the initial portion of\nthe language tag of a document. An exact match should be preferred.\"\n\nSection 10.4 - as above\n\nSection 12.1\n\"ql Language quality is measured by comparing the variant's assigned\nlanguage tag(s) (Section 3.10) to those listed in the request\n     message's Accept-Language field. If no variant has an assigned\n     Content-Language, or if no Accept-Language field is present, the\n     value assigned is \"ql=1\". If at least one variant has an assigned\n     content language, but the one currently under consideration does\n     not, then it should be assigned the value \"ql=0.5\". If any of the\n     variant's content languages are listed in the Accept-Language\n     field, then the value assigned is the maximum of the \"q\"\n     parameter values for those language tags (Section 10.4); if there\n     was no exact match and at least one of the Accept-Language field\n     values is a complete subtag prefix of the content language\n     tag(s), then the \"q\" parameter value of the largest matching\n     prefix is used. If none of the variant's content language tags or\n     tag prefixes are listed in the provided Accept-Language field,\n     then the value assigned is \"ql=0.001\".  \"\n\n\nThe rules should be defined in one section only.\n\n\n-------------------------------------\nTim Greenwood        Open Market Inc\n617 679 0320         greenwd@openmarket.com\n\n\n\n", "id": "lists-010-1589486"}, {"subject": "new cookie draf", "content": "Well, sports fans, there's a new cookie draft.  Regrettably, the name\nis draft-ietf-http-state-man-mec-00.  (I had wanted it to be\ndraft-ietf-http-state-mgmt-06.) I have withdrawn\ndraft-ietf-http-state-mgmt-errata-00, which the new draft subsumes.\n\nFor the record, I know of two planned changes to the draft already:\n\n1) I'll drop the \"same port\" requirement.  (Cookies can return to any\nport on any host to which they could otherwise legitimately be sent.)\n\n2) [10.1.1] When a user agent receives both Set-Cookie and Set-Cookie2,\nif the Set-Cookie2 has no Max-Age attribute, the user agent *must not*\n[change from \"may\"] honor any Expires in Set-Cookie.\n\nDave Kristol\n\n\n\n", "id": "lists-010-15900061"}, {"subject": "cookies &ndash;&ndash; by the wa", "content": "You can look at versions of the new draft with change bars from the RFC via\n<http://portal.research.bell-labs.com/~dmk/cookie-ver.html>\n\nDave Kristol\n\n\n\n", "id": "lists-010-15908331"}, {"subject": "Re: Issues with the cookie draf", "content": "Foteos Macrides wrote:\n> >[DMK]\n> >What I said in my email misrepresents what the spec. says.  I think your\n> >\"better safe than sorry\" approach, though interesting and justifiable,\n> >doesn't match the spec.\n> \n>         Yes, the way we coded it does not \"technically\" match RFC2109.\n> It's presently irrelvant for that spec, because the port contraint\n> yields the same result for https versus http requests.   But that\n> contraint was not part of the original cookie implementations, and\n> what is actually encountered on today's Web.   My point is that,\n> rightly or wrongly, we \"arbitrarily\" took the posture you stated in\n> your reply to Yaron:\n> \n>         \"I don't want a cookie that was originally encrypted to be\n>         returned to the server as cleartext.\"\n> \n> i.e., that Secure should be implicit for encrypted cookies.  It seemed\n> better to let the cookie handling \"fail\" under some circumstances than\n> risk that happening (though it could \"fail\" anyway, at present, for\n> UAs which adopted the RFC2109 port contraint when communicating with\n> servers which do not expect to be contrained in this way).\n> \n>         The reasoning was that under some circumstances, sending an\n> originally encrypted cookie as cleartext might go beyond \"privacy\"\n> risks to an actual security risk, and providers might not understand\n> the issues adequately to include the Secure attribute in such cases,\n> so don't *ever* take that risk.\n\nOn the other hand, I would figure that a site that was smart enough to know some\nof its transactions need to be secured would also know that it has to label its\ncookies as Secure.  Of course this could be hopelessly naive. :-)  I do see a real\nvalue in allowing a secure site to send (by default) insecure cookies, which could\nbe use for various state tracking purposes, and which could be used in insecure\nrequests as well.\n\nDave Kristol\n\n\n\n", "id": "lists-010-15914453"}, {"subject": "Secure cookies surve", "content": "Here's an inexact application survey:\n\nDoes anyone use, or know of an application that uses, cookies that are\nlabeled \"Secure\"?\n\nIf not, I will consider simply removing \"Secure\" from the cookie spec.\n\nDave Kristol\n\n\n\n", "id": "lists-010-15923501"}, {"subject": "Re: 305 Use prox", "content": "> How about a way to indicate that the client should access the origin \n> server directly, i.e., should not use a proxy at all?\n\nWhat exactly do you mean?\nDo you mean an origin server redirecting a client, which\nconnected via a proxy, to try again direct?\n\nOr do you mean a proxy telling a client to go direct?\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department                \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n", "id": "lists-010-15930211"}, {"subject": "Re: determining proxy reliabilit", "content": "My comments are based on the belief that\ndraft-ietf-http-hit-metering-00.txt dated 1/21/97 is current.. let me\nknow if that's not the case.. Jeff makes some references to 'latest\ndraft' that had me confused, but now I think I realize that he just\nmeans an unreleased version..\n\n\nIn a previous episode Jeffrey Mogul said...\n:: \n:: Item #2 is addressed in the latest draft, by adding an optional\n:: timeout to the Meter response-directive (i.e., to the server's\n:: request that the response be hit-metered).  This can't eliminate\n:: the problem of proxy crashes or reboot, but it can bound the\n:: likelihood of report-lost-due-to-proxy-failure.  E.g., if the\n:: timeout is set to 10 minutes, and the mean time between reboots\n:: for the \"average\" proxy is (say) 60 minutes, then there is a 1/6\n:: chance of report loss.  Since I suspect that MTBF for proxies is\n:: probably on the order of days, not hours, the actual loss\n:: probability is likely to be lower.\n\nThis does address my issue nicely.. My first reaction was that it\ncomplicates even further construction of the 'non-naive algorithm for\nscheduling the deletion of hit-count entries' that the proxy is\nstrongly encouraged to use by section 3.5 but the capacity to bundle\ntogether and schedule reports for a single server is far more predictable with\nrespect to a max time value than with respect to a max usage value so\nthe penalty appears worthwhile.\n\n:: \n:: This leaves #3, loss-in-transit.  My experience is that the most\n:: common way for servers to lose HTTP requests is due to internal\n:: congestion (i.e., the SYN_RCVD problem), so if hit-metering\n:: improves caching, the reduction in congestion ought to help this.\n:: But loss due to network partition is also a problem, and (according\n:: to Vern Paxson's SIGCOMM '96 paper) it's getting worse.  This\n:: has inspired me to change the text in the next version of the\n:: draft from \"The proxy is not required to retry the [report]\n:: if it fails\" to \"The proxy is not required to retry the [report]\n:: if it fails (but it should do so, subject to resource constraints).\"\n:: This is still \"best-efforts\", but the specification now encourages\n:: more effort.\n:: \n\nA strictly editorial comment.. I like the content, how about the slightly\nfirmer language: The proxy SHOULD retry the [report] if it fails but\nMAY abort it if resource constraints dictate.\n\n:: I think if the origin server can say \"send a report within\n:: X minutes, if you have anything to report\" then this effectively\n:: does the same thing as a request for 0/0 reports, but without\n:: the additional message overhead.\n\nsold me.\n\n-P\n\n\n\n", "id": "lists-010-15938129"}, {"subject": "Re: Secure cookies surve", "content": "On Thu, 20 Mar 1997, Dave Kristol wrote:\n\n> Here's an inexact application survey:\n> \n> Does anyone use, or know of an application that uses, cookies that are\n> labeled \"Secure\"?\n> \n> If not, I will consider simply removing \"Secure\" from the cookie spec.\n\nI know an application which is quite likely to want the unsecure\nimplication of secure ... that is, set the cookie in a secure response and\nhave it returned in a non-secure transaction. The application is still in\nthe prototype phase.\n\nDave Morris\n\n\n\n", "id": "lists-010-15948105"}, {"subject": "Re: new cookie draf", "content": "Dave Kristol:\n>\n>Well, sports fans, there's a new cookie draft.  Regrettably, the name\n>is draft-ietf-http-state-man-mec-00.  (I had wanted it to be\n>draft-ietf-http-state-mgmt-06.) I have withdrawn\n>draft-ietf-http-state-mgmt-errata-00, which the new draft subsumes.\n>\n>For the record, I know of two planned changes to the draft already:\n>\n>1) I'll drop the \"same port\" requirement.  (Cookies can return to any\n>port on any host to which they could otherwise legitimately be sent.)\n\nDropping this requirement opens a significant security hole, because not all\nservers on the same host need to be run by the same people.  Others have\ncalled this a `marginal case', but I do not want to ignore it: really tiny\nsites need security too.\n\nThe `same port' requirement that is in the spec now is a little too\nrestrictive though.  I'd be happy if the current\n\nDomain Selection\n     The origin server's fully-qualified host name must domain-match the\n     Domain attribute of the cookie.  The origin server's port number\n     must equal the port number of the server that sent the cookie.\n\ngets rewritten to\n\nDomain Selection \n     The origin server's fully-qualified host name must domain-match the\n     Domain attribute of the cookie.  If the cookie does not explicitely\n     specify a Domain attribute, the origin server's port number must\n     equal the port number of the server that sent the cookie.\n\n, but just dropping the port requirement won't do for me.\n\n>Dave Kristol\n\nKoen.\n\n\n\n", "id": "lists-010-15955891"}, {"subject": "Re: draft-holtman-http-safe01.tx", "content": "Patrick McManus:\n>\n>That's not how I read section 3:\n>\n>--\n>3 The Safe response header\n>\n> This header is proposed as an addition to the HTTP/1.x suite.\n>\n> The Safe response header field indicates whether the corresponding\n> request is safe in the sense of Section 9.1.1 (Safe Methods) of the\n> HTTP/1.1 draft specification [1].\n>--\n>\n>It speaks very specifically that the safe response header field talks\n>about the corresponding (i.e. current) request.. I think making any\n>assumptions about future request/response pairs requires clairvoyance\n\nFor the record, your interpretation is not what I meant to say when I wrote\nthe above definition of Safe.  `the request is safe' means that it is always\nsafe.\n\n>(and the Note further in section three claims that it can do this\n>clairvoyance)\n\n...and the note further in the section should have alerted you that you\nwere misreading the definition.\n\n>-Patrick\n\nKoen.\n\n\n\n", "id": "lists-010-15964589"}, {"subject": "Administrivia: getting off this lis", "content": "Folks,\n\nHaving had more than our fair share of bozos in the last week or so (including\na kind gentleman who attempted to send *5MB* to *each* of you), I guess that\nit is time to send a reminder about how to remove yourself from the http-wg\nmailing list.\n\nSend a message to:\n\n  http-wg-request@cuckoo.hpl.hp.com\n          ^^^^^^^\n          don't omit this!\n\nwith a 'Subject' line of:\n\n  unsubscribe\n\nPlease do not send a message to me and ask *me* to unsubscribe you, as I have\nRSI and doing this manually is a *pain*.  Literally.\n\n--\n(http-wg list owner)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n", "id": "lists-010-15972665"}, {"subject": "Re: determining proxy reliabilit", "content": "Patrick McManus writes:\n    My comments are based on the belief that\n    draft-ietf-http-hit-metering-00.txt dated 1/21/97 is current.. let me\n    know if that's not the case.. Jeff makes some references to 'latest\n    draft' that had me confused, but now I think I realize that he just\n    means an unreleased version..\n\nYes, draft-ietf-http-hit-metering-01.txt is currently wending\nits way through the Internet-Draft editor's queues.  Sorry for\nthe confusion.\n    \n    :: This leaves #3, loss-in-transit.  My experience is that the most\n    :: common way for servers to lose HTTP requests is due to internal\n    :: congestion (i.e., the SYN_RCVD problem), so if hit-metering\n    :: improves caching, the reduction in congestion ought to help this.\n    :: But loss due to network partition is also a problem, and (according\n    :: to Vern Paxson's SIGCOMM '96 paper) it's getting worse.  This\n    :: has inspired me to change the text in the next version of the\n    :: draft from \"The proxy is not required to retry the [report]\n    :: if it fails\" to \"The proxy is not required to retry the [report]\n    :: if it fails (but it should do so, subject to resource constraints).\"\n    :: This is still \"best-efforts\", but the specification now encourages\n    :: more effort.\n    \n    A strictly editorial comment.. I like the content, how about the slightly\n    firmer language: The proxy SHOULD retry the [report] if it fails but\n    MAY abort it if resource constraints dictate.\n    \nThe text I put into draft-ietf-http-hit-metering-01.txt says:\n\n      - The proxy is not required to retry the HEAD request if it\n        fails (this is a best-efforts design).  To improve\n        accuracy, however, the proxy SHOULD retry failed HEAD\n        requests, subject to resource constraints.\n\nYour wording is perhaps a little crisper; I'll think about\nusing it in a subsequent draft.\n\nThanks\n-Jeff\n\n\n\n", "id": "lists-010-15979812"}, {"subject": "Re: new cookie draf", "content": "On Thu, 20 Mar 1997, Koen Holtman wrote:\n> >1) I'll drop the \"same port\" requirement.  (Cookies can return to any\n> >port on any host to which they could otherwise legitimately be sent.)\n> \n> Dropping this requirement opens a significant security hole, because not all\n> servers on the same host need to be run by the same people.  Others have\n> called this a `marginal case', but I do not want to ignore it: really tiny\n> sites need security too.\n\nIn defense of my assertion that this is a marginal case:\n\nThe only case in which we would need a port restriction would be when two\nWeb servers are running on the same host with the same server name but\ndifferent port numbers: \n\n+ If domain is not specified in the set-cookie, it defaults to\n  request host and can meet the conditions listed above; and in this\n  case, we should consider a port restriction. \n\n+ If domain is specified, though, the cookie can be sent to any\n  server in the domain given, so a port restriction makes no sense: \n  one could start a server in the specified domain on the same port\n  but a different host, and get around a port restriction. \n\n+ Finally, if two servers are on the same physical host but run\n  under different server names (using vitrual hosting)  -- which is,\n  I would guess, a more common case than the first -- this problem\n  does not arise, so again a port restriction makes no sense. \n\nIt is unlikely (though not impossible) that a privacy- or security-\nsensitive application would be run under the first case: on a host where\nothers could set up a server with the same name but a different port\nnumber.  I still think it is a marginal case. \n\nI certainly agree, however, that \"really tiny sites need security too,\" and\nwould be more than happy to try to improve the port restriction if it is\nimportant to you or others. \n\n> Domain Selection \n>      The origin server's fully-qualified host name must domain-match the\n>      Domain attribute of the cookie.  If the cookie does not explicitely\n>      specify a Domain attribute, the origin server's port number must\n>      equal the port number of the server that sent the cookie.\n\nThis seems reasonable to me, although we're getting into the realm of weird\nimplicit behavior again.  My impulse is to suggest that the 'secure'\nkeyword trigger a same-port restriction (which would have a side effect of\nmaking 'secure'-labelled SSL-transmitted cookies, which will likely come\nfrom port 443, returnable to port 443 only ......hmm) but I suspect that's\nnot going to mesh well with a variety of 'secure' techniques.  The reason I\nsuggest it is that then a keyword indicates a higher level of resturction,\nrather than a default setting on an option attribute (domain). \n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-15988434"}, {"subject": "deflat", "content": "Are there any HTTP/1.1 servers which support the deflate method?\nI think I have a correct (modulo bugs) client implementation of deflate in the\ncurrent (alpha) version of E-scape, and I'd like to test it sometime\nbefore I release 1.0.\n\n\n\n", "id": "lists-010-15998854"}, {"subject": "Re: (ACCEPT*) Last call on draft text for Accept header", "content": "Tim Greenwood:\n>\n>I propose a minor change to the text in 10.4 and question the multiple\n>definitions of matching.\n\n>>From 10.4\n>> |  A language-range matches a language-tag if it exactly equals the tag,\n>> |  or if it is a prefix of the tag such that the first tag character\n>> |  following the prefix is \"-\".  \n>\n>Section 3.10 defines a language-tag as\n>\n>        language-tag  = primary-tag *( \"-\" subtag )\n>\n>        primary-tag   = 1*8ALPHA\n>        subtag        = 1*8ALPHA\n>\n>It is preferable to reword the above section in 10.4  to\n>\n>\"A language-range matches a language-tag if it exactly equals the\n>tag,  or if it equals the primary-tag (see 3.10)\"\n\nI believe your proposal for simplification is based on a misreading of\nthe syntax definition of language tags.  The current rule needs to be\nthis complicated because there can be more than one subtag.\n\nThe matching rule currently defined will allow the range \"i-sami\" to\nmatch the tag \"i-sami-da\".  Your proposed simplification will not\nallow this.\n\nI believe all tags currently defined by RFC1766 (Language tags,\nproposed standard) have at most one subtag, but a future revision of\nRFC1766 may define language tags with three elements:\n\n   i-sami-da  (Harald Alvestrand, the author of RFC1766, is working on\n               things like this)\n\n   i-s-bok    (and many other i-s-??? tags, where ??? is a \n               Summer Institute of Linguistics' Ethnologue 3-character\n               code for a language)\n\nThe rule in the proposed text is supposed to be compatible with such\nadditions to RFC1766, if they are made.\n\n>-------\n>\n>The rules for matching language-range with language-tag are currently \n>addressed in three sections of HTTP 1.1 - and they are not the same. \n\nThis is true: the planned edits will resolve the mess.  The matching\ntext of section 3.10 will be removed, and the matching text of section\n12.1 will also be removed, because the complete chapter 12 will be.\nThis leaves the one matching rule in the proposed section 10.4.\n\n>The rules should be defined in one section only.\n\n>Tim Greenwood        Open Market Inc\n>617 679 0320         greenwd@openmarket.com\n\nI hope I was able to address your concerns.  I will assume that the\ninformation above takes away your problems with the current proposed\ntext, unless I hear from you otherwise.\n\nKoen.\n\n\n\n", "id": "lists-010-1600244"}, {"subject": "Re: Fact-checking: do any inservice proxy caches ever ignor", "content": "In message <199703190831.JAA04987@wsooti08>, Koen Holtman writes:\n>What I remember (though my memory may be faulty) about New Zealand\n>educational caches is this:  at the http BOF at www5, when we were discussing\n>the HTTP/1.1 caching design, someone connected to the New Zealand caches\n>wanted it to be possible for a compliant HTTP/1.1 cache to ignore all\n>attempts at a `reload' if the user was accessing some sites with a `low\n>educational value'.  The cache would just keep returning the old cached\n>object (with a warning) instead of revalidating with the origin server.\n\nActually, that was someone connected with the UK educational/research\ncaches, but the result is the same.\n\n....Roy\n\n\n\n", "id": "lists-010-16005743"}, {"subject": "Re: httpequiv and new http header", "content": ">In HTML, <META HTTP-EQUIV=\"Blah\" is supposed to be equivalent to an\n>HTTP header \"Blah:\", yes ?\n\nNot quite.  It means that the content given has semantics equivalent\nto the HTTP semantics for the header field \"Blah\".\n\n>What is the position on creating new HTTP-EQUIV types (and presumeably\n>equivalent HTTP headers) ?\n\nThat's backwards.  The only possibility is to create new HTTP headers,\nwhich then could be used in HTTP-EQUIV.  Otherwise, use <META NAME=\"Blah\">.\n\n>In the Dublin Core metadata work a form <registry>.<name>[.<type>]\n>seems to be accepted, e.g. \n><META NAME=\"DC.Author\" CONTENT=\"Joe Fish\">\n>and perhaps\n><META HTTP-EQUIV=\"DC.Author.email\" CONTENT=\"jfish@pisces.org\">\n>and the equivalent\n>DC.Author.email: jfish@pisces.org\n>as an HTTP header\n>\n>which might imply that the \"DC\" portion should be reserved\n>for the DC crowd, and registry-less names be reserved for the HTTP group.\n\nThat wouldn't make sense.  The DC. prefix is supposed to associate\nsemantics of the name with Dublin Core semantics, and for the exact same\nreason that HTTP-EQUIV associates the name with HTTP semantics. \nIf we were to reinvent HTML, then we would probably use\n\n    <META NAME=\"HTTP.Expires\" content=\"Wed, 19 Mar 1997 13:15:31 GMT\">\n\ninstead of HTTP-EQUIV (or just skip the whole thing and just use LINK).\n\n>What I don't want to see, obviously, is people generating headers like\n>Expires: 4/5/99\n>Location: Bournemouth\n\nThe server is fully capable of controlling what it sends.\n\n.....Roy\n\n\n\n", "id": "lists-010-16013569"}, {"subject": "Re: deflat", "content": "At 02:24 PM 3/20/97 -0500, nemo/Joel N. Weber II wrote:\n>Are there any HTTP/1.1 servers which support the deflate method?\n>I think I have a correct (modulo bugs) client implementation of deflate in\nthe\n>current (alpha) version of E-scape, and I'd like to test it sometime\n>before I release 1.0.\n\nYou can do most of your testing simply by adding the \"deflate\" content\nencoding on a file - the server doesn't have to deflate it on the fly.\nJigsaw supports this by editing the resource directly and Apache by the\nAddEncoding configuration directive.\n\nThis is what we did when we did the work on compression in the paper\n\"Network Performance Effects of HTTP/1.1, CSS1, and PNG\" which is available\nfrom\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Pipeline.html\n\nIn addition to this we have also done some testing on how HTML\ncase-canonicalization affects compression. You can find the study at\n\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/HTMLCanon.h\ntml\n\nIt shows that lower case HTML tags is better than uppercase and that mixed\ncasing gives the worst result. You can also find a simple \"deflater\" tool\nhere.\n\nAlso, we have done some testing on how compression tends to have a good\neffect on TCP slow start and delayed acknowledgement. You can find this\nnote at\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/LAN.html\n\nAs you might have guessed from the URLs above - we keep a performance\noverview at\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Performance/\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-16023191"}, {"subject": "Re: draft-holtman-http-safe01.tx", "content": "I'm just going to try and summarize my issue with koen's safe draft\n(and dave's subsequent uhint draft as it applies to safe requests). \n\nIn the past permission over whether or not side effects were allowed\nwas always a client side decision, initiated via choice of\nmethod. This served as a sort of 'write protect' mechanism. The fact\nthat some applications may have ignored this isn't germane, they're\nnon compliant in my opinion.\n\nAll I want is a procedure where clients can continue to ask for r/o\nstatus but do so with requests bearing message bodies. The fact that\nthey wish to do so should not have as a requirement previous communication\nwith the hopefully safe resource.\n\nit's been suggested that adding safe: as a request header to post\naccomplishes this, but that in no way assures that a completely\ncompliant 1.0 app honors the safe:'d POST request.. (whereas a compliant\n1.0 app would refuse to do unsafe writes via a GET)..\n\nThe fact that control over r/w should remain with the client at that\ngranularity is analagous to our daily commerce lives... when you are\nin a store and purchase something with your credit card, you can't be\nabsolutely sure that they will bill you what you have agreed on.. but\nyou let them make the transaction anyhow. however if you aren't ready\nto buy anything I don't know anyone who gives their card up at the\ndoor just so they can bill you $0.00.. Why extend un-necessary\nauthority?\n\n-P\n\n\n\n", "id": "lists-010-16032356"}, {"subject": "Re: 305 Use prox", "content": "> >example response:\n> >HTTP/1.1 305 Use Proxy\n> >set-proxy : http://proxy1.foo.com:8080/ scope=http://www.foo.com/services/\n> >\n> >Suggested rules:\n> >Origin servers may NOT send 305, only proxies may send them.\n> \n> Nope.  The original intended purpose of 305 is to allow an origin server\n> to prevent access unless it goes through the appropriate proxy.\n\nI have serious reservations about allowing origin servers to generate\n305's.  It opens up a security hole that's hard to plug at this point\nwith HTTP/1.1 anymore.  This functionality is also redundant with\nexisting, so-called \"reverse proxy\" functionality.  I'm also reluctant\nto mud up the architectural design -- this would be the first time\nthat we allow remote servers to dictate whether to use a proxy or not.\nFinally, it adds unnecessary overhead to clients (and intermediate\nproxies) that have to maintain potentially large tables of what proxy\nto use for a given site/URL.\n\nLet me elaborate on these issues one by one.\n\nSecurity hole:\n\nIf 305 is allowed by origin servers, intermediate HTTP/1.1 proxies\nthat do not understand 305's hop-by-hope requirement will let it\nthrough (I assume at this point it may be too late to impose the\nhop-by-hop requirement for 305, and expect it to be respected by all\nimplementations).  If a client gets a 305 sent by an evil origin\nserver through a proxy, it will override the client's proxy settings,\nbecause the client thinks the proxy redirected it to another proxy.\n\nThis security hole can completely cripple people behind a firewall\nwhere the only way out is through the corporate proxy.  If some evil\nsite sends a 305 that goes through to the client, the client will from\nthen on try to use a proxy that is inaccessible for it (remember, it\nneeds to go thru the company firewall proxy to get out).\n\nIf this security hole is narrowed down by the scope rules, it at the\nsame time limits the use of 305 for other uses which I would consider\nmore reasonable for 305 -- namely load balancing between proxies, and\ndiverting away load from a proxy that is scheduled to go down for\nmaintenance.  See below how 305 is not necessary for making\nserver-originated proxy redirects.\n\nRedundant:\n\n305 is by no means the only way the client can be instructed to go\nthrough a proxy.  A regular 301/302 redirect could be used, pointing\nto a \"reverse\" proxy.  A \"reverse proxy\" appears as a regular server,\nbut is really a proxy when it comes to its content retrieval and\nmanagement.  As far as the client knows, it's an origin server.\n\nThis has benefits: the real origin server is completele hidden, the\nusers will never have to know about its existence.  Also, it keeps the\ndesign cleaner where origin servers are origin servers, regarless the\nimplementation style (regular, replicated, reverse proxy); and proxies\nremain a part of the transport mechanism which is controlled by a flow\nfrom client outward, not by origin servers (because they have no\nknowledge to control the proxy settings of clients).\n\nMudding the architectural design:\n\nSee above paragraph.\n\nOverhead on clients:\n\nBasically, 305 generated by origin servers implies having to maintain\na proxy setting for every URL.  This may be tolerable in clients, but\nit's worse on proxies which get a large amount of requests.  Scope\nrules complicate things worse.\n\nThe reasoning for scope rules is primarily to plug the security hole.\n\nI would like to get rid of the automatic scope rule restrictions\naltogether, which entails not allowing origin servers not to be\nallowed to send 305's at all, and use a combination of regular\nredirects and reverse proxies instead.\n\nBy virtue of \"trusted proxies\" you can allow global proxy redirects\n(if scope is not present), which allows *all* requests earlier\ntargeted for a given proxy, to be completely diverted to another.\nWith auto-scope-rule-restrictions this would be next to useless,\nbecause every new site (at least) would require a new hop by the\noriginal proxy, just to get a new proxy redirect.\n\n\nConclusion:\n\nI would definitely like to have a proxy redirect that can be generated\nby a proxy only.  If people can come up with examples where 305 from\norigin servers is the only way to accomplish something that couldn't\nbe done with regular redirect combined with a reverse proxy, then\nmaybe we could have two separate proxy redirections:\n\n    305 Use proxy-- use proxy, generated by origin servers\n    306 Proxy redirect-- use another proxy instead, gen'd by proxies only\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-16041652"}, {"subject": "Re: Secure cookies surve", "content": "-----BEGIN PGP SIGNED MESSAGE-----\n\nDave Kristol wrote:\n> \n> Here's an inexact application survey:\n> \n> Does anyone use, or know of an application that uses, cookies that are\n> labeled \"Secure\"?\n> \n> If not, I will consider simply removing \"Secure\" from the cookie spec.\n> \n\nIf by removing it, cookies set on secure connections would be returned\non insecure ones (given the removal of the port restrictions), I\nthink that would be bad. I know I have at least one application where\nI would not want secure cookies sent insecurely, so they are marked\n\"Secure\".\n\n- -- \nJeremey Barrett                                  VeriWeb Internet Corp.\nCrypto, Ecash, Commerce Systems                 http://www.veriweb.com/\nPGP key fingerprint =  3B 42 1E D4 4B 17 0D 80  DC 59 6F 59 04 C3 83 64\n\n-----BEGIN PGP SIGNATURE-----\nVersion: 2.6.2\n\niQCVAwUBMzGvAi/fy+vkqMxNAQFv0AP+NG98CkGWN5zjrC6AnaxZDKsZK1yV7gLk\nBYqUsLg6VJ4h+x6GB4vpWgryvZL3+3nbr463z5bfcjJFao+ZkdhHqE2+zW06WIfa\n/GWxxc03tdNpl8wtCYqKevnvvVmhN14CqXYc//+1clBWnRHinA9w+P17GQj7v+zz\naFovQ+7k+YA=\n=7hQh\n-----END PGP SIGNATURE-----\n\n\n\n", "id": "lists-010-16054428"}, {"subject": "Re: deflat", "content": "In a previous episode nemo/Joel N. Weber II said...\n:: \n:: Are there any HTTP/1.1 servers which support the deflate method?\n:: I think I have a correct (modulo bugs) client implementation of deflate in the\n:: current (alpha) version of E-scape, and I'd like to test it sometime\n:: before I release 1.0.\n:: \n\nJoel,\n\nI've got a cgi that will produce content-encoding headers for gzip and\ndeflate that uses our own rfc 1951 (1950&1952) implementation.. I'd\nlove it if you'd test it for interop with your browser.. and let me\nknow how it goes..\n\nhttp://pat.appliedtheory.com/compress/\n\n-P\n\n\n\n", "id": "lists-010-16062090"}, {"subject": "RE: new cookie draf", "content": "I am working on a proposed rewording of sections 4.3.5 and 7.1.\n\nI saw that M Hedlund's suggested wording of allowing a \"user-designated \nagent\" to verify a transaction has been incorporated.  I am concerned\nthat without more explicit description of how those agents would\noperate, the certification authorities, and protocols, third party\nverification will never\nreally happen.  I will try to make my proposal more explicit.\n\n\n\n", "id": "lists-010-16069345"}, {"subject": "Re: new cookie draf", "content": "On Thu, 20 Mar 1997, M. Hedlund wrote:\n\n> The only case in which we would need a port restriction would be when two\n> Web servers are running on the same host with the same server name but\n> different port numbers: \n>[...] \n> \n> + Finally, if two servers are on the same physical host but run\n>   under different server names (using vitrual hosting)  -- which is,\n>   I would guess, a more common case than the first -- this problem\n>   does not arise, so again a port restriction makes no sense. \n\nI believe this is no different than the case of using the same server name\nwhich Koen raised ... with virtual hosting, each virtual host is an alias\nfor the same physical host.  Hence the rogue server running on a high port\ncan virtual host for the same names as the server whose cookies it is\ntrying to intercept.\n\nMy sense of the world of webfarm server services providing virtual hosting\nis that actually using this technique to harvest cookies intended for\nanother server would be quite difficult at best. The rogue server\noperator must:\n\n   a.  Manage to keep the rogue server started and running and receiving\n       requests for a port that athe web farm owner doesn't support\n   b.  Introduce URLs in places likely to be seen and followed by people\n       who also use the approved server\n\nHence I would propose that this issue be be addressed with a security\nconsideration note which describes the possiblity and strongly discourages\nthe use of cookies to carry sensitive information if the application is\nhosted on a shared system where an unauthorized person could run a web\nserver.  A similar warning should exist (it may, I'm being lazy and not\ndouble checking) for the case where the spec allows multiple hosts to\nshare cookies.\n\nPersonally, I think its bad design to include anything more than a basic\nsession ID in the cookie but I know there are many sites that find storing\nuser information in the user's browser attractive so I think they bear the\nresponsiblity for paying attention to the security issues.\n\nDave Morris\n\n\n\n", "id": "lists-010-16077187"}, {"subject": "Re: httpequiv and new http header", "content": "On Thu, 20 Mar 1997, Roy T. Fielding wrote:\n\n> The server is fully capable of controlling what it sends.\n\nAnd the UA of choosing what it might interpret.\n\nThe last time I looked carefully, there is no requirement in either the\nHTML or HTTP specifications that either the server or the UA do anything\nwith the HTTP-EQUIV or any other meta data value. Perhaps I missed\nsomething so my point is out of date, but I think one must examine the\nserver/UA pair implementation to learn what is done.\n\nDave Morris\n\n\n\n", "id": "lists-010-16086739"}, {"subject": "Safe: Yes now, GETWBODY later? [was: Re: draft-holtman-http-safe01.txt", "content": "Following on your summary ... you aren't satisfied that the level of\nclient control you desire exists using a header because a server might\nchoose to ignore it?\n\nAlso, you are attempting to state a requirement which is outside the scope\nof what Koen and I are attempting to address with our respective drafts?\n\nAnd I think we agree that introducting a new method probably requires a\nnew version of the protocol.  Clearly there is a deployment problem with\nproxies which will not recognize a new method.  In your scenario, an\norigin server which didn't recognize the method would reject the request\nso the client wouldn't be miss-led into believing it was safe.\n\n[aside: I'm not sure there is any greater risk that a CGI program attached\nto a server which recognizes GETWBODY would ignore a Safe: Yes in the\nrequest than the risk that the same CGI program would ignore the request\nmethod. Poorly written code is poorly written code. Test for NOT GET and\ncontent length!=0 === POST more often than not.]\n\nAll this to suggest that we can move forward with Safe/UAHINT as written\nas far as your requirement is concerned and you or someone else who wants\nto champion the GETWBODY can describe the proposal in a new INet draft or\nwait until the floor is open for the next HTTP version and lobby for the\nchange at that time.\n\nDoes that seem reasonable?\n\nDave\n\nPS. For the http-wg list ... we are discussion the Safe: Yes proposal in\ndraft-holtman-http-safe-01.txt and the alternative syntax for\nincorporating the same function in the UA-Hint: header proposed in\ndraft-ietf-http-uahint-00.txt.\n\n\n\n", "id": "lists-010-16095016"}, {"subject": "Whats servlets for", "content": "Hello,\nWhats the advantage of HTTP servers in Java and servlets compared\nto HTTP servers in native code with CGI?\n\n\n\n", "id": "lists-010-16104438"}, {"subject": "Re: (ACCEPT*) Last call on draft text for Accept header", "content": "Koen Holtman wrote\n\n> I believe your proposal for simplification is based on a misreading of\n> the syntax definition of language tags.  The current rule needs to be\n> this complicated because there can be more than one subtag.\n> \n> The matching rule currently defined will allow the range \"i-sami\" to\n> match the tag \"i-sami-da\".  Your proposed simplification will not\n> allow this.\n\nThe proposal was intended not as a simplification, but as a \nclarification. Your comment is correct, my proposal would have \nexcluded a match that should be allowed. The text in 10.4 should \nstill be rewritten to use the syntax defined in 3.10. The current \ntext uses the term 'a prefix' which is not defined. It could be \ninterpreted to allow a match of a language range to any but the last \nsubtag of a language-tag. Thus \n\nAccept-Language:cy     (Welsh)\n\ncould match el-cy-x   (Greek in Cyprus with some other subtag).\n\n-------------------------------------\nTim Greenwood        Open Market Inc\n617 679 0320         greenwd@openmarket.com\n\n\n\n", "id": "lists-010-1610914"}, {"subject": "RE: Issues with the cookie draf", "content": "RE: $\nAs the origin server sent out the cookie, why would the origin server\nalso not know what sort of cookie it is receiving back? While it is true\nthat the origin server may have sent out cookies with the name\n\"Version\", the origin server can then reliably detect that it is a new\ncookie by checking the second value and seeing that it is not a legal\nNetscape cookie value. It would seem that the \"$\" is not necessary.\n\nLanguages:\nAs I mentioned in my original proposal, the accept-language header would\nserver the purpose of choosing the language. In the worst case, the\nlanguage is just English. The UTF8 Unicode encoding preserves the lower\nASCII range so when dealing with downlevel clients, one sends UTF8\nEnglish. I do admit woeful ignorance of the language tag issues. Any\nexperts in the house?\n\nDiscard:\nI am fully aware of the Lab PC environment. That is why IE 4.0 NT will\nbe shipping with both private and public personal caches. The private\ncache will only be available based on log-in. The public cache will be\nthe default used by anyone who logs into the machine and who doesn't\nhave a private cache. Thus the distinction your refer to is understood\nby the client. As such the client also has the ability to decide when to\nstore a cookie and when not to. So changing the attribute to Private\nwould mean \"If you are using a user specific cache then you may keep\nthis cookie across log-ins. If you are using a system wide cache, then\nthe cookie must be purged on log-out.\" I believe this is closer to the\ndesired functionality than the current Discard definition.\n\nIncluding Version:\nI actually meant the comment to apply to Set-Cookie not Cookie. Given\nthe use of the set-cookie2 header, version, when equal to 1, would\nappear unnecessary.\n\nMatching Security the cookie was transmitted with:\nI am not going to get religious on the issue, I am just concerned that\nthe language requires impossible behavior. For example, if the system\nhas used some out of band means to determine that it has an isolated\nconnection to the server, for example, they are directly connected by a\nwire, it may be perfectly reasonable to send a secure cookie in clear\ntext. I think the best option is to simply state that the server expects\nthe cookie to be transmitted in a secure manner and leave it at that.\n\nDealing with Malformed cookies:\nMy concern is that handling of end cases caused the state spec to have\nto be revved in the first place. I would think, given past experience\nwith cookies, it would be best to dot every \"i\" and cross every \"t\". In\nthis case I believe it to be appropriate to declare that malformed\ncookies must be ignored. This is especially the case given that HTTP\nprovides no mechanism for the client to return error information to the\nserver.\n\n4.3.2 Rejecting Cookies (how far into the domain do you go):\nI appreciate that it was a long and drawn out debate but that is not a\nsufficient rational for preventing perfectly reasonable behavior. The\ndecision to stop at one domain level is completely arbitrary. It is no\nmore and no less secure than 2 or infinite domain levels deep. I do not\nfeel that an arbitrary choice is a good enough reason to include a\nrequirement in a specification.\n\nQuote David: \"You cannot specify explicitly by Domain and Path the\ndomain\nand path you get by default.\"\nIf you are explicitly defining Domain and Path, what do you care about\nthe default? Perhaps an example would help?\n\nDomain and Path Ordering:\nHow about, cookies are first ordered by domain based on a byte by byte\ncomparison. Within a domain, cookies are path ordered as specified.\n\nInterface Issues:\nI have said my peace on this issue.\n\nYaron\n\n\n> -----Original Message-----\n> From:Dave Kristol [SMTP:dmk@bell-labs.com]\n> Sent:Wednesday, March 19, 1997 8:38 AM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: Issues with the cookie draft\n> \n> Yaron Goland wrote:\n> \n> > Why are names beginning w/$ still reserved? As we have now defined\n> the\n> > position of NAME=VALUE, this restriction is no longer necessary.\n> \n> Unfortunately, it is, given the compatibility rules for combining\n> Set-Cookie and Set-Cookie2 headers.  When the origin server receives a\n> Cookie header, it doesn't know, a priori, whether it's an old or new\n> cookie.  Because Netscape's original spec. separated distinct cookies\n> by\n> ';', the positional placement isn't sufficient to find NAME=VALUE.\n> More to\n> the point, something like Version=1 could look like a NAME=VALUE.  So\n> I\n> retained the '$' reservation, to distinguish returned attribute/values\n> from\n> cookie values.\n> \n> > \n> > Comment should be a language tagged Unicode string not a quoted\n> string.\n> > The actual language used can be implicitly negotiated on by the\n> > accept-language headers with the request. This is clearly not a\n> robust\n> > solution but it is probably appropriate to this situation.\n> \n> I'd like to see a more detailed proposal of how the language gets\n> chosen,\n> what the syntax of the Comment attribute would be, and what the\n> implications would be for displaying Comment's contents to a user\n> (sect.\n> 4.2.2).\n> \n> > \n> > Discard is entering dangerous territory. When exactly does a user\n> agent\n> > terminate? Both MSIE 4.0 and NS 4.0 are moving to desk top models\n> where\n> > the user agent is operational as long as the computer is on.\n> \n> In that model, the user agent terminates when you shut down the\n> computer.\n> \n> > Furthermore, why would you want to discard a cookie when the user\n> agent\n> > terminates? It sounds like this is an attempt to solve the problem\n> of\n> > shared cash behavior. If the cookie is sensitive and if the cache is\n> > shared, we don't want the cookie hanging around. I think we should\n> > change Discard to Private. Private would indicate that the cookie\n> SHOULD\n> > only be recorded if a private cache is in use.\n> \n> What's your definition of a \"private cache\"?  Does a Wintel PC have a\n> private cache?  If so, how about a Wintel PC that sits in a university\n> lab,\n> where it's shared by lots of students?\n> \n> Here's the problem to solve, from the origin server's perspective.\n> The\n> server sends a cookie to a user agent.  The lifetime is meant to be\n> the\n> shorter of, say, 3 hours or the end of a session.  If I say\n> Max-Age=10800,\n> User1 (in a shared PC lab) might finish after one hour and exit the\n> browser, thinking this will eliminate any context that had been\n> instantiated while s/he used the PC.  User2 comes along, starts up the\n> PC,\n> goes to the same site, and inadvertantly starts using User1's cookie,\n> which\n> is \"a bad thing\".  So the origin server also wants to send Discard, so\n> when\n> the user agent session ends (browser exits, user reboots Windows,\n> whatever), User1's cookie is gone.\n> > \n> > Version should be optional, if not included, it should default to\n> V1.\n> \n> Again, the S-C and S-C2 combining rules necessitate an explicit\n> Version, so\n> a server can tell whether it's getting V0 or V1 cookies.  Remember,\n> the\n> origin server only sees a Cookie header (not Cookie2), which is\n> ambiguous.\n> \n> > \n> > The default for Max-Age has the same \"how long is a UA session\"\n> problem\n> > as Discard. IMHO the most robust solution is to have the cookie kept\n> > indefinitely if no Max-Age is included.\n> \n> That *is* the default, unless there's a Discard.  That's why Discard\n> is\n> needed -- to override the default where an application needs to do so.\n> \n> > \n> > 2. Quotes & Responses:\n> > \n> > Quote:\n> > \"When it sends a\n> >      \"secure\" cookie back to a server, the user agent should use no\n> less\n> >      than the same level of security as was used when it received\n> the\n> >      cookie from the server.\"\n> > \n> > Response:\n> > What is greater or lesser security? Do we expect clients to record\n> what\n> > security they were using when they received the cookie and then,\n> through\n> > some as yet undefined mechanism, decide what \"greater\" or \"lesser\"\n> > security than the original security mechanism means? This definition\n> is\n> > too fuzzy to be useful, I believe it should be removed.\n> \n> I agree it's fuzzy, but RFC 2068 says nothing about transport\n> security, so\n> it would be hard to be more specific.  I think we can agree, though,\n> that\n> encryption is more secure than no encryption.  I don't want a cookie\n> that\n> was originally encrypted to be returned to the server as cleartext.\n> So we\n> *have* to say something here, and removing the statement would be\n> wrong.  I\n> invite alternative words that get the point across.\n> \n> > Quote:\n> > \"If an attribute appears more than once in a cookie, the behavior is\n> > undefined.\"\n> > \n> > Response:\n> > Undefined things have a nasty habit of defining themselves. I\n> propose\n> > the sentence read \"If an attribute appears more than once in a\n> cookie,\n> > then the cookie is illegal and MUST be ignored.\"\n> \n> I was trying to be \"generous in what you accept\".  The HTTP spec., for\n> example, does not mandate that a request be ignored if a header is\n> malformed.  It's an implementation decision.  Returning to Set-Cookie,\n> ignoring a duplicate attribute is a valid behavior.  Ignoring the\n> cookie is\n> a valid behavior.\n> \n> > \n> > Quote:\n> > \"HTTP/1.1 servers must send Expires: old-date (where old-date is a\n> date\n> > long in the past) on responses containing Set-Cookie2 response\n> headers\n> > |\n> > unless they know for certain (by out of band means) that there are\n> no\n> > downsteam(sic) HTTP/1.0 proxies..\"\n> > \n> > Response:\n> > I believe this sentence should be changed to read \"HTTP/1.1 servers\n> MUST\n> > send Expires: old-date (where old-date is a date long in the past)\n> on\n> > responses containing Set-Cookie2 response headers meant for single\n> users\n> > unless...\". We allow caching of Set-Cookie2 headers intended for\n> > multiple people.\n> \n> The point of the original paragraph (sect. 4.2.3; could you cite\n> sections,\n> please?) was, I think, that HTTP1.0 caches are unreliable and can't be\n> trusted to honor caching directives correctly.  So stuff must be\n> stored in\n> them pre-expired.  Even cookies intended for multiple users, because\n> there's no way to persuade such older caches that they must revalidate\n> documents and cookies.  HTTP/1.1 caches will ignore Expires in favor\n> of\n> Cache-Control and do the right thing.\n> > \n> > Quote:\n> > \"   * The request-host is a FQDN (not IP address) and has the form\n> HD,\n> >      where D is the value of the Domain attribute, and H is a string\n> >      that contains one or more dots.\"\n> \n> [4.3.2 Rejecting Cookies]\n> > \n> > Response:\n> > The company Blah Inc. has the web site blah.com. Blah sells many\n> > products, one of which is called bar. Bar has been released in\n> several\n> > versions, the newest of which is Foo. Blah wants to be able to\n> present\n> > information to its customer that it thinks the customer will be\n> > interested in and it wants to present this information across all of\n> its\n> > sites. So it sends a cookie whose domain is .blah.com. If a user is\n> > visiting foo.bar.blah.com and receives this cookie they will have to\n> > reject it because it violates the above rule. It is totally\n> appropriate\n> > for Blah Inc. to want to hand out cookies that apply to all the\n> sites it\n> > owns. However instead of doing it simply by having a single cookie,\n> it\n> > now has to clutter the user's hard drive with cookies for every\n> > *.blah.com site visited, not to mention complicating the server's\n> > implementation. I believe this requirement is not reasonable,\n> especially\n> > for complicated sites.\n> \n> Sorry, this piece has a long history of discussion, and I don't think\n> we're\n> willing to change it, although I do understand your point.  The issue\n> was\n> how to provide adequate flexibility to applications (and you don't\n> think we\n> have) while preventing cookie-sharing abuses that might arise from\n> sites\n> that send cookies with too-liberal Domain=.\n> > \n> > Quote:\n> > \"User agents should allow the user to control cookie\n> destruction....\"\n> > \n> > Response:\n> > If a UA maker wants to never allow a customer access to cookie\n> control\n> > mechanisms, that is the UA maker's business, not the standards. We\n> can\n> > not threaten companies by saying \"Well if you don't create your\n> > interface the way we say then you aren't compliant\" and expect to\n> remain\n> > credible as a standards organization. This is not a wire protocol\n> > related issue. It is a feature issue and a matter of competitive\n> > advantage for UAs.\n> \n> This item, of course, is part of the broader discussion about what RFC\n> 2109\n> can and cannot say about UA interfaces.  The members of the sub-group\n> were\n> quite firm in their belief that users should have control of cookies.\n> \n> > \n> > Quote:\n> > \"   * The value for the $Domain attribute must be the value from the\n> > |\n> >      Domain attribute, if any, of the corresponding Set-Cookie2\n> response\n> > |\n> >      header.  Otherwise the attribute should be omitted from the\n> Cookie2\n> > |\n> >      request header.\n> > |\n> > \n> >    * The value for the $Path attribute must be the value from the\n> Path\n> > |\n> >      attribute, if any, of the corresponding Set-Cookie2 response\n> > |\n> >      header.  Otherwise the attribute should be omitted from the\n> Cookie2\n> > |\n> >      request header\"\n> > \n> > Response:\n> > All cookies have Domain and Path values. When not explicitly defined\n> > they are implicitly defined. Thus a user agent will record these\n> values,\n> > explicit or not. The above requirements now dictate that the UA has\n> to\n> > record extra information, an indication if the Domain and Path are\n> > implicit or explicit. I can find no good reason to place this\n> > requirement on the UA. Instead we should simply require that the\n> Domain\n> > and Path, explicit or not, should always be returned with the\n> cookie.\n> \n> Yes, there we are requiring extra information.\n> \n> Here's why.  You cannot specify explicitly by Domain and Path the\n> domain\n> and path you get by default.  For example, suppose x.y.com sends a\n> cookie.\n> If it leaves out Domain=, the default domain is x.y.com.  The cookie\n> will\n> be returned *only* to that site.  However, if you say Domain=.y.com,\n> the\n> cookie gets sent to any site *.y.com, not just x.y.com.  Or you can\n> say\n> .x.y.com, which domain-matches *.x.y.com.\n> \n> There's a similar behavior for Path regarding '/'.\n> > \n> > Quote:\n> > \"Domain Selection\n> >      The origin server's fully-qualified host name must domain-match\n> the\n> >      Domain attribute of the cookie.  The origin server's port\n> number\n> > |\n> >      must equal the port number of the server that sent the cookie.\"\n> > \n> > Response:\n> > Why do we have the port number requirement? If Blah Inc. has an HTTP\n> > server on ports 80 and 81, why would we want to prevent sharing\n> between\n> > two ports on the same system?\n> \n> Well, for one thing the two servers may be administered separately for\n> different purposes, and letting them share cookies seems like a bad\n> idea.\n> \n> > \n> > Quote:\n> > \"If multiple cookies satisfy the criteria above, they are ordered in\n> the\n> > |\n> > Cookie2 header such that those with more specific Path attributes\n> > precede those with less specific.  Ordering with respect to other\n> > attributes (e.g., Domain) is unspecified.\"\n> > \n> > Response:\n> > If we leave domain ordering undefined doesn't that sort of destroy\n> the\n> > utility of requiring path ordering?\n> \n> Okay, I was lazy, following Lou's example in the original spec.  I\n> didn't\n> want to have to specify a multi-dimensional sorting algorithm.  Got\n> any\n> ideas?  (I'm hoping that multiple cookies to the same site are rare,\n> so it\n> isn't that big a problem, but I do feel a little guilty it is so\n> poorly\n> specified.)\n> \n> > \n> > Quote:\n> > \"User agents may offer configurable options that allow the user\n> agent,\n> > or\n> > any autonomous programs that the user agent executes, to ignore the\n> > above rule, so long as these override options default to ``off.''\"\n> > \n> > Response:\n> > Again, I do not feel it is appropriate for this specification to\n> dictate\n> > to UA makers how to build the parts of their product that do not go\n> over\n> > the wire. If a UA maker wants this to default to \"ON\", that is their\n> > business. If the UA maker wants to default to \"ON\" and not allow the\n> > user to change the value, that is also their business. The mission,\n> I\n> > hope, is interoperability, not second guessing UA makers.\n> \n> Same user agent interface discussion as before.  And same user control\n> discussion.\n> > \n> > Quote:\n> > \"This state\n> > management specification therefore requires that a user agent give\n> the\n> > user control over such a possible intrusion, although the interface\n> > through which the user is given this control is left unspecified.\n> > However, the control mechanisms provided shall at least allow the\n> user\n> > \n> >    * to completely disable the sending and saving of cookies.\n> > \n> >    * to determine whether a stateful session is in progress.\n> > \n> >    * to control the saving of a cookie on the basis of the cookie's\n> >      Domain attribute.\"\n> > \n> > Response:\n> > Wire protocols have a massive effect on the range of functions a\n> client\n> > can implement. In effect, they restrict products. Software companies\n> > have decided that interoperability is such an important product\n> feature\n> > that it is worth having their functionality restrained. However\n> there is\n> > another reason behind the software maker's behavior, they know that\n> the\n> > real battle is UI not features. Features tend to be a check-list, so\n> > long as everyone has the same check marks, the competitive field\n> remains\n> > flat. The area of competition becomes primarily one of interface.\n> When\n> > standards step beyond the wire, beyond even functionality, and go\n> into\n> > the area that is the heart of computer software, they render\n> themselves\n> > irrelevant. Companies are not going to give up their competitive\n> > advantage in order to be compliant with a standard. Worse yet, due\n> to\n> > press pressures, companies will be forced to look like they are\n> > compliant, even when they are not. This reduces the ability of the\n> IETF\n> > to be an effective standards setting organization. Once companies\n> are\n> > forced to selectively ignore standards the goal of interoperability\n> > becomes impossible.\n> \n> Ditto.\n> \n> Dave Kristol\n\n\n\n", "id": "lists-010-16111268"}, {"subject": "RE: new cookie draf", "content": "How does your requirement make the situation any more secure? If port 80\nis controlled by good guys and port 81 is controlled by bad guys, but\nboth ports on the same domain, won't the cookie still be transmitted to\nport 81, regardless of your requirement?\n\nI would suggest changing the domain specification to allow for the\ninclusion of port number. So a domain could look like \".foo.bar.com:80\".\nNow if a site is concerned about someone taking over a port, they can\nspecify that only the identified port may be used. If they control the\nentire server and have no worries, they may then leave the port number\nout.\n\nYaron\n\n> -----Original Message-----\n> From:koen@win.tue.nl [SMTP:koen@win.tue.nl]\n> Sent:Thursday, March 20, 1997 9:49 AM\n> To:dmk@research.bell-labs.com\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: new cookie draft\n> \n> Dave Kristol:\n> >\n> >Well, sports fans, there's a new cookie draft.  Regrettably, the name\n> >is draft-ietf-http-state-man-mec-00.  (I had wanted it to be\n> >draft-ietf-http-state-mgmt-06.) I have withdrawn\n> >draft-ietf-http-state-mgmt-errata-00, which the new draft subsumes.\n> >\n> >For the record, I know of two planned changes to the draft already:\n> >\n> >1) I'll drop the \"same port\" requirement.  (Cookies can return to any\n> >port on any host to which they could otherwise legitimately be sent.)\n> \n> Dropping this requirement opens a significant security hole, because\n> not all\n> servers on the same host need to be run by the same people.  Others\n> have\n> called this a `marginal case', but I do not want to ignore it: really\n> tiny\n> sites need security too.\n> \n> The `same port' requirement that is in the spec now is a little too\n> restrictive though.  I'd be happy if the current\n> \n> Domain Selection\n>      The origin server's fully-qualified host name must domain-match\n> the\n>      Domain attribute of the cookie.  The origin server's port number\n>      must equal the port number of the server that sent the cookie.\n> \n> gets rewritten to\n> \n> Domain Selection \n>      The origin server's fully-qualified host name must domain-match\n> the\n>      Domain attribute of the cookie.  If the cookie does not\n> explicitely\n>      specify a Domain attribute, the origin server's port number must\n>      equal the port number of the server that sent the cookie.\n> \n> , but just dropping the port requirement won't do for me.\n> \n> >Dave Kristol\n> \n> Koen.\n\n\n\n", "id": "lists-010-16139736"}, {"subject": "Re: Comments on draft-mogul-http-hit-metering01.tx", "content": "Jeff,\n\n   Please pardon my sluggish response time.\n\n>    *) User path data is lost/not collectable.\n> \n> Some sorts of path data are lost, but not all.  For example, it\n> is pretty simple to structure things so that you can get separate\n> counts for each edge of the path-graph.  This can either be\n> done by using\n> Vary: referrer\n> or, if that proves to be unreliable, using the specialized\n> URL mechanism described in section 9 of the proposal.\n\n   In our analysis of sites, we typically see N x N page-to-page \nconnectivity matrices being between 1% and 10% full, with the\npercent of paths actually followed being about half of this.  While \nthis may seem like a small number, and indeed tends to suck if you \ntook the time to design the pages, if I request referrer tallies \nfor each page and use hourly collection periods, for our site at\nGVU with 20k HTML documents, this translates to potentially 12k referrer \nreports/day for our origin server to process per proxy.  Granted, while \nthe data can be collated by the proxy into hourly batches, this still \ncreates a nice amount of overhead for both proxies and origin servers, \nespecially if this report feature becomes popular.  Scheduling incoming \nreports by the origin server is also an issue, which while doable, \npotentially non-trivial.\n\n   Please don't get me wrong, I'm not whining here, some jammin CPU \nwill be putting in the extra hours while I kick it back, but I do want \nto make sure that we respect the additional resources required.\n\n> We don't assert that this captures all path information; for\n> example, it doesn't capture second-order paths.  You can\n> count the number of times a user got to B from A, and the\n> number of times a user got to C from B, but if there are\n> other frequent paths to B, you can't count the number of\n> times that the path A->B->C was followed (unless you clone\n> the pages to generate unique URLs).  Also, these techniques\n> tend to reduce the effectiveness of caching.\n\n    Shame too, since this tends to be the more interesting information.\n\n>    *) Collection periods can not be reliably controlled. Since caches\n>       are not forced to report by a certain time, an indeterminable\n>       amount of data could be tallied with the next collection period.\n>       The usage-limiting mechanisms can help alleviate this, though a)\n>       not completely and b) at the cost of more traffic (defeating one\n>       of the proposals goals).\n> \n> The draft mentions, in a Note, that we contemplated introducing\n> a \"Meter: timeout=NNN\" response directive to solve a somewhat\n> different problem.  It sounds like this would also solve the\n> collection-period problem.  Jim and I have exchanged email about\n> this, and it sounds like we both think it would be a good idea.\n> I'll add it to the next version, once I figure out the ramifications\n> (which are somewhat complicated by the presence of multiple levels\n> of proxies).\n\n    Definitely better than having to live with no timeouts.\n\n>    *) As a result of these limitations, comparisons between collection\n>    periods can be misleading.  Did a 5% decrease have to do with the stuff\n>    on the site or a faulty cache, or a network failure, or a report\n>    being mis-tallied?  I argue that there is no way to reliably\n>    know.\n> \n> True, but this uncertainty applies whether or not one is using\n> hit-metering.  E.g., I want to know why the number of references\n> to www.shark.com was smaller between 1pm and 2pm than it was between\n> noon and 1pm.  Is it because more people surf the net during their\n> lunch hours, so more of them find my site?  Or is it because some\n> router in Chicago was malfunctioning, and users on the opposite\n> coast couldn't make connections?  Since the Internet is inherently\n> best-effort, we aren't introducing a qualitatively different level\n> of failure-uncertainty.\n\n   Indeed, though I'm of the stance that sampling can alleviate & quantify\nthe amount of failure-uncertainly injected into the system.  \n\n> However, it's not entirely clear that random-sampled cache-busting is\n> free of it's own biases.  For example, if users actually do make fewer\n> references to \"slow\" sites rather than to \"fast\" ones, and if\n> cache-busting increase response times, then the randomly-sampled\n> population might behave inherently differently from the full\n> population.\n\n   Agreed & no solid data that I am aware of on this issue.  \n\nJim.\n\n\n\n", "id": "lists-010-16151305"}, {"subject": "Re: draft-holtman-http-safe01.tx", "content": "Patrick McManus:\n>\n>\n>I'm just going to try and summarize my issue with koen's safe draft\n>(and dave's subsequent uhint draft as it applies to safe requests). \n>\n>In the past permission over whether or not side effects were allowed\n>was always a client side decision, initiated via choice of\n>method. This served as a sort of 'write protect' mechanism. The fact\n>that some applications may have ignored this isn't germane, they're\n>non compliant in my opinion.\n\nI agree.\n\n>All I want is a procedure where clients can continue to ask for r/o\n>status but do so with requests bearing message bodies. The fact that\n>they wish to do so should not have as a requirement previous communication\n>with the hopefully safe resource.\n>\n>it's been suggested that adding safe: as a request header to post\n>accomplishes this, but that in no way assures that a completely\n>compliant 1.0 app honors the safe:'d POST request.. (whereas a compliant\n>1.0 app would refuse to do unsafe writes via a GET)..\n\nYes, adding safe: as a _request_ header does not get you the reliable safety\nyou need: we need a GET-WITH-BODY for this, but we cannot deploy it soon\nbecause all old clients out there won't understand it.\n\nThe Safe response header solves _some_ of the problems we have because we\ndon't have GET-WITH-BODY.  Once we GET-WITH-BODY is deployed though, Safe\nbecomes superfluous.\n\nSo it is Safe now, GET-WITH-BODY later.\n\n>-P\n\nKoen.\n\n\n\n", "id": "lists-010-16163569"}, {"subject": "New Issue: Incompatiblity between caching needs and persistant connections", "content": "Hi,\n\nI have been studing the HTTP/1.1 specs and have noted with interest the\nchanges to HTTP/1.0 which formally define the requirements for agents,\nto use peristant connections and caching.\n\nThere seems to be a certain incompatibility in the methods used to\nachieve caching the ones to reduce latency and bandwitdh requirements by\nthe use fo persistant connections.\n\nPut simply persistant connections require me to place all my related\ndocuments, images, etc on a single comman server, while to facilitate\ncaching it might be very useful to use a comman url for inlaid images.\n\nFor example an entire university might use images located on a master\nimage server and use a single URL for refering to their emblem.\n\nAlso certain very comman images almost always exist in the cache (like\nthe icons used by the apache web server (for mime types..), etc)\n\nSince caching still (and this is a real disappointment) depends on the\nURL matching exactly and not on some new unique name space both the\nrequirements cannot be achieved concurrently.\n\nI have a couple of suggestions for a new scheme which allows for the use\nof the present URL scheme while still retaining the benefits of both\npersistant connections and caching.\n\nI don't think I have been particularly clear on the topics I have\nmentioned but I don't hope the main point is noted.\n\nI'll come up with a page that explains these is a better fashion, but\ndon't you all think this is an issue that has to be opened.\n\nvishnu\n\nvishnu@cs.iitm.ernet.in\n\n\n\n", "id": "lists-010-16172801"}, {"subject": "I-D ACTION:draft-ietf-http-hit-metering01.tx", "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Simple Hit-Metering and Usage-Limiting for HTTP         \n       Author(s) : J. Mogul, P. Leach\n       Filename  : draft-ietf-http-hit-metering-01.txt\n       Pages     : 37\n       Date      : 03/20/1997\n\nThis document proposes a simple extension to HTTP, using a new ``Meter'' \nheader, which permits a limited form of demographic information \n(colloquially called ``hit-counts'') to be reported by caches to origin \nservers, in a more efficient manner than the ``cache-busting'' techniques \ncurrently used.  It also permits an origin server to control the number of \ntimes a cache uses a cached response, and outlines a technique that origin \nservers can use to capture referral information without ``cache-busting.'' \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-hit-metering-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-hit-metering-01.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-hit-metering-01.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-16182434"}, {"subject": "Re: new cookie draf", "content": "Yaron Goland <yarong@microsoft.com> writes:\n\n>I would suggest changing the domain specification to allow for the\n>inclusion of port number. So a domain could look like \".foo.bar.com:80\".\n>Now if a site is concerned about someone taking over a port, they can\n>specify that only the identified port may be used. If they control the\n>entire server and have no worries, they may then leave the port number\n>out.\n\nI agree with Yaron - there are many environments where two ports on the\nsame host are under wildly different and often legitimate ownership, and\nallowing the server to specify a port (but not requiring it to do so)\nwould help in those environments.  Claims that it's difficult to keep\n\"rogue\" servers up are specious, depending as they do on suppositions of\nlocal system management and reliability.  I can point to large mainframe\nhosts that aren't \"rebooted\" for many months at a time, making the\nlifetime of servers running on high-numbered ports rather long.\n\nThat said, the requirement that the DOMAIN attribute specify a\ndomain-suffix (i.e. \".x.com\" is legal, \"x.com\" is not) limits the\nexposure to sites where an entire subdomain is involved.  I'm willing to\naccept the argument that mutually-hostile servers spread across a group\nof hosts and nonetheless using DOMAIN is a vanishingly small population.\n\nI've been avoiding commenting on the new draft, since I thought RFC 2109\nwas good enough, but since I'm writing now, I have to make a few other\npoints:\n\n   1) \"SetCookie2\" is a lousy name for the function we're discussing.\n      \"SetSessionState\" or something similar would be much better than a\n      number-suffixed oddity.  If we MUST use the C-word, perhaps\n      \"CreateCookie\" or \"DefineCookie\" would do the job?\n\n   2) If the resulting request header was \"SessionState\" instead of\n      \"Cookie\", wouldn't all the odd syntax that derives from\n      Cookie-compatability be resolvable?  I understand that this looks\n      like a large change, and is likely to raise someone's ire, but\n      changing from \"SetCookie\" to \"SetCookie2\" has already introduced a\n      sufficiently large incompatability that existing browers will\n      require code changes to support the new mechanism.  If code\n      changes are required, shouldn't we take the opportunity to resolve\n      things like the EXPIRES syntax?\n\n   3) Section 2 \"TERMINOLOGY\" says that:\n\n         Host names can be specified either as an IP address or a FQHN\n         string.  Sometimes we compare one host name with another.  Host\n         A's name domain-matches host B's if\n\n            * both host names are IP addresses and their host name\n              strings match exactly;  or\n\n            * both host names are FQDN strings and their host name\n              strings match exactly;  or\n\n            * A is a FQDN string and has the form NB, where N is a\n              non-empty name string, B has the form .B', and B' is a\n              FQDN string.  (So, x.y.com domain-matches .y.com but not\n              y.com.)\n\n      This implies, especially to those who've never read the DNS RFCs,\n      that FQDN strings are case-sensitive.  Since domain names are not,\n      neither should they be.  I've had enough trouble convincing newbie\n      RFC-readers that this is the case elsewhere in HTTP that I'd like\n      to see it explicitly stated here.  Unless of course I'm completely\n      wrong.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n", "id": "lists-010-16192543"}, {"subject": "Re: new cookie draf", "content": "Ross Patterson wrote:\n> \n> Yaron Goland <yarong@microsoft.com> writes:\n> \n> [about ports...]\n\nThere seem to be two contradictory threads going on here.  On the one hand,\nsome folks want to be able to specify the port, to restrict where cookies\ngo.  On the other, folks want it to be possible to send the same cookie to\nmultiple ports on the same server.  And we have to remember the port 80\n(http) vs. port 443 (https) problem, of sending a cookie in both a secure\nand an insecure connection.\n\nAnother factor is that the same cookie can be sent to multiple domain\nnames.  Is the port transitive?\n\nI oppose the idea of changing the Domain= syntax, for compatibility\nreasons.  I can support a Port= addition to Set-Cookie2.  I had thought of\nsuch before, and the port restriction I added was an attempt to accomplish\nthe restriction implicitly.\n\nI would welcome a *complete* proposal for port handling that addresses the\nconcerns above, and that deals with what the correct behavior should be if\nPort= is omitted from Set-Cookie2.\n\n> [...]\n\n> I've been avoiding commenting on the new draft, since I thought RFC 2109\n> was good enough, but since I'm writing now, I have to make a few other\n> points:\n> \n>    1) \"SetCookie2\" is a lousy name for the function we're discussing.\n>       \"SetSessionState\" or something similar would be much better than a\n>       number-suffixed oddity.  If we MUST use the C-word, perhaps\n>       \"CreateCookie\" or \"DefineCookie\" would do the job?\n\n[Deep breath.  History, dating back to late 1995!]\n\nIn the beginning, those of us in the state management sub-group set out to\nagree on a mechanism.  As a practical matter, with Netscape's cookies\nalready deployed, it made sense to standardize them, tightening up the spec\nwhere we saw holes.  The idea was to have something fully compatible with\nNetscape's cookies and most existing uses.  We planned to use Cookie and\nSet-Cookie for the obvious reasons.\n\nRecently we became aware of forward compatibility problems with MSIE.  My\nattempt to address it has been to add another header which can eventually\nsubsume Set-Cookie.  Meanwhile (and the new I-D gives the rules), pieces of\nSet-Cookie and Set-Cookie2 (the new header) get combined to form a complete\nSet-Cookie2 header.  The goal has been to let V0 cookie clients continue to\nwork, V1 cookie clients begin to work, and both V0 and V1 servers can talk\nto either flavor client.  Set-Cookie2 was chosen as an obvious relation to\nSet-Cookie.  Why muck with it?  (I never liked Cookie or Set-Cookie, but\nthe exist, they work, and they won't go away.)\n> \n>    2) If the resulting request header was \"SessionState\" instead of\n>       \"Cookie\", wouldn't all the odd syntax that derives from\n>       Cookie-compatability be resolvable?  I understand that this looks\n>       like a large change, and is likely to raise someone's ire, but\n>       changing from \"SetCookie\" to \"SetCookie2\" has already introduced a\n>       sufficiently large incompatability that existing browers will\n>       require code changes to support the new mechanism.  If code\n>       changes are required, shouldn't we take the opportunity to resolve\n>       things like the EXPIRES syntax?\n\nIf my paragraphs above haven't answered this, read the I-D (again).  We are\ntrying to introduce the new stuff compatibly.  Yes, it would be nice to do\nsomething from scratch to \"get it right\", but it's a bit late for that.\nThe I-D addresses Expires in the transitional phase.  Note that there is no\nExpires in Set-Cookie2 -- it's replaced by Max-Age.\n\n> \n>    3) Section 2 \"TERMINOLOGY\" says that:\n> \n> [definition of domain-match]\n> \n>       This implies, especially to those who've never read the DNS RFCs,\n>       that FQDN strings are case-sensitive.  Since domain names are not,\n>       neither should they be.  I've had enough trouble convincing newbie\n>       RFC-readers that this is the case elsewhere in HTTP that I'd like\n>       to see it explicitly stated here.  Unless of course I'm completely\n>       wrong.\n\nYou're right, FQDN's are case-INsensitive.  That's a note worth making.\n\nDave Kristol\n\n\n\n", "id": "lists-010-16203440"}, {"subject": "UCI Archives will be down this weeken", "content": "Yes, I do mean all weekend.  Apparently someone has decided to do major\nwork on the power system, which means the network is going to be shutdown\nto avoid a power spike melting the equipment (it has happenned before).\n\nHopefully, Andy's mailing list archive will still be available at\n\n   http://www-uk.hpl.hp.com/people/ange/archives/\n\nBTW, this also means I will not be able to receive or send mail\nall weekend.\n\n.....Roy (at least the weather's great) Fielding\n\n\n\n", "id": "lists-010-1620353"}, {"subject": "Re: new cookie draf", "content": ">   1) \"SetCookie2\" is a lousy name for the function we're discussing.\n>      \"SetSessionState\" or something similar would be much better than a\n>      number-suffixed oddity.  If we MUST use the C-word, perhaps\n>      \"CreateCookie\" or \"DefineCookie\" would do the job?\n\nI was hoping for \"Set-Chips\" (just a small addition to the cookie)\nor \"Set-Brownie\" (that other thing we find at IETF meetings).\n\n....Roy\n\n\n\n", "id": "lists-010-16214838"}, {"subject": "Re: New Issue: Incompatiblity between caching needs and persistant connections", "content": "On Fri, 21 Mar 1997, Natchu Vishnu Priya wrote:\n\n> Put simply persistant connections require me to place all my related\n> documents, images, etc on a single comman server, while to facilitate\n> caching it might be very useful to use a comman url for inlaid images.\n\nI may be wrong, but I believe that e.g. Netscape opens 4 connections\nby default. Can it (or any agent that opens multiple conections) \nhave persistant connections to multiple servers, and win/win ?\n\n\n> For example an entire university might use images located on a master\n> image server and use a single URL for refering to their emblem.\n\nThis seemed to me to be a good idea in general, but I'm not so sure\nabout linking images from places around the world (Blue Ribbon, etc.)\ndue to network problems giving partly-broken pages. See\nhttp://vancouver-webpages.com/CacheNow/ for promotional material :-)\n\nI think some of these problems may go away if/when URNs are deployed\ninstead of URLs (for multiply-sourced objects)\n\nAndrew Daviel         mailto:advax@triumf.ca \nTRIUMF & Vancouver Webpages\n\n\n\n", "id": "lists-010-16222318"}, {"subject": "Re: New Issue: Incompatiblity between caching needs and persistant connections", "content": "HTTP/1.1 clients can have as many TCP connections simultaneously open \nto DIFFERENT HTTP servers as they want.\n- Jim\n\n\n\n", "id": "lists-010-16231899"}, {"subject": "RE: Issues with the cookie draf", "content": "Yaron Goland <yarong@microsoft.com> wrote:\n  > RE: $\n  > As the origin server sent out the cookie, why would the origin server\n  > also not know what sort of cookie it is receiving back? While it is true\n  > that the origin server may have sent out cookies with the name\n  > \"Version\", the origin server can then reliably detect that it is a new\n  > cookie by checking the second value and seeing that it is not a legal\n  > Netscape cookie value. It would seem that the \"$\" is not necessary.\n\nThe server doesn't know whether the user agent is V1-capable, so it has\nto be able to distinguish V0 from V1 cookies and their attributes.  And\nit has to deal with the ',' vs ';' punctuation problem for multiple\ncookies.  (Netscape's original spec. separated cookies in Cookie by ';'.)\nWe thought having a '$' to distinguish attributes made things much easier\nfor the server.  It's not elegant, but it works.\n  > \n  > Languages:\n  > As I mentioned in my original proposal, the accept-language header would\n  > server the purpose of choosing the language. In the worst case, the\n  > language is just English. The UTF8 Unicode encoding preserves the lower\n  > ASCII range so when dealing with downlevel clients, one sends UTF8\n  > English. I do admit woeful ignorance of the language tag issues. Any\n  > experts in the house?\n\nI'm also really bad on the language issues.  That's why I asked for more\ndetails.  I get the gist of what you're asking for, but I don't understand\nthe language and encoding issues clearly enough to know whether what you\nwant is a good idea or how to write it into the spec.\n  > \n  > Discard:\n  > I am fully aware of the Lab PC environment. That is why IE 4.0 NT will\n  > be shipping with both private and public personal caches. The private\n  > cache will only be available based on log-in. The public cache will be\n  > the default used by anyone who logs into the machine and who doesn't\n  > have a private cache. Thus the distinction your refer to is understood\n  > by the client. As such the client also has the ability to decide when to\n  > store a cookie and when not to. So changing the attribute to Private\n  > would mean \"If you are using a user specific cache then you may keep\n  > this cookie across log-ins. If you are using a system wide cache, then\n  > the cookie must be purged on log-out.\" I believe this is closer to the\n  > desired functionality than the current Discard definition.\n\nI would interpret the Discard language about \"sessions\" exactly as you\nhave, namely that when a user logs out his/her cookies that are marked\nDiscard get discarded.\n\nI actually think it's unwise for a multi-user machine to store *any*\ncookies in some kind of shared cookie cache, but that's a separate\ndiscussion.\n  > \n  > Including Version:\n  > I actually meant the comment to apply to Set-Cookie not Cookie. Given\n  > the use of the set-cookie2 header, version, when equal to 1, would\n  > appear unnecessary.\n\nSo is this what you mean?:  if the server sends a Set-Cookie2, then the\nuser agent should assume Version=1 if the server does not send send\nVersion= explicitly.  I suppose that's reasonable, provided the user\nagent sends $Version=1 in Cookie either way.\n\n  > \n  > Matching Security the cookie was transmitted with:\n  > I am not going to get religious on the issue, I am just concerned that\n  > the language requires impossible behavior. For example, if the system\n  > has used some out of band means to determine that it has an isolated\n  > connection to the server, for example, they are directly connected by a\n  > wire, it may be perfectly reasonable to send a secure cookie in clear\n  > text. I think the best option is to simply state that the server expects\n  > the cookie to be transmitted in a secure manner and leave it at that.\n\nI made this change in response to a suggestion from Raymie Stata.  As I\nsaid once before, I can back it out.\n\nAs to your example, if the server can tell there's a direct connection to\nthe client, it could send the cookie without \"Secure\".  But if the cookie\nis labeled \"Secure\", I think it's reasonable for the server to expect the\nclient to use secure means to send it back.  I think we can agree that it's\nhard to say that in a concise and precise way.  Every way of being precise\nbecomes a huge digression into security protocols, relative strengths of\nencryption, and lots of other stuff we don't want to get into here.\n  > \n  > Dealing with Malformed cookies:\n  > My concern is that handling of end cases caused the state spec to have\n  > to be revved in the first place. I would think, given past experience\n  > with cookies, it would be best to dot every \"i\" and cross every \"t\". In\n  > this case I believe it to be appropriate to declare that malformed\n  > cookies must be ignored. This is especially the case given that HTTP\n  > provides no mechanism for the client to return error information to the\n  > server.\n\nIt's possible.  What do others think?  The idea is, if the user agent or,\npresumably, the origin server, receives a Set-Cookie (Cookie) header that\nis non-conforming, the receiver *must* ignore the header.\n\n  > \n  > 4.3.2 Rejecting Cookies (how far into the domain do you go):\n  > I appreciate that it was a long and drawn out debate but that is not a\n  > sufficient rational for preventing perfectly reasonable behavior. The\n  > decision to stop at one domain level is completely arbitrary. It is no\n  > more and no less secure than 2 or infinite domain levels deep. I do not\n  > feel that an arbitrary choice is a good enough reason to include a\n  > requirement in a specification.\n\nIt wasn't completely arbitrary.  The goal was to protect privacy, and\nthe rule in the spec makes it harder for cookies to \"leak\" to servers\nfar removed from their origin.  I realize you (Yaron) have an\napplication where the two-domain offset isn't really \"far\".  Koen\nHoltman was the person most outspoken about the Domain= rules.  Perhaps\nhe would like to chime in?\n\n  > \n  > Quote David: \"You cannot specify explicitly by Domain and Path the\n  > domain\n  > and path you get by default.\"\n  > If you are explicitly defining Domain and Path, what do you care about\n  > the default? Perhaps an example would help?\n\nOkay.  Suppose I visit www.a.com.  If www.a.com sends me a cookie with\nno Domain=, then the default domain is \"www.a.com\" (no leading '.').  I\nwill only send that cookie back to www.a.com, not, for example,\nfoo.a.com.  OTOH, if www.a.com sends me Domain=.a.com, (with a leading\n'.'), I will return the cookie to www.a.com, foo.a.com, etc.  Section\n4.3.2 does not permit Domain=www.a.com.  Consequently, I can't force\nthe first behavior with an explicit Domain=, only with its absence.\nTherefore the presence/absence of $Domain in Cookie has significance.\n\n  > \n  > Domain and Path Ordering:\n  > How about, cookies are first ordered by domain based on a byte by byte\n  > comparison. Within a domain, cookies are path ordered as specified.\n\nFor compatibility, the Path ordering ought to come first.  Then you\ncould do Domain ordering.  Of course there are other details to\nspecify:  Is that left-to-right, based on the Domain= attribute?\nCanonicalized to lower case?  What about the default (omitted) Domain=\nattribute?  What are the consequences of mis-ordered cookies, if any?\n\nDave Kristol\n\n\n\n", "id": "lists-010-16239825"}, {"subject": "Re: I-D ACTION:draft-ietf-http-hit-metering01.tx", "content": "Regarding:\n\n       Title     : Simple Hit-Metering and Usage-Limiting for HTTP         \n       Author(s) : J. Mogul, P. Leach\n       Filename  : draft-ietf-http-hit-metering-01.txt\n       Pages     : 37\n       Date      : 03/20/1997\n\n    ftp://ds.internic.net/internet-drafts/draft-ietf-http-hit-metering-01.txt\n \nThe document still contains several notes labelled \"Design question\"\nor \"Design note\", and a detailed Revision History (instead of change\nbars).  Larry Masinter says that the final draft cannot include\nthese components, since they would not be appropriate in an RFC.\n\nTo this date, I don't think there have been many (or any) comments on\nthe Design Notes/Design Questions identified in the draft.  Consider\nthis as your last change to discuss those notes.  If there is no\nspecific and unresolved discussion of these notes, I will remove them\nand generate a new draft by the pre-Memphis I-D submission deadline\n(next Wednesday, March 26).\n\nAlso note that there have been numerous changes to the document;\nI think I managed to cover all of the normative changes in the\nRevision History section.  The two most important changes are:\n\n   Added a timeout mechanism, to constrain the time-inaccuracy of\n   hit-metering (sections 3.3, 3.5, 5.1, and 5.2).\n\n   Removed the rule that meter request directives apply to all\n   subsequent requests on a transport connection.  These directives now\n   apply only to one request.  Added an explicit requirement that a\n   server must not ask a proxy to do something it did not volunteer to\n   do (section 3.3).\n\nThanks\n-Jeff\n\n\n\n", "id": "lists-010-16254945"}, {"subject": "Secure cookies surve", "content": "will try to respond\n\n\n\n", "id": "lists-010-16264071"}, {"subject": "RE: Issues with the cookie draf", "content": "Version: Yes, the client would be required to send $Version = 1 in a\nresponse, even though the server isn't required to send Version = 1 in\nset-cookie2.\n\nSecurity: Exactly, that is why I think the best thing to say is \"When\nthe secure attribute is included the server expects that the cookie will\nonly be returned on a secure connection.\" We neatly side step the\nvarious issues. If \"secure\" really needs to be defined then someone can\npropose an extension to your spec to specify what type of security and\nhow to choose among multiple types.\n\nDomain Depth: I await Koen's response.\n\nWhen to return Domain: Ahh I see. Why is it that Domain doesn't allow\nFQDN? This would still work w/the matching rules. I'm sure I am missing\nsomething painfully obvious.\n\nDomain and Path Ordering: Yes, indeed. Well as Larry likes to constantly\nremind me in the DAV context, your name is first on the draft. =)\n\nYaron\n\n> -----Original Message-----\n> From:dmk@research.bell-labs.com [SMTP:dmk@research.bell-labs.com]\n> Sent:Friday, March 21, 1997 11:56 AM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:RE: Issues with the cookie draft\n> \n> Yaron Goland <yarong@microsoft.com> wrote:\n>   > RE: $\n>   > As the origin server sent out the cookie, why would the origin\n> server\n>   > also not know what sort of cookie it is receiving back? While it\n> is true\n>   > that the origin server may have sent out cookies with the name\n>   > \"Version\", the origin server can then reliably detect that it is a\n> new\n>   > cookie by checking the second value and seeing that it is not a\n> legal\n>   > Netscape cookie value. It would seem that the \"$\" is not\n> necessary.\n> \n> The server doesn't know whether the user agent is V1-capable, so it\n> has\n> to be able to distinguish V0 from V1 cookies and their attributes.\n> And\n> it has to deal with the ',' vs ';' punctuation problem for multiple\n> cookies.  (Netscape's original spec. separated cookies in Cookie by\n> ';'.)\n> We thought having a '$' to distinguish attributes made things much\n> easier\n> for the server.  It's not elegant, but it works.\n>   > \n>   > Languages:\n>   > As I mentioned in my original proposal, the accept-language header\n> would\n>   > server the purpose of choosing the language. In the worst case,\n> the\n>   > language is just English. The UTF8 Unicode encoding preserves the\n> lower\n>   > ASCII range so when dealing with downlevel clients, one sends UTF8\n>   > English. I do admit woeful ignorance of the language tag issues.\n> Any\n>   > experts in the house?\n> \n> I'm also really bad on the language issues.  That's why I asked for\n> more\n> details.  I get the gist of what you're asking for, but I don't\n> understand\n> the language and encoding issues clearly enough to know whether what\n> you\n> want is a good idea or how to write it into the spec.\n>   > \n>   > Discard:\n>   > I am fully aware of the Lab PC environment. That is why IE 4.0 NT\n> will\n>   > be shipping with both private and public personal caches. The\n> private\n>   > cache will only be available based on log-in. The public cache\n> will be\n>   > the default used by anyone who logs into the machine and who\n> doesn't\n>   > have a private cache. Thus the distinction your refer to is\n> understood\n>   > by the client. As such the client also has the ability to decide\n> when to\n>   > store a cookie and when not to. So changing the attribute to\n> Private\n>   > would mean \"If you are using a user specific cache then you may\n> keep\n>   > this cookie across log-ins. If you are using a system wide cache,\n> then\n>   > the cookie must be purged on log-out.\" I believe this is closer to\n> the\n>   > desired functionality than the current Discard definition.\n> \n> I would interpret the Discard language about \"sessions\" exactly as you\n> have, namely that when a user logs out his/her cookies that are marked\n> Discard get discarded.\n> \n> I actually think it's unwise for a multi-user machine to store *any*\n> cookies in some kind of shared cookie cache, but that's a separate\n> discussion.\n>   > \n>   > Including Version:\n>   > I actually meant the comment to apply to Set-Cookie not Cookie.\n> Given\n>   > the use of the set-cookie2 header, version, when equal to 1, would\n>   > appear unnecessary.\n> \n> So is this what you mean?:  if the server sends a Set-Cookie2, then\n> the\n> user agent should assume Version=1 if the server does not send send\n> Version= explicitly.  I suppose that's reasonable, provided the user\n> agent sends $Version=1 in Cookie either way.\n> \n>   > \n>   > Matching Security the cookie was transmitted with:\n>   > I am not going to get religious on the issue, I am just concerned\n> that\n>   > the language requires impossible behavior. For example, if the\n> system\n>   > has used some out of band means to determine that it has an\n> isolated\n>   > connection to the server, for example, they are directly connected\n> by a\n>   > wire, it may be perfectly reasonable to send a secure cookie in\n> clear\n>   > text. I think the best option is to simply state that the server\n> expects\n>   > the cookie to be transmitted in a secure manner and leave it at\n> that.\n> \n> I made this change in response to a suggestion from Raymie Stata.  As\n> I\n> said once before, I can back it out.\n> \n> As to your example, if the server can tell there's a direct connection\n> to\n> the client, it could send the cookie without \"Secure\".  But if the\n> cookie\n> is labeled \"Secure\", I think it's reasonable for the server to expect\n> the\n> client to use secure means to send it back.  I think we can agree that\n> it's\n> hard to say that in a concise and precise way.  Every way of being\n> precise\n> becomes a huge digression into security protocols, relative strengths\n> of\n> encryption, and lots of other stuff we don't want to get into here.\n>   > \n>   > Dealing with Malformed cookies:\n>   > My concern is that handling of end cases caused the state spec to\n> have\n>   > to be revved in the first place. I would think, given past\n> experience\n>   > with cookies, it would be best to dot every \"i\" and cross every\n> \"t\". In\n>   > this case I believe it to be appropriate to declare that malformed\n>   > cookies must be ignored. This is especially the case given that\n> HTTP\n>   > provides no mechanism for the client to return error information\n> to the\n>   > server.\n> \n> It's possible.  What do others think?  The idea is, if the user agent\n> or,\n> presumably, the origin server, receives a Set-Cookie (Cookie) header\n> that\n> is non-conforming, the receiver *must* ignore the header.\n> \n>   > \n>   > 4.3.2 Rejecting Cookies (how far into the domain do you go):\n>   > I appreciate that it was a long and drawn out debate but that is\n> not a\n>   > sufficient rational for preventing perfectly reasonable behavior.\n> The\n>   > decision to stop at one domain level is completely arbitrary. It\n> is no\n>   > more and no less secure than 2 or infinite domain levels deep. I\n> do not\n>   > feel that an arbitrary choice is a good enough reason to include a\n>   > requirement in a specification.\n> \n> It wasn't completely arbitrary.  The goal was to protect privacy, and\n> the rule in the spec makes it harder for cookies to \"leak\" to servers\n> far removed from their origin.  I realize you (Yaron) have an\n> application where the two-domain offset isn't really \"far\".  Koen\n> Holtman was the person most outspoken about the Domain= rules.\n> Perhaps\n> he would like to chime in?\n> \n>   > \n>   > Quote David: \"You cannot specify explicitly by Domain and Path the\n>   > domain\n>   > and path you get by default.\"\n>   > If you are explicitly defining Domain and Path, what do you care\n> about\n>   > the default? Perhaps an example would help?\n> \n> Okay.  Suppose I visit www.a.com.  If www.a.com sends me a cookie with\n> no Domain=, then the default domain is \"www.a.com\" (no leading '.').\n> I\n> will only send that cookie back to www.a.com, not, for example,\n> foo.a.com.  OTOH, if www.a.com sends me Domain=.a.com, (with a leading\n> '.'), I will return the cookie to www.a.com, foo.a.com, etc.  Section\n> 4.3.2 does not permit Domain=www.a.com.  Consequently, I can't force\n> the first behavior with an explicit Domain=, only with its absence.\n> Therefore the presence/absence of $Domain in Cookie has significance.\n> \n>   > \n>   > Domain and Path Ordering:\n>   > How about, cookies are first ordered by domain based on a byte by\n> byte\n>   > comparison. Within a domain, cookies are path ordered as\n> specified.\n> \n> For compatibility, the Path ordering ought to come first.  Then you\n> could do Domain ordering.  Of course there are other details to\n> specify:  Is that left-to-right, based on the Domain= attribute?\n> Canonicalized to lower case?  What about the default (omitted) Domain=\n> attribute?  What are the consequences of mis-ordered cookies, if any?\n> \n> Dave Kristol\n\n\n\n", "id": "lists-010-16270493"}, {"subject": "(CONTENT NEGOTIATION,VARY) New draft text for Vary header and content negotiation `hooks", "content": "The text below is a new version of the text I posted on Monday.  I\nwish to thank Daniel DuBois and Jeff Mogul for their comments on the\nversion posted on Monday.  There comments led to some improvements and\nadditions in the text below.\n\nChanges with respect to the version posted on Monday are marked with |\nchange bars.  The change bars were added by hand, so no 100% accuracy\ncan be guaranteed.\n\nI expect to make some minor changes to this text in the next few days,\nin order to get it completely into sync with the 1.1 caching text by\nJeff Mogul, which is still being prepared.\n\nIf you have comments on this text, now is the time to comment.  I\nintend to close this issue in the middle of next week.  This means\nthat I will send a last call for disagreement with perceived\nconsensus, together with a possibly improved version of the text\nbelow, on Monday.\n\nKoen.\n\n--snip--\n\n\n** I. Vary+content negotiation new/changed header descriptions\n\n[##Note: The current section 12 needs to be deleted completely from the\nApril 1.1 draft.##]\n\n10.v  Vary\n\n|  [##The first few paragraphs below were rewritten to account for\n|  range retrievals, to shorten sentences, and to make some stylistic\n|  improvements.  The text now accounts for range retrievals, I forgot\n|  to cover them in the previous version.  The role of the\n|  Authorization header has also been made more clear.##]\n\n|  The Vary response-header field is used by an origin server to\n|  signal that the resource identified by the current request is a\n|  varying resource.  A varying resource has multiple entities\n|  associated with it, all of which are representations of the content\n|  of the resource.  If a GET or HEAD request on a varying resource is\n|  received, the origin server will select one of the associated\n|  entities as the entity best matching the request.  Selection of\n|  this entity is based on the contents of particular header fields in\n|  the request message, or on other information pertaining to the\n|  request, like the network address of the sending client.\n\n   If a resource is varying, this has an important effect on cache\n   management, particularly for caching proxies which service a\n|  diverse set of user agents.  All 200 (OK) responses from varying\n   resources must contain at least one Vary header or Alternates\n   header (Section 10.a) to signal variance.\n\n   If no Vary headers and no Alternates headers are present in a 200\n   (OK) response, then caches may assume, as long as the response is\n|  fresh, that the resource in question is not varying, and has only\n|  one associated entity.  Note however that this entity can still\n   change through time, as possibly indicated by a Cache-Control\n   response header (section 10.cc).\n\n|  After selection of the entity best matching the current request,\n|  the origin server will usually generate a 200 (OK) response, but it\n|  can also generate other responses like 206 (Partial Content) or 304\n|  (Not modified) if headers which modify the semantics of the\n|  request, like Range (Section 10.ran) or If-Valid (Section 10.ifva),\n|  are present.  An origin server need not be capable of selecting an\n|  entity for every possible incoming request on a varying resource,\n|  it can choose to generate a 3xx (redirection) or 4xx (client error)\n|  type response for some requests.\n\n|  In a request message on a varying resource, the selecting request\n|  headers are those request headers whose contents were used by the\n|  origin server to select the entity best matching the request. The\n|  Vary header field specifies the selecting request headers and any\n   other selection parameters that were used by the origin server.\n\n       Vary                 = \"Vary\" \":\" 1#selection-parameter\n\n       selection-parameter  = field-name\n                            | \"{\" \"accept-headers\" \"}\"\n                            | \"{\" \"other\" \"}\"\n                            | \"{\" \"unknown\" \"}\"\n                            | \"{\" extension-parameter \"}\"\n\n       extension-parameter  = token\n\n   The presence of a field-name signals that the request-header field\n   with this name is selecting.  The field-name will usually be, but\n   need not be, a request-header field name defined in this\n   specification.  Note that field names are case-insensitive.  The\n   presence of the \"accept-headers\" parameter signals that all request\n   headers whose names start with \"accept\" are selecting.\n\n   The inclusion of the \"other\" parameter in a Vary field signals that\n   parameters other than the contents of request headers, for example\n   the network address of the sending party, play a role in the\n   selection of the response.  The \"unknown\" parameter signals that\n   the origin server is not willing or able to specify the selection\n   parameters used.  If an extension-parameter unknown to the cache is\n   present in a Vary header, the cache must treat it as the \"unknown\"\n   parameter.\n\n|  [##Note that caches have to treat \"other\" and \"unknown\" in the same\n|  way.  I distinguish between them to make interpretation of Vary\n|  headers by humans easier.##]\n\n   If multiple Vary and Alternates header fields are present in a\n   response, these must be combined to give all selecting parameters.\n\n|  The field name \"Host\" must never be included into a Vary header,\n|  clients must ignore it if it is present.  The names of fields which\n|  change the semantics of a GET request, like \"Range\" and \"If-Valid\"\n|  must also never be included, and must be ignored when present.  \n|  \n|  Servers which use access authentication are not obliged to send\n|  \"Vary: Authorization\" headers in responses.  It must be assumed\n|  that requests on authenticated resources can always produce\n|  different responses for different users.  Note that servers can\n|  signal the absence of authentication by including a \"Cache-Control:\n|  public\" header in the response.\n\n   A cache may always store the relayed 200 (OK) responses from a\n   varying resource, and can refresh them according to the rules in\n|  Section aa.bb [##Which will be written by Jeff Mogul##].  The\n|  partial entities in 206 (Partial Content) responses from varying\n|  resources may also be stored.\n\n   When getting a request on a varying resource, a cache can only\n|  return a cached 200 (OK) response to one of its clients in two\n   particular cases.\n\n   First, if a cache gets a request on a varying resource for which it\n   has cached one or more responses with Vary or Alternates headers,\n   it can relay that request towards the origin server, adding an\n|  If-Valid header listing the cache validators in the Cval headers of\n   the cached responses.  If it then gets back a 3xx (Ppp Qqq) [##TBS\n   ##] response with the cache validator of a cached 200 (OK) response\n   in its Cval header, it can return this cached 200 (OK) response to\n   its client, after merging in any of the 3xx response headers as\n   specified in Section xx.yy [##Which will be written by Jeff\n   Mogul##].\n\n|  [##Note: The text above and Jeff's preliminary draft of the caching\n|  text seem to be out slightly out of sync, but there do not seem to\n|  be big semantic gaps.  One difference is that Jeff's text has the\n|  cached include both the cache validator and the variant-id in the\n|  If-Valid headers, whereas the text above describes a scheme were\n|  only the cache validators are included.##]\n\n   Second, if a cache gets a request on a varying resource, it can\n   return to its client a cached, fresh 200 (OK) response which has\n   Vary or Alternates headers, provided that\n\n       - the Vary and Alternates headers of this fresh response\n         specify that only request header fields are selecting\n         parameters,\n\n       - the specified selecting request header fields of the current\n         request match the specified selecting request header fields\n         of a previous request on the resource relayed towards the\n         origin server,\n\n       - this previous request got a 200 (OK) or 3xx (Ppp Qqq)\n         response which had the same cache validator in its CVal header\n         as the cached, fresh 200 (OK) response.\n\n   Two sequences of selecting request header fields match if and only\n   if the first sequence can be transformed into the second sequence\n   by only adding or removing whitespace at places in fields where\n   this is allowed according to the syntax rules in this\n   specification.\n\n   [##Note that a more complicated matching rule could be defined in a\n   future specification.  The rule above reflects the consensus of the\n   editorial group on how complex we can get in HTTP/1.1##]\n\n|  [##Note that the above rule says sequences, not sets of request\n|  headers.  It cannot say sets because, for some request headers\n|  (like Via?) which contain comma-separated lists, if you have two in\n|  a request, the order in which they appear matters.  A simple\n|  matching rule which would allow some forms of re-shuffling and\n|  collapsing of request headers to get a match turned out to be\n|  beyond my capabilities to write.##]\n\n|  If a cached 200 (OK) response may be returned to a request on a\n|  varying resource which included Range request header, then a cache\n|  may also use this 200 (OK) response to construct and return a 206\n|  (Partial Content) response with the requested range.\n\n         Note: Implementation of support for the second case above is\n         mainly interesting in user agent caches, as a user agent\n         cache will generally have an easy way of determining whether\n         the sequence of request header fields of the current request\n         equals the sequence sent in an earlier request on the same\n         resource.  Proxy caches supporting the second case would have\n         to record diverse sequences of request header fields\n         previously relayed; the implementation effort associated with\n         this may not be balanced by a sufficient payoff in traffic\n         savings.  A planned specification of a content negotiation\n         mechanism will define additional cases in which proxy caches\n         can return a cached 200 (OK) response without contacting the\n         origin server.  The implementation effort associated with\n         support for these additional cases is expected to have a much\n         better cost/benefit ratio.\n\n  [##Note that the `planned specification of a content negotiation\n  mechanism' above does not necessarily have to be draft-holtman!'  In\n  theory, a content negotiation mechanism totally unlike draft-holtman\n  could just as well live up to these cost/benefit expectations.##]\n\n10.a  Alternates\n\n   The Alternates response-header field is used by origin servers to\n   signal that the resource identified by the request-URI and the Host\n   request header (present if the request-URI is not an absoluteURI)\n   has the capability to send different responses depending on the\n   accept headers in the request message.  This has an important\n   effect on cache management, particularly for caching proxies which\n   service a diverse set of user agents.  This effect is covered in\n   Section 10.v.\n\n       Alternates           = \"Alternates\" \":\" opaque-field\n\n       opaque-field         = field-value\n\n   The Alternates header is included into HTTP/1.1 to make HTTP/1.1\n   caches compatible with a planned content negotiation mechanism.\n   HTTP/1.1 allows a future content negotiation standard to define the\n   format of the Alternates header field-value, as long as the defined\n   format satisfies the general rules in Section 4.2.\n\n   To ensure compatibility with future experimental or standardized\n   software, caching HTTP/1.1 clients must treat all Alternates\n   headers in a response as synonymous to the following Vary header:\n\n         Vary: {accept-headers}\n\n   and follow the caching rules associated with the presence of this\n   Vary header, as covered in Section 10.v.  HTTP/1.1 allows origin\n   servers to send Alternates headers under experimental conditions.\n\n\n10.u  URI\n\n   The URI entity-header field is used to inform the recipient of\n   other Uniform Resource Identifiers (Section 3.2) by which\n   the resource can be identified.\n\n       URI-header  = \"URI\" \":\" 1#( uri-mirror | uri-name )\n\n       uri-mirror  = \"{\" \"mirror\" <\"> URI <\"> \"}\"\n       uri-name    = \"{\" \"name\" <\"> URI <\"> \"}\"\n\n   Any URI specified in this field can be absolute or relative to the\n   Request-URI. The \"mirror\" form of URI refers to a location which is a\n   mirror copy of the Request-URI. The \"name\" form refers to a\n   location-independent name corresponding to the Request-URI.\n\n   [##Note: According to the issues list, Roy is working on text that\n   explains better what \"mirror\" and \"name\" actually mean.##]\n\n\n** II. Changed status code descriptions\n\n300 Multiple Choices\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 300\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\n406 None Acceptable\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 406\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\n\n** III.  New text for the (new) caching section\n\n13.x Interoperability of varying resources with HTTP/1.0 proxy caches\n\n  [## Note: the text in 13.x could be part of a larger subsection in\n  the 1.1 document##]\n\n  If the correct handling of responses from a varying resource\n  (Section 10.v) by HTTP/1.0 proxy caches in the response chain is\n  important, HTTP/1.1 origin servers can include the following Expires\n  (Section 10.exp) response header in all responses from the varying\n  resource:\n\n     Expires: Thu, 01 Jan 1980 00:00:00 GMT\n\n  If this Expires header is included, the server should usually also\n  include a Cache-Control header for the benefit of HTTP/1.1 caches,\n  for example\n\n     Cache-Control: max-age=604800\n\n  which overrides the freshness lifetime of zero seconds specified by\n  the included Expires header.\n\n\n13.y Cache replacement for varying resources\n\n  If a new 200 (OK) response is received from a non-varying resource\n  while an old 200 (OK) response is cached, caches can delete this old\n  response from cache memory and insert the new response.  For 200\n  (OK) responses from varying resources (Section 10.v), cache\n  replacement is more complex.\n\n  HTTP/1.1 allows the authors of varying resources to guide cache\n  replacement by the inclusion of elements of so-called replacement\n  keys in the responses of these resources.  The replacement key of a\n  varying response consists of two elements, both of which may be\n  empty strings, separated by a semicolon:\n\n       replacement-key  =  variant-id \";\" absoluteURI\n\n  The variant-id element of the replacement key is the variant-id\n| value in the Cval header of the response, if a Cval header which\n  such a value is present, and an empty string otherwise.  The\n  absoluteURI element of the replacement key is the absolute URI given\n  in, or derived from, the Content-Location header of the response if\n  present, and and an empty string if no Content-Location header is\n  present.\n\n|\n  If a cache has stored in memory a 200 (OK) response with a certain\n  replacement key, and receives, from the same resource, a new 200\n  (OK) response which has the same replacement key, this should be\n  interpreted as a signal from the resource author that the old\n  response can be deleted from cache memory and replaced by the new\n  response.\n\n  The replacement key mechanism cannot cause deletion from cache\n  memory of old responses with replacement keys that will no longer be\n  used.  It is expected that the normal `least recently used'\n  replacement heuristics employed by caches will eventually cause such\n  old responses to be deleted.\n\n| All 200 (OK) responses from varying resources should include\n| replacement key elements.  Resource authors may not assume that\n| caches will be able to cache responses not including replacement key\n| elements.  If a Vary header is used to signal variance, the response\n| should include a variant-id value as the replacement key element.\n| The Content-Location header should only be used to supply a\n| replacement key element if an Alternates header is present in the\n| response.\n\n\n[End of document]\n\n\n\n", "id": "lists-010-1627309"}, {"subject": "RE: new cookie draf", "content": "I admit that port is a pit and I don't ever expect anyone to specify a\nport. There are legitimate cases where one would want a cookie to go to\nmultiple ports. So rather than putting a blanket restriction I proposed\nthat we allow the server to decide what behavior they would like. We can\ndefine an attribute \"PORT\", with no argument. If it is included in a\ncookie then the cookie may only be returned on the port it was received\non, this requirement applies to all domains. This recreates the current\nrestriction in the spec. If the server is not concerned about the issue\nof ports, then it leaves the attribute off and the client is free to\nreturn the cookie on any port in a legal domain.\nYaron\n\n> -----Original Message-----\n> From:Dave Kristol [SMTP:dmk@bell-labs.com]\n> Sent:Friday, March 21, 1997 8:56 AM\n> To:Ross Patterson\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: new cookie draft\n> \n> Ross Patterson wrote:\n> > \n> > Yaron Goland <yarong@microsoft.com> writes:\n> > \n> > [about ports...]\n> \n> There seem to be two contradictory threads going on here.  On the one\n> hand,\n> some folks want to be able to specify the port, to restrict where\n> cookies\n> go.  On the other, folks want it to be possible to send the same\n> cookie to\n> multiple ports on the same server.  And we have to remember the port\n> 80\n> (http) vs. port 443 (https) problem, of sending a cookie in both a\n> secure\n> and an insecure connection.\n> \n> Another factor is that the same cookie can be sent to multiple domain\n> names.  Is the port transitive?\n> \n> I oppose the idea of changing the Domain= syntax, for compatibility\n> reasons.  I can support a Port= addition to Set-Cookie2.  I had\n> thought of\n> such before, and the port restriction I added was an attempt to\n> accomplish\n> the restriction implicitly.\n> \n> I would welcome a *complete* proposal for port handling that addresses\n> the\n> concerns above, and that deals with what the correct behavior should\n> be if\n> Port= is omitted from Set-Cookie2.\n> \n> > [...]\n> \n> > I've been avoiding commenting on the new draft, since I thought RFC\n> 2109\n> > was good enough, but since I'm writing now, I have to make a few\n> other\n> > points:\n> > \n> >    1) \"SetCookie2\" is a lousy name for the function we're\n> discussing.\n> >       \"SetSessionState\" or something similar would be much better\n> than a\n> >       number-suffixed oddity.  If we MUST use the C-word, perhaps\n> >       \"CreateCookie\" or \"DefineCookie\" would do the job?\n> \n> [Deep breath.  History, dating back to late 1995!]\n> \n> In the beginning, those of us in the state management sub-group set\n> out to\n> agree on a mechanism.  As a practical matter, with Netscape's cookies\n> already deployed, it made sense to standardize them, tightening up the\n> spec\n> where we saw holes.  The idea was to have something fully compatible\n> with\n> Netscape's cookies and most existing uses.  We planned to use Cookie\n> and\n> Set-Cookie for the obvious reasons.\n> \n> Recently we became aware of forward compatibility problems with MSIE.\n> My\n> attempt to address it has been to add another header which can\n> eventually\n> subsume Set-Cookie.  Meanwhile (and the new I-D gives the rules),\n> pieces of\n> Set-Cookie and Set-Cookie2 (the new header) get combined to form a\n> complete\n> Set-Cookie2 header.  The goal has been to let V0 cookie clients\n> continue to\n> work, V1 cookie clients begin to work, and both V0 and V1 servers can\n> talk\n> to either flavor client.  Set-Cookie2 was chosen as an obvious\n> relation to\n> Set-Cookie.  Why muck with it?  (I never liked Cookie or Set-Cookie,\n> but\n> the exist, they work, and they won't go away.)\n> > \n> >    2) If the resulting request header was \"SessionState\" instead of\n> >       \"Cookie\", wouldn't all the odd syntax that derives from\n> >       Cookie-compatability be resolvable?  I understand that this\n> looks\n> >       like a large change, and is likely to raise someone's ire, but\n> >       changing from \"SetCookie\" to \"SetCookie2\" has already\n> introduced a\n> >       sufficiently large incompatability that existing browers will\n> >       require code changes to support the new mechanism.  If code\n> >       changes are required, shouldn't we take the opportunity to\n> resolve\n> >       things like the EXPIRES syntax?\n> \n> If my paragraphs above haven't answered this, read the I-D (again).\n> We are\n> trying to introduce the new stuff compatibly.  Yes, it would be nice\n> to do\n> something from scratch to \"get it right\", but it's a bit late for\n> that.\n> The I-D addresses Expires in the transitional phase.  Note that there\n> is no\n> Expires in Set-Cookie2 -- it's replaced by Max-Age.\n> \n> > \n> >    3) Section 2 \"TERMINOLOGY\" says that:\n> > \n> > [definition of domain-match]\n> > \n> >       This implies, especially to those who've never read the DNS\n> RFCs,\n> >       that FQDN strings are case-sensitive.  Since domain names are\n> not,\n> >       neither should they be.  I've had enough trouble convincing\n> newbie\n> >       RFC-readers that this is the case elsewhere in HTTP that I'd\n> like\n> >       to see it explicitly stated here.  Unless of course I'm\n> completely\n> >       wrong.\n> \n> You're right, FQDN's are case-INsensitive.  That's a note worth\n> making.\n> \n> Dave Kristol\n\n\n\n", "id": "lists-010-16289137"}, {"subject": "RE: new cookie draf", "content": "On Fri, 21 Mar 1997, Yaron Goland wrote:\n> We can\n> define an attribute \"PORT\", with no argument. If it is included in a\n> cookie then the cookie may only be returned on the port it was received\n> on, this requirement applies to all domains. \n\nThat sounds right.  \n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-16303572"}, {"subject": "RE: Issues with the cookie draf", "content": "On Fri, 21 Mar 1997, Dave Kristol wrote:\n> What do others think?  The idea is, if the user agent or,\n> presumably, the origin server, receives a Set-Cookie (Cookie) header that\n> is non-conforming, the receiver *must* ignore the header.\n\nSince cookies are widely set by content providers through simple server\nextensions (as opposed to just by relatively few server implementors), I\nthink this is a bad idea.  All sorts of non-RFC readers will be setting\ncookies.  Let's make it as easy on them as possible. I would want the\nclient to do its best to figure out the server's intent, while adhering to\nthe spec. \n\nI don't think our current predicament is really caused by clients being too\nliberal in what they accept; it seems to have been caused by the lack of a\nspecification in the original cookie proposal for what a browser should do\nwith extra attributes in a set-cookie.  The RFC contains the line:\n\n> 4.3.1  Interpreting Set-Cookie2 [...]\n> The user agent should ignore attribute-values pairs whose attribute it\n> does not recognize. \n\n...which should solve the problem we have been addressing.  I don't think\nthat just because we've had this problem, we should make the clients\nrequire strict syntax.  What if two browsers have different interpretations\nof a conforming cookie?  If they both reject non-conforming cookies and\ndisagree about what it means to conform, no one set-cookie syntax will\nwork, and we'll have to switch on User-agent -- please, let's not create\nthat problem again! \n\n>   > 4.3.2 Rejecting Cookies (how far into the domain do you go):\n>   > I appreciate that it was a long and drawn out debate but that is not a\n>   > sufficient rational for preventing perfectly reasonable behavior. The\n>   > decision to stop at one domain level is completely arbitrary. It is no\n>   > more and no less secure than 2 or infinite domain levels deep. I do not\n>   > feel that an arbitrary choice is a good enough reason to include a\n>   > requirement in a specification.\n> \n> It wasn't completely arbitrary.  The goal was to protect privacy, and\n> the rule in the spec makes it harder for cookies to \"leak\" to servers\n> far removed from their origin.\n\nI disagree that there was any arbitrariness involved on our part.  The\narbitrary factor is the designation of an organizational unit in a domain\nname.  If you want to change this, come up with a rule that sufficiently\ncovers the following cases:\n\n.com.\n.sun.com\n.foo.sg\n.demon.co.uk\n.bar.com.au\n.powells.pdx.or.us\n\nOn Fri, 21 Mar 1997, Roy T. Fielding wrote:\n> I was hoping for \"Set-Chips\" (just a small addition to the cookie)\n> or \"Set-Brownie\" (that other thing we find at IETF meetings).\n\nOi, what have you started.  ;)  How about \"Set-Baking-Soda\" -- that\nrequired to make cookies fully baked? \n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-16311817"}, {"subject": "RE: new cookie draf", "content": "On Thu, 20 Mar 1997, Jaye, Dan wrote:\n> I saw that M Hedlund's suggested wording of allowing a \"user-designated\n> agent\" to verify a transaction has been incorporated.  I am concerned\n> that without more explicit description of how those agents would operate,\n> the certification authorities, and protocols, third party verification\n> will never really happen.  I will try to make my proposal more explicit. \n\nHmm....if you wanted a standard protocol for communication between a\nuser-agent extension and an external cookie review server, I would say that\nshould be a separate document. \n\nI also don't think any RFC will _make_ it happen.  If this spec allows it\nto happen and people agree there's a need, it certainly can happen without\nfurther specification.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-16322577"}, {"subject": "Re: 305 Use prox", "content": "On Thu, 20 Mar 1997, Ari Luotonen wrote:\n\n> Security hole:\n> \n> If 305 is allowed by origin servers, intermediate HTTP/1.1 proxies\n> that do not understand 305's hop-by-hope requirement will let it\n> through (I assume at this point it may be too late to impose the\n> hop-by-hop requirement for 305, and expect it to be respected by all\n> implementations).  If a client gets a 305 sent by an evil origin\n> server through a proxy, it will override the client's proxy settings,\n> because the client thinks the proxy redirected it to another proxy.\n\nWhat is to stop an evil origin server from sending the 305 anyway? If\nthe proxy doesn't enforce the hop-hop requirement it surely won't do\nsomething special about the evil origin server.\n\n> This security hole can completely cripple people behind a firewall\n> where the only way out is through the corporate proxy.  If some evil\n> site sends a 305 that goes through to the client, the client will from\n> then on try to use a proxy that is inaccessible for it (remember, it\n> needs to go thru the company firewall proxy to get out).\n> \n> If this security hole is narrowed down by the scope rules, it at the\n> same time limits the use of 305 for other uses which I would consider\n> more reasonable for 305 -- namely load balancing between proxies, and\n> diverting away load from a proxy that is scheduled to go down for\n> maintenance.  See below how 305 is not necessary for making\n> server-originated proxy redirects.\n> \n> Redundant:\n> \n> 305 is by no means the only way the client can be instructed to go\n> through a proxy.  A regular 301/302 redirect could be used, pointing\n> to a \"reverse\" proxy.  A \"reverse proxy\" appears as a regular server,\n> but is really a proxy when it comes to its content retrieval and\n> management.  As far as the client knows, it's an origin server.\n\nHardly redundant ... a reverse proxy is quite a bit more difficult to\nestablish as it must explicitly collaborate with the origin server. Also,\neach URL to be redirected must first go to the origin server and also all\nthe way back up the proxy chain.\n\n\n> from client outward, not by origin servers (because they have no\n> knowledge to control the proxy settings of clients).\n\nA lot easier to imagine that a heavily used origin server would know about\ngeographic proxies and be able to make rough guesses when to redirect\na request to a closer proxy than to imagine an individual user client\nhaving enough information available to identify the proxy.\n\n> Overhead on clients:\n> \n> Basically, 305 generated by origin servers implies having to maintain\n> a proxy setting for every URL.  This may be tolerable in clients, but\n\nWith your reverse proxy approach, it would surely be every URL which had\nto be tracked by the client.  The scoping rules allow specification of\nproxy for many URLs.\n\n> it's worse on proxies which get a large amount of requests.  Scope\n> rules complicate things worse.\n\nAny worse than zillions of 301/2 redirects being sent back thru the proxy\nonly now the client then sends the redirected request back thru the same\nproxy? \n\n> \n> The reasoning for scope rules is primarily to plug the security hole.\n\nI think just as important is the ability to have clients use different\nproxies for collections of servers.\n\n> \n> I would like to get rid of the automatic scope rule restrictions\n> altogether, which entails not allowing origin servers not to be\n> allowed to send 305's at all, and use a combination of regular\n> redirects and reverse proxies instead.\n> \n> By virtue of \"trusted proxies\" you can allow global proxy redirects\n> (if scope is not present), which allows *all* requests earlier\n> targeted for a given proxy, to be completely diverted to another.\n\nSuch redirects need more than scope rules. It should be possible/necessary\nto specify the duration of the redirect.\n\n\nI think proxy management would much better handled by a new HTTP method\nvia which the client would 'lease' a proxy assignment which might apply to\nall requests or some subset.  Use the 305 response type of redirection as\na mechanism for detailed rerouting of requests.  The trusted proxy manager\nmight even be asked to validate the 305 redirection.\n\nI believe it is reasonable to expect the origin server or a downstream\nproxy to know more about how a request should be routed to it. Extending\nthe response a bit further, perhaps there should be a varies by geography\nlist of proxies furnished.  The origin server may know the best\nalternatives but not what is best for a particular request.\n\nDave Morris\n\n\n\n", "id": "lists-010-16331117"}, {"subject": "RE: new cookie draf", "content": "On Fri, 21 Mar 1997, M. Hedlund wrote:\n\n> On Fri, 21 Mar 1997, Yaron Goland wrote:\n> > We can\n> > define an attribute \"PORT\", with no argument. If it is included in a\n> > cookie then the cookie may only be returned on the port it was received\n> > on, this requirement applies to all domains. \n> \n> That sounds right.  \n\nAn alternative ... a PORT attribute whose value is a comman delimited list\nof ports on which the cookie may be returned. If the PORT attribute is\nomitted, any port is valid.  If the value of the PORT attribute is NULL,\nthen as Yaron suggested, it may only be sent to the port it was received\nfrom. This allows it to be very tight while not excluding a value like\n \n                port=\"80,443\"\n\nwhich would allow sharing beteen the default HTTP and HTTPS ports.\n\nNote: While I am proposing a mechanism to resolve an issue, I don't share\nthe concern so I will be happy with any solution which allows sharing\nbetween ports.\n\nDave Morris\n\n\n\n", "id": "lists-010-16343525"}, {"subject": "RE: new cookie draf", "content": "Actually I suggested the exact opposite. If PORT is NULL then the cookie\nmay be sent on any port. It is only if a port is specified that there is\na restriction.\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Friday, March 21, 1997 10:21 PM\n> To:http working group\n> Subject:RE: new cookie draft\n> \n> \n> \n> On Fri, 21 Mar 1997, M. Hedlund wrote:\n> \n> > On Fri, 21 Mar 1997, Yaron Goland wrote:\n> > > We can\n> > > define an attribute \"PORT\", with no argument. If it is included in\n> a\n> > > cookie then the cookie may only be returned on the port it was\n> received\n> > > on, this requirement applies to all domains. \n> > \n> > That sounds right.  \n> \n> An alternative ... a PORT attribute whose value is a comman delimited\n> list\n> of ports on which the cookie may be returned. If the PORT attribute is\n> omitted, any port is valid.  If the value of the PORT attribute is\n> NULL,\n> then as Yaron suggested, it may only be sent to the port it was\n> received\n> from. This allows it to be very tight while not excluding a value like\n>  \n>                 port=\"80,443\"\n> \n> which would allow sharing beteen the default HTTP and HTTPS ports.\n> \n> Note: While I am proposing a mechanism to resolve an issue, I don't\n> share\n> the concern so I will be happy with any solution which allows sharing\n> between ports.\n> \n> Dave Morris\n\n\n\n", "id": "lists-010-16351390"}, {"subject": "RE: Issues with the cookie draf", "content": "I'll come up with a rule to handle your cases as soon as you come up\nwith a rule to allow me to share cookies across:\n\ncompanyname.com\nproductname.companyname.com\nversion1.productname.companyname.com\nversion2.productname.companyname.com\nversion3.productname.companyname.com\n\nThe current spec prevents sharing cookies amongst those servers. That\ndoes not seem terribly reasonable.\n\nYaron\n\n> -----Original Message-----\n> From:M. Hedlund [SMTP:hedlund@best.com]\n> Sent:Friday, March 21, 1997 8:02 PM\n> To:Dave Kristol\n> Cc:Yaron Goland; http-wg@cuckoo.hpl.hp.com\n> Subject:RE: Issues with the cookie draft\n> \n> \n> On Fri, 21 Mar 1997, Dave Kristol wrote:\n> > What do others think?  The idea is, if the user agent or,\n> > presumably, the origin server, receives a Set-Cookie (Cookie) header\n> that\n> > is non-conforming, the receiver *must* ignore the header.\n> \n> Since cookies are widely set by content providers through simple\n> server\n> extensions (as opposed to just by relatively few server implementors),\n> I\n> think this is a bad idea.  All sorts of non-RFC readers will be\n> setting\n> cookies.  Let's make it as easy on them as possible. I would want the\n> client to do its best to figure out the server's intent, while\n> adhering to\n> the spec. \n> \n> I don't think our current predicament is really caused by clients\n> being too\n> liberal in what they accept; it seems to have been caused by the lack\n> of a\n> specification in the original cookie proposal for what a browser\n> should do\n> with extra attributes in a set-cookie.  The RFC contains the line:\n> \n> > 4.3.1  Interpreting Set-Cookie2 [...]\n> > The user agent should ignore attribute-values pairs whose attribute\n> it\n> > does not recognize. \n> \n> ...which should solve the problem we have been addressing.  I don't\n> think\n> that just because we've had this problem, we should make the clients\n> require strict syntax.  What if two browsers have different\n> interpretations\n> of a conforming cookie?  If they both reject non-conforming cookies\n> and\n> disagree about what it means to conform, no one set-cookie syntax will\n> work, and we'll have to switch on User-agent -- please, let's not\n> create\n> that problem again! \n> \n> >   > 4.3.2 Rejecting Cookies (how far into the domain do you go):\n> >   > I appreciate that it was a long and drawn out debate but that is\n> not a\n> >   > sufficient rational for preventing perfectly reasonable\n> behavior. The\n> >   > decision to stop at one domain level is completely arbitrary. It\n> is no\n> >   > more and no less secure than 2 or infinite domain levels deep. I\n> do not\n> >   > feel that an arbitrary choice is a good enough reason to include\n> a\n> >   > requirement in a specification.\n> > \n> > It wasn't completely arbitrary.  The goal was to protect privacy,\n> and\n> > the rule in the spec makes it harder for cookies to \"leak\" to\n> servers\n> > far removed from their origin.\n> \n> I disagree that there was any arbitrariness involved on our part.  The\n> arbitrary factor is the designation of an organizational unit in a\n> domain\n> name.  If you want to change this, come up with a rule that\n> sufficiently\n> covers the following cases:\n> \n> .com.\n> .sun.com\n> .foo.sg\n> .demon.co.uk\n> .bar.com.au\n> .powells.pdx.or.us\n> \n> On Fri, 21 Mar 1997, Roy T. Fielding wrote:\n> > I was hoping for \"Set-Chips\" (just a small addition to the cookie)\n> > or \"Set-Brownie\" (that other thing we find at IETF meetings).\n> \n> Oi, what have you started.  ;)  How about \"Set-Baking-Soda\" -- that\n> required to make cookies fully baked? \n> \n> M. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-16360599"}, {"subject": "RE: new cookie draf", "content": "Cool. David, what do you think? We define PORT. If it is included then\nthe cookie may only be returned on the port it is received. If it is not\nincluded then the cookie may be returned on any port within the domain.\nYaron\n\n> -----Original Message-----\n> From:M. Hedlund [SMTP:hedlund@best.com]\n> Sent:Friday, March 21, 1997 7:27 PM\n> To:Yaron Goland\n> Cc:'Dave Kristol'; http-wg@cuckoo.hpl.hp.com\n> Subject:RE: new cookie draft\n> \n> \n> On Fri, 21 Mar 1997, Yaron Goland wrote:\n> > We can\n> > define an attribute \"PORT\", with no argument. If it is included in a\n> > cookie then the cookie may only be returned on the port it was\n> received\n> > on, this requirement applies to all domains. \n> \n> That sounds right.  \n> \n> M. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-16373668"}, {"subject": "Re: I-D ACTION:draft-ietf-http-hit-metering01.tx", "content": "Jeffrey Mogul wrote:\n\n> \n> The document still contains several notes labelled \"Design question\"\n> or \"Design note\", and a detailed Revision History (instead of change\n> bars).  Larry Masinter says that the final draft cannot include\n> these components, since they would not be appropriate in an RFC.\n\nAn \"Experimental\" RFC can contain design questions, but standards-track\n(Proposed Standard) is supposed to have resolved all known design\nquestions.\n\n\n\n", "id": "lists-010-16383391"}, {"subject": "RE: Issues with the cookie draf", "content": "On Sat, 22 Mar 1997, Yaron Goland wrote:\n> I'll come up with a rule to handle your cases as soon as you come up\n> with a rule to allow me to share cookies across:\n> \n> companyname.com\n> productname.companyname.com\n> version1.productname.companyname.com\n> version2.productname.companyname.com\n> version3.productname.companyname.com\n> \n> The current spec prevents sharing cookies amongst those servers. That\n> does not seem terribly reasonable.\n\nI agree that it would be desirable to allow this functionality, and I\nconcede that we as a group were not able to come up with such a rule (which\nI think Lou Montulli also raised as desirable in some cases).  My point\nabout arbitrariness was that we didn't find a way to determine what was a\ncompany/organization name versus what was a \"top\"-level domain -- in other\nwords, the rules that would satisfy the above cases would necessarily fail\nto satisfy the cases I listed.\n\nSince one domain-matching method didn't arise to cover both sets of cases,\nwe decided in favor of the more conservative method -- the one that at\nleast made a strong attempt to protect users from cookie broadcasting.  Let\nme rephrase my challenge: given that we (you and I, at least) seem to agree\nthat one organizational unit should have the ability to use cookies across\nits internal domains, can you propose a domain matching rule that allows\nthat feature _without_ creating a cookie-broadcast situation (where a\ncookie is available to servers outside of the organizational unit)? \n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-16391081"}, {"subject": "RE: new cookie draf", "content": "At 1:39 AM -0800 3/22/97, Yaron Goland wrote:\n>Cool. David, what do you think? We define PORT. If it is included then\n>the cookie may only be returned on the port it is received. If it is not\n>included then the cookie may be returned on any port within the domain.\n\nSounds reasonable to me.\n\nAt the risk of complexifying things, should Port perhaps take a\ncomma-separated list of ports to which the cookie can be sent, rather than\njust to the port from which it came?  That would provide a middle ground\nbetween one port and all.\n\nDave Kristol\n\n\n\n", "id": "lists-010-16400640"}, {"subject": "RE: new cookie draf", "content": "Sounds like an even better idea.\nYaron\n\n> -----Original Message-----\n> From:Dave Kristol [SMTP:dmk@bell-labs.com]\n> Sent:Saturday, March 22, 1997 12:49 PM\n> To:Yaron Goland; 'hedlund@best.com'\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:RE: new cookie draft\n> \n> At 1:39 AM -0800 3/22/97, Yaron Goland wrote:\n> >Cool. David, what do you think? We define PORT. If it is included\n> then\n> >the cookie may only be returned on the port it is received. If it is\n> not\n> >included then the cookie may be returned on any port within the\n> domain.\n> \n> Sounds reasonable to me.\n> \n> At the risk of complexifying things, should Port perhaps take a\n> comma-separated list of ports to which the cookie can be sent, rather\n> than\n> just to the port from which it came?  That would provide a middle\n> ground\n> between one port and all.\n> \n> Dave Kristol\n> \n\n\n\n", "id": "lists-010-16408810"}, {"subject": "RE: Issues with the cookie draf", "content": "We all agree that the spec prevents completely legitimate behavior. Thus\ndemonstrating there is a flaw in the spec. My understanding of IETF\nprocedure is that the onus is now on the spec's editor to either fix the\nflaw or remove the offending section. Is my understanding inaccurate?\n\nBTW, in the near future it is very likely that control over the top\nlevel domains will be opened to competition. Thus, in theory at least,\nthe foobar company could purchase a top level domain name of \"foobar\".\nThus their site would be foobar,www.foobar, or what have you. This spec\nwill now prevent them from being able to share cookies across www.foobar\nand foobar.\n\nYaron\n\nPS Anyone who says \"Wait a minute, that won't be allowed, you will\nalways have to be at least two levels\" doesn't appreciate the\nunbelievable worth of having your own top level domain name. Can you\nimagine how much a company would be willing to pay to allow people to\njust type in their company name on a command line and get their web\nsite?\n\n> -----Original Message-----\n> From:M. Hedlund [SMTP:hedlund@best.com]\n> Sent:Saturday, March 22, 1997 8:15 AM\n> To:Yaron Goland\n> Cc:Dave Kristol; http-wg@cuckoo.hpl.hp.com\n> Subject:RE: Issues with the cookie draft\n> \n> \n> On Sat, 22 Mar 1997, Yaron Goland wrote:\n> > I'll come up with a rule to handle your cases as soon as you come up\n> > with a rule to allow me to share cookies across:\n> > \n> > companyname.com\n> > productname.companyname.com\n> > version1.productname.companyname.com\n> > version2.productname.companyname.com\n> > version3.productname.companyname.com\n> > \n> > The current spec prevents sharing cookies amongst those servers.\n> That\n> > does not seem terribly reasonable.\n> \n> I agree that it would be desirable to allow this functionality, and I\n> concede that we as a group were not able to come up with such a rule\n> (which\n> I think Lou Montulli also raised as desirable in some cases).  My\n> point\n> about arbitrariness was that we didn't find a way to determine what\n> was a\n> company/organization name versus what was a \"top\"-level domain -- in\n> other\n> words, the rules that would satisfy the above cases would necessarily\n> fail\n> to satisfy the cases I listed.\n> \n> Since one domain-matching method didn't arise to cover both sets of\n> cases,\n> we decided in favor of the more conservative method -- the one that at\n> least made a strong attempt to protect users from cookie broadcasting.\n> Let\n> me rephrase my challenge: given that we (you and I, at least) seem to\n> agree\n> that one organizational unit should have the ability to use cookies\n> across\n> its internal domains, can you propose a domain matching rule that\n> allows\n> that feature _without_ creating a cookie-broadcast situation (where a\n> cookie is available to servers outside of the organizational unit)? \n> \n> M. Hedlund <hedlund@best.com>\n> \n> \n\n\n\n", "id": "lists-010-16418361"}, {"subject": "RE: Issues with the cookie draf", "content": "Yaron Goland writes:\n > \n > PS Anyone who says \"Wait a minute, that won't be allowed, you will\n > always have to be at least two levels\" doesn't appreciate the\n > unbelievable worth of having your own top level domain name. Can you\n > imagine how much a company would be willing to pay to allow people to\n > just type in their company name on a command line and get their web\n > site?\n > \n\nThe Netscape browser already does that, after a fashion.  Enter \"microsoft\"\nin the URL window and it will interpret that as \"http://www.microsoft.com\".\n\n(this doesn't mean I disagree with what you said).\n\n--Shel\n\n\n\n", "id": "lists-010-16430699"}, {"subject": "RE: Issues with the cookie draf", "content": "On Sat, 22 Mar 1997, Yaron Goland wrote:\n> We all agree that the spec prevents completely legitimate behavior. Thus\n> demonstrating there is a flaw in the spec. \n\nNo, at least two of us agree that the spec fails to enable desirable\nbehavior.  That doesn't mean there's a flaw in the spec.  In this case, it\nmeans that no standard exists for determining the organizational unit in a\ndomain name -- a prerequisite, as far as I can see, for the behavior you\nwant.  If you want to point fingers, point them at the domain name\nstandard.  The cookie spec does the best it can with the information it is\ngiven.  If you disagree, propose an improvement -- which removing 'domain'\nis not.\n\nWith regards to private top-level domains, we can crumble that cookie when\nwe come to it (if you'll forgive me).  I agree that the situation is just\ngoing to get worse as we start litigating the nature of domain name\nregistries.  However, I have yet to hear how you intend to improve the spec\nin light of your predictions.  Do you really think removing the domain\nrestriction altogether improves the spec?  I would argue that doing so\nwould _create_ a serious flaw where none exists today.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-16439257"}, {"subject": "RE: Issues with the cookie draf", "content": "The domain restriction is not a protection of privacy, it is a\nprohibition against the right of companies to structure their Internet\nuse as they see fit. The free market will do a better job of protecting\nuser's rights then a domain name restriction. If someone is abusing\ncookies, let the press scream it out. If we can implement technical\nprotections to prevent the abuse of cookies, then let us do so. But the\ndomain restriction provides no real protection against the abuse of\ncookies while preventing the legitimate behavior of companies. \n\nFor example, a company which has a different second tier website for\neach of its products is now prevented from sharing cookies between those\nsites. There is absolutely no reason to prevent this behavior. You are\ntrying to establish a relationship between domain names and\norganizational responsibility. However the domain name system was not\nset up to provide this connection, therefore relying upon it is\nunreasonable. The domain solution provides no protection against the\nunauthorized sharing of data, it only makes it slightly inconvenient,\nbut it does prevent legitimate activity. I would argue that the cure is\nworse than the disease and the domain restriction should be removed. I\nam not arguing that the domain attribute should be removed, only the\nrestriction on what cookie servers may put in it.\n\nFurthermore, putting in place a solution we know will break, in the case\nof coming use of top level domains, is yet another reason to remove this\nsection of spec.\n\nIf this faulty behavior can not be remedied then the spec should not be\nallowed to move on in the standards process.\n\nYaron\n\n> -----Original Message-----\n> From:M. Hedlund [SMTP:hedlund@best.com]\n> Sent:Saturday, March 22, 1997 5:37 PM\n> To:Yaron Goland\n> Cc:Dave Kristol; http-wg@cuckoo.hpl.hp.com\n> Subject:RE: Issues with the cookie draft\n> \n> \n> On Sat, 22 Mar 1997, Yaron Goland wrote:\n> > We all agree that the spec prevents completely legitimate behavior.\n> Thus\n> > demonstrating there is a flaw in the spec. \n> \n> No, at least two of us agree that the spec fails to enable desirable\n> behavior.  That doesn't mean there's a flaw in the spec.  In this\n> case, it\n> means that no standard exists for determining the organizational unit\n> in a\n> domain name -- a prerequisite, as far as I can see, for the behavior\n> you\n> want.  If you want to point fingers, point them at the domain name\n> standard.  The cookie spec does the best it can with the information\n> it is\n> given.  If you disagree, propose an improvement -- which removing\n> 'domain'\n> is not.\n> \n> With regards to private top-level domains, we can crumble that cookie\n> when\n> we come to it (if you'll forgive me).  I agree that the situation is\n> just\n> going to get worse as we start litigating the nature of domain name\n> registries.  However, I have yet to hear how you intend to improve the\n> spec\n> in light of your predictions.  Do you really think removing the domain\n> restriction altogether improves the spec?  I would argue that doing so\n> would _create_ a serious flaw where none exists today.\n> \n> M. Hedlund <hedlund@best.com>\n> \n> \n> \n> \n\n\n\n", "id": "lists-010-16448414"}, {"subject": "Re: Issues with the cookie draf", "content": "The issue of cookie sharing and domains is closed in this\nworking group, UNTIL AND UNLESS there are volunteers who are\nwilling to work together and submit a counter-proposal as\nan Internet Draft. The counter-proposal must explain the\nproposal, and, in its Security Considerations section, explain\nhow the deals with the issue of user privacy.\n\nUNTIL OR UNLESS there is such an internet draft, any further\nmessages on this topic are out of order. Please stop.\n\nLarry\n\n\n\n", "id": "lists-010-16460854"}, {"subject": "Transparent content negotiation pointers &amp; statu", "content": "This is to announce the availability of HTML versions of the\ntransparent content negotiation documents, which will be discussed at\nthe HTTP-WG sessions of the upcoming IETF in Memphis:\n\n     1. draft-ietf-http-negotiation-01.txt\n\n        `Transparent Content Negotiation in HTTP'\n\n        Defines the core mechanism. Standards track.\n\n       ABSTRACT\n\n        HTTP allows web site authors to put multiple versions of the\n        same information under a single URL.  Transparent content\n        negotiation is a mechanism, layered on top of HTTP, for\n        automatically selecting the best version when the URL is\n        accessed.  This enables the smooth deployment of new web data\n        formats and markup tags.\n\n\n     2. draft-ietf-http-rvsa-v10-01.txt\n\n        `HTTP Remote Variant Selection Algorithm -- RVSA/1.0'\n\n        Defines the remote variant selection algorithm version 1.0.\n        Standards track.\n\n       ABSTRACT\n\n        [...] A remote variant\n        selection algorithm can be used to speed up the transparent\n        negotiation process. This document defines the remote variant\n        selection algorithm with the version number 1.0.\n\n\nThe txt version of the first document is already 14 days old, the txt\nversion of the second one is new, but only has very small revisions.\nAll documents can be found at the usual place:\n\n     http://gewis.win.tue.nl/~koen/conneg/\n\n\nFor those not deep into content negotiation, here is some background\ninfo in Q&A form.\n\nQ: What is the status of these documents?\n\nA: They are not discussion drafts.  They are finished, complete specs of\na protocol on top of HTTP/1.x.  If you don't know HTTP/1.x, you may have\na hard time reading them, especially the caching sections.\n\nQ: Should I read these documents?\n\nA: Yes, if:\n   1. you are a http-wg member\n   2. or a browser/server/proxy implementer,\nand\n   a. you want to get your technical comments in\n   b. or want to let us know whether you consider implementation.\n\nWe hope to do a last call on (a revision of) the main document \nsoon after Memphis.\n\nQ: Why are these documents so long?\n\nA: They are only 1/3th of the length of the HTTP/1.1 specification.\nThey could have been 1/6th of HTTP/1.1 if all examples were left out.\n\nQ: What is transparent content negotiation anyway?\n\nA: HTTP allows web site authors to put multiple versions of the same\ninformation under a single URL.  Content negotiation is the process of\nchoosing among them.  Transparent content negotiation (TCN) is the\nfirst HTTP content negotiation mechanism which scales well enough to\nbe useful. This enables the smooth deployment of new web data formats\nand markup tags.\n\nQ: What does it negotiate on?\n\nA: TCN only negotiates on _content_, it does not negotiate on\nprotocols or services.  The PEP protocol does that.\n\nTCN negotiates on three of the dimensions which were formalised in\nthe HTTP/1.1 spec:\n\n     1. Media type (MIME type)\n     2. Charset\n     3. Language\n\nNote that the http-wg is _not_ interested in revising the\nspecifications of the above three dimensions in HTTP/1.1.\n\nIf you want something new or different, TCN adds a fourth dimension for\nyou:\n\n     4. Features\n\nFeature negotiation intends to provide for all areas of negotiation\nnot covered by the type, charset, and language dimensions.  Examples\nare negotiation on\n\n       * HTML extensions\n       * Extensions of other media types\n       * Color capabilities of the user agent\n       * Screen size\n       * Output medium (screen, paper, ...)\n       * Preference for speed vs. preference for graphical detail\n\nThe feature negotiation framework is the principal means by which\ntransparent negotiation offers extensibility; a new dimension of\nnegotiation (really a sub-dimension of the feature dimension) can be\nadded without the need for a new standards effort by the simple\nregistration of a `feature tag'.\n\nQ: How do I register a feature tag?\n\nA: We are still working on it.  There is a discussion draft:\ndraft-ietf-http-feature-reg-00.txt, but we want to finish the main\nmechanism before we start discussing registration.  But, as with the\nrevised MIME type registration process, registration is basically open\nto everybody, and acceptance of a registered dimension will be\ndetermined by market forces, not the IETF.\n\nQ: How can feature negotiation do all this and still scale?\n\nA: It is based on some stuff which was developed by mathematicians in\nthe 1920s.  Don't worry, you won't have to be a mathematician to use\nfeature negotiation.  But if you want to understand *why* it works,\nsome background in formal logic will be helpful.\n\nQ: What if feature negotiation won't work for what I have in mind?\n\nA: TCN was built with extensibility in mind, so there would be no\nproblem in extending it at some point in the future.\n\nQ: Why is TCN important?\n\nA: There are a number of reasons:\n\n  1. It is good for internationalisation\n\n  2. It eliminates the need to choose between a `state of the art\n     multimedia homepage' and one which can be viewed by all web\n     users: simply make both and negotiate between them\n\n  3. It is good for spreading the web to other media besides 256 color\n     high-res monitors\n\n  4. Feature negotiation eliminates the need for cache-unfriendly and\n     error-prone user-agent based negotiation\n\n  5. It enables the smooth deployment of new web data formats and\n     markup tags, which is important because\n\n    a. New formats are more fun\n\n    b. New formats are generally smaller, so we can expect bandwidth\n       savings.\n\n       Some figures from a 1995 proxy cache study I did:\n\n                                           media type of object\n                                      ------------------------------\n                                      text,  gif  jpeg other (gif\n                                      html                   +jpeg)\n\n  share in traffic on off-campus link: 30%   22%   20%  28%  (42%)\n\n  campus proxy cache efficiency      : 48%   25%   14%   8%  (20%)\n\n       Note that graphics (gif+jpeg) take a large share (42%) of the\n       total backbone traffic, and that they do not cache well (20%\n       efficiency).  So if TCN allows the smooth transition from\n       gif+jpeg to something smaller (png? CSS? vector graphics?), we\n       can expect considerable savings.  The same is true for other\n       formats.\n\n  6. `Content negotiation was planned from the early days as a\n     flexibility point which separated HTTP and HTML, and would allow\n     evolution of the web in ways we do not yet envision.' <--- Actual\n     TimBL quote\n\nKoen.\n\n\n\n", "id": "lists-010-16467859"}, {"subject": "Re: I-D ACTION:draft-ietf-http-hit-metering01.tx", "content": "Jeffrey Mogul:\n[...]\n>Also note that there have been numerous changes to the document;\n>I think I managed to cover all of the normative changes in the\n>Revision History section.\n[...]\n\nPublic service announcement:\n\nI find that changebars speed up my (re)reading, so I applied my usual tools\nto make them.  I have made the result available at\n\nhttp://gewis.win.tue.nl/~koen/draft-ietf-http-hit-metering-01.txt.cb\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-16481605"}, {"subject": "Re: 305 Use prox", "content": "Ari Luotonen:\n[...]\n>Redundant:\n>\n>305 is by no means the only way the client can be instructed to go\n>through a proxy.  A regular 301/302 redirect could be used, pointing\n>to a \"reverse\" proxy.  A \"reverse proxy\" appears as a regular server,\n>but is really a proxy when it comes to its content retrieval and\n>management.  As far as the client knows, it's an origin server.\n>\n>This has benefits: the real origin server is completele hidden, the\n>users will never have to know about its existence.\n\nI'm confused.  Does the reverse proxy have another hostname?  (I think it\nmust have one for 301/302 to work.) And if so, how can it be completely\ntransparent to the users?\n\nWould you ever have two reverse proxies for one site?  If so, and if they\nhave different hostnames, this would reduce the cache efficiency for\ndownstream caches.\n\nI agree that we have to be very careful when speccing 305.  Would you like\nall your requests to be routed through wwwproxy.doubleclick.com?\n\n>Ari Luotonen* * * Opinions my own, not Netscape's * * *\n\nKoen.\n\n\n\n", "id": "lists-010-16489504"}, {"subject": "Re: 305 Use prox", "content": "In message <199703202122.NAA03558@step.mcom.com>, Ari Luotonen writes:\n>Let me elaborate on these issues one by one.\n>\n>Security hole:\n>\n>If 305 is allowed by origin servers, intermediate HTTP/1.1 proxies\n>that do not understand 305's hop-by-hope requirement will let it\n>through (I assume at this point it may be too late to impose the\n>hop-by-hop requirement for 305, and expect it to be respected by all\n>implementations).  If a client gets a 305 sent by an evil origin\n>server through a proxy, it will override the client's proxy settings,\n>because the client thinks the proxy redirected it to another proxy.\n\nI don't know of any clients that have implemented 305.\nIs it already in the Navigator (or other browser)?\n\n>This security hole can completely cripple people behind a firewall\n>where the only way out is through the corporate proxy.  If some evil\n>site sends a 305 that goes through to the client, the client will from\n>then on try to use a proxy that is inaccessible for it (remember, it\n>needs to go thru the company firewall proxy to get out).\n\nAnd the company firewall let a 305 response go through?  And nobody noticed\nbeing \"crippled\"? And it took them how long to update their own proxy?\nThis does not fit my definition of a security hole.  It fits my definition\nof a note to add to the security section, but that's all.  In any case,\nyou would have to implement protection from this \"hole\" within the\nuser agent, and I see no reason why it can't be configurable.\n\n>Redundant:\n>\n>305 is by no means the only way the client can be instructed to go\n>through a proxy.  A regular 301/302 redirect could be used, pointing\n>to a \"reverse\" proxy.  A \"reverse proxy\" appears as a regular server,\n>but is really a proxy when it comes to its content retrieval and\n>management.  As far as the client knows, it's an origin server.\n\nIt's a gateway. Please strangle the person who came up with \"reverse proxy.\"\n\n>This has benefits: the real origin server is completele hidden, the\n>users will never have to know about its existence.  Also, it keeps the\n>design cleaner where origin servers are origin servers, regarless the\n>implementation style (regular, replicated, reverse proxy); and proxies\n>remain a part of the transport mechanism which is controlled by a flow\n>from client outward, not by origin servers (because they have no\n>knowledge to control the proxy settings of clients).\n\nIt also doubles the number of network requests (the 301/2 isn't free),\nrelies on URL mangling to make requests, and completely breaks cookies\nif the gateway covers multiple domains.  Such a solution only works on\nthe individual corporate scale, whereas we are talking about proxies\ncovering Universities or regional/national networks that exist to prevent\nan unnecessary network request from going outside their own network.\n\nKeep in mind that the client may think it is talking to the origin server,\nbut the 305 response may actually be coming from a network firewall.\n\n>Mudding the architectural design:\n>\n>See above paragraph.\n\nThe architectural design of HTTP/1.1 is described in the introduction\nof RFC 2068.  It includes understanding of proxies and gateways.\n\n>Overhead on clients:\n>\n>Basically, 305 generated by origin servers implies having to maintain\n>a proxy setting for every URL.  This may be tolerable in clients, but\n>it's worse on proxies which get a large amount of requests.  Scope\n>rules complicate things worse.\n\nThey are never forced to maintain that information, but doing so for the\nmost-recently-accessed servers will be to their own benefit.\n\n>The reasoning for scope rules is primarily to plug the security hole.\n>\n>I would like to get rid of the automatic scope rule restrictions\n>altogether, which entails not allowing origin servers not to be\n>allowed to send 305's at all, and use a combination of regular\n>redirects and reverse proxies instead.\n>\n>By virtue of \"trusted proxies\" you can allow global proxy redirects\n>(if scope is not present), which allows *all* requests earlier\n>targeted for a given proxy, to be completely diverted to another.\n>With auto-scope-rule-restrictions this would be next to useless,\n>because every new site (at least) would require a new hop by the\n>original proxy, just to get a new proxy redirect.\n\nThe reason for the scope rules is to broaden it from the current\ndefinition of 305 applying to one and only one URL.  I see no reason\nwhy scope=\"http\" should be impossible, provided the proxy is trusted,\nso I see no basis for this objection.\n\n>Conclusion:\n>\n>I would definitely like to have a proxy redirect that can be generated\n>by a proxy only.  If people can come up with examples where 305 from\n>origin servers is the only way to accomplish something that couldn't\n>be done with regular redirect combined with a reverse proxy, then\n>maybe we could have two separate proxy redirections:\n>\n>    305 Use proxy-- use proxy, generated by origin servers\n>    306 Proxy redirect-- use another proxy instead, gen'd by proxies only\n\nThat would serve no useful purpose -- old proxy implementations will be\nupdated long before we have any large-scale use of 305, and a user agent\nis fully capable of querying the user before obeying a 305 redirect\nreceived from an origin server, if such a thing is a concern.  The user\nagent does, after all, know whether or not it is talking to a proxy.\nNone of this stops people from using gateways as well.\n\nIf a new client is installed behind a network firewall and improperly\nconfigured to make direct network requests, then the firewall can detect\nthose requests and immediately reconfigure the client (with the user's\nacquiescence) to use the appropriate proxy, if and only if an HTTP\norigin server is allowed to send 305 redirects.  This may in fact be\nthe only function of said \"firewall\".\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-16498049"}, {"subject": "Some issues for the lis", "content": "Hi Jim,\n\nHere are some issues for the list at\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/ which you seem to\nhave missed.\n\n1. implied LWS rule does not talk about LWS as delimiter between\ntokens, but as some 1.1 headers use LWS instead of tspecials as\ndelimiters between tokens.\n\nreference: http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q1/0131.html\n\n2. is a CRLF in a quoted-string legal, and what is the relation to\nheader continuation?\n\nreference: http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q1/0324.html\n\n3. confusion about accept-encoding language\n\nreference: http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q1/0176.html\n\n4. The list entry\n\nLANGUAGE-TAG         Language tag matching needs\n                     to be added. \n\nlinks to a discussion on the meaning of q=0.0, which is an issue\nseparate from the LANGUAGE-TAG issue.  The discussion of the issue\n`Language tag matching needs to be added' happens later on\n(http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q1/0052.html) in\nthe same thread.\n\n\nNote addressed mostly to Larry:\n\nI see that the link `The process used to close out the issues for\nProposed Standard' still leads to a `not found' error.  I find this\nworrying.  I had hoped that some subgroup would be set up to generate\nproposed resolutions before Memphis, so that people would have the\nchance to check these resolutions against the discussions in the\nmailing list archive beforehand.  Frankly, I can't see how we are\ngoing to reach closure on even a fraction of >50 issues in Memphis, if\nall we have to start with is printouts of the current issue list and\nour incomplete memories of past threads.\n\nKoen.\n\n\n\n", "id": "lists-010-16512009"}, {"subject": "RE: new cookie draf", "content": "On Sat, 22 Mar 1997, Yaron Goland wrote:\n\n> Actually I suggested the exact opposite. If PORT is NULL then the cookie\n> may be sent on any port. It is only if a port is specified that there is\n> a restriction.\n\nI think perhaps my syntax wasn't clear.  My intent was that three cases\nexist:\n\n a)   PORT attribute not specified, the attribute is NULL\n\n      There are no restrictions\n\n b)   PORT attribute specified but with no value, the value NULL\n \n      Only the source PORT may receive the cookie\n\n c)   PORT attribute with a value in which case the value is a comma\n      delimited list of valid ports.  (e.g, PORT=\"80,443\")\n\n      Only the listed ports may receive the cookie\n\nI think (a) and (b) are your proposal and I added (c) to allow more\nprecise control.\n\nDave\n\n> Yaron\n> \n> > -----Original Message-----\n> > From:David W. Morris [SMTP:dwm@xpasc.com]\n> > Sent:Friday, March 21, 1997 10:21 PM\n> > To:http working group\n> > Subject:RE: new cookie draft\n> > \n> > \n> > \n> > On Fri, 21 Mar 1997, M. Hedlund wrote:\n> > \n> > > On Fri, 21 Mar 1997, Yaron Goland wrote:\n> > > > We can\n> > > > define an attribute \"PORT\", with no argument. If it is included in\n> > a\n> > > > cookie then the cookie may only be returned on the port it was\n> > received\n> > > > on, this requirement applies to all domains. \n> > > \n> > > That sounds right.  \n> > \n> > An alternative ... a PORT attribute whose value is a comman delimited\n> > list\n> > of ports on which the cookie may be returned. If the PORT attribute is\n> > omitted, any port is valid.  If the value of the PORT attribute is\n> > NULL,\n> > then as Yaron suggested, it may only be sent to the port it was\n> > received\n> > from. This allows it to be very tight while not excluding a value like\n> >  \n> >                 port=\"80,443\"\n> > \n> > which would allow sharing beteen the default HTTP and HTTPS ports.\n> > \n> > Note: While I am proposing a mechanism to resolve an issue, I don't\n> > share\n> > the concern so I will be happy with any solution which allows sharing\n> > between ports.\n> > \n> > Dave Morris\n> \n\n\n\n", "id": "lists-010-16521246"}, {"subject": "Re: (ACCEPT*) Last call on draft text for Accept header", "content": "Tim Greenwood:\n>\n>Koen Holtman wrote\n>\n>> I believe your proposal for simplification is based on a misreading of\n>> the syntax definition of language tags.  The current rule needs to be\n>> this complicated because there can be more than one subtag.\n>> \n>> The matching rule currently defined will allow the range \"i-sami\" to\n>> match the tag \"i-sami-da\".  Your proposed simplification will not\n>> allow this.\n>\n>The proposal was intended not as a simplification, but as a \n>clarification. Your comment is correct, my proposal would have \n>excluded a match that should be allowed. The text in 10.4 should \n>still be rewritten to use the syntax defined in 3.10. The current \n>text uses the term 'a prefix' which is not defined. \n\nAh, so _that_ is the problem.  I assumed that 'prefix' was standard\ncomputing terminology, but apparantly it is not.  (Prefix means\n\"initial part of a string\".)  I'll see if I can find an alternative\nwording.\n\n>It could be \n>interpreted to allow a match of a language range to any but the last \n>subtag of a language-tag. Thus \n>\n>Accept-Language:cy     (Welsh)\n>\n>could match el-cy-x   (Greek in Cyprus with some other subtag).\n>\n>-------------------------------------\n>Tim Greenwood        Open Market Inc\n>617 679 0320         greenwd@openmarket.com\n\nKoen.\n\n\n\n", "id": "lists-010-1652360"}, {"subject": "RE: new cookie draf", "content": "On Sat, 22 Mar 1997, Dave Kristol wrote:\n\n> At 1:39 AM -0800 3/22/97, Yaron Goland wrote:\n> >Cool. David, what do you think? We define PORT. If it is included then\n> >the cookie may only be returned on the port it is received. If it is not\n> >included then the cookie may be returned on any port within the domain.\n> \n> Sounds reasonable to me.\n> \n> At the risk of complexifying things, should Port perhaps take a\n> comma-separated list of ports to which the cookie can be sent, rather than\n> just to the port from which it came?  That would provide a middle ground\n> between one port and all.\n\nThat was more or less my proposal. (I tried to describe a tri-state\nsolution)  Given that ports are and issue, this seems like a reasonable\nsolution.\n\nDave Morris\n\n\n\n", "id": "lists-010-16531097"}, {"subject": "Re: 305 Use prox", "content": "On Sun, 23 Mar 1997, Roy T. Fielding wrote:\n> In message <199703202122.NAA03558@step.mcom.com>, Ari Luotonen writes:\n> >\n> >If 305 is allowed by origin servers, intermediate HTTP/1.1 proxies\n> >that do not understand 305's hop-by-hope requirement will let it\n> >through (I assume at this point it may be too late to impose the\n> >hop-by-hop requirement for 305, and expect it to be respected by all\n> >implementations).  If a client gets a 305 sent by an evil origin\n> >server through a proxy, it will override the client's proxy settings,\n> >because the client thinks the proxy redirected it to another proxy.\n> \n> I don't know of any clients that have implemented 305.\n> Is it already in the Navigator (or other browser)?\n\nLynx has implemented 305 for a while.  It only accepts it from an origin\nserver (i.e. if not already using a proxy) and on a request-by-request\nbasis (like 302; no \"proxy settings\" are overridden except for the\ncurrent request).\n\n    Klaus\n\n\n\n", "id": "lists-010-16539896"}, {"subject": "RE: new cookie draf", "content": "Do we want to tight B up so that you always have to specify a port\nnumber? I would be suspicious of a cookie which doesn't know its own\nport and so has to ask the client to record it. Still, I like the\nproposal.\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Sunday, March 23, 1997 11:07 AM\n> To:Yaron Goland\n> Cc:http working group\n> Subject:RE: new cookie draft\n> \n> \n> \n> On Sat, 22 Mar 1997, Yaron Goland wrote:\n> \n> > Actually I suggested the exact opposite. If PORT is NULL then the\n> cookie\n> > may be sent on any port. It is only if a port is specified that\n> there is\n> > a restriction.\n> \n> I think perhaps my syntax wasn't clear.  My intent was that three\n> cases\n> exist:\n> \n>  a)   PORT attribute not specified, the attribute is NULL\n> \n>       There are no restrictions\n> \n>  b)   PORT attribute specified but with no value, the value NULL\n>  \n>       Only the source PORT may receive the cookie\n> \n>  c)   PORT attribute with a value in which case the value is a comma\n>       delimited list of valid ports.  (e.g, PORT=\"80,443\")\n> \n>       Only the listed ports may receive the cookie\n> \n> I think (a) and (b) are your proposal and I added (c) to allow more\n> precise control.\n> \n> Dave\n> \n> > Yaron\n> > \n> > > -----Original Message-----\n> > > From:David W. Morris [SMTP:dwm@xpasc.com]\n> > > Sent:Friday, March 21, 1997 10:21 PM\n> > > To:http working group\n> > > Subject:RE: new cookie draft\n> > > \n> > > \n> > > \n> > > On Fri, 21 Mar 1997, M. Hedlund wrote:\n> > > \n> > > > On Fri, 21 Mar 1997, Yaron Goland wrote:\n> > > > > We can\n> > > > > define an attribute \"PORT\", with no argument. If it is\n> included in\n> > > a\n> > > > > cookie then the cookie may only be returned on the port it was\n> > > received\n> > > > > on, this requirement applies to all domains. \n> > > > \n> > > > That sounds right.  \n> > > \n> > > An alternative ... a PORT attribute whose value is a comman\n> delimited\n> > > list\n> > > of ports on which the cookie may be returned. If the PORT\n> attribute is\n> > > omitted, any port is valid.  If the value of the PORT attribute is\n> > > NULL,\n> > > then as Yaron suggested, it may only be sent to the port it was\n> > > received\n> > > from. This allows it to be very tight while not excluding a value\n> like\n> > >  \n> > >                 port=\"80,443\"\n> > > \n> > > which would allow sharing beteen the default HTTP and HTTPS ports.\n> > > \n> > > Note: While I am proposing a mechanism to resolve an issue, I\n> don't\n> > > share\n> > > the concern so I will be happy with any solution which allows\n> sharing\n> > > between ports.\n> > > \n> > > Dave Morris\n> > \n\n\n\n", "id": "lists-010-16548360"}, {"subject": "RE: new cookie draf", "content": "On Sun, 23 Mar 1997, Yaron Goland wrote:\n\n> Do we want to tight B up so that you always have to specify a port\n> number? I would be suspicious of a cookie which doesn't know its own\n> port and so has to ask the client to record it. Still, I like the\n> proposal.\n\nThe client always has the port available. I think there will be quite a\nfew cases where the CGI program fabricating the cookie doesn't know the\nserver's port and possibly can't get it easily. So I think PORT w/o a\nspecific number has value.\n\nDave\n\n> \n> > -----Original Message-----\n> > From:David W. Morris [SMTP:dwm@xpasc.com]\n> > Sent:Sunday, March 23, 1997 11:07 AM\n> > To:Yaron Goland\n> > Cc:http working group\n> > Subject:RE: new cookie draft\n> > \n> > \n> > \n> > On Sat, 22 Mar 1997, Yaron Goland wrote:\n> > \n> > > Actually I suggested the exact opposite. If PORT is NULL then the\n> > cookie\n> > > may be sent on any port. It is only if a port is specified that\n> > there is\n> > > a restriction.\n> > \n> > I think perhaps my syntax wasn't clear.  My intent was that three\n> > cases\n> > exist:\n> > \n> >  a)   PORT attribute not specified, the attribute is NULL\n> > \n> >       There are no restrictions\n> > \n> >  b)   PORT attribute specified but with no value, the value NULL\n> >  \n> >       Only the source PORT may receive the cookie\n> > \n> >  c)   PORT attribute with a value in which case the value is a comma\n> >       delimited list of valid ports.  (e.g, PORT=\"80,443\")\n> > \n> >       Only the listed ports may receive the cookie\n> > \n> > I think (a) and (b) are your proposal and I added (c) to allow more\n> > precise control.\n> > \n> > Dave\n> > \n> > > Yaron\n> > > \n> > > > -----Original Message-----\n> > > > From:David W. Morris [SMTP:dwm@xpasc.com]\n> > > > Sent:Friday, March 21, 1997 10:21 PM\n> > > > To:http working group\n> > > > Subject:RE: new cookie draft\n> > > > \n> > > > \n> > > > \n> > > > On Fri, 21 Mar 1997, M. Hedlund wrote:\n> > > > \n> > > > > On Fri, 21 Mar 1997, Yaron Goland wrote:\n> > > > > > We can\n> > > > > > define an attribute \"PORT\", with no argument. If it is\n> > included in\n> > > > a\n> > > > > > cookie then the cookie may only be returned on the port it was\n> > > > received\n> > > > > > on, this requirement applies to all domains. \n> > > > > \n> > > > > That sounds right.  \n> > > > \n> > > > An alternative ... a PORT attribute whose value is a comman\n> > delimited\n> > > > list\n> > > > of ports on which the cookie may be returned. If the PORT\n> > attribute is\n> > > > omitted, any port is valid.  If the value of the PORT attribute is\n> > > > NULL,\n> > > > then as Yaron suggested, it may only be sent to the port it was\n> > > > received\n> > > > from. This allows it to be very tight while not excluding a value\n> > like\n> > > >  \n> > > >                 port=\"80,443\"\n> > > > \n> > > > which would allow sharing beteen the default HTTP and HTTPS ports.\n> > > > \n> > > > Note: While I am proposing a mechanism to resolve an issue, I\n> > don't\n> > > > share\n> > > > the concern so I will be happy with any solution which allows\n> > sharing\n> > > > between ports.\n> > > > \n> > > > Dave Morris\n> > > \n> \n\n\n\n", "id": "lists-010-16559524"}, {"subject": "RE: 305 Use prox", "content": "I mean a way for the proxy to tell the client to go direct...I'm not \nas interested in the origin server telling the client to reconnect \n directly instead of using a proxy.  Basically this would be for \nenvironments where the proxy is not straddling a firewall: a proxy \ncould selectively cache origin servers, or send a client directly to \nan origin server that for some reason was known to be not cacheable.\n\nThanks,\nKip\n\n\n-----Original Message-----\nFrom:Josh Cohen [SMTP:josh@netscape.com]\nSent:Thursday, March 20, 1997 11:22 AM\nTo:Compton, Kip\nCc:josh@netscape.com; http-wg@cuckoo.hpl.hp.com\nSubject:Re: 305 Use proxy\n\n> How about a way to indicate that the client should access the origin \n> server directly, i.e., should not use a proxy at all?\n\nWhat exactly do you mean?\nDo you mean an origin server redirecting a client, which\nconnected via a proxy, to try again direct?\n\nOr do you mean a proxy telling a client to go direct?\n\n-----------------------------------------------------------------------  \n------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department                \"My opinions, not \nNetscape's\"\nServer Engineering\njosh@netscape.com \n                      http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------  \n------\n\n\n\n", "id": "lists-010-16571214"}, {"subject": "Re: 305 Use prox", "content": "> I mean a way for the proxy to tell the client to go direct...I'm not \n> as interested in the origin server telling the client to reconnect \n>  directly instead of using a proxy.  Basically this would be for \n> environments where the proxy is not straddling a firewall: a proxy \n> could selectively cache origin servers, or send a client directly to \n> an origin server that for some reason was known to be not cacheable.\n>\nHmm.. That hasnt really been considered as far as I know.\nOverall though, I dont see the need for that.\n\n1. If the client is connected to the proxy and has given the request,\n   whats the disadvantage of completing the request via the proxy?\n\n2. At that same point, once the proxy has determined that the \nresource isnt cacheable, its already begun receiving the content\nfrom the origin server. (typically, since the client did a GET, not\na HEAD )  Since this is a per resource or per URL thing, it would\nend up creating more connections than if the client just completed\nwith the proxy.  \n\nIf the proxy intends to tell the client to go direct for a wide\nscope, ie the whole site http://www.foo.com/ then that could be\ntaken care of by the browser's proxy config.\nWhile this can be tedious for a user to configure their browser,\nthe Proxy Autoconfig settings can do this.\nFYI: Ive heard that Microsoft intends to support the Proxy Autoconfig\nas well..\n\n\n \n> Thanks,\n> Kip\n> \n>\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department            \"Mighty Morphin' Proxy\nRanger\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n", "id": "lists-010-16581361"}, {"subject": "RE: new cookie draf", "content": "Good argument. So, it sounds like we have this one sewn up.\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Sunday, March 23, 1997 1:22 PM\n> To:Yaron Goland\n> Cc:http working group\n> Subject:RE: new cookie draft\n> \n> \n> \n> On Sun, 23 Mar 1997, Yaron Goland wrote:\n> \n> > Do we want to tight B up so that you always have to specify a port\n> > number? I would be suspicious of a cookie which doesn't know its own\n> > port and so has to ask the client to record it. Still, I like the\n> > proposal.\n> \n> The client always has the port available. I think there will be quite\n> a\n> few cases where the CGI program fabricating the cookie doesn't know\n> the\n> server's port and possibly can't get it easily. So I think PORT w/o a\n> specific number has value.\n> \n> Dave\n> \n> > \n> > > -----Original Message-----\n> > > From:David W. Morris [SMTP:dwm@xpasc.com]\n> > > Sent:Sunday, March 23, 1997 11:07 AM\n> > > To:Yaron Goland\n> > > Cc:http working group\n> > > Subject:RE: new cookie draft\n> > > \n> > > \n> > > \n> > > On Sat, 22 Mar 1997, Yaron Goland wrote:\n> > > \n> > > > Actually I suggested the exact opposite. If PORT is NULL then\n> the\n> > > cookie\n> > > > may be sent on any port. It is only if a port is specified that\n> > > there is\n> > > > a restriction.\n> > > \n> > > I think perhaps my syntax wasn't clear.  My intent was that three\n> > > cases\n> > > exist:\n> > > \n> > >  a)   PORT attribute not specified, the attribute is NULL\n> > > \n> > >       There are no restrictions\n> > > \n> > >  b)   PORT attribute specified but with no value, the value NULL\n> > >  \n> > >       Only the source PORT may receive the cookie\n> > > \n> > >  c)   PORT attribute with a value in which case the value is a\n> comma\n> > >       delimited list of valid ports.  (e.g, PORT=\"80,443\")\n> > > \n> > >       Only the listed ports may receive the cookie\n> > > \n> > > I think (a) and (b) are your proposal and I added (c) to allow\n> more\n> > > precise control.\n> > > \n> > > Dave\n> > > \n> > > > Yaron\n> > > > \n> > > > > -----Original Message-----\n> > > > > From:David W. Morris [SMTP:dwm@xpasc.com]\n> > > > > Sent:Friday, March 21, 1997 10:21 PM\n> > > > > To:http working group\n> > > > > Subject:RE: new cookie draft\n> > > > > \n> > > > > \n> > > > > \n> > > > > On Fri, 21 Mar 1997, M. Hedlund wrote:\n> > > > > \n> > > > > > On Fri, 21 Mar 1997, Yaron Goland wrote:\n> > > > > > > We can\n> > > > > > > define an attribute \"PORT\", with no argument. If it is\n> > > included in\n> > > > > a\n> > > > > > > cookie then the cookie may only be returned on the port it\n> was\n> > > > > received\n> > > > > > > on, this requirement applies to all domains. \n> > > > > > \n> > > > > > That sounds right.  \n> > > > > \n> > > > > An alternative ... a PORT attribute whose value is a comman\n> > > delimited\n> > > > > list\n> > > > > of ports on which the cookie may be returned. If the PORT\n> > > attribute is\n> > > > > omitted, any port is valid.  If the value of the PORT\n> attribute is\n> > > > > NULL,\n> > > > > then as Yaron suggested, it may only be sent to the port it\n> was\n> > > > > received\n> > > > > from. This allows it to be very tight while not excluding a\n> value\n> > > like\n> > > > >  \n> > > > >                 port=\"80,443\"\n> > > > > \n> > > > > which would allow sharing beteen the default HTTP and HTTPS\n> ports.\n> > > > > \n> > > > > Note: While I am proposing a mechanism to resolve an issue, I\n> > > don't\n> > > > > share\n> > > > > the concern so I will be happy with any solution which allows\n> > > sharing\n> > > > > between ports.\n> > > > > \n> > > > > Dave Morris\n> > > > \n> > \n\n\n\n", "id": "lists-010-16590112"}, {"subject": "Process for closing out issues lis", "content": "> I see that the link `The process used to close out the issues for\n> Proposed Standard' still leads to a `not found' error.  I find this\n> worrying.\n\nPlease do not worry. We have already completely closed out all of the\nissues for 'proposed standard'. We're now working on 'draft standard',\nand the criteria are very different.\n\n> I had hoped that some subgroup would be set up to generate\n> proposed resolutions before Memphis, so that people would have the\n> chance to check these resolutions against the discussions in the\n> mailing list archive beforehand.\n\nThere is no point in having a 'subgroup' unless it is the group\nof 'those who have implemented HTTP/1.1', since the criteria\nfor Draft Standard center around interoperable implementations.\n\nHere's the process: \n\nEveryone who has implemented an HTTP/1.1 client, server, or proxy is\ninvited to review the issue list and note how they resolved the issue\nin their implementation.\n\nSend mail to http-wg@cuckoo.hpl.hp.com with your collected list.\n\nIf an issue is about something that no one actually implemented, well,\nwe don't need to spend any time on it, since we'll have to toss it\nwhen we go to Draft Standard anyway.\n\nEveryone else (those who have not implemented an HTTP/1.1 client,\nserver or proxy), is asked to please hold their comments.\n\nRegards,\n\nLarry Masinter\n(as HTTP-WG chair)\n\n\n\n", "id": "lists-010-16602710"}, {"subject": "Re: 305 Use prox", "content": "Klaus Weide <kweide@tezcat.com> wrote:\n>On Sun, 23 Mar 1997, Roy T. Fielding wrote:\n>> In message <199703202122.NAA03558@step.mcom.com>, Ari Luotonen writes:\n>> >\n>> >If 305 is allowed by origin servers, intermediate HTTP/1.1 proxies\n>> >that do not understand 305's hop-by-hope requirement will let it\n>> >through (I assume at this point it may be too late to impose the\n>> >hop-by-hop requirement for 305, and expect it to be respected by all\n>> >implementations).  If a client gets a 305 sent by an evil origin\n>> >server through a proxy, it will override the client's proxy settings,\n>> >because the client thinks the proxy redirected it to another proxy.\n>> \n>> I don't know of any clients that have implemented 305.\n>> Is it already in the Navigator (or other browser)?\n>\n>Lynx has implemented 305 for a while.  It only accepts it from an origin\n>server (i.e. if not already using a proxy) and on a request-by-request\n>basis (like 302; no \"proxy settings\" are overridden except for the\n>current request).\n\nNote that this is another \"better safe than sorry\" initial\nimplementation in Lynx, reflecting concerns like those expressed\nby Ari, but going beyond what was actually in the spec at the time\nof the implementation.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-16611943"}, {"subject": "Re: Issues with the cookie draf", "content": "Dave Kristol:\n>\n>Yaron Goland <yarong@microsoft.com> wrote:\n[...]\n>  > Languages:\n>  > As I mentioned in my original proposal, the accept-language header would\n>  > server the purpose of choosing the language. In the worst case, the\n>  > language is just English. The UTF8 Unicode encoding preserves the lower\n>  > ASCII range so when dealing with downlevel clients, one sends UTF8\n>  > English. I do admit woeful ignorance of the language tag issues. Any\n>  > experts in the house?\n>\n>I'm also really bad on the language issues.  That's why I asked for more\n>details.\n\nI'm not a language expert, but my personal opinion as a `negotiation expert'\nis that internationalisation of comments in headers is not ready for prime\ntime.  This is a tar pit I'd rather steer clear of.  Maybe it would be nice\nto add a note that the comment could be the URL of a page which explains the\ncookie in multiple languages.\n\n>  > 4.3.2 Rejecting Cookies (how far into the domain do you go):\n>  > I appreciate that it was a long and drawn out debate but that is not a\n>  > sufficient rational for preventing perfectly reasonable behavior. The\n>  > decision to stop at one domain level is completely arbitrary. It is no\n>  > more and no less secure than 2 or infinite domain levels deep. I do not\n>  > feel that an arbitrary choice is a good enough reason to include a\n>  > requirement in a specification.\n>\n>It wasn't completely arbitrary.\n\nSpecifically, Netscape said that, according to the contacts they had with\nmulti-hosted sites, the 1 domain level restriction would not be too tight\nfor the kinds of services these people had in mind.  And privacy expectation\nconsiderations led to us wanting a number as low as possible, so 1 domain it\nwas.\n\nI don't remember that we had very long discussions about this, we just noted\nthat Netscape's initial choice was a good one.\n\nKoen.\n\n\n\n", "id": "lists-010-16620809"}, {"subject": "Re: (CONTENT NEGOTIATION,VARY) New draft text for Vary header and content negotiation `hooks", "content": "> 10.u  URI\n> \n>    The URI entity-header field is used to inform the recipient of\n>    other Uniform Resource Identifiers (Section 3.2) by which\n>    the resource can be identified.\n> \n>        URI-header  = \"URI\" \":\" 1#( uri-mirror | uri-name )\n> \n>        uri-mirror  = \"{\" \"mirror\" <\"> URI <\"> \"}\"\n>        uri-name    = \"{\" \"name\" <\"> URI <\"> \"}\"\n> \n>    Any URI specified in this field can be absolute or relative to the\n>    Request-URI. The \"mirror\" form of URI refers to a location which is a\n>    mirror copy of the Request-URI. The \"name\" form refers to a\n>    location-independent name corresponding to the Request-URI.\n> \n>    [##Note: According to the issues list, Roy is working on text that\n>    explains better what \"mirror\" and \"name\" actually mean.##]\n\nThis field will be returned to its old definition.  In brief, that is\n\n   The URI entity-header field may contain a list of Uniform Resource\n   Identifiers (Section 3.2) which are considered equivalent to the\n   Request-URI, but which may be mirror locations or location-independent\n   names (URN) for the identified resource.\n\n        URI-header  = \"URI\" \":\" 1#( \"<\" URI \">\" )\n\nThe details are actually more complex, but I'll get to that asap.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1662361"}, {"subject": "Re: Process for closing out issues lis", "content": "Larry Masinter:\n>\n   [Koen:]\n>> I see that the link `The process used to close out the issues for\n>> Proposed Standard' still leads to a `not found' error.  I find this\n>> worrying.\n>\n>Please do not worry. \n[...]\n\nSorry, I tend to worry easily.  The link has `will be updated and issued\nsoon' behind it, so I get into a state of being worried if it is not.\n\nThe process you describe looks like a very good use of our meeting time.\n\nThanks,\n\nKoen.\n\n\n\n", "id": "lists-010-16630426"}, {"subject": "Re: I-D ACTION:draft-ietf-http-hit-metering01.tx", "content": "Jeffrey Mogul:\n[...]\n>    ftp://ds.internic.net/internet-drafts/draft-ietf-http-hit-metering-01.txt\n> \n[...]\n>To this date, I don't think there have been many (or any) comments on\n>the Design Notes/Design Questions identified in the draft.  Consider\n>this as your last change to discuss those notes.\n\nI just read the design notes, and I agree with the way you have resolved the\nknown design choices.\n\n>  If there is no\n>specific and unresolved discussion of these notes, I will remove them\n>and generate a new draft by the pre-Memphis I-D submission deadline\n>(next Wednesday, March 26).\n\nSome other quick comments:\n\n- I'm happy with the new material about user counting. This revision removes\nmy earlier concerns about the support for user counting being overstated.\n\n- Some exotic proxy arrangements could lead to there being a `metering\nsub-graph' instead of a tree.  Have you done the analysis to find out if\nsection 5.6 really covers all subcases?\n\n- Section 3.3: `then the proxy MUST add \"Cache-control:\n   proxy-maxage=0\" to all responses it sends for the resource.'\nYou probably mean: `then the proxy MUST add \"Cache-control:\n   proxy-maxage=0\" to all responses for which metering or limiting was\n   requested.'\nbecause you say earlier on that being metered is a property of a response,\nnot of a resource.\n\n- Section 4.1:\n\n  `The existing (HTTP/1.0) \"cache-busting\" mechanisms for counting\n   distinct users will certainly overestimate the number of users behind\n   a proxy, since it provides no reliable way to distinguish between a\n   user's initial request and subsequent repeat requests caused by\n   insufficient space in the end-client cache.'\n\nHit metering also `provides no reliable way to distinguish between a user's\ninitial request and subsequent repeat requests caused by insufficient space\nin the end-client cache' if I'm correct (there is no If-* header if the page\ndropped out of the cache), so limiting this statement to \"cache-busting\" is\na bit misleading.\n\n  `The \"Cache-control:\n   proxy-maxage=0\" feature of HTTP/1.1 does allow the separation of\n   use-counts and reuse-counts, provided that no HTTP/1.0 proxy caches\n   intervene.'\n\nHow can they intervene?  Do some 1.0 proxies stip off the If-NoMatch headers\nwhen forwarding a request on a stale cache entry, or are you talking about\nsomething else?\n\n- Concluding: I can live with this draft going forward as a proposed\nstandard.  I think it is technically sound.  I'm converting my `no' on the\nprevious last call to a `don't care'.  It is not a `yes' because I still feel\nthere is insufficient evidence that this draft is really needed.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-16638673"}, {"subject": "cookie Port summar", "content": "Here's my summary and elaboration of the proposal for restricting ports\nin cookies.\n\nSet-Cookie2\n1) Syntax:\nport-attr=\"Port\" [ \"=\" <\"> 1#port-list <\"> ]\nport-list=decimal-number\n\nNote:  port-attr is, of course, itself optional.\n\n2) Semantics\nReject cookie if there is a port-list and the original connection was\nnot to a listed port.\n\nCookie:\n1) Syntax:\n(Return Port as $Port, with its value as received in Set-Cookie2, if any.)\n\n2) Semantics, based on the Port attribute in Set-Cookie2:\n- default (no Port) behavior:  send cookie to any port\n- \"Port\" behavior:  send cookie only to port from which it was received\n- \"Port=port-list\" behavior:  send cookie only to a listed port\n\nNote:  Port rules apply only after the Domain rules make the cookie otherwise\nsendable.\n\nComments?\nDave Kristol\n\n\n\n", "id": "lists-010-16649064"}, {"subject": "Re: cookie Port summar", "content": "On Mon, 24 Mar 1997, Dave Kristol wrote:\n\n> 2) Semantics\n> Reject cookie if there is a port-list and the original connection was\n> not to a listed port.\n>\n\nEven for port 80? I'm not saying this is incorrect, but it is\nnon-intuituve, and will likely confuse a lot of people. Remember, people\nmay wish to share cookies across port 80 and (say) port 8080 and may\nassume they only have to include 8080 in the port list.\n\nOn the other hand, it would certainly be useful to exclude port 80. I\ndon't know.\n \n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n", "id": "lists-010-16657313"}, {"subject": "Re: cookie Port summar", "content": "\"Gregory J. Woodhouse\" <gjw@wnetc.com> wrote:\n  > > [DMK]\n  > > 2) Semantics\n  > > Reject cookie if there is a port-list and the original connection was\n  > > not to a listed port.\n  > >\n  > \n  > Even for port 80? I'm not saying this is incorrect, but it is\n  > non-intuituve, and will likely confuse a lot of people. Remember, people\n  > may wish to share cookies across port 80 and (say) port 8080 and may\n  > assume they only have to include 8080 in the port list.\n  > \n  > On the other hand, it would certainly be useful to exclude port 80. I\n  > don't know.\n\nEven for port 80.  Not all servers run on port 80.  If port-list included\nport 80 implicitly, there would be no way to exclude it.  Cookies emitted\nfrom port 8000 would leak to port 80.\n\nDave Kristol\n\n\n\n", "id": "lists-010-16665213"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "Rob Hartill:\n>\n[...]\n>Caching ads is very bad news for people selling ads on a per impression\n>basis. If a content site has to redirect to an ad network then it will\n>either have to cache bust everything or waive goodbye to 30-50% of their\n>revenue OR 30-50% of their ad serving capacity.\n\nHmm, you lost me here.  How does the cache busting by the content site\naffect the counting of ads sent by the ad network site?\n\n>I can't see network sites trusting the content sites to report accurate\n>impression counts, \n\nThen we have a different opinion.  I can easily see this level of trust\nhappening.\n\nAnyway, I think that the value of ad rotation and user profiling is vastly\noverrated.  Of course, it is in the best interest of ad networks to overrate\nthe added value of these things: they are currently the ones who get paid\nfor adding this value.\n\n>Rob Hartill   Internet Movie Database (Ltd)\n\nKoen.\n\n\n\n", "id": "lists-010-16672907"}, {"subject": "Re: cookie Port summar", "content": "Dave Kristol:\n>\n>Here's my summary and elaboration of the proposal for restricting ports\n>in cookies.\n[...]\n>Comments?\n\nThis works for me.  \n\nWith a little more work the default could be made more secure (i.e. only\nsend to the port it came from) in the pure `new cookie' case.  But we are\nprobably stuck with the `send to all ports' default when being compatible\nwith `old cookies' sent in a Set-Cookie without a Set-Cookie2.  Some\nexisting sites which continue sessions on secure pages will rely on this\nless-secure default, I think.\n\n>Dave Kristol\n\nKoen.\n\n\n\n", "id": "lists-010-16681337"}, {"subject": "RE: cookie Port summar", "content": "I must be going dense but the section stating \"Reject cookie if there is\na port-list and the original connection was not to a listed port.\"\nconfuses me. It sounds like something I agree w/but I'm not clear on\nwhat it means.\n\nIf a set-cookie2 with a port list comes down and is accepted and then a\nsecond set-cookie2 comes down, which matches the first cookie, but isn't\nfrom the right port, the second set-cookie2 is to be ignored?\n\nYaron\n\n> -----Original Message-----\n> From:dmk@research.bell-labs.com [SMTP:dmk@research.bell-labs.com]\n> Sent:Monday, March 24, 1997 8:26 AM\n> To:http-wg@cuckoo.hpl.hp.com\n> Subject:cookie Port summary\n> \n> Here's my summary and elaboration of the proposal for restricting\n> ports\n> in cookies.\n> \n> Set-Cookie2\n> 1) Syntax:\n> port-attr=\"Port\" [ \"=\" <\"> 1#port-list <\"> ]\n> port-list=decimal-number\n> \n> Note:  port-attr is, of course, itself optional.\n> \n> 2) Semantics\n> Reject cookie if there is a port-list and the original connection was\n> not to a listed port.\n> \n> Cookie:\n> 1) Syntax:\n> (Return Port as $Port, with its value as received in Set-Cookie2, if\n> any.)\n> \n> 2) Semantics, based on the Port attribute in Set-Cookie2:\n> - default (no Port) behavior:  send cookie to any port\n> - \"Port\" behavior:  send cookie only to port from which it was\n> received\n> - \"Port=port-list\" behavior:  send cookie only to a listed port\n> \n> Note:  Port rules apply only after the Domain rules make the cookie\n> otherwise\n> sendable.\n> \n> Comments?\n> Dave Kristol\n\n\n\n", "id": "lists-010-16688932"}, {"subject": "RE: cookie Port summar", "content": "Yaron Goland <yarong@microsoft.com> wrote:\n\n  > I must be going dense but the section stating \"Reject cookie if there is\n  > a port-list and the original connection was not to a listed port.\"\n  > confuses me. It sounds like something I agree w/but I'm not clear on\n  > what it means.\n  > \n  > If a set-cookie2 with a port list comes down and is accepted and then a\n  > second set-cookie2 comes down, which matches the first cookie, but isn't\n  > from the right port, the second set-cookie2 is to be ignored?\n\nHere's the idea:\n\n1) UA connects to foo.com, port 80.\n2) Server sends Set-Cookie2: x=y; Port=\"8000\"\n3) UA rejects the cookie, because port 80, the port for the request,\ndoes not match any of the ports in the Port= attribute of Set-Cookie2.\n\nDave Kristol\n\n\n\n", "id": "lists-010-16698984"}, {"subject": "Re: Unverifiable Transactions / Cookie draf", "content": "On Mon, 24 Mar 1997, Koen Holtman wrote:\n\n> Rob Hartill:\n> >\n> [...]\n> >Caching ads is very bad news for people selling ads on a per impression\n> >basis. If a content site has to redirect to an ad network then it will\n> >either have to cache bust everything or waive goodbye to 30-50% of their\n> >revenue OR 30-50% of their ad serving capacity.\n> \n> Hmm, you lost me here.  How does the cache busting by the content site\n> affect the counting of ads sent by the ad network site?\n\nIf you don't go out of your way to cache-bust, the number of ads\nserved (redirected) by the content site will be 30-50% larger than\nwhat the network site will receive due to clients/proxies NOT caching\na 302 redirect but caching the network provided ads.\n\nThese figures are empirical.... found the hard (expensive) way.\n\n> >I can't see network sites trusting the content sites to report accurate\n> >impression counts, \n> \n> Then we have a different opinion.  I can easily see this level of trust\n> happening.\n\nIt'd also mean networks giving up far too much control over the system.\nThat's a tough pill for them to swallow.... assuming of course they\nwill try to maintain user tracking in the future.\n\n> Anyway, I think that the value of ad rotation and user profiling is vastly\n> overrated.\n\nI agree with you there.\n\n> Of course, it is in the best interest of ad networks to overrate\n> the added value of these things: they are currently the ones who get paid\n> for adding this value.\n\nI agree there too, the added value should be questioned by the advertisers.\nThey do however provide a valuable source of income or additional income\nfor many popular sites. Anyone who has tried selling advertising for\na site will know that it's tough competeing with the big players.. letting\na big network do all the work is great. I just hope that stomping on\ntheir cookies doesn't lead to complicated/expensive alternatives - that's\nbad news for everyone - can they afford to ditch user tracking ?\n...we'll see.\n\n\n\n--\nRob Hartill   Internet Movie Database (Ltd)\nhttp://us.imdb.com/Oscars/oscars_1996 -  hype free Oscars (R) info.\nhttp://us.imdb.com/usr/sweepstake     -  Win a 56k X2 modem. Free draw.\n\n\n\n", "id": "lists-010-16706659"}, {"subject": "RFC2109 addition..", "content": "The Comment attribute shows incredible forward thinking.  Kudos to\nwhoever came up with that.  I would like to propose, however, an\naddition.\n\nI would like to have a CommentURL which contains the path to comments\nregarding the privacy policies of the site that deal with the cookie.\nI would like this URL to be relative to the URL that issued the cookie,\nunless otherwise specified (as being server relative or fully qualified).\n\nMy reason for this addition is pretty straight forward.  \nI expect the comment fields could get rather large.  I can tell you\nfrom experience that the typical comment is going to look more like\na paragraph than just a few words, and many CGI's (and servers) are not \nintelligent enough to only issue cookie requests once (even if they\nreceive a cookie in the request, they often issue a new one that expires\n3 seconds later on some date in 2012).  Each time the cookie gets\nreissued, the comment would be sent over the network.  A URL is much shorter.  \nThe use of traditional caching methods on the document saves having to send\nthe comment every time.  Additionally, I think companies will be more likely\nto fully explain their privacy policies if they have a page to explain\nthem on, rather than just a comment field.  This will encourage a method\nof informed consent.  Third parties could be the target of these URL's,\nproviding additional value in verifying or auditing privacy statements \nmade by the issuer of the cookie.\n\nI DO NOT, however, think it would be wise to replace the comment field\nall together with just a URL.  I think both methods will have value, and \nthat it's likely that the comment attribute will be implemented sooner \nthan the URL method, which would likely show up in a dialog saying something\nlike \"Click to review the usage policices for this cookie\".\n\nIt should also be clear that requests for the CommentURL should not result\nin a cookie being issued.  If a cookie is issued at the comment URL, it\nshould be denied to avoid any potential loops.\n\nHere's a first crack at the text as I feel it should be included in the\nRFC:\n\n--\nCommentURL=commenturl\nOptional.  The CommentURL allows an origin server to specify a document\nthat explains the usage of this cookie, and could optionally also explain\nthe policies governing the use of information collected through this cookie.\nA user-agent can offer the user the option of inspecting this page before\naccepting a cookie.  Any cookies issued while attempting to retrieve the\ndocument at commenturl should be refused.\n\n--\nI'd appreciate any comments or improvements.\n\nThanks,\n\nJonathan Stark\neTRUST Technical Director\n\n\n\n", "id": "lists-010-16716065"}, {"subject": "Re: (ACCEPT*) Last call on draft text for Accept header", "content": "Koen Holtman:\n>Tim Greenwood:\n[...]\n>>The proposal was intended not as a simplification, but as a \n>>clarification. Your comment is correct, my proposal would have \n>>excluded a match that should be allowed. The text in 10.4 should \n>>still be rewritten to use the syntax defined in 3.10. The current \n>>text uses the term 'a prefix' which is not defined. \n>\n>Ah, so _that_ is the problem.  I assumed that 'prefix' was standard\n>computing terminology, but apparantly it is not.  (Prefix means\n>\"initial part of a string\".)  I'll see if I can find an alternative\n>wording.\n\nOK, here is what I have come up with:  the old sentence\n\n   A language-range matches a language-tag if it exactly equals the tag,\n   or if it is a prefix of the tag such that the first tag character\n   following the prefix is \"-\".\n\nis replaced with\n\n   A language-range matches a language-tag if it exactly equals the\n   tag, or if it exactly equals a prefix (a sub-sequence starting at\n   the first character) of the tag such that the first tag character\n   following the prefix is \"-\".\n\nI also tried to write text which completely avoided the use of the\nword `prefix', but that text turned out to be less intelligible.\n\nI will assume that this change solves the problem until I hear\notherwise.\n\nOn a related note: I'm currently also working with Jim Gettys to\nsimplify a few sentences in the proposed Accept header draft text.  I\nwill post a revised version once we are finished.\n\nKoen.\n\n\n\n", "id": "lists-010-1672047"}, {"subject": "Re: RFC2109 addition..", "content": "On Mon, 24 Mar 1997, Jonathan Stark wrote:\n\n> Here's a first crack at the text as I feel it should be included in the\n> RFC:\n> \n> --\n> CommentURL=commenturl\n> Optional.  The CommentURL allows an origin server to specify a document\n> that explains the usage of this cookie, and could optionally also explain\n> the policies governing the use of information collected through this cookie.\n> A user-agent can offer the user the option of inspecting this page before\n> accepting a cookie.  Any cookies issued while attempting to retrieve the\n> document at commenturl should be refused.\n\nI have been working thru a similar idea before presenting it ... BUT thus\nfar my thought is that there shouldn't be any restrictions on what the\nURL points at, associated cookies, etc. except that we need to work thru\nthe rules to make sure a privacy hole isn't created... but I think if\nthe rules are that retrieving this URL is like following any other link\nthen I don't think there are any new exposures.  (That is the cookie\nissued by this link would have to fit the URL being retrieved.)  THis\nhas the additional advantage in that language issues can be handled via\nnormal UA / server negotiation. A suggested UI for an UA able to do so\nwould be to open a new browser window to follow the link.\n\nDave Morris\n\n\n\n", "id": "lists-010-16725635"}, {"subject": "Section 10.1.1 Combining Set-Cookie and SetCookie", "content": "I fail to understand the rationale behind what to me is the most complex\nsection in the whole document.  Why are we requiring UAs to combine \nthe two headers?\n\nI think there is no siginificant loss of functionality if this whole\nsection is dropped along with the forward reference in the previous\nsection.  Simply require the server to send both with appropriate\nattributes. If the UA understands both forms, it MUST send the new form\nand it must replace an existing matching form 1 with the new form.\n\nOtherwise the UA doesn't understand both and sends the old form.\n\nDave Morris\n\n\n\n", "id": "lists-010-16734425"}, {"subject": "FYI: New IETF WEBDAV Working Grou", "content": "A working group on World Wide Web Distributed Authoring and Versioning\n(WEBDAV) has been formed in the Applications Area of the Internet\nEngineering Task Force.  Keith Moore and Harald Alvestrand are the\nApplications Area Directors, and Keith Moore is the Area Advisor for the\nworking group.  The Chair of the working group is Jim Whitehead, a Ph.D.\nstudent in the Dept. of Information and Computer Science at the University\nof California, Irvine.\n\nThis working group will define the HTTP extensions necessary to enable\ndistributed web authoring tools to be broadly interoperable, while\nsupporting user needs.\n\nAs a broad overview, functionality under consideration by the WEBDAV WG\nincludes:\n\n* Overwrite prevention:  Preventing the \"lost update\" problem where\nmodifications are lost as first one author, then another writes their\nchanges without merging the other author's changes.\n\n* Metadata: The ability to create, remove, modify and query information\nabout Web resources, such as author, creation date, etc.\n\n* Name space management: The ability to copy and move Web resources, and to\nreceive a listing of resources within a particular container (such as\nfilesystem directory-like containers).\n\n* Version management: The ability to perform simple revision control (e.g.\nlike RCS, SCCS, CVS) so that important revisions of a document can be\nstored for later retrieval.  Also, the support of asynchronous\ncollaboration where two or more authors work on the same document in\nparallel revision tracks.\n\n* Linking: The ability to create links between resources of any media type\n(e.g., fully-specified LINK and UNLINK methods).\n\n\nWEBDAV issues are discussed on the mailing list \"w3c-dist-auth@w3.org.\"  To\nsubscribe to the mailing list, send an email with subject \"subscribe\" to\n\"w3c-dist-auth-request@w3.org\".  The World Wide Web Consortium graciously\nhosts this mailing list.\n\nFor more information on WEBDAV, please consult:\n\n  WEBDAV Home Page:\n  http://www.ics.uci.edu/~ejw/authoring/\n\n  WEBDAV Introduction article\n  http://www.ics.uci.edu/~ejw/authoring/intro/webdav_intro.ps\n  http://www.ics.uci.edu/~ejw/authoring/intro/webdav_intro.pdf\n\n  WEBDAV WG Charter:\n  http://www.ics.uci.edu/~ejw/authoring/charter.html\n\n  WEBDAV Requirements Document:\n  http://www.ics.uci.edu/~ejw/authoring/webdav-req-00.html\n\n\nFinally, I'd like to acknowledge and thank Larry Masinter, Dan Connolly,\nKeith Moore, Ralph Swick, Harald Alvestrand, Jim Miller, and Roy Fielding\nfor their assistance along the path to the formation of this working group.\n\n\n- Jim Whitehead <ejw@ics.uci.edu>\n  http://www.ics.uci.edu/~ejw/\n\n\n\n", "id": "lists-010-16742542"}, {"subject": "Re: RFC2109 addition..", "content": "Jonathan, thanks for reviewing the RFC.\n\nOn Mon, 24 Mar 1997, Jonathan Stark wrote:\n> I would like to have a CommentURL which contains the path to comments\n> regarding the privacy policies of the site that deal with the cookie.\n\nThe potential loop problem (setting a cookie on the cookie information page\n-- which you address in your proposed language) is pretty funny. \n\nI think this suggestion creates UI weirdness -- the proposal seems to\nassume that the client can open a second window for review of the policy\nbefore making an accept/decline decision on the cookie.  That may not be\ntrue for, say, a Pilot Web browser, or other limited-UI browsers.  I don't\nthink it is a reason to reject the proposal, but I do think it is worth\nconsidering.  I would say we might want to recommend that a comment always\naccompany a commenturl, so that a limited-UI agent has options.\n\nI like David's point about language negotiation being handled through the\nfollow-up request, though.  \n\n> I would like this URL to be relative to the URL that issued the cookie,\n> unless otherwise specified (as being server relative or fully qualified).\n\nHm....what if I want to reference the generic eTRUST cookie policy (if such\na thing were to exist) at the eTRUST site?  Then eTRUST could say, \"5\nmillion people examined our cookie policy before accepting a cookie this\nmonth, so privacy is important to them.\"  (My point being that there is\nvalue to centralized policies, and thus absolute commenturls, be they for\nprivacy advocates or Better Business Bureaus or whatever.) \n\n> CommentURL=commenturl\n\nHow about \n  CommentURL = '<' commenturl '>'\n\n> Optional.  The CommentURL allows an origin server to specify a document\n> that explains the usage of this cookie, and could optionally also explain\n> the policies governing the use of information collected through this cookie.\n\nAdd: \"A server SHOULD send a comment if sending a commentURL, for use by\nthose browsers unable to display the CommentURL contents.\"\n\nOkay?\n\n> A user-agent can offer the user the option of inspecting this page before\n> accepting a cookie.  \n\nShould be: \"A user-agent MAY...\"\n\n> Any cookies issued while attempting to retrieve the\n> document at commenturl should be refused.\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n", "id": "lists-010-16752321"}, {"subject": "RE: cookie Port summar", "content": "AHHHHHHHH.. I understand. Thanks for the clarification.\n\nIn that case, I too completely buy off on PORT.\n\nYaron\n\n> -----Original Message-----\n> From:dmk@research.bell-labs.com [SMTP:dmk@research.bell-labs.com]\n> Sent:Monday, March 24, 1997 2:50 PM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:RE: cookie Port summary\n> \n> Yaron Goland <yarong@microsoft.com> wrote:\n> \n>   > I must be going dense but the section stating \"Reject cookie if\n> there is\n>   > a port-list and the original connection was not to a listed port.\"\n>   > confuses me. It sounds like something I agree w/but I'm not clear\n> on\n>   > what it means.\n>   > \n>   > If a set-cookie2 with a port list comes down and is accepted and\n> then a\n>   > second set-cookie2 comes down, which matches the first cookie, but\n> isn't\n>   > from the right port, the second set-cookie2 is to be ignored?\n> \n> Here's the idea:\n> \n> 1) UA connects to foo.com, port 80.\n> 2) Server sends Set-Cookie2: x=y; Port=\"8000\"\n> 3) UA rejects the cookie, because port 80, the port for the request,\n> does not match any of the ports in the Port= attribute of Set-Cookie2.\n> \n> Dave Kristol\n\n\n\n", "id": "lists-010-16762303"}, {"subject": "Re: RFC2109 addition..", "content": "Quoting Marc Hedlund:\n\n> Jonathan, thanks for reviewing the RFC.\n> \n> On Mon, 24 Mar 1997, Jonathan Stark wrote:\n> > I would like to have a CommentURL which contains the path to comments\n> > regarding the privacy policies of the site that deal with the cookie.\n> \n> The potential loop problem (setting a cookie on the cookie information page\n> -- which you address in your proposed language) is pretty funny. \n\nYeah, it is funny...  But I don't think most people who are using\ncookies are going to necessarily think about taking the extra\neffort to exclude a certain page.\n\n> I think this suggestion creates UI weirdness -- the proposal seems to\n> assume that the client can open a second window for review of the policy\n\nPerhaps.  I think the prefered method would be a unique dialog box that\nhappens to have html in it... I think it's important that the user deal\nwith this issue before proceeding, and that it look uniquely different\nthat JUST another web page, and the only real way to do that is if the\nbrowser makes the window look a little bit differnt.  But that is UI... \nit's not really our problem here as I understand it.  It's good to think \nof it's limitations, though.  Does anybody who's worked on a browser\nhave any comments?\n\n> before making an accept/decline decision on the cookie.  That may not be\n> true for, say, a Pilot Web browser, or other limited-UI browsers.  I don't\n\nGood point.  Is it reasonable to accept a browser to be able to make two\nconnection at a time?  I'm not sure.  Maybe those browsers would need to\nacquire the ability to make multiple connections or just not support this\nfeature.\n> \n> I like David's point about language negotiation being handled through the\n> follow-up request, though.  \n\nI'm afraid I don't fully understand this issue.  Language negotiation\nis a part of the cookie spec?  I think I missed that part.  Somebody\nwanna point me at a URL?\n\n> > I would like this URL to be relative to the URL that issued the cookie,\n> > unless otherwise specified (as being server relative or fully qualified).\n> \n> Hm....what if I want to reference the generic eTRUST cookie policy (if such\n> a thing were to exist) at the eTRUST site?  Then eTRUST could say, \"5\n> million people examined our cookie policy before accepting a cookie this\n> month, so privacy is important to them.\"  (My point being that there is\n> value to centralized policies, and thus absolute commenturls, be they for\n> privacy advocates or Better Business Bureaus or whatever.) \n\nI agree completely.  But we would get that information from logs or a cgi\nthat counts the hits.  What I said (meant to say?) is that they should\nbe interpreted EXACTLY as a src=\"...\" field in the body of an html doc.\nie, a request to \"http://www.etrust.org/thing/text.html\" could have the \nfollowing values in ComentURL translating to the following pages:\n\npolicies.html->http://www.etrust.org/thing/policies.html\n/policies.html->http://www.etrust.org/policies.html\nhttp://privacy.goodguy.org/disclosure.html->\nhttp://privacy.goodguy.org/disclosure.html\n\nMy purpose in doing this is to make the commenturl value as short as\nis reasonably possible.  The code in most browsers is already there\nto expand server and page relative URLS, so use it.\n\n> \n> > CommentURL=commenturl\n> \n> How about \n>   CommentURL = '<' commenturl '>'\n\nI'm new to this whole process, so I guess I don't understand the\ndifference in notation.  Does this now imply that the attributeline would\nlook like this:\nCommentURL=<http://www.privacy.net/disclosure>\n?\nIf so, I disagree.  It should be parsed the same as all the other\nattributes...\nCommentURL=http://www.privacy.net/disclosure\nHowever you notate that.... :)  I'm not sure... are there problems\nwith escaped characters in a URL meaning something in the Cookie?\nSomebody help me out here...   Maybe <> is necessary?\n\n> > Optional.  The CommentURL allows an origin server to specify a document\n> > that explains the usage of this cookie, and could optionally also explain\n> > the policies governing the use of information collected through this cookie.\n> \n> Add: \"A server SHOULD send a comment if sending a commentURL, for use by\n> those browsers unable to display the CommentURL contents.\"\n\nI question this a little bit.  My goal (in addition to the one you\npointed out, in providing a common location for collecting policies)\nis to reduce the amount of data that has to be sent to the client.\nIf you send a comment AND a URL, that doesn't really achive my goal.\n\nI suspect that \"the market\" will dictate it's own rules on using\none or the other or both based upon the implementations of the day.\nAnd, to some extent, that's probably ok.  I guess my point is that\nthe best practices will change over time, and we maybe shoudn't outline\na static practice in the RFC.  Do others agree or disagree?  This is\na tricky issue.  If you don't set down a best practice, you may not\nget what you want, but it you do, you may end up outdated.\n\nThere certainly is value to both methods, and I do see your point.\nPerhaps the best thing to do would be to recommend that in the absence of\na comment, the CommentURL (if present) should be offered to the user\nin a similar way as the Comment.  This isn't extremely graceful, but then \nsomeone truly concerned has the option to not accept the cookie originally, \nand go look at the comment URL even on a browser that only allows one\nconnection at a time.  I don't really think I'd like what this would look\nlike, but it's something.\n\n> > A user-agent can offer the user the option of inspecting this page before\n> > accepting a cookie.  \n> \n> Should be: \"A user-agent MAY...\"\n\nI'll go half way... how about \"A user-agent should\"? :)\n\n> > Any cookies issued while attempting to retrieve the\n> > document at commenturl should be refused.\n> \n> Marc Hedlund <hedlund@best.com>\n> \n\nThanks for the comments, Marc,\n\nJonathan\n\n\n\n", "id": "lists-010-16772277"}, {"subject": "(DNS) consensus wording resolution", "content": "Koen Holtman and Dave Morris have expressed concerns about the\nrequirement being mandantory (Must vs. should), and proposed an\nalternate based on a arbitrary timeout (with no defense as to how\nthat timeout might be chosen), believing that the implementation\nis difficult.\n\nI talked to both Don Eastlake (Mr. DNS security) and Paul Vixie (Mr. Bind),\nabout the implementation difficulty and the MUST issue.  Don's comment\nwas that this requirement \"should be a MUST\".  Paul also believed this should\nbe a MUST, and that the implementation effort to do a DNS query to\nget the TTL was \"60 lines of code\" (I'm not sure I believe this number,\nbut it is clearly not a large amount of coding effort, though it may\nbe more effort in terms of learning about DNS). \n\nThose who I believe I have data on include:\nMUSTShould\nJim GettysKoen Holtman\nPaul LeachDave Morris\nPaul VixieAnawat Chankhunthod\nDon Eastlake\nMaurizio Codogno\nPhill Hallam-Baker\nHenryk Frystyk\n\nLet me know if I am mis-representing your opinion.\n\nGiven that:\no the current usage of DNS by many web clients is violating\nmandantory parts of the DNS specification,\no the Web now represents the bulk of the network traffic and DNS lookups,\no the implementation difficulty does not look too large,\no the opinions of those most familiar with DNS both agree \nthis should be a MUST,\no that several people are already implementing code to perform\nthis function that will become publically availabile, \no clear majority believe this should be a mandantory requirement,\nI am proceeding with incorporating the requirement as previously circulated.\n- Jim\n\n====================\n\nSection 14 (new subsection to Security Considerations):\n\nDNS Spoofing\n------------\n\nClients using HTTP rely heavily on the Domain Name Service, and are\nthus generally prone to security attacks based on the deliberate\nmis-association of IP addresses and DNS names.  The deployment of\nDNSSEC[DNSSEC] should help this situation.  In advance of this deployment,\nhowever, clients need to be cautious in assuming the continuing\nvalidity of an IP number/DNS name association.\n\nIn particular, HTTP clients should rely on their name resolver for\nconfirmation of an IP number/DNS name association, rather than caching\nthe result of previous host name lookups.  Many platforms already can\ncache host name lookups locally when appropriate, and they should be\nconfigured to do so.  These lookups should be cached, however, only\nwhen the TTL (Time To Live) information reported by the name server\nmakes it likely that the cached information will remain useful.\n\nIf HTTP clients cache the results of a host name lookups in order to\nachieve a performance improvement, they MUST observe the TTL\ninformation reported by DNS.\n\nIf HTTP clients do not observe this rule, they could be spoofed when a\npreviously-accessed server's IP address changes.  As renumbering is\nexpected to become increasingly common [RFC 1900], the possibility of\nthis form of attack will grow.  Observing this requirement thus\nreduces this potential security vulnerability.\n\nThis requirement also improves the load-balancing behavior of clients\nfor replicated servers using the same DNS name and reduces the\nlikelihood of a user's experiencing failure in accessing sites which\nuse that strategy.\n\n\nAddition to 16. References:\n[dnssec]Whatever is appropriate; it is up for a vote at the IESG this\nmonth, and may be issued as an RFC in time.\n[RFC 1900]\nB. Carpenter, Y. Rekhter,\n<a href=\"http://info.internet.isi.edu:80/in-notes/rfc/files/rfc1900.txt\">\nRenumbering Needs Work</a>. RFC 1900, IAB, February 1996.\n\n\n\n", "id": "lists-010-1681527"}, {"subject": "INTEGOK (aka CONTENTMD5", "content": "Ned Freed and Larry Masinter inform me about composite MIME types and\nuse of Content-MD5 by them, leading to some additional explanatory\nwords.  The net net is that the computation doesn't change, but it takes\na few words to explain why.\n\nI think this is really it -- if there are no objections in 24 hours,\nwe'll consider this closed for the next draft (we have to get it out --\neven if minor amendments are required later.)\n\nHere's an update of the section.\n----------------------\n10.13Content-MD5\n\nThe Content-MD5 entity-header field is an MD5 digest of the entity-body,\nas defined in RFC 1864 [xx], for the purpose of providing an end-to-end\nmessage integrity check (MIC) of the entity-body. (Note: an MIC is good\nfor detecting accidental modification of the entity-body in transit, but\nis not proof against malicious attacks.)\n\nContentMD5= \"Content-MD5\" \":\" md5-digest\nmd5-digest= <base64 of 128 bit MD5 digest as per RFC 1864>\n\nThe Content-MD5 header may be generated by an origin server to function\nas an integrity check of the entity-body. Only origin-servers may\ngenerate the Content-MD5 header field; proxies and gateways MUST NOT\ngenerate it, as this would defeat its value as an end-to-end integrity\ncheck. Any recipient of the entity-body, including gateways and proxies,\nMAY check that the digest value in this header field matches that of the\nentity-body as received. \n\nThe MD5 digest is computed based on the content of the entity body,\nincluding any Content-Encoding that has been applied, but not including\nany Transfer-Encoding.  If the entity is received with a\nTransfer-Encoding, that encoding must be removed prior to checking the\nContent-MD5 value against the received entity.\n\nThis has the result that the digest is computed on the octets of the\nentity body exactly as, and in the order that, they would be sent if no\nTransfer-Encoding were being applied.\n\nHTTP extends RFC 1864 to permit the digest to be computed for MIME\ncomposite media-types (e.g., multipart/* and message/rfc822), but this\ndoes not change how the digest is computed as defined in the preceding\nparagraph. \n\n   Note: There are several consequences of this. The entity-body for\n   composite types many contain many body-parts, each with its own MIME\n   and HTTP headers (including Content-MD5, Content-Transfer-Encoding,\n   and Content-Encoding headers). If a body-part has a\n   Content-Transfer-Encoding or Content-Encoding header, it is assumed\n   that the content of the body-part has had the encoding applied,\n   and the body-part is included in the Content-MD5 digest as\n   is -- i.e., after the application. Also, the HTTP Transfer-Encoding\n   header makes no sense within body-parts; if it is present, it is\n   ignored -- i.e. treated as ordinary text.\n\n   Note: while the definition of Content-MD5 is exactly\n   the same for HTTP as in RFC 1864 for MIME entity-bodies,\n   there are several ways in which the application of\n   Content-MD5 to HTTP entity-bodies differs from its\n   application to MIME entity-bodies. One is that HTTP,\n   unlike MIME, does not use Content-Transfer-Encoding,\n   and does use Transfer-Encoding and Content-Encoding.\n   Another is that HTTP more frequently uses binary content types\n   than MIME, so it is worth noting that in such cases,\n   the byte order used to compute the digest is the\n   transmission byte order defined for the type. Lastly,\n   the canonical form of text types in HTTP includes several\n   line break conventions, so conversion of all line breaks\n   to CR-LF is not required before computing or checking\n   the digest: any acceptable convention should be left\n   unaltered for inclusion in the digest.\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-1691353"}, {"subject": "Re: INTEGOK (aka CONTENTMD5", "content": "> Ned Freed and Larry Masinter inform me about composite MIME types and\n> use of Content-MD5 by them, leading to some additional explanatory\n> words.  The net net is that the computation doesn't change, but it takes\n> a few words to explain why.\n\n> I think this is really it -- if there are no objections in 24 hours,\n> we'll consider this closed for the next draft (we have to get it out --\n> even if minor amendments are required later.)\n\nI agree -- this wording is fine.\n\nOnly one very minor nit: In \"Note: while\" the \"while\" should be capitalized.\n\nNed\n\n\n\n", "id": "lists-010-1703976"}, {"subject": "Re: INTEGOK (aka CONTENTMD5", "content": "Paul,\n\nI'm sorry, but the wording is still not right, when you say:\n\n>   Lastly,\n>   the canonical form of text types in HTTP includes several\n>   line break conventions, so conversion of all line breaks\n>   to CR-LF is not required before computing or checking\n>   the digest: any acceptable convention should be left\n>   unaltered for inclusion in the digest.\n\nThe phrase \"canonical form\" is a well known technical term. It is used\nin this context:\n\nWhen you have a large set of items A, and an equivalence relationship\namong those items E, such that two items a and b are deemed to be\nequivalent if E(a,b), it is possible to define a 'canonical form' C of\nitems in A such that if C(a) = c, then E(a,c). Given a canonical form\nC, E(x,y) iff C(x) = C(y). That is, the \"canonical form\" is a unique\nform of an object that can be used for equality testing when testing\nequivalent. \n\nIn the context of MIME types, we say that there are several forms of a\ntext document, namely: one with CRs for linebreaks, one with CRLF for\nlinebreaks, and one with LF for linebreaks, and we wish these to be\ndeemed to be equivalent. For this reason, MIME designates the form\nwith CRLF to be the canonical form, so that you can determine\nequivalence of two text streams by converting them to the canonical\nform.\n\nAt least in SMTP mail, text types are presumed to be transported in\ncanonical form, and MD5 digests are computed on canonical form. By\ncomputing MD5 digests of the canonical form, you are assured that\nequivalent text forms will have the same digest.\n\nNow, we decided that we did not wish HTTP to require transformation of\ntext times into canonical form before transmission, and this is fine.\nHowever, subsequently also allowing the message digest to be computed\non a non-canonical form means that equivalent text streams will have\ndifferent message digests. I can live with that decision too, if\nthat's really what people want. (Canonicalizing a text stream while\ncomputing the digest doesn't seem like it is computationally onerous,\nthough.) It is, however, totally unacceptable to make some statement\nthat\n\n   \"the canonical form of text types in HTTP includes several\n   line break conventions,\"\n\nbecause it either represents a misuse of the phrase \"canonical form\",\nor else asserts that two text streams that differ only by their line\nbreak convention should not be treated equivalently.\n\n\n\n", "id": "lists-010-1712596"}, {"subject": "INTEGOK; CONTENTMD", "content": "Larry won't let me get away with claiming there can be more than one\n\"canonical\" form for something, so I'll be as wishy-washy on the subject\nof whether HTTP entity-bodies are in canonical form as is the rest of\nthe spec. (I agree with Larry, BTW -- I was just trying to be consistent\nwith the explanation of how HTTP relates to MIME in appendix C of the\n1.0 spec.) I removed the word \"canonical\" from the last sentence of the\nsection; that's the only change.\n\n\n\n----------------------------\n10.13Content-MD5\n\nThe Content-MD5 entity-header field is an MD5 digest of the entity-body,\nas defined in RFC 1864 [xx], for the purpose of providing an end-to-end\nmessage integrity check (MIC) of the entity-body. (Note: an MIC is good\nfor detecting accidental modification of the entity-body in transit, but\nis not proof against malicious attacks.)\n\nContentMD5= \"Content-MD5\" \":\" md5-digest\nmd5-digest= <base64 of 128 bit MD5 digest as per RFC 1864>\n\nThe Content-MD5 header may be generated by an origin server to function\nas an integrity check of the entity-body. Only origin-servers may\ngenerate the Content-MD5 header field; proxies and gateways MUST NOT\ngenerate it, as this would defeat its value as an end-to-end integrity\ncheck. Any recipient of the entity-body, including gateways and proxies,\nMAY check that the digest value in this header field matches that of the\nentity-body as received. \n\nThe MD5 digest is computed based on the content of the entity body,\nincluding any Content-Encoding that has been applied, but not including\nany Transfer-Encoding.  If the entity is received with a\nTransfer-Encoding, that encoding must be removed prior to checking the\nContent-MD5 value against the received entity.\n\nThis has the result that the digest is computed on the octets of the\nentity body exactly as, and in the order that, they would be sent if no\nTransfer-Encoding were being applied.\n\nHTTP extends RFC 1864 to permit the digest to be computed for MIME\ncomposite media-types (e.g., multipart/* and message/rfc822), but this\ndoes not change how the digest is computed as defined in the preceding\nparagraph. \n\n   Note: There are several consequences of this. The entity-body for\n   composite types many contain many body-parts, each with its own MIME\n   and HTTP headers (including Content-MD5, Content-Transfer-Encoding,\n   and Content-Encoding headers). If a body-part has a\n   Content-Transfer-Encoding or Content-Encoding header, it is assumed\n   that the content of the body-part has had the encoding applied,\n   and the body-part is included in the Content-MD5 digest as\n   is -- i.e., after the application. Also, the HTTP Transfer-Encoding\n   header makes no sense within body-parts; if it is present, it is\n   ignored -- i.e. treated as ordinary text.\n\n   Note: while the definition of Content-MD5 is exactly\n   the same for HTTP as in RFC 1864 for MIME entity-bodies,\n   there are several ways in which the application of\n   Content-MD5 to HTTP entity-bodies differs from its\n   application to MIME entity-bodies. One is that HTTP,\n   unlike MIME, does not use Content-Transfer-Encoding,\n   and does use Transfer-Encoding and Content-Encoding.\n   Another is that HTTP more frequently uses binary content types\n   than MIME, so it is worth noting that in such cases,\n   the byte order used to compute the digest is the\n   transmission byte order defined for the type. Lastly,\n   the form of text types in HTTP includes several\n   line break conventions, so conversion of all line breaks\n   to CR-LF is not required before computing or checking\n   the digest: any acceptable convention should be left\n   unaltered for inclusion in the digest.\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-1721835"}, {"subject": "(CONTENT NEGOTIATION,VARY) Newer draft text for Vary header and content negotiation `hooks", "content": "The text below is a new version of the text I posted on Saturday.  I\nwish to thank Jeff Mogul, Larry Masinter, Roy Fielding, and Dave\nKristol for their comments on that version.  These comments led to\nsome improvements and additions in the text below.\n\nChanges with respect to the first version posted on Monday are marked\nwith | change bars.  Changes with respect to the second version posted\non Saturday are marked with + change bars.  All change bars were added\nby hand, so no 100% accuracy can be guaranteed.\n\nThere have been some changes in the text to synchronize with the\nplanned caching text by Jeff Mogul.  I expect that both texts will\nneed additional minor changes to get completely in sync.\n\nIf you have comments on this text, now is the time to comment.  I\nintend to close this issue in the middle of this week.\n\n--snip--\n\n\n** I. Vary+content negotiation new/changed header descriptions\n\n[##Note: The current section 12 needs to be deleted completely from the\nApril 1.1 draft.##]\n\n10.v  Vary\n\n+  [##This Vary section mixes the description of a header with a\n+  description of rules for caching.   These two things could be\n+  separated in the final 1.1 document, with the rules for caching\n+  being integrated in the caching section.##]\n\n|  [##The first few paragraphs below were rewritten to account for\n|  range retrievals, to shorten sentences, and to make some stylistic\n|  improvements.  The text now accounts for range retrievals, I forgot\n|  to cover them in the previous version.  The role of the\n|  Authorization header has also been made more clear.##]\n\n|  The Vary response-header field is used by an origin server to\n|  signal that the resource identified by the current request is a\n|  varying resource.  A varying resource has multiple entities\n|  associated with it, all of which are representations of the content\n|  of the resource.  If a GET or HEAD request on a varying resource is\n|  received, the origin server will select one of the associated\n|  entities as the entity best matching the request.  Selection of\n|  this entity is based on the contents of particular header fields in\n|  the request message, or on other information pertaining to the\n|  request, like the network address of the sending client.\n\n   If a resource is varying, this has an important effect on cache\n   management, particularly for caching proxies which service a\n|  diverse set of user agents.  All 200 (OK) responses from varying\n   resources must contain at least one Vary header or Alternates\n   header (Section 10.a) to signal variance.\n\n   If no Vary headers and no Alternates headers are present in a 200\n   (OK) response, then caches may assume, as long as the response is\n|  fresh, that the resource in question is not varying, and has only\n|  one associated entity.  Note however that this entity can still\n   change through time, as possibly indicated by a Cache-Control\n   response header (section 10.cc).\n\n|  After selection of the entity best matching the current request,\n|  the origin server will usually generate a 200 (OK) response, but it\n|  can also generate other responses like 206 (Partial Content) or 304\n|  (Not modified) if headers which modify the semantics of the\n|  request, like Range (Section 10.ran) or If-Valid (Section 10.ifva),\n|  are present.  An origin server need not be capable of selecting an\n+  entity for every possible incoming request on a varying resource;\n|  it can choose to generate a 3xx (redirection) or 4xx (client error)\n|  type response for some requests.\n\n|  In a request message on a varying resource, the selecting request\n|  headers are those request headers whose contents were used by the\n|  origin server to select the entity best matching the request. The\n|  Vary header field specifies the selecting request headers and any\n   other selection parameters that were used by the origin server.\n\n       Vary                 = \"Vary\" \":\" 1#selection-parameter\n\n       selection-parameter  = field-name\n                            | \"{\" \"accept-headers\" \"}\"\n                            | \"{\" \"other\" \"}\"\n                            | \"{\" \"unknown\" \"}\"\n                            | \"{\" extension-parameter \"}\"\n\n       extension-parameter  = token\n\n   The presence of a field-name signals that the request-header field\n   with this name is selecting.  The field-name will usually be, but\n   need not be, a request-header field name defined in this\n   specification.  Note that field names are case-insensitive.  The\n   presence of the \"accept-headers\" parameter signals that all request\n   headers whose names start with \"accept\" are selecting.\n\n+  The inclusion of the \"{other}\" parameter in a Vary field signals\n   that parameters other than the contents of request headers, for\n   example the network address of the sending party, play a role in\n   the selection of the response.\n\n+     Note: This specification allows the origin server to express\n+     that other parameters were used, but does not allow the origin\n+     server to specify the exact nature of these parameters.  This\n+     is left to future extensions.\n\n+  The \"{unknown}\" parameter signals that the origin server is not\n   willing or able to specify the selection parameters used.  If an\n   extension-parameter unknown to the cache is present in a Vary\n+  header, the cache must treat it as the \"{unknown}\" parameter.\n\n+     Note: HTTP/1.1 caches have to treat the \"{other}\" and\n+     \"{unknown}\" parameters in the same way.  For example, presence\n+     of the response header\n+\n+       Vary: accept-language, {other}\n+\n+     requires the same caching behavior as does the presence of \n+    \n+       Vary:  {unknown}\n+     \n+     Use of the \"{unknown}\" parameter is discouraged.  Header fields\n+     which use \"{other}\" are more readable for humans, and better\n+     support the use of heuristics to improve caching performance.\n+\n+\n\n   If multiple Vary and Alternates header fields are present in a\n   response, these must be combined to give all selecting parameters.\n\n+  The field name \"Host\" must never be included into a Vary header;\n|  clients must ignore it if it is present.  The names of fields which\n|  change the semantics of a GET request, like \"Range\" and \"If-Valid\"\n|  must also never be included, and must be ignored when present.  \n\n+  [##Note: Dave Kristol suggested that I change the \"must never be\n+  included\" above to \"must be omitted\", but I think this evokes the\n+  wrong mental model of how servers go about making Vary headers##]\n\n|  Servers which use access authentication are not obliged to send\n|  \"Vary: Authorization\" headers in responses.  It must be assumed\n|  that requests on authenticated resources can always produce\n|  different responses for different users.  Note that servers can\n|  signal the absence of authentication by including a \"Cache-Control:\n|  public\" header in the response.\n\n+  [##Note: the text below could be moved to a separate subsection\n+  elsewhere in the 1.1 draft.  Some of the text at the start of this\n+  section could be repeated at the start of that separate\n+  subsection.##]\n\n   A cache may always store the relayed 200 (OK) responses from a\n   varying resource, and can refresh them according to the rules in\n|  Section aa.bb [##Which will be written by Jeff Mogul##].  The\n|  partial entities in 206 (Partial Content) responses from varying\n|  resources may also be stored.\n\n   When getting a request on a varying resource, a cache can only\n|  return a cached 200 (OK) response to one of its clients in two\n   particular cases.\n\n   First, if a cache gets a request on a varying resource for which it\n   has cached one or more responses with Vary or Alternates headers,\n   it can relay that request towards the origin server, adding an\n+  If-Invalid header listing the cval-info values in the Cval headers\n+  (Section 10.cval) of the cached responses.  If it then gets back a\n+  3xx (Ppp Qqq) [##TBS ##] response with the cval-info of a cached\n   200 (OK) response in its Cval header, it can return this cached 200\n   (OK) response to its client, after merging in any of the 3xx\n   response headers as specified in Section xx.yy [##Which will be\n   written by Jeff Mogul##].\n\n   Second, if a cache gets a request on a varying resource, it can\n   return to its client a cached, fresh 200 (OK) response which has\n   Vary or Alternates headers, provided that\n\n       - the Vary and Alternates headers of this fresh response\n         specify that only request header fields are selecting\n         parameters,\n\n       - the specified selecting request header fields of the current\n         request match the specified selecting request header fields\n         of a previous request on the resource relayed towards the\n         origin server,\n\n       - this previous request got a 200 (OK) or 3xx (Ppp Qqq)\n+        response which had the same cval-info value in its CVal\n         header as the cached, fresh 200 (OK) response.\n\n   Two sequences of selecting request header fields match if and only\n   if the first sequence can be transformed into the second sequence\n   by only adding or removing whitespace at places in fields where\n   this is allowed according to the syntax rules in this\n   specification.\n\n   [##Note that a more complicated matching rule could be defined in a\n   future specification.  The rule above reflects the consensus of the\n   editorial group on how complex we can get in HTTP/1.1##]\n\n|  [##Note that the above rule says sequences, not sets of request\n|  headers.  It cannot say sets because, for some request headers\n|  (like Via?) which contain comma-separated lists, if you have two in\n|  a request, the order in which they appear matters.  A simple\n|  matching rule which would allow some forms of re-shuffling and\n|  collapsing of request headers to get a match turned out to be\n|  beyond my capabilities to write.##]\n\n+  [##Jeff Mogul has made some suggestions for a better matching rule,\n+  though the specification of this rule uses much more text.  A\n+  future version of this text may have a better matching rule in a\n+  separate subsection.##]\n\n|  If a cached 200 (OK) response may be returned to a request on a\n|  varying resource which included Range request header, then a cache\n|  may also use this 200 (OK) response to construct and return a 206\n|  (Partial Content) response with the requested range.\n\n         Note: Implementation of support for the second case above is\n         mainly interesting in user agent caches, as a user agent\n         cache will generally have an easy way of determining whether\n         the sequence of request header fields of the current request\n         equals the sequence sent in an earlier request on the same\n         resource.  Proxy caches supporting the second case would have\n         to record diverse sequences of request header fields\n         previously relayed; the implementation effort associated with\n         this may not be balanced by a sufficient payoff in traffic\n         savings.  A planned specification of a content negotiation\n         mechanism will define additional cases in which proxy caches\n         can return a cached 200 (OK) response without contacting the\n         origin server.  The implementation effort associated with\n         support for these additional cases is expected to have a much\n         better cost/benefit ratio.\n\n  [##Note that the `planned specification of a content negotiation\n  mechanism' above does not necessarily have to be draft-holtman!'  In\n  theory, a content negotiation mechanism totally unlike draft-holtman\n  could just as well live up to these cost/benefit expectations.##]\n\n10.a  Alternates\n\n   The Alternates response-header field is used by origin servers to\n   signal that the resource identified by the request-URI and the Host\n   request header (present if the request-URI is not an absoluteURI)\n   has the capability to send different responses depending on the\n   accept headers in the request message.  This has an important\n   effect on cache management, particularly for caching proxies which\n   service a diverse set of user agents.  This effect is covered in\n   Section 10.v.\n\n       Alternates           = \"Alternates\" \":\" opaque-field\n\n       opaque-field         = field-value\n\n   The Alternates header is included into HTTP/1.1 to make HTTP/1.1\n   caches compatible with a planned content negotiation mechanism.\n   HTTP/1.1 allows a future content negotiation standard to define the\n   format of the Alternates header field-value, as long as the defined\n   format satisfies the general rules in Section 4.2.\n\n   To ensure compatibility with future experimental or standardized\n   software, caching HTTP/1.1 clients must treat all Alternates\n   headers in a response as synonymous to the following Vary header:\n\n         Vary: {accept-headers}\n\n   and follow the caching rules associated with the presence of this\n   Vary header, as covered in Section 10.v.  HTTP/1.1 allows origin\n   servers to send Alternates headers under experimental conditions.\n\n\n10.u  URI\n\n+  [##Note: there used to be text for the URI header here.  It was\n+  removed because Roy Fielding will supply the final text, which will\n+  differ significantly from the text that was here.##]\n+\n\n** II. Changed status code descriptions\n\n300 Multiple Choices\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 300\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\n406 None Acceptable\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 406\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\n\n** III.  New text for the (new) caching section\n\n13.x Interoperability of varying resources with HTTP/1.0 proxy caches\n\n  [## Note: the text in 13.x could be part of a larger subsection in\n  the 1.1 document##]\n\n  If the correct handling of responses from a varying resource\n  (Section 10.v) by HTTP/1.0 proxy caches in the response chain is\n  important, HTTP/1.1 origin servers can include the following Expires\n  (Section 10.exp) response header in all responses from the varying\n  resource:\n\n     Expires: Thu, 01 Jan 1980 00:00:00 GMT\n\n  If this Expires header is included, the server should usually also\n  include a Cache-Control header for the benefit of HTTP/1.1 caches,\n  for example\n\n     Cache-Control: max-age=604800\n\n  which overrides the freshness lifetime of zero seconds specified by\n  the included Expires header.\n\n\n13.y Cache replacement for varying resources\n\n+ [##Note to Jeff: You will have to sync your caching text with the\n+ text below.  I cannot move here without loosing upwards\n+ compatibility, which means that I cannot move at all.##]\n\n  If a new 200 (OK) response is received from a non-varying resource\n  while an old 200 (OK) response is cached, caches can delete this old\n  response from cache memory and insert the new response.  For 200\n  (OK) responses from varying resources (Section 10.v), cache\n  replacement is more complex.\n\n  HTTP/1.1 allows the authors of varying resources to guide cache\n  replacement by the inclusion of elements of so-called replacement\n  keys in the responses of these resources.  The replacement key of a\n  varying response consists of two elements, both of which may be\n  empty strings, separated by a semicolon:\n\n       replacement-key  =  variant-id \";\" absoluteURI\n\n  The variant-id element of the replacement key is the variant-id\n| value in the Cval header of the response, if a Cval header which\n  such a value is present, and an empty string otherwise.  The\n  absoluteURI element of the replacement key is the absolute URI given\n  in, or derived from, the Content-Location header of the response if\n  present, and and an empty string if no Content-Location header is\n  present.\n\n|\n  If a cache has stored in memory a 200 (OK) response with a certain\n  replacement key, and receives, from the same resource, a new 200\n  (OK) response which has the same replacement key, this should be\n  interpreted as a signal from the resource author that the old\n  response can be deleted from cache memory and replaced by the new\n  response.\n\n  The replacement key mechanism cannot cause deletion from cache\n  memory of old responses with replacement keys that will no longer be\n  used.  It is expected that the normal `least recently used'\n  replacement heuristics employed by caches will eventually cause such\n  old responses to be deleted.\n\n| All 200 (OK) responses from varying resources should include\n| replacement key elements.  Resource authors may not assume that\n| caches will be able to cache responses not including replacement key\n| elements.  If a Vary header is used to signal variance, the response\n| should include a variant-id value as the replacement key element.\n| The Content-Location header should only be used to supply a\n| replacement key element if an Alternates header is present in the\n| response.\n\n\n[End of document]\n\n\n\n", "id": "lists-010-1733166"}, {"subject": "Issue: CHARSET, character sets and charse", "content": "I have the issue 'CHARSET'. After the IAB character set workshop, it's\nclear that the entire situation with regard to character sets in IETF\nstandards needs clarification, and that some of that work will happen.\nMy goal here is to keep HTTP from getting tangled up in the mess, by\navoiding controversial terminology or assertions.\n\nSection 3.4 defines the phrase \"character set\" and quotes a definition\nfrom (some) MIME document. On the other hand, the current document\nrarely uses the phrase \"character set\", instead using the more\ntechnical term \"charset\" in most places.\n\nTo simplify things, I make the following proposal. If you don't like\nthis proposal, let me know what you think we should do instead.\n\nI propose changing section 3.4 to be\n\n\n> 3.4 Character Sets and charset\n\n> HTTP uses the concept of a \"charset\" as defined in MIME[ref] to\n> designate a method by which a a sequence of logical characters might\n> be encoded as a sequence of octets and transmitted on the network, and\n> subsequently decoded into a sequence of characters by the recipient.\n\n> In MIME documents, the phrase \"character set\" is used informally to\n> mean the same thing as a \"charset\"; while for simple character\n> mappings such as US-ASCII this is reasonable, for more complex\n> methods, it is not.\n\n> IANA maintains a registry of charset names. This standard establishes\n> a profile of the IANA registry which consists, for each registered\n> charset whether it is intended for use in HTTP and a preferred-case\n> independent token for use in HTTP.\n\n> The following tokens are the initial preferred names to be used within\n> HTTP. This list includes those registered by RFC 1521 [7] -- the\n> US-ASCII [21] and ISO-8859 [22] character sets -- and other names\n> specifically recommended for use within MIME charset parameters.\n\n>   charset = \"US-ASCII\"\n>| \"ISO-8859-1\" | \"ISO-8859-2\" | \"ISO-8859-3\"\n>| \"ISO-8859-4\" | \"ISO-8859-5\" | \"ISO-8859-6\"\n>| \"ISO-8859-7\" | \"ISO-8859-8\" | \"ISO-8859-9\"\n>       | \"ISO-2022-JP\" | \"ISO-2022-JP-2\" | \"ISO-2022-KR\"\n>       | \"UNICODE-1-1-UTF-8\"\n\n\n[** These tokens don't seem to correspond to the tokens used by most\nof the multilingual servers in the world today!!! Why don't we fix\nthem to match? EUC, Shift-JIS, Big5, GB...  It was my impression that\nwe might get IANA to assign a canonical and get out of this buisness.\nWhy can't we do this? **]\n\n[** I note that there was a change suggested that 'the character set\nof an entity body should be labeled as the lowest common denominator\nof the character codes used within that body, with the exception that\nno label is preferred over the labels US-ASCII or ISO-8859-1', and\nobject to this wording as confusing and unnecessary, and wonder why it\nwas put in. **]\n\nSubsequent to this, there is a requirement to search for the phrase\n'character set' and consider its replacement.  In the revised 3.6.1\nCanonicalization and Text Defaults, it borrows the phrase \"character\nset\" but then uses it inconsistently;  as this was a hotly contested\nbit of wording, though, I think we can use 3.6.1 alone.\n\nIn 10.2 \"Accept-Charset\", I suggest changing\n\n< The Accept-Charset request-header can be used to indicate what\n< character sets are acceptable for the response. This field allows\n< clients capable of understanding more comprehensive or special-purpose\n< character sets to signal that capability to a server which is capable\n< of representing documents in those character sets.\n\nto\n\n> The Accept-Charset request-header can be used to indicate which\n> values are acceptable as a charset parameter in any text media type\n> response. This request-header allows clients to indicate their\n> willingness to deal with text in alternate encodings.\" \n\nMove \"The ISO-8859-1 character set can be assumed to be acceptable\nto all user agents\" to the first sentence of the last paragraph, using\n\"charset\" instead of \"character set\".\n\nChange \"Character set values are described in Section 3.4\" to\n\"Charset values are described in Section 3.4\" and \n\nScanning ahead for \"character set\", you will find one more instance in\nthe description of \"qc\"; change the phrase \"character set\" to\n\"charset\".\n\nThat's it.\n\n\n\n", "id": "lists-010-1758088"}, {"subject": "A scary data poin", "content": "Greetings, fellow HTTPers. I just ran across a not-so-heartening chart at\n<http://www.interse.com/webtrends/>. Towards the bottom of the page, there\nis a chart of the ratio of Netscape 1.X browers in use today to Netscape\n2.0. As of March 1, there are still more people running 1.X than 2.0 (58%\nto 42%), and the curves are not steepening.\n\nThis does not bode well for our assumption that deployed Web clients will\nmove to HTTP/1.1 soon after we release the final specs...\n\n\n\n", "id": "lists-010-1769932"}, {"subject": "Re: Issue: CHARSET, character sets and charse", "content": ">>   charset = \"US-ASCII\"\n>>| \"ISO-8859-1\" | \"ISO-8859-2\" | \"ISO-8859-3\"\n>>| \"ISO-8859-4\" | \"ISO-8859-5\" | \"ISO-8859-6\"\n>>| \"ISO-8859-7\" | \"ISO-8859-8\" | \"ISO-8859-9\"\n>>       | \"ISO-2022-JP\" | \"ISO-2022-JP-2\" | \"ISO-2022-KR\"\n>>       | \"UNICODE-1-1-UTF-8\"\n\nListing these charsets by name seems like a slippery slope to me. For\nexample, the brand new RFC 1922 describes ISO-2022-CN and ISO-2022-CN-EXT,\nwhich will probably appear in the IANA registry around the same time we get\nthis spec out, if not sooner.\n\nShould we exclude the Chinese while leaving in the Japanese and Koreans?\nWhy are some charsets \"specifically recommended for use within MIME charset\nparameters.\" These are rhetorical questions, not to be discussed on the\nHTTP WG at this late date. However, I think we might want to come up with a\nmore open, or possibly a much shorter, listing of the \"initial preferred\nnames\".\n\n\n\n", "id": "lists-010-1776636"}, {"subject": "Re: Issue: CHARSET, character sets and charse", "content": "> > The following tokens are the initial preferred names to be used within\n> > HTTP. This list includes those registered by RFC 1521 [7] -- the\n> > US-ASCII [21] and ISO-8859 [22] character sets -- and other names\n> > specifically recommended for use within MIME charset parameters.\n> \n> >   charset = \"US-ASCII\"\n> >| \"ISO-8859-1\" | \"ISO-8859-2\" | \"ISO-8859-3\"\n> >| \"ISO-8859-4\" | \"ISO-8859-5\" | \"ISO-8859-6\"\n> >| \"ISO-8859-7\" | \"ISO-8859-8\" | \"ISO-8859-9\"\n> >       | \"ISO-2022-JP\" | \"ISO-2022-JP-2\" | \"ISO-2022-KR\"\n> >       | \"UNICODE-1-1-UTF-8\"\n> \n> \n> [** These tokens don't seem to correspond to the tokens used by most\n> of the multilingual servers in the world today!!! Why don't we fix\n> them to match? EUC, Shift-JIS, Big5, GB...  It was my impression that\n> we might get IANA to assign a canonical and get out of this buisness.\n> Why can't we do this? **]\n> \n\nIt looks like this is a list of charset values used in other RFCs.\n\nI'd also note that nearly all of them are defined by ISO standards.\n\nIt seems like a adding pragmatic list of national and/or corporate encodings\nin wide use, would be a good thing for interoperability, though\nless nice for standardiztion purists.\n\nI'm not sure who has the expertise to tackle this for a wide range\nof languages though.\n\nWhat's involved in the IANA registration now?\n\nIt might be better to split this off into other internationalization\ndocument(s) so as to not get bogged down.\n\n\n\n", "id": "lists-010-1784958"}, {"subject": "Re: Issue: CHARSET, character sets and charse", "content": ">>>   charset = \"US-ASCII\"\n>>>| \"ISO-8859-1\" | \"ISO-8859-2\" | \"ISO-8859-3\"\n>>>| \"ISO-8859-4\" | \"ISO-8859-5\" | \"ISO-8859-6\"\n>>>| \"ISO-8859-7\" | \"ISO-8859-8\" | \"ISO-8859-9\"\n>>>       | \"ISO-2022-JP\" | \"ISO-2022-JP-2\" | \"ISO-2022-KR\"\n>>>       | \"UNICODE-1-1-UTF-8\"\n> \n> Listing these charsets by name seems like a slippery slope to me. For\n> example, the brand new RFC 1922 describes ISO-2022-CN and ISO-2022-CN-EXT,\n> which will probably appear in the IANA registry around the same time we get\n> this spec out, if not sooner.\n\nThis is a chicken and egg issue.  We wouldn't need a list in HTTP if\nthere were a list of short, charset-friendly, preferred names kept\nby IANA.  However, there isn't.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1794260"}, {"subject": "Re: Issue: CHARSET, character sets and charse", "content": "Everything else looks fine, though it may be editorially difficult\nto make sensible sentences with just \"charset\".\n\n> [** These tokens don't seem to correspond to the tokens used by most\n> of the multilingual servers in the world today!!! Why don't we fix\n> them to match? EUC, Shift-JIS, Big5, GB...  It was my impression that\n> we might get IANA to assign a canonical and get out of this buisness.\n> Why can't we do this? **]\n\nBecause we can't do it until IANA does it [or at least puts in place\nthe initial list and procedures for additions].  Can you work on getting\nthat done via the appropriate channels?\n\n> [** I note that there was a change suggested that 'the character set\n> of an entity body should be labeled as the lowest common denominator\n> of the character codes used within that body, with the exception that\n> no label is preferred over the labels US-ASCII or ISO-8859-1', and\n> object to this wording as confusing and unnecessary, and wonder why it\n> was put in. **]\n\nThat exists (as per a request on the mailing list long ago) to preserve\nbackwards compatibility with 1.0 clients.  It is needed because the\nold MIME specs prefer explicit naming, which breaks many 1.0 systems.\nI don't see what is confusing about it.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1802995"}, {"subject": "Re: Issue: CHARSET, character sets and charse", "content": "> > specifically recommended for use within MIME charset parameters.\n>\n> >   charset = \"US-ASCII\"\n> >     | \"ISO-8859-1\" | \"ISO-8859-2\" | \"ISO-8859-3\"\n> >     | \"ISO-8859-4\" | \"ISO-8859-5\" | \"ISO-8859-6\"\n> >     | \"ISO-8859-7\" | \"ISO-8859-8\" | \"ISO-8859-9\"\n> >       | \"ISO-2022-JP\" | \"ISO-2022-JP-2\" | \"ISO-2022-KR\"\n> >       | \"UNICODE-1-1-UTF-8\"\n>\n>\n> [** These tokens don't seem to correspond to the tokens used by most\n> of the multilingual servers in the world today!!! Why don't we fix\n> them to match? EUC, Shift-JIS, Big5, GB...  It was my impression that\n> we might get IANA to assign a canonical and get out of this buisness.\n> Why can't we do this? **]\n>\n\nI would say that we really can't do much with this, because unless\npeople register the encoding (charset) with IANA, we cannot really\nreference it here. We should allow an arbitrary string to appear here\nas \"extension text\", and that would provide a reasonable loophole for\npeople to use. I think it might be quite a difficult task to come up\nwith a list of all common encodings in use.\n\nSome common encodings used in Asia that are not on this list:\n\n    SHIFT-JIS\n    EUC-JP\n    EUC-GB\n    EUC-KR  \n    EUC-TW\n    BIG5\n    JOHAB\n\n\n\n", "id": "lists-010-1811833"}, {"subject": "(VARY) Removal of `Vary: {unknown}", "content": "I have received several comments to my proposed text for the Vary\nheader indicating that people don't like my current proposed `other'\nand `unknown' parameters, because HTTP/1.1 caches have to treat them\nboth in the same way.\n\nLarry Masinter suggested that I remove `unknown' and keep the\nsemantically equivalent `other'.  This sounds like a good idea to me.\n\nSome background: I included `unknown' because there was an `unknown'\nparameter in the Vary header text by David Robinson.  I cannot rule\nout that there is a good reason, unknown to me, to have `unknown'.  On\nthe other hand, David Robinson may have just included `unknown' as a\nrhetorical device to ease defining the required behavior when\nencountering unknown extension-parameters.\n\nIf you want to keep `unknown', now is the time to speak up.  \n\nI'll be working on the assumption that nobody wants to keep `unknown',\nuntil I get mail from someone who does want to keep it.  The next\nversion of the Vary header text I post will therefore probably remove\n`unknown'.\n\nFor reference, here is the text I have currently on `other' and\n`unknown':\n\n       Vary                 = \"Vary\" \":\" 1#selection-parameter\n\n       selection-parameter  = field-name\n                            | \"{\" \"accept-headers\" \"}\"\n                            | \"{\" \"other\" \"}\"\n                            | \"{\" \"unknown\" \"}\"\n                            | \"{\" extension-parameter \"}\"\n\n       extension-parameter  = token\n\n   [...]\n\n+  The inclusion of the \"{other}\" parameter in a Vary field signals\n   that parameters other than the contents of request headers, for\n   example the network address of the sending party, play a role in\n   the selection of the response.\n\n+     Note: This specification allows the origin server to express\n+     that other parameters were used, but does not allow the origin\n+     server to specify the exact nature of these parameters.  This\n+     is left to future extensions.\n\n+  The \"{unknown}\" parameter signals that the origin server is not\n   willing or able to specify the selection parameters used.  If an\n   extension-parameter unknown to the cache is present in a Vary\n+  header, the cache must treat it as the \"{unknown}\" parameter.\n\n+     Note: HTTP/1.1 caches have to treat the \"{other}\" and\n+     \"{unknown}\" parameters in the same way.  For example, presence\n+     of the response header\n+\n+       Vary: accept-language, {other}\n+\n+     requires the same caching behavior as does the presence of \n+    \n+       Vary:  {unknown}\n+     \n+     Use of the \"{unknown}\" parameter is discouraged.  Header fields\n+     which use \"{other}\" are more readable for humans, and better\n+     support the use of heuristics to improve caching performance.\n+\n+\n\nKoen.\n\n\n\n", "id": "lists-010-1820763"}, {"subject": "Re: Several ContentEncoding", "content": "[David]\n> HTTP/1.0 does not support multiple content-encodings.\n> HTTP/1.1 suggests an implementation of multiple content-encodings which\n> is broken; the ordering of the encodings is not defined. I'm sure I\n> mentioned this on http-wg, but I can't remeber the outcome. Roy?\n\n[Florent]\n> Can Section 4.2 make an exception about ordering for the Content-Encoding\n> headers, or is this too ugly ?\n\nIf we're going to make exceptions, it would be more usual to say that\n'Content-Encoding' can only occur once, and that if there are multiple\nencodings, they must appear ordered and comma separated in a single\nheader.\n\nIn any case, if HTTP/1.1 is going to support multiple\ncontent-encodings, this must be resolved, else we'll revert to the 1.0\nsituation and try again in 1.2.\n\nI'll make sure this gets on the issues list.\n\n\n\n", "id": "lists-010-1830370"}, {"subject": "Re: Several ContentEncoding", "content": "I think the problem here is language. As a general rule the order in\nwhich differently tagged headers appear is not significant, the order in\nwhich tags of the same name appears is significant. A proxy may alter the\norder of differrently named tags but not the order of tags with the same\nname.\n\nThis area impinges on the PEP proposal, PEP is in part intended to solve\nthis problem. Consider a document which is compressed, signed and encrypted\nin that order. It is essential that the transport prserves the information\nthat this is the order in which the transformations occurred.\n\nThe content encoding model is unfortunately limited. I don't think we should\ntry to fix it now. I have a feeling that Jim and Larry would be upset if we\nwere to start.\n\n\nPhill\n\n\n\n", "id": "lists-010-1838506"}, {"subject": "ISO-885910; registration of new charset values; error in MIME draf", "content": "(Replies to this messages should be directed to\nietf-types@uninett.no only.)\n\nLegend:\n> quote from RFC 1521\n: quote from draft-ietf-822ext-mime-reg-03.txt\n% quote from ftp://ftp.isi.edu/in-notes/iana/assignments/character-sets\n/ quote from ISO/IEC 8859-10:1992(E)\n\nRFC 1521 asks for IANA registration of values of the \"charset\"\nparameter:\n\n> 7.1.1.     The charset parameter\n \n>    An initial list of predefined character set names can be found at the\n>    end of this section.  Additional character sets may be registered\n>    with IANA, although the standardization of their use requires the\n>    usual IESG [RFC-1340] review and approval.  Note that if the\n\n> Appendix E -- IANA Registration Procedures\n \n>    MIME has been carefully designed to have extensible mechanisms, and\n>    it is expected that the set of content-type/subtype pairs and their\n>    associated parameters will grow significantly with time.  Several\n>    other MIME fields, notably character set names, access-type\n>    parameters for the message/external-body type, and possibly even\n>    Content-Transfer-Encoding values, are likely to have new values\n>    defined over time.  In order to ensure that the set of such values is\n\nNo registration procedure for character sets is specified in\nRFC 1521, though.\n\nThe current Internet Draft draft-ietf-822ext-mime-reg-03.txt,\n\"Multipurpose Internet Mail Extensions (MIME) Part Four:\nRegistration Procedures\", says:\n\n: Registration of character sets for use in MIME is covered\n: elsewhere and is no longer addressed by this document.\n\nWhich document cover the registration of character sets, then?\n\nThe IANA register file at\n< ftp://ftp.isi.edu/in-notes/iana/assignments/character-sets >\nstates:\n\n% These are the official names for character sets that may be used in\n% the Internet and may be referred to in Internet documentation.\n\nMany of the registered names have been taken from the\ninformational RFC 1345. Is it necessary to write an RFC to get a\nnew character set registered? Is it sufficient to do that?\n\nI'm asking these questions, since I've noticed that part 10 of\nISO 8859 is not included in the IANA registry. That standard was\npublished in 1992:\n\n   ISO/IEC 8859-10:1992(E)\n   Information technology -- 8-bit single-byte coded graphic\n      character sets -- Part 10: Latin alphabet No. 6\n   International Organization for Standardization, 1992-12-15\n\nrom the Scope:\n\n/ This [coded character] set is suited for multiple-language\n/ applications involving Danish, English, Estonian, Finnish,\n/ German, Greenlandic, Icelandic, Sami (Lappish), Latvian,\n/ Lithuanian, Norwegian, Faroese, and Swedish.\n\nRFC 1521 _can_ be read as allowing the use of the \"ISO-8859-10\"\nvalue, even without it being included into the IANA registry:\n\n> 7.1.1.     The charset parameter\n\n>    An initial list of predefined character set names can be found at the\n>    end of this section.  Additional character sets may be registered\n\n>    The defined charset values are:\n>  \n>    US-ASCII -- as defined in [US-ASCII].\n>  \n>         ISO-8859-X -- where \"X\" is to be replaced, as necessary, for the\n>              parts of ISO-8859 [ISO-8859].  Note that the ISO 646\n>              character sets have deliberately been omitted in favor of\n>              their 8859 replacements, which are the designated character\n>              sets for Internet mail.  As of the publication of this\n>              document, the legitimate values for \"X\" are the digits 1\n>              through 9.\n \n>    No other character set name may be used in Internet mail without the\n>    publication of a formal specification and its registration with IANA,\n\nThe statement about legitimate values for \"X\" at the time of the\npublication of RFC 1521 is false. It was published in September\n1993, when the ISO 8859-10 stadnard was 9 months old.\nUnfortunately, this statement hasn't yet been changed in the\nlatest Internet Draft draft-ietf-822ext-mime-imt-04.txt. (I'm\nsure this is due to oversight, not to ignorance.)\n\nLet me also point out that four new parts of ISO 8859 are in the\nISO pipeline:\n\nISO 8859-11: Latin/Thai alphabet\nISO 8859-12: Latin/Devanagari alphabet\nISO 8859-13: Latin alphabet No. 7 (Baltic Rim)\nISO 8859-14: Latin alphabet No. 8 (Celtic)\n\nIt's possible that Latin/Devanagari and Latin (Celtic) will\nappear interchanged in the final part numbering.\n\nI would appreciate any clarification of these questions about\nIETF character set registration.\n\n-- \nOlle Jarnefors, Royal Institute of Technology (KTH) <ojarnef@admin.kth.se>\n\n\n\n", "id": "lists-010-1846468"}, {"subject": "INTEGOK; CONTENT&ndash;MD5 &ndash;&ndash; consensu", "content": "The modification to the last two sentences of the section leaves no\nobjection unmet at this time, so consensus is declared for the next\ndraft.\n\n--------------------------\n10.13Content-MD5\n\nThe Content-MD5 entity-header field is an MD5 digest of the entity-body,\nas defined in RFC 1864 [xx], for the purpose of providing an end-to-end\nmessage integrity check (MIC) of the entity-body. (Note: an MIC is good\nfor detecting accidental modification of the entity-body in transit, but\nis not proof against malicious attacks.)\n\nContentMD5= \"Content-MD5\" \":\" md5-digest\nmd5-digest= <base64 of 128 bit MD5 digest as per RFC 1864>\n\nThe Content-MD5 header may be generated by an origin server to function\nas an integrity check of the entity-body. Only origin-servers may\ngenerate the Content-MD5 header field; proxies and gateways MUST NOT\ngenerate it, as this would defeat its value as an end-to-end integrity\ncheck. Any recipient of the entity-body, including gateways and proxies,\nMAY check that the digest value in this header field matches that of the\nentity-body as received. \n\nThe MD5 digest is computed based on the content of the entity body,\nincluding any Content-Encoding that has been applied, but not including\nany Transfer-Encoding.  If the entity is received with a\nTransfer-Encoding, that encoding must be removed prior to checking the\nContent-MD5 value against the received entity.\n\nThis has the result that the digest is computed on the octets of the\nentity body exactly as, and in the order that, they would be sent if no\nTransfer-Encoding were being applied.\n\nHTTP extends RFC 1864 to permit the digest to be computed for MIME\ncomposite media-types (e.g., multipart/* and message/rfc822), but this\ndoes not change how the digest is computed as defined in the preceding\nparagraph. \n\n   Note: There are several consequences of this. The entity-body for\n   composite types many contain many body-parts, each with its own MIME\n   and HTTP headers (including Content-MD5, Content-Transfer-Encoding,\n   and Content-Encoding headers). If a body-part has a\n   Content-Transfer-Encoding or Content-Encoding header, it is assumed\n   that the content of the body-part has had the encoding applied,\n   and the body-part is included in the Content-MD5 digest as\n   is -- i.e., after the application. Also, the HTTP Transfer-Encoding\n   header makes no sense within body-parts; if it is present, it is\n   ignored -- i.e. treated as ordinary text.\n\n   Note: while the definition of Content-MD5 is exactly\n   the same for HTTP as in RFC 1864 for MIME entity-bodies,\n   there are several ways in which the application of\n   Content-MD5 to HTTP entity-bodies differs from its\n   application to MIME entity-bodies. One is that HTTP,\n   unlike MIME, does not use Content-Transfer-Encoding,\n   and does use Transfer-Encoding and Content-Encoding.\n   Another is that HTTP more frequently uses binary content types\n   than MIME, so it is worth noting that in such cases,\n   the byte order used to compute the digest is the\n   transmission byte order defined for the type. Lastly,\n   HTTP allows transmission of text types with any of\n   several line break conventions and not just the canonical\n   form using CR-LF. Conversion of all line breaks\n   to CR-LF should not be done before computing or checking\n   the digest: the line break convention used in the text\n   actually transmitted should be left unaltered when\n   computing the digest.\n\n\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-1859788"}, {"subject": "(CONTENT NEGOTIATION,VARY) Updated draft text for Vary header and content negotiation `hooks", "content": "The text below is an updated version of the text I posted on Tuesday\n(April 9).  Since then, Larry Masinter made some comments, which led\nto some changes in the text.  I made some other changes to prepare for\nintegration in the main 1.1 document.  Changes with respect to the\nTuesday version are marked with | change bars.\n\nIt expected that the text below will have to be changed in some places\nto synchronize with the planned caching text by Jeff Mogul.  Places\nwere changes are expected are marked with notes.  According to the\ncurrent schedule of the editorial group, the text below will first be\nincorporated (possibly with stylistic improvements) into a first rough\nversion of the 1.1 draft.  After that, additional changes will be\nmade.\n\nI believe there to be rough consensus on all mechanisms outlined\nbelow, with the exception of the described mechanism for generating an\nIf-Invalid header.  If you disagree, please speak up as soon as\npossible.  There is no rough consensus on the specific words used\nbelow; further edits to the words are expected.\n\nKoen.\n\n--snip--\n\n\n** I. Vary+content negotiation new/changed header descriptions\n\n   [##Note: The current section 12 needs to be deleted completely from\n   the April 1.1 draft.##]\n\n10.v  Vary\n\n|  [##Note: With respect to the last 1.1 draft, the Vary header text is\n|  completely new, it replaces no existing text##]\n\n   [##This Vary section mixes the description of a header with a\n   description of rules for caching.   These two things could be\n   separated in the final 1.1 document, with the rules for caching\n   being integrated in the caching section.##]\n\n|\n\n   The Vary response-header field is used by an origin server to\n   signal that the resource identified by the current request is a\n   varying resource.  A varying resource has multiple entities\n   associated with it, all of which are representations of the content\n   of the resource.  If a GET or HEAD request on a varying resource is\n   received, the origin server will select one of the associated\n   entities as the entity best matching the request.  Selection of\n   this entity is based on the contents of particular header fields in\n   the request message, or on other information pertaining to the\n   request, like the network address of the sending client.\n\n   If a resource is varying, this has an important effect on cache\n   management, particularly for caching proxies which service a\n   diverse set of user agents.  All 200 (OK) responses from varying\n   resources must contain at least one Vary header or Alternates\n   header (Section 10.a) to signal variance.\n\n   If no Vary headers and no Alternates headers are present in a 200\n   (OK) response, then caches may assume, as long as the response is\n   fresh, that the resource in question is not varying, and has only\n   one associated entity.  Note however that this entity can still\n   change through time, as possibly indicated by a Cache-Control\n   response header (section 10.cc).\n\n   After selection of the entity best matching the current request,\n   the origin server will usually generate a 200 (OK) response, but it\n   can also generate other responses like 206 (Partial Content) or 304\n   (Not modified) if headers which modify the semantics of the\n   request, like Range (Section 10.ran) or If-Valid (Section 10.ifva),\n   are present.  An origin server need not be capable of selecting an\n   entity for every possible incoming request on a varying resource;\n   it can choose to generate a 3xx (redirection) or 4xx (client error)\n   type response for some requests.\n\n   In a request message on a varying resource, the selecting request\n   headers are those request headers whose contents were used by the\n   origin server to select the entity best matching the request. The\n   Vary header field specifies the selecting request headers and any\n   other selection parameters that were used by the origin server.\n\n       Vary                 = \"Vary\" \":\" 1#selection-parameter\n\n|      selection-parameter  = request-header-name\n|                           | \"{accept-headers}\"\n|                           | \"{other}\"\n|                           | \"{\" extension-parameter \"}\"\n\n|      request-header-name  = field-name\n\n       extension-parameter  = token\n\n|  The presence of a request-header-name signals that the\n|  request-header field with this name is selecting.  Note that the\n|  name need not belong to a request-header field defined in this\n|  specification, and that header names are case-insensitive.  The\n|  presence of the \"{accept-headers}\" parameter signals that all request\n   headers whose names start with \"accept\" are selecting.\n\n   The inclusion of the \"{other}\" parameter in a Vary field signals\n   that parameters other than the contents of request headers, for\n   example the network address of the sending party, play a role in\n   the selection of the response.\n\n      Note: This specification allows the origin server to express\n      that other parameters were used, but does not allow the origin\n      server to specify the exact nature of these parameters.  This\n      is left to future extensions.\n\n|  If an extension-parameter unknown to the cache is present in a Vary\n|  header, the cache must treat it as the \"{other}\" parameter.\n   If multiple Vary and Alternates header fields are present in a\n   response, these must be combined to give all selecting parameters.\n\n   The field name \"Host\" must never be included into a Vary header;\n   clients must ignore it if it is present.  The names of fields which\n   change the semantics of a GET request, like \"Range\" and \"If-Valid\"\n   must also never be included, and must be ignored when present.  \n\n   [##Note: Dave Kristol suggested that I change the \"must never be\n   included\" above to \"must be omitted\", but I think this evokes the\n   wrong mental model of how servers go about making Vary headers##]\n\n   Servers which use access authentication are not obliged to send\n   \"Vary: Authorization\" headers in responses.  It must be assumed\n   that requests on authenticated resources can always produce\n   different responses for different users.  Note that servers can\n   signal the absence of authentication by including a \"Cache-Control:\n   public\" header in the response.\n\n   [##Note: the text below could be moved to a separate subsection\n   elsewhere in the 1.1 draft.  Some of the text at the start of this\n   section could be repeated at the start of that separate subsection.\n|  I advise against the making of such editing moves in the first\n|  rough version of the 1.1 draft##]\n\n|  A cache may store and refresh 200 (OK) responses from a varying\n|  resource according to the rules in Section aa.bb [##meant are the\n|  rules for storing (and when this is disallowed) and refreshing\n|  which are to be written by Jeff Mogul##].  The partial entities in\n|  206 (Partial Content) responses from varying resources may also be\n|  used by the cache.\n\n   When getting a request on a varying resource, a cache can only\n   return a cached 200 (OK) response to one of its clients in two\n   particular cases.\n\n|  [#Note: the paragraph below is likely to change.  Note that this\n|  paragraph talks about the construction mechanism of the If-Invalid\n|  header, the only thing for which I do _not_ detect rough consensus\n|  yet.  Also, this paragraph contains some caching terminology that\n|  may have to be adjusted to the caching text by Jeff Mogul.##]\n\n   First, if a cache gets a request on a varying resource for which it\n   has cached one or more responses with Vary or Alternates headers,\n   it can relay that request towards the origin server, adding an\n   If-Invalid header listing the cval-info values in the Cval headers\n   (Section 10.cval) of the cached responses.  If it then gets back a\n|  3xx (Ppp Qqq) [##TBS in the text by Jeff Mogul##] response with\n   the cval-info of a cached 200 (OK) response in its Cval header, it\n   can return this cached 200 (OK) response to its client, after\n   merging in any of the 3xx response headers as specified in Section\n   xx.yy [##Which will be written by Jeff Mogul##].\n\n   Second, if a cache gets a request on a varying resource, it can\n   return to its client a cached, fresh 200 (OK) response which has\n   Vary or Alternates headers, provided that\n\n       - the Vary and Alternates headers of this fresh response\n         specify that only request header fields are selecting\n         parameters,\n\n       - the specified selecting request header fields of the current\n         request match the specified selecting request header fields\n         of a previous request on the resource relayed towards the\n         origin server,\n\n       - this previous request got a 200 (OK) or 3xx (Ppp Qqq)\n         response which had the same cval-info value in its CVal\n         header as the cached, fresh 200 (OK) response.\n\n|  [#Note: the paragraph below is likely to change.  We know what a\n|  better matching rule (one which allows a match more often) should\n|  look like, and there would be no objections to improving the rule\n|  below, but good words remain to be written.##]\n\n   Two sequences of selecting request header fields match if and only\n   if the first sequence can be transformed into the second sequence\n   by only adding or removing whitespace at places in fields where\n   this is allowed according to the syntax rules in this\n   specification.\n\n   [##Note that a more complicated matching rule could be defined in a\n   future specification.  The rule above reflects the consensus of the\n   editorial group on how complex we can get in HTTP/1.1##]\n\n   [##Note that the above rule says sequences, not sets of request\n   headers.  It cannot say sets because, for some request headers\n   (like Via?) which contain comma-separated lists, if you have two in\n   a request, the order in which they appear matters.  A simple\n   matching rule which would allow some forms of re-shuffling and\n   collapsing of request headers to get a match turned out to be\n   beyond my capabilities to write.##]\n\n   [##Jeff Mogul has made some suggestions for a better matching rule,\n   though the specification of this rule uses much more text.  A\n   future version of this text may have a better matching rule in a\n   separate subsection.##]\n\n   If a cached 200 (OK) response may be returned to a request on a\n   varying resource which included Range request header, then a cache\n   may also use this 200 (OK) response to construct and return a 206\n   (Partial Content) response with the requested range.\n\n         Note: Implementation of support for the second case above is\n         mainly interesting in user agent caches, as a user agent\n         cache will generally have an easy way of determining whether\n         the sequence of request header fields of the current request\n         equals the sequence sent in an earlier request on the same\n         resource.  Proxy caches supporting the second case would have\n         to record diverse sequences of request header fields\n         previously relayed; the implementation effort associated with\n         this may not be balanced by a sufficient payoff in traffic\n         savings.  A planned specification of a content negotiation\n         mechanism will define additional cases in which proxy caches\n         can return a cached 200 (OK) response without contacting the\n         origin server.  The implementation effort associated with\n         support for these additional cases is expected to have a much\n         better cost/benefit ratio.\n\n  [##Note that the `planned specification of a content negotiation\n  mechanism' above does not necessarily have to be draft-holtman!'  In\n  theory, a content negotiation mechanism totally unlike draft-holtman\n  could just as well live up to these cost/benefit expectations.##]\n\n10.a  Alternates\n\n|  [##Note: With respect to the last 1.1 draft, the Alternates header\n|  text is completely new, it replaces no existing text##]\n\n   The Alternates response-header field is used by origin servers to\n   signal that the resource identified by the request-URI and the Host\n   request header (present if the request-URI is not an absoluteURI)\n   has the capability to send different responses depending on the\n   accept headers in the request message.  This has an important\n   effect on cache management, particularly for caching proxies which\n   service a diverse set of user agents.  This effect is covered in\n   Section 10.v.\n\n       Alternates           = \"Alternates\" \":\" opaque-field\n\n       opaque-field         = field-value\n\n   The Alternates header is included into HTTP/1.1 to make HTTP/1.1\n   caches compatible with a planned content negotiation mechanism.\n   HTTP/1.1 allows a future content negotiation standard to define the\n   format of the Alternates header field-value, as long as the defined\n   format satisfies the general rules in Section 4.2.\n\n   To ensure compatibility with future experimental or standardized\n   software, caching HTTP/1.1 clients must treat all Alternates\n   headers in a response as synonymous to the following Vary header:\n\n         Vary: {accept-headers}\n\n   and follow the caching rules associated with the presence of this\n   Vary header, as covered in Section 10.v.  HTTP/1.1 allows origin\n   servers to send Alternates headers under experimental conditions.\n\n\n10.u  URI\n\n|  [##Note: As soon as possible, the URI header section from the old\n|  1.1 draft should be deleted in the new draft, and be replaced with\n|  `TBS'.  Roy Fielding will supply the new text for the URI header\n|  section, which will differ significantly from the text in the old\n|  1.1 draft.##]\n\n|\n|\n\n\n** II. Changed status code descriptions\n\n300 Multiple Choices\n\n|  [##Note: the text below should replace 300 header text in the old\n|  1.1 draft.##]\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 300\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\n406 None Acceptable\n\n|  [##Note: the text below should replace 406 header text in the old\n|  1.1 draft##]\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 406\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\n\n** III.  New text for the (new) caching section\n\n13.x Interoperability of varying resources with HTTP/1.0 proxy caches\n\n| [##Note: With respect to the last 1.1 draft, the text below is\n| completely new, it replaces no existing text. This text should be\n| added to the caching section of the new 1.1 draft.##]\n|\n\n  If the correct handling of responses from a varying resource\n  (Section 10.v) by HTTP/1.0 proxy caches in the response chain is\n  important, HTTP/1.1 origin servers can include the following Expires\n  (Section 10.exp) response header in all responses from the varying\n  resource:\n\n     Expires: Thu, 01 Jan 1980 00:00:00 GMT\n\n  If this Expires header is included, the server should usually also\n  include a Cache-Control header for the benefit of HTTP/1.1 caches,\n  for example\n\n     Cache-Control: max-age=604800\n\n  which overrides the freshness lifetime of zero seconds specified by\n  the included Expires header.\n\n\n13.y Cache replacement for varying resources\n\n| [##Note: With respect to the last 1.1 draft, the text below is\n| completely new, it replaces no existing text.  This text should be\n| added to the caching section of the new 1.1 draft.##]\n\n| [##Note: The Content-Location header mentioned below was not in the\n| old 1.1 draft.  I believe Roy Fielding will supply a section\n| describing this header for the new draft.  This new Content-Location\n| section will be very similar to the section for the Location header\n| in the old 1.1 draft.  Note that the old Location header section\n| will not be removed in the new draft.##]\n\n| [##Note to Jeff Mogul: You will have to sync your caching text with\n  the text below.  I cannot move here without loosing upwards\n  compatibility, which means that I cannot move at all.##]\n\n  If a new 200 (OK) response is received from a non-varying resource\n  while an old 200 (OK) response is cached, caches can delete this old\n  response from cache memory and insert the new response.  For 200\n  (OK) responses from varying resources (Section 10.v), cache\n  replacement is more complex.\n\n  HTTP/1.1 allows the authors of varying resources to guide cache\n  replacement by the inclusion of elements of so-called replacement\n  keys in the responses of these resources.  The replacement key of a\n  varying response consists of two elements, both of which may be\n  empty strings, separated by a semicolon:\n\n       replacement-key  =  variant-id \";\" absoluteURI\n\n  The variant-id element of the replacement key is the variant-id\n  value in the Cval header of the response, if a Cval header which\n  such a value is present, and an empty string otherwise.  The\n  absoluteURI element of the replacement key is the absolute URI given\n  in, or derived from, the Content-Location header of the response if\n  present, and and an empty string if no Content-Location header is\n  present.\n\n  If a cache has stored in memory a 200 (OK) response with a certain\n  replacement key, and receives, from the same resource, a new 200\n  (OK) response which has the same replacement key, this should be\n  interpreted as a signal from the resource author that the old\n  response can be deleted from cache memory and replaced by the new\n  response.\n\n  The replacement key mechanism cannot cause deletion from cache\n  memory of old responses with replacement keys that will no longer be\n  used.  It is expected that the normal `least recently used'\n  replacement heuristics employed by caches will eventually cause such\n  old responses to be deleted.\n\n  All 200 (OK) responses from varying resources should include\n  replacement key elements.  Resource authors may not assume that\n  caches will be able to cache responses not including replacement key\n  elements.  If a Vary header is used to signal variance, the response\n  should include a variant-id value as the replacement key element.\n  The Content-Location header should only be used to supply a\n  replacement key element if an Alternates header is present in the\n  response.\n\n\n[End of document]\n\n\n\n", "id": "lists-010-1871135"}, {"subject": "Several ContentEncoding", "content": "The Apache mailing list stumbled on this problem when discussing\nmultiple Content-Encodings in a document.\n\n[David]\n> HTTP/1.0 does not support multiple content-encodings.\n> HTTP/1.1 suggests an implementation of multiple content-encodings which\n> is broken; the ordering of the encodings is not defined. I'm sure I\n> mentioned this on http-wg, but I can't remeber the outcome. Roy?\n\nThat is, the ordering *is* defined if it appears in a single\nContent-Encoding header, but the spec leaves some margin if\nyou have several ones that you want to collapse.\n\ndraft-ietf-http-v11-spec-01.txt says\n  [section 10.10]\n   If multiple encodings have been applied to a resource, the content\n   codings must be listed in the order in which they were applied.\n\nBut, as Brian pointed out, it also says\n\n  [section 4.2]\n   The order in which header fields are received is not significant.\nand\n   It must be possible to combine the multiple header fields into one\n   \"field-name: field-value\" pair, without changing the semantics of\n   the message, by appending each subsequent field-value to the first,\n   each separated by a comma.\n\nWhich is clearly broken, the semantics *is* changed.\n\nHas this been resolved/discussed ?\nCan Section 4.2 make an exception about ordering for the Content-Encoding\nheaders, or is this too ugly ?\n\n-- Florent\n\n\n\n", "id": "lists-010-1897214"}, {"subject": "Re: Several ContentEncoding", "content": "> That is, the ordering *is* defined if it appears in a single\n> Content-Encoding header, but the spec leaves some margin if\n> you have several ones that you want to collapse.\n\nI wrongly assumed that, because the two statements were right next\nto each other, readers would understand that the second is an exception\nto the first.  However, seeing the mass of confusion even among people\nwho understand the protocol, I suggest the following wording change\nfrom draft 01, section 4.2:\n\n================================================================\n*** draft-ietf-http-v11-spec-01.txtMon Feb 12 16:37:14 1996\n--- order.txtFri Apr 12 08:03:29 1996\n***************\n*** 1464,1483 ****\n  \n!    The order in which header fields are received is not significant. \n!    However, it is \"good practice\" to send General-Header fields first, \n!    followed by Request-Header or Response-Header fields prior to the \n!    Entity-Header fields.\n  \n     Multiple HTTP-header fields with the same field-name may be present \n     in a message if and only if the entire field-value for that header \n     field is defined as a comma-separated list [i.e., #(values)]. It \n     must be possible to combine the multiple header fields into one \n     \"field-name: field-value\" pair, without changing the semantics of \n     the message, by appending each subsequent field-value to the first, \n!    each separated by a comma.\n  \n--- 1464,1485 ----\n  \n!    The order in which header fields with differing field names are\n!    received is not significant. However, it is \"good practice\" to send\n!    General-Header fields first, followed by Request-Header or\n!    Response-Header fields, and ending with the Entity-Header fields.\n  \n     Multiple HTTP-header fields with the same field-name may be present \n     in a message if and only if the entire field-value for that header \n     field is defined as a comma-separated list [i.e., #(values)]. It \n     must be possible to combine the multiple header fields into one \n     \"field-name: field-value\" pair, without changing the semantics of \n     the message, by appending each subsequent field-value to the first, \n!    each separated by a comma.  Thus, the order in which multiple header\n!    fields with the same field-name are received may be significant to\n!    the interpretation of the combined field-value.\n  \n================================================================\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-1905448"}, {"subject": "Persistent Connection", "content": "Hi all,\n\nWe have been working quite a bit on the Persistent connection draft in order\nto get some material to edit into the 1.1 overall draft. Currently the\n\"latest\" Persistent Connections draft can be found at:\n\nhttp://ugly.resnova.com/ietf/draft-ietf-http-ses-ext-05.txt\n\nThe above draft is being used to collect the consensus on this issue, so I\nintend to continue to maintain it. I have not submitted it to the ID editor\nlately as it has been under fairly rapid revision.\n\nPlease read it NOW and send me specific comments. We are at the 11th hour\nhere, so please keep it specific.\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n", "id": "lists-010-1915142"}, {"subject": "Digest Access Authentication Draf", "content": "Comments are requested on the attached draft for Digest Authentication\nfor use with HTTP/1.1.  The desire to keep this draft on the same time\nschedule as HTTP/1.1 and the hope that it can be incorporated as part\nof HTTP/1.1 requires responses (to this list) within a week.\n\n\nThe digest authentication scheme described in this document suffers\nfrom many known limitations.  It is intended as a replacement for\nbasic authentication and nothing more.  Basic authentication, which\nis currently in wide use, sends passwords in clear text.  This is\nserious problem which needs to be rectified as quickly as possible.\nRectifying quickly and painlessly  is the prime objective of digest\nauthentication.\n\nUsers and implementors should be aware that this protocol is not as\nsecure as kerberos, and not as secure as any client-side private-key\nscheme.  Nertheless it is better than nothing, better than what is\ncommonly used with telnet and ftp and better than Basic authentication.\n\n\nSuggestions for changes and improvements are welcomed but will be \njudged in light of the following factors.\n\nPLEASE CONSIDER THESE FACTORS BEFORE SUGGESTING CHANGES:\n\n1.  The objective of digest authentication is NOT to produce the\n  strongest possible authentication scheme or even a particularly \n  strong one.  The objective is to produce the simplest and easiest\n  to implement scheme which does not transmit user passwords in clear\n  text and which can quickly replace Basic authentication.\n\n2.  Commercial implementations of the non-optional parts of this \n  scheme have been in existence and widely deployed for some time.\n  Sigficant changes at this point will break many browsers and servers.\n  Is your suggested change of sufficient importance to outweigh this\n  fact?\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n-------------------------------------------------------------------------\n\nHTTP Working Group                                  Jeffery L. Hostetler\nINTERNET-DRAFT                                               John Franks\n<draft-ietf-http-digest-aa-04.txt>                   Philip Hallam-Baker\n                                                             Paul  Leach\n                                                            Ari Luotonen\n                                                            Eric W. Sink\n                                                     Lawrence C. Stewart\nExpires SIX MONTHS FROM--->                                ????, 1996\n\n\n         A Proposed Extension to HTTP : Digest Access Authentication\n\n\nStatus of this Memo\n\n  This document is an Internet-Draft. Internet-Drafts are working\n  documents of the Internet Engineering Task Force (IETF), its areas,\n  and its working groups. Note that other groups may also distribute\n  working documents as Internet-Drafts.\n\n  Internet-Drafts are draft documents valid for a maximum of six months\n  and may be updated, replaced, or obsoleted by other documents at any\n  time. It is inappropriate to use Internet-Drafts as reference\n  material or to cite them other than as \"work in progress.\"\n\n  To learn the current status of any Internet-Draft, please check the\n  \"1id-abstracts.txt\" listing contained in the Internet-Drafts Shadow\n  Directories on ds.internic.net (US East Coast), nic.nordu.net\n  (Europe), ftp.isi.edu (US West Coast), or munnari.oz.au (Pacific\n  Rim).\n\n  Distribution of this document is unlimited. Please send comments to\n  the proposed HTTP working group at <http-wg@cuckoo.hpl.hp.com>.\n  Discussions of the working group are archived at\n  <URL:http://www.ics.uci.edu/pub/ietf/http/>. General discussions\n  about HTTP and the applications which use HTTP should take place on\n  the <www-talk@www10.w3.org> mailing list.\n\n\nAbstract\n\n  The protocol referred to as \"HTTP/1.0\" includes specification for a\n  Basic Access Authentication scheme.  This scheme is not considered to\n  be a secure method of user authentication, as the user name and\n  password are passed over the network in an unencrypted form.  A\n  specification for a new authentication scheme is needed for future\n  versions of the HTTP protocol.  This document provides specification\n  for such a scheme, referred to as \"Digest Access Authentication\".\n  The encryption method used by default is the RSA Data Security, Inc.\n  MD5 Message-Digest Algorithm [3].\n\n\n\n\nTable of Contents\n\nSTATUS OF THIS MEMO....................................................1\n\n\nABSTRACT...............................................................1\n\n\nTABLE OF CONTENTS......................................................2\n\n\nINTRODUCTION...........................................................2\n\n 1.1  PURPOSE .........................................................\n 1.2  OVERALL OPERATION ...............................................\n 1.3  REPRESENTATION OF DIGEST VALUES .................................\n 1.4  LIMITATIONS .....................................................\n\n2. DIGEST ACCESS AUTHENTICATION SCHEME.................................\n\n 2.1 SPECIFICATION OF DIGEST HEADERS ..................................\n  2.1.1 THE WWW-AUTHENTICATE RESPONSE HEADER ..........................\n  2.1.2 THE AUTHORIZATION REQUEST HEADER ..............................\n  2.1.3 THE AUTHENTICATION-INFO HEADER ................................\n 2.2 DIGEST OPERATION .................................................\n 2.3 SECURITY PROTOCOL NEGOTIATION ....................................\n 2.4 EXAMPLE ..........................................................\n 2.5 PROXY-AUTHENTICATION AND PROXY-AUTHORIZATION .....................\n\n3. SECURITY CONSIDERATIONS............................................\n\n 3.1 COMPARISON WITH BASIC AUTHENTICATION ............................\n 3.2 REPLAY ATTACKS ..................................................\n 3.3 MAN IN THE MIDDLE ...............................................\n 3.4 SPOOFING BY COUNTERFEIT SERVERS .................................\n 3.5 STORING PASSWORDS ...............................................\n 3.6 SUMMARY .........................................................\n\n4.  ACKNOWLEDGMENTS...................................................\n\n\n5. REFERENCES.........................................................\n\n\n6. AUTHORS ADDRESSES..................................................\n\n\n\nIntroduction\n\n\n1.1  Purpose\n\n  The protocol referred to as \"HTTP/1.0\" includes specification for a\n  Basic Access Authentication scheme[1].  This scheme is not considered\n  to be a secure method of user authentication, as the user name and\n  password are passed over the network in an unencrypted form.  A\n  specification for a new authentication scheme is needed for future\n  versions of the HTTP protocol.  This document provides specification\n  for such a scheme, referred to as \"Digest Access Authentication\".\n\n  The Digest Access Authentication scheme is not intended to be a\n  complete answer to the need for security in the World Wide Web. This\n  scheme provides no encryption of object content.  The intent is\n  simply to facilitate secure access authentication.\n\n  It is proposed that this access authentication scheme be included in\n  the proposed HTTP/1.1 specification.\n\n\n1.2  Overall Operation\n\n  Like Basic Access Authentication, the Digest scheme is based on a\n  simple challenge-response paradigm.  The Digest scheme challenges\n  using a nonce value.  A valid response contains a checksum (by\n  default the MD5 checksum) of the username, the password, the given\n  nonce value, and the requested URI.  In this way, the password is\n  never sent in the clear.  Just as with the Basic scheme, the username\n  and password must be prearranged in some fashion which is not\n  addressed by this document.\n\n\n1.3  Representation of digest values\n\n  An optional header allows the server to specify the algorithm used to\n  create the checksum or digest.  By default the MD5 algorithm is used\n  and that is the only algorithm described in this document.\n\n  For the purposes of this document, an MD5 digest of 128 bits is\n  represented as 32 ASCII printable characters.  The bits in the 128\n  bit digest are converted from most significant to least significant\n  bit, four bits at a time to their ASCII presentation as follows.\n  Each four bits is represented by its familiar hexadecimal notation\n  from the characters 0123456789abcdef.  That is binary 0000 gets\n  represented by the character '0', 0001, by '1', and so on up to the\n  representation of 1111 as 'f'.\n\n\n1.4  Limitations\n\n  The digest authentication scheme described in this document suffers\n  from many known limitations.  It is intended as a replacement for\n  basic authentication and nothing more.  It is a password-based system\n  and (on the server side) suffers from all the same problems of any\n  password system.  In particular no provision is made in this protocol\n  for the initial secure arrangement between user and server\n  establishing the user's password.\n\n  Users and implementors should be aware that this protocol is not as\n  secure as kerberos, and not as secure as any client-side private-key\n  scheme.  Nertheless it is better than nothing, better than what is\n  commonly used with telnet and ftp and better than Basic\n  authentication.\n\n  Some keyword-value pairs occurring in headers described below are\n  required to have values which are of the type \"quoted-string\" as\n  defined in section 2.2 of the HTTP/1.1 specification [2].  A\n  consequence is that these values represent strings in the US-ASCII\n  character set.  An unfortunate side effect of this is that digest\n  authentication is not capable of handling either user names or realm\n  names (see 2.1.1 below) which are not expressed in this character set.\n\n\n\n2. Digest Access Authentication Scheme\n\n\n2.1 Specification of Digest Headers\n\n  The Digest Access Authentication scheme is conceptually similar to\n  the Basic scheme.  The formats of the modified WWW-Authenticate\n  header line and the Authorization header line are specified below,\n  using the extended BNF defined in the HTTP/1.1 specification, section\n  2.1.  In addition, a new header, Authentication-info, is specified.\n\n\n\n2.1.1 The WWW-Authenticate Response Header\n\n  If a server receives a request for an access-protected object, and an\n  acceptable Authorization header is not sent, the server responds with\n  a \"401 Unauthorized\" status code, and a WWW-Authenticate header,\n  which is defined as follows:\n\n     WWW-Authenticate    = \"WWW-Authenticate\" \":\" \"Digest\"\n                              digest-challenge\n\n     digest-challenge    = 1#( realm | [ domain ] | nonce |\n                          [ digest-opaque ] |[ stale ] | [ algorithm ] )\n\n     realm               = \"realm\" \"=\" realm-value\n     realm-value         = quoted-string\n     domain              = \"domain\" \"=\" <\"> 1#URI <\">\n     nonce               = \"nonce\" \"=\" nonce-value\n     nonce-value         = quoted-string\n     opaque              = \"opaque\" \"=\" quoted-string\n     stale               = \"stale\" \"=\" ( \"true\" | \"false\" )\n     algorithm           = \"algorithm\" \"=\" ( \"MD5\" | token )\n\n  The meanings of the values of the parameters used above are as\n  follows:\n\n     realm\n     A string to be displayed to users so they know which  username and\n     password to use.  This string should contain  at least the name of\n     the host performing the authentication and might additionally\n     indicate the collection of users who might have access.  An example\n     might be \"registeredusers@gotham.news.com.\"  The realm is a \n     \"quoted-string\" as specified in section 2.2 of the HTTP/1.1 \n     specification [2].\n\n     domain\n     A comma separated list of URIs, as specified for HTTP/1.0.  The\n     intent is that the client could use this information to know the\n     set of URIs for which the same authentication information should be\n     sent.  The URIs in this list may exist on different servers.  If\n     this keyword is omitted or empty, the client should assume that the\n     domain consists of all URIs on the responding server.\n\n     nonce\n     A server-specified data string which may be uniquely generated each\n     time a 401 response is made.  It is recommended that this string be\n     base64 or hexadecimal data.  Specifically, since the string is\n     passed in the header lines as a quoted string, the double-quote\n     character is not allowed.\n\n     The contents of the nonce are implementation dependent.  The\n     quality of the implementation depends on a good choice.  A\n     recommended nonce would include\n\n             H(client-IP \":\" time-stamp \":\" private-key )\n\n     Where client-IP is the dotted quad IP address of the client making\n     the request, time-stamp is a server generated time value,  private-\n     key is data known only to the server.  With a nonce of this form a\n     server would normally recalculate the nonce after receiving the\n     client authentication header and reject the request if it did not\n     match the nonce from that header. In this way the server can limit\n     the reuse of a nonce to the IP address to which it was issued and\n     limit the time of the nonce's validity.  Further discussion of the\n     rationale for nonce construction is in section 3.2 below.\n\n     An implementation might choose not to accept a previously used\n     nonce or a previously used digest to protect against a replay\n     attack.  Or, an implementation might choose to use one-time nonces\n     or digests for POST or PUT requests and a time-stamp for GET\n     requests.  For more details on the issues involved see section 3.\n     of this document.\n\n     The nonce is opaque to the client.\n\n     opaque\n     A string of data, specified by the server, which should returned by\n     the client unchanged.  It is recommended that this string be base64\n     or hexadecimal data.  This field is a \"quoted-string\" as specified\n     in section 2.2 of the HTTP/1.1 specification [2].\n\n     stale\n     A flag, indicating that the previous request from the client was\n     rejected because the nonce value was stale.  If stale is TRUE (in\n     upper or lower case), the client may wish to simply retry the\n     request with a new encrypted response, without reprompting the user\n     for a new username and password.  The server should only set stale\n     to true if it receives a request for which the nonce is invalid but\n     with a valid digest for that nonce (indicating the the client knows\n     the correct username/password).\n\n     algorithm\n     A string indicating the algorithm used to produce the digest or\n     checksum.  If this not present the MD5 algorithm is assumed. In\n     this document the string obtained by applying this algorithm to the\n     data \"data\" will be denoted by H(data).\n\n\n\n2.1.2 The Authorization Request Header\n\n  The client is expected to retry the request, passing an Authorization\n  header line, which is defined as follows.\n\n     Authorization       = \"Authorization\" \":\" \"Digest\" digest-response\n\n     digest-response     = 1#( username | realm | nonce | digest-uri |\n                              response | [ digest ] | [ algorithm ] |\n                              opaque )\n\n     username            = \"username\" \"=\" username-value\n     username-value      = quoted-string\n     digest-uri          = \"uri\" \"=\" digest-uri-value\n     digest-uri-value    = request-uri         ; As specified by HTTP/1.1\n     response            = \"response\" \"=\" response-digest\n     digest             = \"digest\" \"=\" entity-digest\n\n     response-digest     = <\"> 32*LHEX <\">\n     entity-digest      = <\"> 32*LHEX <\">\n     LHEX                = \"0\" | \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" |\n                           \"8\" | \"9\" | \"a\" | \"b\" | \"c\" | \"d\" | \"e\" | \"f\"\n\n\n  The definitions of response-digest and entity-digest above indicate\n  the encoding for their values. The following ones show how the value\n  is computed:\n\n     response-digest     =\n          <\"> < H( H(A1) \":\" nonce-value \":\" H(A2) > <\">\n\n     A1             = username-value \":\" realm-value \":\" password\n     password       = < user's password >\n     A2             = Method \":\" digest-uri-value\n\n\n\n  The \"username-value\" field is a \"quoted-string\" as specified in section\n  2.2 of the HTTP/1.1 specification [2].  \"Method\" is the HTTP request\n  method as specified in section 5.1 of [2].  The \"request-uri\" value is\n  the Request-URI from the request line as specified in section 5.1 of\n  [2].  This may be \"*\", an \"absoluteURL\" or an \"abs_path\" as specified\n  in section 5.1.2 of [2], but it MUST agree with the Request-URI, in\n  particular, it MUST be an \"absoluteURL\" if the Request-URI is an\n  \"absoluteURL\".\n\n  No white space is allowed in any of the strings to which the digest\n  function H() is applied with the exception of the entity-body (see below).\n  For example the string A1 must be of the form \"user:realm:password\" with\n  no white space on either side of the colons.  Likewise, except for the\n  entity-body, the other strings digested by H() must not have white\n  space on either side of the colons which delimit their fields.\n\n  The authenticating server must assure that the document designated\n  by the \"uri\" parameter is the same as the document served.  The\n  purpose of duplicating information from the request URL in this\n  field is to deal with the possibility that an intermediate proxy may\n  alter the client's request.  This altered (but presumably semantically\n  equivalent) request would not result in the same digest as that\n  calculated by the client.\n\n  The optional \"digest\" field contains a digest of the entity body and\n  some of the associated entity headers.  This digest can be useful in\n  both request and response transactions.  In a request it can insure the\n  integrity of POST data or data being PUT to the server.  In a response\n  it insures the integrity of the served document.  The value of the\n  \"digest\" field is an <entity-digest> which is defined as follows.\n\n    entity-digest = <\"> H(H(A1) \":\" nonce-value \":\" Method \":\" \n                                       entity-info \":\" H(entity-body)) <\">\n           ; format is <\"> 32*LHEX <\">\n\n    entity-info = H(\n              digest-uri-value \":\"\n              media-type \":\"         ; Content-type, see section 3.7 of [2]\n              *DIGIT \":\"             ; Content length -- see 10.12 of [2]\n              content-coding \":\"     ; Content-encoding, see section 3.5 of [2]\n              last-modified \":\" ; last modified date - see section 10.25 of [2]\n              expires           ; expiration date; see section 10.19 of [2]\n              )\n              ; format is <\"> 32*LHEX <\">\n\n    last-modified   = rfc1123-date  ; see section 3.2.1 of [2]\n    expires         = rfc1123-date\n\n\n  The entity-info elements incorporate the values of the URI used to\n  request the entity as well as the associated entity headers\n  Content-type, Content-length, Content-encoding, Last-modified, and\n  Expires.  These headers are all end-to-end headers (see section ??? of [2])\n  which must not be modified by proxy caches.  The \"entity-body\" is as\n  specified by section 10.13 of [2] or RFC 1864.\n\n  Note that not all entities will have an associated URI or all of\n  these headers.  For example, an entity which is the data of a\n  POST request will typically not have a digest-uri-value or\n  Last-modified or Expires headers.  If an entity does not have a\n  digest-uri-value or a header corresponding to one of the entity-info\n  fields, then that field is left empty in the computation of\n  entity-info.  All the colons specified above are present, however.\n  For example the value of the entity-info associated with POST data\n  which has content-type \"text/plain\", no content-encoding and a length \n  of 255 bytes would be H(:text/plain:255:::).\n\n  In the entity-info computation, except for the blank after the comma\n  in \"rfc1123-date\", there must be no white space between \"words\" and\n  \"tspecials\", and exactly one blank between \"words\" (see section 2.2 \n  of [2]).\n\n\n\n\n\n2.1.3 The Authentication-info Header\n\n  When authorization succeeds, the Server may optionally provide a\n  Authentication-info header indicating that the server wants to\n  communicate some information regarding the successful authentication\n  (such as an entity digest or a new nonce to be used for the next\n  transaction).  It has two fields, digest and nextnonce.  Both\n  are optional.\n\n\n    Authentication-info = \"Authentication-info\" \":\" \n                                      1#( digest | nextnonce )\n\n    nextnonce      = \"nextnonce\" \"=\" nonce-value\n\n    digest = \"digest\" \"=\" entity-digest\n\n\n\n  The optional digest allows the client to verify that the body\n  of the response has not been changed en-route.  The server would\n  probably only send this when it has the document and can compute it.\n  The server would probably not bother generating this header for CGI\n  output.  The value of the \"digested-entity\" is an <entity-digest> which\n  is computed as described above.  \n\n  The value of the nextnonce parameter is the nonce the server wishes\n  the client to use for the next authentication response.  Note that\n  either field is optional.  In particular the server may send the\n  Authentication-info header with only the nextnonce field as a means of\n  implementing one-time nonces.  If the nextnonce field is present the\n  client is strongly encouraged to use it for the next WWW-Authenticate\n  header.  Failure of the client to do so may result in a request to\n  re-authenticate from the server with the \"stale=TRUE.\"\n\n\n\n\n\n2.2 Digest Operation\n\n  Upon receiving the Authorization information, the server may check\n  its validity by looking up its known password which corresponds to\n  the submitted username.  Then, the server must perform the same MD5\n  operation performed by the client, and compare the result to the\n  given response-digest.\n\n  Note that the HTTP server does not actually need to know the user's\n  clear text password.  As long as H(A1) is available to the server,\n  the validity of an Authorization header may be verified.\n\n  A client may remember the username, password and nonce values, so\n  that future requests within the specified <domain> may include the\n  Authorization line preemptively.  The server may choose to accept the\n  old Authorization information, even though the nonce value included\n  might not be fresh. Alternatively, the server could return a 401\n  response with a new nonce value, causing the client to retry the\n  request.  By specifying stale=TRUE with this response, the server\n  hints to the client that the request should be retried with the new\n  nonce, without reprompting the user for a new username and password.\n\n  The opaque data is useful for transporting state information around.\n  For example, a server could be responsible for authenticating content\n  which actual sits on another server.  The first 401 response would\n  include a domain field which includes the URI on the second server,\n  and the opaque field for specifying state information.  The client\n  will retry the request, at which time the server may respond with a\n  301/302 redirection, pointing to the URI on the second server.  The\n  client will follow the redirection, and pass the same Authorization\n  line, including the <opaque> data which the second server may\n  require.\n\n  As with the basic scheme, proxies must be completely transparent in\n  the Digest access authentication scheme. That is, they must forward\n  the WWW-Authenticate, Authentication-info and Authorization headers\n  untouched. If a proxy wants to authenticate a client before a request\n  is forwarded to the server, it can be done using the Proxy-\n  Authenticate and Proxy-Authorization headers.\n\n\n2.3 Security Protocol Negotiation\n\n  It is useful for a server to be able to know which security schemes a\n  client is capable of handling.\n\n  If this proposal is accepted as a required part of the HTTP/1.1\n  specification, then a server may assume Digest support when a client\n  identifies itself as HTTP/1.1 compliant.\n\n  It is possible that a server may want to require Digest as its\n  authentication method, even if the server does not know that the\n  client supports it.  A client is encouraged to fail gracefully if the\n  server specifies any authentication scheme it cannot handle.\n\n\n\n\n\n2.4 Example\n\n  The following example assumes that an access-protected document is\n  being requested from the server.  The URI of the document is\n  \"http://www.nowhere.org/dir/index.html\".  Both client and server know\n  that the username for this document is \"Mufasa\", and the password is\n  \"CircleOfLife\".\n\n  The first time the client requests the document, no Authorization\n  header is sent, so the server responds with:\n\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: Digest    realm=\"testrealm@host.com\",\n                            nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\",\n                            opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"\n\n  The client may prompt the user for the username and password, after\n  which it will respond with a new request, including the following\n  Authorization header:\n\nAuthorization: Digest       username=\"Mufasa\",\n                            realm=\"testrealm@host.com\",\n                            nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\",\n                            uri=\"/dir/index.html\",\n                            response=\"e966c932a9242554e42c8ee200cec7f6\",\n                            opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"\n\n\n\n2.5 Proxy-Authentication and Proxy-Authorization\n\n  The digest authentication scheme may also be used for authenticating\n  users to proxies or proxies to end servers.  Unlike the WWW-\n  Authenticate, and Authorization headers, the proxy versions, Proxy-\n  Authenticate and Proxy-Authorization, apply only to the current\n  connection and must not be passed upstream or downstream.  The\n  transactions for proxy authentication are very similar to those\n  already described.  Upon receiving a request which requires\n  authentication the proxy/server must issue the \"HTTP/1.1 401\n  Unauthorized\" header followed by a \"Proxy-Authenticate\" header of the\n  form\n\n     Proxy-Authentication     = \"Proxy-Authentication\" \":\" \"Digest\"\n                                   digest-challenge\n\n  where digest-challenge is as defined above in section 2.1. The\n  client/proxy must then re-issue the request with a Proxy-Authenticate\n  header of the form\n\n     Proxy-Authorization      = \"Proxy-Authorization\" \":\"\n                                   digest-response\n\n  where digest-response is as defined above in section 2.1. When\n  authorization succeeds, the Server may optionally provide a Proxy-\n  Authentication-info header of the form\n\n     Proxy-Authentication-info = \"Proxy-Authentication-info\" \":\" nextnonce\n\n  where nextnonce has the same semantics as the nextnonce field in the\n  Authentication-info header described above in section 2.1.\n\n  Note that in principle a client could be asked to authenticate itself\n  to both a proxy and an end-server.  It might receive an \"HTTP/1.1 401\n  Unauthorized\" header followed by both a WWW-Authenticate and a Proxy-\n  Authenticate header.  However, it can never receive more than one\n  Proxy-Authenticate header since such headers are only for immediate\n  connections and must not be passed on by proxies.  If the client\n  receives both headers is must respond with both the Authorization and\n  Proxy-Authorization headers as described above which will likely\n  involve different combinations of username, password, nonce, etc.\n\n\n3. Security Considerations\n\n  Digest Authentication does not provide a strong authentication\n  mechanism.  That is not its intent.  It is intended solely to replace\n  a much weaker and even dangerous authentication mechanism: Basic\n  Authentication.  An important design constraint is that the new\n  authentication scheme be free of patent and export restrictions.\n\n  Most needs for secure HTTP transactions cannot be met by Digest\n  Authentication.  For those needs SSL or SHTTP are more appropriate\n  protocols.  In particular digest authentication cannot be used for\n  any transaction requiring encrypted content.  Nevertheless many\n  functions remain for which digest authentication is both useful and\n  appropriate.\n\n\n3.1 Comparison with Basic Authentication\n\n  Both Digest and Basic Authentication are very much on the weak end of\n  the security strength spectrum. But a comparison between  the two\n  points out the utility, even necessity, of replacing Basic by Digest.\n\n  The greatest threat to the type of transactions for which these\n  protocols are used is network snooping.  This kind of transaction\n  might involve, for example, online access to a database whose use is\n  restricted to paying subscribers.  With Basic authentication an\n  eavesdropper can obtain the password of the user.  This not only\n  permits him to access anything in the data base, but often worse,\n  will permit access to anything else the user protects with the same\n  password.\n\n  By contrast, with Digest Authentication the eavesdropper only gets\n  access to the transaction in question and not to the user's password.\n  The information gained by the eavesdropper would permit a replay\n  attack, but only with a request for the same document and even that\n  might be difficult.\n\n\n\n\n3.2 Replay Attacks\n\n  A replay attack against digest authentication would usually be\n  pointless for a simple GET request since an eavesdropper would\n  already have seen the only document he could obtain with a replay.\n  This is because the URI of the requested document is digested in the\n  client response and the server will only deliver that document. By\n  contrast under Basic Authentication once the eavesdropper has the\n  user's password any document protected by that password is open to\n  him.  A GET request containing form data could only be \"replayed\"\n  with the identical data.  However, this could be problematic if it\n  caused a CGI script to take some action on the server.\n\n  Thus, for some purposes, it is necessary to protect against replay\n  attacks.  A good digest implementation can do this in various ways.\n  The server created \"nonce\" value is implementation dependent, but if\n  it contains a digest of the client IP, a timestamp, and a private\n  server key (as recommended above) then a replay attack is not simple.\n  An attacker must convince the server that the request is coming from\n  a false IP address and must cause the server to deliver the document\n  to an IP address different from the address to which it believes it\n  is sending the document.  An attack can only succeed in the period\n  before the timestamp expires.  Digesting the client IP and timestamp\n  in the nonce permits an implementation which does not maintain state\n  between transactions.\n\n  For applications where no possibility of replay attack can be\n  tolerated the server can use one-time response digests which will not\n  be honored for a second use.  This requires the overhead of the\n  server remembering which digests have been used until the nonce\n  timestamp (and hence the digest built with it) has expired, but it\n  effectively protects against replay attacks. Instead of maintaining a\n  list of the values of used digests, a server would hash these values\n  and require re-authentication whenever a hash collision occurs.\n\n  An implementation must give special attention to the possibility of\n  replay attacks with POST and PUT requests.  A successful replay attack\n  could result in counterfeit form data or a counterfeit version of a\n  PUT file.  The use of one-time digests or one-time nonces is\n  recommended.  It is also recommended that the optional <digest> be\n  implemented for use with POST or PUT requests to assure the integrity\n  of the posted data.  Alternatively, a server may choose to allow\n  digest authentication only with GET requests. Responsible server\n  implementors will document the risks described here as they pertain to\n  a given implementation.\n\n\n3.3 Man in the Middle\n\n  Both Basic and Digest authentication are vulnerable to \"man in the\n  middle\" attacks, for example, from a hostile or compromised proxy.\n  Clearly, this would present all the problems of eavesdropping.  But\n  it could also offer some additional threats.  In particular, even\n  with digest authentication, a hostile proxy might spoof the client\n  into making a request the attacker wanted rather than one the client\n  wanted.  Of course, this is still much harder than a comparable\n  attack against Basic Authentication.\n\n  There are several attacks on the \"disgest\" field in the\n  Authentication-info header.  In particular, the attacker can alter any\n  of the entity-headers not incorporated in the computation of the digest,\n  The attacker can alter most of the request headers in the client's\n  request, and can alter any response header in the origin-server's reply,\n  except those headers whose values are  incorporated into the \"digest\"\n  field.  \n  \n  Alteration of Accept* or User-Agent request headers can only result\n  in a denial of service attack that returns content in an unacceptable\n  media type or language. Alternation of cache control headers also can\n  only result in denial of service. Alteration of Host will be detected,\n  if the full URL is in the response-digest. Alteration of Referer or\n  From is not important, as these are only hints.\n\n\n3.4 Spoofing by Counterfeit Servers\n\n  Basic Authentication is vulnerable to spoofing by counterfeit\n  servers. If a user can be led to believe that she is connecting to a\n  host containing information protected by a password she knows when in\n  fact she is connecting to a hostile server then the hostile server\n  can request a password, store it away for later use, and feign an\n  error.  This type of attack is more difficult with Digest\n  Authentication.\n\n\n3.5 Storing passwords\n\n  Digest authentication requires that the authenticating agent (usually\n  the server) store some data derived from the user's name and password\n  in a \"password file\" associated with a given realm.  Normally this\n  might contain pairs consisting of username and H(A1), where H(A1) is\n  the digested value of the username, realm, and password as described\n  above.\n\n  The security implications of this are that if this password file is\n  compromised then an attacker gains immediate access to documents on\n  the server using this realm.  Unlike, say a standard UNIX password\n  file, this information need not be decrypted in order to access\n  documents in the server realm associated with this file.  On the\n  other hand, decryption, or more likely a brute force attack, would be\n  necessary to obtain the user's password.  This is the reason that the\n  realm is part of the digested data stored in the password file.  It\n  means that if one digest authentication password file is compromised,\n  it does not automatically compromise others with the same username\n  and password (though it does expose them to brute force attack).\n\n  There are two important security consequences of this.  First the\n  password file must be protected as if it contained unencrypted\n  passwords, because for the purpose of accessing documents in its\n  realm, it effectively does.\n\n  A second consequence of this is that the realm string should be\n  unique among all realms which any single user is likely to use.  In\n  particular a realm string should include the name of the host doing\n  the authentication.  The inability of the client to authenticate the\n  server is a weakness of Digest Authentication.\n\n\n3.6 Summary\n\n  By modern cryptographic standards Digest Authentication is weak.  But\n  for a large range of purposes it is valuable as a replacement for\n  Basic Authentication.  It remedies many, but not all, weaknesses of\n  Basic Authentication.  Its strength may vary depending on the\n  implementation.  In particular the structure of the nonce (which is\n  dependent on the server implementation) may affect the ease of\n  mounting a replay attack.  A range of server options is appropriate\n  since, for example, some implementations may be willing to accept the\n  server overhead of one-time nonces or digests to eliminate the\n  possibility of replay while others may satisfied with a nonce like\n  the one recommended above restricted to a single IP address and with\n  a limited lifetime.\n\n  The bottom line is that *any* compliant implementation will be\n  relatively weak by cryptographic standards, but *any* compliant\n  implementation will be far superior to Basic Authentication.\n\n\n\n4.  Acknowledgments\n\n  In addition to the authors, valuable discussion instrumental in\n  creating this document have come from Peter J Churchyard, Ned Freed,\n  and David Kristol.\n\n\n5. References\n\n   [1]  T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen.\n        \"Hypertext Transfer Protocol -- HTTP/1.0\"\n        Internet-Draft (work in progress), UC Irvine,\n        <URL:http://ds.internic.net/internet-drafts/\n        draft-ietf-http-v10-spec-00.txt>, March 1995.\n\n   [2]  T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen...\n        \"Hypertext Transfer Protocol -- HTTP/1.1\"\n        TBS\n\n   [3]  RFC 1321.  R.Rivest, \"The MD5 Message-Digest Algorithm\",\n        <URL:http://ds.internic.net/rfc/rfc1321.txt>,\n        April 1992.\n\n\n\n\n6. Authors Addresses\n\n   John Franks\n   john@math.nwu.edu\n   Professor of Mathematics\n   Department of Mathematics\n   Northwestern University\n   Evanston, IL 60208-2730, USA\n\n   Phillip M. Hallam-Baker\n   hallam@w3.org\n   European Union Fellow\n   CERN\n   Geneva\n   Switzerland\n\n   Jeffery L. Hostetler\n   jeff@spyglass.com\n   Senior Software Engineer\n   Spyglass, Inc.\n   3200 Farber Drive\n   Champaign, IL  61821, USA\n\n   Paul J. Leach\n   paulle@microsoft.com\n   Microsoft Corporation\n   1 Microsoft Way\n   Redmond, WA 98052, USA\n\n   Ari Luotonen\n   luotonen@netscape.com\n   Member of Technical Staff\n   Netscape Communications Corporation\n   501 East Middlefield Road\n   Mountain View, CA 94043, USA\n\n   Eric W. Sink\n   eric@spyglass.com\n   Senior Software Engineer\n   Spyglass, Inc.\n   3200 Farber Drive\n   Champaign, IL  61821, USA\n\n   Lawrence C. Stewart\n   stewart@OpenMarket.com\n   Open Market, Inc.\n   215 First Street\n   Cambridge, MA  02142, USA\n\n\n\n", "id": "lists-010-1922753"}, {"subject": "RE: Persistent Connection", "content": "This is incorrect EBNF:\n  Connection-header = \"Connection\" \":\" connection-token\n                                       0#( \",\" connection-token )\n\nThe '#' means \"comma separated list\", so the \",\" is redundant.\nThis is correct (and simpler):\n  Connection-header = \"Connection\" \":\" 1#connection-token\n\nPaul\n>----------\n>From: hopmann@holonet.net[SMTP:hopmann@holonet.net]\n>Sent: Friday, April 12, 1996 10:54 AM\n>To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Persistent Connections\n>\n>Hi all,\n>\n>We have been working quite a bit on the Persistent connection draft in\n>order\n>to get some material to edit into the 1.1 overall draft. Currently the\n>\"latest\" Persistent Connections draft can be found at:\n>\n>http://ugly.resnova.com/ietf/draft-ietf-http-ses-ext-05.txt\n>\n>The above draft is being used to collect the consensus on this issue,\n>so I\n>intend to continue to maintain it. I have not submitted it to the ID\n>editor\n>lately as it has been under fairly rapid revision.\n>\n>Please read it NOW and send me specific comments. We are at the 11th\n>hour\n>here, so please keep it specific.\n>\n>Alex Hopmann\n>ResNova Software, Inc.\n>hopmann@holonet.net\n>\n>\n\n\n\n", "id": "lists-010-1972834"}, {"subject": "(ACCEPT*) Consensu", "content": "I am pleased to announce that I believe there to be consensus on the\ntext for the sections:\n\n  3.10 Language Tags \n  (9. Status Code Definitions) 416 Not Acceptable\n  10.1 Accept \n  10.2 Accept-Charset \n  10.3 Accept-Encoding \n  10.4 Accept-Language \n  14.7 Privacy issues connected to Accept headers\n\nThe only open issue connected to these sections is the issue on\nwhether the specification should use the term `charset' or `character\nset'.\n\nThis closes the following issues on the HTTP/1.1 issues list:\n\n QMXB NOTACCEPT LANGUAGETAGS ACCEPT ACCEPTCHARSET ACCEPTENCODING\n ACCEPTLANGUAGE BOTH ACCEPT-PRIVACY\n\nIf you believe that there is no consensus on one of these issues,\nplease announce this as soon as possible.\n\nBelow is the consensus text.  The change bars are computer-generated,\nand indicate changes with respect to the text posted on Friday, April\n5.  Changed words behind the changebars are typeset in capital\nletters.\n\nSee the end of this message for diffs between\ndraft-ietf-http-v11-spec-01.txt and the new consensus text.\n\n=====================================================================\n\n3. Protocol Parameters \n\n  3.10 Language Tags \n\n   [##Note: I moved the language tag matching discussion that used to\n   be in this Section to Section 10.4 (Accept-Language).  Some other\n   minor edits were made.##]\n\n  A language tag identifies a natural language spoken, written, or\n  otherwise conveyed by human beings for communication of information to\n| other human beings. Computer languages are explicitly excluded. HTTP\n  uses language tags within the Accept-Language and Content-Language\n  fields.\n\n  The syntax and registry of HTTP language tags is the same as that\n  defined by RFC 1766 [1]. In summary, a language tag is composed of 1\n  or more parts: A primary language tag and a possibly empty series of\n  subtags:\n\n          language-tag  = primary-tag *( \"-\" subtag )\n\n          primary-tag   = 1*8ALPHA\n          subtag        = 1*8ALPHA\n\n  Whitespace is not allowed within the tag and all tags are\n  case-insensitive.  The namespace of language tags is administered by\n  the IANA. Example tags include:\n\n         en, en-US, en-cockney, i-cherokee, x-pig-latin\n\n  where any two-letter primary-tag is an ISO 639 language abbreviation\n  and any two-letter initial subtag is an ISO 3166 country code.\n\n\n9. Status Code Definitions\n\n| 416 Not Acceptable\n\n|  [##Note: The previous version had the 413, not 416 above, but\n|  413 has since been taken by another new error type.##]\n\n|  [##Note: the new 416 is similar to the 406 response code in the old\n   draft.  406 cannot be used for content negotiation compatibility\n   reasons##]\n|\n\n  The resource identified by the Request-URI and Host request header\n  (present if the request-URI is not an absoluteURI) is only capable of\n  generating response entities which have content characteristics not\n  acceptable according to the accept headers sent in the request.\n\n  HTTP/1.1 servers are allowed to return responses which are not\n  acceptable according to the accept headers sent in the request. In\n| some cases, this may even be preferable over sending a 416\n  response. User agents are encouraged to inspect the headers of an\n| incoming response to determine if it is acceptable. If THE RESPONSE is\n| not ACCEPTABLE, user agents should interrupt the receipt of the\n| response if doing so would save network resources.  If it IS unknown\n  whether an incoming response would be acceptable, a user agent should\n  temporarily stop receipt of more data and query the user for a\n  decision on further actions.\n\n   [## Note: the paragraph above could be moved to a more convenient\n   location in the 1.1 document if the editor finds one.  Note that\n   the above rule was discussed extensively on the content negotiation\n   mailing list.  A short summary of the main reason behind this rule:\n   20 line HTTP servers.##]\n\n\n10 Header Field Definitions \n\n  10.1 Accept \n\n  The Accept request-header field can be used to specify certain media\n  types which are acceptable for the response.  Accept headers can be\n  used to indicate that the request is specifically limited to a small\n  set of desired types, as in the case of a request for an in-line\n  image.\n\n  The field may be folded onto several lines and more than one\n  occurrence of the field is allowed, with the semantics being the same\n  as if all the entries had been in one field value.\n\n         Accept         = \"Accept\" \":\" #(\n|                              media-range\n                               [ ( \":\" | \";\" ) \n                                 range-parameter \n                                 *( \";\" range-parameter ) ]\n|                             | extension-token )\n\n\n         media-range    = ( \"*/*\"\n                          | ( type \"/\" \"*\" )\n                          | ( type \"/\" subtype )\n                          ) *( \";\" parameter )\n\n         range-parameter = ( \"q\" \"=\" qvalue ) \n                         | extension-range-parameter\n         extension-range-parameter = ( token \"=\" token )\n         extension-token = token \n\n  The asterisk \"*\" character is used to group media types into ranges,\n  with \"*/*\" indicating all media types and \"type/*\" indicating all\n  subtypes of that type. The range-parameter q is used to indicate the\n  media type quality factor for the range, which represents the user's\n  preference for that range of media types. The default value is q=1. In\n  Accept headers generated by HTTP/1.1 clients, the character separating\n  media-ranges from range-parameters should be a \":\". HTTP/1.1 servers\n  should be tolerant of use of the \";\" separator by HTTP/1.0 clients.\n\n  The example \n\n|        Accept: audio/*: q=0.2, audio/basic\n\n  should be interpreted as \"I prefer audio/basic, but send me any audio\n  type if it is the best available after an 80% mark-down in quality.\"\n\n  If no Accept header is present, then it is assumed that the client\n  accepts all media types. If Accept headers are present, and if the\n| SERVER cannot send a response which is acceptable according to the\n  Accept headers, then the server should send an error response with the\n| 416 (not acceptable) status code, though the sending of an\n| UNACCEPTABLE response is also allowed.\n\n  A more elaborate example is \n\n|        Accept: text/plain: q=0.5, text/html,\n|                text/x-dvi: q=0.8, text/x-c\n\n  Verbally, this would be interpreted as \"text/html and text/x-c are the\n  preferred media types, but if they do not exist, then send the\n  text/x-dvi entity, and if that does not exist, send the text/plain\n  entity.\"\n\n  Media ranges can be overridden by more specific media ranges or\n  specific media types. If more than one media range applies to a given\n  type, the most specific reference has precedence. For example,\n\n         Accept: text/*, text/html, text/html;level=1, */*\n\n  have the following precedence: \n\n         1) text/html;level=1\n         2) text/html\n         3) text/*\n         4) */*\n\n  The media type quality factor associated with a given type is\n  determined by finding the media range with the highest precedence\n  which matches that type. For example,\n\n         Accept: text/*:q=0.3, text/html:q=0.7, text/html;level=1,\n                 */*:q=0.5\n\n| would cause the following VALUES to be associated: \n\n         text/html;level=1                          = 1\n         text/html                                  = 0.7\n         text/plain                                 = 0.3\n         image/jpeg                                 = 0.5\n         text/html;level=3                          = 0.7\n\n        Note: A user agent may be provided with a default set of\n        quality values for certain media ranges. However, unless\n        the user agent is a closed system which cannot interact\n        with other rendering agents, this default set should be\n        configurable by the user. \n\n\n  10.2 Accept-Charset \n\n  The Accept-Charset request-header field can be used to indicate what\n  character sets are acceptable for the response. This field allows\n  clients capable of understanding more comprehensive or special-purpose\n  character sets to signal that capability to a server which is capable\n  of representing documents in those character sets. The ISO-8859-1\n  character set can be assumed to be acceptable to all user agents.\n\n         Accept-Charset = \"Accept-Charset\" \":\"\n                   1#( charset [ \";\" \"q\" \"=\" qvalue ] )\n\n  Character set values are described in Section 3.4. Each charset may be\n  given an associated quality value which represents the user's\n  preference for that charset. The default value is q=1. An example is\n\n         Accept-Charset: iso-8859-5, unicode-1-1;q=0.8\n\n  If no Accept-Charset header is present, the default is that any\n  character set is acceptable. If an Accept-Charset header is present,\n| and if the SERVER cannot send a response which is acceptable according\n  to the Accept-Charset header, then the server should send an error\n| response with the 416 (not acceptable) status code, though the sending\n| of an UNACCEPTABLE response is also allowed.\n\n  10.3 Accept-Encoding \n\n  The Accept-Encoding request-header field is similar to Accept, but\n  restricts the content-coding values (Section 3.5) which are acceptable\n  in the response.\n\n         Accept-Encoding         = \"Accept-Encoding\" \":\" \n                                   #( content-coding )\n\n  An example of its use is \n\n         Accept-Encoding: compress, gzip\n\n  If no Accept-Encoding header is present in a request, the server may\n  assume that the client will accept any content coding. If an\n| Accept-Encoding header is present, and if the SERVER cannot send a\n  response which is acceptable according to the Accept-Encoding header,\n| then the server should send an error response with the 416 (not\n  acceptable) status code.\n\n\n  10.4 Accept-Language \n\n  The Accept-Language request-header field is similar to Accept, but\n  restricts the set of natural languages that are preferred as a\n  response to the request.\n\n         Accept-Language         = \"Accept-Language\" \":\"\n                                   1#( language-range [ \";\" \"q\" \"=\" qvalue ] )\n\n          language-range     = ( ( 1*8ALPHA *( \"-\" 1*8ALPHA ) ) \n                         | \"*\" )\n\n  Each language-range may be given an associated quality value which\n  represents an estimate of the user's comprehension of the languages\n  specified by that range. The quality value defaults to \"q=1\" (100%\n  comprehension). For example,\n\n         Accept-Language: da, en-gb;q=0.8, en;q=0.7\n\n  would mean: \"I prefer Danish, but will accept British English (with\n  80% comprehension) and other types of English (with 70%\n  comprehension).\"\n\n  A language-range matches a language-tag if it exactly equals the tag,\n| or if it EXACTLY EQUALS a prefix (A SUB-SEQUENCE STARTING AT THE FIRST\n| CHARACTER) of the tag such that the first tag character following the\n  prefix is \"-\".  The special range \"*\", if present in the\n  Accept-Language field, matches every tag not matched by any other\n  ranges present in the Accept-Language field.\n\n       Note: This use of a prefix matching rule does not imply that\n       language tags are assigned to languages in such a way that it is\n       always true that if a user understands a language with a certain\n       tag, then this user will also understand all languages with tags\n       for which this tag is a prefix. The prefix rule simply allows the\n       use of prefix tags if this is the case.\n\n  The language quality factor assigned to a language-tag by the\n  Accept-Language field is the quality value of the longest\n  language-range in the field that matches the language-tag. If no\n  language-range in the field matches the tag, the language quality\n  factor assigned is 0. If no Accept-Language header is present in a\n  request, the server should assume that all languages are equally\n| acceptable. If an Accept-Language header is present, THEN ALL\n| LANGUAGES WHICH ARE ASSIGNED A QUALITY FACTOR GREATER THAN 0 are\n| ACCEPTABLE. IF the SERVER cannot GENERATE a response FOR an audience\n| capable of understanding at least one ACCEPTABLE LANGUAGE, it CAN send\n  a response that uses one or more un-accepted languages.\n\n  It may be contrary to be privacy expectations of the user to send an\n  Accept-Language header with the complete linguistic preferences of the\n  user in every request. For a discussion of this issue, see Section\n| 14.7.\n\n        Note: As intelligibility is highly dependent on the individual\n        user, it is recommended that client applications make the\n        choice of linguistic preference available to the user. If the\n        choice is not made available, then the Accept-Language\n        header field must not be given in the request. \n\n\n14 Security Considerations\n\n| 14.7 Privacy issues connected to Accept headers\n\n   [## Note: I believe someone else (Brian Behlendorf?) was also\n   writing text about this, so I only include some concerns about\n   Accept-Language important from a European viewpoint.  The concern\n   of user tracking through Accept headers is not covered below, see\n   Section 6.2 of draft-holtman for a discussion of this concern##]\n\n|  [## Note: update in the above note: Brian Behlendorf does not seem\n|  to be responding, so I will take over writing text about user\n|  tracking.##]\n\n  Accept request headers can reveal information about the user to all\n  servers which are accessed. The Accept-Language header in particular\n  can reveal information the user would consider to be of a private\n  nature, because the understanding of particular languages is often\n  strongly correlated to the membership of a particular ethnic\n  group. User agents which offer the option to configure the contents of\n  an Accept-Language header to be sent in every request are strongly\n  encouraged to let the configuration process include a message which\n  makes the user aware of the loss of privacy involved.\n\n  An approach that limits the loss of privacy would be for a user agent\n  to omit the sending of Accept-Language headers by default, and to ask\n| the user whether it should start sending Accept-Language headers to a\n| server if it detects, by looking for any Vary or Alternates response\n  headers generated by the server, that such sending could improve the\n  quality of service.\n\n=====================================================================\n\n\nBelow is a diff listing between draft-ietf-http-v11-spec-01.txt and\nthe new consensus text.\n\nLines preceded by - were in draft-ietf-http-v11-spec-01.txt.\nLines preceded by + are the new consensus wording.\n\nThe diff listing below was computer generated and edited by hand to\nimprove readability.\n\n=====================================================================\n\n 3. Protocol Parameters \n\n 3.10 Language Tags \n\n A language tag identifies a natural language spoken, written, or\n otherwise conveyed by human beings for communication of information to\n other human beings. Computer languages are explicitly excluded.  HTTP\n-uses language tags within the Accept-Language, Content-Language, and\n-URI-header fields.\n+uses language tags within the Accept-Language and Content-Language\n+fields.\n\n The syntax and registry of HTTP language tags is the same as that\n defined by RFC 1766 [1]. In summary, a language tag is composed of 1\n or more parts: A primary language tag and a possibly empty series of\n subtags:\n\n      language-tag  = primary-tag *( \"-\" subtag )\n\n      primary-tag   = 1*8ALPHA\n      subtag        = 1*8ALPHA\n\n Whitespace is not allowed within the tag and all tags are\n case-insensitive. The namespace of language tags is administered by\n the IANA. Example tags include:\n\n     en, en-US, en-cockney, i-cherokee, x-pig-latin\n\n where any two-letter primary-tag is an ISO 639 language abbreviation\n and any two-letter initial subtag is an ISO 3166 country code.\n\n-In the context of the Accept-Language header (Section 10.4), a\n-language tag is not to be interpreted as a single token, as per RFC\n-1766, but as a hierarchy. A server should consider that it has a match\n-when a language tag received in an Accept-Language header matches the\n-initial portion of the language tag of a document. An exact match\n-should be preferred. This interpretation allows a browser to send, for\n-example:\n-\n-    Accept-Language: en-US, en; ql=0.95\n-\n-when the intent is to access, in order of preference, documents in\n-US-English (\"en-US\"), 'plain' or 'international' English (\"en\"), and\n-any other variant of English (initial \"en-\").\n-\n-    Note: Using the language tag as a hierarchy does not imply \n-    that all languages with a common prefix will be understood \n-    by those fluent in one or more of those languages; it simply \n-    allows the user to request this commonality when it is true \n-    for that user.\n\n 9.  Status Code Definitions\n\n-406 None Acceptable\n-\n-The server has found a resource matching the Request-URI, but not one\n-that satisfies the conditions identified by the Accept and\n-Accept-Encoding request headers. Unless it was a HEAD request, the\n-response should include an entity containing a list of resource\n-characteristics and locations from which the user or user agent can\n-choose the one most appropriate. The entity format is specified by the\n-media type given in the Content-Type header field. Depending upon the\n-format and the capabilities of the user agent, selection of the most\n-appropriate choice may be performed automatically.\n\n+416 Not Acceptable\n+ \n+The resource identified by the Request-URI and Host request header\n+(present if the request-URI is not an absoluteURI) is only capable of\n+generating response entities which have content characteristics not\n+acceptable according to the accept headers sent in the request.\n+\n+HTTP/1.1 servers are allowed to return responses which are not\n+acceptable according to the accept headers sent in the request. In\n+some cases, this may even be preferable over sending a 416\n+response. User agents are encouraged to inspect the headers of an\n+incoming response to determine if it is acceptable. If the response is\n+not acceptable, user agents should interrupt the receipt of the\n+response if doing so would save network resources.  If it is unknown\n+whether an incoming response would be acceptable, a user agent should\n+temporarily stop receipt of more data and query the user for a\n+decision on further actions.\n\n 10.  Header Field Definitions\n\n\n 10.1  Accept\n\n-The Accept response-header field can be used to indicate a list of\n-media ranges which are acceptable as a response to the request. The\n-asterisk \"*\" character is used to group media types into ranges, with\n-\"*/*\" indicating all media types and \"type/*\" indicating all subtypes\n-of that type. The set of ranges given by the client should represent\n-what types are acceptable given the context of the request. The Accept\n-field should only be used when the request is specifically limited to\n-a set of desired types, as in the case of a request for an in-line\n-image, or to indicate qualitative preferences for specific media\n-types.\n\n+The Accept request-header field can be used to specify certain media\n+types which are acceptable for the response.  Accept headers can be\n+used to indicate that the request is specifically limited to a small\n+set of desired types, as in the case of a request for an in-line\n+image.\n\n The field may be folded onto several lines and more than one\n occurrence of the field is allowed, with the semantics being the same\n as if all the entries had been in one field value.\n\n     Accept         = \"Accept\" \":\" #(\n                      media-range\n\n-                     [ \";\" \"q\" \"=\" qvalue ]\n-                     [ \";\" \"mxb\" \"=\" 1*DIGIT ] )\n\n+                     [ ( \":\" | \";\" ) \n+                     range-parameter \n+                     *( \";\" range-parameter ) ]\n+                     | extension-token )\n\n\n     media-range    = ( \"*/*\"\n                    |   ( type \"/\" \"*\" )\n                    |   ( type \"/\" subtype )\n                      ) *( \";\" parameter )\n\n+       range-parameter = ( \"q\" \"=\" qvalue ) \n+                       | extension-range-parameter\n+       extension-range-parameter = ( token \"=\" token )\n+       extension-token = token \n+\n\n-The parameter q is used to indicate the quality factor, which\n-represents the user's preference for that range of media types. The\n-parameter mxb gives the maximum acceptable size of the Entity-Body, in\n-decimal number of octets, for that range of media types.  Section 12\n-describes the content negotiation algorithm which makes use of these\n-values. The default values are: q=1 and mxb=undefined (i.e.,\n-infinity).\n\n+The asterisk \"*\" character is used to group media types into ranges,\n+with \"*/*\" indicating all media types and \"type/*\" indicating all\n+subtypes of that type. The range-parameter q is used to indicate the\n+media type quality factor for the range, which represents the user's\n+preference for that range of media types. The default value is q=1. In\n+Accept headers generated by HTTP/1.1 clients, the character separating\n+media-ranges from range-parameters should be a \":\". HTTP/1.1 servers\n+should be tolerant of use of the \";\" separator by HTTP/1.0 clients.\n\n The example\n\n-       Accept: audio/*; q=0.2, audio/basic\n+       Accept: audio/*: q=0.2, audio/basic\n\n should be interpreted as \"I prefer audio/basic, but send me any audio\n type if it is the best available after an 80% mark-down in quality.\"\n\n-If no Accept header is present, then it is assumed that the client\n-accepts all media types with quality factor 1. This is equivalent to\n-the client sending the following accept header field:\n-\n-    Accept: */*; q=1\n-\n-or\n-\n-    Accept: */*\n-\n-If a single Accept header is provided and it contains no field value,\n-then the server must interpret it as a request to not perform any\n-preemptive content negotiation (Section 12) and instead return a 406\n-(none acceptable) response if there are variants available for the\n-Request-URI.\n\n+If no Accept header is present, then it is assumed that the client\n+accepts all media types. If Accept headers are present, and if the\n+server cannot send a response which is acceptable according to the\n+Accept headers, then the server should send an error response with the\n+416 (not acceptable) status code, though the sending of an\n+unacceptable response is also allowed.\n\n A more elaborate example is\n\n-    Accept: text/plain; q=0.5, text/html,\n-            text/x-dvi; q=0.8; mxb=100000, text/x-c\n+    Accept: text/plain: q=0.5, text/html,\n+            text/x-dvi: q=0.8, text/x-c\n\n Verbally, this would be interpreted as \"text/html and text/x-c are the\n preferred media types, but if they do not exist, then send the\n-text/x-dvi entity if it is less than 100000 bytes, otherwise send the\n-text/plain entity.\"\n+text/x-dvi entity, and if that does not exist, send the text/plain\n+entity.\"\n\n Media ranges can be overridden by more specific media ranges or\n specific media types. If more than one media range applies to a given\n type, the most specific reference has precedence. For example,\n\n-    Accept: text/*, text/html, text/html;version=2.0, */*\n+    Accept: text/*, text/html, text/html;level=1, */*\n\n have the following precedence:\n\n-    1) text/html;version=2.0\n+    1) text/html;level=1\n     2) text/html\n     3) text/*\n     4) */*\n\n-The quality value associated with a given type is determined by\n-finding the media range with the highest precedence which matches that\n-type. For example,\n+The media type quality factor associated with a given type is\n+determined by finding the media range with the highest precedence\n+which matches that type. For example,\n\n-    Accept: text/*;q=0.3, text/html;q=0.7, text/html;version=2.0,\n-            */*;q=0.5\n+    Accept: text/*:q=0.3, text/html:q=0.7, text/html;level=1,\n+            */*:q=0.5\n\n would cause the following values to be associated:\n\n-    text/html;version=2.0                      = 1\n+    text/html;level=1                          = 1\n     text/html                                  = 0.7\n     text/plain                                 = 0.3\n     image/jpeg                                 = 0.5\n     text/html;level=3                          = 0.7\n\n-It must be emphasized that the Accept field should only be used when\n-it is necessary to restrict the response media types to a subset of\n-those possible or when the user has been permitted to specify\n-qualitative values for ranges of media types. If no quality factors\n-have been set by the user, and the context of the request is such that\n-the user agent is capable of saving the entity to a file if the\n-received media type is unknown, then the only appropriate value for\n-Accept is \"*/*\", or an empty value if the user desires reactive\n-negotiation.\n-\n     Note: A user agent may be provided with a default set of \n     quality values for certain media ranges. However, unless the \n     user agent is a closed system which cannot interact with \n     other rendering agents, this default set should be \n     configurable by the user.\n\n\n 10.2  Accept-Charset\n\n The Accept-Charset request-header field can be used to indicate what\n character sets are acceptable for the response. This field allows\n clients capable of understanding more comprehensive or special-purpose\n character sets to signal that capability to a server which is capable\n-of representing documents in those character sets. The US-ASCII\n+of representing documents in those character sets. The ISO-8859-1\n character set can be assumed to be acceptable to all user agents.\n\n-    Accept-Charset = \"Accept-Charset\" \":\" 1#charset\n\n+       Accept-Charset = \"Accept-Charset\" \":\"\n+                 1#( charset [ \";\" \"q\" \"=\" qvalue ] )\n\n\n-Character set values are described in Section 3.4. An example is\n- \n-    Accept-Charset: iso-8859-1, unicode-1-1\n-\n-If no Accept-Charset field is given, the default is that any character\n-set is acceptable. If the Accept-Charset field is given and the\n-requested resource is not available in one of the listed character\n-sets, then the server should respond with the 406 (none acceptable)\n-status code.\n\n+Character set values are described in Section 3.4. Each charset may be\n+given an associated quality value which represents the user's\n+preference for that charset. The default value is q=1. An example is\n+\n+       Accept-Charset: iso-8859-5, unicode-1-1;q=0.8\n+   \n+If no Accept-Charset header is present, the default is that any\n+character set is acceptable. If an Accept-Charset header is present,\n+and if the server cannot send a response which is acceptable according\n+to the Accept-Charset header, then the server should send an error\n+response with the 416 (not acceptable) status code, though the sending\n+of an unacceptable response is also allowed.\n\n 10.3  Accept-Encoding\n\n The Accept-Encoding request-header field is similar to Accept, but\n restricts the content-coding values (Section 3.5) which are acceptable\n in the response.\n\n     Accept-Encoding         = \"Accept-Encoding\" \":\" \n                               #( content-coding )\n\n An example of its use is\n\n     Accept-Encoding: compress, gzip\n\n-If no Accept-Encoding field is present in a request, the server may\n+If no Accept-Encoding header is present in a request, the server may\n assume that the client will accept any content coding. If an\n-Accept-Encoding field is present, but contains an empty field value,\n-then the user agent is refusing to accept any content coding.\n+Accept-Encoding header is present, and if the server cannot send a\n+response which is acceptable according to the Accept-Encoding header,\n+then the server should send an error response with the 416 (not\n+acceptable) status code.\n\n\n 10.4  Accept-Language\n\n The Accept-Language request-header field is similar to Accept, but\n restricts the set of natural languages that are preferred as a\n response to the request.\n\n     Accept-Language = \"Accept-Language\" \":\"\n-                      1#( language-tag [ \";\" \"q\" \"=\" qvalue ] )\n+                      1#( language-range [ \";\" \"q\" \"=\" qvalue ] )\n\n+        language-range     = ( ( 1*8ALPHA *( \"-\" 1*8ALPHA ) ) \n+                             | \"*\" )\n\n-The language-tag is described in Section 3.10. Each language may be\n-given an associated quality value which represents an estimate of the\n-user's comprehension of that language. The quality value defaults to\n-\"q=1\" (100% comprehension) for listed languages. This value may be\n-used in the server's content negotiation algorithm (Section 12). For\n-example,\n- \n-    Accept-Language: da, en-gb;q=0.8, de;q=0.55\n-\n-would mean: \"I prefer Danish, but will accept British English (with\n-80% comprehension) or German (with a 55% comprehension).\"\n\n+Each language-range may be given an associated quality value which\n+represents an estimate of the user's comprehension of the languages\n+specified by that range. The quality value defaults to \"q=1\" (100%\n+comprehension). For example,\n\n+       Accept-Language: da, en-gb;q=0.8, en;q=0.7\n\n+would mean: \"I prefer Danish, but will accept British English (with\n+80% comprehension) and other types of English (with 70%\n+comprehension).\"\n\n+A language-range matches a language-tag if it exactly equals the tag,\n+or if it exactly equals a prefix (a sub-sequence starting at the first\n+character) of the tag such that the first tag character following the\n+prefix is \"-\".  The special range \"*\", if present in the\n+Accept-Language field, matches every tag not matched by any other\n+ranges present in the Accept-Language field.\n+\n+     Note: This use of a prefix matching rule does not imply that\n+     language tags are assigned to languages in such a way that it is\n+     always true that if a user understands a language with a certain\n+     tag, then this user will also understand all languages with tags\n+     for which this tag is a prefix. The prefix rule simply allows the\n+     use of prefix tags if this is the case.\n\n-If the server cannot fulfill the request with one or more of the\n-languages given, or if the languages only represent a subset of a\n-multi-linguistic Entity-Body, it is acceptable to serve the request in\n-an unspecified language. This is equivalent to assigning a quality\n-value of \"q=0.001\" to any unlisted language.\n-\n-If no Accept-Language header is present in the request, the server\n-should assume that all languages are equally acceptable.\n\n+The language quality factor assigned to a language-tag by the\n+Accept-Language field is the quality value of the longest\n+language-range in the field that matches the language-tag. If no\n+language-range in the field matches the tag, the language quality\n+factor assigned is 0. If no Accept-Language header is present in a\n+request, the server should assume that all languages are equally\n+acceptable. If an Accept-Language header is present, then all\n+languages which are assigned a quality factor greater than 0 are\n+acceptable. If the server cannot generate a response for an audience\n+capable of understanding at least one acceptable language, it can send\n+a response that uses one or more un-accepted languages.\n+\n+It may be contrary to be privacy expectations of the user to send an\n+Accept-Language header with the complete linguistic preferences of the\n+user in every request. For a discussion of this issue, see Section\n+14.7.\n+\n\n\n     Note: As intelligibility is highly dependent on the \n     individual user, it is recommended that client applications \n     make the choice of linguistic preference available to the \n     user. If the choice is not made available, then the \n     Accept-Language header field must not be given in the \n     request.\n\n+14 Security Considerations\n+\n+14.7 Privacy issues connected to Accept headers\n+\n+Accept request headers can reveal information about the user to all\n+servers which are accessed. The Accept-Language header in particular\n+can reveal information the user would consider to be of a private\n+nature, because the understanding of particular languages is often\n+strongly correlated to the membership of a particular ethnic\n+group. User agents which offer the option to configure the contents of\n+an Accept-Language header to be sent in every request are strongly\n+encouraged to let the configuration process include a message which\n+makes the user aware of the loss of privacy involved.\n+\n+An approach that limits the loss of privacy would be for a user agent\n+to omit the sending of Accept-Language headers by default, and to ask\n+the user whether it should start sending Accept-Language headers to a\n+server if it detects, by looking for any Vary or Alternates response\n+headers generated by the server, that such sending could improve the\n+quality of service.\n\n[end of text.]\n\n\n\n", "id": "lists-010-1983479"}, {"subject": "(USERTRACKING) Draft tex", "content": "The following issue, owned by Brian Behlendorf, is on the HTTP/1.1\nissues list:\n\n* BB USERTRACKING Privacy from user tracking based on accept \n  ([holtman 6.2, 6.3])\n\nI mailed Brian for status about this on April 7, but he has not\nresponded so far.  I'll take over the issue until I hear from Brian.\nBelow is proposed text for the 1.1 draft, adapted from [holtman 6.2,\n6.3].\n\nI'd like to close this issue in a few days: I do not think it is very\ncontroversial.  If you have any comments on, or improvements for, the\ntext below, please send them in private e-mail. I'll summarize on the\nlist.\n\nKoen.\n\n----snip----\n\n   [## Note: the text below could be in a separate section, but could\n   also be added to the existing section 14.7 on `Privacy issues\n   connected to Accept headers'.##]\n\n14. Security Considerations\n\n14.x User tracking based on accept headers\n\n   Elaborate user-customized accept header fields sent in every\n   request, in particular if these include quality values, can be used\n   by servers as relatively reliable and long-lived user identifiers.\n   Such user identifiers would allow content providers to do\n   click-trail tracking, and would allow collaborating content\n   providers to match cross-server click-trails or form submissions of\n   individual users.  Note that for many users not behind a proxy, the\n   network address of the host running the user agent will also serve\n   as a long-lived user identifier.  In environments where proxies are\n   used to enhance privacy, user agents should be conservative in\n   offering accept header configuration options to end users.  As an\n   extreme privacy measure, proxies could filter the accept headers in\n   relayed requests.  General purpose user agents which provide a high\n   degree of header configurability should warn users about the loss\n   of privacy which can be involved.\n\n\n[End of text]\n\n\n\n", "id": "lists-010-2024387"}, {"subject": "Re: (ACCEPT*) Consensu", "content": "> |  [##Note: the new 416 is similar to the 406 response code in the old\n>    draft.  406 cannot be used for content negotiation compatibility\n>    reasons##]\n\nI am not aware of any such reasons.  Please explain.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-2033246"}, {"subject": "Re: (ACCEPT*) Consensu", "content": "Roy T. Fielding:\n>\n>> |  [##Note: the new 416 is similar to the 406 response code in the old\n>>    draft.  406 cannot be used for content negotiation compatibility\n>>    reasons##]\n>\n>I am not aware of any such reasons.  Please explain.\n\nFirst some background:\n\nThe old 1.1-01 draft required 406 responses to contain `a list of\nresource characteristics and locations from which the user or user\nagent can choose the one most appropriate.'  Such a thing is not\nrequired in the 416 response I defined, so I thought it best to assign\na new code, rather than rewrite the 406 text.\n\nAccording to my current draft texts, we will have:\n\n  416 Not Acceptable\n\n  The resource identified by the Request-URI and Host request header\n  (present if the request-URI is not an absoluteURI) is only capable of\n  generating response entities which have content characteristics not\n  acceptable according to the accept headers sent in the request.\n\n  HTTP/1.1 servers are allowed to return responses which are not\n  acceptable according to the accept headers sent in the request. In\n  some cases, this may even be preferable over sending a 416\n  response. User agents are encouraged to inspect the headers of an\n  incoming response to determine if it is acceptable. If the response is\n  not acceptable, user agents should interrupt the receipt of the\n  response if doing so would save network resources.  If it IS unknown\n  whether an incoming response would be acceptable, a user agent should\n  temporarily stop receipt of more data and query the user for a\n  decision on further actions.\n\nand\n\n406 None Acceptable\n\n   This status code is reserved for future use by a planned content\n   negotiation mechanism.  HTTP/1.1 user agents receiving a 406\n   response which includes a Location header can treat this response\n   as they would treat a 303 (See Other) response.  If no Location\n   header is included, the appropriate action is to display the entity\n   enclosed in the response to the user.\n\nNow the compatibility reasons:\n\nUser agents may want to take an automatic action when getting a 416\n(Not Acceptable) response.a An example would be popping up a dialog box\nwith a standard error message (in the user's native language), while\nleaving the original page containing the link followed on screen.\nSuch automatic action would have the advantage that the user can still\nsee the original link followed.\n\nThis is why 406 and 416 cannot be merged into one status code.  Doing\nthe 416 automatic action for a 406 response would be wholly\ninappropriate: you absolutely want the entity included in the 406\nresponse to be displayed on screen, because this entity will likely be\na list of links to the various alternate resources that are available.\n\nOf course, I know of no existing user agent that does automatic\nactions on 4xx error responses.  But we have to account for future\nagents doing it: that is why we have these 3-digit response codes in\nthe first place.\n\nWe could swap 406 and 416 around if you think this better reflects\nhistorical use.  I don't think it would.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-2041044"}, {"subject": "Re: (ACCEPT*) Consensu", "content": "Only some marginal comments.\n\n>   A language tag identifies a natural language spoken, written, or\n>   otherwise conveyed by human beings for communication of information to\n> | other human beings. Computer languages are explicitly excluded. HTTP\n\nI like this definition of the meaning of language tags. It's\nmore general than that in RFC 1766\n\n:   The language tag always defines a language as spoken (or written) by\n:   human beings for communication of information to other human beings.\n:   Computer languages are explicitly excluded.\n\nand allows for sign languages for deaf people.\n\n>   Whitespace is not allowed within the tag and all tags are\n>   case-insensitive.  The namespace of language tags is administered by\n>   the IANA. Example tags include:\n> \n>          en, en-US, en-cockney, i-cherokee, x-pig-latin\n> \n>   where any two-letter primary-tag is an ISO 639 language abbreviation\n>   and any two-letter initial subtag is an ISO 3166 country code.\n\nPlease add a note to the effect that only \"en\" and \"en-US\" are\nlegal language tags at the time of publication of the document,\nand that the other tags are included only to illustrate how\nlanguage tags registered in the future may look.\n\n>   It may be contrary to be privacy expectations of the user to send an\n>   Accept-Language header with the complete linguistic preferences of the\n>   user in every request. For a discussion of this issue, see Section\n> | 14.7.\n\nI suppose the second occurence of \"be\" on the first line should\nbe removed.\n\n-- \nOlle Jarnefors, Royal Institute of Technology (KTH) <ojarnef@admin.kth.se>\n\n\n\n", "id": "lists-010-2051676"}, {"subject": "Re: (ACCEPT*) Consensu", "content": "> The old 1.1-01 draft required 406 responses to contain `a list of\n> resource characteristics and locations from which the user or user\n> agent can choose the one most appropriate.'  Such a thing is not\n> required in the 416 response I defined, so I thought it best to assign\n> a new code, rather than rewrite the 406 text.\n\nThat would be incorrect -- the 416 response would still require some\nmessage in the response as a fall-back to older clients (or simply\nas more information).\n\nWhat you describe is exactly what 406 is intended to do, so use it.\nRewrite whatever text no longer applies, but do not introduce a new\nerror code.  There do not exist any implementations that depend on\ndraft 01's description of 406 (not that it would matter, since the\ndescription is essentially the same even after your changes).\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-2061030"}, {"subject": "Re: (USERTRACKING) Draft tex", "content": "While I like Koen's draft I think that there is another point to bring up here, \nthe \"don't build systems that depend on it\" point. If clients start producing \nexotic accept headers as a means of supporting bogus demographic schemes we have \na clear violation of the \"separation of concerns\" principle. It will be \ndifficult enough to develop the content negotiation scheme. It will be much \nharder todo this if changing the accept header mechanism will break demographic \nprotocols.\n\n\nThere are several hacks which have been agressively marketed as solutions to the \ndemographics problem which will prevent usefull work by proxies. Chief culprits \ninclude systems based on cookies, adding user identifiers to the URI and so on.\n\nThe point about these systems is that they are fragile and will break as caching \nis more widely deployed. State Info is a mechanism for improving client/server \ninteraction. They are sub-optimal as session identifiers.\n\nI hope that we can address these issues directly in 1.2 and to that end I have 3 \nW3C technical drafts which describe an explicit mechanism for supporting session \nidentifiers. \n\nI don't have a problem with a site tracking the browsing patterns of its users \nwithin its site. After all persistent connections would reveal such information \nif they were long enough. The real privacy issues occur when going across sites.\n\n\nWe can't get into the discussion of the privacy issue at this time but when we \ndo I believe that there is much more of a middle ground between the content \nproviders and privacy advocates than people may believe. The main impediment to \na working solution is likely to be the one-man-and-his-perl-script companies \nwith a clueless hack they want to preserve until they go for a ludicrously \noverpriced IPO.\n\n\nPhill\n\n\n\n", "id": "lists-010-2069458"}, {"subject": "http URL defn in HTTP/1.", "content": "Hi,  \n\nIn reading though the HTTP/1.0 memo with info header\n \nHTTP Working Group                         T. Berners-Lee, MIT/LCS\nINTERNET-DRAFT                              R. Fielding, UC Irvine\n<draft-ietf-http-v10-spec-05.html>             H. Frystyk, MIT/LCS\nExpires August 19, 1996                          February 19, 1996\n\n\nI'm a bit confused by section 3.2.2 http URL.  Based on the\ngeneral syntax presented in section 3.2.1, an http URL can\ninclude 'params'.  \n\n[Section 3.2.2 of RFC1738 shows the use of ;type=<typecode>\nas intended for ftp URLs.  Section 3.3 of RFC1738 describing\nan http URL admits no 'params'.]\n\n\nShould section 3.2.2 be modified to the following?\n\nhttp_URL = \"http:\" \"//\" host [\":\" port] [\"/\" [path] [\"?\" query]]\n\n\n(apologies if this issue has been raised before)\n\n-- Dan\n\n\nDan Larner, larner@parc.xerox.com\nXerox PARC, 3333 Coyote Hill R., Palo Alto, CA, USA 94304\n(415) 812-4871 (office), (415) 812-4890 (fax)\n\n\n\n", "id": "lists-010-2078076"}, {"subject": "RE: (ACCEPT*) Consensu", "content": "I agree with Roy.\n\nThat said, isn't \"require\" a little strong? I seem to recall that it was\nstrongly suggested that a text/plain or test/html document be returned\nwith all error responses, with info about what went wrong, and (if\nappropriate) links to click on, etc, to help the user rectify the\nsituation. The spec for 406 adds more detail about what should be in the\ndocument when the response is 406.\n\nAfter all, while it might be rude to not return such information,\nnothing much will break if a server didn't. I wouldn't buy such a\nserver, but that's exactly what forces the implementors to do a better\njob.\n\n>----------\n>From: Roy T. Fielding[SMTP:fielding@avron.ICS.UCI.EDU]\n>Sent: Sunday, April 14, 1996 3:10 PM\n>To: Koen Holtman\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: (ACCEPT*) Consensus \n>\n>> The old 1.1-01 draft required 406 responses to contain `a list of\n>> resource characteristics and locations from which the user or user\n>> agent can choose the one most appropriate.'  Such a thing is not\n>> required in the 416 response I defined, so I thought it best to assign\n>> a new code, rather than rewrite the 406 text.\n>\n>That would be incorrect -- the 416 response would still require some\n>message in the response as a fall-back to older clients (or simply\n>as more information).\n>\n>What you describe is exactly what 406 is intended to do, so use it.\n>Rewrite whatever text no longer applies, but do not introduce a new\n>error code.  There do not exist any implementations that depend on\n>draft 01's description of 406 (not that it would matter, since the\n>description is essentially the same even after your changes).\n>\n> ...Roy T. Fielding\n>    Department of Information & Computer Science   \n>(fielding@ics.uci.edu)\n>    University of California, Irvine, CA 92717-3425   \n>fax:+1(714)824-4056\n>    http://www.ics.uci.edu/~fielding/\n>\n>\n\n\n\n", "id": "lists-010-2086537"}, {"subject": "Re: (ACCEPT*) Consensu", "content": "Roy T. Fielding:\n>\n>> The old 1.1-01 draft required 406 responses to contain `a list of\n>> resource characteristics and locations from which the user or user\n>> agent can choose the one most appropriate.'  Such a thing is not\n>> required in the 416 response I defined, so I thought it best to assign\n>> a new code, rather than rewrite the 406 text.\n>\n>That would be incorrect -- the 416 response would still require some\n>message in the response as a fall-back to older clients (or simply\n>as more information).\n\nDon't get me wrong, I completely agree that a `not acceptable'\nresponse must include _some_ error message as an entity.  What I did\nin my text for 416 is getting rid of the requirement that this error\nmessage contains `a list of resource characteristics and locations\nfrom which the user or user agent can choose the one most\nappropriate'.\n\nThe `you have to send along a list with appropriate alternatives'\nrequirement in the 1.1-00 draft means that you can only send a 406\nresponse if you indeed have appropriate alternatives.\n\nBut for the majority of URLs, there are no other alternative versions,\nthe only one you have is the one you just found to be unacceptable.\nA resource\n  http://blah.com/bla.mpeg\nfor which there is no alternative representation available cannot send\na 406 error under the 1.1-00 draft.  That is why I defined a new error\ncode, 416, for it to use.\n\n>What you describe is exactly what 406 is intended to do, so use it.\n\nYou may have intended 406 to be used like this, but the language in\nthe old spec did not allow 406 to be used in this way by plain\nresources.\n\n>Rewrite whatever text no longer applies, but do not introduce a new\n>error code.  There do not exist any implementations that depend on\n>draft 01's description of 406 (not that it would matter, since the\n>description is essentially the same even after your changes).\n\nI guess this means you want me to swap 406 and 416, because 416 is my\nrewritten 604.  OK, I have no problems with that.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-2097535"}, {"subject": "Re: http URL defn in HTTP/1.", "content": ">From:  Dan Larner <larner@parc.xerox.com>\n>\n>[Section 3.2.2 of RFC1738 shows the use of ;type=<typecode>\n>as intended for ftp URLs.  Section 3.3 of RFC1738 describing\n>an http URL admits no 'params'.]\n>\n\nBut RFC1738 (sorry I've got no reference, but my copy of the RFC\nis in another country right now), does specify that the ';' is a\nreserved characer in HTTP URLs, presumably to allow for future\nextensions, like params.\n\n>Should section 3.2.2 be modified to the following?\n>\n>http_URL = \"http:\" \"//\" host [\":\" port] [\"/\" [path] [\"?\" query]]\n\nI hope not, because I'm about to propose a \"version\" parameter to\nallow access to back revisions of a page stored in a version\ncontrol system like RCS or SCCS.\n\n- David\n\n\n\n", "id": "lists-010-2106964"}, {"subject": "Re: (ACCEPT*) Consensu", "content": "Olle Jarnefors:\n>\n>Only some marginal comments.\n>\n>>   A language tag identifies a natural language spoken, written, or\n>>   otherwise conveyed by human beings for communication of information to\n>> | other human beings. Computer languages are explicitly excluded. HTTP\n>\n>I like this definition of the meaning of language tags. It's\n>more general than that in RFC 1766\n\nYes. Note that the language tags discussion in the HTTP document\nshould not be interpreted to mean that HTTP allows more than RFC 1766\ndoes.  The text is this broad to ensure that HTTP clients will not be\ndisallowed from using the language tags that a future revision of RFC\n1766 may define.\n\n[...]\n>>   Whitespace is not allowed within the tag and all tags are\n>>   case-insensitive.  The namespace of language tags is administered by\n>>   the IANA. Example tags include:\n>> \n>>          en, en-US, en-cockney, i-cherokee, x-pig-latin\n>> \n>>   where any two-letter primary-tag is an ISO 639 language abbreviation\n>>   and any two-letter initial subtag is an ISO 3166 country code.\n>\n>Please add a note to the effect that only \"en\" and \"en-US\" are\n>legal language tags at the time of publication of the document,\n>and that the other tags are included only to illustrate how\n>language tags registered in the future may look.\n\nGood idea, I'll try to come up with some text.\n\n>\n>>   It may be contrary to be privacy expectations of the user to send an\n>>   Accept-Language header with the complete linguistic preferences of the\n>>   user in every request. For a discussion of this issue, see Section\n>> | 14.7.\n>\n>I suppose the second occurence of \"be\" on the first line should\n>be removed.\n\nOops, that second `be' should be a `the'.\n\n>Olle Jarnefors, Royal Institute of Technology (KTH) <ojarnef@admin.kth.se>\n\nKoen.\n\n\n\n", "id": "lists-010-2115198"}, {"subject": "Re: http URL defn in HTTP/1.", "content": ">>Should section 3.2.2 be modified to the following?\n>>\n>>http_URL = \"http:\" \"//\" host [\":\" port] [\"/\" [path] [\"?\" query]]\n\n>I hope not, because I'm about to propose a \"version\" parameter to\n>allow access to back revisions of a page stored in a version\n>control system like RCS or SCCS.\n\nI hope nobody intends to make such a proposal until after the 1.1 \ndraft is submitted...\n\n... in any case I think there might be somewhat more of a problem \nwith versions than one might think at first glance. Having built a \nserver on top of CMS a couple of years ago I don't think that there\nis a comprehensive version naming strategy avaliable.\n\nI think that it may be a more profitable approach to look at versioning \nin terms of annotations and link semantics and not in terms of URLs.\nAfter all we may well have an MD5 URN someday [I already have a protocol\nfor resolving these]. There would be little opportunity to make an\nMD5 URN work with versioning...\n\nAnyway, enough said... we can return to this in May.\n\nPhill\n\n\n\n", "id": "lists-010-2124950"}, {"subject": "PERSIST:  headers needed at all", "content": "I had a bizarre thought concerning persistent connections.\n\n<asbestos>\nSuppose we decreed that all HTTP/1.1 agents understand persistent\nconnections, without requiring them to pass special headers.  We also\ngrant clients and servers the right to close connections at their\ndiscretion following a request/response exchange (as we already do).\nServers could, optionally, honor Connection: Keep-alive from HTTP/1.0\nclients.\n\nI've probably overlooked something, but at first (and second) thought,\nthe presence of an HTTP/1.1 protocol version in the request and\nresponse should be enough to tell\n\na) a server that the client expects to hold the connection open\nb) a client that the server will try to hold the connection open.\n\nIf all the proxies between a user agent and an origin server understand\nHTTP/1.1, the entire connection can be held open.\n</asbestos>\n\nDropping the Connection header would simplify the protocol.  Why not?\n\nDave Kristol\n\n\n\n", "id": "lists-010-2133405"}, {"subject": "Re: PERSIST:  headers needed at all", "content": "Cool. Simplify the protocol and encourage the single-most important\npart of making http more well-behaved in the face of congestion\ncontrol. \n\nSince you can always close the connection anyway, why not?\n\n\n\n", "id": "lists-010-2141462"}, {"subject": "Re: PERSIST: headers needed at all", "content": "    Suppose we decreed that all HTTP/1.1 agents understand persistent\n    connections, without requiring them to pass special headers.  We also\n    grant clients and servers the right to close connections at their\n    discretion following a request/response exchange (as we already do).\n    \nThis has some obvious attractions, but before we took this step\nwe would have to get consensus on two issues:\n\n(1) all server (and cache) implementors would have to agree\nthat they are willing to implement persistent connections.  So\nfar, we've operated under the assumption that this is\noptional, so we have not tried to obtain near-unanimous consent.\n\n(2) more subtly, the existing proposal does not allow a client\nto pipeline its requests unless it has received a Persist:\nheader from the server.  \"Pipeline\" means \"send multiple\nrequests before receiving any responses.  So we would also have\nto make sure that there was never any reason that a 1.1 server\ncould not handle such behavior.  As I said, up to now we\nhave not tried to obtain near-unanimous consent for this.\n\nI'm not myself opposed to your proposal, but I'm not interested in\nspending a lot of time arguing about it if there are people opposed\nto it.\n\n    Dropping the Connection header would simplify the protocol.  Why not?\n\nThe Connection header is there for orthogonal extensibility reasons.\nIts purpose is to allow HTTP/1.1 caches to ignore future hop-by-hop\nheaders that may be defined in HTTP/1.x.  Roy added it to HTTP/1.1\nbecause he observed that if it had been in HTTP/1.0, we could have\nmore easily added persistent-connection support in a compatible way.\n\n-Jeff\n\n\n\n", "id": "lists-010-2148875"}, {"subject": "Re: PERSIST: headers needed at all", "content": "One problem is that there is a cost associated with holding a\nconnection open on the server.  If the client knows that it will\nnot be reusing the connection after the current request, then not\nasking for persistence means that the server can switch its process\nresources to a new connection immediately after sending the last\nresponse (as opposed to waiting for the close or timeout from the\nclient).  On the other hand, one might say that if the server is\nreaching its resource constraints, then it can close the connection\nanyway.\n\nThere are some clients that will not want to implement more than one\nrequest per connection (batch-mode and text-only clients in particular)\nand yet will want to take advantage of the other features of HTTP/1.1.\nAlthough they could do so even with your proposal, their inability\nto signal that they did not want persistence would use up server\nresources unnecessarily.\n\nI think the deciding issue is the problem of writing to a socket\nafter the other end has attempted to close it (but the close has\nnot yet reached the writer).  It is difficult to handle such an event\nbecause there is no indication of why the socket was closed or\nwhen (without looking at the TCP ack sequence, which generally isn't\navailable to the HTTP programmer).  If we could come up with a decidable\nset of rules for what to do when the connection closes and the\nclient isn't expecting it to close, then I suppose we could do without\nthe Persist header.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-2158013"}, {"subject": "Re: PERSIST: headers needed at all", "content": "I don't have real strong feelings about pushing on this idea.\nIt just seemed like a nice simplifying assumption.  I realize it's\na bit late in the game to propose such a change.\n\nJeffrey Mogul <mogul@pa.dec.com> wrote (in part):\n\n  > (2) more subtly, the existing proposal does not allow a client\n  > to pipeline its requests unless it has received a Persist:\n  > header from the server.  \"Pipeline\" means \"send multiple\n  > requests before receiving any responses.  So we would also have\n  > to make sure that there was never any reason that a 1.1 server\n  > could not handle such behavior.  As I said, up to now we\n  > have not tried to obtain near-unanimous consent for this.\n\nAn HTTP/1.1 response would signal that pipelining further requests after\nthe first is possible.  There is the question of what the client should\ndo if the server closes the connection (because it only wanted to handle\na single request) while the client is trying to send a train of further\nrequests.  I don't know the answer.\n\n[...]\n  > The Connection header is there for orthogonal extensibility reasons.\n  > Its purpose is to allow HTTP/1.1 caches to ignore future hop-by-hop\n  > headers that may be defined in HTTP/1.x.  Roy added it to HTTP/1.1\n  > because he observed that if it had been in HTTP/1.0, we could have\n  > more easily added persistent-connection support in a compatible way.\n\nYes, and that function could presumably be retained.  But an origin\nserver need not look for it.\n\n\"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU> wrote (in part):\n\n  > I think the deciding issue is the problem of writing to a socket\n  > after the other end has attempted to close it (but the close has\n  > not yet reached the writer).  It is difficult to handle such an event\n  > because there is no indication of why the socket was closed or\n  > when (without looking at the TCP ack sequence, which generally isn't\n  > available to the HTTP programmer).  If we could come up with a decidable\n  > set of rules for what to do when the connection closes and the\n  > client isn't expecting it to close, then I suppose we could do without\n  > the Persist header.\n\nAll true.  And yet, there's always the danger that the connection to\nthe client might close unexpectedly, and the client would have to\nrecover gracefully.  Would adopting my proposal make the situation\nworse?\n\nDave\n\n\n\n", "id": "lists-010-2167326"}, {"subject": "Re: PERSIST: headers needed at all", "content": "On Mon, 15 Apr 1996, Roy T. Fielding wrote:\n\n> One problem is that there is a cost associated with holding a\n> connection open on the server.  If the client knows that it will\n> not be reusing the connection after the current request, then not\n> asking for persistence means that the server can switch its process\n> resources to a new connection immediately after sending the last\n> response (as opposed to waiting for the close or timeout from the\n> client). \n> \n\nPerhaps there could be a \"Connection: non-persistant\" header.  It makes\nsense to have the most common situation be the default (requiring no\nheader) and add a header for the exceptional or at less common behavior.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-2177427"}, {"subject": "Re: PERSIST: headers needed at all", "content": "> \"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU> wrote (in part):\n>   > I think the deciding issue is the problem of writing to a socket\n>   > after the other end has attempted to close it (but the close has\n>   > not yet reached the writer).  It is difficult to handle such an event\n>   > because there is no indication of why the socket was closed or\n>   > when (without looking at the TCP ack sequence, which generally isn't\n>   > available to the HTTP programmer).  If we could come up with a decidable\n>   > set of rules for what to do when the connection closes and the\n>   > client isn't expecting it to close, then I suppose we could do without\n>   > the Persist header.\n> All true.  And yet, there's always the danger that the connection to\n> the client might close unexpectedly, and the client would have to\n> recover gracefully.  Would adopting my proposal make the situation\n> worse?\n\nIt seems like the \"worst case\" is the same, but the \"average case\"\nfor a server that really doesn't want to do persistent connections\nis worse. If the client and server have similar \"expectations\"\nI'd think there would be fewer connections closed ungracefully.\n\nI'm not sure where the side effects of this would fall out, though.\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-010-2186017"}, {"subject": "DNS lookup code..", "content": "As a late follow-on to the discussion about interaction of DNS and\nHTTP clients and proxies:\n\nOver the last couple of days, for various reasons, I've thrown\ntogether some code to do DNS lookups which return time-to-live (and\nincidentally, don't block the process).  The code at this point is is\n*not* industrial strength, viz. the caveats in the README (in\nparticular, the one about resolver search rules --- if you're at\nstuffed.down.quilt.org and you want crazy.quilt.org, with this code,\nyou have to ask for it by its full name; you can't just ask for\n\"crazy\").\n\nThat said, the code does at least pass smoke tests on several\ndifferent systems, and so may be of interest to some people here.\nIt's available in ftp://ftp.ai.mit.edu/pub/users/rst/dnshack.tar.gz\nfor the curious.  Comments appreciated, particularly from those who\nhave been dealing with DNS longer than I have.\n\nIf this is too far from official WG business, my apologies, but given\nsome of the comments about DNS interfaces in the prior discussions on\nthat topic, I thought it would be of interest.\n\nrst\n\n\n\n", "id": "lists-010-2195089"}, {"subject": "Re: http URL defn in HTTP/1.", "content": ">From:  hallam@w3.org\n>From: davidf@mks.com\n>\n>>I hope not, because I'm about to propose a \"version\" parameter to\n>>allow access to back revisions of a page stored in a version\n>>control system like RCS or SCCS.\n>\n>I hope nobody intends to make such a proposal until after the 1.1 \n>draft is submitted...\n\nWhy?  The syntax for HTTP URLs hasn't changed between 1.0 and 1.1,\nso there's no dependence on the version of HTTP.\n\n>... in any case I think there might be somewhat more of a problem \n>with versions than one might think at first glance. Having built a \n>server on top of CMS a couple of years ago I don't think that there\n>is a comprehensive version naming strategy avaliable.\n\nI don't expect to document semantics of version names.\nAs long as there are no colons in the version name, then my\nproposal should manage.  Even then, escaping colons, or using\n&#xx; would work.  We already have a prototype working as a\nserver extension for the Netscape and MS servers.\n\n>\n>I think that it may be a more profitable approach to look at versioning \n>in terms of annotations and link semantics and not in terms of URLs.\n>After all we may well have an MD5 URN someday [I already have a protocol\n>for resolving these]. There would be little opportunity to make an\n>MD5 URN work with versioning...\n\nAnnotations and link semantics might also be appropriate.  After\ntalking to Ari about how to present the idea, in light of his\nexperience with the byte-range proposal, I figured that URLs are\na good start, because the have the advantaqge that the users can\nenter them directly to fetch a particular revision.\n\n- David\n\n\n\n", "id": "lists-010-2202313"}, {"subject": "cookie draft availabl", "content": "A new draft of the state management, aka \"cookie\", draft is available at\nhttp://www.research.att.com/~dmk/cookie.html\n\n1) Don't be deceived by the name or date on late versions of the\ndocument.  This document has not been submitted as an I-D yet.  That\nwill happen after thorough vetting on the http-wg mailing list.  And\nthe name will be draft-ietf-http-state-mgmt-01 when it *is* submitted.\n\n2) You're welcome to examine the various intermediate versions, but you\nshould really pay attention to the \"latest version\", which is just that,\nand which contains change-bars from the previous I-D,\ndraft-kristol-http-state-mgmt-00.  Versions are identified by my change\nmanagement versions; the latest is 2.16.\n\n3) Address comments, or at least copies of comments, to the http-wg\nmailing list.\n\nBarring any serious flaws, I will submit a new I-D to IETF on Thursday,\nApril 25.  Please review and comment on the above document by 19:00 GMT\n(5 PM EDT) Wednesday, April 24.\n\nDave Kristol\nfor the state management sub-group\n\n\n\n", "id": "lists-010-2211037"}, {"subject": "Re: PERSIST: headers needed at all", "content": "John Franks <john@math.nwu.edu> wrote (yesterday):\n  > Perhaps there could be a \"Connection: non-persistant\" header.  It makes\n  > sense to have the most common situation be the default (requiring no\n  > header) and add a header for the exceptional or at less common behavior.\n\nAlbert-Lunde@nwu.edu (Albert Lunde) wrote (yesterday):\n  > > [I wrote]:\n  > > All true.  And yet, there's always the danger that the connection to\n  > > the client might close unexpectedly, and the client would have to\n  > > recover gracefully.  Would adopting my proposal make the situation\n  > > worse?\n  > \n  > It seems like the \"worst case\" is the same, but the \"average case\"\n  > for a server that really doesn't want to do persistent connections\n  > is worse. If the client and server have similar \"expectations\"\n  > I'd think there would be fewer connections closed ungracefully.\n\nI like John Franks's idea (but let's spell it \"non-persistent :-).\nSo the rules would be:\n\nServer:\n    For an HTTP/1.1 request, keep the connection open, unless there's a\n    Connection: non-persistent (or whatever) header.  Furthermore, if\n    the server chose to close the connection, it would send a\n    Connection: non-persistent response header.\n\n    For an HTTP/1.0 request, a server could implement the existing,\n    sometimes honored Connection: keepalive convention.\n\nClient:\n    An HTTP/1.0 client would, of course, do what it does now, which may\n    include using the existing Connection: keepalive convention.\n\n    An HTTP/1.1 client expect a connection to remain open, but would\n    decide based on the first response from a server.  If it didn't\n    want a connection to remain open, it would send a Connection:\n    non-persistent request header.\n\n    If the HTTP/1.1 client receives an HTTP/1.0 response, it expects\n    the connection to close.  Otherwise it expects the connection to\n    remain open, unless it receives a Connection: non-persistent\n    response header.\n\n\nSuperficially this proposal may seem complex.  However, for the default\ncase HTTP/1.1 agents need send no extra headers to get/maintain the\npreferred kept-alive connection behavior.  While Connection:\nnon-persistent can be used to advise, it isn't essential to the\nprotocol's functioning correctly.\n\nDave Kristol\n\n\n\n", "id": "lists-010-2219383"}, {"subject": "Re: cookie draft availabl", "content": "One comment on this:\n\n   * Cache-control: Max-Age=0 or Expires:<a date in the past>.\n     (Example: Expires: Thu, 01 Jan 1980 00:00:00 GMT), to ``pre-\n     expire'' a cache entry.  The resource may be cached, but the cache\n     must validate it before returning it to the client.\n\nThe word \"must\" here is not accurate if \"max-age=0\" is used.  The\nprotocol explicitly allows caches to \"cheat\" on expiration dates,\nnot because I think it's a good idea, but because people have insisted\nthat they will do it anyway.\n\nAlthough there is some residual grumbling about it, the latest version\nnow includes a \"must-revalidate\" Cache-control directive, which seems\nto be more appropriate for what you are trying to do here.  So I would\nreplace that paragraph with these three:\n\n   * \"Cache-control: must-revalidate\", which means that the\n     resource may be cached, but the cache must validate it before\n     returning it to the client.\n\n   * \"Cache-control: Max-Age=0\", to ``pre-expire'' a cache entry.  The\n     resource may be cached, but the cache SHOULD validate it before\n     returning it to the client.  However, not all caches will do so\n     in every case.\n\n   Because HTTP/1.0 caches ignore Cache-control directives, it might\n   also be appropriate to send \"Expires: <a date in the past>\"\n   (Example: Expires: Thu, 01 Jan 1980 00:00:00 GMT) to avoid\n   improper caching.\n\n-Jeff\n\n\n\n", "id": "lists-010-2229024"}, {"subject": "Re: cookie draft availabl", "content": "Dave,\n\nI would restart the abstract with a much more concise statement of\nwhat you are up to, like:\n\n|This proposal specifies a method for creating a stateful session\n|with HTTP requests and responses.  It describes two new headers,\n|Cookie: and Set-Cookie:, which carry state information between\n|participating origin servers and user agents.  The method described\n|differs from Netscape's Cookie proposal, but it can interoperate with\n|HTTP/1.0 user agents which use Netscape's method.\n\nI'm afraid I feel, in general, that the explanatory material in the\ndraft needs to be tighter.  There is some good stuff in here, but the\nintegration needs work.  The list of Koen's dimensions for the\nsolution space of stateful dialogs, for example, seems to be a useful\nthing for implementors to keep in mind, but I believe that the bullets\nwould have to be fleshed out a good bit before someone who has not\nbeen following the discussion would be sure of what you meant.  The\ncurrent user agent methods for displaying links is a problem in the\nother direction; it is a fairly detailed list of examples, but it is\nbasically a digression.\n\nI've also found a number of things that need clarification:\n\nIs the port part of the Fully Qualified Host Name?  Section 4.3\nimplies that it might be, but the definition of FQHN doesn't say the\nport is included.\n\nAll of the Examples in 5.1 name HTTP/1.0; is this meant to be 1.1 ?\n\nIn section 7.1, you list a number methods for User Agent Control of\nCookies, one of which is \"control the saving of a cookie on the basis\nof the cookie's Domain attribute\".  That should be broadened at least\nto allow the user agent to completely disable the saving of cookies\n(it would be nice, of course to control the saving of cookies on the\nbasis of arbitrary attributes, but that might be too much to ask for).\n\n \nIn Section 8.2, you discuss Cookie spoofing, but I believe that you\nare missing at least one of the possible problems--the way domain\nmatching is described, it appears that someone from the host sub.tld\ncould successfully get or spoof cookies for anyhost.sub.tld .  If that\nis not the case, you should describe how it is prevented; if it is,\nyou should make it clear.\n\nThe paradigm you describe implies that proxies/caches never introduce\na Set-Cookie header; I think it would be useful to make that a MUST\nNOT (to prevent overlap with potential Cookies set by the origin\nservers, which might happen if cache designers believed that this set\nof headers provided an appropriate method for tracking cache usage).\n\nThe implementation limits of 300 cookies and 20 per host seem a bit\nhigh (think of our friends writing the browser on the PDA), what\nwent into that number?  (Depending on how you viewed sessions, for example,\nyou could say 20 per host and 20 total--each session ending when you\nwent to a new host.  That would be really limiting, obviously, but why\n300?)\n\nSorry to hit you with a bunch of these things--but I will say in my\ndefense that I didn't wait until the 24th :).\n\nBest Regards,\nTed Hardie\n\n\n\n", "id": "lists-010-2237090"}, {"subject": "HTTP extensibility: musings and a proposa", "content": "Lately, I've been thinking about HTTP extensibility and contrasting it\nwith object system extensibility. The implicit type compatibility rules\nin the HTTP is much looser than for (e.g.) C++ -- one can add often new\nheaders to HTTP requests and responses in a backwards compatible way,\nwhereas such extensions would not pass the type checker of most strongly\ntyped languages. New headers can't have arbitrary semantics: they have\nto have semantics such that proxies can just forward them, and servers\ncan ignore them, if not understood.\n\nAnother interesting aspect of HTTP in this regard is the Connection:\nheader. Proxies are supposed to delete headers named in the Connection\nheader before forwarding them. This allows addition of headers that are\nonly supposed to apply to directly connected clients and servers to be\nadded to HTTP, without fear that they will be forwarded out of scope by\nunaware proxies.\n\nThis notion can be extended -- it is easy to imagine adding headers that\norigin-servers can't just ignore when they don't understand them. In\nsuch cases, they should return a status code saying that they reject the\nrequest; the client might retry using some alternate, perhaps less\nfucntional, headers that the server did understand.\n\nThe description of such a header might be like this:\n------------------\n10.xx Critical\n\nThe Critical request header lists other header names that the server\nmust understand in order to correctly service the request. If the server\ndoes not understand the listed header names, then it must respond with\nstatus code 5xx Unknown Header, instead of the default action of\nignoring the header and executing the request. The purpose of this\nheader is to allow new headers to be introduced in future versions of\nHTTP that servers can't just ignore.\n\nCritical= \"Critical\" \":\" 0#( header-name )\n--------------------\n\nIF (and I admit it's a big if) this all seems super-obvious, then it\nmight be a good idea to put this into HTTP 1.1, in order to allow us to\nadd things to 1.2 that might otherwise have to wait. If there's the\nleast little controversy, or even any details to work out, then its too\nlate for 1.1. (And I realize that this seems like it might overlap with\nPEP, which I haven't read because it wouldn't be in 1.1 and so I could\nput it off -- I'll accept abuse on that score.)\n\nBut I thought it was an interesting enough idea to bring it up now.\n\nComments?\n\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-2247506"}, {"subject": "Re: HTTP extensibility: musings and a proposa", "content": "Read the pep draft: \nftp://ftp.ietf.cnri.reston.va.us/internet-drafts/draft-khare-http-pep-01.txt\n\nDonald\n\nOn Wed, 17 Apr 1996, Paul Leach wrote:\n\n> Date: Wed, 17 Apr 1996 17:28:30 -0700\n> From: Paul Leach <paulle@microsoft.com>\n> To: \"'http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com'\"\n     <http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n> Subject: HTTP extensibility: musings and a proposal\n> \n> Lately, I've been thinking about HTTP extensibility and contrasting it\n> with object system extensibility. The implicit type compatibility rules\n> in the HTTP is much looser than for (e.g.) C++ -- one can add often new\n> headers to HTTP requests and responses in a backwards compatible way,\n> whereas such extensions would not pass the type checker of most strongly\n> typed languages. New headers can't have arbitrary semantics: they have\n> to have semantics such that proxies can just forward them, and servers\n> can ignore them, if not understood.\n> \n> Another interesting aspect of HTTP in this regard is the Connection:\n> header. Proxies are supposed to delete headers named in the Connection\n> header before forwarding them. This allows addition of headers that are\n> only supposed to apply to directly connected clients and servers to be\n> added to HTTP, without fear that they will be forwarded out of scope by\n> unaware proxies.\n> \n> This notion can be extended -- it is easy to imagine adding headers that\n> origin-servers can't just ignore when they don't understand them. In\n> such cases, they should return a status code saying that they reject the\n> request; the client might retry using some alternate, perhaps less\n> fucntional, headers that the server did understand.\n> \n> ...\n> \n> But I thought it was an interesting enough idea to bring it up now.\n> \n> Comments?\n> \n> ----------------------------------------------------\n> Paul J. Leach            Email: paulle@microsoft.com\n> Microsoft                Phone: 1-206-882-8080\n> 1 Microsoft Way          Fax:   1-206-936-7329\n> Redmond, WA 98052\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\nhttp://www.cybercash.com           http://www.eff.org/blueribbon.html\n\n\n\n", "id": "lists-010-2258629"}, {"subject": "A not quite internet draft of the HTTP 1.1 is available", "content": "We will announce on the mailing list when the real internet draft is sent\nto the Internet drafts editor.\n\nWith some luck, this will be by Monday.  (it won't be before the weekend,\nunfortunately).\n\nIn the meanwhile...\n\nThe draft here is now substantially complete, though some sections\nare still being edited.  Areas outside of caching are not\nexpected to significantly changed between now and then.\nPersistent connections and content negotiation are in\nthis draft. The caching section is now complete in critical\nareas, and has been updated but there are still some slushy \nand fluid sections (marked as such in the draft).  I've not\nhad time to sanity check after major editing in the caching\nparts of the draft last night.\nRemaining questions are in bold face.\n\nNote that renumbering of sections is substantially similar to previous\ndrafts, to make it easier for you to find things.\nYou can inspect the work in progress, \n\no in <A href=\"Revs/Rev36.doc\">versions in Microsoft Word \n(with revision marks)</A>,\no in <A href=\"Revs/Rev36Clean.doc\">versions in Microsoft Word \n(without revision marks)</A>,\no <A href=\"Revs/Rev36Clean.htm\">HTML</A> (though the HTML version \nloses information, and I've made no attempt to clean it up yet), \no and <A href=\"Revs/Rev36.ps.gz\">\nGzip'ed Postscript of document with revisions</A> \no and <A href=\"Revs/Rev36Clean.ps.gz\">\nGzip'ed Postscript of document without revisions</A> \n\ncan be looked at. The Word version is most useful; you'll find annotations to\nindicate why edits were made (or can work from an issue back to\nthe changes); if Word isn't available to you, you'll find the\nPostscript nearly as useful, with both changebars and strikeouts\nand underlines for replacement text.\n- Jim Gettys\n\n\n\n", "id": "lists-010-2271029"}, {"subject": "A not quite internet draft of the HTTP 1.1 is available", "content": "I'm embarrassed; the previous message had entirely relative URL's\nto the documents.  If you didn't happen to know that previous versions\nwere available from the Issues page, you'd be mystified.  This message\nfixes this.\n\nWe will announce on the mailing list when the real internet draft is sent\nto the Internet drafts editor.\n\nWith some luck, this will be by Monday.  (it won't be before the weekend,\nunfortunately).\n\nIn the meanwhile...\n\nThe draft here is now substantially complete, though some sections\nare still being edited.  Areas outside of caching are not\nexpected to significantly changed between now and then.\nPersistent connections and content negotiation are in\nthis draft. The caching section is now complete in critical\nareas, and has been updated but there are still some slushy \nand fluid sections (marked as such in the draft).  I've not\nhad time to sanity check after major editing in the caching\nparts of the draft last night.\nRemaining questions are in bold face.\n\nNote that renumbering of sections is substantially similar to previous\ndrafts, to make it easier for you to find things.\nYou can inspect the work in progress, \n\no in <A href=\"http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/Revs/Rev36.doc\">\nversions in Microsoft Word (with revision marks)</A>,\no in <A href=\"http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/Revs/Rev36Clean.doc\">\nversions in Microsoft Word (without revision marks)</A>,\no <A href=\"http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/Revs/Rev36Clean.htm\">HTML</A> \n(though the HTML version loses information, and I've made no attempt to clean it up yet), \no and <A href=\"http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/Revs/Rev36.ps.gz\">\nGzip'ed Postscript of document with revisions</A> \no and <A href=\"http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/Revs/Rev36Clean.ps.gz\">\nGzip'ed Postscript of document without revisions</A> \n\ncan be looked at. The Word version is most useful; you'll find annotations to\nindicate why edits were made (or can work from an issue back to\nthe changes); if Word isn't available to you, you'll find the\nPostscript nearly as useful, with both changebars and strikeouts\nand underlines for replacement text.\n- Jim Gettys\n\n\n\n", "id": "lists-010-2279274"}, {"subject": "PERSIST:  propose to make defaul", "content": "The editorial group that is working on the HTTP/1.1 draft is strongly\ninterested in making persistent connections the default behavior for\nHTTP/1.1 agents.  The goal is to encourage widespread use of persistent\nconnections as quickly as possible.\n\nIF YOU DISAGREE, please address your objections to the http-wg mailing\nlist as quickly as possible.  I review the proposal below.\n\nDave Kristol\n=============\nThe presence of an HTTP/1.1 protocol version in the request and\nresponse is enough to tell\n\na) a server that the client expects to hold the connection open\nb) a client that the server will try to hold the connection open.\n\nIf all the proxies between a user agent and an origin server understand\nHTTP/1.1, the entire connection can be held open.\n\nAn HTTP/1.1 agent that does *not* want to keep a connection open sends a\nConnection: 1 [that's a change:  digit one, meaning \"just one\nconnection\"] request or response header.\n\nSo the rules would be:\n\n(HTTP/1.1) Server:\n    For an HTTP/1.1 request, keep the connection open, unless there's a\n    Connection: 1 header.  If the server chooses to close the\n    connection, it should send a Connection: 1 response header.\n\n    For an HTTP/1.0 request, a server could implement the existing,\n    sometimes-honored Connection: keepalive convention.\n\nClient:\n    An HTTP/1.0 client would, of course, do what it does now, which may\n    include using the existing Connection: keepalive convention.\n\n    An HTTP/1.1 client expects a connection to remain open, but would\n    decide to keep it open based on whether the response from a server\n    contains a Connection: 1 header.  If the client doesn't want a\n    connection to remain open, it should send a Connection: 1 request\n    header.\n\n    If the HTTP/1.1 client receives an HTTP/1.0 response, it expects\n    the connection to close, unless it receives a Connection: keepalive\n    response and it honors that convention.\n\n\nSuperficially this proposal may seem complex.  However, for the default\ncase HTTP/1.1 agents need send no extra headers to get/maintain the\npreferred kept-alive connection behavior.  While Connection: 1 can be\nused to advise, it isn't essential to the protocol's functioning\ncorrectly.\n\n\n\n", "id": "lists-010-2288337"}, {"subject": "Re: PERSIST:  propose to make defaul", "content": "At 04:35 PM 4/18/96 EDT, you wrote:\n>An HTTP/1.1 agent that does *not* want to keep a connection open sends a\n>Connection: 1 [that's a change:  digit one, meaning \"just one\n>connection\"] request or response header.\n\nI'd probably prefer text over \"1\", whether it was \"Connection: close\" or\n\"Connection: no-keepalive\" or whatever.\n\nI keep thinking there's something wrong with this that we don't see.  Why\nwas the persistent connections group bothering with the Persist: <correct\nhostname> requirement if it's not an issue here?  Were they ignoring the\nleveragable behavior of intermediary proxies downgrading the HTTP version of\nthe request to the origin server to the proxies highest supported version?\nIs that behavior absolutely universal amoung existing proxies?  Am I asking\nquestions that have already been asked an answered? (probably)\n\n-----\nthe Programmer formerly known as Dan          \n                                     http://www.spyglass.com/~ddubois/\n\n\n\n", "id": "lists-010-2297490"}, {"subject": "Re: cookie draft availabl", "content": "  > I would restart the abstract with a much more concise statement of\n  > what you are up to, like:\n  > \n  > |This proposal specifies a method for creating a stateful session\n  > |with HTTP requests and responses.  It describes two new headers,\n  > |Cookie: and Set-Cookie:, which carry state information between\n  > |participating origin servers and user agents.  The method described\n  > |differs from Netscape's Cookie proposal, but it can interoperate with\n  > |HTTP/1.0 user agents which use Netscape's method.\nI like it.  I'll change to use your words.\n  > \n  > I'm afraid I feel, in general, that the explanatory material in the\n  > draft needs to be tighter.  There is some good stuff in here, but the\n  > integration needs work.  The list of Koen's dimensions for the\nI invite some specifics, especially specific wording.\n  > solution space of stateful dialogs, for example, seems to be a useful\n  > thing for implementors to keep in mind, but I believe that the bullets\n  > would have to be fleshed out a good bit before someone who has not\n  > been following the discussion would be sure of what you meant.  The\nProbably true, but probably unimportant.  Adding words here, I think,\nwould only distract further.\n  > current user agent methods for displaying links is a problem in the\n  > other direction; it is a fairly detailed list of examples, but it is\n  > basically a digression.\nThe point was to reassure vendors (e.g., Netscape) that they're\nprobably doing what's needed already, and users, that they shouldn't\nnecessarily expect too much.\n  > \n  > I've also found a number of things that need clarification:\n  > \n  > Is the port part of the Fully Qualified Host Name?  Section 4.3\n  > implies that it might be, but the definition of FQHN doesn't say the\n  > port is included.\nNo, the port is not part of the FQHN, but it *is* used as a key for\nstoring cookies.  That is, the client must distinguish cookies that it\nreceives from servers on different ports of the same host.\n  > \n  > All of the Examples in 5.1 name HTTP/1.0; is this meant to be 1.1 ?\nYes.  I changed them.\n  > \n  > In section 7.1, you list a number methods for User Agent Control of\n  > Cookies, one of which is \"control the saving of a cookie on the basis\n  > of the cookie's Domain attribute\".  That should be broadened at least\n  > to allow the user agent to completely disable the saving of cookies\n  > (it would be nice, of course to control the saving of cookies on the\n  > basis of arbitrary attributes, but that might be too much to ask for).\nI guess I need to know what you mean by \"save\".  If you disable sending\ncookies (first bullet), have you achieved what you want?  Or should the\nbullet read \"sending and saving\"?\n  > \n  >  \n  > In Section 8.2, you discuss Cookie spoofing, but I believe that you\n  > are missing at least one of the possible problems--the way domain\n  > matching is described, it appears that someone from the host sub.tld\n  > could successfully get or spoof cookies for anyhost.sub.tld .  If that\n  > is not the case, you should describe how it is prevented; if it is,\n  > you should make it clear.\nI believe a cookie from anyhost.sub.tld would indeed by sent to host\nsub.tld.\n  > \n  > The paradigm you describe implies that proxies/caches never introduce\n  > a Set-Cookie header; I think it would be useful to make that a MUST\n  > NOT (to prevent overlap with potential Cookies set by the origin\n  > servers, which might happen if cache designers believed that this set\n  > of headers provided an appropriate method for tracking cache usage).\nOkay.\n  > \n  > The implementation limits of 300 cookies and 20 per host seem a bit\n  > high (think of our friends writing the browser on the PDA), what\n  > went into that number?  (Depending on how you viewed sessions, for example,\n  > you could say 20 per host and 20 total--each session ending when you\n  > went to a new host.  That would be really limiting, obviously, but why\n  > 300?)\nGood point.  The numbers come from the original Netscape proposal.  Lou\nMontulli felt that application writers needed some certainty about what\na client could save.  I don't have a good defense for 300 specifically.\nLou?\n  > \n  > Sorry to hit you with a bunch of these things--but I will say in my\n  > defense that I didn't wait until the 24th :).\nIndeed!  Thanks for being early.\n\nDave\n\n\n\n", "id": "lists-010-2306160"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "    >An HTTP/1.1 agent that does *not* want to keep a connection open sends a\n    >Connection: 1 [that's a change:  digit one, meaning \"just one\n    >connection\"] request or response header.\n    \n    I'd probably prefer text over \"1\", whether it was \"Connection:\n    close\" or \"Connection: no-keepalive\" or whatever.\n    \nThis was discussed during today's editorial teleconference.  The\nrationale was that this is (1) the most economical encoding we\ncould think of, and (2) it's legal according to the current grammar.\nThese might not be such great reasons, but it seems to work.\n\n    I keep thinking there's something wrong with this that we don't see.\n    \nThat's why we wanted to give people a chance to object.  If someone\nseems something actually wrong with this, we won't do it.  But if\nnobody can find anything wrong, it seems like the right thing to do.\n\n    Why was the persistent connections group bothering with the\n    Persist: <correct hostname> requirement if it's not an issue here?\n    Were they ignoring the leveragable behavior of intermediary proxies\n    downgrading the HTTP version of the request to the origin server to\n    the proxies highest supported version?  Is that behavior absolutely\n    universal amoung existing proxies?  Am I asking questions that have\n    already been asked an answered? (probably)\n\nThe original motivation behind \"Persist: hostname\" was based on\nshared misunderstanding of the HTTP version mechanism.  E.g.,\nwhat gets sent according to this grammar:\n       Request-Line   = Method SP Request-URI SP HTTP-Version CRLF\n\nThe group that devised the Persist: hostname approach was (almost\nentirely) under the mistaken belief that the HTTP-Version value\nwas end-to-end, but in fact it is hop-by-hop.  With only an end-to-end\nversion numbering scheme, we would have needed the hostname to check\nthat the hop-by-hop connection made sense.  But with a hop-by-hop\nindication of HTTP/1.1, it's not needed.\n\nOf course, this new approach would break if anyone had shipped\nan HTTP implementation that identifies itself as HTTP/1.1 without\nactually complying with the yet-to-be-issued HTTP/1.1 spefication.\nBut nobody is that foolish, right?\n\n-Jeff\n\n\n\n", "id": "lists-010-2318134"}, {"subject": "Re: cookie draft availabl", "content": ">  > The implementation limits of 300 cookies and 20 per host seem a bit\n>  > high (think of our friends writing the browser on the PDA), what\n>  > went into that number?  (Depending on how you viewed sessions, for example,\n>  > you could say 20 per host and 20 total--each session ending when you\n>  > went to a new host.  That would be really limiting, obviously, but why\n>  > 300?)\n>Good point.  The numbers come from the original Netscape proposal.  Lou\n>Montulli felt that application writers needed some certainty about what\n>a client could save.  I don't have a good defense for 300 specifically.\n\nI think that 300 is high if the cookies are to be used for state maintenance.\nBut if sitres want to use them for demographic data traking they will want many \nmore. It is enough of a pain in the ass to have to log into the ney york times \nevery day as it is.\n\nIf on the other hand we introduce client generated unlinkable session IDs\nmuch of the demographic data excahnge stuff can be done with them instead and\ncookies can be confined to operations that actually need state. This would mean \nthat a person with a PDA could probably function at shopping malls etc with a \nvery small number of cookies (20).\n\nThere is another benefit. Flushing the cookie cache need not disrupt the session \ninformation. Imagine the poor looser with a PDA is having hassle with an \ninteraction. A very natural need would be to want to \"reset\" the connection and \nstart from scratch. \n\nI'm minded of the Apple Mac problem here. I loath using Macs because they tend \nto be very opinionated about saving me from myself. This is all very well if the \nsystem is bug free. But when you hit a bug on a Mac you are likely to end up \nhaving to reboot because the system is waiting for an event that is never going \nto happen. I've often wanted a button to bypass some stupid warning box \ndemanding that an application really quit rather than try to save files on a \ndisk I took out of the machine for very good reasons. I think we need to provide \nthe user with a way to avoid having to flush usefull information when trying to \nreset the system after loosing with some CGI monkey's broken perl scripts.\n\n\nPhill\n\n\n\n", "id": "lists-010-2327678"}, {"subject": "Re: HTTP extensibility: musings and a proposa", "content": "> The Critical request header lists other header names that the server\n> must understand in order to correctly service the request. If the server\n\nWe called it \"Mandatory\" in draft 00, but got rid of it in favor of PEP\ndue to the niggling little details that turned out to be pervasive.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-2337044"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "> I'd probably prefer text over \"1\", whether it was \"Connection: close\" or\n> \"Connection: no-keepalive\" or whatever.\n\nI suppose that \"close\" is okay -- it just needs to be short and not likely\nto conflict with an an end-to-end header field.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-2345496"}, {"subject": "Re: HTTP extensibility: musings and a proposa", "content": "that was supposed to be draft 00 of HTTP/1.0 (over a year ago) when\nMandatory was defined.\n\n......Roy\n\n\n\n", "id": "lists-010-2353706"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "At 5:30 PM  -0700 4/18/96, Roy T. Fielding wrote:\n>> I'd probably prefer text over \"1\", whether it was \"Connection: close\" or\n>> \"Connection: no-keepalive\" or whatever.\n>\n>I suppose that \"close\" is okay -- it just needs to be short and not likely\n>to conflict with an an end-to-end header field.\n\nI would also like to voice a vote for \"close\" instead of \"1\". \"1\" might be\nmisread as \"l\" by a lame programmer, and they might also try to do \"2\" or\nsomething like that. Let's reduce the chance that someone will be lame on a\nMUST feature.\n\n\n\n", "id": "lists-010-2361171"}, {"subject": "Schedule for next HTTP 1.1 drafts", "content": "First, the bad news: there isn't a new draft yet.\n\nSecond, the good news: as of early this week, things are actually\ncoming together, since the problems reconciling\ncaching with content negotiation and range was tracked down to\nlack of shared understanding of key terminology.  For the first\ntime, I think the 1.1 draft is coming together.\n\nOK, so here is the roadmap.\n\nThe remaining major material (part of the range spec) and some additional\nsolidifying material in the caching area will get edited into the\nsource of the document over the weekend.  Gods (which ever you prefer)\nwilling, I will submit a new ID for the 1.1 spec this Monday, which\nwill be substantially complete for the first time, and almost all\nsignificant technical issues resolved.  I will announce it when it\nhappens on the W.G list; please grab it immediately, rather than\nwaiting for the ID editor to do his thing (which takes several days).\n\nThere are several caveats on Monday's ID:\n1) there will still be some minor areas not worked out completely;\nthis will be indicated in the text.\n2) the document will still have the current organization,\nwhich is cumbersome (caching, persistent connections, content negotiation\nshould be discussed early on in the document).  It is not clear at\nthis time that we will have time to reorganize the document before\nsubmission to the IESG.\n3) this draft will not have all the terminology problems\nstraightened out.  We only realized this week that this was the\nsource of the endless trashing that had been going on, and I won't\nhave time to try to incorporate any changes to clarify this.\n4) for those of you who may be looking at the document I made\navailable today, I make no guarantee about any material making this Monday's\nID I don't have in my hands by 10:00 AM Saturday EDT.\n\nDespite the above, Monday's draft is the one that you should review\ncarefully.  We have time only for one further draft after this before\nsubmission to IESG.  The latest that this second draft can happen is\nMay 2nd.  I have to go to a close friend's wedding on Friday, May 3rd.\n(he was my best man, and I'm in the wedding party of his wedding).\nThis implies that the latest I can recieve comments to make the draft\nhappen is April 30, and may need to be earlier (I'll have a better\nfeeling after I see how hard getting the Monday ID out is.).  \n\nExactly how we'll handle comments is yet to be worked out; but as an outline,\nI'd like them to go to the working group list (so there is a archival\nrecord), and probably have the people who've been responsible for\nparticular areas vet the comments and get me the results.  (It is not\nclear to me that I can single handedly wade through too many\ncomments).\n\nIn general, the earlier comments are made, the more likely we can\nreact to them.\n\nNote that if there are no objections raised, we plan to integrate\nthe digest authentication draft into the last ID (John Franks will be sending\nmail soon on this topic) of the HTTP 1.1 document.\n\nOk, in summary:\nMonday April 22   ID draft 02 issue\nTuesday, April 30, 9AM EDT Deadline for comments on April 22 draft.\nThursday, April 25   Decision point on incorporating digest.\nThursday, May 2   ID draft 03 issue\n\nIf the ID03 is deemed adequate by the working group,\nLarry will submit it to the IESG for standards track approval.\nNote that proposed standards can cycle (i.e. get reissued) if\nwe find some significant problem.  I don't know the IESG meeting\ndates, unfortunately, so that is as far as I can go.\n\nNote that there are several forcing functions on these dates outside\nof our control:\n1) various product schedules, which we've been led to believe\nhave influence on what is going on.\n2) The next web conference in Paris, the week of May 6-10, which\nlots of us will be at.\n3) I've not taken a day off for almost a month now, and I'm\ntaking a weeks vacation after Paris.  I'd like to get reacquainted with\nmy wife and daughter.  So if we don't make the above dates,\nwe'll miss by a significant margin.\n- Jim Gettys\n\n\n\n", "id": "lists-010-2369093"}, {"subject": "Digest Authentication in HTTP/1.", "content": "Last week a draft of the Digest Access Authentication Scheme was\nposted to this list for comment.  A number of helpful comments were\nreceived.  All involved improvements of the exposition rather than\nsubstantive changes in the specification.  These improvements have\nbeen incorporated into the specification draft.\n\nThe HTTP/1.1 editorial group plans to incorporate the contents of the digest\nauthentication draft into the HTTP/1.1 draft specification unless \nsubstantive and well founded objections are raised by Wed. April 24.\nIf you have such objections please post to the HTTP working list,\nthe WWW-security list or send mail to me.\n\nThe latest version of the draft is attached below.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n-----------------------------------------------------------------------\n\n\nHTTP Working Group                                           John Franks\nINTERNET-DRAFT                                       Philip Hallam-Baker\n<draft-ietf-http-digest-aa-??.txt>                  Jeffery L. Hostetler\n                                                             Paul  Leach\n                                                            Ari Luotonen\n                                                            Eric W. Sink\n                                                     Lawrence C. Stewart\n\nExpires SIX MONTHS FROM--->                                ???, 1996\n\n\n         A Proposed Extension to HTTP : Digest Access Authentication\n\n\nStatus of this Memo\n\n  This document is an Internet-Draft. Internet-Drafts are working\n  documents of the Internet Engineering Task Force (IETF), its areas,\n  and its working groups. Note that other groups may also distribute\n  working documents as Internet-Drafts.\n\n  Internet-Drafts are draft documents valid for a maximum of six months\n  and may be updated, replaced, or obsoleted by other documents at any\n  time. It is inappropriate to use Internet-Drafts as reference\n  material or to cite them other than as \"work in progress.\"\n\n  To learn the current status of any Internet-Draft, please check the\n  \"1id-abstracts.txt\" listing contained in the Internet-Drafts Shadow\n  Directories on ds.internic.net (US East Coast), nic.nordu.net\n  (Europe), ftp.isi.edu (US West Coast), or munnari.oz.au (Pacific\n  Rim).\n\n  Distribution of this document is unlimited. Please send comments to\n  the HTTP working group at <http-wg@cuckoo.hpl.hp.com>.\n  Discussions of the working group are archived at\n  <URL:http://www.ics.uci.edu/pub/ietf/http/>. General discussions\n  about HTTP and the applications which use HTTP should take place on\n  the <www-talk@www10.w3.org> mailing list.\n\n\nAbstract\n\n  The protocol referred to as \"HTTP/1.0\" includes specification for a\n  Basic Access Authentication scheme.  This scheme is not considered to\n  be a secure method of user authentication, as the user name and\n  password are passed over the network in an unencrypted form.  A\n  specification for a new authentication scheme is needed for future\n  versions of the HTTP protocol.  This document provides specification\n  for such a scheme, referred to as \"Digest Access Authentication\".\n  The digesting method used by default is the RSA Data Security, Inc.\n  MD5 Message-Digest Algorithm [3].\n\n\n\n\nTable of Contents\n\nSTATUS OF THIS MEMO....................................................\n\n\nABSTRACT...............................................................\n\n\nTABLE OF CONTENTS......................................................\n\n\nINTRODUCTION...........................................................\n\n 1.1  PURPOSE .........................................................\n 1.2  OVERALL OPERATION ...............................................\n 1.3  REPRESENTATION OF DIGEST VALUES .................................\n 1.4  LIMITATIONS .....................................................\n\n2. DIGEST ACCESS AUTHENTICATION SCHEME.................................\n\n 2.1 SPECIFICATION OF DIGEST HEADERS ..................................\n  2.1.1 THE WWW-AUTHENTICATE RESPONSE HEADER ..........................\n  2.1.2 THE AUTHORIZATION REQUEST HEADER ..............................\n  2.1.3 THE AUTHENTICATION-INFO HEADER ................................\n 2.2 DIGEST OPERATION .................................................\n 2.3 SECURITY PROTOCOL NEGOTIATION ....................................\n 2.4 EXAMPLE ..........................................................\n 2.5 PROXY-AUTHENTICATION AND PROXY-AUTHORIZATION .....................\n\n3. SECURITY CONSIDERATIONS............................................\n\n 3.1 COMPARISON WITH BASIC AUTHENTICATION ............................\n 3.2 REPLAY ATTACKS ..................................................\n 3.3 MAN IN THE MIDDLE ...............................................\n 3.4 SPOOFING BY COUNTERFEIT SERVERS .................................\n 3.5 STORING PASSWORDS ...............................................\n 3.6 SUMMARY .........................................................\n\n4.  ACKNOWLEDGMENTS...................................................\n\n\n5. REFERENCES.........................................................\n\n\n6. AUTHORS ADDRESSES..................................................\n\n\n\nIntroduction\n\n\n1.1  Purpose\n\n  The protocol referred to as \"HTTP/1.0\" includes specification for a\n  Basic Access Authentication scheme[1].  This scheme is not considered\n  to be a secure method of user authentication, as the user name and\n  password are passed over the network in an unencrypted form.  A\n  specification for a new authentication scheme is needed for future\n  versions of the HTTP protocol.  This document provides specification\n  for such a scheme, referred to as \"Digest Access Authentication\".\n\n  The Digest Access Authentication scheme is not intended to be a\n  complete answer to the need for security in the World Wide Web. This\n  scheme provides no encryption of object content.  The intent is\n  simply to facilitate secure access authentication.\n\n  It is proposed that this access authentication scheme be included in\n  the proposed HTTP/1.1 specification.\n\n\n1.2  Overall Operation\n\n  Like Basic Access Authentication, the Digest scheme is based on a\n  simple challenge-response paradigm.  The Digest scheme challenges\n  using a nonce value.  A valid response contains a checksum (by\n  default the MD5 checksum) of the username, the password, the given\n  nonce value, and the requested URI.  In this way, the password is\n  never sent in the clear.  Just as with the Basic scheme, the username\n  and password must be prearranged in some fashion which is not\n  addressed by this document.\n\n\n1.3  Representation of digest values\n\n  An optional header allows the server to specify the algorithm used to\n  create the checksum or digest.  By default the MD5 algorithm is used\n  and that is the only algorithm described in this document.\n\n  For the purposes of this document, an MD5 digest of 128 bits is\n  represented as 32 ASCII printable characters.  The bits in the 128\n  bit digest are converted from most significant to least significant\n  bit, four bits at a time to their ASCII presentation as follows.\n  Each four bits is represented by its familiar hexadecimal notation\n  from the characters 0123456789abcdef.  That is, binary 0000 gets\n  represented by the character '0', 0001, by '1', and so on up to the\n  representation of 1111 as 'f'.\n\n\n1.4  Limitations\n\n  The digest authentication scheme described in this document suffers\n  from many known limitations.  It is intended as a replacement for\n  basic authentication and nothing more.  It is a password-based system\n  and (on the server side) suffers from all the same problems of any\n  password system.  In particular, no provision is made in this protocol\n  for the initial secure arrangement between user and server to\n  establish the user's password.\n\n  Users and implementors should be aware that this protocol is not as\n  secure as kerberos, and not as secure as any client-side private-key\n  scheme.  Nevertheless it is better than nothing, better than what is\n  commonly used with telnet and ftp, and better than Basic\n  authentication.\n\n  Some keyword-value pairs occurring in headers described below are\n  required to have values which are of the type \"quoted-string\" as\n  defined in section 2.2 of the HTTP/1.1 specification [2].  A\n  consequence is that these values represent strings in the US-ASCII\n  character set.  An unfortunate side effect of this is that digest\n  authentication is not capable of handling either user names or realm\n  names (see 2.1.1 below) which are not expressed in this character set.\n\n\n\n2. Digest Access Authentication Scheme\n\n\n2.1 Specification of Digest Headers\n\n  The Digest Access Authentication scheme is conceptually similar to\n  the Basic scheme.  The formats of the modified WWW-Authenticate\n  header line and the Authorization header line are specified below,\n  using the extended BNF defined in the HTTP/1.1 specification, section\n  2.1.  In addition, a new header, Authentication-info, is specified.\n\n\n\n2.1.1 The WWW-Authenticate Response Header\n\n  If a server receives a request for an access-protected object, and an\n  acceptable Authorization header is not sent, the server responds with\n  a \"401 Unauthorized\" status code, and a WWW-Authenticate header,\n  which is defined as follows:\n\n     WWW-Authenticate    = \"WWW-Authenticate\" \":\" \"Digest\"\n                              digest-challenge\n\n     digest-challenge    = 1#( realm | [ domain ] | nonce |\n                          [ digest-opaque ] |[ stale ] | [ algorithm ] )\n\n     realm               = \"realm\" \"=\" realm-value\n     realm-value         = quoted-string\n     domain              = \"domain\" \"=\" <\"> 1#URI <\">\n     nonce               = \"nonce\" \"=\" nonce-value\n     nonce-value         = quoted-string\n     opaque              = \"opaque\" \"=\" quoted-string\n     stale               = \"stale\" \"=\" ( \"true\" | \"false\" )\n     algorithm           = \"algorithm\" \"=\" ( \"MD5\" | token )\n\n  The meanings of the values of the parameters used above are as\n  follows:\n\n     realm\n     A string to be displayed to users so they know which username and\n     password to use.  This string should contain at least the name of\n     the host performing the authentication and might additionally\n     indicate the collection of users who might have access.  An example\n     might be \"registered_users@gotham.news.com\".  The realm is a \n     \"quoted-string\" as specified in section 2.2 of the HTTP/1.1 \n     specification [2].\n\n     domain\n     A comma-separated list of URIs, as specified for HTTP/1.0.  The\n     intent is that the client could use this information to know the\n     set of URIs for which the same authentication information should be\n     sent.  The URIs in this list may exist on different servers.  If\n     this keyword is omitted or empty, the client should assume that the\n     domain consists of all URIs on the responding server.\n\n     nonce\n     A server-specified data string which may be uniquely generated each\n     time a 401 response is made.  It is recommended that this string be\n     base64 or hexadecimal data.  Specifically, since the string is\n     passed in the header lines as a quoted string, the double-quote\n     character is not allowed.\n\n     The contents of the nonce are implementation dependent.  The\n     quality of the implementation depends on a good choice.  A\n     recommended nonce would include\n\n             H(client-IP \":\" time-stamp \":\" private-key )\n\n     Where client-IP is the dotted quad IP address of the client making\n     the request, time-stamp is a server-generated time value,  private-\n     key is data known only to the server.  With a nonce of this form a\n     server would normally recalculate the nonce after receiving the\n     client authentication header and reject the request if it did not\n     match the nonce from that header. In this way the server can limit\n     the reuse of a nonce to the IP address to which it was issued and\n     limit the time of the nonce's validity.  Further discussion of the\n     rationale for nonce construction is in section 3.2 below.\n\n     An implementation might choose not to accept a previously used\n     nonce or a previously used digest to protect against a replay\n     attack.  Or, an implementation might choose to use one-time nonces\n     or digests for POST or PUT requests and a time-stamp for GET\n     requests.  For more details on the issues involved see section 3.\n     of this document.\n\n     The nonce is opaque to the client.\n\n     opaque\n     A string of data, specified by the server, which should be returned by\n     the client unchanged.  It is recommended that this string be base64\n     or hexadecimal data.  This field is a \"quoted-string\" as specified\n     in section 2.2 of the HTTP/1.1 specification [2].\n\n     stale\n     A flag, indicating that the previous request from the client was\n     rejected because the nonce value was stale.  If stale is TRUE (in\n     upper or lower case), the client may wish to simply retry the\n     request with a new encrypted response, without reprompting the user\n     for a new username and password.  The server should only set stale\n     to true if it receives a request for which the nonce is invalid but\n     with a valid digest for that nonce (indicating that the client knows\n     the correct username/password).\n\n     algorithm\n     A string indicating the algorithm used to produce the digest or\n     checksum.  If this not present the MD5 algorithm is assumed. In\n     this document the string obtained by applying this algorithm to the\n     data \"data\" will be denoted by H(data).\n\n\n\n2.1.2 The Authorization Request Header\n\n  The client is expected to retry the request, passing an Authorization\n  header line, which is defined as follows.\n\n     Authorization       = \"Authorization\" \":\" \"Digest\" digest-response\n\n     digest-response     = 1#( username | realm | nonce | digest-uri |\n                              response | [ digest ] | [ algorithm ] |\n                              opaque )\n\n     username            = \"username\" \"=\" username-value\n     username-value      = quoted-string\n     digest-uri          = \"uri\" \"=\" digest-uri-value\n     digest-uri-value    = request-uri         ; As specified by HTTP/1.1\n     response            = \"response\" \"=\" response-digest\n     digest             = \"digest\" \"=\" entity-digest\n\n     response-digest     = <\"> 32LHEX <\">\n     entity-digest      = <\"> 32LHEX <\">\n     LHEX                = \"0\" | \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" |\n                           \"8\" | \"9\" | \"a\" | \"b\" | \"c\" | \"d\" | \"e\" | \"f\"\n\n\n  The definitions of response-digest and entity-digest above indicate\n  the encoding for their values. The following definitions show how the value\n  is computed:\n\n     response-digest     =\n          <\"> < H( H(A1) \":\" unquoted nonce-value \":\" H(A2) > <\">\n\n     A1             = unquoted username-value \":\" unquoted realm-value \n                                                \":\" password\n     password       = < user's password >\n     A2             = Method \":\" digest-uri-value\n\n\n\n  The \"username-value\" field is a \"quoted-string\" as specified in section\n  2.2 of the HTTP/1.1 specification [2].  However, the surrounding quotation\n  marks are removed in forming the string A1.  Thus if the Authorization\n  header includes the fields\n\n    username=\"Mufasa\", realm=\"myhost@testrealm.com\" \n\n  and the user Mufasa has password \"CircleOfLife\" then H(A1) would be\n  H(Mufasa:myhost@testrealm.com:CircleOfLife) with no quotation marks in \n  the digested string.\n\n  No white space is allowed in any of the strings to which the digest\n  function H() is applied unless that white space exists in the quoted\n  strings or entity body whose contents make up the string to be\n  digested.  For example, the string A1 in the illustrated above must be\n  Mufasa:myhost@testrealm.com:CircleOfLife with no white space on either\n  side of the colons.  Likewise, the other strings digested by H() must\n  not have white space on either side of the colons which delimit their\n  fields unless that white space was in the quoted strings or entity\n  body being digested.\n\n  \"Method\" is the HTTP request method as specified in section 5.1 of\n  [2].  The \"request-uri\" value is the Request-URI from the request line\n  as specified in section 5.1 of [2].  This may be \"*\", an \"absoluteURL\"\n  or an \"abs_path\" as specified in section 5.1.2 of [2], but it MUST\n  agree with the Request-URI. In particular, it MUST be an \"absoluteURL\"\n  if the Request-URI is an \"absoluteURL\".\n\n  The authenticating server must assure that the document designated\n  by the \"uri\" parameter is the same as the document served.  The\n  purpose of duplicating information from the request URL in this\n  field is to deal with the possibility that an intermediate proxy may\n  alter the client's request.  This altered (but presumably semantically\n  equivalent) request would not result in the same digest as that\n  calculated by the client.\n\n  The optional \"digest\" field contains a digest of the entity body and\n  some of the associated entity headers.  This digest can be useful in\n  both request and response transactions.  In a request it can insure the\n  integrity of POST data or data being PUT to the server.  In a response\n  it insures the integrity of the served document.  The value of the\n  \"digest\" field is an <entity-digest> which is defined as follows.\n\n    entity-digest = <\"> H(H(A1) \":\" unquoted nonce-value \":\" Method \":\" \n                                       entity-info \":\" H(entity-body)) <\">\n           ; format is <\"> 32LHEX <\">\n\n    entity-info = H(\n              digest-uri-value \":\"\n              media-type \":\"         ; Content-type, see section 3.7 of [2]\n              *DIGIT \":\"             ; Content length -- see 10.12 of [2]\n              content-coding \":\"     ; Content-encoding, see 3.5 of [2]\n              last-modified \":\"      ; last modified date, see 10.25 of [2]\n              expires                ; expiration date; see 10.19 of [2]\n              )\n\n    last-modified   = rfc1123-date  ; see section 3.2.1 of [2]\n    expires         = rfc1123-date\n\n\n  The entity-info elements incorporate the values of the URI used to\n  request the entity as well as the associated entity headers\n  Content-type, Content-length, Content-encoding, Last-modified, and\n  Expires.  These headers are all end-to-end headers (see section TBS of [2])\n  which must not be modified by proxy caches.  The \"entity-body\" is as\n  specified by section 10.13 of [2] or RFC 1864.\n\n  Note that not all entities will have an associated URI or all of\n  these headers.  For example, an entity which is the data of a\n  POST request will typically not have a digest-uri-value or\n  Last-modified or Expires headers.  If an entity does not have a\n  digest-uri-value or a header corresponding to one of the entity-info\n  fields, then that field is left empty in the computation of\n  entity-info.  All the colons specified above are present, however.\n  For example the value of the entity-info associated with POST data\n  which has content-type \"text/plain\", no content-encoding and a length \n  of 255 bytes would be H(:text/plain:255:::).\n\n  In the entity-info computation, except for the blank after the comma\n  in \"rfc1123-date\", there must be no white space between \"words\" and\n  \"tspecials\", and exactly one blank between \"words\" (see section 2.2 \n  of [2]).\n\n  Implementors should be aware of how authenticated transactions\n  interact with proxy caches.  The HTTP/1.1 protocol specifies that when\n  a shared cache (see section 13.10 of [2]) has received a request\n  containing an Authorization field and a response from relaying that\n  request, it MUST NOT return that response as a reply to any other\n  request, unless one of two Cache-control (see TBS) directives was\n  present in the response.  If the original response included the\n  ``must-revalidate'' Cache-control directive, the cache MAY use the\n  entity of that response in replying to a subsequent request, but MUST\n  first revalidate it with the origin server, using the request headers\n  from the new request to allow the origin server to authenticate the\n  new request.  Alternatively, if the original response included the\n  ``public'' Cache-control directive, the response entity MAY be\n  returned in reply to any subsequent request.\n\n\n\n2.1.3 The Authentication-info Header\n\n  When authorization succeeds, the Server may optionally provide a\n  Authentication-info header indicating that the server wants to\n  communicate some information regarding the successful authentication\n  (such as an entity digest or a new nonce to be used for the next\n  transaction).  It has two fields, digest and nextnonce.  Both\n  are optional.\n\n\n    Authentication-info = \"Authentication-info\" \":\" \n                                      1#( digest | nextnonce )\n\n    nextnonce      = \"nextnonce\" \"=\" nonce-value\n\n    digest = \"digest\" \"=\" entity-digest\n\n\n\n  The optional digest allows the client to verify that the body\n  of the response has not been changed en-route.  The server would\n  probably only send this when it has the document and can compute it.\n  The server would probably not bother generating this header for CGI\n  output.  The value of the \"digested-entity\" is an <entity-digest> which\n  is computed as described above.  \n\n  The value of the nextnonce parameter is the nonce the server wishes\n  the client to use for the next authentication response.  Note that\n  either field is optional.  In particular the server may send the\n  Authentication-info header with only the nextnonce field as a means of\n  implementing one-time nonces.  If the nextnonce field is present the\n  client is strongly encouraged to use it for the next WWW-Authenticate\n  header.  Failure of the client to do so may result in a request to\n  re-authenticate from the server with the \"stale=TRUE.\"\n\n\n\n\n\n2.2 Digest Operation\n\n  Upon receiving the Authorization information, the server may check\n  its validity by looking up its known password which corresponds to\n  the submitted username.  Then, the server must perform the same MD5\n  operation performed by the client, and compare the result to the\n  given response-digest.\n\n  Note that the HTTP server does not actually need to know the user's\n  clear text password.  As long as H(A1) is available to the server,\n  the validity of an Authorization header may be verified.\n\n  A client may remember the username, password and nonce values, so\n  that future requests within the specified <domain> may include the\n  Authorization line preemptively.  The server may choose to accept the\n  old Authorization information, even though the nonce value included\n  might not be fresh. Alternatively, the server could return a 401\n  response with a new nonce value, causing the client to retry the\n  request.  By specifying stale=TRUE with this response, the server\n  hints to the client that the request should be retried with the new\n  nonce, without reprompting the user for a new username and password.\n\n  The opaque data is useful for transporting state information around.\n  For example, a server could be responsible for authenticating content\n  which actually sits on another server.  The first 401 response would\n  include a domain field which includes the URI on the second server,\n  and the opaque field for specifying state information.  The client\n  will retry the request, at which time the server may respond with a\n  301/302 redirection, pointing to the URI on the second server.  The\n  client will follow the redirection, and pass the same Authorization\n  line, including the <opaque> data which the second server may\n  require.\n\n  As with the basic scheme, proxies must be completely transparent in\n  the Digest access authentication scheme. That is, they must forward\n  the WWW-Authenticate, Authentication-info and Authorization headers\n  untouched. If a proxy wants to authenticate a client before a request\n  is forwarded to the server, it can be done using the Proxy-\n  Authenticate and Proxy-Authorization headers.\n\n\n2.3 Security Protocol Negotiation\n\n  It is useful for a server to be able to know which security schemes a\n  client is capable of handling.\n\n  If this proposal is accepted as a required part of the HTTP/1.1\n  specification, then a server may assume Digest support when a client\n  identifies itself as HTTP/1.1 compliant.\n\n  It is possible that a server may want to require Digest as its\n  authentication method, even if the server does not know that the\n  client supports it.  A client is encouraged to fail gracefully if the\n  server specifies any authentication scheme it cannot handle.\n\n\n\n\n\n2.4 Example\n\n  The following example assumes that an access-protected document is\n  being requested from the server.  The URI of the document is\n  \"http://www.nowhere.org/dir/index.html\".  Both client and server know\n  that the username for this document is \"Mufasa\", and the password is\n  \"CircleOfLife\".\n\n  The first time the client requests the document, no Authorization\n  header is sent, so the server responds with:\n\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: Digest    realm=\"testrealm@host.com\",\n                            nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\",\n                            opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"\n\n  The client may prompt the user for the username and password, after\n  which it will respond with a new request, including the following\n  Authorization header:\n\nAuthorization: Digest       username=\"Mufasa\",\n                            realm=\"testrealm@host.com\",\n                            nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\",\n                            uri=\"/dir/index.html\",\n                            response=\"e966c932a9242554e42c8ee200cec7f6\",\n                            opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"\n\n\n\n2.5 Proxy-Authentication and Proxy-Authorization\n\n  The digest authentication scheme may also be used for authenticating\n  users to proxies or proxies to end servers.  Unlike the WWW-\n  Authenticate, and Authorization headers, the proxy versions, Proxy-\n  Authenticate and Proxy-Authorization, apply only to the current\n  connection and must not be passed upstream or downstream.  The\n  transactions for proxy authentication are very similar to those\n  already described.  Upon receiving a request which requires\n  authentication, the proxy/server must issue the \"HTTP/1.1 401\n  Unauthorized\" header followed by a \"Proxy-Authenticate\" header of the\n  form\n\n     Proxy-Authentication     = \"Proxy-Authentication\" \":\" \"Digest\"\n                                   digest-challenge\n\n  where digest-challenge is as defined above in section 2.1. The\n  client/proxy must then re-issue the request with a Proxy-Authenticate\n  header of the form\n\n     Proxy-Authorization      = \"Proxy-Authorization\" \":\"\n                                   digest-response\n\n  where digest-response is as defined above in section 2.1. When\n  authorization succeeds, the Server may optionally provide a Proxy-\n  Authentication-info header of the form\n\n     Proxy-Authentication-info = \"Proxy-Authentication-info\" \":\" nextnonce\n\n  where nextnonce has the same semantics as the nextnonce field in the\n  Authentication-info header described above in section 2.1.\n\n  Note that in principle a client could be asked to authenticate itself\n  to both a proxy and an end-server.  It might receive an \"HTTP/1.1 401\n  Unauthorized\" header followed by both a WWW-Authenticate and a Proxy-\n  Authenticate header.  However, it can never receive more than one\n  Proxy-Authenticate header since such headers are only for immediate\n  connections and must not be passed on by proxies.  If the client\n  receives both headers, it must respond with both the Authorization and\n  Proxy-Authorization headers as described above, which will likely\n  involve different combinations of username, password, nonce, etc.\n\n\n3. Security Considerations\n\n  Digest Authentication does not provide a strong authentication\n  mechanism.  That is not its intent.  It is intended solely to replace\n  a much weaker and even more dangerous authentication mechanism: Basic\n  Authentication.  An important design constraint is that the new\n  authentication scheme be free of patent and export restrictions.\n\n  Most needs for secure HTTP transactions cannot be met by Digest\n  Authentication.  For those needs SSL or SHTTP are more appropriate\n  protocols.  In particular digest authentication cannot be used for\n  any transaction requiring encrypted content.  Nevertheless many\n  functions remain for which digest authentication is both useful and\n  appropriate.\n\n\n3.1 Comparison with Basic Authentication\n\n  Both Digest and Basic Authentication are very much on the weak end of\n  the security strength spectrum. But a comparison between the two\n  points out the utility, even necessity, of replacing Basic by Digest.\n\n  The greatest threat to the type of transactions for which these\n  protocols are used is network snooping.  This kind of transaction\n  might involve, for example, online access to a database whose use is\n  restricted to paying subscribers.  With Basic authentication an\n  eavesdropper can obtain the password of the user.  This not only\n  permits him to access anything in the database, but, often worse,\n  will permit access to anything else the user protects with the same\n  password.\n\n  By contrast, with Digest Authentication the eavesdropper only gets\n  access to the transaction in question and not to the user's password.\n  The information gained by the eavesdropper would permit a replay\n  attack, but only with a request for the same document, and even that\n  might be difficult.\n\n\n\n\n3.2 Replay Attacks\n\n  A replay attack against digest authentication would usually be\n  pointless for a simple GET request since an eavesdropper would\n  already have seen the only document he could obtain with a replay.\n  This is because the URI of the requested document is digested in the\n  client response and the server will only deliver that document. By\n  contrast under Basic Authentication once the eavesdropper has the\n  user's password, any document protected by that password is open to\n  him.  A GET request containing form data could only be \"replayed\"\n  with the identical data.  However, this could be problematic if it\n  caused a CGI script to take some action on the server.\n\n  Thus, for some purposes, it is necessary to protect against replay\n  attacks.  A good digest implementation can do this in various ways.\n  The server created \"nonce\" value is implementation dependent, but if\n  it contains a digest of the client IP, a timestamp, and a private\n  server key (as recommended above) then a replay attack is not simple.\n  An attacker must convince the server that the request is coming from\n  a false IP address and must cause the server to deliver the document\n  to an IP address different from the address to which it believes it\n  is sending the document.  An attack can only succeed in the period\n  before the timestamp expires.  Digesting the client IP and timestamp\n  in the nonce permits an implementation which does not maintain state\n  between transactions.\n\n  For applications where no possibility of replay attack can be\n  tolerated the server can use one-time response digests which will not\n  be honored for a second use.  This requires the overhead of the\n  server remembering which digests have been used until the nonce\n  timestamp (and hence the digest built with it) has expired, but it\n  effectively protects against replay attacks. Instead of maintaining a\n  list of the values of used digests, a server would hash these values\n  and require re-authentication whenever a hash collision occurs.\n\n  An implementation must give special attention to the possibility of\n  replay attacks with POST and PUT requests.  A successful replay attack\n  could result in counterfeit form data or a counterfeit version of a\n  PUT file.  The use of one-time digests or one-time nonces is\n  recommended.  It is also recommended that the optional <digest> be\n  implemented for use with POST or PUT requests to assure the integrity\n  of the posted data.  Alternatively, a server may choose to allow\n  digest authentication only with GET requests. Responsible server\n  implementors will document the risks described here as they pertain to\n  a given implementation.\n\n\n3.3 Man in the Middle\n\n  Both Basic and Digest authentication are vulnerable to \"man in the\n  middle\" attacks, for example, from a hostile or compromised proxy.\n  Clearly, this would present all the problems of eavesdropping.  But\n  it could also offer some additional threats.  In particular, even\n  with digest authentication, a hostile proxy might spoof the client\n  into making a request the attacker wanted rather than one the client\n  wanted.  Of course, this is still much harder than a comparable\n  attack against Basic Authentication.\n\n  There are several attacks on the \"digest\" field in the\n  Authentication-info header.  In particular, the attacker can alter any\n  of the entity-headers not incorporated in the computation of the digest,\n  The attacker can alter most of the request headers in the client's\n  request, and can alter any response header in the origin-server's reply,\n  except those headers whose values are  incorporated into the \"digest\"\n  field.  \n  \n  Alteration of Accept* or User-Agent request headers can only result\n  in a denial of service attack that returns content in an unacceptable\n  media type or language. Alteration of cache control headers also can\n  only result in denial of service. Alteration of Host will be detected,\n  if the full URL is in the response-digest. Alteration of Referer or\n  From is not important, as these are only hints.\n\n\n3.4 Spoofing by Counterfeit Servers\n\n  Basic Authentication is vulnerable to spoofing by counterfeit\n  servers. If a user can be led to believe that she is connecting to a\n  host containing information protected by a password she knows, when in\n  fact she is connecting to a hostile server, then the hostile server\n  can request a password, store it away for later use, and feign an\n  error.  This type of attack is more difficult with Digest\n  Authentication.\n\n\n3.5 Storing passwords\n\n  Digest authentication requires that the authenticating agent (usually\n  the server) store some data derived from the user's name and password\n  in a \"password file\" associated with a given realm.  Normally this\n  might contain pairs consisting of username and H(A1), where H(A1) is\n  the digested value of the username, realm, and password as described\n  above.\n\n  The security implications of this are that if this password file is\n  compromised, then an attacker gains immediate access to documents on\n  the server using this realm.  Unlike, say a standard UNIX password\n  file, this information need not be decrypted in order to access\n  documents in the server realm associated with this file.  On the\n  other hand, decryption, or more likely a brute force attack, would be\n  necessary to obtain the user's password.  This is the reason that the\n  realm is part of the digested data stored in the password file.  It\n  means that if one digest authentication password file is compromised,\n  it does not automatically compromise others with the same username\n  and password (though it does expose them to brute force attack).\n\n  There are two important security consequences of this.  First the\n  password file must be protected as if it contained unencrypted\n  passwords, because for the purpose of accessing documents in its\n  realm, it effectively does.\n\n  A second consequence of this is that the realm string should be\n  unique among all realms which any single user is likely to use.  In\n  particular a realm string should include the name of the host doing\n  the authentication.  The inability of the client to authenticate the\n  server is a weakness of Digest Authentication.\n\n\n3.6 Summary\n\n  By modern cryptographic standards Digest Authentication is weak.  But\n  for a large range of purposes it is valuable as a replacement for\n  Basic Authentication.  It remedies many, but not all, weaknesses of\n  Basic Authentication.  Its strength may vary depending on the\n  implementation.  In particular the structure of the nonce (which is\n  dependent on the server implementation) may affect the ease of\n  mounting a replay attack.  A range of server options is appropriate\n  since, for example, some implementations may be willing to accept the\n  server overhead of one-time nonces or digests to eliminate the\n  possibility of replay while others may satisfied with a nonce like\n  the one recommended above restricted to a single IP address and with\n  a limited lifetime.\n\n  The bottom line is that *any* compliant implementation will be\n  relatively weak by cryptographic standards, but *any* compliant\n  implementation will be far superior to Basic Authentication.\n\n\n\n4.  Acknowledgments\n\n  In addition to the authors, valuable discussion instrumental in\n  creating this document has come from Peter J. Churchyard, Ned Freed,\n  and David M. Kristol.\n\n\n5. References\n\n   [1]  T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen.\n        \"Hypertext Transfer Protocol -- HTTP/1.0\"\n        Internet-Draft (work in progress), UC Irvine,\n        <URL:http://ds.internic.net/internet-drafts/\n        draft-ietf-http-v10-spec-00.txt>, March 1995.\n\n   [2]  T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen...\n        \"Hypertext Transfer Protocol -- HTTP/1.1\"\n        TBS\n\n\n   [3]  RFC 1321.  R.Rivest, \"The MD5 Message-Digest Algorithm\",\n        <URL:http://ds.internic.net/rfc/rfc1321.txt>,\n        April 1992.\n\n\n\n\n6. Authors Addresses\n\n   John Franks\n   john@math.nwu.edu\n   Professor of Mathematics\n   Department of Mathematics\n   Northwestern University\n   Evanston, IL 60208-2730, USA\n\n   Phillip M. Hallam-Baker\n   hallam@w3.org\n   European Union Fellow\n   CERN\n   Geneva\n   Switzerland\n\n   Jeffery L. Hostetler\n   jeff@spyglass.com\n   Senior Software Engineer\n   Spyglass, Inc.\n   3200 Farber Drive\n   Champaign, IL  61821, USA\n\n   Paul J. Leach\n   paulle@microsoft.com\n   Microsoft Corporation\n   1 Microsoft Way\n   Redmond, WA 98052, USA\n\n   Ari Luotonen\n   luotonen@netscape.com\n   Member of Technical Staff\n   Netscape Communications Corporation\n   501 East Middlefield Road\n   Mountain View, CA 94043, USA\n\n   Eric W. Sink\n   eric@spyglass.com\n   Senior Software Engineer\n   Spyglass, Inc.\n   3200 Farber Drive\n   Champaign, IL  61821, USA\n\n   Lawrence C. Stewart\n   stewart@OpenMarket.com\n   Open Market, Inc.\n   215 First Street\n   Cambridge, MA  02142, USA\n\n\n\n", "id": "lists-010-2379200"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "On Thu, 18 Apr 1996, Dave Kristol wrote:\n\n> IF YOU DISAGREE, please address your objections to the http-wg mailing\n> list as quickly as possible.  I review the proposal below.\n\nI disagree ... this is being proposed with far to little time to reflect\non inplications such as Roy Fielding raises with respect to clean and\nunclean close.\n\nHaving just recently spent days figuring out connection reset messages\nfrom Netscape which turned out to be our server ignoring the \nnon-protocol extra CR-LF following POST content. I think this proposal\nhas significant potential for increasing the probability that unclean\nclose will become the norm.  At best this will require significantly\nmore user friendly error handling.\n\nIt seems to me that the fundamental problem here will be a HTTP/1.1\nclient initiating a persistent connection with what turns out to be a\nHTTP/1.0 server w/o knowing the server is able to handle the additional\ndata. If I recall correctly some of the issues raised earlier in\nthe two phase PUT discussions, an HTTP/1.0 server could close after\nsending a brief response with the possiblity that the close would\nresult in a reset which could leave the client quite confused.\n\nIsn't this a relatively minor change which could be included with\nHTTP/1.2? At that point there will be more general experience\ndeploying the more conservative approach as well as more experience\nwriting the code.\n\nSo I would recommend against the change at this time but having said\nmy piece, I can accept the proposal as currently stated.\n\nDave Morris\n\n\n\n", "id": "lists-010-2430979"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "On Thu, 18 Apr 1996, David W. Morris wrote:\n\n> \n> \n> On Thu, 18 Apr 1996, Dave Kristol wrote:\n> \n> > IF YOU DISAGREE, please address your objections to the http-wg mailing\n> > list as quickly as possible.  I review the proposal below.\n> \n> I disagree ... this is being proposed with far to little time to reflect\n> on inplications such as Roy Fielding raises with respect to clean and\n> unclean close.\n> \n> Having just recently spent days figuring out connection reset messages\n> from Netscape which turned out to be our server ignoring the \n> non-protocol extra CR-LF following POST content. I think this proposal\n> has significant potential for increasing the probability that unclean\n> close will become the norm.  At best this will require significantly\n> more user friendly error handling.\n> \n> It seems to me that the fundamental problem here will be a HTTP/1.1\n> client initiating a persistent connection with what turns out to be a\n> HTTP/1.0 server w/o knowing the server is able to handle the additional\n> data. If I recall correctly some of the issues raised earlier in\n> the two phase PUT discussions, an HTTP/1.0 server could close after\n> sending a brief response with the possiblity that the close would\n> result in a reset which could leave the client quite confused.\n> \n...\n\n> So I would recommend against the change at this time but having said\n> my piece, I can accept the proposal as currently stated.\n> \n\nYour concerns are significant, but I don't understand how they are\naffected by the Connection: header.  HTTP/1.1 clients and servers will\nengage in both persistent and non-persistent connections.  They must\ncommunicate to each other which type of connection they wish to use.\nWe are not discussing what the default *behavior* of a client should\nbe (persistent vs non-persistent).  That is up to the implementation.\nWe are only discussing how the chosen behavior is communicated from\nclient to server.  This communication doesn't require a header\nfor every request.  One type of connection can be the default and no\n\"Connection:\" header is needed, while the other must be indicated by a\n\"Connection:\" header. \n\nThe discussion at hand is concerned with which should be the default\nin the sense of which is communicated by the absence of a\n\"Connection:\" header.  However this issue is decided, a client has the\noption to make every connection non-persistent, if it chooses.\nWhichever way this issue is decided I don't see how it would affect\nerror conditions.  A 1.1 client connecting to a server of unknown\nvintage will have the same problems to deal with in either case.\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-2440288"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "\"David W. Morris\" <dwm@shell.portal.com> wrote:\n  > On Thu, 18 Apr 1996, Dave Kristol wrote:\n  > \n  > > IF YOU DISAGREE, please address your objections to the http-wg mailing\n  > > list as quickly as possible.  I review the proposal below.\n  > \n  > I disagree ... this is being proposed with far to little time to reflect\n  > on inplications such as Roy Fielding raises with respect to clean and\n  > unclean close.\n  > \n  > Having just recently spent days figuring out connection reset messages\n  > from Netscape which turned out to be our server ignoring the \n  > non-protocol extra CR-LF following POST content. I think this proposal\n  > has significant potential for increasing the probability that unclean\n  > close will become the norm.  At best this will require significantly\n  > more user friendly error handling.\n\nI don't see the [sorry] connection between this Netscape bug and the\npersistent connections issue.\n  > \n  > It seems to me that the fundamental problem here will be a HTTP/1.1\n  > client initiating a persistent connection with what turns out to be a\n  > HTTP/1.0 server w/o knowing the server is able to handle the additional\n  > data. If I recall correctly some of the issues raised earlier in\n  > the two phase PUT discussions, an HTTP/1.0 server could close after\n  > sending a brief response with the possiblity that the close would\n  > result in a reset which could leave the client quite confused.\n\nI think John Franks described this quite elegantly.  We all agree that\nwe want persistent connections; we're just changing the default\nexpectation.  We've neither subtracted nor added problems.  No matter\nwhich way you do it, clients and servers must deal with unexpected\ncloses.\n  > \n  > Isn't this a relatively minor change which could be included with\n  > HTTP/1.2? At that point there will be more general experience\n  > deploying the more conservative approach as well as more experience\n  > writing the code.\n\nI tend to be conservative, too, and I really hesitated before even\nputting forth the idea so late in the game, but the more I think about\nthe \"persistent by default\" idea, the more comfortable I am with it.\nIf we wait until HTTP/1.2, we will have 3 x 3 way combinatorics to\nworry about, which will increase the odds that different versions won't\ninter-operate correctly.\n  > \n  > So I would recommend against the change at this time but having said\n  > my piece, I can accept the proposal as currently stated.\n\nNoted, and thanks for your comments.\nDave Kristol\n\n\n\n", "id": "lists-010-2450923"}, {"subject": "Re: cookie draft availabl", "content": "There was a brief private exchange concerning the minimum number of\ncookies that a client had to support.  Here are Lou Montulli's remarks\non the subject.\n\n  > 300 was the minimum that our integrated applications developers\n  > felt comfortable with.  I agree that would be a difficult\n  > goal to accomplish on a PDA.  \n\nDave Kristol\n\n\n\n", "id": "lists-010-2461150"}, {"subject": "Proxy authenticatio", "content": "Proxy authentication, if I read it right, does not work for architectures \nwith more than one proxy between the browser and server, each with their\nown security needs. Section 2.5 of the DAA spec says:\n\n\" the proxy versions, Proxy-\n  Authenticate and Proxy-Authorization, apply only to the current\n  connection and must not be passed upstream or downstream. \"\n\nThis needs to be fixed. We are working with proxies as stream transducers,\nthat can be piped to one-another or to (or from) firewall/caching proxies.\n(See \"Application-Specific Proxy Servers as HTTP Stream Transducers\" in\nWWW4, and our paper on using these for group annotation services in WWW5\nfor more information.) I believe that Digest Authentication should be passed\nasap, but the Proxy Authentication looks seperable from DAA, and I don't\nrecall seeing this issue discussed in the working group.\n\nThe problem arises in setups such as a group of browsers using a single\ngroup annotation proxy, which in turn proxies through a firewall to\norganizationally external servers. Under the current proposal, the clients\ncan authenticate to the group annotation proxy, which can then impose its\nsecurity policy on its data and services. However, only the group annotation\nproxy can authenticate to the firewall proxy. The firewall proxy would have \nto trust the group annotation proxy to never pass on requests from users\nnot authorized to use the firewall proxy, which is an unreasonable and\neasily breakable reliance. A scheme that allowed each browser to authenticate\nto each proxy would allow each proxy to maintain its own security policy\nand make its own checks. \n\nAlso, from a security architecture point of view, you definately want to\nbe able to authenticate the end user (browser) at any intervening proxy.\nWas there some reason to draft the proxy authentication with this \nrestriction?\nMez\n\n\n\n", "id": "lists-010-2468459"}, {"subject": "Re: cookie draft availabl", "content": "Ted Hardie:\n>\n>Dave,\n>\n[...]\n>The list of Koen's dimensions for the\n>solution space of stateful dialogs, for example, seems to be a useful\n>thing for implementors to keep in mind, but I believe that the bullets\n>would have to be fleshed out a good bit before someone who has not\n>been following the discussion would be sure of what you meant.\n\nActually, I wrote this list as a checklist for seeing if a stateful\ndialog _support_ mechanism (like cookies) is good enough.  So the list\nis for the http-wg, not for implementers, and can be deleted from the\ndraft.  Fleshing it out is not worth the trouble I think.\n\n[...] \n>In Section 8.2, you discuss Cookie spoofing, but I believe that you\n>are missing at least one of the possible problems--the way domain\n>matching is described, it appears that someone from the host sub.tld\n>could successfully get or spoof cookies for anyhost.sub.tld .\n\nEek!   I believe you are right, we missed that case.  anyhost.sub.tld\nwould indeed get\n\n      Cookie: $Version=\"1\";\n                      session_id=\"1234\";\n                      session_id=\"1111\";\n                                         ^^^^^No Domain=sub.tld here!\n\nI believe NetScape's original cookie mechanism required a Domain= in\nthe Set-Cookie header in order for the session_id=\"1111\" cookie set by\nsub.tld to get sent to anyhost.sub.tld, but the current rules do not\nrequire this.  \n\nWe could tweaks the draft to solve this: we could change the following\nsentences from Section 4.3.4:\n\n The value for the      \n domain attribute must be the value from the Domain attribute, if any, of  \n the corresponding Set-Cookie response header.  Otherwise the attribute    \n should be omitted from the Cookie request header.  \n\ninto\n\n The value for the domain attribute must be the (default) value of the\n Domain attribute of the corresponding Set-Cookie response header, if\n this value is not equal to the request-host of the request to be\n sent.  Otherwise the attribute should be omitted from the Cookie\n request header.\n\nBut I think the above tweak would not be the best solution (and we do\nneed to solve this).  We could solve this, and better match current\nuse of domain=, and improve the readability of the draft in several\nplaces, if we go for domain attributes which always start with a dot:\n\n  Set-Cookie: session_id=\"1111\";Domain=\".sub.tld\".\n\nOnly the default domain attribute, the request-host, would not start\nwith a dot.\n\nI include editing instructions which would bring this about below.\nNote that there also are two editing instructions which fix a mistake\nand an oversight in the draft, these are unconnected to domains\nstarting with dots.\n\n>                        Best Regards,\n>                                Ted Hardie\n\nKoen.\n\n--snip--\n\n\n#2.  TERMINOLOGY\n#\n\nRewrite text below:\n\n#Hosts can be specified either as an IP address or a FQHN string.\n#Sometimes we compare one host with another.  Host A domain-matches host\n#B if\n#\n#   * both hosts are IP addresses and their host strings match exactly;\n#     or\n#\n#   * both hosts are FQHN strings and their host strings match exactly;\n#     or\n#\n#   * both hosts are FQHN strings and host A's string has the form N.B,\n#     where B is host B's FQHN string, and N is a name string that may     |\n#     have embedded dots, but not leading or trailing dots.  If host B     |\n#     has a leading dot, the leading dot is ignored.  (So, x.y.com         |\n#     domain-matches .y.com.)\n#\n#Note that domain-match is not a commutative operation: a.b.c.com\n#domain-matches c.com, but not the reverse.                                |\n\nto:\n\nHosts can be specified either as an IP address or a FQHN string.\nSometimes we compare one host with a domain string.  Host A\ndomain-matches a domain B if\n\n   * the host is an IP address and equals the domain string\n     exactly;\n\n   * the host is an FQHN string and equals the domain string\n     exactly;\n\n   * the host is an FQHN string which has the from NB, where \n     where B is the domain string, and N is non-empty string.\n     (So the host x.y.com domain-matches the domain .y.com.).\n\n\n\n#4.2.2  Set-Cookie Syntax  The syntax for the Set-Cookie response header\n\nRewrite\n\n#Domain=domain\n#     Optional.  The Domain attribute specifies the host and domain name\n#     for which the cookie is valid.\n\nto\n\nDomain=domain\n      Optional.  The Domain attribute specifies the domain for which\n      the cookie is valid.  The domain must always start with a dot.\n\n\n#4.3.2  Rejecting Cookies \n#\n#To prevent possible security or privacy\n#violations, a user agent rejects a cookie (shall not store its\n#information) if any of the following is true:\n#\n#   * The value for the Path attribute is not a prefix of the request-\n#     URI.\n\nInsert here:\n\n    *  A value for the Domain attribute was supplied in the\n       Set-Cookie header, and this value does not start with a dot\n\n#\n#   * The value for the Domain attribute contains no embedded dots.        |\n#\n#   * The value for the request-host does not domain-match the Domain      *\n#     attribute.\n#\n\nRewrite text below:\n\n#   * The request-host is a name (not IP address), D is the value of the\n#     Domain attribute, and request-host has the form H.D, where H is a\n#     string, and H contains one or more dots.                             |\n\nto:\n\n   * The request-host is a name (not IP address), D is the value of\n     the Domain attribute, and request-host has the form HD,\n     where H is a string, and H contains one or more dots.\n\n#\n#Examples:                                                                 |\n#\n#   * A Set-Cookie from request-host y.x.foo.com for Domain=foo.com would\n#     be rejected, because H is y.x and contains a dot.                    |\n\nChange Domain=foo.com above to Domain=.foo.com\n\n#\n#   * A Set-Cookie from request-host x.foo.com for Domain=foo.com would    |\n#     be accepted.                                                         |\n\nChange Domain=foo.com above to Domain=.foo.com\n\n#\n#   * A Set-Cookie with Domain=.com, Domain=com., or Domain=.com., will    |\n#     always be rejected, because there is no embedded dot.\n\nDelete `Domain=com., ' above.\n\n\n#4.3.4  Sending Cookies to the Origin Server  \n\n#Domain Selection\n#     The Domain attribute of the cookie must domain-match the origin\n#     server's fully-qualified host name.                                  *\n\nOOPS!! The above text contains a mistake, it should be rewritten to:\n\n Domain Selection \n     The origin server's fully-qualified host name must\n     domain-match the Domain attribute of the cookie.\n\neven if we don't switch to domains starting with a dot.\n\n\n#8.2  Cookie Spoofing\n#\n#Proper application design can avoid spoofing attacks from related         |\n#domains.  Consider:\n#\n#  1.  User agent makes request to victim.hacker.edu, gets back cookie     |\n#      session_id=\"1234\" and sets the default domain victim.hacker.edu.\n\nOops!  Why did I not notice this sooner?  `hacker.edu' should of\ncourse be `cracker.edu', or else old-time hackers will accuse us of\npolluting their language.\n\nPlease do a global query-replace from `hacker.edu' to `cracker.edu'.\n\n#  2.  User agent makes request to spoof.hacker.edu, gets back cookie      |\n#      session-id=\"1111\", with Domain=\"hacker.edu\".                        |\n\nChange Domain=\"hacker.edu\" above to Domain=\".cracker.edu\"\n\n#  3.  User agent makes request to victim.hacker.edu again, and passes     |\n#\n#      Cookie: $Version=\"1\";                                               |\n#                      session_id=\"1234\";                                  |\n#                      session_id=\"1111\"; $Domain=\"hacker.edu\"             |\n\nChange Domain=\"hacker.edu\" above to Domain=\".cracker.edu\"\n\n#\n#      The server at victim.hacker.edu should detect that the second       |\n#      cookie was not one it originated by noticing that the Domain        |\n#      attribute is not for itself and ignore it.\n#\n#8.3  Unexpected Cookie Sharing\n#\n\nChange the text:\n\n#A user agent should make every attempt to prevent the sharing of session  |\n#information between hosts that do not domain-match.\n\nto:\n\nA user agent should make every attempt to prevent the sharing of session  |\ninformation between hosts that are not in the same domain.\n\nThat is all.\n\n\n\n", "id": "lists-010-2477919"}, {"subject": "Re: cookie draft availabl", "content": "Dave Kristol asked me to provide some specifics for tightening up some\nof the expository language in the draft.  Below are some suggested\nwording changes.  The original text is included with a > where I\nreferenced only a single sentence or so ; where I worked on a section,\nonly the section numbers are present.  In no case did I intend to\nchange the behavior of the protocol, so if you see something that\nlooks like it means something different, please flag it!\n\nRegards,\nTed Hardie\n________________________________________________________________________ \n>Fully-qualified host name (FQHN) means either the numeric IP address \n>of a host, or its full Internet domain name, resolved to a top-level \n>domain such as .com or .uk.\n\nFully qualified host name (FQHN) means either the numeric IP address\nof a host, or its fully qualified domain name.  The fully qualified\ndomain name is preferred, for the reasons given in RFC 1900 (ref).\nThe port number does not form part of the FQHN.\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n3. State and Sessions\n\nThis proposal describes a method for creating stateful sessions with\nHTTP requests and responses.  Currently, HTTP servers respond to each\nclient request without relating that request to previous or subsequent\nrequests; the proposed method allows clients and servers who wish to\nexchange state information to place http requests and responses within\na larger context. This context we term a session.  This context might,\nfor example, be used to create a \"shopping cart\", in which user\nselections can be aggregated before purchase, or a magazine browsing\nsystem, in which a user's previous reading affects which offerings are\npresented.\n\nThere are, of course, may different potential contexts and thus many\ndifferent potential types of session.  The designers' paradigm for the\nsessions created by the exchange of cookies has these key attributes:\n\n1. Each session has a beginning and an end.\n\n2. Each session is relatively short-lived.\n\n3. Either the user agent or the origin server may terminate a session.\n\n4. The session is created by the exchange of state information.\n\nAdditionally, the designers wish to create a system which fits within\nthe dimensions of the solution space for stateful dialogs identified\nby Koen Hamilton.  The system is thus meant to be:\n\n* Simple to implement.\n\n* Simple to use.\n\n* Compatible with previous implementations.\n\n* Easily deployed after standardization.\n\n* Reliable.\n\n* Protective of the privacy of its users.\n\n* Compatible with cache control.\n\n* Low-risk when used with older, non-compatible caches.\n\n* As complex as possible within the framework of its other\nrequirements.\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n4.2.1 \n>(Note that \"session\" here is a logical connection, not a physical one.\n>These logical sessions should not be confused with various \"keepalive\" \n>proposals for physical sessions.)\n\n(Note that the \"session\" here does not refer to a persistent network\nconnection but to a logical session created from HTTP requests and\nresponses.  The presence or absence of a persistent connection should\nhave no effect on the use of cookie-derived sessions).\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n4.3.5\n\nUsers must have control over sessions in order to insure privacy (see\nPrivacy Section Below).  To ease implementation and to prevent an\nadditional layer of complexity where adequate safeguards exist, this\nproposal does distinguish, however, between transactions which are\nverifiable and those which are unverifiable.  A transaction is\nverifiable if the user has the option to review the request-URI prior\nto its use in the transaction.  A transaction is unverifiable if the\nuser does not have that option.  Unverifiable transactions typically\noccur when a user-agent automatically request inlined or embedded\nentities and when a user agent resolves a redirection (3xx) response\nfrom an origin server.  Thus origin transactions, those directly\ninitiated by the user, tend to be verifiable where transactions\nconsequent on those origin transactions tend to be unverifiable.\n\nWhen making an unverifiable transaction, a user agent must only enable\nsessions if a cookie with a domain attribute D was sent or received in\nthe origin transaction and the host name in the Request-URI of the\nunverifiable transaction domain-matches D.\n\nThis restriction prevents a malicious service author from using\nunverifiable transactions to induce a user agent to start or continue\na session with a server in a different domain.  The starting or\ncontinuation of such sessions could be contrary to the privacy\nexpectations of the user, and could also present a security problem.\nUser agents may offer configurable options that allow the user agent,\nor any autonomous programs that the user agent executes to ignore the\nabove rule, so long as these override options default to \"off\".\n\nNB: Many current user agents already provide acceptable review\nmethods, which render most user-selected links verifiable.  Among the\ncurrent review methods are: \n........as before\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n>4.4 How an Origin Server Interprets Cookie \n4.4 How an Origin Server Interprets a Cookie\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n6.3 \n>Furthermore, they should provide the following minimum capabilities: \nGeneral-use user agents should provide the following\nminimum capabilities: \n........as before (to the end of indented requirements).\n\nUser agents created for specific purposes or for limited-capacity\ndevices must provide at least 20 cookies of 4096 bytes, to ensure that\nthe user can interact with a session-based origin server.\n\n\n\n", "id": "lists-010-2493662"}, {"subject": "Re: Proxy authenticatio", "content": "> Proxy authentication, if I read it right, does not work for architectures \n> with more than one proxy between the browser and server, each with their\n> own security needs.\n\nThat is not accurate and was addressed on the list a while back:\n\n   <http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q1/0365.html>\n\n> Section 2.5 of the DAA spec says:\n> \n> \" the proxy versions, Proxy-\n>   Authenticate and Proxy-Authorization, apply only to the current\n>   connection and must not be passed upstream or downstream. \"\n\nThat part of the Digest spec is wrong.  The decision of whether or not\nthat information is passed along is made by the Proxy.  Each proxy\nalong the line may forward or interpret or rewrite the proxy-AA header fields.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-2507087"}, {"subject": "Re: cookie draft availabl", "content": "Ted Hardie:\n>\n[...]\n>\n>Additionally, the designers wish to create a system which fits within\n>the dimensions of the solution space for stateful dialogs identified\n>by Koen Hamilton.  The system is thus meant to be:\n         ^^^^^^^^\n\nThat should be Holtman.\n\nAs I said before, I think this solution space list is best removed\nfrom the draft.\n\n>\n>* Simple to implement.\n>\n>* Simple to use.\n>\n>* Compatible with previous implementations.\n>\n>* Easily deployed after standardization.\n>\n>* Reliable.\n>\n>* Protective of the privacy of its users.\n>\n>* Compatible with cache control.\n>\n>* Low-risk when used with older, non-compatible caches.\n>\n>* As complex as possible within the framework of its other\n>requirements.\n\nKoen.\n\n\n\n", "id": "lists-010-2515531"}, {"subject": "Re: Proxy authenticatio", "content": "Thanks Roy. I'm glad that the DAA spec is wrong. If proxy authentication\ninformation is not forced to be stripped, then I believe we can support\nour architecture. I'm sorry I didn't respond to that mail when it went by.\nWe clearly disagree about the security needs of the second proxy. You state\nthere is no way that it needs to authenticate the UA. The scenario in my\nlast mail message to the WG indicates that it does. But as long as the \nprotocol supports it, we can disagree.\n        Mez\n\nAt 05:40 PM 4/19/96 -0700, Roy T. Fielding wrote:\n>> Proxy authentication, if I read it right, does not work for architectures \n>> with more than one proxy between the browser and server, each with their\n>> own security needs.\n>\n>That is not accurate and was addressed on the list a while back:\n>\n>   <http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q1/0365.html>\n>\n>> Section 2.5 of the DAA spec says:\n>> \n>> \" the proxy versions, Proxy-\n>>   Authenticate and Proxy-Authorization, apply only to the current\n>>   connection and must not be passed upstream or downstream. \"\n>\n>That part of the Digest spec is wrong.  The decision of whether or not\n>that information is passed along is made by the Proxy.  Each proxy\n>along the line may forward or interpret or rewrite the proxy-AA header fields.\n>\n>\n> ...Roy T. Fielding\n>    Department of Information & Computer Science    (fielding@ics.uci.edu)\n>    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n>    http://www.ics.uci.edu/~fielding/\n>\n>\n>\n\n\n\n", "id": "lists-010-2523604"}, {"subject": "Re: Proxy authenticatio", "content": "On Fri, 19 Apr 1996, Roy T. Fielding wrote:\n\n> > Proxy authentication, if I read it right, does not work for architectures \n> > with more than one proxy between the browser and server, each with their\n> > own security needs.\n> \n> That is not accurate and was addressed on the list a while back:\n> \n>    <http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q1/0365.html>\n> \n> > Section 2.5 of the DAA spec says:\n> > \n> > \" the proxy versions, Proxy-\n> >   Authenticate and Proxy-Authorization, apply only to the current\n> >   connection and must not be passed upstream or downstream. \"\n> \n> That part of the Digest spec is wrong.  The decision of whether or not\n> that information is passed along is made by the Proxy.  Each proxy\n> along the line may forward or interpret or rewrite the proxy-AA header fields.\n\nFirst I want to point out that this issue has nothing to do with\nDigest Authentication per se.  It is a question of the behavior of the\n\"Proxy-Authenticate\" and \"Proxy-Authorization\" headers as specified in\nsections 10.30 and 10.31 of the HTTP/1.1 draft.  Obviously that\nspecification applies to Basic Authentication, Digest Authentication,\nand presumably to any yet to be deised authentication using these\nheaders.\n\nThe current working version of <draft-ietf-http-v11-spec-02.html> says\nin section 10.30\n\n   \"Unlike WWW-Authenticate, the Proxy-Authenticate header field applies\n   only to the current connection and must not be passed on to downstream\n   clients.\"\n\nand in section 10.31\n\n   \"Unlike Authorization, the Proxy-Authorization applies only to the\n   current connection and must not be passed on to upstream servers.\"\n\n\nThe Digest Authentication specification is merely reflecting these\nsections.\nIf changes are made to these sections then, of course, the Digest\nAuthentication specification will be changed to be compatible.\nUntil that time, I suggest that Roy's assertion that the digest\nspecification is \"wrong\" should be interpreted to mean that he\nwould favor a change in the HTTP/1.1 draft specification sections\nquoted above.\n\nOn this subject, if a scheme like that described in \n\n  <http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q1/0365.html>\n\nis to be adopted, it needs to be thought out carefully.  For example,\nconsider a chain of agents like\n\n         A --> B --> C --> D\n\nwhere C requires authentication from A, and D requires authentication\nfrom B.  I believe that under the current specification for Basic\nAuthentication, when B receives a Proxy-Authenticate header from C it\nwould have no way to know if this originated from C and is intended to\nbe passed on to A or originated from D and is intended for B.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-2532986"}, {"subject": "Re: Proxy authenticatio", "content": "On Sat, 20 Apr 1996, Mez wrote:\n\n> Thanks Roy. I'm glad that the DAA spec is wrong. If proxy authentication\n> information is not forced to be stripped, then I believe we can support\n> our architecture. I'm sorry I didn't respond to that mail when it went by.\n> We clearly disagree about the security needs of the second proxy. You state\n> there is no way that it needs to authenticate the UA. The scenario in my\n> last mail message to the WG indicates that it does. But as long as the \n> protocol supports it, we can disagree.\n\nI have modified the digest access authentication document so that it\nrefers to the HTTP/1.1 specification rather than repeating what is in\nthat specification.  This is what it should have done in the first\nplace.\n\nThis means that the digest authentication document no longer makes any\nreference to proxies passing on Proxy-Auth* headers, but instead\nrefers to the HTTP/1.1 specification.\n\nA diff showing the changes is attached below.  The old version is first\nand the new version follows it.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n***************\n*** 557,569 ****\n  2.5 Proxy-Authentication and Proxy-Authorization\n  \n    The digest authentication scheme may also be used for authenticating\n!   users to proxies or proxies to end servers.  Unlike the WWW-\n!   Authenticate, and Authorization headers, the proxy versions, Proxy-\n!   Authenticate and Proxy-Authorization, apply only to the current\n!   connection and must not be passed upstream or downstream.  The\n!   transactions for proxy authentication are very similar to those\n!   already described.  Upon receiving a request which requires\n!   authentication, the proxy/server must issue the \"HTTP/1.1 401\n    Unauthorized\" header followed by a \"Proxy-Authenticate\" header of the\n    form\n  \n--- 559,572 ----\n  2.5 Proxy-Authentication and Proxy-Authorization\n  \n    The digest authentication scheme may also be used for authenticating\n!   users to proxies, proxies to proxies, or proxies to end servers by use\n!   of the Proxy-Authenticate and Proxy-Authorization headers. These headers\n!   are instances of the general Proxy-Authenticate and Proxy-Authorization\n!   headers specified in sections 10.30 and 10.31 of the HTTP/1.1\n!   specification [2] and their behavior is subject to restrictions\n!   described there.  The transactions for proxy authentication are very\n!   similar to those already described.  Upon receiving a request which\n!   requires authentication, the proxy/server must issue the \"HTTP/1.1 401\n    Unauthorized\" header followed by a \"Proxy-Authenticate\" header of the\n    form\n  \n\n\n\n", "id": "lists-010-2544050"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "On Fri, 19 Apr 1996, Dave Kristol wrote:\n\n> \"David W. Morris\" <dwm@shell.portal.com> wrote:\n>   > On Thu, 18 Apr 1996, Dave Kristol wrote:\n>   > \n>   > > IF YOU DISAGREE, please address your objections to the http-wg mailing\n>   > > list as quickly as possible.  I review the proposal below.\n>   > \n>   > I disagree ... this is being proposed with far to little time to reflect\n>   > on inplications such as Roy Fielding raises with respect to clean and\n>   > unclean close.\n>   > \n>   > Having just recently spent days figuring out connection reset messages\n>   > from Netscape which turned out to be our server ignoring the \n>   > non-protocol extra CR-LF following POST content. I think this proposal\n>   > has significant potential for increasing the probability that unclean\n>   > close will become the norm.  At best this will require significantly\n>   > more user friendly error handling.\n> \n> I don't see the [sorry] connection between this Netscape bug and the\n> persistent connections issue.\n\nI didn't and don't call it a Netscape bug. It the reference was made in\nan obviously unclear attempt the difficulties of providing user friendly\nand helpful diagnosis when one of the peers of a connection receives\nan unexpected close/reset.\n\n>   > \n>   > It seems to me that the fundamental problem here will be a HTTP/1.1\n>   > client initiating a persistent connection with what turns out to be a\n>   > HTTP/1.0 server w/o knowing the server is able to handle the additional\n>   > data. If I recall correctly some of the issues raised earlier in\n>   > the two phase PUT discussions, an HTTP/1.0 server could close after\n>   > sending a brief response with the possiblity that the close would\n>   > result in a reset which could leave the client quite confused.\n> \n> I think John Franks described this quite elegantly.  We all agree that\n> we want persistent connections; we're just changing the default\n> expectation.  We've neither subtracted nor added problems.  No matter\n> which way you do it, clients and servers must deal with unexpected\n> closes.\n\nTHe difference is whether clients and servers can report unexpected closes\nas an error and provide diagnostic information or if the unexpected closes\nmust be tolerated silently for the correct operation of the protocol. This\nchange to the default behavior will make diagnosis of network failures\neven more difficult then they are. What exactly is the error analysis\nsupport you would expect from a client or server when the protocol is\nso blase about unexpected closes.\n\nQuite simply, my increasing discomfort with this proposed change stems from\nthe change in semantic significance for what is basically a communications\nfailure ... the unexpected close.\n\n>   > \n>   > Isn't this a relatively minor change which could be included with\n>   > HTTP/1.2? At that point there will be more general experience\n>   > deploying the more conservative approach as well as more experience\n>   > writing the code.\n> \n> I tend to be conservative, too, and I really hesitated before even\n> putting forth the idea so late in the game, but the more I think about\n> the \"persistent by default\" idea, the more comfortable I am with it.\n> If we wait until HTTP/1.2, we will have 3 x 3 way combinatorics to\n> worry about, which will increase the odds that different versions won't\n> inter-operate correctly.\n\nI would call increasing the dependancy on unexpected close more like\ninter-tolerate than inter-operate.\n\n>   > So I would recommend against the change at this time but having said\n>   > my piece, I can accept the proposal as currently stated.\n\nI can only hope that the developers responsible for their product's RAS\ncharacteristics (Reliability, Availability, Servicability) have seen\nand thought about this proposal. We haven't provided much time.\n\nDave Morris\n\n\n\n", "id": "lists-010-2554584"}, {"subject": "Re: cookie draft availabl", "content": "On Fri, 19 Apr 1996, Dave Kristol wrote:\n\n> There was a brief private exchange concerning the minimum number of\n> cookies that a client had to support.  Here are Lou Montulli's remarks\n> on the subject.\n> \n>   > 300 was the minimum that our integrated applications developers\n>   > felt comfortable with.  I agree that would be a difficult\n>   > goal to accomplish on a PDA.  \n\nThere seems to be an assumption that the full client implemenation would\nreside in the PDA rather than being split between the PDA and a larger\nPDAserversystem.  The comments I've read about the Oracle InternetBox\nproposal as well as two pre-InternetBox projects I consulted on briefly \nlead me to believe that resource constrained PDAs etc. will not stand\nalone. Hence, I don't think 300 cookies is really a problem.\n\nDave Morris\n\n\n\n", "id": "lists-010-2566827"}, {"subject": "Re: cookie draft availabl", "content": ">There seems to be an assumption that the full client implemenation would\n>reside in the PDA rather than being split between the PDA and a larger\n>PDAserversystem.  The comments I've read about the Oracle InternetBox\n>proposal as well as two pre-InternetBox projects I consulted on briefly \n>lead me to believe that resource constrained PDAs etc. will not stand\n>alone. Hence, I don't think 300 cookies is really a problem.\n\nHang on, if I buy a PDA claiming to give me access to the Web I want\nit to talk standard protocols, not some proprietary protocol that locks\nme in to one vendor.\n\nI think that we should consider the HTTP based PDA in  our designs rather than \nassume that they will use non standard protocols and create a self fullfilling \nprophecy.\n\nI don't see any reason why a person should really need so many cookies and I \nhavent seen an actual justification apart from reference to people in the bowels \nof Netscape who apparently have opinions. \n\nI think that this is arguing for a \"should\" figure for the number of cookies \nrather than a \"must\". Since there is no requirement for the user to turn the \ncookies feature on there can be no logic in requiring that hardware must be \ncapable of supporting a certain number of cookies.\n\nWe shoulkd also be carefull of suggesting enhancements because they only cost \n$20 or so. Some PDAs will cost less than than to build in total.\n\n\nPhill \n\n\n\n", "id": "lists-010-2575283"}, {"subject": "Re: cookie draft availabl", "content": "On Sun, 21 Apr 1996 hallam@w3.org wrote:\n> I don't see any reason why a person should really need so many cookies and I \n> havent seen an actual justification apart from reference to people in the \n> bowels of Netscape who apparently have opinions. \n\nHere's one:\n\nOne potential use of Cookies is to store a \"preferences\" or .rc file for a\nuser who visits a particular site repeatedly.  A server could provide a\nnumber of choices and store that user's choices as cookies, affecting\ntheir view of the site on future visits.\n\nLet's say this practice becomes common.  (I think it will.  I work at a\nWeb development shop, Organic Online, and I know of five large sites\noutside of our clients that are contemplating or implementing such a\nmodel.)  Wiping out a cookie would no longer be a matter of wiping out a\nsession.  Instead, it would cause a user's view of the site to go against\ntheir anticipations. \n\nI argued in the state subgroup that:\n1. the user should be asked for confirmation at the close of\n   the browser execution if a cookie requests to be kept beyond\n   that point (i.e., to some future date or forever); and\n2. if the user allows that cookie to remain in the cookie\n   database, it should _never_ be automatically deleted unless\n   mandated by its expiration date as originally set.\n\nMy argument was that no one would be happy if an OS started wiping out\npreferences files when some internal limit was reached; nor should we\nspecify that cookies be wiped out just because some magic number is\nreached.\n\nThe group decided, instead, to require the user control over the cookie\ndatabase without further specifying how that control is given\n(contradicting my point #1 above).  I can live with this decision, because\nit gives implementors the ability to avoid modal dialog boxes popping up\neverywhere if they so choose.\n\nStill, my argument above can be applied to this discussion, giving us a\ngood reason to specify at least some minimum for cookie storage.\n\n[Rohit says I like to bring up this example because it causes problems for\nthe models typically discussed in reference to cookies, such as shopping\nbaskets.  He's right -- I think the cookie discussion has been driven far\ntoo much by the requirements of shopping baskets, and not enough with a\nconsideration of other models.  Hence this counterexample.]\n\nMarc Hedlund <marc@organic.com> <hedlund@best.com>\n\n\n\n", "id": "lists-010-2584022"}, {"subject": "Re: cookie draft availabl", "content": ">Here's one:\n>\n>One potential use of Cookies is to store a \"preferences\" or .rc file for a\n>user who visits a particular site repeatedly.  A server could provide a\n>number of choices and store that user's choices as cookies, affecting\n>their view of the site on future visits.\n\nIf one is customizing content on that scale then one should have the server\nresources to store information server side. Its not that hard to do if one has a \nthreaded server architecture.\n\nIf servers need to use client side resources then I believe there should be user \ncontrol of the process. \n\nPhill\n\n\n\n", "id": "lists-010-2595399"}, {"subject": "Re: cookie draft availabl", "content": "On Sun, 21 Apr 1996 hallam@w3.org wrote:\n> If one is customizing content on that scale then one should have the server\n> resources to store information server side. \n\nThat's one view, but cookies were proposed in part to avoid server-side\ndatabases of this sort.\n\n> If servers need to use client side resources then I believe there should be \n> user control of the process. \n\nI agree, and I think most of the state subgroup agrees, too.  The\ndisagreement is over how the user should be given control, so that was\nunspecified.\n\nMarc Hedlund <marc@organic.com> <hedlund@best.com>\n\n\n\n", "id": "lists-010-2603237"}, {"subject": "Re: cookie draft availabl", "content": "Disk space is about 1,000 times cheaper than flash ram, and gets cheaper \nthe more you buy it. When you can store 1K of date for a million users \nfor a hundred dollars or so, it's much better to keep the data on the \nserver. \nIf the server needs the data for correct operation, they should look \nafter it, unless the client volunteers to keep it of its own volition.\n\nSimon\n\n\n---\nThey say in  online country             So which side are you on boys\nThere is no middle way                  Which side are you on\nYou'll either be a Usenet man           Which side are you on boys\nOr a thug for the CDA                   Which side are you on?\n  National Union of Computer Operatives; Hackers, local 37   APL-CPIO\n\n\n\n", "id": "lists-010-2612017"}, {"subject": "Re: cookie draft availabl", "content": "On Sun, 21 Apr 1996, Simon Spero wrote:\n> Disk space is about 1,000 times cheaper than flash ram, and gets cheaper \n> the more you buy it. When you can store 1K of date for a million users \n> for a hundred dollars or so, it's much better to keep the data on the \n> server. \n\nOkay, so we should throw out the cookie draft?  I'm not saying your point\nof view is ridiculous, I'm just saying that there are cases in which\nclientside data is desirable.\n\n> If the server needs the data for correct operation, they should look \n> after it, unless the client volunteers to keep it of its own volition.\n\nI didn't say the server _needed_ the data for correct operation.  What I\nsaid was the user will expect that preferences they set will be saved. \nAllowing a low threshhold for discarding cookies goes against least\nsurprise.  (Again, I'd rather have _no_ automatic discards for cookies the\nuser has decided to keep.) \n\nMarc Hedlund <marc@organic.com> <hedlund@best.com>\n\n\n\n", "id": "lists-010-2620391"}, {"subject": "Re: cookie draft availabl", "content": "> On Sun, 21 Apr 1996 hallam@w3.org wrote:\n> > I don't see any reason why a person should really need so many cookies and I\n> > havent seen an actual justification apart from reference to people in the\n> > bowels of Netscape who apparently have opinions.\n> Here's one:\n> One potential use of Cookies is to store a \"preferences\" or .rc file for a\n> user who visits a particular site repeatedly.\n\nWhile a very good argument for why a user would want lots of cookies,\nand an example that should be kept in mind when thinking about the\ndesign of such things, I think it fails to do what Marc says it does:\n\n> Still, my argument above can be applied to this discussion, giving us a\n> good reason to specify at least some minimum for cookie storage.\n\nI think that dictating any minimum requirement as a \"must\" is\ncounterproductive, as it has an adverse effect on implementation and\nthe minimum machine requirements.\n\nFor example, my favorite desktop machine box everything (and I do mean\nEVERYTHING) with dynamically allocated objects, so that the limits on\nthe number of something is constrained by available memory, not some\ninternal limits. Any client built for that box would almost certainly\ndo the same for cookies. Am I going to try and get reserve space for\n300 cookies, and not start if I can't get it? Maybe I'm going to try\nand do this when the first request for a cookie comes in, and pretend\nnot to implement cookies if I can't get it? Nah, I'm going to ignore\nthe spec, and dynamically allocate space for cookies as I need them.\nIf a request comes in that I can't satisfy for lack of space, I'm\ngoing to ask the user to correct the problem so I can go on, or to\nignore the request and not set up a cookie. I'm going to do this\nwhether it happens with only 1 cookie already stored, or 3000 already\nstored - it's the only choice that makes sense.\n\nrom the PDA perspective, consider a browser that runs fine with a\ncouple of dozen cookies on the 1Meg version of the PDA. If there is a\nrequired minimum that pushes the memory usage up enough to require the\n2meg version of the machine, am I going to cut out the segment of the\nmarket that didn't buy the more expensive machine just to meet the\nletter of the spec? Nah, I'm going to ignore the spec and sell it for\nthe 1 Meg machine - it's the only choice that makes sense.\n\nThe \"must\" minimum cookie count is 1. I don't see any reason to\nspecify a specific minimum, though it might be worth noting that\nwanting to store hundreds, if not thousands, of cookies should not be\nunexpected.\n\n<mike\n\n\n\n", "id": "lists-010-2630086"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "On Sun, 21 Apr 1996, David W. Morris wrote:\n> \n> \n> Quite simply, my increasing discomfort with this proposed change stems from\n> the change in semantic significance for what is basically a communications\n> failure ... the unexpected close.\n> \n...\n> \n> I can only hope that the developers responsible for their product's RAS\n> characteristics (Reliability, Availability, Servicability) have seen\n> and thought about this proposal. We haven't provided much time.\n> \n> Dave Morris\n> \n\nDave, there has been no proposal to change the semantic significance\nof an unexpected close.  There has been no proposed change which impinges\nin the slightest on RAS characteristics.\n\nPlease take a moment to seriously entertain the possibility that the\npeople saying you have misunderstood the proposal might be correct.\n\nHere is another explanation of the proposed change: \n\nThe HTTP/1.1 spec *could* (but doesn't) require that every request and\nevery response contain one of two mutually exclusive headers -- either\n\"Connection: close\" or \"Connection: persistent\".  Which of the two to\nsend is, of course, entirely up to the sender.  Either side can choose\neither as its default and either side can change what it is sending at\nany time.\n\nWhile this isn't what the draft spec does, it is very close. Since the\ntwo connection headers are mutually exclusive, a \"shorthand\" can be\nused to save a few bytes.  Instead of sending \"Connection: close\" the\nabsence of any Connection header implies that Connection: close should\nbe understood.  This is what is currently specified.  Other than\nsaving a few bytes nothing else is changed.  It is just as if that\nConnection: close header were sent.  This shorthand has no effect on\nerror conditions, unexpected closes, TCP stacks, or the phase of the\nmoon.\n\nNow, what has been proposed is a change in this shorthand.  Instead of\nusing the absence of a Connection header to mean \"Connection: close\"\nit would be used to indicate \"Connection: persistent\" and the\n\"Connection: close\" header would be explicitly sent.  The only thing\nwhich has changed is encoding of information in headers.  There has\nbeen no change in the default, or expected or error handling behavior\nof either clients or servers.  The new proposal and the previous\nproposal are logically equivalent.  Their headers communicate exactly\nthe same information (albeit in different encodings) and result in\nexactly the same behavior in response to that information.\n\nLet me say it again.  The proposed change affects the encoding of\nheaders, nothing more.  It's just as if we decided to change a header\nto require that it be in French rather than English but didn't change\nanything else.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-2639611"}, {"subject": "Re: cookie draft availabl", "content": "M. Hedlund writes:\n > \n > On Sun, 21 Apr 1996, Simon Spero wrote:\n > > Disk space is about 1,000 times cheaper than flash ram, and gets cheaper \n > > the more you buy it. When you can store 1K of date for a million users \n > > for a hundred dollars or so, it's much better to keep the data on the \n > > server. \n > \n > Okay, so we should throw out the cookie draft?  I'm not saying your point\n > of view is ridiculous, I'm just saying that there are cases in which\n > clientside data is desirable.\n > \nThe fact is that different services will do it different ways,\nincluding the \"middle way\" of using cookies to have client side tokens\nwhich are keys to a server side database.\n\nThe cost of server side databases, which makes them a bit harder to\nscale especially on distributed sites, is the cost of accessing and\nmaintaining the database.  The benefits of server side databases include\nease of control, ability to analyze the data, ability to change the\ndata format without having to maintain backward compatibility, etc.\nThe benefits of client side databases include not having to garbage\ncollect the server side database, efficiency of server side\nprocessing (i.e. no file or database I/O required to access the\ndatabase).  So there's no need to resort to a \"less filling, tastes\ngreat\" argument about this.\n\n--Shel\n\n\n\n", "id": "lists-010-2650532"}, {"subject": "Re: cookie draft availabl", "content": "At 02:51 PM 4/21/96 -0700, M. Hedlund wrote:\n>My argument was that no one would be happy if an OS started wiping out\n>preferences files when some internal limit was reached; nor should we\n>specify that cookies be wiped out just because some magic number is\n>reached.\n\nBut this is totally an implmentation issue.  If someone sells a PDA that\nonly stores 3 cookies, they're going to get burned in the market.  There's\nno need for protocol designers to tell PDA vendors how to build their systems.\n\nBut, assuming we're going to mention it anyway, I think a SHOULD is more\nappropriate that a MUST.\n\n-----\nthe Programmer formerly known as Dan          \n                                     http://www.spyglass.com/~ddubois/\n\n\n\n", "id": "lists-010-2659325"}, {"subject": "Re: Proxy authenticatio", "content": "Thanks John,\n\n> > > Section 2.5 of the DAA spec says:\n> > > \n> > > \" the proxy versions, Proxy-\n> > >   Authenticate and Proxy-Authorization, apply only to the current\n> > >   connection and must not be passed upstream or downstream. \"\n> > \n> > That part of the Digest spec is wrong.  The decision of whether or not\n> > that information is passed along is made by the Proxy.  Each proxy\n> > along the line may forward or interpret or rewrite the proxy-AA header fields.\n> \n[...]\n\n> The current working version of <draft-ietf-http-v11-spec-02.html> says\n> in section 10.30\n> \n>    \"Unlike WWW-Authenticate, the Proxy-Authenticate header field applies\n>    only to the current connection and must not be passed on to downstream\n>    clients.\"\n> \n> and in section 10.31\n> \n>    \"Unlike Authorization, the Proxy-Authorization applies only to the\n>    current connection and must not be passed on to upstream servers.\"\n>\n> The Digest Authentication specification is merely reflecting these\n> sections.\n> If changes are made to these sections then, of course, the Digest\n> Authentication specification will be changed to be compatible.\n> Until that time, I suggest that Roy's assertion that the digest\n> specification is \"wrong\" should be interpreted to mean that he\n> would favor a change in the HTTP/1.1 draft specification sections\n> quoted above.\n\nI favor that too. I've outlined our actual need for that change. And I know\nthat our need is not particularly aberrant, since many distributed security\nsystems have this support in it (see \"Cascaded Authentication\" by Sollins,\nthe paper on DSSA delegation by Gasser and McDermott, and DCE delegatin \nsupport, which I know customers requested specficially).\n\n> \n> On this subject, if a scheme like that described in \n> \n>   <http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q1/0365.html>\n> \n> is to be adopted, it needs to be thought out carefully.  For example,\n> consider a chain of agents like\n> \n>          A --> B --> C --> D\n> \n> where C requires authentication from A, and D requires authentication\n> from B.  I believe that under the current specification for Basic\n> Authentication, when B receives a Proxy-Authenticate header from C it\n> would have no way to know if this originated from C and is intended to\n> be passed on to A or originated from D and is intended for B.\n\nThis is a good point. I don't know of any need for a fully general solution\n(although a fully general solution will satisfy more unseen future needs).\nIn the short term, the only need we have is the ability of A to authenticate\nto B, C, and D. \n\nOne possibility for a fully general solution would be to allow authentication/\nauthorization failures to propogate back through the chain. So, if C issues\na proxy-authenticate, B would consider whether it was equipped to handle an\nauthentication request from that realm. If it could, it would attempt. If not,\nor if it's attempt failed, it could pass it back to A. Alternatively,\nB could send the request back the chain, and send on authentication information\nfrom both A and itself (if they both had it), and let C implement it's\ndesired policy.\n\nI can't say I'm fully comfortable with either of these, but I don't know what\nother concrete security policies people have out there, other than ours. \nThe best solution would provide authentication information from the whole\nchain, in a manner that preserved the order information (most particularly,\ndifferentiated A from the rest). Rohit, can this be done with PEP?\nMez\n\n\n\n", "id": "lists-010-2666808"}, {"subject": "Re: PERSIST: propose to make defaul", "content": "    The difference is whether clients and servers can report unexpected\n    closes as an error and provide diagnostic information or if the\n    unexpected closes must be tolerated silently for the correct\n    operation of the protocol. This change to the default behavior will\n    make diagnosis of network failures even more difficult then they\n    are. What exactly is the error analysis support you would expect\n    from a client or server when the protocol is so blase about\n    unexpected closes.\n\n    Quite simply, my increasing discomfort with this proposed change\n    stems from the change in semantic significance for what is\n    basically a communications failure ... the unexpected close.\n\nI think other people have answered your questions about the syntactical\nissues.  You raise a good point, though, that we are basically hiding\nadditional complexity from the end-user (by expecting the client\nsoftware to retry after certain \"failures\") and so we could end up\nburying diagnostic information, as well.\n\nIt might be instructive to draw an analogy to a demand-paged VM\nsystem.  In both cases, \"the system\" hides certain faults (page\nfaults or TCP closes) from the \"user\" (a program in the VM example,\na person in the HTTP example) and silently patches things up.  This\nprovides the user with a relatively simple abstraction, at the cost\nof some more implementation work and the possiblity of \"performance\nfailures\" if the working set exceeds the available resources.\n(Note that in both the VM example and the persistent connections\nexample, the critical issue is whether we get enough locality of\nreference to offset the cost of handling the \"faults\".)\n\nMost VM systems that I know about (except perhaps those on PCs)\nkeep statistics related to page-fault rates and causes, and this\nallows a system administrator to manage and configure the system\nto optimize performance.  It would be a good idea for an HTTP\nserver to keep track of its actions in closing persistent connections,\nalong with whatever tuning knobs exist that could affect this.\n\nFor example, what fraction of connections are closed because the\nsystem had no free connection resources?  What is the mean (and\nvariance) of the number of requests/connection?  Etc.\n\nVM problems are also dealt with by changing program behavior\n(or, more realistically, by trying to educate programmers about\nhow not to misuse an LRU page cache.)  We certainly ought to\npay some attention to this aspect of HTTP persistent connections,\nbut the teaching probably has to be directed at the people who\ndesign Web services, not at the 30 million (+/- 40 million) actual\nweb users.\n\n-Jeff\n\n\n\n", "id": "lists-010-2678635"}, {"subject": "Submission of HTTP/1.1 Draft 0", "content": "Bear with me; this is the first time I've done this.\n\nPlease find draft-ietf-http-v11-spec-02.txt at\nURL: http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/ID02/ID02.txt\nand the postscript in:\nURL: http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/ID02/ID02Clean.ps\n\nThank you very much, on behalf of the HTTP Working group.\n\nLet me know if there are any problems\n\n- Jim Gettys\n\n\n\n", "id": "lists-010-2688442"}, {"subject": "Re: cookie draft availabl", "content": "Marc:\n> (Again, I'd rather have _no_ automatic discards for cookies the\n> user has decided to keep.) \n> \n> \nThis is the essence of the whole discussion.  Cookies can be treated\nas an extensible file system from the user's point of view, subject\nto memory limits just as file systems are.  Developers can even use\nthe file system tools to manage the cookie folder.\n\nbob\n\n\n\n", "id": "lists-010-2695821"}, {"subject": "Re: Submission of HTTP/1.1 Draft 0", "content": "Hi Jim,\n\nPlease e-mail the Internet-Draft submission directly to me\nat cclark@cnri.reston.va.us\n\nPlease be sure to send one version at a time.\n\nThanks,\n\nCynthia Clark\nIETF Internet-Drafts Administrator\n\n\n -----  Forwarded Message  ------\n\n\nTo: internet-drafts@CNRI.Reston.VA.US\nCc: http-wg@cuckoo.hpl.hp.com\nSubject: Submission of HTTP/1.1 Draft 02\nDate: Tue, 23 Apr 96 00:05:27 -0400\nFrom: jg@w3.org\nX-Mts: smtp\ncontent-length: 379\n\nBear with me; this is the first time I've done this.\n\nPlease find draft-ietf-http-v11-spec-02.txt at\nURL: http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/ID02/ID02.txt\nand the postscript in:\nURL: http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/ID02/ID02Clean.ps\n\nThank you very much, on behalf of the HTTP Working group.\n\nLet me know if there are any problems\n\n- Jim Gettys\n\n\n\n", "id": "lists-010-2703262"}, {"subject": "Re: Proxy authenticatio", "content": "I was considering whether there would be any problems implementing what we\nneeded with proxy authentication in a future version, if the current \nrestrictions are passed. We could authentication the user (agent) to any\nproxy in line before a restricted proxy, but not after, because any\nrestricted proxies would throw out proxy-authenticate and proxy-authorization\nheaders it didn't understand. While that would work fine for services on\nthe user side of, say, a firewall, it would not work for services on the\nservice side of a firewall, which we also support. So passing the current\nrestrictions would pose problems that we could not overcome in configurations\ninvolving proxies that support them.\n\nJust who should I be engaging with over this? Roy agrees that the restrictions\nare wrong :-), and John is willing to follow the lead of HTTP 1.1 (though he\npoints out some good concerns), and Jim claims he'll make any changes that\nfolks agree to. \n\nMez\n\n\n\n", "id": "lists-010-2712278"}, {"subject": "YA cookie draft, v2.2", "content": "The latest draft of the cookie spec. is at\nhttp://www.research.att.com/~dmk/cookie.html\n\nThe new version incorporates suggestions and corrections by Jeff Mogul,\nTed Hardie, and Koen Holtman, plus some slight rewording concerning\n\"minimum capabilities\" prompted by several people.\n\nYou can get versions with change bars from either the I-D (v2.10)\nor the previously announced draft (v2.16).\n\nI'll extend the comment period to 2100 GMT, Thursday, April 25.  (Yes,\nwe're all busy with the HTTP draft.)  I believe we're close to\nconvergence anyway.\n\nDave Kristol\n\n\n\n", "id": "lists-010-2720427"}, {"subject": "Authorization heade", "content": "Roy Fielding's Nov. 22, 1995 HTTP/1.1 draft allows (sect. 11) more than\none set of (different auth-scheme) credentials in the Authorization\nheader.  Jim Gettys's ID02 draft does not.\n\nIs this a design choice or a bug?\n\nDave Kristol\n\n\n\n", "id": "lists-010-2727979"}, {"subject": "Re: Proxy authenticatio", "content": "> I was considering whether there would be any problems implementing what we\n> needed with proxy authentication in a future version, if the current \n> restrictions are passed. We could authentication the user (agent) to any\n> proxy in line before a restricted proxy, but not after, because any\n> restricted proxies would throw out proxy-authenticate and proxy-authorization\n> headers it didn't understand. While that would work fine for services on\n> the user side of, say, a firewall, it would not work for services on the\n> service side of a firewall, which we also support. So passing the current\n> restrictions would pose problems that we could not overcome in configurations\n> involving proxies that support them.\n\nHmmm, at some point when we have a high-bandwidth connection\n(i.e., not e-mail) I'd like to hear an explanation of how you\ncan authenticate user agents through non-trusted proxies.\nAs far as HTTP/1.1 goes, we did not intend to support such a\nfeature because it would be better done within an extensible security\narchitecture (i.e., that thing Rohit has been working on).\n\nProxy-Authenticate and Proxy-Authorization are (or should be) defined \naccording to the Netscape Proxy implementation.  The current spec is\nlacking the wording that I sent in a couple months ago about how the\nproxy should forward the field if the credentials do not apply to it.\nOne problem is that the realm is not sent with the credentials (only\nwith the challenge), and thus things can still get messed-up if more\nthan one proxy is demanding credentials on a single request.\n\n> Just who should I be engaging with over this? Roy agrees that the restrictions\n> are wrong :-), and John is willing to follow the lead of HTTP 1.1 (though he\n> points out some good concerns), and Jim claims he'll make any changes that\n> folks agree to. \n\nI'll update the 1.1 spec, but anything more than Proxy-Authenticate and\nProxy-Authorization will have to wait until a later protocol version.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-2734346"}, {"subject": "Re: YA cookie draft, v2.2", "content": "Mea culpa.  If you were among the industrious few to (try to) grab the\nnew cookie draft between the time of my announcement and about 1:45 EDT\n(1745 GMT), you didn't get it.  I screwed up.  Please try again:\nhttp://www.research.att.com/~dmk/cookie.html\n\nWith apologies,\nDave Kristol\n\n\n\n", "id": "lists-010-2743954"}, {"subject": "Re: Proxy authenticatio", "content": "On Tue, 23 Apr 1996, Roy T. Fielding wrote:\n\n> \n> Proxy-Authenticate and Proxy-Authorization are (or should be) defined \n> according to the Netscape Proxy implementation.  The current spec is\n> lacking the wording that I sent in a couple months ago about how the\n> proxy should forward the field if the credentials do not apply to it.\n> One problem is that the realm is not sent with the credentials (only\n> with the challenge), and thus things can still get messed-up if more\n> than one proxy is demanding credentials on a single request.\n> \n\nIn the current 02 version of the spec it is not clear (to me) that\nthe realm is required to be unique across hosts/proxies.  Here is what\nthe spec says:\n\n\n            realm          = \"realm\" \"=\" realm-value\n            realm-value    = quoted-string\n\n\n\n      The realm attribute (case-insensitive) is required for all\n      authentication schemes which issue a challenge. The realm value (case-\n      sensitive), in combination with the canonical root URL of the server\n      being accessed, defines the protection space. These realms allow the\n      protected resources on a server to be partitioned into a set of\n      protection spaces, each with its own authentication scheme and/or\n      authorization database. The realm value is a string, generally assigned\n      by the origin server, which may have additional semantics specific to\n      the authentication scheme.\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-2751030"}, {"subject": "Re: YA cookie draft, v2.2", "content": "Dave Kristol:\n>\n>The latest draft of the cookie spec. is at\n>http://www.research.att.com/~dmk/cookie.html\n\nOnly two comments:\n\n\n#4.3.5  Sending Cookies in Unverifiable Transactions  Users must have      |\n#control over sessions in order to insure privacy.\n                                   ^^^^^^\n\nShouldn't this be `assure'?\n\n\n#8.2  Cookie Spoofing\n#\n[...]\n#Note that a server at cracker.edu could send a cookie to the client and   |\n#subsequently get both of the cookies in the preceding example as well as  |\n#its own.\n\nI was confused by this, and after re-reading it twice, I think this is\nwrong.  I believe this should be:\n\n Note that a server called cracker.edu could send a cookie to the\n client without an explicit domain, and subsequently get the second\n cookie in the preceding example as well as its own.\n\n\nKoen.\n\n\n\n", "id": "lists-010-2760599"}, {"subject": "Re: YA cookie draft, v2.2", "content": "koen@win.tue.nl (Koen Holtman) wrote:\n  > >\n  > >The latest draft of the cookie spec. is at\n  > >http://www.research.att.com/~dmk/cookie.html\n  > \n  > Only two comments:\n  > \n  > \n  > #4.3.5  Sending Cookies in Unverifiable Transactions  Users must have      |\n  > #control over sessions in order to insure privacy.\n  >                                    ^^^^^^\n  > \n  > Shouldn't this be `assure'?\nWell, maybe \"ensure\".\n  > \n  > \n  > #8.2  Cookie Spoofing\n  > #\n  > [...]\n  > #Note that a server at cracker.edu could send a cookie to the client and   |\n  > #subsequently get both of the cookies in the preceding example as well as  |\n  > #its own.\n  > \n  > I was confused by this, and after re-reading it twice, I think this is\n  > wrong.  I believe this should be:\n  > \n  >  Note that a server called cracker.edu could send a cookie to the\n  >  client without an explicit domain, and subsequently get the second\n  >  cookie in the preceding example as well as its own.\n\nNo.  Actually, the whole passage must be dropped.  I put it in when Ted\nHardie observed the problem as stated.  But we've fixed the problem by\nrequiring explicit leading dots in Domain=.\n\nIf a server at cracker.edu sent a cookie to the client, it would only\nget back its own cookie.  It could only set Domain=cracker.edu, which\nis also the default Domain.  (Domain=.cracker.edu would not\ndomain-match the host name (cracker.edu), and the cookie would be\ndiscarded.) Since cookies with domains victim.cracker.edu and\n.cracker.edu do not domain-match \"cracker.edu\", neither cookie in the\nexample would get send to the bad guy.\n\nDave\n\n\n\n", "id": "lists-010-2768586"}, {"subject": "Re: YA cookie draft, v2.2", "content": "Dave Kristol:\n>\n>koen@win.tue.nl (Koen Holtman) wrote:\n>  > #control over sessions in order to insure privacy.\n>  > \n>  > Shouldn't this be `assure'?\n>Well, maybe \"ensure\".\n\n:) Actually, `ensure' was the word I was thinking of first, but I\ncould not find it in any on-line dictionary.\n\n>  > #8.2  Cookie Spoofing\n>  > #\n>  > [...]\n>  > #Note that a server at cracker.edu could send a cookie to the client and\n>  > #subsequently get both of the cookies in the preceding example as well as\n>  > #its own.\n>  > \n>  > I was confused by this, and after re-reading it twice, I think this is\n>  > wrong.  I believe this should be:\n>  > \n>  >  Note that a server called cracker.edu could send a cookie to the\n>  >  client without an explicit domain, and subsequently get the second\n>  >  cookie in the preceding example as well as its own.\n>\n>No.  Actually, the whole passage must be dropped.\n\nYou are right.  My correction above is also incorrect.\n\n>Dave\n\nKoen.\n\n\n\n", "id": "lists-010-2777846"}, {"subject": "NULLRequest (Sect. 4.1", "content": "I believe NULL-Request was added to handle the lingering CRLF following\na POSTed entity body.  It may have an unexpected and unintended\ninteraction with persistent connections:  because the NULL-Request has\nno HTTP-Version and no Connection header, the server is obliged to\nclose the connection after servicing the NULL-Request.\n\nDave Kristol\n\n\n\n", "id": "lists-010-2786236"}, {"subject": "Re: Proxy authenticatio", "content": "> In the current 02 version of the spec it is not clear (to me) that\n> the realm is required to be unique across hosts/proxies.\n\nIt isn't, but I was thinking in terms of having a user agent include\nmore than one Proxy-Authorization header field in a request such\nthat it can be passed on to two different proxies down the line.\nWe can't do that right now because there is no way to differentiate\nbetween multiple credentials (even if they correspond to different\nrealms in the prior challenge response).  In other words, we need PEP,\nand that won't be available until after 1.1 is finished, so can\nwe table this discussion for now and get back to it in June?\n\nEither that, or just ask Rohit to solve it (he's good about that).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-2794722"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "> I believe NULL-Request was added to handle the lingering CRLF following\n> a POSTed entity body.  It may have an unexpected and unintended\n> interaction with persistent connections:  because the NULL-Request has\n> no HTTP-Version and no Connection header, the server is obliged to\n> close the connection after servicing the NULL-Request.\n\nWell, Dave, I must say that you have a perverse way of reading specs\nthat discovers problems I would never even dream of.  ;-)\nI've noticed that some implementers manage to derive similar\ninterpretations, so I'm glad you find them first.\n\nAny thoughts as to what/where words need to be added to prevent this\nunfortunate event?\n\n.......Roy\n\n\n\n", "id": "lists-010-2803469"}, {"subject": "Re: Proxy authenticatio", "content": "On Tue, 23 Apr 1996, Roy T. Fielding wrote:\n\n> > In the current 02 version of the spec it is not clear (to me) that\n> > the realm is required to be unique across hosts/proxies.\n> \n> It isn't, but I was thinking in terms of having a user agent include\n> more than one Proxy-Authorization header field in a request such\n> that it can be passed on to two different proxies down the line.\n> We can't do that right now because there is no way to differentiate\n> between multiple credentials (even if they correspond to different\n> realms in the prior challenge response).  In other words, we need PEP,\n> and that won't be available until after 1.1 is finished, so can\n> we table this discussion for now and get back to it in June?\n> \n\nI think that is a good idea.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-2811152"}, {"subject": "Twophase send", "content": "I see that the Monday internet draft still includes two-phase POSTS\nand PUTS.  I strongly object to this \n a) on procedural grounds\n b) on technical grounds.\n\n\nAd a:\n\na.1) The issues list says:\n\nTwo Phase methods: \n         JM Section 8.4 POST\n         Two-phase POST removed\n         ^^^^^^^^^^^^^^^^^^^^^^\n         Mogul has writeup of result of discussion?\n         Status: need writeup, WG review \n\nand I clearly remember that we indeed conclude that two-phase methods\nshould not be in 1.1 at the end of the two-phase wars some months ago.\n\na.2) Also, the 02 draft says:\n\n      POST requests must obey the entity transmission requirements set\n      out in section 8.4.1 [which talks about two-phase].\n\nWhile the 01/00 drafts said:\n\n   HTTP/1.1 allows for a two-phase process to occur in accepting and \n   processing a POST request. If the media type of the posted entity \n   is not \"application/x-www-form-urlencoded\" [5], an HTTP/1.1 client \n   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  \n   must pause between sending [....]\n\nThis considerable change was not discussed.\n\n\nAd b:\n\nb.1) Two-phase saves bandwidth sometimes, at the cost of speed\n(round-trips) for each POST request, no matter how small.  I have seen\nno statistics that this tradeoff improves current conditions, while I\nsuspect that it does not in many cases. Two-phase thus adds complexity\nwithout having established the need for this.  If we have it, it\nshould at least be optional for small POST requests.\n\nb.2) The new requirement that two-phase is also used for normal POSTS\nof small forms means degradation of performance for many existing\nforms applications when upgraded to 1.1.  It may also decrease my\nchance of making a successful POST transaction (with a busy search\nengine) if the backbone is dropping a significant number of packets.\n\nb.3) Finally, the MUST/SHOULD text about two-phase does not take\nproxies, especially 1.0 proxies, into account.\n\n\nIf I am to agree with two-phase staying in, I would require all points\nabove to be convincingly addressed.\n\nKoen.\n\n\n\n", "id": "lists-010-2819831"}, {"subject": "Re: Proxy authenticatio", "content": "> realms in the prior challenge response).  In other words, we need PEP,\n> and that won't be available until after 1.1 is finished, so can\n> we table this discussion for now and get back to it in June?\n> \n> Either that, or just ask Rohit to solve it (he's good about that).\n\nOnly if you're right, Roy, about proxies being able to pass proxy \nauthentication information on at their own discretion (I still haven't seen\na draft of anything with that wording). Or if you're sure that PEP information\nwouldn't be stopped by 1.1 proxies (the headers would be different, so they'd\nbe ignored and passed on, yes?).\n\nOf course, I vastly prefer Rohit simply solving the problem right now :-).\nMez\n\n\n\n", "id": "lists-010-2829022"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "\"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU> wrote:\n  > > [DMK]\n  > > I believe NULL-Request was added to handle the lingering CRLF following\n  > > a POSTed entity body.  It may have an unexpected and unintended\n  > > interaction with persistent connections:  because the NULL-Request has\n  > > no HTTP-Version and no Connection header, the server is obliged to\n  > > close the connection after servicing the NULL-Request.\n  > \n  > Well, Dave, I must say that you have a perverse way of reading specs\n  > that discovers problems I would never even dream of.  ;-)\n\nUm, thanks, I guess.  It comes from intense language lawyering for the\nANSI C spec.\n\n  > I've noticed that some implementers manage to derive similar\n  > interpretations, so I'm glad you find them first.\n\nWell, I found it because indeed I was trying to figure out exactly how\nto implement null requests and noticed the presumed unintended\nside-effect.\n  > \n  > Any thoughts as to what/where words need to be added to prevent this\n  > unfortunate event?\n\nNot really.  The NULL-Request idea seems attractive at first glance,\nbut it makes me nervous that a server should silently eat and discard\nan arbitrary number of blank lines (assuming the persistent connection\nissue gets side-stepped).  That opens the gate for a malicious client\nto mount a denial of service attack by pumping a stream of CRLF's at a\nserver.  I'm more inclined to deal with it as a special-case hack:\n\n- if there is a POST, and\n- if the connection is held open,\n- (and if the request is HTTP/1.0?),\nthe server should look for, and silently discard, a CRLF that follows\nthe entity body.\nDave\n\n\n\n", "id": "lists-010-2837434"}, {"subject": "I-D ACTION:draft-ietf-http-v11-spec02.txt, .p", "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : Hypertext Transfer Protocol -- HTTP/1.1                 \n       Author(s) : R. Fielding, H. Frystyk, T. Berners-Lee, \n                   J. Gettys, J. Mogul\n       Filename  : draft-ietf-http-v11-spec-02.txt, .ps\n       Pages     : 164\n       Date      : 04/23/1996\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol for\ndistributed, collaborative, hypermedia information systems. It is a \ngeneric, stateless, object-oriented protocol which can be used for many \ntasks, such as name servers and distributed object management systems, \nthrough extension of its request methods (commands). A feature of HTTP is \nthe typing and negotiation of data representation, allowing systems to be \nbuilt independently of the data being transferred.      \n                   \nHTTP has been in use by the World-Wide Web global information initiative \nsince 1990. This specification defines the protocol referred to as \n_HTTP/1.1_.                                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-v11-spec-02.txt\".\n Or \n     \"get draft-ietf-http-v11-spec-02.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-02.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-02.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-02.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-2846765"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "On Wed, 24 Apr 1996, Dave Kristol wrote:\n\n> The NULL-Request idea seems attractive at first glance,\n> but it makes me nervous that a server should silently eat and discard\n> an arbitrary number of blank lines (assuming the persistent connection\n> issue gets side-stepped).  That opens the gate for a malicious client\n> to mount a denial of service attack by pumping a stream of CRLF's at a\n> server.  I'm more inclined to deal with it as a special-case hack:\n> \n> - if there is a POST, and\n> - if the connection is held open,\n> - (and if the request is HTTP/1.0?),\n> the server should look for, and silently discard, a CRLF that follows\n> the entity body.\n>\n\nI went back to look at my implementation of this to see how I had\nhandled it for Connection keep-alive.\n\nWhat I did was \n - if the connection is a continuation (i.e. has been held open), and\n - if the request line is just \"CRLF\"\ndiscard the request and keep open the connection\n\n - if the connection is a new one, and\n - if the request line is \"CRLF\"\ndiscard the request and close connection\n\nThis is essentially the NULL-request idea with the provision that \nprevious keep-alive + NULL-request => continue keep-alive\n\nI wasn't worried for my implementation about the attack you mention\nbecause I have a max number of requests per connection after which the\nconnection is closed.  But even without this I don't see the attack\nyou mention as any worse than a stream of requests for persistent\nconnection and a non-existent file.  Wouldn't that have essentially\nthe same effect?\n\nI think the simplest behavior when receiving a request line consisting\nof only CRLF is\n\nif (previous request was persistent)\ndiscard request and continue persistent connection\nelse\ndiscard and close connection\n\nI have no real objection to also requiring that the previous\nconnection be a POST, but then we have to say what happens if it\nwasn't a POST.  Similarly if it is not HTTP/1.0 or not following a\npersistent connection we have to deal with an error.\n\nDoes anyone have a sense whether or not it is realistic to get people\nto stop the POST + CRLF hack in HTTP/1.1?  As I understand it, the\npoint was to work around broken servers which would not read POST data\nif it did not end with a LF.  Is this still a problem?\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-2857483"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": ">\n>Date:  Wed, 24 Apr 96 09:59:07 EDT\n>From:  dmk@allegra.att.com (Dave Kristol)\n>To:  fielding@avron.ics.uci.edu\n>Subject:  Re: NULL-Request (Sect. 4.1)\n>Cc:  http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>\n>\n>\"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU> wrote:\n>  > > [DMK]\n>  > > I believe NULL-Request was added to handle the lingering CRLF following\n>  > > a POSTed entity body.  It may have an unexpected and unintended\n>  > > interaction with persistent connections:  because the NULL-Request has\n>  > > no HTTP-Version and no Connection header, the server is obliged to\n>  > > close the connection after servicing the NULL-Request.\n>  > \n>  > Well, Dave, I must say that you have a perverse way of reading specs\n>  > that discovers problems I would never even dream of.  ;-)\n>\n>Um, thanks, I guess.  It comes from intense language lawyering for the\n>ANSI C spec.\n\nI applaud such perverted minds; I disagree with some of the conclusions\nbelow, however.\n\n>\n>  > I've noticed that some implementers manage to derive similar\n>  > interpretations, so I'm glad you find them first.\n>\n>Well, I found it because indeed I was trying to figure out exactly how\n>to implement null requests and noticed the presumed unintended\n>side-effect.\n>  > \n>  > Any thoughts as to what/where words need to be added to prevent this\n>  > unfortunate event?\n>\n>Not really.  The NULL-Request idea seems attractive at first glance,\n>but it makes me nervous that a server should silently eat and discard\n>an arbitrary number of blank lines (assuming the persistent connection\n>issue gets side-stepped).  That opens the gate for a malicious client\n>to mount a denial of service attack by pumping a stream of CRLF's at a\n>server.  I'm more inclined to deal with it as a special-case hack:\n>\nThis is a red herring.  There are N denial of service attacks possible\non an HTTP server where N is probably measured in thousands.  This one\nis no more serious than any of the others.  A common response will be\nfor servers to close connections if they have served many requests\non the same connection.\n\nMore realistically, a single server is likely to round robin between\nconnections; the NULL-request will just be the request from that\nclient (and be particularly fast to determine there is no work to do\n(if I were doing a deliberate attack, I'd pick requests that take\nalot of work by the server to do, not a no-op that would immediately\nget me back to a schedular of some sort.)\n  \n>- if there is a POST, and\n>- if the connection is held open,\n>- (and if the request is HTTP/1.0?),\n>the server should look for, and silently discard, a CRLF that follows\n>the entity body.\n>Dave\n>\n\nProtocols can only stand so many special purpose hacks.  If you\ncontinue to add things like this that are special case everwhere\non a per method basis, there lies long term madness.  No one will get\nimplementations right, and interoperability suffers (not to mention\nspagetti code everywhere).\n\nIt seemed to use like a less obtrusive solution to the problem of\nexisting broken post scripts was to add a general no-op (also somewhat\nuseful for performance measurements) rather than bandaid the post\nmethod.  I believe you need to look at this solution in that light.\nAnd I'm glad you're mind is so strange as to find the problems in\nit as first written.\n- Jim\n\n\n\n", "id": "lists-010-2867823"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "I'm somewhat disturbed by calling CRLF a \"NULL Request\". This seems to me to \nopen up a lot of opportunities to fall into holes when writing statements like \n\"Every request receives a response code.\"\n\nPlus I don't think it quite captures the problem for the server writer. Server \nwriters are going to get extraneous CRLFs appended to certain content types and \nthey have to be aware of that.\n\n\n\nI suggest we call it what it is, a KLUGDE (or a hickup). Then we are not likely \nto commit an error later on by inadvertently refering to \"Requests\" and \nincluding NULL requests by mistake.\n\nI think that there should also be a statement to warn server writers not to \ndepend on the additional CRLF. \n\nI don't see that we really need to allow an unbounded stream of NULL-Kludges. \nThere can only be one NULL kludge produced and that will occur after a request \nis sent. In CSP we would have -\n\n\nClient = out?Full-Request -> \n(out!NULL-Kludge -> Client-Response | Client-Response)\nClient-Response = in?Full-Response -> Client\n\nThen we have a server -\n\n\nServer = in?Full-Request -> \n(in?NULL-Kludge! -> Server-Response | Server-Response)\nServer-Response = in?Full-Response -> Client\n\n\nAnd a proxy looks like :-\n\n\nProxy = in?Full-Request -> \n(in?NULL-Kludge! -> Proxy-Response | Proxy-Response |\nProxy-Delegate)\nProxy-Delegate = ... something horrible...\nProxy-Response = in?Full-Response -> Client\n\n\nKeeping that extra CRLF bug about is a real pain.\n\n\nPhill\n\n\n\n", "id": "lists-010-2879236"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "    I'm somewhat disturbed by calling CRLF a \"NULL Request\". This seems\n    to me to open up a lot of opportunities to fall into holes when\n    writing statements like \"Every request receives a response code.\"\n\n    Plus I don't think it quite captures the problem for the server\n    writer.  Server writers are going to get extraneous CRLFs appended\n    to certain content types and they have to be aware of that.\n\n    I suggest we call it what it is, a KLUGDE (or a hickup). Then we\n    are not likely to commit an error later on by inadvertently\n    refering to \"Requests\" and including NULL requests by mistake.\n\n    I think that there should also be a statement to warn server\n    writers not to depend on the additional CRLF.\n    \nI think you are on the right track here.  In RFC791, there is a succinct\ndescription of what has become known as the \"robustness principle\"\n\nIn general, an implementation must be conservative in its\nsending behavior, and liberal in its receiving behavior.\n\nThis was restated in RFC1122, and is generally a good guide for\ndeciding how to approach this kind of borderline-acceptable behavior.\n\nI suggest that the HTTP spec say something vaguely along these lines:\n\n(1) Don't send extra CR-LFs in HTTP messages\n(2) The recipient of an HTTP message should ignore CR-LFs\n(and probably other \"whitespace\") if by ignoring that\nwhitespace it can convert a syntactically incorrect message into\na syntactically correct one without any ambiguity.\n\nIn other words,\nyou all\nc a n\nread this \n\nsentence\n\neven if it looks like e. e. cummings wrote it.\n\nThen we don't need to worry about the complexities of trying to\ndefine a null method, just to avoid honoring the robustness principle.\n\n-Jeff\n\n\n\n", "id": "lists-010-2887989"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "  > It seemed to use like a less obtrusive solution to the problem of\n  > existing broken post scripts was to add a general no-op (also somewhat\n  > useful for performance measurements) rather than bandaid the post\n  > method.  I believe you need to look at this solution in that light.\n\nI like John Franks's implementation, where inter-request CRLF's are\ndropped on a persistent connection.  However, I think it will be tricky\nto craft words to say that.  And I agree with Phillip Hallam-Baker's\nconcern that gobbling CRLFs silently violates the request-response\nparadigm.  I recognize the possible utility of a null request, but it\nwould seem to merit a response, which leads to the question, \"What kind\nof response, when you don't know the protocol version, etc.?\"\n\nI'm inclined instead to add a note in 7.2.2 (Entity Body - Length) that\nsome older client implementations included a superfluous CRLF following\nan entity body, and that robust servers should ignore those extra\ncharacters.  (Yes, I recall that that's what we started with before\nheading toward the null request \"solution\".)\n\n  > And I'm glad you're mind is so strange as to find the problems in\n  > it as first written.\nGee, thanks!  It's nice to know my warped mind is so appreciated.  :-)\n\nDave Kristol\n\n\n\n", "id": "lists-010-2896674"}, {"subject": "3.2.1 General Syntax: net_pat", "content": "The syntax for net_path would appear to be buggy:\n\n             net_path       = \"//\" net_loc [ abs_path ]\n             net_loc        = *( pchar | \";\" | \"?\" )\n\nrom these I derive the following net_path\n///foo/bar\nwith net_loc -> <empty string>.  Is that intended?\n\nDave Kristol\n\n\n\n", "id": "lists-010-2905247"}, {"subject": "5.1.2 RequestUR", "content": "5.1.2 says:\n             Request-URI    = \"*\" | absoluteURI | abs_path\n\nThe intent is to require a specific form of absoluteURI, but the syntax\nat 3.2.1:\n             absoluteURI    = scheme \":\" *( uchar | reserved )\nallows something much broader.\n\nPerhaps we need:\nrequest-abs-URI= scheme \":\" net_path\n\nWhat can/should we say about \"scheme\"?\n\nDave Kristol\n\n\n\n", "id": "lists-010-2912489"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": ">  > It seemed to use like a less obtrusive solution to the problem of\n>  > existing broken post scripts was to add a general no-op (also somewhat\n>  > useful for performance measurements) rather than bandaid the post\n>  > method.  I believe you need to look at this solution in that light.\n>\n> I like John Franks's implementation, where inter-request CRLF's are\n> dropped on a persistent connection.  However, I think it will be tricky\n> to craft words to say that.  And I agree with Phillip Hallam-Baker's\n> concern that gobbling CRLFs silently violates the request-response\n> paradigm.  I recognize the possible utility of a null request, but it\n> would seem to merit a response, which leads to the question, \"What kind\n> of response, when you don't know the protocol version, etc.?\"\n>\n> I'm inclined instead to add a note in 7.2.2 (Entity Body - Length) that\n> some older client implementations included a superfluous CRLF following\n> an entity body, and that robust servers should ignore those extra\n> characters.  (Yes, I recall that that's what we started with before\n> heading toward the null request \"solution\".)\n\nI guess I'm less concerned about \"violating the request/response model\nof HTTP\".  Then again, I've designed two previous streaming protocol based\nsystems :-).\n\nThe above looks like another possibility that is not a band-aid solution.\nI'm happy either with a general null-request solution, or with recrafting\n7.2.2.  I am unhappy with any solution that would either hack individual\nmethods, or be version dependent or depend too much on the details of\nthe transport connection state.\n\nBut note the last paragraph of 7.2.2:\n  When a Content-Length is given in a message where an entity body is allowed,\n  its field value MUST exactly match the number of OCTETs in the entity body.\n  HTTP/1.1 user agents MUST notify the user when an invalid length is\n  recieved and detected.\n\nExactly what would you say?  What exactly would 7.2.2 look like?\nAnd preferably notify new clients if they\nare buggy, so that buggy implementations don't continue to be generated (less\nimportant, but sometimes key).\n- Jim\n\n\n\n", "id": "lists-010-2919899"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "[ Warning --- half-baked idea follows ]\n\nPerhaps the spurious CRLFs following Netscape POST transactions would\nbe easier to deal with if they were viewed as being appended to the\nbeginning of the request following the POST rather than being appended\nto the end of the POST itself.  That is, we could declare that in\nconnections where HTTP/1.0 keep-alive back compatibility is desired,\nservers should allow a request-line to be *preceded* by an arbitrary\namount of spurious white-space, including CRLF combinations, which\nthey should simply ignore.\n\nI'm starting from the --- hopefully non-bogus --- theory that the\nCRLFs are in the stream no matter what, that we want the servers to\nignore them, and the problem is coming up with a way of saying that\nwithout messing up the rest of the document.  Unfortunately, the two\nsuggestions floated so far do complicate the document a bit ---\ncalling them \"null requests\" creates an exception to every rule\nelsewhere which \"all requests\" should follow, while considering them\nto be an addendum to the POST-request itself messes up the semantics\nof Content-length.\n\nThe hope here, then, is that less of the document depends on the\nsyntax of request-lines then on these other things, and so sweeping\nthese CRLFs under that part of the rug, rather than another, results\nin a somewhat less unsightly bulge.  However, there may very well be a\ndependance in the document that I've missed....\n\n[ End half-baked idea. ]\n\nrst\n\n\n\n", "id": "lists-010-2929375"}, {"subject": "OPTIONS/absoluteURI/*, Sect. 5.1.", "content": "Does it make sense to ask for OPTIONS * for a particular virtual host\non a server?  Suppose foo.com offers hosting for a.com and b.com on its\nserver.  I can make this request:\nOPTIONS * HTTP/1.1\nHost: a.com\n\nSuppose I know that the server at foo.com (a.com) supports HTTP/1.1.\nHow do I make the above request without using a Host header?  Like\nthis?\nOPTIONS http://a.com HTTP/1.1\n\nThe words in 5.1.2 don't say what to do if abs_path in an absoluteURI\nsent to an origin server is null.  (For that matter, 5.1.2 doesn't\nreally discuss how an origin server interprets absoluteURI or what\nforms are valid, as I indicated in an earlier message.)  If the answer\nfor null abs_path is, \"the same thing as for proxy servers\", then my\nguess above is right.  Which means the choice for default abs_path is\nmethod-dependent.\n\nDave Kristol\n\n\n\n", "id": "lists-010-2938280"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "jg@w3.org:\n>\n>I guess I'm less concerned about \"violating the request/response model\n>of HTTP\".  Then again, I've designed two previous streaming protocol based\n>systems :-).\n>\n>The above looks like another possibility that is not a band-aid solution.\n>I'm happy either with a general null-request solution, or with recrafting\n>7.2.2.  I am unhappy with any solution that would either hack individual\n>methods, or be version dependent or depend too much on the details of\n>the transport connection state.\n\nHow about this:\n\n             Full-Request   = Request-Line              ; Section 5.1\n                              *( General-Header         ; Section 4.3\n                               | Request-Header         ; Section 5.2\n                               | Entity-Header )        ; Section 7.1\n                              CRLF\n                              [ Entity-Body ]           ; Section 7.2\n                              [ CRLF ]\n                              ^^^^^^^^\n\nTogether with a text\n\n Clients SHOULD NOT include the optional CRLF at the end of a request,\n but servers MUST be tolerant of clients which do include this CRLF.\n\n   Note: Many existing HTTP/1.0 clients add a CRLF at the end of a POST\n   request.\n\nKoen.\n\n\n\n", "id": "lists-010-2945426"}, {"subject": "Host requestheader MUST accompany..", "content": "Words to that effect appear in Sect. 8 and App.D.1.  They should say,\ninstead, that either an absoluteURI or Host request-header must\naccompany all HTTP/1.1 requests.\n\nDave Kristol\n\n\n\n", "id": "lists-010-2954278"}, {"subject": "RE: NULLRequest (Sect. 4.1", "content": "This is not as good as saying it is at the beginning, from the\nstandpoint of straightforward parsing. It is harder to look for optional\nstuff at the end, when no more stuff may be forthcoming -- I don't know\nhow long to wait to see if it arrives. It is easy to discard optional\nstuff at the beginning -- I've already decided to wait for a request,\nand I just throw out initial CRLFs.\n\nPaul \"speaks from having his web server hung by similar things\"\n\n>----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Sent: Wednesday, April 24, 1996 1:30 PM\n>To: jg@w3.org\n>Cc: dmk@allegra.att.com; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: NULL-Request (Sect. 4.1)\n>\n>jg@w3.org:\n>>\n>>I guess I'm less concerned about \"violating the request/response model\n>>of HTTP\".  Then again, I've designed two previous streaming protocol based\n>>systems :-).\n>>\n>>The above looks like another possibility that is not a band-aid solution.\n>>I'm happy either with a general null-request solution, or with recrafting\n>>7.2.2.  I am unhappy with any solution that would either hack individual\n>>methods, or be version dependent or depend too much on the details of\n>>the transport connection state.\n>\n>How about this:\n>\n>             Full-Request   = Request-Line              ; Section 5.1\n>                              *( General-Header         ; Section 4.3\n>                               | Request-Header         ; Section 5.2\n>                               | Entity-Header )        ; Section 7.1\n>                              CRLF\n>                              [ Entity-Body ]           ; Section 7.2\n>                              [ CRLF ]\n>                              ^^^^^^^^\n>\n>Together with a text\n>\n> Clients SHOULD NOT include the optional CRLF at the end of a request,\n> but servers MUST be tolerant of clients which do include this CRLF.\n>\n>   Note: Many existing HTTP/1.0 clients add a CRLF at the end of a POST\n>   request.\n>\n>Koen.\n>\n>\n\n\n\n", "id": "lists-010-2961426"}, {"subject": "RE: NULLRequest (Sect. 4.1", "content": "I think this is good. This is easier than Jeffs suggestion, which allows\nextra LWS in lots of places.\n\nTo effect this:\nChange the first line of the definition of Full_Request in section 4.1\n(Message Types) and section 5 (why are they duplicated?) from\n>Full-Request = Request-Line\n>to\n>Full-Request = *( CRLF ) Request-Line\n\nPaul\n>----------\n>From: rst@ai.mit.edu[SMTP:rst@ai.mit.edu]\n>Sent: Wednesday, April 24, 1996 1:00 PM\n>To: dmk@allegra.att.com; jg@w3.org\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: NULL-Request (Sect. 4.1)\n>\n>[ Warning --- half-baked idea follows ]\n>\n>Perhaps the spurious CRLFs following Netscape POST transactions would\n>be easier to deal with if they were viewed as being appended to the\n>beginning of the request following the POST rather than being appended\n>to the end of the POST itself.  That is, we could declare that in\n>connections where HTTP/1.0 keep-alive back compatibility is desired,\n>servers should allow a request-line to be *preceded* by an arbitrary\n>amount of spurious white-space, including CRLF combinations, which\n>they should simply ignore.\n>\n>I'm starting from the --- hopefully non-bogus --- theory that the\n>CRLFs are in the stream no matter what, that we want the servers to\n>ignore them, and the problem is coming up with a way of saying that\n>without messing up the rest of the document.  Unfortunately, the two\n>suggestions floated so far do complicate the document a bit ---\n>calling them \"null requests\" creates an exception to every rule\n>elsewhere which \"all requests\" should follow, while considering them\n>to be an addendum to the POST-request itself messes up the semantics\n>of Content-length.\n>\n>The hope here, then, is that less of the document depends on the\n>syntax of request-lines then on these other things, and so sweeping\n>these CRLFs under that part of the rug, rather than another, results\n>in a somewhat less unsightly bulge.  However, there may very well be a\n>dependance in the document that I've missed....\n>\n>[ End half-baked idea. ]\n>\n>rst\n>\n>\n>\n>\n\n\n\n", "id": "lists-010-2974953"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": ">This is not as good as saying it is at the beginning, from the\n>standpoint of straightforward parsing. It is harder to look for optional\n>stuff at the end, when no more stuff may be forthcoming -- I don't know\n>how long to wait to see if it arrives. It is easy to discard optional\n>stuff at the beginning -- I've already decided to wait for a request,\n>and I just throw out initial CRLFs.\n>\n>Paul \"speaks from having his web server hung by similar things\"\n\nI believe this statement.  Now how to write the BNF that\nexpresses throwing away inital CRLF's?  I'm not good enough at this \nextended BNF used in this spec to trust myself to do it right... \nCould you (or someone else) who is expert do so?  I think we've\nconverged on the right solution here, if we can write it down correctly.\n- Jim\n\n\n\n", "id": "lists-010-2988257"}, {"subject": "Re: Host requestheader MUST accompany..", "content": ">Words to that effect appear in Sect. 8 and App.D.1.  They should say,\n>instead, that either an absoluteURI or Host request-header must\n>accompany all HTTP/1.1 requests.\n\nI don't believe that was the resolution agreed to.\n\nThere was insufficient trust that implementers would get it right,\nif it were optional.\n\nBetter to require sending the redundant information all the time; simple\nrule, can't be mis-interpreted.\n- Jim\n\n\n\n", "id": "lists-010-2997201"}, {"subject": "Re: Host requestheader MUST accompany..", "content": "At 04:33 PM 4/24/96 EDT, Dave Kristol wrote:\n>Words to that effect appear in Sect. 8 and App.D.1.  They should say,\n>instead, that either an absoluteURI or Host request-header must\n>accompany all HTTP/1.1 requests.\n\nFor now it's probably safer to require that Host: appear on ALL 1.1\nrequests, regardless of whether or not the Request-URI includes the Host\ninformation.  there's more assurance this way that some people won't screw\nit up.\n\nMore importantly: consider what happens if ClientFoo does not send the\nHost:, instead deciding to send absoluteURI, and ClientFoo is talking to a\nproxy.  If that proxy is old, it will strip the host info out of the\nReqest-URI and pass it on.  Now our 1.1 origin server gets no Host: and no\nabsoluteURI.  Sure - we could make special case language saying \"You have to\nsend Host:, unless you are talking to a 1.1 server you dont have to, but you\ndo have to if your talking to a proxy.\"  What was it JG was saying?:\n\"Protocols can only stand so many special purpose hacks.\"\n\nAs such, I think we have good reason, and rough consensus, for Host: header\nto be mandatory on all 1.1 requests.  Which is why the issue was closed, and\nthe language exists in the spec.\n\n-----\nthe Programmer formerly known as Dan          \n                                     http://www.spyglass.com/~ddubois/\n\n\n\n", "id": "lists-010-3004683"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "Paul writes:\n    I think this is good. This is easier than Jeffs suggestion, which allows\n    extra LWS in lots of places.\n    \n    To effect this:\n    Change the first line of the definition of Full_Request in section 4.1\n    (Message Types) and section 5 (why are they duplicated?) from\n    >Full-Request = Request-Line\n    >to\n    >Full-Request = *( CRLF ) Request-Line\n    \nI agree that this would be a good way to make an explicit but\nnot particularly kludgey change for HTTP/1.1 implementations,\nif they were the only implementations we need to worry about.\n\nHowever, one would have to be quite careful to say that there\nare different grammars for senders and receivers, because senders\nhave to avoid sending extra CRLFs to HTTP/1.0 servers.  And I\nthink that having something like\n    \n    Clients MUST generate\n    Full-Request = Request-Line\n    and servers MUST accept\n    Full-Request = *( CRLF ) Request-Line\n\nmight be more confusing than simply stating a specific\napplication of the robustness principle.\n\n-Jeff\n\n\n\n", "id": "lists-010-3013040"}, {"subject": "Re: 3.2.1 General Syntax: net_pat", "content": "The triple slash is used quite deliberately in the LDAP URL scheme.\nThere it denotes an URL that can be used through any globally coordinated\nLDAP server.\n\nI would object if the LDAP scheme was made illegal just after publication.\n\n           Harald A\n\n\n\n", "id": "lists-010-3022867"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": ">This is not as good as saying it is at the beginning, from the\n>standpoint of straightforward parsing. It is harder to look for optional\n>stuff at the end, when no more stuff may be forthcoming -- I don't know\n>how long to wait to see if it arrives. \n\nPaul here encapsulates the nub of the argument. This may be the way we may wish \nto present the problem. It isn't a solution for the server writers.\n\nServer writers need to be aware of the bogus CRLF problem and \"cope\". This may \nlead to butt-ugly code but sending spurious data was a butt-ugly hack in the \nfirst place. There are TCP/IP stacks which break unless the server writer codes \narround this lunacy. \n\nI think we all agree that we don't like the fact that the iceberg exists. The \nproblem is to avoid it holing the ships :-) \n\n\n\nI think Jeff's wording with clients must not do the wrong thing, servers must be \naware that this is a very common form of garbage is a good one.\n\n\nPhill\n\n\n\n", "id": "lists-010-3030299"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "I think we are in danger of getting into a rat hole here. Or maybee we are \nalrteady in it and are setting about widening it :-)\n\n\nThe problem I see with NULL-Request is that certain IESG entities have a \"wart \nalarm\" that goes off when an ugly kluge makes its way into a spec. There is a \ntension here between the \"keep it clean\" brigade and the \"make it work\" brigade. \nAs a longstanding member of the \"keep it clean\" brigade and a somewhat extreeme \none I think that in this case we need to be carefull.\n\nGiven that we are talking about HTTP/1.1 is there any reason why we cannot \nchange the statement \"clients SHOULD NOT send a NULL-Request\" to \"clients MUST \nNOT generate a NULL-Request\". Here I would differentiate send from generate in \nthat a client generates a message and a proxy recieves input and passes it on. \nIn this case it might just possibly be permissable for a stupid proxy such as a \ntunnel to send across spurious CRLFs.\n\n\nAs for Roberts \"attach them to the next message\" hack. It doesn't work since a \nproblem is caused by TCP/IP stacks which send out ABORT if a connection is \nclosed with unread data in the input buffer. This has been a reason for many \nWindows HTTP servers loosing. \n\nI think that what we need is some \"speed bump\" warning notice somewhere. There \nis a tension here between the traditional IETF approach of \"describe the spec, \nthe whole spec and nothing but the spec\" and the \"implementation guide\" \napproach. I think that a large part of the reason for the thickness of the spec \nis not the complexity of HTTP itself but the complexity of the implementation \nspace. The implementation guide approach is the best way to get people to write \nto the spec IMHO. We might have a problem with getting the merits of this case \nover with the IESG.\n\n\nI defend the 123 pages of HTTP as being approximately $10million of IPO market\ncapitalization per page making each page worth approximately as much as an\noriginal copy of Magna Carta.\n\nPhill\n\n\n\n", "id": "lists-010-3038390"}, {"subject": "Re: Twophase send", "content": "This one almost completely got dropped on the floor; there were some\nmail exchanges that Jeff got to me at the last minute, but not final\nwording that had undergone wide review.  Rather than leaving the \nsituation completely wrong (as was in the 01 draft), I made an attempt \nto put something together out of the mail messages over last weekend, \nbut it was far from polished.\n\nThat section (entity transmission requirements) needs careful\nredrafting before I consider that issue closed.  Note it is marked\nSlushy.\n\nPlease CAREFULLY read what is now there.  It is closer to what needs\nto be said, but not done. Ignore the first sentence of the section which is \ncompletely bogus; I should have deleted it), and suggest better words.  \nI'll also rework it when I get a chance (section 8.4.1) myself if others\ndon't beat me to it.  And the issues list does say it needs WG review,\nwhich it is now getting.\n- Jim\n\n\n\n", "id": "lists-010-3047540"}, {"subject": "Re: Twophase send", "content": "jg@w3.org:\n[...]\n>That section (entity transmission requirements) needs careful\n>redrafting before I consider that issue closed.  Note it is marked\n>Slushy.\n\nA careful redrafting of 8.4.1 will only address one (problem b.3) of\nmy five problems.\n\nThe main theme of my message was: I want two-phase sends *to be\nremoved completely* from 1.1.  I want http/1.0 style one-phase posts.\nThis means throwing out 8.4.1 completely.\n\nI include again my objections to the inclusion of two-phase below.\n\nPlease add this issue to the agenda of the upcoming phone conference.\n\nKoen.\n\n--snip---\n\nI see that the Monday internet draft still includes two-phase POSTS\nand PUTS.  I strongly object to this \n a) on procedural grounds\n b) on technical grounds.\n\nAd a:\n\na.1) The issues list says:\n\nTwo Phase methods: \n         JM Section 8.4 POST\n         Two-phase POST removed\n         ^^^^^^^^^^^^^^^^^^^^^^\n         Mogul has writeup of result of discussion?\n         Status: need writeup, WG review \n\nand I clearly remember that we indeed conclude that two-phase methods\nshould not be in 1.1 at the end of the two-phase wars some months ago.\n\na.2) Also, the 02 draft says:\n\n      POST requests must obey the entity transmission requirements set\n      out in section 8.4.1 [which talks about two-phase].\n\nWhile the 01/00 drafts said:\n\n   HTTP/1.1 allows for a two-phase process to occur in accepting and \n   processing a POST request. If the media type of the posted entity \n   is not \"application/x-www-form-urlencoded\" [5], an HTTP/1.1 client \n   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  \n   must pause between sending [....]\n\nThis considerable change was not discussed.\n\n\nAd b:\n\nb.1) Two-phase saves bandwidth sometimes, at the cost of speed\n(round-trips) for each POST request, no matter how small.  I have seen\nno statistics that this tradeoff improves current conditions, while I\nsuspect that it does not in many cases. Two-phase thus adds complexity\nwithout having established the need for this.  If we have it, it\nshould at least be optional for small POST requests.\n\nb.2) The new requirement that two-phase is also used for normal POSTS\nof small forms means degradation of performance for many existing\nforms applications when upgraded to 1.1.  It may also decrease my\nchance of making a successful POST transaction (with a busy search\nengine) if the backbone is dropping a significant number of packets.\n\nb.3) Finally, the MUST/SHOULD text about two-phase does not take\nproxies, especially 1.0 proxies, into account.\n\nIf I am to agree with two-phase staying in, I would require all points\nabove to be convincingly addressed.\n\n\n\n", "id": "lists-010-3055569"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "Having begun this thread, and in the interests of closure, I want to\nmake a proposal that draws on remarks by Paul Leach and Jeff Mogul.\n\n1) Remove references to NULL-Request.\n\n2) In Sect. 5, where it now says \"A NULL-Request MUST be ignored,\"\nsubstitute the following:\n\"In the interest of robustness, HTTP/1.1 servers SHOULD ignore\nnull request lines (ones that comprise just CRLF).\"\n\nDave Kristol\n\n\n\n", "id": "lists-010-3072091"}, {"subject": "Re: 3.2.1 General Syntax: net_pat", "content": "> The syntax for net_path would appear to be buggy:\n> \n>              net_path       = \"//\" net_loc [ abs_path ]\n>              net_loc        = *( pchar | \";\" | \"?\" )\n> \n> From these I derive the following net_path\n> ///foo/bar\n> with net_loc -> <empty string>.  Is that intended?\n\nYes.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3079279"}, {"subject": "Re: 5.1.2 RequestUR", "content": "> 5.1.2 says:\n>              Request-URI    = \"*\" | absoluteURI | abs_path\n> \n> The intent is to require a specific form of absoluteURI, but the syntax\n> at 3.2.1:\n>              absoluteURI    = scheme \":\" *( uchar | reserved )\n> allows something much broader.\n\nThe syntax is correct.  The intent is to allow any form of absolute URI.\n\n> What can/should we say about \"scheme\"?\n\nNothing beyond what is already said.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3086935"}, {"subject": "Re: Twophase send", "content": "> b.1) Two-phase saves bandwidth sometimes, at the cost of speed\n> (round-trips) for each POST request, no matter how small.  I have seen\n> no statistics that this tradeoff improves current conditions, while I\n> suspect that it does not in many cases. Two-phase thus adds complexity\n> without having established the need for this.  If we have it, it\n> should at least be optional for small POST requests.\n\nThis is simply untrue.  The two-phase mechanism does not come into play\nuntil AFTER the first request encountered A FAILED CONNECTION WITH RESET.\n\n> b.2) The new requirement that two-phase is also used for normal POSTS\n> of small forms means degradation of performance for many existing\n> forms applications when upgraded to 1.1.  It may also decrease my\n> chance of making a successful POST transaction (with a busy search\n> engine) if the backbone is dropping a significant number of packets.\n\nAlso untrue.\n\n> b.3) Finally, the MUST/SHOULD text about two-phase does not take\n> proxies, especially 1.0 proxies, into account.\n\nAgain, not true.  The section uses the term \"client\" exactly as defined\nby the specification.\n\n> If I am to agree with two-phase staying in, I would require all points\n> above to be convincingly addressed.\n\nNone of your points apply to the existing text.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3094786"}, {"subject": "Re: 5.1.2 RequestUR", "content": "I said earlier:\n> 5.1.2 says:\n>              Request-URI    = \"*\" | absoluteURI | abs_path\n> \n> The intent is to require a specific form of absoluteURI, but the syntax\n> at 3.2.1:\n>              absoluteURI    = scheme \":\" *( uchar | reserved )\n> allows something much broader.\n>\n> What can/should we say about \"scheme\"?\n\nIt's clear I wasn't clear about what I meant to say....\n\nI would think that for the *http* specification, particularly for an\norigin server, there are the following additional requirements that\nare currently unstated:\n\n- absoluteURI must be an http_URL (3.2.2)\n- (should https_URL and shttp_URL also be mentioned?)\n- host must be the FQDN of the host to which the request is sent\n\nDave Kristol\n\n\n\n", "id": "lists-010-3104095"}, {"subject": "Re: Twophase send", "content": "Roy T. Fielding:\n>\n>> b.1) Two-phase saves bandwidth sometimes, at the cost of speed\n>> (round-trips) for each POST request, no matter how small.  I have seen\n>> no statistics that this tradeoff improves current conditions, while I\n>> suspect that it does not in many cases. Two-phase thus adds complexity\n>> without having established the need for this.  If we have it, it\n>> should at least be optional for small POST requests.\n>\n>This is simply untrue.  The two-phase mechanism does not come into play\n>until AFTER the first request encountered A FAILED CONNECTION WITH RESET.\n\nIf this is the case, my problems with two-phase would mostly\ndisappear.  If you are right, I misinterpreted (the context of?) the\nfollowing language in the spec:\n\n    If the client knows that the server is an HTTP/1.1 (or later) server,\n    because of the server protocol version returned with a previous request\n    on the same persistent connection [alternatively:  within the past <N>\n    hours], it MUST wait for a response.  If the client believes that the\n               ^^^^\n    server is a 1.0 or earlier server, it    SHOULD continue transmitting\n    its request after waiting at least [5] seconds for a status response.\n\nand I strongly suggest that this part is rewritten to make it more\nclear when this MUST comes into play.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-3111843"}, {"subject": "Re: 5.1.2 RequestUR", "content": ">> 5.1.2 says:\n>>              Request-URI    = \"*\" | absoluteURI | abs_path\n>> \n>> The intent is to require a specific form of absoluteURI, but the syntax\n>> at 3.2.1:\n>>              absoluteURI    = scheme \":\" *( uchar | reserved )\n>> allows something much broader.\n>>\n>> What can/should we say about \"scheme\"?\n>\n>It's clear I wasn't clear about what I meant to say....\n>\n>I would think that for the *http* specification, particularly for an\n>origin server, there are the following additional requirements that\n>are currently unstated:\n>\n>- absoluteURI must be an http_URL (3.2.2)\n\nI don't think so.  Proxies also act as gateways between protocols,\nso a client that only knows how to do HTTP can still get resources\nin other protocols (e.g. ftp, gopher, etc.).  If you make this\nrequirement, then you require all clients to have native access\nsupport.  This seems bad to me.\n\n>- (should https_URL and shttp_URL also be mentioned?)\n\nAnd whatever URL type comes up in the future?  Seems best to be silent,\nrather than restrictive.  Not to mention the problem of referencing \nnon-standards documents.\n\n>- host must be the FQDN of the host to which the request is sent\n\nNow this makes some sense to me; to require the FQDN in any URL type\nThis is the last trace of the host problem.  I'm not familiar enough with\nall the specs to say if we have the situation right in the collection\nof HTTP related specs.\n\nEditorial comment here:  It is often good practice not to over-specify\nsomething, which might make an implementation non-compliant for no\nperceptible good reason (we have the example in the first situation\nabove).  Conversely, when you specify something precisely,\nwhich may be needed for interoperable implementations, you must\nfully understand all the consequences. For example, the broken (esthetically\nugly) wide lines and arcs in the X Window System are the result \nof a simple and  precise specification the consequences of which were \nnot fully understood until after the fact; we ran into what was\nstill a research problem without realizing it.  You can get burned both ways.\n\n- Jim\n\n\n\n", "id": "lists-010-3120931"}, {"subject": "RE: Twophase send", "content": ">----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Subject: Re: Two-phase sends\n>\n[omissions...]\n>\n>a.1) The issues list says:\n>\n>Two Phase methods: \n>         JM Section 8.4 POST\n>         Two-phase POST removed\n>         ^^^^^^^^^^^^^^^^^^^^^^\n>         Mogul has writeup of result of discussion?\n>         Status: need writeup, WG review \n>\n>and I clearly remember that we indeed conclude that two-phase methods\n>should not be in 1.1 at the end of the two-phase wars some months ago.\n\nWhat's in section 8.4.1, while still labeled \"two phase\" is quite\ndifferent than the original two phase -- the original was pessimistic,\nand this one is optimistic.\n>\n>a.2) Also, the 02 draft says:\n>\n>      POST requests must obey the entity transmission requirements set\n>      out in section 8.4.1 [which talks about two-phase].\n>\n>While the 01/00 drafts said:\n>\n>   HTTP/1.1 allows for a two-phase process to occur in accepting and \n>   processing a POST request. If the media type of the posted entity \n>   is not \"application/x-www-form-urlencoded\" [5], an HTTP/1.1 client \n>   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  \n>   must pause between sending [....]\n>\n>This considerable change was not discussed.\n\nSee below -- the removal of performance penalties in all normal cases\n>made it unnecessary to make an exception for form data.\n>\n>Ad b:\n>\n>b.1) Two-phase saves bandwidth sometimes, at the cost of speed\n>(round-trips) for each POST request, no matter how small.  I have seen\n>no statistics that this tradeoff improves current conditions, while I\n>suspect that it does not in many cases. Two-phase thus adds complexity\n>without having established the need for this.  If we have it, it\n>should at least be optional for small POST requests.\n\nRTFM. It does not say this. It says that IF THE CLIENT SEES THE\nCONNECTION CLOSE WITHOUT ANY STATUS INDICATION that it has to go into\ntwo phase mode. Before that, it can try the HTTP/1.0 style PUT or POST,\nwith the added requirment that it is obliged to monitor the connection\nfor errors and stop sending if if gets one. Hence, there will normally\nbe no performance penalty.\n>\n>b.2) The new requirement that two-phase is also used for normal POSTS\n>of small forms means degradation of performance for many existing\n>forms applications when upgraded to 1.1.  It may also decrease my\n>chance of making a successful POST transaction (with a busy search\n>engine) if the backbone is dropping a significant number of packets.\n\nI think the response for b.1 covers this case as well.\n>\n>b.3) Finally, the MUST/SHOULD text about two-phase does not take\n>proxies, especially 1.0 proxies, into account.\n\nYes it does. Read the definitions of \"client\" and \"server\". A proxy is\nboth a client and a server; the rules for clients and servers in 8.4.1\napply unchanged to proxies in their role as client or server. E.g.: A\n1.1 user agent talking to a 1.0 proxy won't use two phase, as 8.4.1 only\napplies to 1.1 clients talking to a 1.1 server. Ditto for a 1.0 proxy\ntalking to a 1.1 server.\n>\n>If I am to agree with two-phase staying in, I would require all points\n>above to be convincingly addressed.\n\nWell, I tried.\n\nPaul\n>\n>\n\n\n\n", "id": "lists-010-3129862"}, {"subject": "RE: Twophase send", "content": "(I wish I'd seen this before sending my last message on this topic...)\n\n>----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Subject: Re: Two-phase sends\n>\n[ommissions...]\n>\n>If this is the case, my problems with two-phase would mostly\n>disappear.  If you are right, I misinterpreted (the context of?) the\n>following language in the spec:\n>\n>    If the client knows that the server is an HTTP/1.1 (or later)\n>server,\n>    because of the server protocol version returned with a previous\n>request\n>    on the same persistent connection [alternatively:  within the past\n><N>\n>    hours], it MUST wait for a response.  If the client believes that\n>the\n>               ^^^^\n>    server is a 1.0 or earlier server, it    SHOULD continue\n>transmitting\n>    its request after waiting at least [5] seconds for a status\n>response.\n\nThis occurs in the paragraph right after the one saying that clients\nhave to use two-phase if they get a closed connection with no status. It\nis an elaboration on the requirements of two phase mode on clients. It\nis followed by more elaboration for clients and servers.\n>\n>and I strongly suggest that this part is rewritten to make it more\n>clear when this MUST comes into play.\n\nIf a section header \"8.4.1.1 Two phase mode\" were inserted before these\n>paragraphs, would that set the context more solidly?\n\nPaul\n\n\n\n", "id": "lists-010-3142977"}, {"subject": "201 Create", "content": "9.2 2xx Successful\n      ...\n      create the resource before using this Status-Code. If the action cannot\n      be carried out immediately, the server MUST include in the response body\n      a description of when the resource will be available; otherwise, the\n      server SHOULD respond with 202 (accepted).\n\nI believe the last sentence is wrong and should read:\n      If the action cannot be carried out immediately, the server MUST\n      include in the response body a description of when the resource\n      will be available, and the server SHOULD respond with 202\n                       ======\n      (accepted).\n\nDave Kristol\n\n\n\n", "id": "lists-010-3155013"}, {"subject": "Re: 5.1.2 RequestUR", "content": "> I would think that for the *http* specification, particularly for an\n> origin server, there are the following additional requirements that\n> are currently unstated:\n> \n> - absoluteURI must be an http_URL (3.2.2)\n> - (should https_URL and shttp_URL also be mentioned?)\n> - host must be the FQDN of the host to which the request is sent\n\nAbsolutely NOT on all three items.  HTTP is used as a generic\ninterface protocol for resolving any URI, including URNs\n(which do not have a hostname) and any number of other\nidentifiers which follow the URI syntax.  That syntax is correctly\ndescribed in the spec.\n\nThe http URL is also described because this is the only specification\nwhich can describe it.  https and shttp have their own specifications\nand are not relevant to this protocol.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3161696"}, {"subject": "Re: 5.1.2 RequestUR", "content": "  > > [DMK]\n  > > I would think that for the *http* specification, particularly for an\n  > > origin server, there are the following additional requirements that\n  > > are currently unstated:\n  > > \n  > > - absoluteURI must be an http_URL (3.2.2)\n  > > - (should https_URL and shttp_URL also be mentioned?)\n  > > - host must be the FQDN of the host to which the request is sent\n\n\"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU> wrote:\n  > Absolutely NOT on all three items.  HTTP is used as a generic\n  > interface protocol for resolving any URI, including URNs\n  > (which do not have a hostname) and any number of other\n  > identifiers which follow the URI syntax.  That syntax is correctly\n  > described in the spec.\nI said \"particularly for an origin server\".  I believe an *origin\nserver* sees either a relativeURI or an absoluteURI in the form of an\nhttp_URL on the request line.  I'm trying to nail down what an origin\nserver should expect to see.  It should be specified somewhere.\n\nDave Kristol\n\n\n\n", "id": "lists-010-3169834"}, {"subject": "3.17.2 Byte Range", "content": "In Section 3.17.2, the spec says that the if a last-byte-pos value is\nlarger than the current length of the entity, it is assumed to be\nequal to the current length of the entity.  From my reading, a server\nwould thus send first-byte-pos to the end of entity.\n\nI'm afraid this behavior doesn't make sense to me.  It seems to me\nthat this should be treated as an invalid byte-range-spec, and that\nthe recipient should ignore it and send back the entire entity.\n\nThe sender already has a method of requesting from first-byte-pos to\nthe end of the entity (by leaving off the the last-byte-pos), and\nI don't understand why we want to duplicate this behavior in response\nto what looks like an error.  \n\nTed Hardie\n\n\n\n", "id": "lists-010-3178328"}, {"subject": "Re: 3.17.2 Byte Range", "content": "    In Section 3.17.2, the spec says that the if a last-byte-pos value is\n    larger than the current length of the entity, it is assumed to be\n    equal to the current length of the entity.  From my reading, a server\n    would thus send first-byte-pos to the end of entity.\n    \nI think you are referring to a case like this: /foo.gif is 1024 bytes\nlong, and a client sends\nGET /foo.gif HTTP/1.1\nRange: bytes = 0-2047\nThe spec now implies that the returned response would have\nContent-Range: bytes 0-1023/1024\nIs this consistent with your reading?\n\n    I'm afraid this behavior doesn't make sense to me.  It seems to me\n    that this should be treated as an invalid byte-range-spec, and that\n    the recipient should ignore it and send back the entire entity.\n    \n    The sender already has a method of requesting from first-byte-pos to\n    the end of the entity (by leaving off the the last-byte-pos), and\n    I don't understand why we want to duplicate this behavior in response\n    to what looks like an error.  \n\nI copied this part of the spec from draft-ietf-http-range-retrieval-00.txt,\nwhich had met with general approval 9at least in that section).  If\nwe change it, we'll need to make sure that nobody objects.\n\nMy own feeling is that a simple application of the robustness principle\nhere would say \"return the most helpful response\", not \"return an error\".\nAfter all, the response is clearly marked with a Content-Length\nthat allows the recipient to decide if the right thing happened.\n\nConsider this case: the client currently has a cached copy of a GIF\nfile, and wants to see if the copy at the server has been updated.\nHowever, it's a huge interlaced GIF file, so the client only wants\nto see the first \"layer\" (or whatever it's called) initially.\n\nSo the client might send\nGET /huge.gif HTTP/1.1\nIf-Invalid: \"cache-validator-value-1\"\nRange: bytes = 0-4095\n\nMeanwhile, the copy at the server HAS been updated, but the service\nauthor has figured out that the image ought to be a lot simpler, and\nit now only has 2048 bytes.  Since the new validator doesn't match\nthe one in the If-invalid, the server would normally return a 200\n(OK) response to this conditional GET, not a 304 (Not modified)\nresponse.  In this case, it seems perfectly reasonable to return\nthe entire smaller file, rather than telling the poor client\n207 (Range Out Of Bounds) meaning \"sorry, you will have to ask me again,\neven though I could have sent you something useful.\"\n\nI.e., I don't think this is an error condition.\n\n-Jeff\n\n\n\n", "id": "lists-010-3185817"}, {"subject": "Re: Twophase send", "content": "Paul Leach:\n>\n>If a section header \"8.4.1.1 Two phase mode\" were inserted before these\n>paragraphs, would that set the context more solidly?\n\nProbably, but this is not what I would call a stylistically nice\nsolution.\n\nLet's just say that I managed to mis-interpret 8.4.1 as requiring that\ntwo-phase must always be used if the server is known to be 1.1.  I'm\nhappy to leave it up to Jim to rewrite the section into something\nwhich does not allow such misinterpretation.\n\n>Paul\n\nKoen.\n\n\n\n", "id": "lists-010-3195171"}, {"subject": "Re: 3.17.2 Byte Range", "content": "Jeff writes:     \n> I think you are referring to a case like this: /foo.gif is 1024 bytes\n> long, and a client sends\n> GET /foo.gif HTTP/1.1\n> Range: bytes = 0-2047\n> The spec now implies that the returned response would have\n> Content-Range: bytes 0-1023/1024\n> Is this consistent with your reading?\n> \n\nThat is consistent with my reading, but it isn't really the case\nI was worried about.  Consider the case where a client sends\n\nGET /foo.gif HTTP/1.1\nRange: bytes = 1500-2047\n\nand foo.gif is only 1800 bytes long.  My reading is that right\nnow the returned response would have a \n\nContent-Range: bytes 1500-1799/1800\n\nAnd the client would end up with the last 300 bytes, just as if a\n\nRange bytes = 1500- had been sent.  \n\nIt seems that we agree that isn't what we want, and that if the byte\nrange specified is out of bounds, the robustness prinicipal implies\nreturning the *whole* entity.  In the case you describe, I can see how\nwhat is written gets the client the whole entity, but I'm not sure\nthat it does imply that it in all cases.  In other words, I think\nthere is an ambiguity here that we need to plug.\n\nThis makes me wonder whether there isn't another ambiguity here.  I\nhad assumed from this section:\n\n      If the last-byte-pos value is present, it must be greater than or equal\n      to the first-byte-pos in that byte-range-spec, or the byte-range-spec is\n      invalid.  The recipient of an invalid byte-range-spec must ignore it.\n \nThat the recipient of an invalid byte-range-spec ignored it by treating\nthe request as if it did not contain a Range: header (once again, returning\nthe whole entity on a GET request).  Am I misreading that?\n\nRegards,\nTed Hardie\n\n\n\n", "id": "lists-010-3203376"}, {"subject": "Re: 3.17.2 Byte Range", "content": "    That is consistent with my reading, but it isn't really the case\n    I was worried about.  Consider the case where a client sends\n    \n    GET /foo.gif HTTP/1.1\n    Range: bytes = 1500-2047\n    \n    and foo.gif is only 1800 bytes long.  My reading is that right\n    now the returned response would have a \n    \n    Content-Range: bytes 1500-1799/1800\n    \n    And the client would end up with the last 300 bytes, just as if a\n    \n    Range bytes = 1500- had been sent.  \n    \n    It seems that we agree that isn't what we want,\n\nNo, I think this is acceptable behavior.\n\n    and that if the byte\n    range specified is out of bounds, the robustness prinicipal implies\n    returning the *whole* entity.\n\nI'm not sure about that last implication.  I think we really have\ntwo choices:\n(1) the server returns a straightforward interpretation\nof what the client asked for, and if the client doesn't\nlike it, the client can ask for something different.\n(2) the server tries to protect the client from receiving\nsomething that might or might not be what the client\nwants.\n\nChoice #2 seems to be unnecessarily restrictive.  Since the responses\nare always unambiguously marked with a Content-Range header, there\nis no danger (if we adopt choice #1) of confusing a properly implemented\nclient.  However, #2 runs the risk of requiring an extra round-trip\nin a case where the client might actually be happy with the 1500-1799\nrange.\n\nIf the client really does not want to receive a smaller range than\nit asked for, it could do something like\n\nGET /foo.gif HTTP/1.1\nRange: bytes =1500-2047\nIf-Valid: \"xyzzy\"\n\nknowing that if there is any change in the size of /foo.gif, this\nwill change the validator and make this conditional request return\n412 (Precondition Failed).\n\nUsing persistent-connection and pipelining, the client might even\nsend something like\n\nGET /foo.gif HTTP/1.1\nRange: bytes =1500-2047\nIf-Valid: \"xyzzy\"\n\nHEAD /foo.gif HTTP/1.1\nIf-Invalid: \"xyzzy\"\n\nso that within one RTT it knows the current size of the object,\nif it has changed (at the cost of a few more bytes on the wire).\n\n      If the last-byte-pos value is present, it must be greater than or equal\n      to the first-byte-pos in that byte-range-spec, or the byte-range-spec is\n      invalid.  The recipient of an invalid byte-range-spec must ignore it.\n \n    That the recipient of an invalid byte-range-spec ignored it by treating\n    the request as if it did not contain a Range: header (once again, returning\n    the whole entity on a GET request).  Am I misreading that?\n\nI believe the intention behind this rule is to say that if a server\nreceives something obviously bogus like\n\nGET /foo.gif HTTP/1.1\nRange: 4-3\n\nthen it should return the whole entity rather than an error.   But\nmaybe the server should return 400 (Bad Request) in this case.\n\n-Jeff\n\n\n\n", "id": "lists-010-3211888"}, {"subject": "Re: 3.17.2 Byte Range", "content": "Jeff writes:\n> I'm not sure about that last implication.  I think we really have\n> two choices:\n> (1) the server returns a straightforward interpretation\n> of what the client asked for, and if the client doesn't\n> like it, the client can ask for something different.\n> (2) the server tries to protect the client from receiving\n> something that might or might not be what the client\n> wants.\n> \n> Choice #2 seems to be unnecessarily restrictive.  Since the responses\n> are always unambiguously marked with a Content-Range header, there\n> is no danger (if we adopt choice #1) of confusing a properly implemented\n> client.  However, #2 runs the risk of requiring an extra round-trip\n> in a case where the client might actually be happy with the 1500-1799\n> range.\n\n<deletia, detailing various scenarios>\n\n\n> I believe the intention behind this rule is to say that if a server\n> receives something obviously bogus like\n> \n> GET /foo.gif HTTP/1.1\n> Range: 4-3\n> \n> then it should return the whole entity rather than an error.   But\n> maybe the server should return 400 (Bad Request) in this case.\n> \n> -Jeff\n> \n\nSigh.  And it all looked so simple, briefly.  I think that returning\nthe whole entity is a reasonable response whenever the byte-range-spec\ndoesn't fit reality.  There may be cases where a client wants the last\n300 bytes of something that it thought had 700 more bytes, but I'm not\nvery comfortable with them.  If the client doesn't care about how many\nbytes remain, there are already other, better ways of handling that,\nand they fit situations where the client is uncertain about how many\nbytes remain (or just wants \"from here to the end\"). It is also easy\nfor the client to know what to do with the whole entity, and sending\nthe whole entity means that the client doesn't need to do any new\nchecks for integrity.  I believe Jeff is right that this problem won't\noccur very often for well-implemented clients, but I think that the\nproblem *will* occur.\n\nI propose the following wording change within 3.17.2 :\n\nDelete:\n\n      If the last-byte-pos value is present, it must be greater than or equal\n      to the first-byte-pos in that byte-range-spec, or the byte-range-spec is\n      invalid.  The recipient of an invalid byte-range-spec must ignore it.\n  and\n      If the last-byte-pos value is larger than the current length of the\n      entity, it is assumed to be equal to the current length of the entity. \n\nInsert after \"If the last-byte-pos value is absent\":\n\n      If the last-byte-pos value is present, it must represent a possible\n      value within the byte-range-spec.  It must be greater than or equal\n      to the first-byte-pos in the byte-range-spec and less than or equal\n      to the current length of the entity in bytes.  Similarly, if the first-\n      byte-pos is present, it must represent a possible value.  It must be \n      greater than or equal to zero and it must not be greater than the \n      last-byte-pos value.  The recipient of an invalid byte-range-spec\n      must ignore it and return the entire entity.\n\n\nThis is more restrictive than the original wording, but I believe it\ncloses off an ambiguity and makes for a set of rules which will be\neasy for implementors to follow.\n\n\nComments?\n\nregards,\nTed Hardie\n\n\n\n", "id": "lists-010-3221769"}, {"subject": "Various problems in the 02 draf", "content": "Here is a list of problems in the 02 draft I have found so far.  I\nhave not included problems which are already being discussed on the\nlist or in the editorial group.\n\nI'll let Jim Gettys decide on the appropriate action for each of these\nproblems.  Some may be solved by simple editing, others may require a\ncall for discussion by the wg or asking questions to the author of a\nparticular piece of draft text.\n\n1.  \n\nThe new header fields have not been added to the BNF for\ngeneral-header, request-header, etc. yet.\n\n2.  \n\nThe note at the end of 303 (see other) should be at the end of 302\n(moved temporarily)\n\n3.\n\n 10.7.1 SLUSHY: Restrictions on What is Cachable\n [...]\n successful validation. If there is neither a cache validator nor an\n explicit expiration time associated with a response, we do not expect it\n to be cached, but certain caches may violate this expectation (for\n example, when little or no network connectivity is available) as long as\n they explicit mark their responses using the Warning mechanism describe\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^         \n in section 10.51.\n\nIt is unclear to me which Warning code should be used for this\nmarking.  A new code will probably need to be introduced.\n\n\n4. \n\n  10.7.2 Restrictions On What May be Stored by a Cache\n\n  [...]\n   recognize or obey this directive, malicious or compromised caches may\n   not recognize or obey this directive, and all communications networks\n                                             ^^^\n   may be vulnerable to eavesdropping.\n\nRemove `all'.  There may be trouble with people bringing up quantum\ncryptography.\n\n5.\n\n  10.7.3 Modifications of the Basic Expiration Mechanism\n\n  (or later) cache than to an HTTP/1.0 cache. This may be useful if\n  certain HTTP/1.0 caches improperly calculate ages or expiration times,\n  perhaps due to badly unsynchronized clocks.\n                 ^^^^^^^^^^^^^^^^^^^^  \nRemove `badly' or `un'.\n\n6.\n  10.7.4 SLUSHY: Controls over cache revalidation and reload\n\nThis section mixes informational text with text stating requirements.\n\nThe section mentions cookies without giving a reference or\nexplanation.\n\n\n7.\n\n 10.7.6 Miscellaneous restrictions\n\n In certain circumstances, an intermediate cache (proxy) may find it\n useful to convert the encoding of an entity body. For example, a proxy\n ^^^^^^^^^^^^^^^^^          \n might use a compressed content-coding to transfer the body to a client\n on a slow link.\n\nThis implicitly allows conversion of entity bodies by proxy caches.  I\ndon't think this was ever allowed before, and in any case it breaks\nrange retrieval (afaik, range requests work on the entity data in the\nresponse, not on the unencoded version of this data.)\n\nAlso, I cannot think of any case in which inclusion of \"no-transform\"\nwould be needed to ensure correct service.\n\n8.  \n\n    10.8 Connection\n\nThe BNF is incomplete.  The rule\n\n   connection-token = token\n\nmust be added.\n\n    connection-token 0#( \",\" connection-token )\n\ncan be simplified to\n\n    1#connection-token\n\n9.\n\n  10.8.1 Persist\n\nThe BNF is incomplete.  Add rules for param-name and value.\n\nThe text\n\n  The Persist header itself is optional, and is used only if a parameter\n  is being sent. HTTP/1.1 does not define any parameters.\n\nis either confusing, or implies that 1.1 clients can never include\nthis header.\n\n10.\n\n  10.16 Content-Location\n\n  [...]\n  A server SHOULD provide a Content-Location if, when including\n  an entity in response to a GET request on a negotiated resource, the\n  entity corresponds to a specific, non-negotiated location which can be\n  accessed via the Content-Location URI.\n\nIf this is to use the terminology of the new Section 12 to reflect\ndraft-holtman-http-negotiation, this should be:\n\n  A server SHOULD provide a Content-Location if, when including an\n  entity in response to a GET request on a *transparently* negotiated\n  resource, the entity corresponds to a specific, *not transparently*\n  negotiated location which can be accessed via the Content-Location\n  URI.\n\nHowever, I see no need to include the above sentence in the 1.1 spec.\nIt is just an over-specification which may confuse people.\n\n11.\n\n   10.19 SLUSHY Expires\n\nLast sentence:\n\n   HTTP/1.1 servers should not send Expires dates more than one\n   year in the future.\n\nI have no idea why this should be required.  In any case, this\nrequirement has had no review by the wg afaik.\n\n12.\n\n  10.33 Range\n\nLast sentence:\n\n  In some cases, it may be more appropriate to use the Range-If header\n  (see section 10.104) instead of the Range header.\n                       ^^^^^^^\n\nAccording to the Range-If section, this should be `in addition to'.\n\n13.\n\n 10.46 Age\n\n Caches transmit age values using:\n\n              Age = \"Age\" \":\" age-value\n\n              age-value = delta-seconds\n\n Age values are non-negative decimal integers, representing time in\n seconds.\n\n If a cache receives a value larger than the largest positive integer it\n can represent, or if any of its age calculations overflows, it MUST NOT\n                                                             ^^^^^^^^^^^\n transmit an Age header.  Otherwise, HTTP/1.1 caches MUST send an Age\n ^^^^^^^^^^^^^^^^^^^^^^^\n header in every response.  Caches SHOULD use a representation with at\n least 31 bits of range.\n\nThis makes the Age header useless as an indicator that the response is\nnot authoritative (not generated or validated by the origin server),\nwhich is *very* bad.  It also goes against several principles for the\ndesign of fault tolerant systems.  I strongly suggest that the marked\ntext is replaced by\n\n  it MUST transmit an Age header with its largest positive integer.\n\n\n14.\n\n   10.47 CVal\n\n      Examples:\n\n            CVal: \"xyzzy\"\n            CVal: \"xyzzy\"/W\n            CVal: \"xyzzy\";3\n                          ^\n            CVal: \"xyzzy\"/W;3\n                            ^\n\nTo reflect Section 3.14, this should be\n\n            CVal: \"xyzzy\";\"3\"\n            CVal: \"xyzzy\"/W;\"3\"\n\nSame for the examples in 10.48 and 10.49.\n\n\n15.\n\n   10.55 SLUSHY: Range-If\n\nIn my opinion, this mechanism adds to much complexity to the caching\nmodel.  It should be removed.  In my opinion, this will not lead to\nany significant loss in performance.\n\n\n16.\n\n  13.2.1 Server-Specified Expiration\n\n  [...]\n  If an origin server wishes to force a semantically transparent cache to\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^\n  validate every request, it may assign an explicit expiration time in the\n\nDelete the marked text above, it just makes the sentence more\nconfusing.\n\n17.\n\n  13.2.4 Client-controlled Behavior\n\n  While the origin server (and to a lesser extent, intermediate caches)\n  are the primary source of expiration information,\n\nI have no idea what the above text fragment means exactly.\n\n\n18.\n\n 13.12.2 SLUSHY: Varying Resources\n\nThis section says `vary' where it should say `vary or\nalternates' in many places.\n\n\n19.\n\n 13.12.2 SLUSHY: Varying Resources\n\n [...]\n Section 10.52 on Vary defines the canonical form for selecting\n headers.\n\nThe current 10.52 does not define such a canonical form.  It could be\nrewritten to state its matching rule in terms of equality of the\ncanonical forms defined in 13.12.2.\n\n\n20.\n\n 13.12.2 SLUSHY: Varying Resources\n\n [...]\n When a response is received that includes a Content-Location header but\n no variant-ID, then the update key is (content-location-URI, null), and\n the entry key for the response is (content-location-URI, null, sel-hdr-\n values).\n\nI can interpret this as implying that the following response from\nspoof.city.edu:\n\n  HTTP/1.1 200 OK\n  ...\n  Content-Location: http://www.microsoft.com/\n  Cache-control: max-age=1000000\n  ...\n\n  <h1>0S/2 RULEZ!</h1>\n\noverwrites the cached homepage of a well-known company.  Such\nmisinterpretations must be avoided.  Note that 10.16 has the same\ninterpretation problem to a lesser extent.\n\n\n21.\n\n    G.1 Support for Content Negotiation by Proxy Caches\n\n    The response received from the upstream server\n    may refresh a stale 200 response that was cached for the varying\n    resource a side effect. XXX previous sentence doesn't make sense\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nMy advise is to delete this previous sentence above.\n\n\n22.\n\n  G. Proxy Cache Implementation Guidelines\n     [...]\n  G.2  Propagation of Changes in Opaque Selection\n\nSection G.2 is not meant for proxy cache implementers, but for\nresource authors.\n\n\nThat is all I have for now.  I still have to read most of the slushy\nstuff.\n\nKoen.\n\n\n\n", "id": "lists-010-3232202"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "Dave Kristol says\n\n% Having begun this thread, and in the interests of closure, I want to\n% make a proposal that draws on remarks by Paul Leach and Jeff Mogul.\n% \n% 1) Remove references to NULL-Request.\n% \n% 2) In Sect. 5, where it now says \"A NULL-Request MUST be ignored,\"\n% substitute the following:\n% \"In the interest of robustness, HTTP/1.1 servers SHOULD ignore\n% null request lines (ones that comprise just CRLF).\"\n\nI may be wrong, but I think that at the moment there is no 1.1 client\nwhich sends the spurious CRLF. So, couldn't we say\n\n\"In the interest of robustness, HTTP/1.1 servers SHOULD ignore\nnull request lines (ones that comprise just CRLF) from HTTP/1.0 clients\"\n\ntogether with \"HTTP/1.1 clients MUST NOT generate a NULL-Request\" \n\n(and maybe say something about proxies which MUST eliminate \nNULL-Requests)??\n\n.mau.\n\n\n\n", "id": "lists-010-3247531"}, {"subject": "Re: 5.1.2 RequestUR", "content": "> I said \"particularly for an origin server\".  I believe an *origin\n> server* sees either a relativeURI or an absoluteURI in the form of an\n> http_URL on the request line.  I'm trying to nail down what an origin\n> server should expect to see.  It should be specified somewhere.\n\nNo, it doesn't need to be specified anywhere beyond the syntax\nlimitations already specified.  The server is capable of controlling\nits own namespace, period.  The protocol does not make any additional\nlimitations where none are needed, particularly when they would hinder\nuse of HTTP outside the typical Internet browser realm.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3255553"}, {"subject": "Re: NULLRequest (Sect. 4.1", "content": "mau@beatles.cselt.stet.it (Maurizio Codogno) wrote:\n  > I may be wrong, but I think that at the moment there is no 1.1 client\n  > which sends the spurious CRLF. So, couldn't we say\n  > \n  > \"In the interest of robustness, HTTP/1.1 servers SHOULD ignore\n  > null request lines (ones that comprise just CRLF) from HTTP/1.0 clients\"\n  > \n  > together with \"HTTP/1.1 clients MUST NOT generate a NULL-Request\"\nThe idea is right, but an HTTP/1.1 client that follows the\nspecification will never send the spurious CRLF anyway, so nothing\nextra need be said.\n  > \n  > (and maybe say something about proxies which MUST eliminate\n  > NULL-Requests)??\nSame remarks apply here.\n\nDave Kristol\n\n\n\n", "id": "lists-010-3263573"}, {"subject": "Re: 5.1.2 RequestUR", "content": "\"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU> wrote:\n  > > [DMK]\n  > > I said \"particularly for an origin server\".  I believe an *origin\n  > > server* sees either a relativeURI or an absoluteURI in the form of an\n  > > http_URL on the request line.  I'm trying to nail down what an origin\n  > > server should expect to see.  It should be specified somewhere.\n  > \n  > No, it doesn't need to be specified anywhere beyond the syntax\n  > limitations already specified.  The server is capable of controlling\n  > its own namespace, period.  The protocol does not make any additional\n  > limitations where none are needed, particularly when they would hinder\n  > use of HTTP outside the typical Internet browser realm.\n\nOkay, I'll accept your arguments.  However, there is still a piece I feel\nis missing:\n\n1) We want clients to send, and servers to accept, an absoluteURI.\n2) Eventually we want to obviate the need for Host:.\n3) The purpose of absoluteURI and/or Host is to specify the virtual host\nfor (at least) an http_URL.\n\nThe reason I've been trying to restrict the absoluteURI syntax is that\nthen it becomes easy to identify the virtual host.  I believe somewhere\nin the spec. there should be an explanation of how to identify the\nvirtual host of \"the resource identified by the request\".\n\nDave Kristol\n\n\n\n", "id": "lists-010-3271357"}, {"subject": "[CACHING] corrected_initial_age computation questio", "content": "I am having a little problem with the proposed computation for the\ncorrection to the received_age. Draft02, p. 87 states:\n\ncorrected_initial_age = corrected_received_age + (now - request_time)\n\nIf I don't miss something: 'now-request_time' measures the round time\ntrip between the receiver and the sender, while what we want to\nmeasure is only the time between the sender and the receiver (since\nthe Age value at the sender is supposed to be updated right before the\nsender emits it - see paragraph 5 of the same section).\n\nWhile I understand that as an approximation, this is on the safe side,\nI would like to know if this was intentional (in which case it 's\nprobably safer to state it in the text). Otherwise, I would suggest\nnaively dividing the computed RTT by two:\n\ncorrected_initial_age = corrected_received_age + (now - request_time)/2\n\nAnselm.\n\n\n\n", "id": "lists-010-3280242"}, {"subject": "8.12 TRACE and entit", "content": "As I implement TRACE, I'm puzzled by this passage:\n\n                                          A TRACE request MUST NOT include an\n      entity body and MUST include a Content-Length header field with a value\n      of zero (0).\n\nWhy must there be a Content-Length: 0 header?\n\n1) Wouldn't the absence of a Content-Length header be equivalent?\n\n2) If a server does not find a Content-Length header in the\nrequest, is it obliged to return an error?\n\nDave Kristol\n\n\n\n", "id": "lists-010-3287878"}, {"subject": "[ContentMD5 and Message Digest Authentication.], MD5 broken", "content": "Appoloigies to the list for reopening a closed issue, but the circumstances have \nchanged substantially.\n\n\nProblem\n-------\n\n I have been given a demonstration of a successful cryptanalitic attack against \na slightly modified MD5 compressor function. This attack allows an attacker to \ncreate a message that has a given MD5' value. While the attack is presently \nagainst a modified version of MD5 the full attack appears to be merelty a matter \nof additional CPU. at present the attack takes about 50 MIPS days.\n\n\nConsequences\n------------\n\nThe consequences for Message Digest Authentication cannot be assesed at this \npoint. I strongly suspect that the construction used is resistant to the \ncryptanalisis but require more details.\n\nThe consequences for the Message-MD5 tag are on the surface not as serious since \nthe current use for the tag is as a pure checksum. It is not difficult to \nimagine later schemes which would depend on the cryptographic security of the \nchecksum. For example a cache scheme which exchanges lists of signed headers \nwithout entity bodies.\n\nOn a more practical level, MD5 is likely to fall out of use over the next few\nyears. I expect that SHA will replace it, at least in the interim. SHA is \nresistant to similar attacks against MD4 and against differential cryptanalisis \nwhile MD5 is not.\n\n\nSolution\n--------\n\nAt present it appears that we cannot forward the Message Digest authentication \ndraft in its present form. We could simply replace MD5 with SHA and strongly \nrecommend its use.\n\n\nFor Content-MD5 I don't believe that the arguments for compatibility with the \nMIME spec are now valid. The breaking of the MD5 algorithm makes it unlikely \nthat this specification will be widely adopted in its current form.\n\nWe can fix the problem by simply introducing an algorithm parameter. Ie:-\n\nContent-Digest: 2A1238912371239587; alg=SHA\n\nThis change was strongly recommended by Ron Rivest, author of MD5.\n\n\nIf we don't make this change I suspect that in HTTP/1.2 we will be either \ncarrying a little used \"orphan\" tag or we will be presenting the following:-\n\n\nContent-MD5: 2A1238912371239587; alg=SHA\n\nThis construction is likely to break for obvious reasons.\n\n\nSection 10.13 will need modification. I suggest that the spirit of the change be \n\"this is how to do things if you are using MD5\". The references to RFC 1864 can \nstill stand. We are simply adding in an option to use other algorithms.\n\n\nCode\n----\n\nSHA is avaliable form the following locations:\n\nSHA.ZIP - Secure Hash Algorithm, written by Peter Gutmann in 1992.\nftp.dsi.unimi.it:/pub/security/crypt/code/SHS.tar.gz (this first one contains a \nfile with the Peter Gutmann\nimplementation) \nftp.dsi.unimi.it:/pub/security/crypt/code/gillogly-sha.tar.gz\nftp.dsi.unimi.it:/pub/security/crypt/code/sha.tar.gz\nftp.informatik.uni-hamburg.de:/pub/virus/texts/crypto/shs.zip \n\n\nOr if you want other goodies try :\nhttp://www.openmarket.com/techinfo/applied.htm\n\n\nPhill\n\n\n\n", "id": "lists-010-3295112"}, {"subject": "Re: [CACHING] corrected_initial_age computation questio", "content": "    I am having a little problem with the proposed computation for the\n    correction to the received_age. Draft02, p. 87 states:\n    \n    corrected_initial_age = corrected_received_age + (now - request_time)\n    \n    If I don't miss something: 'now-request_time' measures the round time\n    trip between the receiver and the sender, while what we want to\n    measure is only the time between the sender and the receiver (since\n    the Age value at the sender is supposed to be updated right before the\n    sender emits it - see paragraph 5 of the same section).\n    \n    While I understand that as an approximation, this is on the safe side,\n    I would like to know if this was intentional (in which case it 's\n    probably safer to state it in the text).\n\nThis *is* intentional.  The Age calculations are meant to be as\nconservative as possible, and because it is not true that all\nInternet paths are symmetrical, the most conservative assumption\nis that all of the round-trip delay is in the server-to-client\ndirection.\n\nThere are existing examples of highly assymetrical paths in the\nInternet.\n\n-Jeff\n\n\n\n", "id": "lists-010-3306020"}, {"subject": "Re: [ContentMD5 and Message Digest Authentication.], MD5 broken", "content": "> Content-MD5: 2A1238912371239587; alg=SHA\n> \n> This construction is likely to break for obvious reasons.\n\nPhill, this has already been discussed to death.  There is no advantage\nto using a generic parameter name for an Entity-Header -- they can be added\nor removed at any time.  The only thing you accomplish in such a situation\nis for programs to have to parse the contents of the header field in\norder to know whether or not it is applicable to them, which is a\nbad design.\n\nThe obvious way to handle a new digest algorithm like SHA is\n\n   Content-SHA: 2A1238912371239587\n\nwhich is exactly how the HTTP protocol is designed.  Leave it be.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3314268"}, {"subject": "RE: [ContentMD5 and Message Digest Authentication.], MD5 broken", "content": "Digest Auth already has the algorithm as a parameter. The name\n\"Content-MD5\" can't be changed for historical reasons.\n\n>----------\n>From: Roy T. Fielding[SMTP:fielding@avron.ICS.UCI.EDU]\n>Sent: Friday, April 26, 1996 4:08 PM\n>To: hallam@w3.org\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: [Content-MD5 and Message Digest Authentication.], MD5\n>broken. \n>\n>> Content-MD5: 2A1238912371239587; alg=SHA\n>> \n>> This construction is likely to break for obvious reasons.\n>\n>Phill, this has already been discussed to death.  There is no advantage\n>to using a generic parameter name for an Entity-Header -- they can be\n>added\n>or removed at any time.  The only thing you accomplish in such a\n>situation\n>is for programs to have to parse the contents of the header field in\n>order to know whether or not it is applicable to them, which is a\n>bad design.\n>\n>The obvious way to handle a new digest algorithm like SHA is\n>\n>   Content-SHA: 2A1238912371239587\n>\n>which is exactly how the HTTP protocol is designed.  Leave it be.\n>\n>\n> ...Roy T. Fielding\n>    Department of Information & Computer Science   \n>(fielding@ics.uci.edu)\n>    University of California, Irvine, CA 92717-3425   \n>fax:+1(714)824-4056\n>    http://www.ics.uci.edu/~fielding/\n>\n>\n\n\n\n", "id": "lists-010-3323043"}, {"subject": "Re: 8.12 TRACE and entit", "content": "> As I implement TRACE, I'm puzzled by this passage:\n> \n>                                           A TRACE request MUST NOT include an\n>       entity body and MUST include a Content-Length header field with a value\n>       of zero (0).\n> \n> Why must there be a Content-Length: 0 header?\n\nOverkill on my part, leftover from the prior definition.  It would be\nbetter as:\n\n    A TRACE request MUST NOT include an entity.\n\n> 1) Wouldn't the absence of a Content-Length header be equivalent?\n\nYes (combined with the absence of Transfer-Encoding) -- I found a bug\nin the spec (Section 7.2) related to that earlier this week.\nIt is going to take some time to get all the proposed changes\nwritten into something useful for Jim.\n\n> 2) If a server does not find a Content-Length header in the\n> request, is it obliged to return an error?\n\nNo.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3335185"}, {"subject": "Re: 5.1.2 RequestUR", "content": "> Okay, I'll accept your arguments.  However, there is still a piece I feel\n> is missing:\n> \n> 1) We want clients to send, and servers to accept, an absoluteURI.\n\nNo, we only want servers to accept an absoluteURI.  Clients can't\nsend it until 2.0.\n\n> 2) Eventually we want to obviate the need for Host:.\n\nNot in 1.x.  The stuff added to the third block of text in section 5.1.2\nis wrong -- it needs to be removed.\n\n> 3) The purpose of absoluteURI and/or Host is to specify the virtual host\n> for (at least) an http_URL.\n\nThat is only one of many things that absoluteURI is used for in the\nspecification.  In fact, it is probably the least important of them.\n\n> The reason I've been trying to restrict the absoluteURI syntax is that\n> then it becomes easy to identify the virtual host.  I believe somewhere\n> in the spec. there should be an explanation of how to identify the\n> virtual host of \"the resource identified by the request\".\n\nI believe Koen asked for such a section of text to be added to the\nsection on Requests.  However, that must not change the protocol syntax.\n\n(and it ain't gunna write itself)\n\n......Roy\n\n\n\n", "id": "lists-010-3343312"}, {"subject": "Re: [ContentMD5 and Message Digest Authentication.], MD5 broken", "content": "Roy writes\n\n>The obvious way to handle a new digest algorithm like SHA is\n>\n>   Content-SHA: 2A1238912371239587\n>\n>which is exactly how the HTTP protocol is designed.  Leave it be.\n\nThis means that each time the digest algorithm has to be changed \nwe should issue an RFC for a new version of HTTP. It would be nicer\nto be able to simply add an entry in the IANA registry.\n\nI disagree with Roy, I know of no other tag where the algorithm is \nembedded. We do not have Content-Text-HTML, we dont have \nContent-GZIPPED. I think that HTTP is designed in exactly the \nopposite manner. \n\nOne of the reasons why protocol specifications should not have \ndependencies on the security on particular algorithms is that \nit means that if the algorithm is compromised a few days before\nits due to be published it is not necessary to consider late\nchanges.\n\nFor the sake of getting the spec out the door, I'll agree with Roy \nprovided he agrees not to object to my proposals for Content-SHA,\nContent-RIPEMD and Content-MD6.\n\n\n\nPaul writes:\n\n>Digest Auth already has the algorithm as a parameter. The name\n>\"Content-MD5\" can't be changed for historical reasons.\n\n\nThe problem with digest auth that I hadn't anticipated is that as presently \nstated the spec means that if you change the keyed digest algorithm you also \nneed to exchange a separate shared secert.\n\nThis is bad practice cryptographically, (mea culpa). The shared secret \ngeneration step uses only the one way property of MD5. The authentication \nfunction uses the collision resistance funtion and the one-wayness. Its only the \ncollision resistance that is compromised, not the one-wayness. Ie refering to \nthe spec where we have\n\nDigest = H ( H(A1), nonce, H(a2))\n\nA1 = username : realm : password\na2 = Method : URI\n\nThe password file stores H(a1), so it is convenient to use a single digest \nfunction for all keyed digests ie we have\n\n\nDigest = SHA ( MD5(A1), nonce, SHA(a2))\n\nIts the SHA(a2) component that is really sensitive.\n\n\nPhill\n\n\n\n", "id": "lists-010-3351219"}, {"subject": "Re: [ContentMD5 and Message Digest Authentication.], MD5 broken", "content": ">>The obvious way to handle a new digest algorithm like SHA is\n>>\n>>   Content-SHA: 2A1238912371239587\n>>\n>>which is exactly how the HTTP protocol is designed.  Leave it be.\n> \n> This means that each time the digest algorithm has to be changed \n> we should issue an RFC for a new version of HTTP. It would be nicer\n> to be able to simply add an entry in the IANA registry.\n\nHow many times do I have to say this?  YOU DON'T NEED AN RFC TO ADD\nAN ENTITY-HEADER FIELD.  There is absolutely, unequivocally, no difference\nwhatsoever between adding a new parameter and adding a new Entity-Header\nfield.  That is how I designed the HTTP specification, because it is\nalways the most efficient way to handle entity metainformation, and the\nbest way to handle extensibility of the payload.  You don't even have\nto register it with IANA (though you may want to).\n\nCrikey. There's about 40 pages of gunk in the new spec which has never\nbeen reviewed by anyone outside the editorial group (and not enough by\nthem), and we are spending all our time arguing about TRIVIALITIES that\nwere settled over a year ago!  Hell, I already have 60 marked-up changes\nthat need to be made, and that's only in the first 40 pages.  We have\nmore than enough obvious problems to deal with already.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3361237"}, {"subject": "RE: [ContentMD5 and Message Digest Authentication.], MD5 broken", "content": ">----------\n>From: hallam@w3.org[SMTP:hallam@w3.org]\n>Sent: Friday, April 26, 1996 5:14 PM\n>\n>Paul writes:\n>\n>>Digest Auth already has the algorithm as a parameter. The name\n>>\"Content-MD5\" can't be changed for historical reasons.\n>\n>The problem with digest auth that I hadn't anticipated is that as\n>presently \n>stated the spec means that if you change the keyed digest algorithm you\n>also \n>need to exchange a separate shared secert.\n\nNot really. The shared secret is the password, not its hash. Giving\nH(A1) to a server is just a way a group of servers can be given the\npassword without needing to have them all have the password in\nplaintext. How they get it betwen themselves is outside the scope of the\nspec.\n>\n>This is bad practice cryptographically, (mea culpa). The shared secret \n>generation step uses only the one way property of MD5. The\n>authentication \n>function uses the collision resistance funtion and the one-wayness. Its\n>only the \n>collision resistance that is compromised, not the one-wayness. Ie\n>refering to \n>the spec where we have\n>\n>Digest = H ( H(A1), nonce, H(a2))\n>\n>A1 = username : realm : password\n>a2 = Method : URI\n>\n>The password file stores H(a1), so it is convenient to use a single\n>digest \n>function for all keyed digests ie we have\n>\n>\n>Digest = SHA ( MD5(A1), nonce, SHA(a2))\n\nAnd if we do nothing, it will be\n   Digest = SHA(SHA(A1), nonce, SHA(A2))\n\nDoesn't seem like a big deal. Servers that want to support both MD5 and\nSHA will have to have SHA(A1) and/or MD5(A1) entries in a table for each\nuser.\n\nIn your scheme, servers that only want to support SHA would have to have\nan implementation of MD5 available -- and they might not have a license\nfrom RSA DSI.\n>\n\n\n\n", "id": "lists-010-3370696"}, {"subject": "Re: Rewrite of 13.12 (Cache keys", "content": "The entire section 3.12 is wrong, and overspecification anyway, so\njust delete it.  Define the interface, not how its implemented!\n\nWhy it is wrong:\n\nThere is only one cache key and that is the Request-URI (plus Host\nif it is a server-side cache like Harvest).  No matter what the\nrequest profile may be, and how that may affect variant selection,\nthe key is always the Request-URI.  The key points to an entry which\nconsists of cache-relevant information about the resource and a list\nof pointers to the responses the cache has received from prior requests\non that Request-URI.  The cache then uses the current request profile,\ncompares it to the cache-relevant information, and decides whether or\nnot it can\n\n    a) just deliver one of the cached entities\n    b) make a conditional request on the next inbound server\n    c) make an unconditional request on the next inbound server\n\nThe decision cannot be made by simple key-comparison, so the request\nprofile is not considered a key.\n\nIf (b) or (c) returns a new entity, the cache MUST store it in the\nsame way as the request was made (i.e., it already knows the entry\nin the cache -- there is no replacement key).  The new entity replaces\none of the old entities if it has the same variant-id or content-location\nand was generated at a later Date.  If the new entity has a \ncontent-location, the cache must mark as stale all other Request-URI\nentries which refer to that content-location, but it is not a \ncache key (it is an entity key, internal to the cache DB operation).\n\nFor reasons of security, completeness, simplicity, and transparency,\nthe cache cannot serve a response received using one Request-URI to\na client requesting some other Request-URI, even when such would be\nindicated by transparent negotiation.  Simply obeying that principle\nremoves most of the complexity that Koen is talking about.\n\nIs this the only way to implement an HTTP cache?  No.\nIs it the best way?  Not sure.\nIs it something suitable for IETF standardization?  Not a chance.\n\nPut it in a book, or an Informational spec, but keep it out of the\ninterface specification.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3383136"}, {"subject": "SPOOF issue (Was: Re: Rewrite of 13.12 (Cache keys)", "content": "Roy T. Fielding:\n>\n[...]\n>\n>For reasons of security, completeness, simplicity, and transparency,\n>the cache cannot serve a response received using one Request-URI to\n>a client requesting some other Request-URI,  even when such would be\n>indicated by transparent negotiation.  Simply obeying that principle\n>removes most of the complexity that Koen is talking about.\n\nI think a mechanism by which the response for URI 1 influences what is\ncached for URI 2 is needed in the long run, to optimize caching for\ntransparent negotiation.  But I have argued strongly, in yesterday's\nphone conference, against the inclusion in the plain 1.1 spec of such\na mechanism.\n\nSome people in the phone conference yesterday wanted such a mechanism,\nthough they did not explain, as far as I could tell, why they wanted\nit, let alone why they wanted it in plain 1.1.  This mechanism is\ncertainly not needed as a content negotiation `hook'.\n\nI see I'm the issue owner for spoofing, so I'll now make an official\nstatement:\n\n There is _no consensus_ in the WG now, and there will likely be no\n consensus in the next few weeks, about the need to include, into\n plain 1.1, a mechanism by which the response for URI 1 can change or\n create data cached for URI 2.\n\n According to the guidelines for deciding what is in 1.1, this means\n that such a mechanism must _not_ be in the 1.1 document, and that the\n SPOOF issue should be labeled as (not 1.1).\n\n Consensus may appear, soon, about a mechanism by which the response\n for URI 1 can _make stale_ the data cached for URI 2.\n\nThis official statement is not just based on Roy and me being opposed\nto such a mechanism in 1.1 today, but also on my knowledge that other\nmembers of the wg, who are not in the editorial group, have objected\nto such mechanisms in the past, or have stated that anti-spoofing\nrules will need careful consideration.\n\nI strongly urge the people who want a spoofing mechanism in plain 1.1\nto supply to the WG reasons for having it.  If I hear nothing in the\nnext few days, I will close the SPOOF issue.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-3393325"}, {"subject": "Re: Rewrite of 13.12 (Cache keys", "content": "Roy T. Fielding:\n>\n>The entire section 3.12 is wrong, and overspecification anyway, so\n>just delete it. \n\nI agree, delete it.\n\nIt is an overspecification, and an overspecification of a wrong\nmechanism at that.  Though I was able to more or less fix 13.12.2 and\n13.12.3, I think 13.12.1 and 13.12.4 are beyond fixing. I noted\nearlier about 13.12.1:\n\n|     QUESTION: does the text below, or does it not, take both 200 and\n|     304 responses into account?  If so, how?  Is this in a part of\n|     Section 13 I have not read yet?\n\nI read the rest of section 13, and the 200/304 problem is not fixed\nthere.  The problem, by the way, is that the 13.12 text fails to\naddress the subtleties of 304 responses updating the headers in cached\n200 responses.\n\nI now conclude that 13.12 is beyond salvation, even after selecting\nopaque validators are removed.\n\n[Note for Roy: in yesterdays phone conference we decided to require\nquoted-string variant-IDs everywhere and to remove selecting opaque\nvalidators]\n\nIf 13.12 is removed, this leaves us with the Vary section, which\ndescribes cache lookups, and Section 13.20, which describes cache\nreplacement.  I mail will simplified text for 13.20 tomorrow (this\nsection can be simplified because variant-IDs are always required).\n\nI agree with Roy that the spec does not need to talk about entry keys\nat all: entry keys are an implementation mechanism invisible from a\nprotocol user viewpoint. \n\n> Define the interface, not how its implemented!\n>\n>Why it is wrong:\n\n[Good explanation of why it is wrong deleted]\n\n>  If the new entity has a \n>content-location, the cache must mark as stale all other Request-URI\n>entries which refer to that content-location, but it is not a \n>cache key (it is an entity key, internal to the cache DB operation).\n\nIs this requirement for marking as stale stated somewhere in the spec?\nI believe it is not.  I suggest it is added in a rewrite of section\n10.16 (Content-Location).  I will try to post such a rewrite tomorrow.\n\nKoen.\n\n\n\n", "id": "lists-010-3403794"}, {"subject": "Re: [ContentMD5 and Message Digest Authentication.], MD5 broken", "content": "hallam@w3.org writes:\n[...]\n > We can fix the problem by simply introducing an algorithm parameter. Ie:-\n > Content-Digest: 2A1238912371239587; alg=SHA\n > This change was strongly recommended by Ron Rivest, author of MD5.\n\nI don't want to whine with \"I said it... I said it...\" but\nFYI draft-demailly-cd-header-00.txt which suggested exactly that\nalmost 6 monthes ago expires in 15 days... just in time ;-)\n\n( http://www.lyot.obspm.fr/~dl/draft-contentdigest.txt )\n\nRegards\ndl\n\nps: I know the issued has been settled very democraticaly\nalready... but as the big chief says \"you don't need an rfc to add\nan entity-header field\", feel free to use Content-Digest in your\n(incompatible :-() implementations... \n\n\n\n", "id": "lists-010-3413437"}, {"subject": "EDITS for Section 5 (Request) (Was: Re: 5.1.2 RequestURI", "content": "[Note for Jim Gettys: please perform the edits indicated below as soon\nas you can.]\n\nRoy T. Fielding:\n>[Dave Kristol:]\n>> Okay, I'll accept your arguments.  However, there is still a piece I feel\n>> is missing:\n>> \n>> 1) We want clients to send, and servers to accept, an absoluteURI.\n>\n>No, we only want servers to accept an absoluteURI.  Clients can't\n>send it until 2.0.\n>\n>> 2) Eventually we want to obviate the need for Host:.\n>\n>Not in 1.x.  The stuff added to the third block of text in section 5.1.2\n>is wrong -- it needs to be removed.\n\nI include a corrected 5.1.2 below.\n\n>> 3) The purpose of absoluteURI and/or Host is to specify the virtual host\n>> for (at least) an http_URL.\n>\n>That is only one of many things that absoluteURI is used for in the\n>specification.  In fact, it is probably the least important of them.\n>\n>> The reason I've been trying to restrict the absoluteURI syntax is that\n>> then it becomes easy to identify the virtual host.  I believe somewhere\n>> in the spec. there should be an explanation of how to identify the\n>> virtual host of \"the resource identified by the request\".\n>\n>I believe Koen asked for such a section of text to be added to the\n>section on Requests.  However, that must not change the protocol syntax.\n>\n>(and it ain't gunna write itself)\n\nAlso included below.\n\n>......Roy\n\n------------\n\nCorrected 5.1.2, with computer-generated change bars:\n\n\n    5.1.2 Request-URI\n\n    The Request-URI is a Uniform Resource Identifier (Section 3.2) and\n    identifies the resource upon which to apply the request.\n\n           Request-URI    = \"*\" | absoluteURI | abs_path\n\n    To allow for transition to absoluteURIs in all requests in future\n|   versions of HTTP, all HTTP/1.1 servers MUST accept the absoluteURI\n|   form in requests, even though HTTP/1.1 clients will only generate\n|   them in requests to proxies.  Versions of HTTP after HTTP/1.1 may\n    require absoluteURIs everywhere, after HTTP/1.1 or later have become\n    the dominant implementations. The three options for Request-URI are\n    dependent on the nature of the request. The asterisk _*_ means that\n    the request does not apply to a particular resource, but to the\n    server itself, and is only allowed when the Method used does not\n    necessarily apply to a resource. One example would be\n\n           OPTIONS * HTTP/1.1\n\n|   [##first sentence deleted##] If the absoluteURI form is used, any\n    Host request-header included with the request MUST be ignored.  The\n    absoluteURI form is required when the request is being made to a\n    proxy. The proxy is requested to forward the request and return the\n|   response, or to serve an response from cache according to the rules\n|   for caching. Note that the proxy MAY forward the request on to\n    another proxy or directly to the server specified by the\n    absoluteURI. In order to avoid request loops, a proxy MUST be able\n    to recognize all of its server names, including any aliases, local\n    variations, and the numeric IP address. An example Request-Line\n    would be:\n\n           GET http://www.w3.org/pub/WWW/TheProject.html HTTP/1.1\n\n    The most common form of Request-URI is that used to identify a\n|   resource on an origin server or gateway. In this case, the absolute\n|   path of the URI MUST be transmitted (see Section 3.2.1,\n    abs_path). For example, a client wishing to retrieve the resource\n    above directly from the origin server would create a TCP connection\n    to port 80 of the host _www.w3.org_ and send the lines:\n\n           GET /pub/WWW/TheProject.html HTTP/1.1\n           Host:www.w3.org\n\n    followed by the remainder of the Full-Request. Note that the absolute\n    path cannot be empty; if none is present in the original URI, it MUST be\n    given as _/_ (the server root).\n\n    If a proxy receives a request without any path in the Request-URI and\n    the method used is capable of supporting the asterisk form of request,\n    then the last proxy on the request chain MUST forward the request with\n    _*_ as the final Request-URI. For example, the request\n\n           OPTIONS http://www.ics.uci.edu:8001 HTTP/1.1\n\n    would be forwarded by the proxy as\n\n           OPTIONS * HTTP/1.1\n\n    after connecting to port 8001 of host \"www.ics.uci.edu\".\n\n    The Request-URI is transmitted as an encoded string, where some\n    characters may be escaped using the _% HEX HEX_ encoding defined by\n    RFC 1738 [4]. The origin server MUST decode the Request-URI in order\n    to properly interpret the request.  In requests that they forward,\n    proxies MUST NOT rewrite the _abs_path_ part of a Request-URI in any\n    way except as noted above to replace a null abs_path with\n    _*_. Illegal Request-URIs SHOULD be responded to with an appropriate\n    status code.  (Proxies MAY transform the Request-URI for internal\n    processing purposes, but SHOULD NOT send such a transformed\n|   Request-URI in forwarded requests.  [##sentence deleted at this\n|   point##] The main reason for this rule is to make sure that the form\n    of Request-URIs is well specified, to enable future extensions\n    without fear that they will break in the face of some\n    rewritings. Another is that one consequence of rewriting the\n    Request-URI is that integrity or authentication checks by the server\n    may fail; since rewriting MUST be avoided in this case, it may as\n    well be proscribed in general.\n\n      Note: servers writers SHOULD be aware that some existing proxies\n      do some rewriting.\n\n\n-----------\n\nText for the new section 5.3:\n\n  5.3 The resource identified by a request\n\n  HTTP/1.1 origin servers MUST compute the URI of the resource\n  identified by a request in the following way.\n\n   1. If Request-URI is an absoluteURI, the URI of the identified\n      resource is the decoded absoluteURI.  Any Host header in the\n      request is ignored.\n\n   2. If the request-URI is an abs_path, and the request includes a\n      Host header field, the URI of the identified resource is\n\n        \"http://\" host-value decoded-abs_path\n\n      where host-value is the value in the host header field, and\n      decoded-abs_path the decoded abs_path value.\n\n   3. If the request-URI is an abs_path, and the request does not\n      include a Host header field (this may be the case in requests\n      from HTTP/1.0 clients), the server MUST either respond with an\n      error message, or take the URI of the identified resource to be\n\n        \"http://\" heuristic-host-value decoded-abs_path\n\n      where some heuristic is used to compute the\n      heuristic-host-value.  If the network address of a HTTP/1.1\n      server is bound to only one Internet host domain name, the\n      server can always use that Internet host domain name for the\n      heuristic-host-value.\n\n\n--------------------------\n\nKoen.\n\n\n\n", "id": "lists-010-3421986"}, {"subject": "EDITS for Section 10.16 (ContentLocation", "content": "Here is new text for the Content-Location section.  The change-bars\nare computer-generated.  These changes reflect the removal of spoofing\nmechanisms from the spec, and also fix some terminology.  To Jim\nGettys: please perform the edits indicated below as soon as you can.\n\nKoen.\n\n--snip--\n\n   10.16 Content-Location\n\n   The Content-Location entity-header field is used to define the\n   location of the specific resource associated with the entity enclosed\n   in the message. A server SHOULD provide a Content-Location if, when\n|  including an entity in response to a GET request on a varying\n|  resource, the entity corresponds to a specific location which can be\n   accessed via the Content-Location URI. A server SHOULD provide a\n   Content-Location with any 200 (OK) response which was internally (not\n   visible to the client) redirected to a resource other than the one\n   identified by the request and for which correct interpretation of\n   that resource may require knowledge of its actual\n|  location. [##sentence deleted here##]\n\n           Content-Location = \"Content-Location\" \":\" absoluteURI\n\n   If no Content-Base header field is present, the value of Content-\n   Location also defines the base URL for the entity (see Section 10.9).\n\n|  Under this specification, the response to a request on one URI can\n|  never create or update information cached for another URI.  For\n|  security reasons, when storing the response in cache memory, caches\n|  MUST use the resource URI indicated by the request, and MUST NOT use\n|  the URI in the Content-Location header field.\n\n|  XXX The paragraph below needs to be reviewed: there may not be\n|  consensus about including it.\n\n|  If cache memory contains an existing entry for the resource indicated\n|  by the Content-Location header, and if this indicated resource is\n|  located on the same origin server and is not a varying resource, the\n|  cache SHOULD compare any cache validators in the current response\n|  with the cache validators of the entry.  If the validators do not\n|  match, and if the age of the response is less than the age of the\n|  entry, then the cache SHOULD remove or make stale the entry.\n\n\n\n", "id": "lists-010-3436045"}, {"subject": "EDITS for Section 13.20 (Cache Replacement for Varying Resources", "content": "Here is new text for the cache replacement section.  The change-bars\nare computer-generated.  These changes reflect the removal of opaque\nselecting validators from the spec.  \n\nTo Jim Gettys: please perform the edits indicated below as soon as you\ncan.  Also, delete Section 13.12 (cache keys), except for sub-section\n13.12.4 (Canonicalization of URIs), which should be merged with\nSection 3.2.2 (http URL).\n\nKoen.\n\n--snip--\n\n\n   13.20 Cache Replacement for Varying Resources\n\n   If a new 200 (OK) response is received from a non-varying resource\n   while an old 200 (OK) response is cached, caches can delete this old\n   response from cache memory and insert the new response.  For 200 (OK)\n|  responses from varying resources (Section 10.vary), cache replacement\n   is more complex.\n\n   HTTP/1.1 allows the authors of varying resources to guide cache\n|  replacement by the inclusion of variant-IDs in the responses of these\n|  resources. [##lots of complexity deleted here##] If a cache has\n|  stored in memory a 200 (OK) response with a certain variant-ID, and\n   receives, from the same resource, a new 200 (OK) response which has\n|  the same variant-ID, this should be interpreted as a signal from the\n   resource author that the old response can be deleted from cache\n   memory and replaced by the new response.\n\n|  The variant-ID mechanism cannot cause deletion from cache memory of old\n|  responses with variant-IDs that will no longer be used.  It is expected\n   that the normal 'least recently used' update heuristics employed by\n   caches will eventually cause such old responses to be deleted.\n\n|  All responses from varying resources SHOULD include variant-IDs.  If\n|  these are not present, the resource author can expect caches to\n|  correctly handle requests on the varying resource, but cannot expect\n|  the caching to be efficient.\n\n\n\n", "id": "lists-010-3445612"}, {"subject": "Re: EDITS for Section 10.16 (ContentLocation", "content": "Koen Holtman:\n> Here is new text for the Content-Location section.  The change-bars\n> are computer-generated.  These changes reflect the removal of spoofing\n> mechanisms from the spec, and also fix some terminology.  To Jim\n> Gettys: please perform the edits indicated below as soon as you can.\n> \n> --snip--\n> \n>    10.16 Content-Location\n> \n>    The Content-Location entity-header field is used to define the\n>    location of the specific resource associated with the entity enclosed\n>    in the message. A server SHOULD provide a Content-Location if, when\n> |  including an entity in response to a GET request on a varying\n> |  resource, the entity corresponds to a specific location which can be\n>    accessed via the Content-Location URI. A server SHOULD provide a\n>    Content-Location with any 200 (OK) response which was internally (not\n>    visible to the client) redirected to a resource other than the one\n>    identified by the request and for which correct interpretation of\n>    that resource may require knowledge of its actual\n> |  location. [##sentence deleted here##]\nThis may cause the false sense of Content-Location applicable only to \nvarying resources, however there are good reasons to use it for simple URIs:\nURIs\nhttp://src.doc.ic.ac.uk/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://sunsite.doc.ic.ac.uk/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://phoenix.doc.ic.ac.uk/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://146.169.43.1/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://155.198.191.4/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://193.63.255.1/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://146.169.2.10/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://146.169.16.11/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://146.169.17.5/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://146.169.32.5/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://146.169.33.5/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nhttp://155.198.1.40/computing/internet/internet-drafts/draft-ietf-http-v11-spec-02.txt\nidentify the same resource. (Its worth mentioning that internet-drafts have a\nlot of URIs on different servers, but that problem will be addressed by\nURNs hopely in the near future.)\nI guess the official name of the http server is 'src.doc.ic.ac.uk', and clients\n(and caches) should use that name, but we cant be sure, unless the server says\nthat (using Content-Location).\nStoring under 12 keys the same resource isn't economic, we need a possibility\nto spare that disk space!\nFor other protocols the 12 variants of the internet host reference is the same,\nbut for http (since Host:) may have different meaning.\nHow should a client determine that the server in question deesn't hosts \nvirtuals servers?\n(Entity header comparison for every URL isn't enough effective,\nbut works.) Perhaps we should add some header declaring that the server does\nhost virtual servers?\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n", "id": "lists-010-3454491"}, {"subject": "I-D ACTION:draft-ietf-http-state-mgmt01.txt, .p", "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : Proposed HTTP State Management Mechanism                \n       Author(s) : D. Kristol, L. Montulli\n       Filename  : draft-ietf-http-state-mgmt-01.txt, .ps\n       Pages     : 18\n       Date      : 04/26/1996\n\nThis proposal specifies a way to create a stateful session with HTTP \nrequests and responses.  It describes two new headers, Cookie and \nSet-Cookie, which carry state information between participating origin \nservers and user agents.  The method described here differs from Netscape's\nCookie proposal, but it can interoperate with HTTP/1.0 user agents that use\nNetscape's method.  (See the HISTORICAL section.)                          \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-state-mgmt-01.txt\".\n Or \n     \"get draft-ietf-http-state-mgmt-01.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-mgmt-01.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-state-mgmt-01.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-state-mgmt-01.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-3466300"}, {"subject": "EDIT for sec 14.", "content": "      14.1 Purpose\n      HTTP's greatest strength and its greatest weakness has been its\n      simplicity.  Prior to persistent connections, a separate TCP connection\n      was established to fetch each URL, increasing the load on HTTP servers,\n      and causing congestion on the Internet.   The use of inline images and\n      other associated data often requires a client to make multiple requests\n      of the same server in a short amount of time.   An excellent analysis of\n      these performance problems is available [2]; analysis and results from a\n      prototype implementation are in [32, 33].\n\n\nThe last two references should be [33, 34].  I believe that \nreference [2] is also incorrect but I am not certain what it \nshould be.  Reference [2] is the gopher protocol.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-3476719"}, {"subject": "RE: [ContentMD5 and Message Digest Authentication.], MD5 broken", "content": "On Fri, 26 Apr 1996, Paul Leach wrote:\n\n> \n> \n> >From: hallam@w3.org[SMTP:hallam@w3.org]\n> >\n\n> >The problem with digest auth that I hadn't anticipated is that as\n> >presently \n> >stated the spec means that if you change the keyed digest algorithm you\n> >also \n> >need to exchange a separate shared secert.\n> \n\nIn light of the problem raised by Phill I suggest the following.\nWe had already decides to try to \"dock\" the digest auth document with\nHTTP/1.0.  I think that a reasonable thing to do now would be to \nseparate the digest auth spec into two parts, only one of which would\ndock with HTTP/1.1.  Unless I am mistaken the only part parts affected by the\nMD5 weakness is the optional \"digest\" field of the the response header\nand the optional \"digest\" field of the AuthenticationInfo header. \nI think we should \"fire the explosive bolts\" on this part making it\na separate option extension described in a separate document (and \nemphasing the use of a different algorithm).  Meanwhile the authentication\nrole of digest, i.e. its use as a replacement for basic would still\nbe intact and still secure.\n\nThoughts?\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-3484324"}, {"subject": "HTTP 1.1 document terminology", "content": "As you all know, we've had terminology problems that has caused much\nanguish, where different people have had slightly different views of\nwhat the same term means (and those people then ended up generating\nnew terminology to compensate).  We only recently figured out this \ndisconnect was the cause of much of the anguish of the last month.\nThe terms \"resource\", \"variant\", \"entity\", \"entity instance\" were being\nused in sloppy ways (and as there are only three things to be described,\nthe fact we were using four is a fundamental sign of the problem).\nIt would help the discussions to try to be precise in terminology.\n\nRoy Fielding took a stab at a set of terminology and definitions to help\nthis situation.  I've also borrowed from some terminology Tim Berners-Lee\nhad put together.  What is below is derived mostly from either one or \nthe other, or from previous usage in the draft, where it had not been\nconfused.  Note the introduction of resource entity; when working\nthrough the spec, it became clear that Roy was correct on needing that term\nas well.\n\nAfter quite a bit of anguish on several people's part, I think we're\nconverging.\n\n\nThere are two areas which aren't quite settled to my complete satisfaction.\n\n1) Roy has been pushing for \"entity ID\" or \"entity\nidentifier\" to replace opaque validators in the document; \nhaving been wandering through the document wrestling with\nthese terminology issues, I've gotten more than a glimmer of what he's\ntalking about, and now agree that a term independent of validator is\nin fact correct, and probably necessary to avoid confusion in the long\nrun.  The problem that ID is confusing too many people.  This doesn't\nmean the term \"entity ID\" is good, or bad, or indifferent, just that\ntoo many people have too many preconceptions about what identifier\nmeans.  It was suggested was that we use \"tag\" for this term.  This\nraises a problem: the document already talks about language tags, and\nthis term is bound in other RFC's not under our control.  \nI don't think it wise to try to rename \"language tag\" to something \nelse, due to this other use in IETF terminology.  \nUnless someone can come up with a better term (fast) I\nplan to use the term \"entity tag\" in the document, rather than just\ntag.  IfValid, and IfInvalid become IfMatch and IfNoMatch, and so\nforth.\n\n2) I like Tim's term of \"generic resource\" for a resource that\nyou can do content negotiation against (and I incorporated some words\nof his into the terminology section below), rather than Roy's\nsuggestion of group resource.  Tim uses \"specific resource\" for a\nresource that has only a single entity associated with it; I'm less\nhappy about that.  Roy's suggestion is \"individual resource\", with\nwhich I'm also luke warm.  I'll go with \"specific resource\" unless\nsomeone argues me out of it or suggests a better term.  Similarly, if\nsomeone has a bit of blinding insight for a better term, please speak\nup now.\n\nAs editor, it is my responsiblity to see that the resulting document\nreflects a single uniform set of terminology that won't confuse too\nmany people.  Any suggestions on the above two points (and if the\ndefn's below have problems), would be appreciated.  I will attempt to\nmake the next draft conform to this terminology.\n\n- Jim Gettys\n\n\n\n\n\n1.3 Terminology\n===============\nThis specification uses a number of terms to refer to the roles played\nby participants in, and objects of, the HTTP communication.\n\nconnection\n----------\nA transport layer virtual circuit established between two application\nprograms for the purpose of communication.\n\nmessage\n-------\nThe basic unit of HTTP communication, consisting of a structured\nsequence of octets matching the syntax defined in section 8 and\ntransmitted via the connection.\n\nrequest\n-------\nAn HTTP request message (as defined in section 9).\n\nresponse\n--------\nAn HTTP response message (as defined in section 10).\n\nresource\n--------\nA network data object or service that can be identified by a URI\n(section 7.2). A \"resource\" is a concept (a little like a\nPlatonic ideal).  When represented electronically, a resource may be\nof the kind which corresponds to only one possible bit stream\nrepresentation.  An example is the text version of an Internet\nRFC. That never changes. It will always has the same checksum.\n\ngeneric resource\n----------------\nOn the other hand, a resource may be generic in that as a concept it\nis well specified but not so specifically specified that it can only\nbe represented by a single bit stream.  In this case, other URIs may\nexist which identify a resource more specifically. These other URIs\nidentify resources too, and there is a relationship of genericity\nbetween the generic and the relatively specific resource.  As an\nexample, successively specific resources might be\n\n1. The Bible\n2. The Bible, King James Version\n3. The Bible, KJV, in English\n4. A particular ASCII rendering of the KJV Bible in English\n\nEach resource may have a URI.  The authority which allocates the URI\nis the authority which determines to what it refers: Therefore, that\nauthority determines to what extent that resource is generic or\nspecific.  When we discuss electronic resources, an interesting fact\nis that a small number of dimensions of genericity emerge.\n\nTime\nA resource may vary with time. For example, \"The Wall Street Journal\"\nvaries with time. Each issue is a time-specific resource, which does\nnot change with time. Most home pages on the Web change with time, in\na less periodic way.\n\nLanguage\nWhen a document is translated, it is useful to be able to refer to it\neither in the generic, or to a particular specific translation.\n\nRepresentation\nA given resource may have many ways in which it can be represented on\nthe wire, using different Content-Types (section 18.19).  As an\nexample, an image may be represented in PNG or GIF format.\n\nspecific resource\n-----------------\nA resource that is not a generic resource.\n\nvariant\n-------\nAn specific resource that is a member of at least one generic\nresource.  Sometimes called a resource variant.  Note that the set of\nvariants of a generic resource may change over time as well.\n\nentity\n------\nThe set of information transferred as the payload of a request or\nresponse.  An entity consists of metainformation in the form of\nEntity-Header fields and content in the form of an Entity-Body, as\ndescribed in section 11 \n\nresource entity\n---------------\nA particular representation, rendition,\nencoding, or presentation of a resource at a particular point in time.\nResources not supporting content negotiation are bound to a single\nentity.  Generic resources supporting content negotiation are bound to\na set of one or more entities, whose membership may vary over time.\n\ncontent negotiation\n-------------------\nThe mechanism for selecting the appropriate variant of a generic\nresource when applying a request, is described in section 15.\n\nentity tag\n----------\n\nAn identifier for an entity.  An identifier for a particular entity is\ncalled a \"strong entity tag.\"  An identifier for an equivalent set of\nentities is called a \"weak entity tag.\"\n\nclient\n------\nAn application program that establishes connections for the purpose of\nsending requests.\n\nuser agent\n----------\nThe client which initiates a request. These are often browsers,\neditors, spiders (web-traversing robots), or other end user tools.\n\nserver\n------\nAn application program that accepts connections in order to service\nrequests by sending back responses. Any given program MAY be capable\nof being both a client and a server; our use of these terms refers\nonly to the role being performed by the program for a particular\nconnection, rather than to the program's capabilities in\ngeneral. Likewise, any server MAY act as an origin server, proxy,\ngateway, or tunnel, switching behavior based on the nature of each\nrequest.\n\norigin server\n-------------\nThe server on which a given resource resides or is to be created.\n\nproxy\n-----\nAn intermediary program which acts as both a server and a client for\nthe purpose of making requests on behalf of other clients. Requests\nare serviced internally or by passing them, with possible translation,\non to other servers. A proxy MUST interpret and, if necessary, rewrite\na request message before forwarding it. Proxies are often used as\nclient-side portals through network firewalls and as helper\napplications for handling requests via protocols not implemented by\nthe user agent.\n\ngateway\n-------\nA server which acts as an intermediary for some other server. Unlike a\nproxy, a gateway receives requests as if it were the origin server for\nthe requested resource; the requesting client may not be aware that it\nis communicating with a gateway. Gateways are often used as\nserver-side portals through network firewalls and as protocol\ntranslators for access to resources stored on non-HTTP systems.\n\ntunnel\n------\nA tunnel is an intermediary program which is acting as a blind relay\nbetween two connections. Once active, a tunnel is not considered a\nparty to the HTTP communication, though the tunnel may have been\ninitiated by an HTTP request. The tunnel ceases to exist when both\nends of the relayed connections are closed. Tunnels are used when a\nportal is necessary and the intermediary cannot, or should not,\ninterpret the relayed communication.\n\ncache\n-----\nA program's local store of response messages and the subsystem that\ncontrols its message storage, retrieval, and deletion. A cache stores\ncachable responses in order to reduce the response time and network\nbandwidth consumption on future, equivalent requests. Any client or\nserver MAY include a cache, though a cache cannot be used by a server\nwhile it is acting as a tunnel.\n\nfirsthand\n---------\nA response is firsthand if it comes directly and without unnecessary\ndelay from the origin server, perhaps via one or more proxies.  A\nresponse is also firsthand if its validity has just been checked\ndirectly with the origin server.\n\nexplicit expiration time\n------------------------\nThe time at which the origin server intends that an entity should no\nlonger be returned by a cache without further validation.\n\nheuristic expiration time\n-------------------------\nAn expiration time assigned by a cache when no explicit expiration time \nis available.\n\nage\n---\nThe age of a response is the time since it was generated by, or\nsuccessfully validated with, the origin server.\n\nfreshness lifetime\n------------------\nThe length of time between the generation of a response and its\nexpiration time.\n\n\nfresh\n-----\nA response is fresh if its age has not yet reached its freshness lifetime.\n\nstale\n-----\nA response is stale if its age has passed its freshness lifetime.\n\nvalidator\n---------\nA tag or a Last-Modified date and time used to validate whether a cache \nentry is \"fresh\" or \"stale.\"\n\n\n\n", "id": "lists-010-3496112"}, {"subject": "EDIT for section 13.3.", "content": "I suggest the the following paragraph from 13.3.4 \n\n        Note: in order to provide semantically transparent caching, an\n        origin server should avoid reusing a specific strong opaque\n        validator value for two different instances of an entity, or\n        reusing a specific weak opaque validator value for two\n        semantically different instances of an entity.  Caches entries\n        may persist for arbitrarily long periods, regardless of\n        expiration times, so it may be inappropriate to expect that a\n        cache will never again attempt to validate an entry using a\n        validator that it obtained at some point in the past.\n\nbe replaced with\n\n   Note: in order to provide semantically transparent caching, an origin\n   server should not reuse a strong opaque validator value for an entity\n   corresponding to the same resource (URI) as the entity for which the\n   validator value was first used, even if the reuse would occur after\n   the expiration time of the original entity.  A server should not reuse\n   a weak opaque validator value for a semantically different entity\n   corresponding to the same resource as the entity for which it was\n   first used.  Caches' entries may persist for arbitrarily long periods,\n   regardless of expiration times, so it may be inappropriate to expect\n   that a cache will never again attempt to validate an entry using a\n   validator that it obtained at some point in the past.\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-3514584"}, {"subject": "10.12 ContentLengt", "content": "A\n      valid Content-Length field value is required on all HTTP/1.1 request\n      messages containing an entity body.\n\nNot so if the entity body uses Transfer-Encoding: chunked.\n\nPerhaps we should say:\nHTTP/1.1 request messages that contain an entity body MUST contain\neither a valid Content-Length field value or a Transfer-Encoding: chunked\nentity header and a chunked entity body.\n\nDave Kristol\n\n\n\n", "id": "lists-010-3523044"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "On Mon, 29 Apr 1996 jg@w3.org wrote:\n\n> \n> entity tag\n> ----------\n> \n> An identifier for an entity.  An identifier for a particular entity is\n> called a \"strong entity tag.\"  An identifier for an equivalent set of\n> entities is called a \"weak entity tag.\"\n> \n\n\nWould you consider following substitute.  It is much more detailed,\nbut this was a contentious issue.  While the name and precise wording\nis not so important I think it is a good idea to have this level of\nspecificity:\n\n\n\n  entity tag\n  ----------\n \n  A string associated with an entity and used to distinguish it from\n  other entities which have the same URI.  The class of entities being\n  distinguished may include entities which only existed in the past or\n  which will exist in the future, but they all have, have had, or will\n  have, the same URI.  A \"strong entity tag\" is an entity tag which can\n  only be shared by two entities if they are equivalent by octet\n  equality.  A\"weak entity tag\" is an entity tag which can be shared\n  by two entities if they are semantically equivalent.  An entity tag is\n  not intended to distinguish entities with different URI's as they are\n  distinguished by their respective URI's.\n \n\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-3529521"}, {"subject": "10.22 Hos", "content": "10.22 says:\n\nAll Internet-based HTTP/1.1 servers MUST respond with a\n      400 status code to any HTTP/1.1 request message which lacks a Host\n      header field.\n\nThis conflicts with 5.1.2:\n\n                                       If the absoluteURI form is\n      used, any Host request-header included with the request MUST be ignored.\n\nI think 5.1.2 has it right, in which case 10.22 should read:\n\nAll Internet-based HTTP/1.1 servers MUST respond with a\n      400 status code to any HTTP/1.1 request message that lacks both an\n      absoluteURI in the request line and a Host header field.\n\nDave Kristol\n\n\n\n", "id": "lists-010-3538696"}, {"subject": "Re: [ContentMD5 and Message Digest Authentication.], MD5 broken", "content": ">Not really. The shared secret is the password, not its hash. Giving\n>H(A1) to a server is just a way a group of servers can be given the\n>password without needing to have them all have the password in\n>plaintext. How they get it betwen themselves is outside the scope of the\n>spec.\n\nActually I was pretty keen on the shared secret being the hash. The idea being \nthat the server need not ever know the password itself. This would be secure \nenough for many applications.\n\nThe additional hassle probably isn't worthwhile at this stage.\n\n\n>In your scheme, servers that only want to support SHA would have to have\n>an implementation of MD5 available -- and they might not have a license\n>from RSA DSI.\n\nActually the license terms are merely that you call it RSA-MD5 and tell pe0op,e \nthat you use it, and those are only if you use Ron's code.\n\n\nO.K. so things look pretty much alright provided we put in a note to mention \nthat SHA is preferred over MD5.\n\n\nPhill\n\n\n\n", "id": "lists-010-3546087"}, {"subject": "Re: HTTP 1.1 document terminology (suggestions", "content": "On Mon, 29 Apr 1996 jg@w3.org wrote:\n\n> \n> As you all know, we've had terminology problems that has caused much\n> anguish, where different people have had slightly different views of\n> what the same term means (and those people then ended up generating\n> new terminology to compensate).  We only recently figured out this \n> disconnect was the cause of much of the anguish of the last month.\n> The terms \"resource\", \"variant\", \"entity\", \"entity instance\" were being\n> used in sloppy ways (and as there are only three things to be described,\n> the fact we were using four is a fundamental sign of the problem).\n> It would help the discussions to try to be precise in terminology.\n> \n> \n\n\nJim, I think you have made a great deal of progress on this important\ntask.  Just how important it is has become clear because of the \ndifficulties we have had.\n\nI have a few suggestions for your consideration.  Things marked with\n'>' a the beginning of a line have not been changed, i.e. the \"entity\"\ndefinition is unchanged but I would suggest moving it ahead of the other\ndefinitions.  Comments or questions are marked with \"###\".\n\n\n\n> entity\n> ------\n> The set of information transferred as the payload of a request or\n> response.  An entity consists of metainformation in the form of\n> Entity-Header fields and content in the form of an Entity-Body, as\n> described in section 11 \n\n\nresource\n-------- \n\nA network data object or service that can be identified by a URI\n(section 7.2). A \"resource\" is a concept (a little like a Platonic\nideal).  In general a resource may correspond to a single entity or to\nmany entities.  These entities could exist simultaneously, or\nserially, or both.  The resource may include entities which will only\nexist in the future.  The defining characteristic that unites these\nentities in a resource is the sharing of a common URI.\n\n\nspecific resource\n----------------- \n\nA resource may be of the kind which corresponds to only one possible\noctet stream representation.  An example is the text version of an\nInternet RFC. That never changes. It will always have the same\nchecksum.  Such a resource corresponds to only one entity and is\ncalled a specific resource.\n\n### Question: Is this what a specific resource should be, or should it\n### be something like a homepage which only has one representation at\n### a time?  If the definition above is correct then specific\n### resources are rather rare.  We might want to use specific resource\n### to mean a resource that has only one representation at a time.  It\n### could be called a \"serial resource.\"\n\n\ngeneric resource\n----------------\n\nOn the other hand, a resource may be generic in that as a concept it\nis well specified but not so specifically specified that it can only\nbe represented by a single octet stream.  Such a resource corresponds\nto multiple entities or potential entities.  Other URIs may or may\nnot exist which identify a resource more specifically. These other\nURIs identify resources too, and there is a relationship of genericity\nbetween the generic and the relatively specific resource.  As an\nexample, successively specific resources might be\n \n> 1. The Bible\n> 2. The Bible, King James Version\n> 3. The Bible, KJV, in English\n> 4. A particular ASCII rendering of the KJV Bible in English\n> \n> [continue as in original]\n\n...\n\n\n> \n> variant\n> -------\n> A specific resource that is a member of at least one generic\n> resource.  Sometimes called a resource variant.  Note that the set of\n> variants of a generic resource may change over time as well.\n> \n\n\n### There are two reasons calling a variant a \"specific resource\"\n### seems problematic to me.  1) Every resource should have a URI, but\n### a variant may not (unless I misunderstand) e.g. a resource may\n### have a French and English version determined by content\n### negotiation.  These are variants, but there need not be a URI for\n### just the French version.  2) A variant corresponds to something\n### like \"the French version\" so it can change with time.  It would\n### make sense for a variant to be a \"serial resource,\" but not a\n### specific resource.\n\n> \n> resource entity\n> ---------------\n> A particular representation, rendition,\n> encoding, or presentation of a resource at a particular point in time.\n> Resources not supporting content negotiation are bound to a single\n> entity.  Generic resources supporting content negotiation are bound to\n> a set of one or more entities, whose membership may vary over time.\n> \n\n### I don't understand why this term is needed.  Is there such a thing\n### as an entity which is not a resource entity?\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-3554414"}, {"subject": "11. Access Authenticatio", "content": "Suppose I have a URL space that I want to protect.  I would prefer to\nuse Digest authentication if the user-agent understands it.  Otherwise\nI can use Basic.  Is it meaningful (and correct) to send:\n\nWWW-Authenticate:  Digest realm=\"foo\", nonce=\"xxx\", opaque=\"yyy\",\n   Basic realm=\"foo\"\n\n(Yes, same realm name, although I suppose I could tolerate different\nnames.)\n\nIs it valid to ask for authentication with more than one set of\ncredentials for the same scheme, e.g.,\nWWW-Authenticate: Basic realm=\"foo\",\n  Basic realm=\"bar\"\n\nIf these are reasonable headers, then I think 10.44 WWW-Authenticate\nshould stipulate something about the order of credentials in\nWWW-Authenticate, such as that they are in the order of preference from\nthe origin server.  (If the headers are unreasonable, then the grammar\nfor 10.44, 1#challenge, is wrong.)\n\nDave Kristol\n\n\n\n", "id": "lists-010-3567420"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "jg@w3.org:\n>\n[....]\n>        2) I like Tim's term of \"generic resource\" for a resource that\n>you can do content negotiation against (and I incorporated some words\n>of his into the terminology section below), rather than Roy's\n>suggestion of group resource.\n\nWhile the terms `generic/specific resource' would work, the words that\nexplain them are totally inappropriate for the 1.1 spec.  I guess you\ntook them from a research-like paper, where Tim explored a continuum\nbetween totally specific and totally generic resources.  Such a\ncontinuum does _not_ exist for 1.1 resources.  Generic or specific is\na binary property for 1.1 resources, and a property that is orthogonal\nto variance through time.  In summary, Tim's words would totally\nconfuse the reader.  The edits suggested by John Franks improve the\ntext somewhat, but even with these edits it stays confusing.\n\nAlso, sentences like `(a little like a Platonic ideal)' and `When we\ndiscuss electronic resources, an interesting fact is that a small\nnumber of dimensions of genericity emerge.' do not belong in a\nprotocol specification.\n\n[...]\n>   I'll go with \"specific resource\" unless\n>someone argues me out of it or suggests a better term.  Similarly, if\n>someone has a bit of blinding insight for a better term, please speak\n>up now.\n\nI am using \"plain resource\" to denote \"non-generic resource\" in my\ndraft of the next content negotiation internet draft.\n\nUsing \"plain resource\" has two advantages:\n\n  - \"plain\" is short\n  - \"plain\" implies things like `the simple case', `the normal case',\n    `the thing we started out with (the kind provided by 1.0 servers)',\n    `the case without the special rules', which are all implications\n    we want to make.\n\nIs this blinding enough?\n\n\n>variant\n>-------\n>An specific resource that is a member of at least one generic\n>resource.  Sometimes called a resource variant.  Note that the set of\n>variants of a generic resource may change over time as well.\n\nThis `variant' is not the kind of variant referred to in the 1.1\ncaching section.  There, a variant is an entity bound to a varying\nresource as I defined it in the Vary header section.  This kind of\nvariant that is identified by a tuple (URI,variant-ID), it does not\n(necessarily) have its own URI: this variant is _not_ a resource\n(unless a resource is something which is not always identified by a\nURI).  The truth about the relation between variants and resources is\ncomplicated, and is revealed with gory graphical detail in the text\nincluded below.\n\nI have been working on the next version of the content negotiation\ninternet draft.  I hope I will get it finished before the web\nconference, but cannot guarantee that I will, as contributing fixes to\nthe 1.1 draft is higher on my priority list.\n\nI include some parts of my draft of the content negotiation\nterminology/overview section below.  Read it for the terminology\nexplanations, not for the terms themselves, I plan to adjust them to\nthe 1.1 terminology once this terminology is known.  Feel free to\nsteal text you can use.  You do not need to steal from 3.4 below: 1.1\ndoes not need to distinguish between opaque and transparent resources,\nand 1.1 can ignore the extra level of indirection between a\ntransparent resource and its variants.\n\nA table:\n\n 1.1-02 text          New 1.1 terminology  Terminology in my text below\n\n   resource             resource             resource \n   varying resource     generic resource     varying resource\n   non-varying res.     specific resource    plain resource\n   variant              resource entity      variant \n     <not mentioned>    variant              alternate\n   entity               entity               entity\n   transparent c. n.    content negotiation  transparent c.n.\n   opaque c.n.          <not taken into      opaque c.n.\n                         account?>\n\nKoen.\n\n-------snip--------\n\n[Note: this text has not been proof-read by me yet!]\n\n\n3.2 Variants and entities\n\n   variant\n\n        A particular representation, rendition, encoding, or\n        presentation of a piece of information.  Varying resources have\n        multiple variants associated with them.  Plain resources have\n        only one variant.  A variant is a container of data.  The\n        contents of the variant may be updated over time.\n\n   entity\n\n        A piece of data, usually representing the contents of a variant\n        at a given point in time, transferred in a HTTP transaction.  An\n        entity is transferred as metainformation in the form of entity\n        headers and content in the form of an entity body.\n\n\n3.3 Resources and negotiation\n\n   resource\n\n        A network information repository or service, residing on an\n        origin server, that can be identified by a URI.  In this\n        specification, resources are active parties in the\n        communication.  Our sentence `the resource X returns Y' would be\n        `if a request is made on the resource X, the server returns Y'\n        in the terminology of [1].\n\n   This specification divides all resources into classes as follows:\n\n                  -- plain resources\n                /\n               /\n     resources\n               \\                         -- opaque resources\n                \\                      /\n                  -- varying resources \n                                       \\\n                                         -- transparent resources\n\n\n   Opaque and transparent resources are collectively called varying\n   resources.  At any given moment in time, a resource is either a plain\n   resource, an opaque resource, or a transparent resource.  Resources\n   can change their class, but it is not expected that this will occur\n   often.\n\n   HTTP/1.1 [1] defines the distinction between plain and varying\n   resources. This specification defines the additional distinction\n   between opaque and transparent resources.  HTTP/1.1 defines minimal\n   caching behavior for all varying resources.  This specification\n   defines additional, optional, caching behavior for transparent\n   resources.\n\n   plain resource\n\n        A resource which is not a varying resource.  A plain resource\n        only has one variant associated with it, and will return either\n        this variant, or an error message for every request.  Resources\n        which have no associated variants, but instead return\n        redirection (301, 302, 303) type responses, are also called\n        plain resources.  Plain resources are called non-varying\n        resources in [1].  Traditional HTTP/1.0 servers only provide\n        plain resources.\n\n   varying resource\n\n        A varying resource has multiple variants associated with it, all\n        of which are representations of the content of the resource.  If\n        a GET or HEAD request on a varying resource is received, the\n        server will try to select one variant as the one best matching\n        the request.  This selection process is called content\n        negotiation.  If a resource is varying, this has an important\n        effect on cache management, particularly for caching proxies\n        which service a diverse set of user agents.\n\n   negotiated resource\n\n        synonym for varying resource\n\n   content negotiation\n\n        The process by which the best variant is selected when a GET or\n        HEAD request is made on a varying resource.  Selection is based\n        on a matching of the properties of the variants to the\n        capabilities of the requesting user agent.  Content negotiation\n        may not always succeed in finding the best variant, for example\n        because none of the variants are acceptable to the user agent.\n        In this case, a 3xx (redirection) or 4xx (client error) type\n        response can be generated.  \n\n   There are two types of content negotiation: opaque negotiation and\n   transparent negotiation.\n\n   opaque negotiation\n\n        With opaque negotiation, the selection of the best\n        variant is done by an algorithm located at the origin\n        server, and unknown to the proxies and user agents involved.\n        Selection is based on the contents of particular header fields\n        in the request message, or on other information pertaining to\n        the request, like the network address of the sending client.\n\n   A typical example of opaque negotiation would be the selection of a\n   text/html response in a particular language based on the contents of\n   the Accept-Language request header field.  A disadvantage of opaque\n   negotiation is that the request headers may not always contain enough\n   information to allow for selection.  If the Accept header\n\n            Accept: text/*: q=0.3, text/html, */*: q=0.5\n\n   is sent in a request on a varying resource which has a video/mpeg and\n   a video/quicktime variant, the selection algorithm in the origin\n   server will either have to make a default choice, or return an error\n   response which allows the user to decide on further actions.\n\n   opaque resource\n\n        A resource which is opaquely negotiated. Opaque resources always\n        include a Vary header every 2xx class response.\n\n\n3.4 Transparent resources and transparent negotiation\n\n   A transparent resource does not directly `contain' variants.  Instead,\n   a transparent resource binds to multiple alternate resources, which\n   in turn `contain' variants.  \n\n   An example is a resource http://x.org/intro, which uses negotiation\n   to provide a paper about x.org:\n\n    resource                      alternates                variants\n    --------                      ----------                --------\n\n                           -----  plain resource ---------- variant 1\n                         /      http://x.org/paper.html.notables\n                        /\n    transparent resource -------  plain resource ---------- variant 2\n    http://x.org/paper  \\       http://x.org/paper.html.tables\n                         \\\n                           -----  plain resource ---------- variant 3\n                                http://x.org/paper.ps\n\n   In this example, the transparent resource http://x.org/paper has\n   three different alternates and three different variants.  Note that\n   there is a one-to-one mapping between alternate URIs and variants.\n\n   An alternate resource can be a plain resource or an opaque resource,\n   but can never be a transparent resource itself.\n\n   alternate resource\n\n        A resource bound to by a transparent resource.  An alternate\n        resource is either a plain resource or an opaque resource, but\n        can never be a transparent resource itself.\n\n   best alternate resource\n\n        The alternate resource best matching to the user agent\n        preferences for the request.  If none of the alternates can be\n        handled by the user agent, the request does not have a best\n        alternate resource.\n\n   transparent negotiation\n\n        With transparent negotiation, the selection of the best\n        alternate resource is done by a distributed algorithm which can\n        perform computation steps in the origin server, in proxies, or\n        in the user agent.  Transparent negotiation guarantees that, if\n        the user agent supports the transparent negotiation algorithm\n        and is correctly configured, the request will always correctly\n        yield either the best alternate resource, or an error message\n        indicating that none of the alternates bound to the resource can\n        be handled by the user agent.\n\n   transparent resource\n\n        A resource which is transparently negotiated.  A transparent\n        resource binds to multiple alternate resources, which together\n        contain all variants available for the transparent resource.\n        Transparent resources always include an Alternates header in\n        every 2xx and 3xx class response.  This Alternates header\n        describes  the alternate resources bound to the transparent\n        resource.\n\n   A more complicated example is a resource http://x.org/intro, which\n   uses negotiation to provide introductory information about x.org:\n\n    resource                      alternates                variants\n    --------                      ----------                --------\n\n                           -----  plain resource ---------- variant 1\n                         /      http://x.org/intro.mpeg\n                        /\n    transparent resource -------  plain resource ---------- variant 2\n    http://x.org/intro  \\       http://x.org/intro.quicktime\n                         \\\n                          \\                          ------ variant 3\n                           \\                       /       \n                             ---  opaque resource   ------- variant 4\n                                http://x.org/intro.html\n                                                   \\\n                                                     ------ variant 5\n\n   In this example, the transparent resource http://x.org/intro has\n   three different alternates and five different variants.  It is not\n   expected that opaque alternate resources like the\n   http://x.org/intro.html above will be used often.  \n\n   An opaque alternate resource could be useful, for example, to\n   opaquely negotiate around bugs in the HTML rendering code of certain\n   user agents.  The resource http://x.org/intro.html above would use an\n   opaque algorithm located at the origin server, which looks at the\n   User-Agent request header to select a html variant which will render\n   without triggering bugs.  If variant 4 is returned when\n   http://x.org/intro is accessed, this will be the result of\n   transparent negotiation, which selected http://x.org/intro.html as\n   the best alternate, followed by opaque negotiation, which selected\n   variant 4 as the best variant.\n\n   Note that use of opaque resources is not necessary to negotiate on\n   features, rather than bugs, of HTML rendering engines.  The feature\n   negotiation facilities of transparent negotiation allow for\n   negotiation on HTML rendering features with greater flexibility and\n   efficiency.\n\n[End of text]\n\n\n\n", "id": "lists-010-3574584"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "A few quibbles ...\n\n> ...  IfValid, and IfInvalid become IfMatch and IfNoMatch, and so\n> forth.\n\nThat should be \"If-Match\" and \"Unless-Match\", for consistency with\nother (past and future) conditions.\n\n> request\n> -------\n> An HTTP request message (as defined in section 9).\n\n  An HTTP request message, as defined in section 9.\n\n> response\n> --------\n> An HTTP response message (as defined in section 10).\n\n  An HTTP response message, as defined in section 10.\n\n> resource\n> --------\n> A network data object or service that can be identified by a URI\n> (section 7.2). A \"resource\" is a concept (a little like a\n> Platonic ideal).  When represented electronically, a resource may be\n> of the kind which corresponds to only one possible bit stream\n> representation.  An example is the text version of an Internet\n> RFC. That never changes. It will always has the same checksum.\n\nBad example -- RFCs do change (obsoleted, updated, etc. in header).\nIn any case, this places too much focus on bit streams (which do not\nhave anything to do with a resource aside from being one representation\nresulting from a GET.  Keep in mind that a resource is essentially the\nsame as an \"Object\" in OO design methodology, and thus is more than just\nthe data representation even when that is static.\n\nI prefer the original, since the additional wording just confuses things.\n\n  resource\n  --------\n  A network data object or service that can be identified by a URI\n  (section 7.2).\n\n> generic resource\n> ----------------\n\n> On the other hand, a resource may be generic in that as a concept it\n> is well specified but not so specifically specified that it can only\n> be represented by a single bit stream.  In this case, other URIs may\n> exist which identify a resource more specifically. These other URIs\n> identify resources too, and there is a relationship of genericity\n> between the generic and the relatively specific resource.  As an\n\nI would prefer a more concise (and less circular) definition:\n\n  A resource that provides a generic interface to one or more other\n  resources.  In this case, other URIs may exist which identify the\n  resource(s) more specifically.  As an\n> example, successively specific resources might be\n> \n> 1. The Bible\n> 2. The Bible, King James Version\n> 3. The Bible, KJV, in English\n> 4. A particular ASCII rendering of the KJV Bible in English\n> \n> Each resource may have a URI.  The authority which allocates the URI\n> is the authority which determines to what it refers: Therefore, that\n> authority determines to what extent that resource is generic or\n> specific.  When we discuss electronic resources, an interesting fact\n> is that a small number of dimensions of genericity emerge.\n> \n> Time\n> A resource may vary with time. For example, \"The Wall Street Journal\"\n> varies with time. Each issue is a time-specific resource, which does\n> not change with time. Most home pages on the Web change with time, in\n> a less periodic way.\n\nNOTE: Allowing time to be a dimension of genericity will mean that almost\nall resources will be generic, including those that are non-negotiable\nat a particular instant in time.  Is this useful?\n\n> Language\n> When a document is translated, it is useful to be able to refer to it\n> either in the generic, or to a particular specific translation.\n> \n> Representation\n> A given resource may have many ways in which it can be represented on\n> the wire, using different Content-Types (section 18.19).  As an\n> example, an image may be represented in PNG or GIF format.\n> \n> specific resource\n> -----------------\n> A resource that is not a generic resource.\n> \n> variant\n> -------\n> An specific resource that is a member of at least one generic\n> resource.  Sometimes called a resource variant.  Note that the set of\n> variants of a generic resource may change over time as well.\n\n  A specific resource that is a member of at least one generic\n> resource.  Sometimes called a resource variant.  Note that the set of\n  variants of a generic resource may change over time.\n\n> entity\n> ------\n> The set of information transferred as the payload of a request or\n> response.  An entity consists of metainformation in the form of\n> Entity-Header fields and content in the form of an Entity-Body, as\n> described in section 11 \n  described in section 11.\n  \n> resource entity\n> ---------------\n> A particular representation, rendition,\n> encoding, or presentation of a resource at a particular point in time.\n> Resources not supporting content negotiation are bound to a single\n> entity.  Generic resources supporting content negotiation are bound to\n> a set of one or more entities, whose membership may vary over time.\n> \n> content negotiation\n> -------------------\n> The mechanism for selecting the appropriate variant of a generic\n> resource when applying a request, is described in section 15.\n                                    as\n> ...\n> \n> cache\n> -----\n> A program's local store of response messages and the subsystem that\n> controls its message storage, retrieval, and deletion. A cache stores\n> cachable responses in order to reduce the response time and network\n> bandwidth consumption on future, equivalent requests. Any client or\n> server MAY include a cache, though a cache cannot be used by a server\n> while it is acting as a tunnel.\n\n  cachable\n  --------\n  A response is cachable if a cache is allowed to store a copy of the\n  response message for use in answering subsequent requests. The rules for\n  determining the cachability of HTTP responses are defined in Section XX.\n  Even if a resource is cachable, there may be additional constraints on\n  when and if a cache can use the cached copy for a particular request.\n\n>...\n> validator\n> ---------\n> A tag or a Last-Modified date and time used to validate whether a cache \n> entry is \"fresh\" or \"stale.\"\n\n  An entity tag or Last-Modified value when used to validate whether a cache \n  entry is \"fresh\" or \"stale.\"\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-3596782"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "[Warning... Philosophy flame comming]\n\nSummary of note, this is not the time to open up this can of worms. The wording \nin the spec may appear broken but its best not to change it this close to the \ndeadline.\n\n\n>resource\n>--------\n>A network data object or service that can be identified by a URI\n>(section 7.2). A \"resource\" is a concept (a little like a\n>Platonic ideal).  When represented electronically, a resource may be\n>of the kind which corresponds to only one possible bit stream\n>representation.  An example is the text version of an Internet\n>RFC. That never changes. It will always has the same checksum.\n\nIf you wish to use a Platonic grounding then the principle of ideals\nis best read as being a primitive grasp of the \"abstraction\" principle.\nThe \"ideals\" which Plato describes are universal exemplars. That is \nvery much counter to what the Web is about. \n\nThe first problem with stating that resources are \"a little\" like Platonic \nideals is that this is precisely the opposite of what they are. The second is \nthat by definition _everything_ is \"a bit like\" a Platonic ideal, i.e. \nimperfect.\n\nQuote from MIT philosophy grad student:-\n[Far better to be quoting from Hegel, (section 5 philosophy of right\nidea = concept and actuation, without actuation concept is meaningless.\n(and if you can quote from Hegel to clarify something you have problems]\n\n\n>generic resource\n>----------------\n>On the other hand, a resource may be generic in that as a concept it\n>is well specified but not so specifically specified that it can only\n>be represented by a single bit stream.  In this case, other URIs may\n>exist which identify a resource more specifically. These other URIs\n>identify resources too, and there is a relationship of genericity\n>between the generic and the relatively specific resource.  As an\n>example, successively specific resources might be\n\n\nI think that this is serving to cloud the issue rather than clarify \nit. There are two possible groundings for the Web. One is in logical\npositivism, the other is in Hermeneutics. The philosophy of the Web\nBOF I held did not have anyone supporting the Logical Positivist\napproach, not even myself and I wrote a thesis on the basis of such\na grounding. \n\nI think that the attempt to separate \"resource\" from \"generic resource\"\ntries to make clear a point which the Web has profitably left fuzzy.\nThe problem is that there is no \"Universal\" ideal. All perceptions are\nmediated by the observer, through the sensory apparatus, belief system\netc. One of the reasons why Logical Positivism collapsed as a metaphysical\nproject was that quantum mechanics and the Goedel incompleteness\nproof demonstrated that logic alone was not a sufficient metaphysical\ntheory and that there was no universal comparability of experience.\n\nFor ZEUS the logical positivist approach was sufficient. We were dealing\nwith a closed form problem. The Web is attempting to deal with open \nform. It is meant to be a system of knowledge, not merely an information \nsystem. That is why it is important to get the terms right.\n\n\nIf you must ground the terms then I suggest getting a bona fide Philosopher\nto do the heavy lifting. Dr. Herwitz is probably your best bet. I suggest that \nyou don't open up that can of worms just before the completion deadline however. \nIt may not look like it but its the naming problem can of worms you are opening. \nThe failure of the URI group was due in large part to amateur expeditions in \nepistemology and semiotics. Whether you agree or disagree with Hermeneutics the \nclass of problems the URI group failed to solve are a class of problems the \nhermeneutics litterature predicts cannot be solved.\n\n\nThere really is no difference between generic resources and resources. All \nresources are generic resources. The distinction to be made between signs is \ndependent on context. \n\n\nIf we must settle this question I can get a MIT/Harvard A.B.D* in philosophy to \nprovide consulting in return for food. They get touchy about demarcation \ndisputes you know...\n\n\nPhill\n\n[A.B.D. = All But Disertation]\n\n\n\n", "id": "lists-010-3610619"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "hallam@w3.org:\n>\n>[Warning... Philosophy flame comming]\n>\n>Summary of note, this is not the time to open up this can of\n>worms. The wording in the spec may appear broken but its best not to\n>change it this close to the deadline.\n\nThe wording is broken, and I suggest it is mostly _changed back_ to\nthe wording in the Monday 1.1-02 draft. Specifically:\n\nresource -> a thing identified by an URI, as in 1.1-02\n\nentity  -> defined by Jeff's wording for entity instance\n\nresource entity -> defined by Jeff's wording for entity/the\n                   the current wording for resource entity\n                   (THESE THINGS DO NOT NEED TO HAVE THEIR OWN URIs!)\n\n(I'd rather have a different word than resource entity.  What about\n`entity source'?  The idea is that the entity source can generate both\nfull and range entities.  Range entities add additional complexity\nwhich is not reflected well in the new terminology definitions.)\n\ngeneric resource/varying resource ->\n    (as in my Vary header section:)\n\n    A resource is currently generic if it currently includes a Vary or\n    Alternates header in its 200/206 responses.  \n\n(Note: philosophically speaking, the above is an empirical definition:\ngeneric resources are defined by _the observable things they do_, not\nby what they _are_ or _mean_.  This may seem a bit circular to\nneo-Platonists, but it is the only way to stay clear of the\nphilosophical problems Phill has described with so much detail. It is\nOK to add some lines to the definition about what genericity would\nmean in a Platonic sense, but only to help implementers form a mental\nmodel, not as a way of defining the protocol feature.)\n\n(Note: as I said in my previous message: genericity is a _binary_\npredicate in the 1.1 spec.  The words about `successively specific\nresources' are confusing and must be deleted.  As Roy also noted,\nallowing time as a dimension of genericity makes the terminology\nuseless for 1.1)\n\n    A generic resource has as set of resource entities associated with\n    it, which can change contents though time.  A generic resource can\n    at any point in time change into a specific resource.  This change\n    will be reflected by the absence of Vary and Alternates headers in\n    subsequent responses.\n\nspecific resource/plain resource ->\n\n    A resource is currently specific if it not currently generic.\n    ....\n\n    A specific resource can at any point in time change into a generic\n    resource.  This change will be reflected by the presence of a Vary\n    or Alternates headers in subsequent responses.\n\nnegotiable resource -> is the same as a generic resource.  Use of this\nterm is must be avoided everywhere except in the content negotiation\nsection.\n\n(Note to Larry Masinter: It is true that 1.0 servers manage to provide\n`negotiation' by using `dynamic resources' that always generate a\nnon-cachable response tailored to the request headers on the fly.\nSuch dynamic resources include an Expires: <yesterday> header (or omit\nLast-Modified headers or use access authentication) to avoid caching,\nbut they do not send a Vary or Alternates header, so they cannot be\ncalled specific/varying resources, and thus cannot be called\nnegotiated under 1.1.)\n\nvariant -> THIS WORD IS NOT NEEDED AND BEST DELETED.\n           (a variant can be interpreted as a container which\n           successively contains related resource entities.)\n\nMy analysis of why we do not need the term variant: we introduced this\nterm to talk about variant-IDs.  However, the only semantics\nassociated with variant-IDs is that they can be used for cache\nreplacement.  Therefore, you can rename `variant-ID' to\n`replacement-key', which eliminates the problem of having to talk\nabout what a variant is.  Rewriting the spec to never use the word\n`variant' is not that hard.  Note that I managed not to use it in the\nVary header section.  All 1.1 mechanisms can be described without\nreferring to the concept of `a container which successively contains\nrelated resource entities'.\n\nGoing through the whole spec for the purpose of removing the word\n`variant':\n\n      3.14 Variant IDs\n\n      Variant-IDs are used to identify specific entities (variants) of a\n      varying resource; see section 13.8.3 for how they are used.\n\n            variant-id = quoted-string\n\n      Variant-IDs are compared using string octet-equality; case is\n      significant.\n\ncan be renamed to replacement keys:\n\n            replacement-key = quoted-string\n\n\n      3.16 Variant Sets\n\n      Validator sets are used for doing conditional retrievals on varying\n      resources; see section 13.8.3.\n\n            variant-set = 1#variant-set-item\n            variant-set-item = opaque-validator \";\" variant-id\n\ncan be renamed to validator-sets.\n\n      13.7.5 SLUSHY: Scope of Expiration\n\n      HTTP/1.1's expiration model is that as soon as any variant of a URI\n      becomes stale, all variants becomes stale as well.  Thus, _freshness_\n      applies to all the variants of URI, rather than any particular variant.\n      Dates and expires etc. apply to any cached variant that a proxy might\n      have with a URI and not just the one particular entity.\n\nEEK!  13.7.5 is plain wrong, and must be deleted.  Why did I not find\nthis before?\n\n      13.8 Caching and Content Negotiation\n      The HTTP content negotiation mechanism interacts with caching in\nseveral\n      ways:\n\n        .  A varying resource (one subject to content negotiation) may be\n           bound to more than one entity. Each of these entities is called a\n           _variant_ of the resource.\n            ^^^^^^^ \n\nreplace variant by `resource entity' or whatever term you come up\nwith.  Replace variant-ID with `Cval header value' most of the time.\nNote that only `Cval header values' which include `variant-IDs' can be\nsent in `Unless-<something>' headers for requests on varying\nresources.\n\n      13.12 SLUSHY: Cache Keys\n\nDelete completely, except for 13.12.4.\n\n      13.20 Cache Replacement for Varying Resources\n\nUse the text I posted on the list recently, with every `variant-ID'\nreplaced by `replacement-key'.\n\nKoen.\n\n\n\n", "id": "lists-010-3622196"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "Ouch!  My teeth hurt from grinding.\n\nPlease, there's no word \"genericity\".  Try \"generality\".  With\napologies to non-native English speakers, if we're going to use English\nfor the spec., let's at least make it correct English.\n\nDave Kristol\n\n\n\n", "id": "lists-010-3636000"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "On Tue, 30 Apr 1996, Dave Kristol wrote:\n\n> \n> Please, there's no word \"genericity\".  \n> \n\nWell, you are right in the sense that it does not occur in the Oxford\nEnglish Dictionary, but it is a very widely used technical term in\nmathematics.  It means the property of being generic (which is another\ntechnical term).  I just did a search of Mathematical Revues and found\n68 articles with the word \"genericity\" in the title.\n\nThis is off the subject though as your point was well taken.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-3643414"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "By the reactions I got (and when I reread my message) it is pretty\nclear my cold (or the drugs I took for it) did not improve my last\nmessage on terminology.  It did have the effect of clarifying where a\nnumber of people do have slightly different ideas for the same term.\nIn particular, different people seem to have different concepts of \nwhat a variant is.\n\nIn any case, this is the terminology I plan to go with in the next draft.\nSome of the definitions probably need tweaking; I suspect people\nwill be happier with what is below than the previous definitions.\n\nRight now, I'm more concerned with getting each term used where it should\nbe than quibbiling over the exact definition.  \n\nKoen suggested \"plain resource\" rather than \"specific resource\".  I\naccepted this suggestion, as it got us out of various regarding the use\nof specific elsewhere in the document, and it has the right connotation.\n\nJeff suggested \"resourcelet\" as a place-holder for what Roy called\n\"resource entity\".  While not happy with resource entity, I ended up\neven less happy with resourcelet.  So I'm going with resource entity\n(which I am less unhappy with), and unless someone magically generates\na better name instantaneously (I'm tired of going back and forth on\nthis one, so it would have to be pretty good).\n\nSo the explicit renames are:\n1) I'm changing the name of the CVal header to ETag.  \n2) I changed the name of the If-Valid header to If-Match.  \n3) I call If-Invalid If-NoMatch.  \n4) Roy would prefer If-NoMatch to be called Unless-Match.  \nBut we don't have any other Unless'es at the moment, \nso I currently believe If-NoMatch to be better.\n5) I also renamed Range-If to If-Range, for the hobgoblin \nof consistency.\n\n\nI'm now trying to plow my way through the rest of the editing.\n- Jim\n\n\n\n\nTerminology \n\nThis specification uses a number of terms to refer to the roles played\nby participants in, and objects of, the HTTP communication.\n\nconnection\n\nA transport layer virtual circuit established between two application\nprograms for the purpose of communication.\n\nmessage\n\nThe basic unit of HTTP communication, consisting of a structured\nsequence of octets matching the syntax defined in section 8 and\ntransmitted via the connection.\n\nrequest\n\nAn HTTP request message as defined in section 9.\n\nresponse\n\nAn HTTP response message as defined in section 10.\n\nresource\n\nA network data object or service that can be identified by a URI\n(section 7.2).  When represented electronically, a resource may be\neither a plain resource, which corresponds to only one possible\nrepresentation, or a generic resource.\n\ngeneric resource\n\nA resource that is a set of closely related representations of the\nsame document, form, applet, etc. A generic resource is always\nidentified by a URI. The individual representations may each be\nidentified by a unique URI, or by the combination of the generic\nresource's URI and a variant-ID, or by the combination of the generic\nresource's URI and some \"content-negotiation\" mechanism.  In this\ncase, other URIs may exist which identify a resource more\nspecifically.\n\nplain resource\n\nA resource that is not a generic resource.  A plain resource is always\nidentified by a URI.\n\nentity\n\nThe set of information transferred as the payload of a request or\nresponse, representing a resource entity.  An entity consists of\nmetainformation in the form of Entity-Header fields and content in the\nform of an Entity-Body, as described in section 11.\n\nresource entity\n\nA specific representation, rendition, encoding, or presentation of a\nnetwork data object or service, either a plain resource or a specific\nmember of a generic resource.  A resource entity might be identified\nby a URI, or by the combination of a URI and a variant-ID, or by the\ncombination of a URI and some other mechanism.\n\nvariant\n\nA resource entity that is a member of at least one generic resource.\nSometimes called a resource variant.  Note that the set of variants of\na generic resource may change over time as well.\n\ncontent negotiation\n\nThe mechanism for selecting the appropriate variant of a generic\nresource when servicing a request, as described in section 15.\n\nentity tag\n\nAn opaque string associated with an entity and used to distinguish it\nfrom other instances of the same resource entity.  A \"strong entity\ntag\" is one that may be shared by two entities of a resource entity\nonly if they are equivalent by octet equality.  A \"weak entity tag\" is\none that may be shared by two entities of a resource entity if they\nare semantically equivalent.  A given entity tag value may be used for\nentities identified by two different URIs, or by the same URI and two\ndifferent variant-IDs, without implying anything about the equivalence\nof these entities.\n\nclient\n\nAn application program that establishes connections for the purpose of\nsending requests.\n\nuser agent\n\nThe client which initiates a request. These are often browsers,\neditors, spiders (web-traversing robots), or other end user tools.\n\nserver\n\nAn application program that accepts connections in order to service\nrequests by sending back responses. Any given program MAY be capable\nof being both a client and a server; our use of these terms refers\nonly to the role being performed by the program for a particular\nconnection, rather than to the program's capabilities in\ngeneral. Likewise, any server MAY act as an origin server, proxy,\ngateway, or tunnel, switching behavior based on the nature of each\nrequest.\n\norigin server\n\nThe server on which a given resource resides or is to be created.\n\nproxy\n\nAn intermediary program which acts as both a server and a client for\nthe purpose of making requests on behalf of other clients. Requests\nare serviced internally or by passing them on, with possible\ntranslation, on to other servers. A proxy MUST interpret and, if\nnecessary, rewrite a request message before forwarding it. Proxies are\noften used as client-side portals through network firewalls and as\nhelper applications for handling requests via protocols not\nimplemented by the user agent.\n\ngateway\n\nA server which acts as an intermediary for some other server. Unlike a\nproxy, a gateway receives requests as if it were the origin server for\nthe requested resource; the requesting client may not be aware that it\nis communicating with a gateway. Gateways are often used as\nserver-side portals through network firewalls and as protocol\ntranslators for access to resources stored on non-HTTP systems.\n\ntunnel\n\nAn intermediary program which is acting as a blind relay between two\nconnections. Once active, a tunnel is not considered a party to the\nHTTP communication, though the tunnel may have been initiated by an\nHTTP request. The tunnel ceases to exist when both ends of the relayed\nconnections are closed. Tunnels are used when a portal is necessary\nand the intermediary cannot, or should not, interpret the relayed\ncommunication.\n\ncache\n\nA program's local store of response messages and the subsystem that\ncontrols its message storage, retrieval, and deletion. A cache stores\ncachable responses in order to reduce the response time and network\nbandwidth consumption on future, equivalent requests. Any client or\nserver MAY include a cache, though a cache cannot be used by a server\nwhile it is acting as a tunnel.\n\ncachable\n\nA response is cachable if a cache is allowed to store a copy of the\nresponse message for use in answering subsequent requests. The rules\nfor determining the cachability of HTTP responses are defined in\nSection 16.  Even if a resource is cachable, there may be additional\nconstraints on when and if a cache can use the cached copy for a\nparticular request.\n\nfirsthand\n\nA response is firsthand if it comes directly and without unnecessary\ndelay from the origin server, perhaps via one or more proxies.  A\nresponse is also firsthand if its validity has just been checked\ndirectly with the origin server.  explicit expiration time The time at\nwhich the origin server intends that an entity should no longer be\nreturned by a cache without further validation.  heuristic expiration\ntime An expiration time assigned by a cache when no explicit\nexpiration time is available.\n\nage\n\nThe age of a response is the time since it was generated by, or\nsuccessfully validated with, the origin server.\n\nfreshness lifetime\n\nThe length of time between the generation of a response and its\nexpiration time.\n\nfresh\n\nA response is fresh if its age has not yet reached its freshness lifetime.\n\nstale\n\nA response is stale if its age has passed its freshness lifetime. A\ncache may use a fresh response without validating it, but \"normally\"\nmay not use a stale response without first validating it.  (\"Normally\"\nmeans \"unless configured to provide better performance at the expense\nof transparency.\")  Therefore, what expires is the cache's authority\nto use a cached response, without validation, in its reply to a\nsubsequent request.\n\nsemantically transparent \n\nIdeally, an HTTP/1.1 cache would be \"semantically transparent.\" That\nis, use of the cache would not affect either the clients or the\nservers in any way except to improve performance. When a client makes\na request via a semantically transparent cache, it receives exactly\nthe same entity headers and entity body it would have received if it\nhad made the same request to the origin server, at the same time.\n\nvalidator\n\nAn entity tag, or a Last-Modified time, which is used to find out\nwhether a cache entry is a semantically transparent copy of a resource\nentity.  A cache entry is semantically transparent if its validator\nexactly matches the validator that the server would provide for\ncurrent instance of that resource entity.\n\n\n\n", "id": "lists-010-3651602"}, {"subject": "Terminology &ndash;&ndash; another tr", "content": "Here is another attempt at terminology based on ideas suggested\nby various people.  I am attempting to get away from philosophy \nand move towards operational definitions.  Among all the ideas\nposted so far, I like Koen's best.  I don't agree with him about\n\"variant\" though.  We can eliminate the word, but it would be \na mistake to eliminate the concept.  It is central to what we \nare talking about.  \n\nHere are some tentative definitions with comments marked by ##\n\n## The definition of entity should be first.  We seem to agree on\n## it.  It is easy to define.  Other things should be defined in\n## terms of it.\n\nentity\n------\nThe information or data transferred in an HTTP transaction.\nAn entity consists of metainformation in the form of\nEntity-Header fields and content in the form of an Entity-Body, as\ndescribed in section 11.\n\n\nresource\n--------\n\nA network data object or service that can be identified by a URI\n(section 7.2). A resource comprises all the entities which have been\nor could be produced in response to an HTTP request with a given URI.\nNumerous factors can affect the specific entity produced by a\nresource.\n\nThese include: \n   1) Time -- a resource may change over time, \n   2) Headers -- for example, Accept, Accept-Language, or authentication\n      headers may cause different entities to be produced,\n   3) Source of request -- e.g. distribution of a document may be limited\n      by IP address,\n   4) Query part of the URI.\n   5) Others?\n\n\nvariant\n-------\n\nA variant of a resource is a subset of the entities which comprise that\nresource.  It is a subset consisting of those entities which have been\nor could be produced in response to requests varying over time, but\nwith constant URI (except variation of the query part), constant\nheaders and all with the same request source.  \n\n## VARIANT is the term we have had the most difficulty defining.  I\n## think that operationally something close to the definition above is\n## forced on us.  For caching to work, a \"strong entity tag\"\n## consisting of a \"strong change-indicator\" and a \"variant-id\" must\n## uniquely determine an entity within a resource (URI).  The\n## change-indicator reflects only the time dimension.  Thus variant-id\n## must distinguish entities based on all other dimensions.  Hence, if\n## a variant-id determines a \"variant\" then a variant *must* consist of\n## the entities obtained by variations in only the time dimension.\n\nsimple resource\n---------------\n\nA resource with only one variant.\n\n## Formerly called a specific or plain resource\n\n\ncompound resource\n---------------\n\nA resource with more than one variant.\n\n## Formerly called a generic resource.\n\n\n\n\n\n\n\n  John Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-3668363"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "On Tue, 30 Apr 1996, Jim Gettys wrote:\n\n> Koen suggested \"plain resource\" rather than \"specific resource\".  I\n> accepted this suggestion, as it got us out of various regarding the use\n> of specific elsewhere in the document, and it has the right connotation.\n\nConsider \"simple resource\" versus \"compound resource\".\n\n> \n> Jeff suggested \"resourcelet\" as a place-holder for what Roy called\n> \"resource entity\".  While not happy with resource entity, I ended up\n> even less happy with resourcelet.  So I'm going with resource entity\n\nI am bothered by the fact that a \"resource entity\" is not an \"entity.\"\n(That is correct isn't it?)  Given this I think \"resourcelet\" is better.\n\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-3678163"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "Dave Kristol writes:\n > Ouch!  My teeth hurt from grinding.\n > \n > Please, there's no word \"genericity\".  Try \"generality\".  With\n > apologies to non-native English speakers, if we're going to use English\n > for the spec., let's at least make it correct English.\n > \n > Dave Kristol\n > \n\nUntil a few years ago, \"generic\" was only an adjective.\nNow even my Webster's admits it is sometimes a noun used to identify\nnon-name-brand over-the-counter medications.\nThe OOP community has been using the (non-)word \"genericity\" for years.\nWhere do you think new words come from anyway?  Somebody has to make them up.\n\n\n\n", "id": "lists-010-3686673"}, {"subject": "Re: 10.22 Hos", "content": "At 02:28 PM 4/29/96 EDT, you wrote:\n>10.22 says:\n>\n>All Internet-based HTTP/1.1 servers MUST respond with a\n>      400 status code to any HTTP/1.1 request message which lacks a Host\n>      header field.\n>\n>This conflicts with 5.1.2:\n>\n>                                       If the absoluteURI form is\n>      used, any Host request-header included with the request MUST be ignored.\n\nI don't see a conflict here.  All 1.1 requests must contain a Host\nheader.   IF the request also contains an absoluteURI, the host\ninformation contained in it will take precedence over the information\nsupplied in the Host header.\n\n>I think 5.1.2 has it right, in which case 10.22 should read:\n>\n>All Internet-based HTTP/1.1 servers MUST respond with a\n>      400 status code to any HTTP/1.1 request message that lacks both an\n>      absoluteURI in the request line and a Host header field.\n>\n\nThen you've basically said that all HTTP 1.1 clients must generate only\nabsoluteURIs in HTTP/1.1 requests, which contradicts the language\nin 5.1.2 that states\n\n        To allow for transition to absoluteURIs in all requests in future\nversions\n        of HTTP, HTTP/1.1 servers MUST accept the absoluteURI form in requests,\n        even though HTTP/1.1 clients will not normally generate them.   Versions\n        of HTTP after HTTP/1.1 may require absoluteURIs everywhere, after\n        HTTP/1.1 or later have become the dominant implementations.\n\n\n--\nSteve Wingardswingard@spyglass.com\nSpyglass, Inc.,  1240 E. Diehl Road, Naperville, IL 60563(708) 245-6581\n\n\n\n", "id": "lists-010-3694377"}, {"subject": "Re: 10.22 Hos", "content": "Steve Wingard <swingard@spyglass.com> wrote:\n  > At 02:28 PM 4/29/96 EDT, [dmk@allegra.att.com] wrote:\n  > >10.22 says:\n  > >\n  > >All Internet-based HTTP/1.1 servers MUST respond with a\n  > >      400 status code to any HTTP/1.1 request message which lacks a Host\n  > >      header field.\n  > >\n  > >This conflicts with 5.1.2:\n  > >\n  > >                                       If the absoluteURI form is\n  > >      used, any Host request-header included with the request MUST be ignored.\n  > \n  > I don't see a conflict here.  All 1.1 requests must contain a Host\n  > header.   IF the request also contains an absoluteURI, the host\n  > information contained in it will take precedence over the information\n  > supplied in the Host header.\n\nI admit it's fuzzy, but the second clause implies one could have\nomitted Host.  Changing \"any\" to \"the\" would make it more definite that\nthere has to have been one.\n  > \n  > >I think 5.1.2 has it right, in which case 10.22 should read:\n  > >\n  > >All Internet-based HTTP/1.1 servers MUST respond with a\n  > >      400 status code to any HTTP/1.1 request message that lacks both an\n  > >      absoluteURI in the request line and a Host header field.\n  > >\n  > \n  > Then you've basically said that all HTTP 1.1 clients must generate only\n  > absoluteURIs in HTTP/1.1 requests, which contradicts the language\n  > in 5.1.2 that states\n  > \n  >         To allow for transition to absoluteURIs in all requests in future\n  > versions\n  >         of HTTP, HTTP/1.1 servers MUST accept the absoluteURI form in requests,\n  >         even though HTTP/1.1 clients will not normally generate them.   Versions\n  >         of HTTP after HTTP/1.1 may require absoluteURIs everywhere, after\n  >         HTTP/1.1 or later have become the dominant implementations.\n\nThere are four cases:\n    1. relativeURL, no Host\n    2. relativeURL, Host\n    3. absoluteURL, no Host\n    4. absoluteURL, Host\n\nI'm trying to identify which require a 400 status code.  I believe only\ncase 1 deserves it, which is what my suggested wording says.  However,\nthe original words in 10.22 state that case 3 is also an error.  I\ndon't think it should be.\n\nDave Kristol\n\n\n\n", "id": "lists-010-3702734"}, {"subject": "Re: 10.22 Hos", "content": ">There are four cases:\n>    1. relativeURL, no Host\n>    2. relativeURL, Host\n>    3. absoluteURL, no Host\n>    4. absoluteURL, Host\n>\n>I'm trying to identify which require a 400 status code.  I believe only\n>case 1 deserves it, which is what my suggested wording says.  However,\n>the original words in 10.22 state that case 3 is also an error.  I\n>don't think it should be.\n\nYes, and it is intended that it be an error.  The sense of the group\nwas that while it was in theory overkill to always require host,\nit was too likely that implementers would get it wrong.  And a single,\nwidespread popular client might make the attempt to deal with the\nhost problem moot.  \n\nSo the intent is to always require host in 1.1, and require servers\nto generate errors if they detect anyone not playing by the rules,\nso that non-conforming clients get caught quickly.\nThis requirement might get eased in future versions, if we succeed\nin overcoming the host problem.\n- Jim\n\n\n\n", "id": "lists-010-3712216"}, {"subject": "Re: HTTP 1.1 document terminology", "content": "Jim Gettys:\n>\n\nJim,\n\nYour new terminology definitions look very good.  I include some minor\nnitpicks below.\n\n\n>resource\n>\n>A network data object or service that can be identified by a URI\n>(section 7.2).  When represented electronically, a resource may be\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>either a plain resource, which corresponds to only one possible\n>representation, or a generic resource.\n\n`When represented electronically' can be removed.  Better still, it\ncan be replaced by `At any point in time'.\n\n>\n>generic resource\n>\n>A resource that is a set of closely related representations of the\n>same document, form, applet, etc. A generic resource is always\n>identified by a URI. The individual representations may each be\n>identified by a unique URI, or by the combination of the generic\n>resource's URI and a variant-ID, or by the combination of the generic\n>resource's URI and some \"content-negotiation\" mechanism.  In this\n>case, other URIs may exist which identify a resource more\n                                             ^^^^^^^^ \n>specifically.\n\nYou probably mean `representation', not `resource', here.\n\nKoen.\n\n\n\n", "id": "lists-010-3719799"}, {"subject": "Re: SPOOF issue (Was: Re: Rewrite of 13.12 (Cache keys)", "content": "> Some people in the phone conference yesterday wanted such a mechanism,\n> though they did not explain, as far as I could tell, why they wanted\n> it, let alone why they wanted it in plain 1.1.  This mechanism is\n> certainly not needed as a content negotiation `hook'.\n\nOne use for having a response invalidate cache entries would be to\nallow invalidations to be returned from \"POST\" requests. \n\nThis is probably necessary for folks who send new documents using POST\nto different URLs than the URL that served the original document.\n\nThis would only allow a kind of sequential access transparency: the\ncache is only transparent if you're using the same cache for access\nand update. That's not so onerous a requirement.\n\nI agree that this has nothing to do with content negotiation, but it\nis a situation where the capability is quite useful.\n\nAnother situation is where the origin server does an internal\nredirect; I'm afraid we punted on this issue, and we perhaps shouldn't\nhave. \n\n> There is _no consensus_ in the WG now, and there will likely be no\n> consensus in the next few weeks, about the need to include, into\n> plain 1.1, a mechanism by which the response for URI 1 can change or\n> create data cached for URI 2.\n\nJust a question before dropping this completely: is this something\nthat web proxies do today? Does anyone have any data or examples?\nI just don't want to forget the \"running code\" part.\n\nLarry\n\n\n\n", "id": "lists-010-3727938"}, {"subject": "Re: Problem with authentication challenge BNF and Diges", "content": "> As far as I know this has not been mentioned on the list...\n\nUmmm, yes it has.  Waay back in August or September.\nI'll see if I can find it in the archives later.\nIn any case, we made a clear decision on the WG mailing list that\nthe difficulty (not impossiblility) of parsing this was less important\nthan not breaking current practice.\n\n> all (however, it does include them in the examples in Section 2.4). Hence \n> the two versions are in conflict and digest is in internal conflict. Another \n> problem is that the challenge BNF requires a single space between the \n> auth-scheme and the realm and digest doesn't. \n> \n> We can do two things:\n> \n> 1) Use the Digest draft version and remove the comma intirely. This will \n> require\n>    changes to the HTTP/1.1 draft\n\nNope -- fix the Digest draft instead (if it can be fixed).\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-4333583"}, {"subject": "RE: Problem with authentication challenge BNF and Diges", "content": "My very strong hunch is that proposal two is not backwards compatible\nwith existing implementations of Digest Auth. Originally, I believe that\nonly one challenge was permitted per WWW-Authenticate header; the\nability to have several was added later. \n\n>----------\n>From: Henrik Frystyk Nielsen[SMTP:frystyk@w3.org]\n>Sent: Friday, July 05, 1996 3:35 PM\n>To: john@math.nwu.edu; fielding@w3.org; jg@w3.org\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Problem with authentication challenge BNF and Digest\n>\n>\n>As far as I know this has not been mentioned on the list...\n>\n>While writing the parser for the access authentication challenge\n>parser, it \n>became clear that it is almost impossible to distinguish the auth-param\n>from \n>a new challenge (there may be multiple challenges separated by comma).\n>\n>According to the latest Digest Authentication draft from John Franks\n>the \n>described syntax for the WWW-authenticate header doesn't contain a\n>comma at \n>all (however, it does include them in the examples in Section 2.4).\n>Hence \n>the two versions are in conflict and digest is in internal conflict.\n>Another \n>problem is that the challenge BNF requires a single space between the \n>auth-scheme and the realm and digest doesn't. \n\nI don't see this in the Digest draft. It says\n     WWW-Authenticate    = \"WWW-Authenticate\" \":\" \"Digest\"\n                              digest-challenge\n\n     digest-challenge    = 1#( realm | [ domain ] | nonce |\n                          [ digest-opaque ] |[ stale ] | [ algorithm ] )\n\nThe parameters in \"digest-challenge\" _are_ comma separated, because of\nthe \"1#\" at the beginning. The HTTP draft requires 1*SP between the\nscheme name and its params, whereas the digest draft allows any LWS (by\ndefault). I'll bet that almost all implementations allow LWS in\nWWW-Authenticate despite what the HTTP spec says -- my lexer, for\nexample, always skips LWS.\n\nAlso, the rules for combining multiple instances of headers of the same\nname together require that a comma be inserted between the value fields\nof each header. This, plus backwards compatibility mean, I'm afraid,\nthat we'll just have to live with the near ambiguity. It isn't worth\nchanging the HTTP spec at this point.\n\nIf you want to change the Digest spec to be clearer about the 1*SP, we\ncould do that when it goes to Draft Standard.\n\nPaul\n>\n\n\n\n", "id": "lists-010-4343275"}, {"subject": "Re: proposed HTTP changes for charse", "content": "I have already covered these questions ad-nauseum.\n\n  1) HTTP has *always* used a default charset value of ISO-8859-1.\n     All implementations to the contrary had KNOWN failure conditions\n     and did not work as intended except within locally controlled\n     environments.\n\n  2) The HTTP version defines the communication capability of the\n     immediately adjacent client or server -- it NEVER indicates that\n     feature capabilities of the user agent.\n\n  3) None of the issues you have raised involve a technical problem\n     with the HTTP/1.1 protocol -- they are POLITICAL problems that\n     are an artifact of historical reality, a reality which the IETF\n     is not capable of changing.\n\n  4) Labelling the charset with its real value if it is different than\n     iso-8859-1 *always* works, both in old an new practice, because\n     any user agent incapable of handling a charset value is also\n     incapable of handling a charset other than iso-8859-1.  The only\n     time problems occur is when iso-8859-1 data is labelled as such\n     and then delivered to an older client.\n\n  5) Whether or not a client is capable of understanding the charset\n     parameter is NOT a function of the protocol version -- ALL HTTP/1.0\n     clients MUST understand charset, even if HTTP/1.0 is not a \"standard\",\n     because that is part of the HTTP/1.0 definition (see RFC 1945).\n\nI see no point in continuing this discussion unless you can demonstrate\na real problem that needs to be solved and can be solved within the\nconstraints of HTTP/1.1.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-4357543"}, {"subject": "Re: Problem with authentication challenge BNF and Diges", "content": ">> As far as I know this has not been mentioned on the list...\n> \n> Ummm, yes it has.  Waay back in August or September.\n> I'll see if I can find it in the archives later.\n\nFound it: <http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q3/0162.html>\n\n.....Roy\n\n\n\n", "id": "lists-010-4367222"}, {"subject": "Re: Problem with authentication challenge BNF and Diges", "content": "\"Roy T. Fielding\" writes:\n> >> As far as I know this has not been mentioned on the list...\n> > \n> > Ummm, yes it has.  Waay back in August or September.\n> > I'll see if I can find it in the archives later.\n> \n> Found it: <http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q3/0162.html>\n\nThanks Roy, I'll add the code then even though it's not pretty :-)\n\nHenrik\n\n\n\n", "id": "lists-010-4375368"}, {"subject": "Re: proposed HTTP changes for charse", "content": "Roy T. Fielding wrote:\n\n>   3) None of the issues you have raised involve a technical problem\n>      with the HTTP/1.1 protocol -- they are POLITICAL problems that\n>      are an artifact of historical reality, a reality which the IETF\n>      is not capable of changing.\n> \n>   4) Labeling the charset with its real value if it is different than\n>      iso-8859-1 *always* works, both in old an new practice, because\n>      any user agent incapable of handling a charset value is also\n>      incapable of handling a charset other than iso-8859-1.  The only\n>      time problems occur is when iso-8859-1 data is labeled as such\n>      and then delivered to an older client.\n> \n> I see no point in continuing this discussion unless you can demonstrate\n> a real problem that needs to be solved and can be solved within the\n> constraints of HTTP/1.1.\n\nDemonstration of a real problem following...\n\nSuppose we have a server that delivers a page with\n\nContent-type: text/html; charset=iso-8859-2\n\nOn the other side of a connection we'll probably (50-60% in my logs) have\nNetscape 2.0 on Windows CEE. CEE is Central & Eastern Europe version,\nLatin 2 fonts come with OS. Netscape 2.0 can switch code page when it\nreceives charset parameter (so I've been told). Everything should work,\nbut it doesn't. Why?\n\nBecause Latin 2 does *not* mean the same for ISO and Microsoft. Microsoft\ndelivers their systems with something they sometimes call CP1250 and\nsometimes Latin 2. That code page has all of the ISO 8859-2 characters, but\nsome of them are at different positions. Positions from 128 to 159 are\nfilled with something, but that's not the problem. The problem is that\nthey swapped two 32-character blocks. They wanted to have copyright (or\ntrademark, I don't recall any more) sign at the same position as in\nLatin 1.\n\nI couldn't find any charset with 1250 in its name in IANA registry, but\nthere is iso-8859-2-windows-latin-2, and I suppose that's the name of\nthe code page, since nothing else fits. I don't use PCs (except as text\nterminals for Unix) and I'm not 100% sure, but I think that Netscape\ncan't recognize that in charset parameter and it would show the page\nwith default charset, which is ISO 8859-1. Wrong, again.\n\n<note>Netscape 3.0 beta has a workaround for this, with lots of bugs\nat this stage. Bug reports filled and delivered. But this is just one\nbrowser.</note>\n\nThe typical server here will send a page with CP1250 (without charset),\nthe page would inform the user that he should manually switch to Latin 2\nencoding, and offer 2 or 3 links for other encodings (those pages would\nagain be sent without charset parameter).\n\nI hacked my server a bit, wrote several CGI programs and it's a little\nsmarter than others. It can convert HTML pages to 5 different code pages or\n3 different ASCII approximations on the fly. I'll probably add some more\noutput representations. I think Macs use the 6th code page for Latin 2\nand two more approximations would be handy.\nThe conversion is automatic if browser sends Accept-charset header.\nLynx 2.5 is the only one at the moment. Other browsers will receive\nsome kind of menu.\n\nToo many code pages are in use (ISO 646 has a fair amount of users) and\nbrowsers are currently incapable to deal with them. Servers (or proxies)\ncould. Not with labeling Content-type, because it would only pass the\npotato to the browser. Servers could convert, but they MUST know which\ncode page user on the other side has installed. HTTP 1.1 spec says that\nabsence of Accept-charset means that any charset is acceptable and almost\nall browsers don't bother to send it. I'd like to change that to\nsomething like this:\n\nNo Accept-charset       --   HTTP 1.1 agent is capable of representing\n                             ISO 8859-1 only.\nAccept-charset: *       --   Any charset is acceptable. I doubt that this\n     will be true for browsers, but it would be\n     useful for robots.\nIf the agent can use charsets other than ISO 8859-1, then it MUST, MUST\nand MUST send Accept-charset header with those charsets listed.\n\n-- \nLife is a sexually transmitted disease.\n\ndave@fly.cc.fer.hr\ndave@zemris.fer.hr\n\n\n\n", "id": "lists-010-4383435"}, {"subject": "Re: proposed HTTP changes for charse", "content": ">I hacked my server a bit, wrote several CGI programs and it's a little\n>smarter than others. It can convert HTML pages to 5 different code pages or\n>3 different ASCII approximations on the fly. I'll probably add some more\n>output representations. I think Macs use the 6th code page for Latin 2\n>and two more approximations would be handy.\n>The conversion is automatic if browser sends Accept-charset header.\n>Lynx 2.5 is the only one at the moment. Other browsers will receive\n>some kind of menu.\n\nDitto with DynaWeb, except that DynaWeb it supports far,\nmore encodings. As it is, with Japanese, you have to make some\narbitrary decision based soley on things like User-Agent for deciding\nwhat to send to the client, and what the client is sending to you.\n\n>If the agent can use charsets other than ISO 8859-1, then it MUST, MUST\n>and MUST send Accept-charset header with those charsets listed.\n\nI agree with this sentiment 100%. Unless browsers start sending\ninformation to servers, it is impossible to add multilingual\nintelligence to them, and have them work all the time.\n\n\n\n", "id": "lists-010-4395878"}, {"subject": "Strange proxy behavio", "content": "I wrote an nph CGI program which outputs headers in this order:\n\nDate:\nServer:\nContent-type:\nContent-length:\nLast-modified:\n\nNeither Netscape proxy 2.0b4 nor CERN httpd 3.0 (acting as proxy) kept\nentity body in the cache. Then I looked at server output and rearranged\nheaders to this order:\n\nDate:\nServer:\nLast-modified:\nContent-type:\nContent-length:\n\nAnd entity body was cached. HTTP 1.0 says this:\n\n   The order in which header fields are received is not significant.\n   However, it is \"good practice\" to send General-Header fields first,\n   followed by Request-Header or Response-Header fields prior to the\n   Entity-Header fields.\n    \nLast-modified, content-type & content-length are all entity headers.\nIs there a small print somewhere in the spec that requires this behavior?\nIf there isn't, I think there should be a warning about this in the\nspecifications. I was checking what they do with Pragma: no-cache directive.\nI'd never notice it otherwise.\n\n-- \nLife is a sexually transmitted disease.\n\ndave@fly.cc.fer.hr\ndave@zemris.fer.hr\n\n\n\n", "id": "lists-010-4404525"}, {"subject": "IfRange Questio", "content": "I have a question about the intention of If-Range behavior under\ncertain conditions.\n\nGiven a request of the form:\n\nGET /doc.html HTTP/1.1\nIf-Match: \"abc\"\nRange: 500-1000\nIf-Range: \"xyz\"\n\nGiven the If-Match fails, you would return 412 except for the If-Range\nbeing present. The problem is now what is meant to happen. The If-Range\nhas used a different entity tag which *does* match. Section 14.27 says\nthat if the entity tag matches, the server should return the sub-range\nwith a 206 return code, otherwise the entire entity and a 200 return\ncode.\n\nSince the spec allows it, is there a reason to use a different entity\ntag in the If-Range header? Is there a reason to have a parameter for\nIf-Range instead of refering to If-Match or If-Unmodified-Since parameters?\n\nIt seems putting an entity tag (or date) with the If-Range is redundant unless\nthere is a reason a client would want to retrieve a byte range of an\nentity which doesn't match the primary entity tag sent.\n\nHopefully this is clear and hopefully I didn't miss the answer when\nlooking at the list archives.\n\nthanks,\n\nPaul\n\n\nPaul Hethmon\nphethmon@utk.edu\n----------------------------------------------------------\nComputerman -- Agricultural Policy Analysis Center\n----------------------------------------------------------\nNeoLogic Ftp & Mail Servers\n----------------------------------------------------------\nKnoxville Warp User's: http://apacweb.ag.utk.edu/os2\n----------------------------------------------------------\n\n\n\n", "id": "lists-010-4412164"}, {"subject": "Re: IfRange Questio", "content": "On Sun, 7 Jul 1996, Paul Hethmon wrote:\n\n> I have a question about the intention of If-Range behavior under\n> certain conditions.\n> \n> Given a request of the form:\n> \n> GET /doc.html HTTP/1.1\n> If-Match: \"abc\"\n> Range: 500-1000\n> If-Range: \"xyz\"\n> \n> Given the If-Match fails, you would return 412 except for the If-Range\n> being present. The problem is now what is meant to happen. The If-Range\n> has used a different entity tag which *does* match. Section 14.27 says\n> that if the entity tag matches, the server should return the sub-range\n> with a 206 return code, otherwise the entire entity and a 200 return\n> code.\n\nHowever section 14.36 specifically states that the Range: header does\nnot affect anything but a 200 response - so in this case the If-Range\nheader just doesn't matter, you'd return a 412.\n\n> Since the spec allows it, is there a reason to use a different entity\n> tag in the If-Range header? Is there a reason to have a parameter for\n> If-Range instead of refering to If-Match or If-Unmodified-Since parameters?\n\nYes. Yes. You've asked two different questions here. The first is\neasy:\n\nGET /foo HTTP/1.1\nRange: 500-1000\nIf-Match: \"abc, \"xyz\"\nIf-Range: \"xyz\"\n\nThis clearly says: if the entity is \"abc\", send me the whole thing, if\nit's \"xyz\", send me the second 500 bytes, otherwise, send me a 412.\n\nAs for the second question, the reason that If-Range is a seperate\nheader than If-Match/None-Match/Modified-Since/Unmodified-Since is to\nmake it easy for servers to not support ranges. If the semantics of\nIf-* changed when a range request was present, that would mean that\nall servers would need to at least be aware of ranges, otherwise the\nwrong behavior could result. This would include the millions of\nexisting servers, if someone wanted to use a date with a Range\nrequest, since that would entail the use of If-Modified-Since.\n\nSo If-Range is a perfect solution, since it can be ignored by\nnon-range-supporting browsers along with Range:, and the correct\nresult can always be obtained. (a full 200 response, if availble).\n\n> It seems putting an entity tag (or date) with the If-Range is redundant unless\n> there is a reason a client would want to retrieve a byte range of an\n> entity which doesn't match the primary entity tag sent.\n\nIf-Range and the other If-* headers won't be used together 99.9% of\nthe time. See, here's the intended scenario (or something like it):\n\nA server sends a browser the first 100 bytes of a 1000 byte\ndocument with an entity tag of \"abc\". But the connection gets broken\nand the browser wants to go back and get the rest of it. So it sends:\n\nGET /foo HTTP/1.1\nRange: 100-999\nIf-Range: \"abc\"\n\nRemember that the browser wants to, eventually, have the whole\ndocument. If the server supports ranges, and the etag is still \"abc\"\nfor that document, it sends the missing 900 bytes. If the server\ndoesn't support ranges, or the entity tag has changed, it gets sent\nthe whole document, which is what the browser wanted.\n\nWith If-Match, it would have gotten a 412, and it would have had to\nretry the document - doing this for *every* single document that one\ndoes a range request for (which could be a lot of them) could get\nslow. So the If-Range header is provided to \"short-circuit\" the\nadditional request.\n\nAt least, that's my take on it all. I only speak for me.\n\n-- \n________________________________________________________________________\nAlexei Kosut <akosut@nueva.pvt.k12.ca.us>      The Apache HTTP Server\nURL: http://www.nueva.pvt.k12.ca.us/~akosut/   http://www.apache.org/\n\n\n\n", "id": "lists-010-4420522"}, {"subject": "Re: IfRange Questio", "content": "Addressed to: Alexei Kosut <akosut@nueva.pvt.k12.ca.us>\n              HTTP-WG <http-wg@cuckoo.hpl.hp.com>\n\n** Reply to note from Alexei Kosut <akosut@nueva.pvt.k12.ca.us> 07/07/96  3:08pm -0700\n> \n> If-Range and the other If-* headers won't be used together 99.9% of\n> the time. See, here's the intended scenario (or something like it):\n> \n> A server sends a browser the first 100 bytes of a 1000 byte\n> document with an entity tag of \"abc\". But the connection gets broken\n> and the browser wants to go back and get the rest of it. So it sends:\n> \n> GET /foo HTTP/1.1\n> Range: 100-999\n> If-Range: \"abc\"\n\nOk. I see what I was reading now. I was reading the first paragraph\nof 14.27 and thinking If-Range was meant as a modifier to only\na conditional GET using If-Unmodified-Since or If-Match.\n\nI guess I just needed someone to point out the obvious.\n\nthanks,\n\nPaul\n\n\n\n\nPaul Hethmon\nphethmon@utk.edu\n----------------------------------------------------------\nComputerman -- Agricultural Policy Analysis Center\n----------------------------------------------------------\nNeoLogic Ftp & Mail Servers\n----------------------------------------------------------\nKnoxville Warp User's: http://apacweb.ag.utk.edu/os2\n----------------------------------------------------------\n\n\n\n", "id": "lists-010-4431880"}, {"subject": "Re: proposed HTTP changes for charse", "content": "> Date:          Fri, 05 Jul 1996 17:25:38 -0700\n> From:          \"Roy T. Fielding\" <fielding@liege.ICS.UCI.EDU>\n> \n> I have already covered these questions ad-nauseum.\n> \n>   1) HTTP has *always* used a default charset value of ISO-8859-1.\n\nThis is wrong.  Repeating a falsehood ad nauseam doesn't make it any \ntruer.  The default has stopped being ISO 8859-1 ever since the very \nfirst non-Latin-1 document was transmitted - unlabelled.\n\nServers have never sent charset, and thus have always \"defaulted\" to \nwhatever was in the document being sent,  Latin-1 or not.\n\nClients have always defaulted to whatever was the (generally) unique \nencoding they could deal with, Latin-1 or not.\n\nProxies have never paid any attention, and have never defaulted to \nanything.\n\n>      All implementations to the contrary had KNOWN failure conditions\n>      and did not work as intended except within locally controlled\n>      environments.\n\nAll implementations that assume Latin-1 have KNOWN failure conditions \nand do not work as intended when presented with unlabelled \nnon-Latin-1 documents, that is outside of locally controlled Latin-1 \nenvironments.  ISO 8859-1 is just a locally useful charset, whose \nrelative importance is shrinking daily.\n\n>   2) The HTTP version defines the communication capability of the\n>      immediately adjacent client or server -- it NEVER indicates that\n>      feature capabilities of the user agent.\n\nWho said anything else?  The version number indicates what protocol \nfeatures can be used.  A server receiving 1.0 knows it can reply with \nMIME-like headers followed by a blank line and an entity, which it \ncannot do with with 0.9.\n\nMy point was that a client sending 1.1 should guarantee that it can \ndeal gracefully with a charset parameter (behave no worse than \nwithout charset), something that is not true today with 1.0, despite \nthe language in RFC 1945.\n\n>   3) None of the issues you have raised involve a technical problem\n>      with the HTTP/1.1 protocol -- they are POLITICAL problems that\n>      are an artifact of historical reality, a reality which the IETF\n>      is not capable of changing.\n\nBy attempting to enforce a default charset with no justification in \ncurrent practice or technical, the IETF would be making a political \nstatement: the western hemisphere wants a free ride, let the rest of \nthe world deal with the charset labelling problem.  You are right \nthat Latin-1 as a default is an historical artefact; it *was* the \ndefault when it was the only encoding, but has not been ever since.\n\n>   4) Labelling the charset with its real value if it is different than\n>      iso-8859-1 *always* works, both in old an new practice,...\n\nYou're seeing only one side of the coin.  The world does not revolve \naround ISO 8859-1.  Today, lots of people *need* to set their browser \nto a default other than Latin-1, because most of the stuff they read \nis in languages not representable in Latin-1 and is unlabelled.  When \nthey get a document - unlabelled - in Latin-1 (or anything else but \ntheir current default), they see garbage.  No, it doesn't always \nwork.\n\n>   5) Whether or not a client is capable of understanding the charset\n>      parameter is NOT a function of the protocol version -- ALL HTTP/1.0\n>      clients MUST understand charset, even if HTTP/1.0 is not a \"standard\",\n>      because that is part of the HTTP/1.0 definition (see RFC 1945).\n\nSee above.  True in theory, but not (yet) in practice.  Hence a \nserver cannot count on the 1.0 label to mean that the client will \ngrok charset; I know, I've tried, got burned and had to hack \nsomething based on User-Agent:.\n\n> I see no point in continuing this discussion unless you can demonstrate\n> a real problem that needs to be solved and can be solved within the\n> constraints of HTTP/1.1.\n\nI have described the problem above.  It lies outside of a \nLatin-1-centric world, so perhaps you have not encountered it before, \nbut it nevertheless exists.  It really makes interoperability a \npipedream in places where Latin-1 is irrelevant, and even more where \nmultiple encodings are used for one language.\n\nAs for the constraints of HTTP/1.1, there has been a lot of loose \ntalk about backward compatibility, but no definite problem case \ncited, except for a potential one with proxies.  The latter was, I \nthink, fixed if only origin servers are required to send charset.  If \nthat is not the case, please say so.  I'd rather hear this than \n\"Let's not discuss it\".\n\nFinally, why is HTTP/1.1 constrained to mandate English and ISO \n8859-1 as defaults in the new Warning: header?  I have not seen any \njustification, so let's please fix that.  Has anyone noticed that \nmandating a default language amounts to telling those who do not \nspeak that language that they cannot write complete 1.1 servers, lest \nthey hire a translator?\n\nRegards,\n-- \nFrancois Yergeau <yergeau@alis.com>\nAlis Technologies Inc., Montreal\nTel : +1 (514) 747-2547\nFax : +1 (514) 747-2561\n\n\n\n", "id": "lists-010-4440778"}, {"subject": "Re: proposed HTTP changes for charse", "content": "On Fri, 5 Jul 1996, Roy T. Fielding wrote:\n\n> I have already covered these questions ad-nauseum.\n>   4) Labelling the charset with its real value if it is different than\n>      iso-8859-1 *always* works, both in old an new practice, because\n>      any user agent incapable of handling a charset value is also\n>      incapable of handling a charset other than iso-8859-1.  The only\n>      time problems occur is when iso-8859-1 data is labelled as such\n>      and then delivered to an older client.\n\nThis isn't true. I was recently writing a chat CGI program and tried\nlabelling something as ISO-2022-JP. It caused the otherwise Japanese \ndisplay capable browser client (MSIE 3.0b1) to choke. It refused to\ndisplay the charset labelled file, instead attempting to download to a\nfile. If I *didn't* label it I was fine. The issue of charset labelling\nbreaking browsers cannot be neatly shoved off that way. It breaks\nnon-latin1 1.0 browsers just as badly as latin1  1.0 browsers. If it is\nunacceptable to mandate charset labelling becasue it breaks latin1\nbrowsers - it is equally unacceptable to break non-latin 1 browsers.\n\nI am trying to figure out why charset being a MUST for 1.1 is even an\nissue at all.  Let's see if I have this right.\n\nCase 1) A client *issues* a 1.1 request to a 1.0 server.\n\nThe server chokes on the 1.1 level and returns a 400 error.  Ok. No\nproblem the client can now try to back off to 1.0 - which won't be\nlabelled with a charset most likely. \n\nCase 2) A client issues a 1.1 request to a 1.1 server.\n\nIt gets a charset *always*. No problem - since there *ARE* no HTTP/1.1\nbrowsers in existence today there is no compatiblity issue.\n\nCase 3) A client issues a 1.0 request to a 1.1 server\n\nServer serves up as a 1.0 server without charset labelling (same as\ntoday's servers). No problem.\n\nCase 4) A client issues a 1.1 request to a 1.1 server.\n\nIt gets a 1.1 responese including charset *always*. No problem.\nSince there are no 1.1 browsers today, you can mandate charset\nsafely as far as browsers are concerned.\n\nOk. All of these cases work ok. So the problem has got to be when you\nstick a proxy in the line. How does mandating charsets break proxies?\nI don't see it.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-4453160"}, {"subject": "Re: proposed HTTP changes for charse", "content": "On Mon, 8 Jul 1996, Benjamin Franz wrote:\n\n> On Fri, 5 Jul 1996, Roy T. Fielding wrote:  \n> > 4) Labellingthe charset with its realvalue if it is different than \n> > iso-8859-1 *always* works, both in old an new practice, because \n> > any user agent incapable of handling a charset value is also \n> >incapable of handling a charset other than iso-8859-1.  The only \n> > time problems occur is when iso-8859-1 data is labelled as such \n> > and then delivered to an older client.\n\n> This isn't true. I was recently writing a chat CGI program and tried \n> labelling something as ISO-2022-JP. It caused an otherwise Japanese \n> display capable browser client (MSIE 3.0b1 with NJWIN running) to choke. \n> It refused to display the charset labelled file, instead attempting to\n> download to a file. If I *didn't* label it I was fine. The issue of\n> charset labelling breaking browsers cannot be neatly shoved off that\n> way. It breaks non-latin1 1.0 browsers just as badly as latin1 1.0\n> browsers. If it is unacceptable to mandate charset labelling becasue\n> it breaks latin1 > browsers - it is equally unacceptable to break\n> non-latin 1 browsers. \n>\n> I am trying to figure out why charset being a MUST for 1.1 is even an \n> issue at all.  Let's see if I have this right. \n\nReading this over I realized I had failed to insert a necessary logical\nstep here. The discussion after this point assumed that we were simply\ngoing to live with the fact that under rare circumstances a 1.1 response\nwas going to be passed to a 1.0 client and break it (presumably through a \nproxy).\n\nSince labelling non-ISO-8859-1 charsets in the Content-Type is *proven* to\nbreak at least some 1.0 browsers, making charset a MUST for non-ISO8859-1\ncharsets is an incompatible change from 1.0. If this is going to be\ninserted into 1.1 - there is no reason at all other than local bias not to\nmake it a MUST for *ALL* charsets including ISO-8859-1.  Otherwise the\nMUST language for non-ISO-8859-1 charsets should be abandoned as being a\npolitical (it doesn't break MY browser) rather than a technical (it\ndoesn't break ANY browser) statement.\n\nIf charset is going to be inserted in a *compatible* way - it will have\nto be done via its own header (Charset: ISO-8859-1). I wasn't here for the\ndebates on that - so if someone knows why that was rejected, please pipe\nup with a summary.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-4463899"}, {"subject": "Re: proposed HTTP changes for charse", "content": "Benjamin Franz writes:\n...\n > \n > Ok. All of these cases work ok. So the problem has got to be when you\n > stick a proxy in the line. How does mandating charsets break proxies?\n > I don't see it.\n > \n > -- \n > Benjamin Franz\n > \n > \n\nOne case is when a 1.1 proxy receives a document from a 1.0 server,\nand it is unlabelled.  The proxy stores the document in its cache, and\non a later request from a 1.1 client, has to do something about the\ncharset.  If charset labelling is mandatory the proxy has to guess,\nwhich is not going to work.  So if charset labelling is mandatory in\n1.1, either the proxy has to have some way of indicating the content\nhas an unknown charset, or (ugh) it would have to revert to 1.0\nprotocol so that it could legally send an unlabelled response.\n\nShel\n\n\n\n", "id": "lists-010-4474803"}, {"subject": "Re: proposed HTTP changes for charse", "content": "On Mon, 8 Jul 1996, Shel Kaphan wrote:\n\n> Benjamin Franz writes:\n> ...\n>  > \n>  > Ok. All of these cases work ok. So the problem has got to be when you\n>  > stick a proxy in the line. How does mandating charsets break proxies?\n>  > I don't see it.\n>  > \n>  > -- \n>  > Benjamin Franz\n>  > \n>  > \n> \n> One case is when a 1.1 proxy receives a document from a 1.0 server,\n> and it is unlabelled.  The proxy stores the document in its cache, and\n> on a later request from a 1.1 client, has to do something about the\n> charset.  If charset labelling is mandatory the proxy has to guess,\n> which is not going to work.  So if charset labelling is mandatory in\n> 1.1, either the proxy has to have some way of indicating the content\n> has an unknown charset, or (ugh) it would have to revert to 1.0\n> protocol so that it could legally send an unlabelled response.\n\nReverting to 1.0 may not be pretty - but it has the tremendous virtue of\n*working*. It seems the right thing to do in any case. Attempting to\n'upgrade' a response from 1.0 to 1.1 seems questionable practice at best\nand promises to break things.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-4483590"}, {"subject": "&quot;charset&quot; issue is closed in HTTPW", "content": "Folks, we had it out. It's over here.\n\nHTTP/1.1 is now closed in the HTTP working group, and, in particular,\nthe issue of charset labelling.  I believe that the HTTP working group\nhas reviewed the issues sufficiently, that the language now in the\nspecification meets common approval of the members of the working\ngroup, and that further discussion _here_ is now pointless.\n\nIt was not inappropriate to bring it up in the meeting or for us to\ndiscuss it on the list, but we're ready now to move on.\n\nThe appeal process in IETF is spelled out in RFC 1602, section 3.6, if\nyou disagree. Since the last call period is coming to a close, I\nbelieve the only action that will have any affect would be for you to\nraise the issue before the area directors or the IESG as a whole.\n\n> The IESG has received a request from the HyperText Transfer Protocol\n> Working Group to consider \"Hypertext Transfer Protocol -- HTTP/1.1\"\n> <draft-ietf-http-v11-spec-05.txt, .ps> for the status of Proposed\n> Standard.\n\n> The IESG plans to make a decision in the next few weeks, and solicits\n> final comments on this action.  Please send any comments to the\n> iesg@ietf.org or ietf@ietf.org mailing lists by July 5, 1996.\n\n\n\n", "id": "lists-010-4492909"}, {"subject": "Unregistered charset values in HTTP 1.1, the ISO-8859* value", "content": "Two more charset-related issues in the HTTP 1.1 draft:\n\n\n1)  Unregistered charset values\n\nIn draft-ietf-http-v11-spec-05.txt is said:\n\n> 3.4 Character Sets\n\n> Although HTTP allows an arbitrary token to be used as a charset value,\n> any token that has a predefined value within the IANA Character Set\n> registry MUST represent the character set defined by that registry.\n\nMy reading of this is that, in HTTP 1.1, any charset\nvalue not registered with IANA can be used for\nnon-registered character sets that can be used after\nprivate agreement. In my view this is an unnecessary\ndeviation from the long-established rules for MIME, that\nprivate charset values must start with \"x-\". According to\ndraft-ietf-822ext-mime-imt-05.txt:\n\n: No character set name other than those defined above may be\n: used in Internet mail without the publication of a formal\n: specification and its registration with IANA, or by private\n: agreement, in which case the character set name must begin\n: with \"X-\".\n\n\n2) Preferred MIME names for ISO-8859 character sets\n\nThe HTTP 1.1 draft doesn't explicitly recommend the use\nof any particular subset of the more than 200 character\nsets registered with IANA. I suppose that this decision\n(which I don't believe is wise) is the result of earlier\nWG discussions, which I unfortunately have missed.\n\nHowever, the draft seems to indirectly give preference to\nsome of the more popular or well-defined character sets\nin this section:\n\n> 19.8.1 Charset Registry\n> \n> The following names should be added to the IANA character set registry\n> under the category \"Preferred MIME name\" and this section deleted.\n> \n>        \"US-ASCII\"\n>        | \"ISO-8859-1\" | \"ISO-8859-2\" | \"ISO-8859-3\"\n>        | \"ISO-8859-4\" | \"ISO-8859-5\" | \"ISO-8859-6\"\n>        | \"ISO-8859-7\" | \"ISO-8859-8\" | \"ISO-8859-9\"\n>        | \"ISO-2022-JP\" | \"ISO-2022-JP-2\" | \"ISO-2022-KR\"\n>        | \"SHIFT_JIS\" | \"EUC-KR\" | \"GB2312\" | \"BIG5\" | \"KOI8-R\"\n> \n> Please also add the following new alias as the \"preferred MIME name\":\n> \n>        \"EUC-JP\" for \"EXTENDED_UNIX_CODE_PACKED_FORMAT_FOR_JAPANESE\"\n\nThere are some problematic cases in this list:\n\na) \"ISO-8859-10\" is left out, though this part of ISO 8859\n   was adopted as early as in 1992.\n\nb) It's unclear what charset registration the preferred\n   MIME name \"GB2312\" shall designate: the ISO-registered\n   character set GB_2312-80 (MIBenum: 57) or the only\n   incompletely described GB2312 (MIBenum: 2025), which\n   of course already has the proposed preferred MIME name\n   as it's principal name. It is also possible that these\n   two registrations actually refer to exactly the same\n   character set, and should be merged in the IANA registry.\n\nc) The meanings of the two charset values ISO_8859-6:1987\n   (ISO-8859-6) and ISO_8859-8:1988 (ISO-8859-8) is not\n   clear. The ISO standards are silent on the question of\n   in which order the right-to-left characters of these\n   standards should be coded in a string.\n\n   RFC 1556 explains that three different interpretations\n   are possible: visual order, explicit logical order,\n   and implicit logical order. It proposes that the\n   values \"ISO-8859-6\" and \"ISO-8859-8\" shall be used for\n   text coded in visual order, that \"ISO-8859-6-E\" and\n   \"ISO-8859-8-E\" shall be used for text coded in\n   explicit order, and \"ISO-8859-6-I\" and \"ISO-8859-8-I\"\n   for text coded in implicit order. The MIME draft \n   draft-ietf-822ext-mime-imt-05.txt follows RFC 1556\n   regarding the meanings of \"ISO-8859-6\" and\n   \"ISO-8859-8\".\n\n   Current practice is more complex, unfortunately, which\n   is clear from a recent message to the Hebrew-oriented\n   mailing list ILAN-H, enclosed at the\n   end of this message.\n\n   For Arabic, visual order and explicit logical order\n   are seldom used. Normally, implicit logical order is\n   used, and the charset label then is \"ISO-8859-6\", not\n   \"ISO-8859-6-I\".\n\n   In Hebrew, on the other hand, \"ISO-8859-8\" usually has\n   the meaning of visual order, while the more popular\n   implicit order (as defined by the Unicode bidi\n   algorithm) is indicated by the \"ISO-8859-8-I\" charset\n   value.\n\n   It should also be noted that the current draft for\n   internationalization of HTML, draft-ietf-html-i18n-04.txt,\n   only specifies implicit order for bi-directional text.\n\n   I would recommend that current practice rather than\n   RFC theory is followed. If the indirect way of\n   favouring certain character sets for HTTP - to let\n   IANA assign \"preferred MIME names\" only for a few of\n   all the registered charset values - is followed,\n   \"ISO-8859-6\" should be retained as such a name, but\n   \"ISO-8859-8\" should be replaced by \"ISO-8859-8-I\".\n\n/Olle\n\n-- \nOlle Jarnefors, Royal Institute of Technology (KTH) <ojarnef@admin.kth.se>\n\nIncluded message from the ILAN-H mailing list:\n\nDate: Fri, 5 Jul 1996 21:04:43 -0500 (CDT)\nFrom: Alexandre Khalil <iskandar@ee.tamu.edu>\nReply-To: Alexandre Khalil <iskandar@ee.tamu.edu>\nTo: ILAN-H  Discussion in and about Hebrew in the network\n <ILAN-H@taunivm.tau.ac.il>\nCc: Arabic script mailing list <reader@leb.net>,\n \"ITISALAT: IT IS Arabic Language And Technology.\"\n <itisalat@listserv.georgetown.edu>\nSubject: Re: RFC 1556\nIn-Reply-To: <ILAN-H%96070415340381@VM.TAU.AC.IL>\nMessage-Id: <Pine.GSO.3.93.960705204813.3148r-100000@ee.tamu.edu>\nMime-Version: 1.0\nContent-Type: TEXT/PLAIN; charset=US-ASCII\nSender: owner-ilan-h@VM.TAU.AC.IL\n\nOn Thu, 4 Jul 1996, Hank Nussbacher wrote:\n\n>On Thu, 4 Jul 1996 14:21:52 +0200 you said:\n\n>>Uri Bruck wrote:\n\n[...]\n\n>>> Second, at least for ISO-8859-6, visual directionality is rarely, if ever,\n>>> used, and ISO-8859-6 is taken to mean Arabic in implicit directionality.\n\n>>I assume you are talking about the use of \"iso-8859-6\" as\n>>a charset parameter in e-mail according to MIME here.\n>>Does also a de-facto standard for directionality in the\n>>encoding of Hebrew text with charset value \"iso-8859-8\"\n>>exist? Is it the same as for \"iso-8859-6\"?\n\n>>If this is so, I would suggest that the revised RFC 1556\n>>should document the de facto standards, and introduce new\n>>values \"iso-8859-6v\" and \"iso-8859-8v\" for visual\n>>directionality.\n\n[...]\n\n>Incidentally, iso-8859-8 == iso-8859-8-v so no need for a visual\n>charset.  After knocking this subject around for a few more weeks,\n>would anyone like to volunteer to amend RFC1556?  I am a bit busy\n>and will only do it/get to it if no one else steps forward (probably\n>during August).\n\n  In summary\n\n        iso-8859-8 == iso-8859-8-v          as Hank said\nand\n        iso-8859-6 == iso-8859-6-i          as Uri pointed out\n\n\n  Also, visual and explicit encoding for iso-8859-6 have practically not\nbeen in use and with the appearance of better multi-script software such\nAccentSoft's and Alis that supports UTF/8 and its explicit directional\ntagging, these along with iso-8859-e might never see significant usage.\n\n  Shouldn't we promote Unicode in its various avatars or is there still a\nneed to finetune 8 bit encodings?\n\nalex\n\n\n\n", "id": "lists-010-4501312"}, {"subject": "Re: proposed HTTP changes for charse", "content": "Larry informs us that he wants to close the issues of charset and \nlanguage in warning headers (and for content, I assume), because \nthere has been enough review by the group.\n\nAs for the Warning: header, I haven't seen any discussion.  Have I \nmissed anything?  For the record, I did raise the issue at the \nMontreal IETF (although the minutes do not mention it), and at least \ntwice on this list.\n\nAs for content labelling, it certainly was reviewed, but I am left \nwith the impression that the basic issue (is ISO 8859-1 acceptable \nas a default) was not resolved.  Anyway, it was not a relaxed and \nserene discussion, so I am sure glad it is over.\n\nI personnally find it unfortunate that HTTP/1.1 will end up doing \nnothing to fix the current brokenness and will remain artificially \nbiased, but it seems there is nothing more I can do about it.  So be \nit !\n\nRegards,\n-- \nFrancois Yergeau <yergeau@alis.com>\nAlis Technologies Inc., Montreal\nTel : +1 (514) 747-2547\nFax : +1 (514) 747-2561\n\n\n\n", "id": "lists-010-4519575"}, {"subject": "Re: proposed HTTP changes for charse", "content": "The IETF process does not prevent the promotion of standards with\n'bias'. From RFC 1602:\n\n>      IETF Working Groups are generally able to reach consensus, which\n>      sometimes requires difficult compromises between differing\n>      technical solutions.  However, there are times when even\n>      reasonable and knowledgeable people are unable to agree.  To\n>      achieve the goals of openness and fairness, such conflicts must be\n>      resolved with a process of open review and discussion.\n>      Participants in a Working Group may disagree with Working Group\n>      decisions, based either upon the belief that their own views are\n>      not being adequately considered or the belief that the Working\n>      Group made a technical choice which essentially will not work.\n>      The first issue is a difficulty with Working Group process, and\n>      the latter is an assertion of technical error.  These two kinds of\n>      disagreements may have different kinds of final outcome, but the\n>      resolution process is the same for both cases.\n\nI believe that we have considered the alternatives on these issues\nadequately. Although I understand the basis for your disagreement, I\ndon't think there is grounds for you to claim that your own views \"are\nnot being adequately considered\".  And I believe that the choice that\nwe made will 'work', with some careful attention to backward\ncompatibility on the part of implementors -- but this kind of\nattention is always necessary and part of implementation advice, and\nnot necessarily the protocol itself. So I don't think there are\ngrounds to claim that \"the Working Group made a technical choice which\nessentially will not work.\" So, that's why I think we're done. If you\ndisagree on either of these points, you must convince IESG and/or IAB.\n\nThere is a third alternative listed in RFC 1602:\n\n>      *    If a concern involves a reasonable difference in technical\n>           approach, but does not substantiate a claim that the Working\n>           Group decision will fail to perform adequately, the Working\n>           Group participant may wish to pursue formation of a separate\n>           Working Group.  The IESG and IAB encourage alternative points\n>           of view and the development of technical options, allowing\n>           the general Internet community to show preference by making\n>           its own choices, rather than by having legislated decisions.\n\nI don't think this is actually a serious possibility, but it may be\nthe only alternative left if you follow out the process.\n\nThis is all a very touchy issue. Hopefully, those of you who want to\npursue this issue can reach those implementors whose products are in\n\"beta\" (which is what I presume the 'b1' is all about) and convince\nthem to fix their browsers if they break when handed explicit charset,\nat the same time convincing content providers to label their\ncontent. Doing so is important to the community, but not something\nthat can easily be accomplished by sprinkling more 'MUST' and\n'SHOULD's in the HTTP/1.1 specification.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-4528249"}, {"subject": "charset bugs (was: proposed HTTP changes for charset", "content": "(Not intended for HTTP/1.1 spec consumption since it's closed)\n\nNo charset labelling scheme will work if the browsers have bugs that\nchoke them when it's used. The solution is to fix the bugs. The\nexistence of bugs does not disprove Roy's statements about old practice\n-- the HTTP/1.0 spec, while only \"de-facto\" and \"post-facto\", does\nconstitute a reasonable way of deciding whether or not an HTTP/1.0\napplication has a bug in it by virtue of it departing far enough from\ncommunity norms.\n\nI've reported the bug discussed below and the issue with ISO-8859-2 and\ncode pages to our dev team. Problems with other companies' products\nshould be brought to their attention.\n\nIf we get to the point where all browsers understand charset labels when\npresent, then we can argue whether we have to go farther -- but I can't\nthink of anything more that could be added to the 1.1 spec itself that\nwill make bugs any less likely. And, in fact, making the spec more\nstringent than what the consumers really want will only lead to\ndeliberate violations.\n\n>----------\n>From: Benjamin Franz[SMTP:snowhare@netimages.com]\n>Sent: Monday, July 08, 1996 7:09 AM\n>To: Roy T. Fielding\n>Cc: Francois Yergeau; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com;\n>http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: proposed HTTP changes for charset \n>\n>On Fri, 5 Jul 1996, Roy T. Fielding wrote:\n>\n>> I have already covered these questions ad-nauseum.\n>>   4) Labelling the charset with its real value if it is different than\n>>      iso-8859-1 *always* works, both in old an new practice, because\n>>      any user agent incapable of handling a charset value is also\n>>      incapable of handling a charset other than iso-8859-1.  The only\n>>      time problems occur is when iso-8859-1 data is labelled as such\n>>      and then delivered to an older client.\n>\n>This isn't true. I was recently writing a chat CGI program and tried\n>labelling something as ISO-2022-JP. It caused the otherwise Japanese \n>display capable browser client (MSIE 3.0b1) to choke. It refused to\n>display the charset labelled file, instead attempting to download to a\n>file. If I *didn't* label it I was fine. The issue of charset labelling\n>breaking browsers cannot be neatly shoved off that way. It breaks\n>non-latin1 1.0 browsers just as badly as latin1  1.0 browsers. If it is\n>unacceptable to mandate charset labelling becasue it breaks latin1\n>browsers - it is equally unacceptable to break non-latin 1 browsers.\n>\n>I am trying to figure out why charset being a MUST for 1.1 is even an\n>issue at all.  Let's see if I have this right.\n>\n>Case 1) A client *issues* a 1.1 request to a 1.0 server.\n>\n>The server chokes on the 1.1 level and returns a 400 error.  Ok. No\n>problem the client can now try to back off to 1.0 - which won't be\n>labelled with a charset most likely. \n>\n>Case 2) A client issues a 1.1 request to a 1.1 server.\n>\n>It gets a charset *always*. No problem - since there *ARE* no HTTP/1.1\n>browsers in existence today there is no compatiblity issue.\n>\n>Case 3) A client issues a 1.0 request to a 1.1 server\n>\n>Server serves up as a 1.0 server without charset labelling (same as\n>today's servers). No problem.\n>\n>Case 4) A client issues a 1.1 request to a 1.1 server.\n>\n>It gets a 1.1 responese including charset *always*. No problem.\n>Since there are no 1.1 browsers today, you can mandate charset\n>safely as far as browsers are concerned.\n>\n>Ok. All of these cases work ok. So the problem has got to be when you\n>stick a proxy in the line. How does mandating charsets break proxies?\n>I don't see it.\n>\n>-- \n>Benjamin Franz\n>\n>\n\n\n\n", "id": "lists-010-4538537"}, {"subject": "Last Call: A Proposed Extension to HTTP : Digest Access Authentication to Proposed Standar", "content": "The IESG has received a request from the HyperText Transfer Protocol\nWorking Group to consider \"A Proposed Extension to HTTP : Digest Access\nAuthentication\" <draft-ietf-http-digest-aa-04.txt> for the status of\nProposed Standard.\n\nThe IESG plans to make a decision in the next few weeks, and solicits\nfinal comments on this action.  Please send any comments to the\niesg@ietf.org or ietf@ietf.org mailing lists by July 22, 1996.\n\n\n\n", "id": "lists-010-4552732"}, {"subject": "Draft 06 submitted to internet drafts directory", "content": "I just submitted draft 06 to the Internet drafts directory.\n\nIn the meanwhile, you can get versions from W3C's HTTP\npage located at http://www.w3.org/pub/WWW/Protocols/.\n\nNote that if you wish to see differences from draft 05 (quite small),\nthe best way is to use the Word or Postscript versions of the document\nwith revision marks; only significant changes are visible (i.e. I've\nsuppressed minor formatting fixes such as removal of unneeded spaces,\nand the like).  You'll find using these versions much easier than\nusing diffs of the text version against the previous text version, as\nthis text version has seen further tweaking (including some manual\nwork) to print better than in previous drafts.\n\nThis draft reflects comments recieved during last call period.\n\nThank you all for your efforts.\nSincerely,\nJim Gettys\n\n\n\n", "id": "lists-010-4561076"}, {"subject": "multihost virtual sites for HTTP 1.", "content": "With the Host: header in HTTP/1.1 it is possible to efficiently and\neasily make a single host act as many \"virtual\" Web sites.\n\nWhat I'd like to see in HTTP/1.2 is the ability to easily and\nefficiently make multiple hosts act like a single virtual Web site, so\nthat one can build huge web sites.\n\nFor example,\nhttp://www.megasoft.com/\ncould be the root of the whole, HUGE, publically accessible corporate\nweb of the MegaSoft Corp. The mapping of this corporate web onto\nphysical Web Servers should be flixible and mutable over time -- as the\nsite grows, more servers need to be added; and as the content changes,\nthe load on various pieces will change too, necessitating rebalancing\nthe content across the servers, even if more aren't needed.\n\nIt is _almost_ possible to do this in HTTP/1.1 today. The 302 (Moved\nTemporarily) response can be used to automatically redirect someone who\ndid a GET on\nhttp://www.megasoft.com/very/long/resource/path/because/its/a/big/site.\nhtml\nto\nhttp://server42.megasoft.com/its/a/big/site.html\nby including a Location: header with that URL, where, say, server42 is\nset up to handle all requests that start with\nhttp://www.megasoft.com/very/long/path/because/\n(said setup being a server implementation issue, not a protocol issue)\nso that the GET is transparently redirected to server42.\n\nI said \"almost\" above, because the 302 is only allowed by the 1.1 spec\nto work automatically for GET and HEAD requests \"since this might change\nthe conditions under which the request was issued\". For other requests,\nthe user has to be prompted first. In order to make a multi-host virtual\nsite, this needs to be automatic for all requests.\n\nAnother factor that makes it an \"almost\" is that even if 302 was alow to\nwork automatically for all requests, it wouldn't be efficient enough to\nscale well -- every request would come in to www.megasoft.com's server\nto be redirected. This would both load www.megasoft.com, and cause the\nclient to suffer an extra round trip. (The 302s can have an Expires: to\nallow them to be cached, and that would help, but there would be an\nawful lot of them.) The way to make it efficient is to somehow note that\nthe 302 applies to _all_ the resources with a particular prefix, and\nonly cache this one thing.\n\nA third factor is that, in order to scale, it is sometimes necessary to\nreplicate the content on multiple servers. However, 302 responses can\nonly contain one URI (and hence server) in the Location: header returned\nto specify where the redirect is to go. (Arguably, there is a way to do\nthis with the DNS, so I'm flexible on this.)\n\nI would like to add an option response header that says that it is OK to\nredirect all requests with a certain prefix of the Request-URI to a list\nof other places. (It's a new response header instead of a new status\ncode for backwards compatibility. Old clients will just ignore it and\nprompt on methods other than GET and HEAD).\n\nA possible syntax:\nReferral= \"Referral\" \":\" prefix 1#referral-URI\nprefix  = absoluteURI\nreferral= absoluteURI\n\nThe prefix has to be a prefix of the original Request-URI -- this\nprevents spoofing.\nThe response to this is to take everything after the prefix in the\noriginal requestURI, append it to one of the referrals to get a new URI,\nand retry the request using the new URI. If there is a Cache-Control or\nExpires header, then it can be cached, and used for subsequent requests\nfor resources that have the same prefix -- thus getting the client\ndirectly to the correct server without an extra round trip.\n\nThis proposal is nothing more than the application to HTTP of the tried\nand true referral mechanisms of distributed directory and file systems.\n\nComments?\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-4568279"}, {"subject": "Demographic", "content": "Jeff Mogul and I valunteered to write up a draft on simple demographics\n-- not the be-all and end-all, but something that would be enough to get\nan appreciable number of content providers to stop sending cache-busting\nresponses.\n\nIt is alleged that some advertisers want to pay content providers, not\nby the \"hit\", but by the \"nibble\" -- the number of people who actually\nclick on the ad to get more info.\n\nNow, HTTP already has a mechanism for doing this: the \"Referer\" header.\nHowever, it is normally disabled for privacy reasons -- according the\nthe spec\n\"Because the source of the link may be private information\nor may reveal an otherwise private information source, it is\nstrongly recommended that the user be able to select whether\nor not the Referer field is sent.\"\n\nIn the case of ads, the source of the link _really wants_ to let the\nreferred-to page know where the reference came from.\n\nSuppose we augmented to semantics of the Referer header so that, if it\nis used as a _response_ header (it is currently just a request header),\nit means that the server requests that a referer header is sent on any\nlink followed from this page. If the user wishes to browse without\nleaving any trail of where they came from, they could override this, of\ncourse -- but I'm thinking that we would recommend AGAINST this, for the\nfollowing reason:\n\nIf one doesn't do some such thing as sending Referer, then I imagine\nthat what content providers would do is have the URI that is the target\nof the link be unique to the content provider, so that the advertiser\ncan tell which content provider is the source of each hit. So, all that\nturning off Referer does is cause cache-busting behavior.\n\nWhat I'm looking for are comments on the privacy concerns with such an\napproach.\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-4582096"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "> From http-wg-request@cuckoo.hpl.hp.com Tue Jul  9 15:24 PDT 1996\n> \n> With the Host: header in HTTP/1.1 it is possible to efficiently and\n> easily make a single host act as many \"virtual\" Web sites.\n> \n> What I'd like to see in HTTP/1.2 is the ability to easily and\n> efficiently make multiple hosts act like a single virtual Web site, so\n> that one can build huge web sites.\n\nI think this is just an implementation, or rather, a site setup/administration\nproblem than a protocol defeciency.  How about using a round-robin DNS\nto map the hostname to multiple irons, use the power of such multiple\nhosts in the cluster to serve nfs mounted files from the same or similar pool.\n\nThis will not necessitate redirection with Location: header;  the name\nmegasoft.com becomes virtual and need not be heavily loaded; the contents\nneed not be replicated as they are all NFS mounted and hence distributed\nin the network (may be high speed because they are all potentially co-located).\n\nMay be there are some other issues I am missing in this simpleton approach.\n\ns.r.\n\n========================================================\nS.R.Venkatramananraven@Eng.Sun.COM\nStaff Engineer(415)786-9106\nInternet Solutions Engineering2550 Garcia Ave, M/S UMPK16-120\nSun Microsystems, Inc (SunSoft)Mountain View, CA 94043-1100\n\n\n\n", "id": "lists-010-4592063"}, {"subject": "RE: multihost virtual sites for HTTP 1.", "content": ">----------\n>From: S.Venkatramanan@Eng.Sun.COM[SMTP:S.Venkatramanan@Eng.Sun.COM]\n>Subject: Re: multi-host virtual sites for HTTP 1.2\n>\n>\n>> From http-wg-request@cuckoo.hpl.hp.com Tue Jul  9 15:24 PDT 1996\n>> \n>> With the Host: header in HTTP/1.1 it is possible to efficiently and\n>> easily make a single host act as many \"virtual\" Web sites.\n>> \n>> What I'd like to see in HTTP/1.2 is the ability to easily and\n>> efficiently make multiple hosts act like a single virtual Web site, so\n>> that one can build huge web sites.\n>\n>I think this is just an implementation, or rather, a site\n>setup/administration\n>problem than a protocol defeciency.  How about using a round-robin DNS\n>to map the hostname to multiple irons, use the power of such multiple\n>hosts in the cluster to serve nfs mounted files from the same or similar\n>pool.\n>\n>This will not necessitate redirection with Location: header;  the name\n>megasoft.com becomes virtual and need not be heavily loaded; the contents\n>need not be replicated as they are all NFS mounted and hence distributed\n>in the network (may be high speed because they are all potentially\n>co-located).\n>\n>May be there are some other issues I am missing in this simpleton approach.\n\nYou're right that this scales. But the proposal from my original mail\nshould be quite a bit more efficient, because in your suggestion every\nrequest gets handled by two hosts instead of one, and all content makes\nthree trips through a network controller instead of one. For the same\nreasons, latency is better when the request goes straight to the host\nthat has the resource.\n\nIn addition, there are issues when updates are allowed. I didn't point\nthis out in my original mail, but when the hosts are individually named,\na request that does an update can quite easily be redirected to the\n\"master\" which processes all updates.\n\nPaul\n\n\n\n", "id": "lists-010-4601636"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "At 3:47 PM -0700 7/9/96, S.R. Venkatramanan wrote:\n>> From http-wg-request@cuckoo.hpl.hp.com Tue Jul  9 15:24 PDT 1996\n>>\n>> With the Host: header in HTTP/1.1 it is possible to efficiently and\n>> easily make a single host act as many \"virtual\" Web sites.\n>>\n>> What I'd like to see in HTTP/1.2 is the ability to easily and\n>> efficiently make multiple hosts act like a single virtual Web site, so\n>> that one can build huge web sites.\n>\n>I think this is just an implementation, or rather, a site setup/administration\n>problem than a protocol defeciency.  How about using a round-robin DNS\n>to map the hostname to multiple irons, use the power of such multiple\n>hosts in the cluster to serve nfs mounted files from the same or similar pool.\n\nThis misses a fair amount of the advantages suggested by Paul. For one,\nevery host in the round-robin would have to have all of the same content;\nPaul's suggestion allows partial redirection. Thus,\nhttp://www.megacorp/root1/... could be redirected to one host that has just\nthat content, while http://www.megacorp/root2/... could be redirected to a\ndifferent host with different content.\n\nFurther, Paul's suggestion makes it easier to add redundancy.\nhttp://www.megacorp/root1/... could point to two or more hosts that are at\nvery different locations. If host1 is down because of hardware or network\nfailure, a good client would try host2 and so on. Of course, these hosts\nhave to try to have the same content, but it would be much less (and\ntherefore more likely to be the same) than for the entire MegaCorp tree.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-4612999"}, {"subject": "Re: Demographic", "content": "[This as Cc'd to the list, as the original was sent to the list, as well\nas the fact that it seems a relevant thing to discuss here.  I'm including\nthis message at the top, just to say \"I hope this is appropriate, yet I'm\nnot sure.\"]\n\nOn Tue, 9 Jul 1996, Paul Leach wrote:\n\n> Jeff Mogul and I valunteered to write up a draft on simple demographics\n> -- not the be-all and end-all, but something that would be enough to get\n> an appreciable number of content providers to stop sending cache-busting\n> responses.\n\nIf you need any help, let me know.\n\n> Suppose we augmented to semantics of the Referer header so that, if it\n> is used as a _response_ header (it is currently just a request header),\n> it means that the server requests that a referer header is sent on any\n> link followed from this page. If the user wishes to browse without\n> leaving any trail of where they came from, they could override this, of\n> course -- but I'm thinking that we would recommend AGAINST this, for the\n> following reason:\n\nGood point.  The referrer header should be able to be requested to be \nsent to the next page, but _only_ if the server that the link is on says \nto go for it.  If the server does not imply it, it should be assumed no.\n\nBut also, it may be worthwhile to have the Referrer header sent if the \npage the browser is going to is on the same server as the link to it.\n\ni.e. <URL: http://www.megacorp.com/whatever.html>has a link to <URL:\nhttp://www.megacorp.com/something.html> this should be highly reccommended\nthat the browser send the Referrer header to the second one, stating that \nit came from the first document.  As they are on the same host, there \nisn't really any large security issue.\n\nAnother idea would be for the first server to send a 'Trusted hosts'\nheader, that would imply that any hosts specified there are considered\ntrusted and the referrer should be sent to them.  This way, if it is a \nprivate server, they can allow referrer header to be sent to their sites \n(such as <URL: http://www.megacorp.com> has a private site for their \nemployees, too at <URL: http://employees-only.megacorp.com/private/> the \nemployees only place can include the former (public) WWW server as a \n'Trusted Host' so that they can determine which hits came from inside \ntheir company.  Now, this also brings up an additional point: do we allow \nthe user to specify Untrusted hosts?\n\n> What I'm looking for are comments on the privacy concerns with such an\n> approach.\n\nThere would probably be a great deal of concern over a browser sending \nthe headers without asking the user, much like the fuss over Netscape \nNavigator (tm) sending the 'From: ' header with the configured e-mail \naddress in one of their Beta versions.  While referrer doesn't seem to be \nas large of a security risk or privacy issue, it could cause some \nnervousness among users and companies, if they were relying on security \nby obscurity (not telling others about a private site, rather than \nprotecting it)\n\nAbout the only way to avoid this is to make sure that the spec says that \nthis should default to 'none' if the 'trusted hosts' isn't there, or the \nsite with the link doesn't say that it wants Referrer sent.\n\nIf applicable, the spec should also make reference to the User having \ncontrol as well.\n\n--\nKris \"The Doctor\" Benson <kris@hackers-unlimited.com>\nPresident, Hackers Unlimited\nPersonal HomePage: http://www.hackers-unlimited.com/doctorkb/\nHackers Unlimited: http://www.hackers-unlimited.com/\nJAPH, HTMLer, Webmaster, UNIX guy for hire...\n#####  May your beard and your .sig grow longer with wisdom  #####\n\n\n\n", "id": "lists-010-4622324"}, {"subject": "Re: Demographic", "content": "I would rather see a single mechanism for indicating the privacy\ncategory (or categories) of the content of a message, than a specific\nheader field for every conceivable category or method by which the\nprivacy might be compromised.  It most certainly should not be limited\nto referrals from prior GET responses.\n\nLikewise, it should make the common case efficient -- that is, no such\nfield would indicate that it is okay to send Referer (which is the common\ncase today -- the HTTP spec only suggests that the browser be configurable\nto avoid sending it, not that it shouldn't send Referer by default).\n\nNaturally, this should be done in PEP (or an equivalent replacement if\nPEP is not in HTTP/1.2) since that matches PEP's intended capabilities.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-4633797"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "> What I'd like to see in HTTP/1.2 is the ability to easily and\n> efficiently make multiple hosts act like a single virtual Web site, so\n> that one can build huge web sites.\n\nUmmm, assuming it is decent server software, the only real limitation\non site size right now is the bandwidth of the incoming network.\nSplitting requests onto different server machines can be easily\naccomplished using a very fast gateway box which sits on the trunk\nand routes requests to the internal servers for processing.  The only\nthing the gateway would need to do is look for the beginning of each\nmessage and multiplex from there (which is more difficult than\nIP routing, but not much more).\n\nI think that would be a lot easier than changing HTTP to accomodate\ncomplex URL rewriting rules.  There are good reasons to support a\nURI rewriting mechanism on the client side (e.g., to make the URI\nsyntax truly extensible, support automated mirroring, support URNs,\netc.), but I haven't considered single-site performance to be one\nof them.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-4641898"}, {"subject": "Re: Unregistered charset values in HTTP 1.1, the ISO-8859* value", "content": "In draft-ietf-http-v11-spec-05.txt is said:\n\n> 3.4 Character Sets\n\n> Although HTTP allows an arbitrary token to be used as a charset value,\n> any token that has a predefined value within the IANA Character Set\n> registry MUST represent the character set defined by that registry.\n\nand Olle presented a reading of this that makes it seem like HTTP is\npermitting unregistered charset values. Frankly, I don't think anyone\ncares much what this says; it was the case that the MIME documents\nwere in flux. Clearly, using non-standard charset values is\nnon-standard.\n\n> 2) Preferred MIME names for ISO-8859 character sets\n\n> The HTTP 1.1 draft doesn't explicitly recommend the use\n> of any particular subset of the more than 200 character\n> sets registered with IANA. I suppose that this decision\n> (which I don't believe is wise) is the result of earlier\n> WG discussions, which I unfortunately have missed.\n\nThere have been innumerable discussions on the issues of charset\nnames, and no desire in the HTTP working group to actually prevent\nanyone from using any charset they want if properly labelled (the\nsubject of the last heated debate.) There's no simple reason why HTTP\nitself should restrict itself to some charset names over others since\nthe client can send Accept-Charset to identify the charset values it's\nwilling to deal with. This is a significantly different situation than\nmail, which is sent preemptively without direct communication.\n\nHowever, most of the comments here do not change the HTTP\nspecification, but only the recommendations for what should happen to\nthe IANA registration. As such, I would assume that the charset\nregistration issues could merged with the consideration of\ndraft-freed-charset-reg-00.txt as BCP, while the HTTP/1.1 spec could\nbe processed.\n\nThe intent was not to establish a monopoly on \"preferred MIME names\",\nand not to reinterpret any of the charset registrations, but just to\npick the short names that are currently in use in HTTP.\n\n\n\n", "id": "lists-010-4650948"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "> syntax truly extensible, support automated mirroring, support URNs,\n> etc.), but I haven't considered single-site performance to be one\n> of them.\n\nI think the power of Pauls suggestion is the ability to create a\nsingle virtual server which is physically distributed around the \nworld. Heavily used subtrees can be replicated via mechanisms outside\nof the scope of HTTP while others continue to be served from a\ncentral site.  \n\nBy use of expiration, the real servers can change much like 800#\nassociations can change on a scheduled basis.\n\nSounds worth pursuing further to me.\n\nDave Morris\n\n\n\n", "id": "lists-010-4661669"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": ">> syntax truly extensible, support automated mirroring, support URNs,\n>> etc.), but I haven't considered single-site performance to be one\n>> of them.\n> \n> I think the power of Pauls suggestion is the ability to create a\n> single virtual server which is physically distributed around the \n> world. Heavily used subtrees can be replicated via mechanisms outside\n> of the scope of HTTP while others continue to be served from a\n> central site.  \n\nUmmm, that is what the Web is -- a single virtual server which is\nphysically distributed around the world.  You are just forgetting\nthat the top level in the HTTP hierarchy is the DNS name resolution\nservice.  A fully hierachical name service, such as that proposed\nby the path scheme, is certainly better -- but getting it to work\nefficiently with the existing DNS (or the cost of replicating DNS\nin a separate system) is non-trivial.\n\n> By use of expiration, the real servers can change much like 800#\n> associations can change on a scheduled basis.\n> \n> Sounds worth pursuing further to me.\n\nSounds like the URN problem all over again.  For example, see\n\n  http://www.ics.uci.edu/pub/ietf/uri/draft-ietf-uri-roy-urn-urc-00.txt\n\nand note the similarities.  I am not opposed to fixing that problem,\nbut I don't consider single-site performance to be a concern.\n\nI wouldn't try to solve it in an IETF working group, regardless.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-4669874"}, {"subject": "Re: Unregistered charset values in HTTP 1.1, the ISO-8859* value", "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n\n> > 3.4 Character Sets\n> \n> > Although HTTP allows an arbitrary token to be used as a charset value,\n> > any token that has a predefined value within the IANA Character Set\n> > registry MUST represent the character set defined by that registry.\n> \n> and Olle presented a reading of this that makes it seem like HTTP is\n> permitting unregistered charset values. Frankly, I don't think anyone\n> cares much what this says; it was the case that the MIME documents\n> were in flux. Clearly, using non-standard charset values is\n> non-standard.\n\nI wouldn't go that far. Certainly for experimental\npurposes or within a certain community a character set\nthat still hasn't been registered should be possible to\nuse. But in that case the use of a charset value starting\nwith \"x-\" (and therefore guaranteed to never be\nregistered by IANA) should be required by the HTTP 1.1\nspecification, as it always has been in MIME\nspecifications.\n\nAs an example, among users of the Sami languages (also\ncalled Lappish) in Northern Norway, Sweden and Finland, a\nnew 8-bit character set, similar to ISO 8859-1 but\nincluding also the letters used in Sami that 8859-1\nlacks, was recently defined and it is now tried out in\nemail and on the WWW. \"charset=x-saami\" is used to\nidentify this character set, which is not yet registered\nwith IANA. Though the designers of this new character set\nhopes it will eventually become part 16 of ISO 8859, I\ndon't think the HTTP specification should allow the use\nof \"charset=iso-8859-16\" for this character set, or any\nother, until \"iso-8859-16\" has been registered as a\ncharset value by IANA.\n\nI suggest that the quote from the draft above is\namended by this text:\n\n+ Unregistrered character sets may be used for\n+ experimental purposes or according to the convention of\n+ a certain user community, but a value starting with\n+ \"X-\" or \"x-\" must then be used. Such values are\n+ guaranteed never to be registered by IANA.\n\n> However, most of the comments here do not change the HTTP\n> specification, but only the recommendations for what should happen to\n> the IANA registration. As such, I would assume that the charset\n> registration issues could merged with the consideration of\n> draft-freed-charset-reg-00.txt as BCP, while the HTTP/1.1 spec could\n> be processed.\n\nYes, this should not be allowed to delay the processing\nof HTTP 1.1 further.\n\n> The intent was not to establish a monopoly on \"preferred MIME names\",\n> and not to reinterpret any of the charset registrations, but just to\n> pick the short names that are currently in use in HTTP.\n\nThe HTTP 1.1 draft talks about \"preferred MIME names\",\nthe charset registration draft requires \"a single primary\nname\" if multiple names for the same character set are\nregistered. The current registry has one \"Name:\" field\nfor each character set, and zero or more \"Alias:\" fields.\n\nI suppose that preferred MIME name = primary name = IANA-name.\nIn that case the changes section 19.8.1 in the HTTP draft\nasks for are:\n- register EUC-JP as the primary name for\n  EXTENDED_UNIX_CODE_PACKED_FORMAT_FOR_JAPANESE\n- make US-ASCII the primary name instead of ANSI_X3.4-1968\n- make ISO-8859-1 the primary name instead of ISO_8859-1:1987\n- make ISO-8859-2 the primary name instead of ISO_8859-2:1987\n- make ISO-8859-3 the primary name instead of ISO_8859-3:1988\n- make ISO-8859-4 the primary name instead of ISO_8859-4:1988\n- make ISO-8859-5 the primary name instead of ISO_8859-5:1988\n- make ISO-8859-6 the primary name instead of ISO_8859-6:1987\n- make ISO-8859-7 the primary name instead of ISO_8859-7:1987\n- make ISO-8859-8 the primary name instead of ISO_8859-8:1988\n- make ISO-8859-9 the primary name instead of ISO_8859-9:1989\n\nThese changes would be improvements, in my opinion.\n\nIn the following cases nothing has to be done, because\nthese values are already the primary names of registered\ncharacter sets:\n- ISO-2022-JP\n- ISO-2022-JP-2\n- ISO-2022-KR\n- SHIFT_JIS\n- GB231\n- EUC-KR\n- BIG5\n- KOI-8R\n\n/Olle\n\n-- \nOlle Jarnefors, Royal Institute of Technology (KTH) <ojarnef@admin.kth.se>\n\n\n\n", "id": "lists-010-4679504"}, {"subject": "Re: Unregistered charset values in HTTP 1.1, the ISO-8859* value", "content": "> In the following cases nothing has to be done, because\n> these values are already the primary names of registered\n> character sets:\n> - KOI-8R\n\nKOI8-R\n\n\n\n", "id": "lists-010-4693050"}, {"subject": "Re: Demographic", "content": "Paul Leach:\n[...]\n>It is alleged that some advertisers want to pay content providers, not\n>by the \"hit\", but by the \"nibble\" -- the number of people who actually\n>click on the ad to get more info.\n[...]\n>What I'm looking for are comments on the privacy concerns with such an\n>approach.\n\nI don't think that a two-way referer field can solve the nibble count\nproblem.  For it to work, two-way referer would have to be enabled by\ndefault, but for privacy reasons, it would have to be disabled by\ndefault.  My proposal is to add no extra mechanism, and to rely on\nschemes that embed the referrer in the URI like this:\n \n    http://www.blah.com/index?from=site1\n \nBy having the above URI point to a CGI script which returns a 302\nredirect to the real home page http://www.blah.com/ , this scheme can\nbe made to act in a cache-friendly way, especially if the 302 can be\ncached by proxies which report hits.\n \nIn my opinion, HTTP already supports nibble counting in an adequate way.\nThere is no need to add a new mechanism.  The gains which could be had\nby adding a new --working-- mechanism would not outweigh the cost of the\nmechanism and its introduction.\n \n>Paul J. Leach            Email: paulle@microsoft.com\n\nKoen.\n\n\n\n", "id": "lists-010-4701161"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "> From paulh@imc.org Tue Jul  9 16:56 PDT 1996\n> \n> At 3:47 PM -0700 7/9/96, S.R. Venkatramanan wrote:\n> >> From http-wg-request@cuckoo.hpl.hp.com Tue Jul  9 15:24 PDT 1996\n> >>\n> >> With the Host: header in HTTP/1.1 it is possible to efficiently and\n> >> easily make a single host act as many \"virtual\" Web sites.\n> >>\n> >> What I'd like to see in HTTP/1.2 is the ability to easily and\n> >> efficiently make multiple hosts act like a single virtual Web site, so\n> >> that one can build huge web sites.\n> >\n> >I think this is just an implementation, or rather, a site setup/administration\n> >problem than a protocol defeciency.  How about using a round-robin DNS\n> >to map the hostname to multiple irons, use the power of such multiple\n> >hosts in the cluster to serve nfs mounted files from the same or similar pool.\n> \n> This misses a fair amount of the advantages suggested by Paul. For one,\n> every host in the round-robin would have to have all of the same content;\n\nNot really, if you nfs mount the contents across the hosts in the pool.\n\n> Paul's suggestion allows partial redirection. Thus,\n> http://www.megacorp/root1/... could be redirected to one host that has just\n> that content, while http://www.megacorp/root2/... could be redirected to a\n> different host with different content.\n\nJust like this, megacorp/root1/... files can be on a disk on one host\nand others can mount this filesystem.  megacorp/root2/... can be on a\ndifferent host's disks and can be mounted by other hosts including the\nfirst one holding the files for megacorp/root1/...   I hope you get the\npicture now.\n> \n> Further, Paul's suggestion makes it easier to add redundancy.\n> http://www.megacorp/root1/... could point to two or more hosts that are at\n> very different locations. If host1 is down because of hardware or network\n> failure, a good client would try host2 and so on. Of course, these hosts\n> have to try to have the same content, but it would be much less (and\n> therefore more likely to be the same) than for the entire MegaCorp tree.\n\nThis, again, can be accomplished very easily by nfs.  The automounter map\ncan specify the order in which the hosts holding the contents will be\naccessed and thus any level of redundancy can be built in.\n\nHaving said all these, I should agree with Paul (paulle@microsoft.com)\nin what he said in his followup mail.  i.e., The nature of content placement\nand distribution (NFS) necessitates the data flow thru multiple network\ninterfaces which could be avoided by a protocol spec.  It is just like\nhaving a provision in ftp to direct data transfers between a third host and\nthe initial host, instead of the ones that have control connection setup.\n\nWell, again,  proxie servers take care of most of these situations\nfor static contents....\n\ns.r.\n\nPS:  I am not sure if this discussion is appropriate for this distribution.\n     Larry, pl let me know if it is not and I will shut up.\n\n========================================================\nS.R.Venkatramananraven@Eng.Sun.COM\nStaff Engineer(415)786-9106\nInternet Solutions Engineering2550 Garcia Ave, M/S UMPK16-120\nSun Microsystems, Inc (SunSoft)Mountain View, CA 94043-1100\n\n\n\n", "id": "lists-010-4710013"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "On Wed, 10 Jul 1996, Roy T. Fielding wrote:\n\n> \n> Sounds like the URN problem all over again.  For example, see\n> \n>   http://www.ics.uci.edu/pub/ietf/uri/draft-ietf-uri-roy-urn-urc-00.txt\n> \n> and note the similarities.  I am not opposed to fixing that problem,\n> but I don't consider single-site performance to be a concern.\n> \n> I wouldn't try to solve it in an IETF working group, regardless.\n\nI agree with this. It isn't a HTTP issue but an implementation issue.  I\ncan think of a half dozen solutions for the 'mono-server overload' problem\noff the top off my head. None of the solutions require changing HTTP -\nonly local or DNS configurations. In it simplest form, you could just use\nredirects to bounce people to the 'nearest' or the 'least loaded' server\nautomatically (although simple random distribution probably would work\nquite well for load balancing - *especially* on the big sites.). \n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-4721691"}, {"subject": "Re: Demographic", "content": "On Wed, 10 Jul 1996, Koen Holtman wrote:\n\n> Paul Leach:\n> [...]\n> >It is alleged that some advertisers want to pay content providers, not\n> >by the \"hit\", but by the \"nibble\" -- the number of people who actually\n> >click on the ad to get more info.\n> [...]\n> >What I'm looking for are comments on the privacy concerns with such an\n> >approach.\n> \n> I don't think that a two-way referer field can solve the nibble count\n> problem.  For it to work, two-way referer would have to be enabled by\n> default, but for privacy reasons, it would have to be disabled by\n> default.  My proposal is to add no extra mechanism, and to rely on\n> schemes that embed the referrer in the URI like this:\n>  \n>     http://www.blah.com/index?from=site1\n>  \n> By having the above URI point to a CGI script which returns a 302\n> redirect to the real home page http://www.blah.com/ , this scheme can\n> be made to act in a cache-friendly way, especially if the 302 can be\n> cached by proxies which report hits.\n\n#!/usr/bin/perl\n$_ = $ENV{'PATH_INFO'};\ns#^/##o;\nprint \"Location: $_\\r\\n\\r\\n\";\n\n(Ok - so, I'm depending on the server to add the 302 code. If you want\nto, you can add the one line change).\n\nI use this to count hits on links going *out* from our site. It wouldn't\nbe hard to integrate the functionality into a server for performance.\nAnyway - it doesn't require any changes to HTTP.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-4730968"}, {"subject": "I-D ACTION:draft-ietf-http-v11-spec06.txt, .p", "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\nNote: This revision reflects comments received during the last call period.\n\n       Title     : Hypertext Transfer Protocol -- HTTP/1.1                 \n       Author(s) : R. Fielding, J. Gettys, J. Mogul, \n                   H. Frystyk, T. Berners-Lee\n       Filename  : draft-ietf-http-v11-spec-06.txt, .ps\n       Pages     : 153\n       Date      : 07/09/1996\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol for\ndistributed, collaborative, hypermedia information systems. It is a \ngeneric, stateless, object-oriented protocol which can be used for many \ntasks, such as name servers and distributed object management systems, \nthrough extension of its request methods. A feature of HTTP is the typing \nand negotiation of data representation, allowing systems to be built \nindependently of the data being transferred.   \n                            \nHTTP has been in use by the World-Wide Web global information initiative \nsince 1990. This specification defines the protocol referred to as \n\"HTTP/1.1\".                                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-v11-spec-06.txt\".\n Or \n     \"get draft-ietf-http-v11-spec-06.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-06.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-06.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-06.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-4740344"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "On Tue, 9 Jul 1996, Paul Leach wrote:\n\n> With the Host: header in HTTP/1.1 it is possible to efficiently and\n> easily make a single host act as many \"virtual\" Web sites.\n> \n> What I'd like to see in HTTP/1.2 is the ability to easily and\n> efficiently make multiple hosts act like a single virtual Web site, so\n> that one can build huge web sites.\n> \n> For example,\n> http://www.megasoft.com/\n> could be the root of the whole, HUGE, publically accessible corporate\n> web of the MegaSoft Corp. The mapping of this corporate web onto\n> physical Web Servers should be flixible and mutable over time -- as the\n> site grows, more servers need to be added; and as the content changes,\n> the load on various pieces will change too, necessitating rebalancing\n> the content across the servers, even if more aren't needed.\n> \n> It is _almost_ possible to do this in HTTP/1.1 today. The 302 (Moved\n> Temporarily) response can be used to automatically redirect someone who\n> did a GET on\n> http://www.megasoft.com/very/long/resource/path/because/its/a/big/site.\n> html\n> to\n> http://server42.megasoft.com/its/a/big/site.html\n\n...\n\n> \n> Comments?\n> \n> \n\nI think that much of what you want can be achieved in HTTP/1.1 with\nthe Content-Base header and the use of relative URLs.  For example,\nthe root server might serve just one root document, but it would also\nset the Content-Base to server42.megasoft.com/whatever.  This would\ncause all requests for relative URLs from the root doc to go to\nserver42, which could have been chosen based on current load, client\nlocation, etc.  For a multiple level hierarchy you could repeat this\nwith the addition of using the document requested to pick the\ncontent-base.\n\nE.g. if the root doc contains links to docs A, B, C then server46 must\nbe able to serve these documents.  When it serves doc B, it can set\nthe content-base to server46B which must contain everything linked to\nin document B.\n\nI think this can work fairly well with proxies.  It would be necessary\nto make sure (as much as possible) that the same server is always chosen\nfor a given proxy.  E.g. the \"46\" of server46 might be a function of\nthe client (or proxy) IP address.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-4751161"}, {"subject": "Re: Unregistered charset values in HTTP 1.1, the ISO-8859* value", "content": "Olle Jarnefors wrote:\n> \n> In the following cases nothing has to be done, because\n> these values are already the primary names of registered\n> character sets:\n> ...\n> - GB231\n\nThis should be \"GB2312\".\n\n\n> - KOI-8R\n\nAs somebody else mentioned, this should be \"KOI8-R\".\n\n\nErik\n\n\n\n", "id": "lists-010-4761285"}, {"subject": "Re: Unregistered charset values in HTTP 1.1, the ISO-8859* value", "content": "Just noticed the following error *in* the draft document:\n<URL:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-06.txt>\n\n\n14.45 Warning\n...\nEnglish and the default character set is ISO-8599-1.\n...                                          ^^^^\nIf a character set other than ISO-8599-1 is used, it MUST be encoded in\n                                  ^^^^\n\n\nThat should of course be \"ISO-8859-1\".\n \n - KW\n\n\n\n", "id": "lists-010-4769728"}, {"subject": "RE: Demographic", "content": ">----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Subject: Re: Demographics\n>\n>Paul Leach:\n>[...]\n>>It is alleged that some advertisers want to pay content providers, not\n>>by the \"hit\", but by the \"nibble\" -- the number of people who actually\n>>click on the ad to get more info.\n>[...]\n>>What I'm looking for are comments on the privacy concerns with such an\n>>approach.\n>\n>I don't think that a two-way referer field can solve the nibble count\n>problem.  For it to work, two-way referer would have to be enabled by\n>default, but for privacy reasons, it would have to be disabled by\n>default.\n\nI think you've got this wrong, but it doesn't matter -- I think your\nproposal is cleaner. (Both proposal are just ways that the site declares\nthat it doesn't care if the referer info is disclosed to the target.)\n\n>  My proposal is to add no extra mechanism, and to rely on\n>schemes that embed the referrer in the URI like this:\n> \n>    http://www.blah.com/index?from=site1\n> \n>By having the above URI point to a CGI script which returns a 302\n>redirect to the real home page http://www.blah.com/ , this scheme can\n>be made to act in a cache-friendly way, especially if the 302 can be\n>cached by proxies which report hits.\n\nIf the 302 is cached, then the referrer info will be lost. Unless you\nmean that the cache reports hits on cached 302 responses? I hadn't\nthought about this, but I guess that it would probably fall right out...\nit might be important to be clear that hitcounts should be reported in\nthis case, though.\n\nFor ads that are placed on a lot of sites, this would result in a lot of\ncached 302s for the same underlying page.\n\nHowever, you've made me realize that caches completely break my original\nscheme....\n\nIf fact, \"Referer\" makes it very hard for a cache to be semantically\ntransparent: if a cache was trying to be semantically transparent, the\npresence of \"Referer\" on a GET request should cause a cache to do a\nconditional GET on the Request-URI.  The only way to avoid this would be\nto remember each different value of Referer: had been seen and to report\nhit counts on each of them (somehow). Not only is this more complex than\nusing cached 302s, it also would result in exactly as many remembered\nReferers as cached 302s in your proposal.\n> \n>In my opinion, HTTP already supports nibble counting in an adequate way.\n>There is no need to add a new mechanism.  The gains which could be had\n>by adding a new --working-- mechanism would not outweigh the cost of the\n>mechanism and its introduction.\n\nI agree. I do think it needs to be explained, because it isn't obvious\n(wasn't to me, anyway, so I'll put such an explanation in the draft,\nwith credit to you (if you don't mind).\n\nPaul\n\n\n\n", "id": "lists-010-4778137"}, {"subject": "short names for header", "content": "from minutes, HTTP Working Group, IETF June 96, Montreal\n|Aug 1:   (Leach)   draft on sticky headers, short names for headers, and\n\nCan short names for headers (Good Idea) be compatible with existing practice in\nHTTP/1.x or must it wait until 2.x?  Does allowing aliases for the names of the\nheader fields alter the general message parsing algorithm or message semantics\nas specified in 3.1 of HTTP/1.1?\n\n-marc\n\n-- \n\n\n\n", "id": "lists-010-4789893"}, {"subject": "RE: short names for header", "content": "I'm not sure. I have been thinking about it some, and have the following\npreliminary observations:\n\n1. It could only be used if you knew you were talking to a 1.2\napplication.\n\n2. A 1.2 proxy talking to a 1.1 or earlier application would have to\nconvert.\n\n3. Signatures or hashes would have to be computed on the canonical, full\nname, form.\n\nI haven't seen any showstoppers yet to using it in 1.2. \n\n>----------\n>From: Marc Salomon[SMTP:marc@ckm.ucsf.edu]\n>Sent: Wednesday, July 10, 1996 11:35 AM\n>To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: short names for headers\n>\n>from minutes, HTTP Working Group, IETF June 96, Montreal\n>|Aug 1:   (Leach)   draft on sticky headers, short names for headers, and\n>\n>Can short names for headers (Good Idea) be compatible with existing practice\n>in\n>HTTP/1.x or must it wait until 2.x?  Does allowing aliases for the names of\n>the\n>header fields alter the general message parsing algorithm or message\n>semantics\n>as specified in 3.1 of HTTP/1.1?\n>\n>-marc\n>\n>-- \n>\n>\n\n\n\n", "id": "lists-010-4797358"}, {"subject": "RE: Demographic", "content": "Paul Leach writes:\n...\n > If fact, \"Referer\" makes it very hard for a cache to be semantically\n > transparent: if a cache was trying to be semantically transparent, the\n > presence of \"Referer\" on a GET request should cause a cache to do a\n > conditional GET on the Request-URI.  The only way to avoid this would be\n > to remember each different value of Referer: had been seen and to report\n > hit counts on each of them (somehow). Not only is this more complex than\n > using cached 302s, it also would result in exactly as many remembered\n > Referers as cached 302s in your proposal.\n > > \n\nNot that I'm recommending this per se, but if a server cared about\nthis, it could use the Vary header to indicate the result\nvaries on the Referer request-header, causing any caches to treat the\nresults independently. \n\nOf course this would mean a lot more bookkeeping entries in caches, but\nimplementation-wise, if caches could have unique entries for resources that\nwere logically different but physically identical (using a digest hash\nor something), this wouldn't be that much uglier than some\nspecial case method of keeping track specifically by Referer.\n\n--Shel\n\n\n\n", "id": "lists-010-4807082"}, {"subject": "Re: Demographic", "content": ">It is alleged that some advertisers want to pay content providers, not\n>by the \"hit\", but by the \"nibble\" -- the number of people who actually\n>click on the ad to get more info.\n[...]\n>What I'm looking for are comments on the privacy concerns with such an\n>approach.\n\nI had a series of discussions with the folks like clickshare who are \ntrying to make money from selling demographic data.\n\nMy first approach was to push the referer field - tracking ads was \none of the original applications I had in mind for it. Its a pity that\nthe concern for privacy that has reduced the impact of the referer\nfield was not present when cookies were thrown in. One of the problems\nwith concerns about security, privacy etc is that the criteria being\napplied tend to shift depending on who proposed what. \n\nI think that any discussion about privacy needs to take account of\nthe following realities :-\n\n1) Content costs money to provide. In a capitalist system there must\nbe mechanisms that cover these costs or content won't exist.\n\n2) Vanity publishing and technology research will not continue to \npay for the New York Times etc. indefinitely. A lot of content\nproviders have been prepared to give away content for free just\nto learn the potential of the technology. If we cannot provide\nmechanisms to pay for content then sites will soon start \ndisappearing.\n\n3) The current protocols admit any number of ad-hoc hacks that create\nlinkage. Most of these mean that documents has to be\ncustomized for each reader which in turn means that caching will\nnot work. This model enforces a communication with the host server.\n\n4) Payment for content on a subscription model limits the audience for\na product, it means that the rich inter-linked nature of the\nWeb is lessened. The marginal cost of following a link becomes\nvery substantial. If charging mechanisms are restricted to\nsubscriptions alone the objective of disintermediation, removing\nthe power that Murdoch, Maxwell and their cronies have over\nthe movement of information will be lost. We will only be able\nto buy content that comes from large publishing corporations.\nThere will not be the leavening of small independent \nself-published works.\n\nConsider the reason why the software industry is so inovative.\nA large number of the key ideas come from independents who can\ninovate without the constraints of consistency of corporate\nview that constrains large companies. This keeps the large\ncompanies honest. They have to keep up with the pace set by the\nsmall companies or watch their  market disappear. I want to\nsee that type of competition in news reporting so that the whole\nstory comes out, not just the part that suits the politics of\na newspaper proprietor. Remember the Time Cyberporn article?\nIn that case Time were caught out and they retracted (if \ngrudgingly). Normally the would not have retracted, they publish\nthe facts so the facts are defined to be what they published.\n\n5) Current anonymous payment schemes are not practical for small \npayments. Public key cryptography is barely practical even \nwith amortisation schemes such as payword. Empirical evidence\nsuggests that the holder of key patents has an unfounded\nbelief that electronic commerce cannot take place without him.\n\nConsequently any payment scheme for small or large payments\nis almost guaranteed to NOT provide the unlinkability which is\ntechnically feasible. \n\nTherefore if we reject advertising as a model on the basis\nthat it has privacy problems we are likely to end up with a\nmodel where the purchaser BOTH pays and loses privacy.\n\n\nThe requests that I have had for demographics tracking have had to \ntake account of the following \"needs\".\n\n1) Identify readership\n  a) volume (exposures/hits)\n  b) interest in/purchase of advertised product\n  c) demographic grouping \n\n2) Provide audited measures of above in a manner which is\n  a) consistent across content providers\n  b) commensurate with other measurements from other media.\n\n2b is a key point, the recent Nielssen report recieved criticism from\na certain quarter because it alledgedly overstated the number of\nInternet \"users\". In fact the study had deliberately chosen a\ndefinition of \"user\" which was commensurate with other media, in this\ncase the number of people in a household wich had internet access and\nnot the number of people who had used the internet. Why measure it\nthis way? Well consider a case where you want to buy a new car, you\nknow that your son can get details of prices via the internet, you\nask your son to do the search. In other words the potential audience\noutreach is not necessarily the direct readership. There are similar\nfudge factors for other media (pass on readership of periodicals).\n\nOne important point is that the advertisers are not really interested\nin \"readers\", they are interested in an index that corresponds with the\nmeasures they already use. \n\n\nPhill\n  \n\nPhill\n\n\n\n", "id": "lists-010-4815855"}, {"subject": "RE: Demographic", "content": "Neat observation. Certainly better than some special purpose hack. But\nI'll bet not many caches will try the optiimization you suggested to\navoid duplicate entries.\n\n>----------\n>From: Shel Kaphan[SMTP:sjk@amazon.com]\n>Sent: Wednesday, July 10, 1996 12:19 PM\n>To: Paul Leach\n>Cc: 'koen@win.tue.nl'; 'http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com'\n>Subject: RE: Demographics\n>\n>Paul Leach writes:\n>...\n> > If fact, \"Referer\" makes it very hard for a cache to be semantically\n> > transparent: if a cache was trying to be semantically transparent, the\n> > presence of \"Referer\" on a GET request should cause a cache to do a\n> > conditional GET on the Request-URI.  The only way to avoid this would be\n> > to remember each different value of Referer: had been seen and to report\n> > hit counts on each of them (somehow). Not only is this more complex than\n> > using cached 302s, it also would result in exactly as many remembered\n> > Referers as cached 302s in your proposal.\n> > > \n>\n>Not that I'm recommending this per se, but if a server cared about\n>this, it could use the Vary header to indicate the result\n>varies on the Referer request-header, causing any caches to treat the\n>results independently. \n>\n>Of course this would mean a lot more bookkeeping entries in caches, but\n>implementation-wise, if caches could have unique entries for resources that\n>were logically different but physically identical (using a digest hash\n>or something), this wouldn't be that much uglier than some\n>special case method of keeping track specifically by Referer.\n>\n>--Shel\n>\n\n\n\n", "id": "lists-010-4827802"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "I realize that there is some debate about the necessity of a\nmechanism such as Paul's proposed \"Referral\" header, but I tend\nto agree that it allows things that cannot be done with DNS\nmultiple bindings, NFS cross-mounts, or clever proxies.\n\nIn particular, it looks like it would allow a single logical \"site\"\nto be distributed geographically around the Internet, which might\nbe a Very Good Thing for such potential hot spots as the 1996\nUS presidential election server.\n\nI consider it potentially quite important that Paul's proposal\nallows more than one referral-URI.  This not only allows replication\nfor performance, it also allows replication for fault-tolerance.\nThe use of multiple DNS bindings, for example, doesn't help much\nif one of the actual servers is dead (or your path through the\nInternet to that server is broken).\n\nOn the other hand, Paul's description (\"append [the suffix of\nthe original URI] to one of the referrals\") might not be sufficiently\nwell specified.  I.e., if a client chooses one of the referral-URIs,\nand then after a few successful transactions that server seems to\ndie, should the client re-negotiate with the original server?\nPick another referral-URI?  Keep trying?  The behavior of the\nclient in this case has some potential implications for the\nfailure semantics, especially if the client is updating a\ndatabase at the server end.\n\nPaul's proposed BNF was:\n\nReferral= \"Referral\" \":\" prefix 1#referral-URI\nprefix  = absoluteURI\nreferral= absoluteURI\n\nI assume that the last line should have been:\nreferral-URI= absoluteURI\n\nI'd like to suggest an elaboration, to:\n\nReferral= \"Referral\" \":\" prefix 1#referral-info\nprefix  = absoluteURI\nreferral-info= referral-URI delta-seconds metric\nreferral-URI= absoluteURI\nmetric= 1*DIGIT\n\nThus, a referral would include not only a new URI, but a maximum\nage (\"time-to-live\" in DNS terms) and a cost metric.  The client\nalgorithm would then be\nchoose from the unexpired referral-infos (those whose\ndelta-seconds is less than the Age of the response)\nthe referral-URI with the lowest metric.\nOr maybe that should be \"highest metric\"?  I dunno.\n\nWe could perhaps get rid of the expiration (delta-seconds) parameter if\nwe made it explicit that a referral lasts only as long as the\nCache-control: max-age of the response that carries it.  But\nyou need an expiration mechanism of some sort, or else these bindings\nare impossible to revoke.\n\nWe might want to modify that client algorithm so that \"unexpired\nreferral-info\" includes \"which has not been unresponsive recently\".\nI.e., if a server is playing hard-to-GET, drop it from the list.\n\n-Jeff\n\n\n\n", "id": "lists-010-4837594"}, {"subject": "RE: multihost virtual sites for HTTP 1.", "content": ">----------\n>From: Roy T. Fielding[SMTP:fielding@liege.ICS.UCI.EDU]\n>Sent: Tuesday, July 09, 1996 9:06 PM\n>To: Paul Leach\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: multi-host virtual sites for HTTP 1.2 \n>\n>> What I'd like to see in HTTP/1.2 is the ability to easily and\n>> efficiently make multiple hosts act like a single virtual Web site, so\n>> that one can build huge web sites.\n>\n>Ummm, assuming it is decent server software, the only real limitation\n>on site size right now is the bandwidth of the incoming network.\n\nTrue enough today, but I think it has to be considered short sighted\ngiven the exponential growth rate of the net and the even larger growth\nof \"Intranets\" (organization nets converting to use Internet protocols).\n\nWe have a T3 connection to the net -- 45 megabits/sec. It isn't\nsaturated, but it won't be too long as the net grows exponentially. And\nthen we'll get an OC3... \nWe can put multiple A records in for www.microsoft.com, and that will\nwork as long as all the pages under www.microsoft.com fit conveniently\non one server. At some point it becomes \nthan one server can handle; then we put in the gateway like you propose\nbelow.\n\nBut this is just our public info. What about all the internal pages?\nThey're about 2 orders of magnitude larger in number. I can't put them\non one server's disks, even if I replicate the server to handle the\nload, and I don't want to funnel all my internal accesses through\ngateways, any more than I would want to funnel all my network file\nsystem traffic though gateways. And as I distribute and re-distribute\nthem across multiple servers' disks, I don't want to have to change\ntheir names -- some of them are well known, and the rest still require\nfixing all the links that point at them.\n\nIt is nearly axiomatic in distributed systems design that in order to\nreally scale, you have to be able to replicate and partition and cache.\nThe proposal was a small addition to HTTP to allow one to partition\ntransparently to the names chosen (within a site) without the cost of\n302s for each and every one.\n\n>Splitting requests onto different server machines can be easily\n>accomplished using a very fast gateway box which sits on the trunk\n>and routes requests to the internal servers for processing.  The only\n>thing the gateway would need to do is look for the beginning of each\n>message and multiplex from there (which is more difficult than\n>IP routing, but not much more).\n\nIt needs to do more than just look at the front of each message -- it\nalso needs to forward the request and relay the response. At full load,\n>it would need to forward 90 megabits/sec to handle our _current_ site. Any\nsingle point will eventually become a bottleneck as the load increases.\nAt some point, I have to replicate the gateway to handle the load.\nSoftware that implements a protocol to avoid all that is lots cheaper.\nPlus it lets me move pages within my site with changing all the links to\n>the pages.\n>\n>I think that would be a lot easier than changing HTTP to accomodate\n>complex URL rewriting rules.\n\nThey're not that complex -- just substitute one prefix for another. But\nOK, it would be better if no rewriting rules were needed. You've\nprompted me to think of a slightly different scheme where they aren't\nneeded: the referral header just has a prefix and a new hostname to use\nfor all Request-URIs that start with that prefix; the Request-URI and\nthe Host sent to that new hostname are left unchanged from the original\nrequest. The servers are configured to know what subtree(s) of the name\nspace they own, and look up in their local file system relative to that.\n\n>  There are good reasons to support a\n>URI rewriting mechanism on the client side (e.g., to make the URI\n>syntax truly extensible, support automated mirroring, support URNs,\n>etc.), but I haven't considered single-site performance to be one\n>of them.\n\nAll of the arguments in your message apply equally well to the DNS --\nwhy does it have referrals?\nIt could have just used CNAME -- that's the analogy to the way HTTP uses\nof links to glue together the distributed servers.\n\nDitto for AFS and X.500.\n\nPaul\n\n\n\n", "id": "lists-010-4848061"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "> I'd like to suggest an elaboration, to:\n> \n> Referral= \"Referral\" \":\" prefix 1#referral-info\n> prefix  = absoluteURI\n> referral-info= referral-URI delta-seconds metric\n> referral-URI= absoluteURI\n> metric= 1*DIGIT\n> \n> Thus, a referral would include not only a new URI, but a maximum\n> age (\"time-to-live\" in DNS terms) and a cost metric.  The client\n> algorithm would then be\n> choose from the unexpired referral-infos (those whose\n> delta-seconds is less than the Age of the response)\n> the referral-URI with the lowest metric.\n> Or maybe that should be \"highest metric\"?  I dunno.\n> \n> We could perhaps get rid of the expiration (delta-seconds) parameter if\n> we made it explicit that a referral lasts only as long as the\n> Cache-control: max-age of the response that carries it.  But\n> you need an expiration mechanism of some sort, or else these bindings\n> are impossible to revoke.\n> \n> We might want to modify that client algorithm so that \"unexpired\n> referral-info\" includes \"which has not been unresponsive recently\".\n> I.e., if a server is playing hard-to-GET, drop it from the list.\n\nSomething like this seems like a reasonable idea. This does wander over\ninto the ground that URNs hoped to cover, but it makes the server responsible\nfor distributing it's own replication information, and so avoids the\nwider infastructure questions.\n\nI think the most important thing if something like this is to be useful\nis to have an explicit model of the semantics that mandates some things,\nsuggests others, and explicitly leaves others up to implementations. If people\nstart with a vauge model and apply lots of huristics, interoperability\ncould suffer.\n\nThe gopher developers tried to introduce server replication at one point, but\nit was not widely implemented, perhaps because the number of servers\nseeing single-server overload was smaller. (Though the need for server\nreplication has been evident since the onset of archie, and people have\nbeen trying to offer better solutions.)\n\nIn the case of a widely geographically distributed resource (like say\ninfo-mac or rfc mirror sites), a cost metric could only serve as\na first approximation to what a client (or proxy) might determine\nemperically were the actual network costs. Though, like MX record\npriorities, it might still represent a server-side preference\namong alternatives that seem to have equal utility to the client.\n\n(Which makes it important to define whose \"cost\" is in the model.)\n\nI have visions on multinomal logit choice models dancing in my head,\nbut I suppose that gets to be a bit much for a poor web client\nto deal with on a per-site basis....\n\nBut it does have one property I like: I'd say rather than just say\n\"choose the referal info with the best cost metric\" I'd like\nto say that clients should choose in a quasi-random way among\noptions with an equal cost metric.  (What they do in the case\nof unequal costs needs to be refined further to define what\nthis would mean.)\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n", "id": "lists-010-4861075"}, {"subject": "Re: short names for header", "content": "> from minutes, HTTP Working Group, IETF June 96, Montreal\n> |Aug 1:   (Leach)   draft on sticky headers, short names for headers, and\n> \n>Can short names for headers (Good Idea) be compatible with existing practice in\n>HTTP/1.x or must it wait until 2.x?  Does allowing aliases for the names of the\n>header fields alter the general message parsing algorithm or message semantics\n>as specified in 3.1 of HTTP/1.1?\n\nYes, it does -- changing existing header field names (i.e., short names or\ntokenizing) can only be done in HTTP/2.x.  Besides, it doesn't make any\nsense to introduce short names now when the GOAL should be a multiplexed\nand tokenized grammar for HTTP/2.0, and that can be accomplished in six\nmonths if people don't waste their time on minor tweaks that are equally\nincompatible with 1.x.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-4871857"}, {"subject": "RE: multihost virtual sites for HTTP 1.", "content": ">----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>\n>I consider it potentially quite important that Paul's proposal\n>allows more than one referral-URI.  This not only allows replication\n>for performance, it also allows replication for fault-tolerance.\n>The use of multiple DNS bindings, for example, doesn't help much\n>if one of the actual servers is dead (or your path through the\n>Internet to that server is broken).\n>\n>On the other hand, Paul's description (\"append [the suffix of\n>the original URI] to one of the referrals\") might not be sufficiently\n>well specified.\n\nAgreed. I promised Larry to get some indication that I wanted this out\nwithin a week of the IETF, so not every detail was perfect. Of course\nwith more time it would have been :-).\n\n>  I.e., if a client chooses one of the referral-URIs,\n>and then after a few successful transactions that server seems to\n>die, should the client re-negotiate with the original server?\n>Pick another referral-URI?  Keep trying?  The behavior of the\n>client in this case has some potential implications for the\n>failure semantics, especially if the client is updating a\n>database at the server end.\n\nGood point.\n>\n>Paul's proposed BNF was:\n>\n>Referral= \"Referral\" \":\" prefix 1#referral-URI\n>prefix  = absoluteURI\n>referral= absoluteURI\n>\n>I assume that the last line should have been:\n>referral-URI= absoluteURI\n\nYes.\n>\n>I'd like to suggest an elaboration, to:\n>\n>Referral= \"Referral\" \":\" prefix 1#referral-info\n>prefix  = absoluteURI\n>referral-info= referral-URI delta-seconds metric\n>referral-URI= absoluteURI\n>metric= 1*DIGIT\n\nI was thinking about this too. I didn't want to complicate the original\nproposal.\n>\n>Thus, a referral would include not only a new URI, but a maximum\n>age (\"time-to-live\" in DNS terms) and a cost metric.\n\nI thought the TTL would be in Expires header for the cached 302 response\nin which this was returned.\n\n>  The client\n>algorithm would then be\n>choose from the unexpired referral-infos (those whose\n>delta-seconds is less than the Age of the response)\n>the referral-URI with the lowest metric.\n>Or maybe that should be \"highest metric\"?  I dunno.\n\nYou probably want to spread the requests out among the referral-URIs in\nproportion to the cost; otherwise everyone will jump on the lowest cost\nsite and it will get overloaded.\n\nIt turns out this is exactly was the proposal for SRV RRs in DNS does,\nwhich is why I dithered about having whether it was better to use DNS.\nAlso, there's another proposal for location RRs in DNS (I think the name\nis LOC) that include longitude and latitude -- not perfect, as all the\nbackbone suppliers tell me, but certainly good enough to pick a server\non the same continent in preference to one on a different continent. If\nit makes sense to have them in the DNS, it makes sense to have them\nhere, too -- or to just use the DNS. The problem with the latter is that\nSRV and LOC aren't standards, I don't know if they will become\nstandards, and even if they do it takes a while to get them into the\n\"official\" DNS implementation.\n\nMaybe a DNS guru can comment? Or a friend of one can forward it along?\nI'd hate to duplicate the functionality.\n>\n>We could perhaps get rid of the expiration (delta-seconds) parameter if\n>we made it explicit that a referral lasts only as long as the\n>Cache-control: max-age of the response that carries it.  But\n>you need an expiration mechanism of some sort, or else these bindings\n>are impossible to revoke.\n\nRight. Expires or max-age seemed like a good thing to re-use.\n>\n>We might want to modify that client algorithm so that \"unexpired\n>referral-info\" includes \"which has not been unresponsive recently\".\n>I.e., if a server is playing hard-to-GET, drop it from the list.\n\nAll good suggestions.\n\nI wanted to see if there was enough interest to produce a real I-D.\nSounds like there is. Any suggestions received to date will be\nincorporated; as will answers to as many objections as I can --\nincluding future ones received as I'm writing, so keep them coming.\n\nThanks,\n\nPaul\n\n\n\n", "id": "lists-010-4880188"}, {"subject": "Re: Unregistered charset values in HTTP 1.1, the ISO-8859* value", "content": "> I wouldn't go that far. Certainly for experimental\n> purposes or within a certain community a character set\n> that still hasn't been registered should be possible to\n> use. But in that case the use of a charset value starting\n> with \"x-\" (and therefore guaranteed to never be\n> registered by IANA) should be required by the HTTP 1.1\n> specification, as it always has been in MIME\n> specifications.\n\nNo. The MIME specifications have always been broken for that reason.\nRFC 1521 required behavior that no implementation ever obeys --\nthat being a check of the IANA registry before transmission of a token \nwhich is not prefixed by \"x-\".  Requirements which serve no useful purpose,\nand which are directly contradicted by all known implementations, do not\nbelong in any RFC.\n\nEven if such a requirement were added, it is poor engineering to implement\na system that relies on \"flag days\" in which to switch from existing use\nof an x-token to use of its registered non-x equivalent.  The result is that\na good implementation treats any \"x-\" prefix as if it didn't exist, so that\nthe same application won't die when somebody eventually gets around to\nregistering that particular token.  In other words, a good implementation\ndoes not distinguish between registered and non-registered tokens, and at\nthat point there is no reason to use the \"x-\" prefix for anything.\n\nOur experience with \"x-\" prefixes has been uniformly bad; rather than\nencouraging registration of all tokens, they encourage the easy route\nof just using the x-token as the standard for that type.  I see no reason\nto further encourage their use.  IANA should certainly continue its\nnormal behavior of never registering \"x-\" prefixed tokens, but that\nregistration process is outside the scope of HTTP.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-4893158"}, {"subject": "Re: short names for header", "content": "rom the Montreal minutes suggestion for a new charter's milestones:\n\n> Aug 1:   (Leach)   draft on sticky headers, short names for headers, and\n>       context identifiers\n\nI suggest dropping this as a product of the working group.  Although it\nis certainly possible to compress the protocol through two adjacent\ncommunicating parties by the addition of stateful interaction (what is\nmeant by sticky headers and context identifiers) and tokenizing protocol\nelements (a small part of which is covered by short names for headers),\nit is not a trivial task and needs to be extremely careful with regard\nto proxy-proxy communication with requests from multiple user agents\nbeing interleaved.  This falls into the category of \"research\" and\nshould be investigated outside the standards arena.\n\nIn other words, I don't think this task can be accomplished within the\nlifetime of this WG. Furthermore, even if it were accomplished, our long\nterm (within a year) plans for HTTP would end up replacing it by the\nmultiplexing and tokenizing of HTTP/2.x.  Therefore, I think it would be\nbetter to focus within the WG on things that can be done before the\nnext IETF meeting, and encourage other efforts to focus on the research\nwork that already needs doing for multiplexing and tokenizing of HTTP/2.x.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-4903432"}, {"subject": "Re: Demographic", "content": "Paul Leach:\n>[Koen Holtman:]\n>>  My proposal is to add no extra mechanism, and to rely on\n>>schemes that embed the referrer in the URI like this:\n>> \n>>    http://www.blah.com/index?from=site1\n>> \n>>By having the above URI point to a CGI script which returns a 302\n>>redirect to the real home page http://www.blah.com/ , this scheme can\n>>be made to act in a cache-friendly way, especially if the 302 can be\n>>cached by proxies which report hits.\n>\n>If the 302 is cached, then the referrer info will be lost. Unless you\n>mean that the cache reports hits on cached 302 responses?\n\nYes, this is what I mean.\n\n>Paul\n\nKoen.\n\n\n\n", "id": "lists-010-4912323"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "Another set of thoughts about the replication proposal and my\ncall for explicit semantics... an area where the details get\npotentially sticky is conditional requests, and caching.\n\n(Another areas is POSTs: but the site can avoid ambigities if the\nwould cause problems by not replicating, say, their cgi directories.)\n\nMy first reaction was that maybe the problems could be solved by\nusing only GET and HEAD across the replicated servers, but I\ncan still see some concerns.\n\nThe concerns are less acute when all the replicated servers\nshare a networked file system, like NFS, DFS, ASF, etc. but there\nstill might be annoying race conditions. It gets more goofy\nwhen the replicated servers are mirrored, say, by a nightly\nupdate via ftp and cron jobs.\n\nIf I do something like a conditional GET with an if-modified-since\non a particular server, the result depends on the state of that\nserver but maybe not on changes at another site that hasn't been\nreplicated.\n\nThere might be some way to use the HTTP/1.1 semantics along with\ninformation about the replication schedule to return consistent\nreplies, but I don't yet understand them all well enough to comment.\n\nAnother approach is to say that this is all a meta-protocol\nlayered on top of caching and so forth. But this would loose\nthe potential bentifit that a smart cache might be able to\nreturn any of the replicas.\n\nSomeone who understands the caching in HTTP/1.1 better than I\nmight be able to unravel this further. In any case, this is an\narea where I'd suggest some explicit guidelines be attached to\nthe proposal.\n\n\n\n", "id": "lists-010-4920354"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "On the referer question :-\n\nA lot of the content providers asked me if there could be a mechanism to \nidentify when a hit had come from a person's hotlist. I don't see this as \ndifficult, we simply need to define a conventional URI for an (unnamed) person's \nhotlist. eg. URI:hotlist. \n\nPhill\n\n\n\n", "id": "lists-010-4928886"}, {"subject": "Remapping charset", "content": "In few last messages regarding charsets, I was to notice that many troubles\ncould be handled via some character remapping mechanism.\n\nHow this should work is as follows:\n\n-let's suppose that browser cannot accept document charset, then\ndocument should be translated into charset which is supported\nby browser, but lest likely to loose some characters\n\n(Example: with CP-1250 and iso-8859-2 there are only some\n  swapped blocks of chars, says Dave, so then there should\n  never be a loss in conversion, but we don't need to have\n  both, duplicate versions of documents -- one for UN*X\n  machines, and one for Wind*ws CEE [and still more for\n  other platforms ...]\n)\n\n- conversion mechanism is simple transliteration, different codes are\n  being assigned for the same characters\n\nMirsad Todorovac\n\n\n\n", "id": "lists-010-4936618"}, {"subject": "RE: Demographic", "content": "I have been following this thread for the past couple of days and being from a company that makes its living from measurement and  analysis on access information of most of the major sites on the internet I can assure you that the refer field is of the highest importance to advertisers and sites.  I agree with phil that we live in a capitalistic society where advertising pays for a great many things or subsidizes them.  For example magazines and newspapers.  I do agree that privacy is a concern on the Internet but if we as technologist do not provide a solution to content providers that will provide this information then sites and advertisers will just figure a way to hack around it.  You see this already with sites like yahoo that redirect back to there sites when people leave.  This is a performance hit that yahoo is willing to accept and apparently so is the consumer.\nI guess I am a little confused as to why people are concerned with the privacy issue.  Please don't shoot me I just maybe na?ve here.  If I come from site A to site B and use the refer as a way for site B to know that I came to it how does site B know how I am.  As far as a know there is no personal information stored in the request from site A to site B or is this wrong?  My question is one of are we being to paranoid about the information?  I personal think that we could actually provide a mechanizes to give more information now and still create no privacy issues.  -------------------------------------Gene TrentDir. of TechnologyI/PRO785 Market St. 13th floorSan Francisco, Ca 94103PH: 415-976-8627FAX 415-975-8617Pager 415-208-4382URL www.ipro.com----------From:  hallam@Etna.ai.mit.edu[SMTP:hallam@Etna.ai.mit.edu]Sent:  Wednesday, July 10, 1996 12:57 PMTo:  Paul Leach; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.comCc:  hallam@Etna.ai.mit.eduSubject:  Re: Demographics  >It is alleged that some advertisers want to pay content providers, not>by the \"hit\", but by the \"nibble\" -- the number of people who actually>click on the ad to get more info.[...]>What I'm looking for are comments on the privacy concerns with such an>approach.I had a series of discussions with the folks like clickshare who are trying to make money from selling demographic data.My first approach was to push the referer field - tracking ads was one of the original applications I had in mind for it. Its a pity thatthe concern for privacy that has reduced the impact of the refererfield was not present when cookies were thrown in. One of the problemswith concerns about security, privacy etc is that the criteria beingapplied tend to shift depending on who proposed what. I think that any discussion about privacy needs to take account ofthe following realities :-1) Content costs money to provide. In a capitalist system there mustbe mechanisms that cover these costs or content won't exist.2) Vanity publishing and technology research will not continue to pay for the New York Times etc. indefinitely. A lot of contentproviders have been prepared to give away content for free justto learn the potential of the technology. If we cannot providemechanisms to pay for content then sites will soon start disappearing.3) The current protocols admit any number of ad-hoc hacks that createlinkage. Most of these mean that documents has to becustomized for each reader which in turn means that caching willnot work. This model enforces a communication with the host server.4) Payment for content on a subscription model limits the audience fora product, it means that the rich inter-linked nature of theWeb is lessened. The marginal cost of following a link becomesvery substantial. If charging mechanisms are restricted tosubscriptions alone the objective of disintermediation, removingthe power that Murdoch, Maxwell and their cronies have overthe movement of information will be lost. We will only be ableto buy content that comes from large publishing corporations.There will not be the leavening of small independent self-published works.a, in thiscase the number of people in a household wich had internet access andnot the number of people who had used the internet. Why measure itthis way? Well consider a case where you want to buy a new car, youknow that your son can get details of prices via the internet, youask your son to do the search. In other words the potential audienceoutreach is not necessarily the direct readership. There are similarfudge factors for other media (pass on readership of periodicals).One important point is that the advertisers are not really interestedin \"readers\", they are interested in an index that corresponds with themeasures they already use. Phill  Phill\n\n\n\n", "id": "lists-010-4944267"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "Paul Leach writes:\n\n| Maybe a DNS guru can comment? Or a friend of one can forward it along?\n| I'd hate to duplicate the functionality.\n\nI'm not a DNS guru, but here's my two new pence's worth anyway...\n\nThe docs in question are: draft-gulbrandsen-dns-rr-srvcs-03.txt &\nRFC 1876, by the way.\n\nLOC is supported in BIND 4.9.4, which is the current production\nrelease.  SRV has been slated for BIND 4.9.5, which is meant to be\ncoming out later this year.  One of the co-authors, Paul Vixie, is\nalso the BIND maintainer.  For more info, check out the Internet\nSoftware Consortium Web pages at <URL:http://www.isc.org/isc/>\n\nOK, so SRV can be expected to see the light of day quite soon in\nthe most widely used DNS implementation.  Can I interest any\nbrowser and/or proxy authors in taking advantage of it ?  Methinks\nadded resilience via SRV would be a good selling point ... :-)\n\nMartin\n\nPS There was also something about putting SRV up for Proposed\nStandard\n\n\nielding[SMTP:fielding@liege.ICS.UCI.EDU]\n>Sent: Wednesday, July 10, 1996 8:17 PM\n>To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Cc: Larry Masinter\n>Subject: Re: short names for headers \n>\nFrom the Montreal minutes suggestion for a new charter's milestones:\n>\n>> Aug 1:   (Leach)   draft on sticky headers, short names for headers, and\n>>       context identifiers\n>\n>I suggest dropping this as a product of the working group.  Although it\n>is certainly possible to compress the protocol through two adjacent\n>communicating parties by the addition of stateful interaction (what is\n>meant by sticky headers and context identifiers) and tokenizing protocol\n>elements (a small part of which is covered by short names for headers),\n>it is not a trivial task and needs to be extremely careful with regard\n>to proxy-proxy communication with requests from multiple user agents\n>being interleaved.  This falls into the category of \"research\" and\n>should be investigated outside the standards arena.\n>\n>In other words, I don't think this task can be accomplished within the\n>lifetime of this WG. Furthermore, even if it were accomplished, our long\n>term (within a year) plans for HTTP would end up replacing it by the\n>multiplexing and tokenizing of HTTP/2.x.  Therefore, I think it would be\n>better to focus within the WG on things that can be done before the\n>next IETF meeting, and encourage other efforts to focus on the research\n>work that already needs doing for multiplexing and tokenizing of HTTP/2.x.\n>\n> ...Roy T. Fielding\n>    Department of Information & Computer Science    (fielding@ics.uci.edu)\n>    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n>    http://www.ics.uci.edu/~fielding/\n>\n>\n\n\n\n", "id": "lists-010-4957545"}, {"subject": "RE: multihost virtual sites for HTTP 1.", "content": "If you're not convinced that you want that large a (virtual) site, OK by\nme. But I liked your other reasons for referrals, too:\n\n[Roy said:]\nThere are good reasons to support a\nURI rewriting mechanism on the client side (e.g., to make the URI\nsyntax truly extensible, support automated mirroring, support URNs,\netc.), but I haven't considered single-site performance to be one\nof them.\n\nAdministrative flexibility and distributed sites were also mentioned as\nother good uses.\n\nMore comments below.\n\n>----------\n>From: Roy T. Fielding[SMTP:fielding@liege.ICS.UCI.EDU]\n>Subject: Re: multi-host virtual sites for HTTP 1.2 \n>\n>Well, I still don't like it -- I see no reason for such a huge site\n>to be located behind a single DNS name.  The top URLs can be on the\n>main site (or just redirections from that site) with all other\n>references relative from there.  I don't see why HTTP needs to \n>replicate the work of DNS.\n\nIt doesn't. It is applying the same principles used by DNS to the part\nof the HTTP URL that DNS doesn't own, for the same reasons as DNS uses\nthem in its part of the URL.\n>\n>In any case, please be sure to make the syntax parsable\n>\n>>>Referral= \"Referral\" \":\" prefix 1#referral-URI\n>>>prefix  = absoluteURI\n>>>referral-URI= absoluteURI\n>\n>can't be parsed because \",\" is valid in absoluteURI.  In other words,\n>you need to use <> or \"\" as delimiters.\n\nOK. Thanks for pointing it out.\n>\n>Finding a way to do it with multiple Link header fields would be better.\n\nSo, would you suggest something like:\nLink: Referral <absolute-URI-1>\nLink: Referral <absolute-URI-2>\n...\n\nAnd why do you like it better? It seems weird to me, but in the way that\nsometimes says to me that it's because my intuition hasn't been trained\nproperly yet.\n\nPaul\n\n\n\n", "id": "lists-010-4968191"}, {"subject": "Re: Demographic", "content": "Perhaps we need to address the following questions separately:-\n\n1) What is the information that the Advertisers/Auditors want?\n   I see this as beeing at two levels,\n\n   1) What is the information they want to obtain?\n   2) What measurements do they want to capture to achieve (1)\n\n   Referer comes under (2) in my view, as do most protocol level\n   hacks. The important question under (1) is \"what financial \n   value will an advertiser recive from advertising at a site?\"\n\n2) What mechansims are needed to transport information captured \nunder (1) \n\nNote that in my Log file format and Proxy notification specs I \nworked mainly on the second issue and tried to make the transport\nmechanism as separate as possible from the issue of what must be\ntransported.\n\nA second scenario I think that the Newspapers in particular should\nconsider. Say I am a comuter who wants to download the Times each\nnight and pick up a loaded laptop on the way out to work. In that case\nit would be very usefull for the laptop to inform the originator what\nmaterial was actually read the next time a download was taken.\n\nThis is a material benefit to the user - the newspaper can discover\nwhat articles it publishes are important to readers and which they \ndon't bother reading. \n\n[Yep Privacy issues up the Wazzo, privacy is only one ethical dimension \nhowever]\n\nPhill\n\n\n\n", "id": "lists-010-4978342"}, {"subject": "chopping lis", "content": "OK, here's a list of sections (drawn from the v11-06 draft) which\n*I* think might perhaps more properly belong in an implementors\nguide.  You will probably disagree with me :-)\n\nTo avoid a flamewar on the list...  Can I suggest that anyone with\nstrong opinions about which parts should be in the base HTTP spec\nplease sit down with a strong cup of coffee and go over the draft\nbefore posting anything?  Thanks!\n\nYou might also consider mailing me first - I'll summarise any\ncomments I receive to the list.\n\nThe bottom line seems to be...\n\n  1. this cuts the spec down quite a bit, but perhaps at the\n        cost of some things which really do need to be in it?\n        These can be extracted and put back in, of course\n\n  2. still (IMHO) some scope for trimming the remaining bits\n        of the spec, but not on a whole section basis?\n\n  3. MUSTs and SHOULDs aplenty - what is their status in an\n        implementation guide.  Advisory?\n\nMartin\n\n\n\n8 Connections.............................................41\n 8.1 Persistent Connections ..............................41\n  8.1.1 Purpose ..........................................41\n  8.1.2 Overall Operation ................................42\n  8.1.3 Proxy Servers ....................................43\n  8.1.4 Practical Considerations .........................43\n 8.2 Message Transmission Requirements ...................44\n\n12 Content Negotiation....................................64\n 12.1 Server-driven Negotiation ..........................65\n 12.2 Agent-driven Negotiation ...........................66\n 12.3 Transparent Negotiation ............................66\n\n13 Caching in HTTP........................................67\n  13.1.1 Cache Correctness ...............................68\n  13.1.2 Warnings ........................................69\n  13.1.3 Cache-control Mechanisms ........................70\n  13.1.4 Explicit User Agent Warnings ....................70\n  13.1.5 Exceptions to the Rules and Warnings ............70\n  13.1.6 Client-controlled Behavior ......................71\n 13.2 Expiration Model ...................................71\n  13.2.1 Server-Specified Expiration .....................71\n  13.2.2 Heuristic Expiration ............................72\n  13.2.3 Age Calculations ................................72\n  13.2.4 Expiration Calculations .........................75\n  13.2.5 Disambiguating Expiration Values ................75\n  13.2.6 Disambiguating Multiple Responses ...............76\n 13.3 Validation Model ...................................77\n  13.3.1 Last-modified Dates .............................77\n  13.3.2 Entity Tag Cache Validators .....................78\n  13.3.3 Weak and Strong Validators ......................78\n  13.3.4 Rules for When to Use Entity Tags and Last-modified Dates\n   .......................................................80\n  13.3.5 Non-validating Conditionals .....................81\n 13.4 Response Cachability ...............................82\n 13.5 Constructing Responses From Caches .................82\n  13.5.1 End-to-end and Hop-by-hop Headers ...............83\n  13.5.2 Non-modifiable Headers ..........................83\n  13.5.3 Combining Headers ...............................84\n  13.5.4 Combining Byte Ranges ...........................84\n 13.6 Caching Negotiated Responses .......................85\n 13.7 Shared and Non-Shared Caches .......................86\n 13.8 Errors or Incomplete Response Cache Behavior .......86\n 13.9 Side Effects of GET and HEAD .......................86\n 13.10 Invalidation After Updates or Deletions ...........87\n 13.11 Write-Through Mandatory ...........................87\n 13.12 Cache Replacement .................................88\n 13.13 History Lists .....................................88\n\n15 Security Considerations...............................130\n 15.1 Authentication of Clients .........................130\n 15.2 Offering a Choice of Authentication Schemes .......131\n 15.3 Abuse of Server Log Information ...................132\n 15.4 Transfer of Sensitive Information .................132\n 15.5 Attacks Based On File and Path Names ..............133\n 15.6 Personal Information ..............................133\n 15.7 Privacy Issues Connected to Accept Headers ........134\n 15.8 DNS Spoofing ......................................134\n 15.9 Location Headers and Spoofing .....................135\n\n19 Appendices............................................141\n 19.3 Tolerant Applications .............................142\n 19.4 Differences Between HTTP Entities and RFC 1521 Entities   143\n  19.4.1 Conversion to Canonical Form ...................143\n  19.4.2 Conversion of Date Formats .....................144\n  19.4.3 Introduction of Content-Encoding ...............144\n  19.4.4 No Content-Transfer-Encoding ...................144\n  19.4.5 HTTP Header Fields in Multipart Body-Parts .....144\n  19.4.6 Introduction of Transfer-Encoding ..............144\n  19.4.7 MIME-Version ...................................145\n 19.5 Changes from HTTP/1.0 .............................145\n  19.5.1 Changes to Simplify Multi-homed Web Servers and Conserve IP\n  Addresses .............................................145\n 19.6 Additional Features ...............................146\n  19.6.1 Additional Request Methods .....................146\n  19.6.2 Additional Header Field Definitions ............148\n 19.7 Compatibility with Previous Versions ..............150\n  19.7.1 Compatibility with HTTP/1.0 Persistent Connections151\n\n\n\n", "id": "lists-010-4986224"}, {"subject": "Re: Remapping charset", "content": "Mirsad Todorovac wrote:\n> \n> -let's suppose that browser cannot accept document charset, then\n> document should be translated into charset which is supported\n> by browser, but lest likely to loose some characters\n> \n> (Example: with CP-1250 and iso-8859-2 there are only some\n>   swapped blocks of chars, says Dave, so then there should\n\nNot only swapped blocks. In ISO 8859-2 positions from 129 to 159\nare not used. CP1250 uses all 256 characters.\n\n>   never be a loss in conversion, but we don't need to have\n>   both, duplicate versions of documents -- one for UN*X\n>   machines, and one for Wind*ws CEE [and still more for\n>   other platforms ...]\n\nThis is an implementation question and does not belong here really.\nHowever, I think it's good to point out these things from time to time\nbecause some (most?) implementors don't know about it.\n\n-- \nLife is a sexually transmitted disease.\n\ndave@fly.cc.fer.hr\ndave@zemris.fer.hr\n\n\n\n", "id": "lists-010-4998260"}, {"subject": "Confusion about Age: accuracy vs. safet", "content": "    > This change (only requiring Age on responses taken from a cache)\n    > breaks the clock-deskewing properties of the original Age design.\n    > The problem comes when a pre-1.1 cache is in the path and has\n    > held onto the response for a while.  It's safest to add the Age\n    > header as soon as possible, since it reduces the chances that\n    > a skewed clock could result in incorrect comparisons.\n    \n    That is incorrect -- in fact, adding an Age header onto a response\n    received from somewhere else can only make the algorithm less accurate.\n    The reason is because the outbound requester is already going to add\n    the amount of time it takes for the response to arrive, including any\n    time spent by the cache in question.  Therefore, adding an Age header\n    onto a response retrieved by a further network request will double\n    the amount of Age that the outbound requester will observe,\n    for no reason whatsoever, and guaranteeing that the Age calculation\n    will be wrong.\n\nI didn't say that adding the Age header early, rather than late,\nmakes the Age value more accurate.  It doesn't.\n\nI said it makes it *safer*.  Which is to say, if there are any\npotential errors in the system, either invisible caches or hosts with\noffset clocks, it is safest to err on the side of overestimating\nthe Age, not underestimating it.  This means that a cache will be\nless likely to declare a response \"fresh\" when it is actually \"stale\",\nwhich I have consistently argued is far more harmful than the\nalternative (having to contact the server for an entry that might\ntechnically be \"fresh\").  The latter can happen anyway if the cache\nhas insufficient space, so it's an error we can live with.\n\nThe algorithm in the HTTP/1.1 specification tries to not to\nunderestimate the Age.  This is intentional, and it is neither\n\"incorrect\" nor \"for no reason whatsoever.\"\n\n-Jeff\n\n\n\n", "id": "lists-010-5006639"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "    But it does have one property I like: I'd say rather than just say\n    \"choose the referal info with the best cost metric\" I'd like\n    to say that clients should choose in a quasi-random way among\n    options with an equal cost metric.  (What they do in the case\n    of unequal costs needs to be refined further to define what\n    this would mean.)\n\nI agree; we should include an explicit recommendation to\nrandomly choose between equal-metric referrals.  But we\nneed to think a bit harder about whether this choice should\nbe made per-request or \"per session\", however one defines\nthat.  I suspect that if clients do it per-request, this \nwill play havoc with caches and with updates.\n\nBut I would also expect servers to offer referral-lists that\ninclude referrals of quite different metrics.  This would\nbe used to tell a client, for example, to use the \"nearest\"\nsite as long as it was up, but to fail over to a more distant\nsite if the preferred one became unreachable.  It's not\ninconceivable that a server could offer different referral-lists\nto different clients, depending on each client's location in the\nInternet topology.\n\n-Jeff\n\n\n\n", "id": "lists-010-5016188"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "Paul Hoffman writes in <v03007607ae08a187ad5b@[165.227.113.247]>:\n>This misses a fair amount of the advantages suggested by Paul. For one,\n>every host in the round-robin would have to have all of the same content;\n>Paul's suggestion allows partial redirection. Thus,\n>http://www.megacorp/root1/... could be redirected to one host that has just\n>that content, while http://www.megacorp/root2/... could be redirected to a\n>different host with different content.\n>\n>Further, Paul's suggestion makes it easier to add redundancy.\n>http://www.megacorp/root1/... could point to two or more hosts that are at\n>very different locations. If host1 is down because of hardware or network\n>failure, a good client would try host2 and so on. Of course, these hosts\n>have to try to have the same content, but it would be much less (and\n>therefore more likely to be the same) than for the entire MegaCorp tree.\n\nThis feels to me like an extension of the UNIX filesystem paradigm to Web \ndocument hierarchies (\"The UNIX Time-Sharing System\", D. M. Ritchie and K. \nThompson, The Bell System Technical Journal, July-August 1978):\n\n     \"A hierarchial filesystem incorporating demountable volumes\"\n\nexcept that the document hierarchy  would be spread among multiple disk \ndrives, computers, and geographic locations -- altogether a Good Thing.\n\nAlthough this would gain the Web some of the advantages of URNs, it still \nwould not provide the \"eternal global document ID\" feature that will be a \ncentral advantage to URNs.  However, I suspect the URN researchers will be \nable to leverage off of the knowledge to be gained by implementing Paul et. \nal.'s ideas for redundancy and document hierarchy spreading.\n\n<rant>\nSomething that I have sorely missed in my use of VMS, Microsoft DOS/Windows \n3.1, and MVS is the transparency of \"a hierarchial filesystem incorporating \ndemountable volumes\".  Users (including programmers) should just not have to \ncare about precisely where their documents are located, except in \nlow-resource situations.  If it could be managed, they shouldn't have to \ncare about precisely which processor is serving their needs at that moment, \neither.  The original idea behind filesystems was to allow programmers and \nusers to access their data without knowing precise platter/track/sector \nlocations of their data.\n\nAlthough multi-host virtual sites (MHVS?) has some aspects of a research \nproject, the proposed implementation is simple enough that it could be \nimplemented within 1 or 2 browser revisions (as the browser does the work). \n Further work along this line could result in a world-wide document web that \nalso looks like a set of hierarchies, where the hierarchies differ from each \nother depending on where you start (two hierarchies might even contain each \nother).  This would require an extension of MHVS to permit referrals to \ndifferent domains, which would involve some sort of cryptographic \nauthentication scheme.\n</rant>\n\n\nAs Paul Leach said in his original message:\n>This proposal is nothing more than the application to HTTP of the tried\n>and true referral mechanisms of distributed directory and file systems.\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-5024567"}, {"subject": "Re: short names for header", "content": "> Header compression has been in TCP for quite a while, so I don't think\n> that the topic is \"research\". It is true that good careful engineering\n> will be required, but that's different that research.\n\nHTTP is not TCP.  Just about everything ever planned for HTTP has been\ndone before in something else.  The research question is what happens\nwhen you add that functionality to the existing HTTP system.  Does it work\nas intended? Does the improvement justify the added complexity?\n\nThose are research questions (and yes, careful engineering does involve\nresearch whenever the exact same thing has not been done before).  I like\nresearch (obviously) -- I just don't like research in the name of\nstandardization.\n\n> Why don't you wait until the draft comes out? I promised it by the end\n> of the month, and it is well under way. If it is so hard to do, it\n> should be obvious at that point. A draft will also enable experimental\n> implementations, to gauge how much improvement it might make.\n\nThat does not justify a milestone on the WG charter -- we can do all that\noutside the committee structure.\n\n> If the improvement looks good, we would be very interested in deploying\n> such a scheme fairly quickly.\n\nYou can deploy such a scheme any time you want -- you just can't call it\nHTTP.  Note that the Upgrade header field exists for that very purpose.\nOnce you have answered the research questions, a specification of it can\nstart through the standards cycle, along with any other performance\nimprovements at an equivalent level of maturity.\n\n> Even if HTTP/2.x could be finished within a year, deployment will take\n> much longer becuase of the installed base of 1.x. But HTTP/2.x seems\n> much more like research than any 1.x enhancement under discussion. \n\nHTTP/2.0 (consisting of just HTTP/1.1 in a tokenized form and encapsulated\nin a multiplexed stream) can be done in six months.  We already have\nspecifications (albeit incomplete) for both, and more than one test\nimplementation showing primary gains from multiplexing.  As such, it has\nthe exact same risk factor, cost, and implementation time as any incompatible\nchange to the protocol.  The difference is in the payoff.\n\nEven so, I would not have this WG try to standardize HTTP/2.0 -- it is\nstill research.  In fact, I would encourage any vendor to begin work\non it immediately, since 99% of the effort is just enabling the internals\nof the application (client or server) to deal with any form of a\nmultiplexed, persistent connection; once that is done, converting it to\nthe final \"standard\" form is easy.\n\n> Rapid incremental evolution, as opposed to revolution, has usually been\n> more successful at getting technology to the user quickly. It's just\n> part of the sociology of large distributed systems.\n\nAllow me to refresh people's memory.  A group of people involved in the\ndevelopment of the Web met at the Chicago WWW2 conference (October 94,\nbefore this WG existed) to discuss the future of HTTP and the plan of\nattack for improving its security, performance, etc.  Henrik and I\nvolunteered to rewrite the HTTP specification, since it was already a\nyear out-of-date and it was very clear that new people entering the Web\nproject did not understand the full capabilities of the protocol, and we\nneeded a shared understanding in order to avoid breaking its good qualities\nwhen advanced to the next generation.  Simon Spero and Dave Raggett\n(with Andy Norman) volunteered to work on a binary version of HTTP\n[I can't remember if it included multiplexing at that time or not -- I do\nknow multiplexing was present soon afterword].  A few months later, we\nput together the HTTP BOF at San Jose which began the HTTP-WG.\n\nSmall improvements in the protocol were to be done in HTTP/1.x in parallel\nwith the large performance change in HTTPng, for precisely the reason you\ngive: Rapid incremental evolution.  The difference, however, is that\nevolution within the HTTP/1.x family would retain the existing message\nformat and field names to maximize compatibility (reducing complexity\nof the implementations) and maintain a \"safe harbor\" for those unable or\nunwilling to sacrifice simplicity for speed.\n\nAdding persistent connections was one of those small improvements -- removing\nthe dependence on a single transaction per TCP connection was a basic step\ntoward supporting multi-layered proxy configurations efficiently and a\nbetter understanding of long-lived, multiplexed connections.  And yet, it\ntook over six months of prodding before we could get people to experiment\nwith it -- before we could do the basic tests necessary to determine if\nit was even worthwhile adding to the proposed standard.  In hindsight, I now\nregret that we even bothered to do that much -- it alone added over eight\nmonths to the delay in getting HTTP/1.1 finished (and, BTW, I was mighty\npissed-off when the same person who insisted on it being in HTTP/1.1 later\ncomplained about how long it took to get draft 00 to the WG).\n\nAlmost two years later, the performance problems of HTTP remain the same.\nSo does the solution. The only thing that is lacking is the PEOPLE dedicated\nto writing a complete specification of the solution that we already know\nwill work.  This doesn't have to happen inside the WG -- it just has to\nhappen.\n\nIn the mean time, the notion that anyone can write a complete specification\nof something that hasn't been tried before, and then get this WG to agree\nthat adding statefulness to an otherwise stateless protocol is a good thing,\nall in less than 4 months, is utterly ludicrous.  Attempting to do that\nwithin this WG will only starve-out the other products of the WG, such as\na real specification of content negotiation, because people like me will\nhave to spend hours generating long-winded messages just to explain why\nit is the wrong way to proceed instead of doing something that would\ncontribute toward accomplishing our other goals.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5035179"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "> If you're not convinced that you want that large a (virtual) site, OK by\n> me. But I liked your other reasons for referrals, too:\n> \n> [Roy said:]\n> There are good reasons to support a\n> URI rewriting mechanism on the client side (e.g., to make the URI\n> syntax truly extensible, support automated mirroring, support URNs,\n> etc.), but I haven't considered single-site performance to be one\n> of them.\n> \n> Administrative flexibility and distributed sites were also mentioned as\n> other good uses.\n\nYes, but the best solution for those problems do not involve changes\nto the HTTP protocol -- the URI resolution table is a data format that\ncan be included in any entity (and thus subjected to things like\ndigital signatures, expiration times, and all the other things necessary\nto avoid spoofing and denial of service).\n\n>>Well, I still don't like it -- I see no reason for such a huge site\n>>to be located behind a single DNS name.  The top URLs can be on the\n>>main site (or just redirections from that site) with all other\n>>references relative from there.  I don't see why HTTP needs to \n>>replicate the work of DNS.\n>\n>It doesn't. It is applying the same principles used by DNS to the part\n>of the HTTP URL that DNS doesn't own, for the same reasons as DNS uses\n>them in its part of the URL.\n\nYes, but why? Why can't it leave site-dispersal to the DNS part and\nthe site-specific name resolution to the remainder?  I know why it would\nbe useful (it was the impetus of the \"path\" scheme, after all) -- my\nquestion is why is it *necessary* to support a server with a single DNS\nname that actually exists on separate sites.  Furthermore, if such a thing\nis necessary, why not do it as part of the DNS resolution response\n(like an MX record).\n\n>>Finding a way to do it with multiple Link header fields would be better.\n> \n> So, would you suggest something like:\n> Link: Referral <absolute-URI-1>\n> Link: Referral <absolute-URI-2>\n> ...\ner,\n        Link: <absolute-URI-1>; rel=\"mirror\"\n\nwas more along the lines I was thinking.  It probably won't fit the\nexact problem you are solving, but it would be nice if it could.\n\n> And why do you like it better? It seems weird to me, but in the way that\n> sometimes says to me that it's because my intuition hasn't been trained\n> properly yet.\n\nBecause implementation of Link in general is important.  It is the next\nbig step on the road to salvation (or something).  It is probably the most\nimportant aspect of the HTTP protocol that hasn't been implemented\n(correctly, at least).  Why, if I owned a browser that implemented Link\ncorrectly and was at least on a par with other browsers, I'd own the\nmarket within three months.  It would make Java applets look like\nyesterday's underwear -- stiff and unwieldy.\n\nHmmmm, I guess I could use some sleep...\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5048926"}, {"subject": "Re: chopping lis", "content": "> OK, here's a list of sections (drawn from the v11-06 draft) which\n> *I* think might perhaps more properly belong in an implementors\n> guide.  You will probably disagree with me :-)\n\nOooh, you got the last part right.  Aside from being out of scope\n(this issue, what would be in the spec, was settled in February),\nthe draft isn't even in the WG any more.  The only thing I would\nconsider removing from it at this point is the Appendix on Additional\nHTTP things that are not in the standard, and even then only if\nthe IESG wants it to be shortened.  It is time to spend our brain cells\non more useful pursuits.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5059801"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "    >> This change (only requiring Age on responses taken from a cache)\n    >> breaks the clock-deskewing properties of the original Age design.\n    >> The problem comes when a pre-1.1 cache is in the path and has\n    >> held onto the response for a while.  It's safest to add the Age\n    >> header as soon as possible, since it reduces the chances that\n    >> a skewed clock could result in incorrect comparisons.\n>     \n>     That is incorrect -- in fact, adding an Age header onto a response\n>     received from somewhere else can only make the algorithm less accurate.\n>     The reason is because the outbound requester is already going to add\n>     the amount of time it takes for the response to arrive, including any\n>     time spent by the cache in question.  Therefore, adding an Age header\n>     onto a response retrieved by a further network request will double\n>     the amount of Age that the outbound requester will observe,\n>     for no reason whatsoever, and guaranteeing that the Age calculation\n>     will be wrong.\n> \n> I didn't say that adding the Age header early, rather than late,\n> makes the Age value more accurate.  It doesn't.\n> \n> I said it makes it *safer*.  Which is to say, if there are any\n> potential errors in the system, either invisible caches or hosts with\n> offset clocks, it is safest to err on the side of overestimating\n> the Age, not underestimating it.\n\nIt doesn't make it safer -- the algorithm already takes that into account.\nWhat you are saying is that every step along the way should add the amount\nof time it took to satisfy the request EVEN IF THE REQUEST CAME DIRECTLY\nFROM THE ORIGIN.  That means that if the request passes through three\ncaches (A, B, C) with each segment taking (a, b, c, d) amount of time to\nsatisfy the request\n\n     UA  ------->  A  ------->  B  --------->  C  ------->  OS\n            a           b              c             d\n\nthen the Age will be calculated as follows:\n\n     At  C:  age=d\n         B:  age=d+(c+d)\n         A:  age=d+(c+d)+(b+c+d)\n        UA:  age=d+(c+d)+(b+c+d)+(a+b+c+d)\n\nwhen we all know that the REAL age at UA must be less than (a+b+c+d).\nNote that (a+b+c+d) will always be added by UA.\n\nThe wording that I submitted would have corrected that error in the\nspecification.  Fortunately, the existing wording of the spec is only\nambiguous -- we'll just have to interpret \"cache\" to mean that it only\ntakes effect on retrieval from the caching mechanism.\n\n> The algorithm in the HTTP/1.1 specification tries to not to\n> underestimate the Age.  This is intentional, and it is neither\n> \"incorrect\" nor \"for no reason whatsoever.\"\n\nThe algorithm is correct -- the explanation and requirement in the section\non Age was and is incorrect, because it implies that all hops must add\nto the Age.  It was not intentional -- I was present during all of\nthe discussions on Age and it was not designed to be incorrect.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5067582"}, {"subject": "HTTP/1.2 topics and beyon", "content": "Roy, you wrote, at the end of one of your messages:\n> Hmmmm, I guess I could use some sleep...\nI'll go along with that.\n\nIn case people have forgotten where we are in the process, we're\ntrying to close on the list of topics to be considered for HTTP/1.2.\nThat's why the discussion is a bit wide-ranging at the moment. I\nexpect that we'll want to shut down discussion to focus on those\nthings that have made the cut. But it's not inappropriate for people\nto bring up proposals at the moment, including \"multi-host virutal\nsites\".\n\nIn the case of the implementation guide, there have been and continue\nto be topics where we've not extended the specification to explain how\nsomething might be implemented, even though that discussion would be\nuseful in some specification. We've rejected proposals with the\ndismissal that \"it belongs in an implementation guide\". Martin\nHamilton volunteered to gather together topics that belong \"in an\nimplementation guide\", whether they are new or are current parts of\nthe HTTP/1.1 specification.\n\nIt was inappropriate to chastise the \"chopping list\" as \"out of\nscope\", since it was explicitly asked for, and there was concurrence\nthat it would be very useful to have such a list.  I think the only\nproblem was that you misunderstood the intent: the goal isn't to chop\nanything from THIS draft. We may be able to split HTTP/1.1 into two\nparts when we go from Proposed to Draft, and move the implementation\nadvice elements into the informational guide.\n\nI don't think we've decided to do this; but the first step was to\ngather together a set of topics and see what resulted. Martin did so,\nand asked for feedback on the list he generated -- not on the question\nof whether or not such a list would be useful.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-5078578"}, {"subject": "Starting HTTPN", "content": "There's some concern in the community that \"HTTP-WG will close, what\nwill happen to HTTP-NG\". The standard answer of \"Another Working\nGroup\" hasn't been all that reassuring, without there being much\nevidence of such.\n\nSo, I'd like to strongly suggest that those who are interested in\nstarting a HTTP-NG working group (\"binary format HTTP, without MIME\nmessage headers\") get together, create a charter, a list of\nmilestones, find document editor(s) and a chair and begin NOW to lay\nthe groundwork for starting HTTP-NG review now, if things are far\nenough along.\n\nI'd suggest that the charter of HTTP-NG-WG be narrowed to concern\nitself only with the transport issues (tokenization, multiplexing,\netc.)  rather than an open ended list of HTTP enhancements.\n\nHaving a draft charter and a WG proposal will remove some of the myths\nsurrounding HTTP-NG.\n\nI don't recall if there's a public HTTP-NG mailing list; creating one\nso that all HTTP-NG discussion can move there (rather than here on\nHTTP-WG) would be a good first step.\n\nLarry\n\n\n\n", "id": "lists-010-5087885"}, {"subject": "Re: Unregistered charset values in HTTP 1.1, the ISO-8859* value", "content": "Olle Jarnefors wrote:\n> \n> b) It's unclear what charset registration the preferred\n>    MIME name \"GB2312\" shall designate: the ISO-registered\n>    character set GB_2312-80 (MIBenum: 57) or the only\n>    incompletely described GB2312 (MIBenum: 2025), which\n>    of course already has the proposed preferred MIME name\n>    as it's principal name. It is also possible that these\n>    two registrations actually refer to exactly the same\n>    character set, and should be merged in the IANA registry.\n\nNo. While I agree that the name \"gb2312\" is lousy (\"euc-cn\" would have\nbeen better), the descriptions provided with the IANA registrations for\ngb2312 and gb_2312-80 are:\n\nName: GB_2312-80                                        [RFC1345,KXS2]\nMIBenum: 57\nSource: ECMA registry\nAlias: iso-ir-58\nAlias: chinese\nAlias: csISO58GB231280\n\nName: GB2312\nMIBenum: 2025\nSource: Chinese for People's Republic of China (PRC) mixed one byte, \n        two byte set: \n          20-7E = one byte ASCII \n          A1-FE = two byte PRC Kanji \n        See GB 2312-80 \n        PCL Symbol Set Id: 18C\nAlias: csGB2312\n\nSince the GB_2312-80 entry mentions iso-ir-58, it is clear that this is\na single character set in the ISO 2022 sense. I.e. this charset does not\ninclude the single-byte ASCII characters.\n\nThe gb2312 entry explicitly mentions single-byte ASCII, so it is clear\nthat this is referring to the charset actually used on the net, whose\nname might more properly be \"euc-cn\" (to follow euc-kr's and euc-jp's\nlead).\n\nBy the way, there is an RFC (1922) that proposes even more names for\nsome of the same things:\n\n  ftp://ds.internic.net/rfc/rfc1922.txt\n\nThis document mentions \"cn-gb\" and \"cn-big5\", which appear to be the\nsame as \"gb2312\" and \"big5\" respectively, but they have not been added\nto the IANA registry yet.\n\nAnarchy and chaos rule.\n\n\nErik\n\n\n\n", "id": "lists-010-5096332"}, {"subject": "Re: HTTP/1.2 topics and beyon", "content": "> It was inappropriate to chastise the \"chopping list\" as \"out of\n> scope\", since it was explicitly asked for, and there was concurrence\n> that it would be very useful to have such a list.  I think the only\n> problem was that you misunderstood the intent: the goal isn't to chop\n> anything from THIS draft. We may be able to split HTTP/1.1 into two\n> parts when we go from Proposed to Draft, and move the implementation\n> advice elements into the informational guide.\n\nWell, that's all fine and good, but there was no indication of that in\nMartin's message.  To avoid misunderstandings in the future, I strongly\nsuggest that you stop referring to it as a \"chopping list\"; even with\nan implementation guide, it won't be possible to just \"chop\" those sections.\n\nAn implementation guide should be just that -- go through the steps\nof typical and non-typical scenarios and describe how one might implement\neach step according to the standard.  When that is done and approved by\nactual implemeters of the protocol, go through the RFC (hopefully it will\nbe one by then) and decide what has been duplicated.\n\nIn any case, what will matter in terms of content are all at the level of\na paragraph -- there is no point in talking about including or excluding\nwhole sections.\n\nBTW, I personally find it unlikely that a WG can come to consensus on\nany implementation guide, aside from one in the form of a FAQ.  A good\nimplementation guide requires the time and resources of a good author\nwilling to examine the cracks and crevices of actual implementations --\nsomething which is normally done in a book (as exemplified by Stevens).\nBut, I'd certainly welcome a good implementation guide, and there's no\nreason the authors can't make it a book later on.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5105753"}, {"subject": "Re: Starting HTTPN", "content": "swop the wg for an ng on this list and your set.\n\n---\nCause maybe  (maybe)      | In my mind I'm going to Carolina\nyou're gonna be the one that saves me | - back in Chapel Hill May 16th.\nAnd after all      | Email address remains unchanged\nYou're my firewall -          | ........First in Usenet.........\n\n\n\n", "id": "lists-010-5115399"}, {"subject": "Re: Remapping charset", "content": "Mirsad Todorovac writes:\n\n> \n> In few last messages regarding charsets, I was to notice that many troubles\n> could be handled via some character remapping mechanism.\n> \n> How this should work is as follows:\n> \n> -let's suppose that browser cannot accept document charset, then\n> document should be translated into charset which is supported\n> by browser, but lest likely to loose some characters\n> \n> (Example: with CP-1250 and iso-8859-2 there are only some\n>   swapped blocks of chars, says Dave, so then there should\n>   never be a loss in conversion, but we don't need to have\n>   both, duplicate versions of documents -- one for UN*X\n>   machines, and one for Wind*ws CEE [and still more for\n>   other platforms ...]\n> )\n> \n> - conversion mechanism is simple transliteration, different codes are\n>   being assigned for the same characters\n\nRFC1345 actually provides specifications for doing such simple\ntransliteration, with fallbacks.\n\nkeld simonsen\n\n\n\n", "id": "lists-010-5123026"}, {"subject": "Any interest in automatic linkupdating", "content": "Is anyone interested in a simple mod to HTTP to allow more efficient link\nupdating when a 301 response occurs?  I once designed an addition to HTTP\nto allow updating of referrers the first time any user follows an old\nlink, rather than waiting on (at best?) a periodic link-check performed by\nthe server.\n\nI designed it as an exercise; now that I have it, and we're in\n1.2-issues-phase, I thought I'd ask the WG if there's any interest.  Is\nthere enough long-term efficiency gain to warrant a change to the\nprotocol?  Implementors of link-editing HTTP servers, does this appeal to\nyou at all?\n\nCheers,\nJames Marshall\njsm@crl.com\n\n\n\n", "id": "lists-010-5130942"}, {"subject": "Re: Any interest in automatic linkupdating", "content": "On Sat, 13 Jul 1996, James Marshall wrote:\n\n> Is anyone interested in a simple mod to HTTP to allow more efficient link\n> updating when a 301 response occurs?  I once designed an addition to HTTP\n\nI would be interested ... if the WG wanted to consider the possiblity\nbut I can anticipate several concerns which might be difficult to\nresolve:\n\na.  The referer is often unrelated to the owner or the refered document.\n    In that role, I would not want automatic update of anything on \n    my site w/o some very careful controls.\n\nb.  I have a shipping product which includes a built in reference to\n    an unrelated external site. This product will make thousands of\n    users into micro-web servers w/o any ability to deal with any form\n    of automatic notification. I only offer this as the penultimate\n    example of difficulty which needs to be considered (and makes me\n    think our maintenance release should have a way to deal with\n    broken imbeded links...)\n\nDave Morris\n\n\n\n", "id": "lists-010-5139193"}, {"subject": "Re: multihost virtual sites for HTTP 1.", "content": "Hello,\n\nFirst of all, I would like to take this time to express the\ngreat relief I have found after reading through this list.  I\nhave been struggling with the task of automated server replication\nand designing a mission-critical implementation of a web server\nfor quite some time.\n\nDuring the period in which I have considered options, I have\nconcluded at least two important points which I have not\nyet seen in this thread:\n\n(BTW, I like the multiple \"Link:\" response solution and I\nam basing my suggestions on it or a similair convention)\n\n1.  Server replication must include link-state information\n  - take it from the IP working groups who develop OSPF and\n    other link-state aware protocols - no need to reinvent the\n    wheel\n  - this could be implemented at both the server and browser\n    level much like MX preference records are preassigned at\n    the DNS level\n  - accordingly, at the browser level or at some other authority,\n    a table of information about candidate servers should be\n    maintained for fast lookup with preferences accounting for\n    everything from country codes to avg. response time\n2.  Some authority other than http servers may be used\n  - in order for the browser to retrieve the initial multiple\n    \"Link:\" responses, it has to contact the only server it\n    knows about - the primary original server indicated by\n    the URL.  if that server is down or otherwise unable to\n    communicate information about other \"Link:\"'ed replicated\n    servers, then all efforts are lost\n  - some kind of analogy can be drawn by understanding why\n    multiple name servers should be placed on different physical\n    networks so that loosing a single network will not render\n    hosts actually located on different networks unresolvable\n\nI look forward to replies and comments!\n\n- Thank you!\nBrad Block\n\n\n\n", "id": "lists-010-5147773"}, {"subject": "Language negotiation and robot", "content": "Language negotiation seems to work quite well (at least for\nWestern languages) between eg. the Apache server and\nMosaic-l10n, Netscape 3 and (I presume) Lynx 2.5).\n\nA description of this may be found at \nhttp://vancouver-webpages.com/multilingual/howto.html\nand elsewhere.\n\nI am interested in the interaction between this and web robots; \nspecifically, how to tell the robot that the document is available\nin more than one language.\n\nUsing the Apache type-map model, if no Accept-language\nfield is sent, one gets the first URL (assuming qs is not used or are equal).\nWould it be valid to do something like this:\n\nURI: start; vary=\"language\"\n\nURI: unknown.html\nContent-type: text/html\nContent-language: en,fr,de\n\nURI: german.html\nContent-type: text/html\nContent-language: de\n\nURI: english.html\nContent-type: text/html\nContent-language: en\netc.\n\nso that a robot with language-accept unset would retrieve the \n\"Content-language: en,fr,de\" header, and then retry with \nAccept-language set to en, fr, de in turn?\n\nAs this multiple-language header seems to be valid in the current draft \nspec., I presume it should not break (future) browsers.\n\nWith my current setup (Mosaic-l10n2.6, Apache 0.6) this doesn't quite work,\nas unknown.html is returned when accept-language is set to \"en\". Putting\na bogus language first has the desired effect, but this is obviously a \nkludge.\nhttp://vancouver-webpages.com/multilingual/samples8.var, samples8.var.txt \n\nAlternatively, I could just add all the other languages to the english \none, so that I have:\n\nURI: english-ca.html\nContent-type: text/html\nContent-language: en-CA,en-US,en-GB,de,fr-FR\n\nwhich would work, but would mean I would have to clutter up the \"English\" \npage with instructions about setting up Accept-language.\n\nAndrew Daviel\n\nandrew@vancouver-webpages.com \nhttp://vancouver-webpages.com  : home of searchBC\n\n\n\n", "id": "lists-010-5156316"}, {"subject": "Re: Any interest in automatic linkupdating", "content": "At 16:31 -0400 96.07.13, James Marshall wrote:\n>Is anyone interested in a simple mod to HTTP to allow more efficient link\n>updating when a 301 response occurs?  I once designed an addition to HTTP\n>to allow updating of referrers the first time any user follows an old\n>link, rather than waiting on (at best?) a periodic link-check performed by\n>the server.\n>\n>I designed it as an exercise; now that I have it, and we're in\n>1.2-issues-phase, I thought I'd ask the WG if there's any interest.  Is\n>there enough long-term efficiency gain to warrant a change to the\n>protocol?  Implementors of link-editing HTTP servers, does this appeal to\n>you at all?\n\nI agree that updating broken links is a useful feature in a web server.  But why does this need to be addressed in the standard?  Is this really anything more than a useful utility for webmasters  ( as opposed to a client-server communication issue which must be addressed in the standard )?\n\n\nApologies in advance if I'm just being dense :-)\n\n\n\n\n------------\nVance Huntley\n\nDirector of Technology & GenesisJive Guy\nWebGenesis, Inc., Ithaca, NY 14850\n\nvance@webgenesis.com --- 607.255.8499\n\nCheck out \"The Globe\"!  ---  http://www.theglobe.com/\n------------\n\n\n\n", "id": "lists-010-5165809"}, {"subject": "Minor issues with HTTP 1.1 draft ", "content": "I can't find any trace of these in the http-wg archives, and they seem like\n\"editing\" issues, so I hope they're appropriate to bring up at this point in\nthe RFC process.  There are a few small points in draft 6 of the HTTP 1.1\nspecification that seem to be incorrect:\n\n   1) On page 30, section 4.2 \"Message Headers\" states 'Applications SHOULD\n      follow \"common form\" when generating HTTP constructs, ...'.  The draft\n      does not contain any definition of \"common form\", although by context it\n      appears to mean \"headers without RFC-822-style folding\".\n\n   2) On page 95, section 14.9 \"Cache-Control\" contains in the BNF for\n      cache-request-directive, the alternative:\n\n         '| \"max-stale\" \"=\" [ delta-seconds ]'\n\n      It seems this should read:\n\n         '| \"max-stale\" [ \"=\" delta-seconds ]'\n\n      All other usages of \"thing=value\" where the value is optional take the\n      form where the \"=\" is omitted with the value (i.e.  \"max-stale\", not\n      \"max-stale=\").\n\n   3) On page 105, section 14.15 \"Content-Location\" contains the phrase '...\n      it is only a statement.  of the location of the resource ...', where the\n      period after 'statement' is a typo.\n\n   4) On pages 128-129, sections 14.45 \"Warning\", the references to\n      \"ISO-8599-1\" are typos, they should read \"ISO-8859-1\".  They should\n      probably also have pointers to reference [22] \"ISO-8859 ...\".\n\n   5) On page 134, section 15.8 \"DNS Spoofing\" states that 'The deployment of\n      DNSSEC should help this situation.\" without citation of DNSSEC.  There\n      doesn't appear to be any RFC to cite at this time, although there are\n      several valid Internet Drafts.  Either a citation should be added (in\n      possible violation of the proper use of Internet Drafts) or the\n      statement should be removed.\n\n   6) On pages 137-139, section 17 \"References\", items [5] and [12] are not\n      cited anywhere within the draft, and item [27] is omitted from the list.\n\nThanks for all the good work, the draft is extremely readable and generally\nquite clear of some rather complex topics (especially cache behavior).\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n", "id": "lists-010-5174914"}, {"subject": "Re: Any interest in automatic linkupdating", "content": "I see I wasn't very clear in my first post-- let me apply some Windex: \n\nGiven the current HTTP standard, I don't see how to update a link the\nfirst time any user follows it; the best method I know is for the owning\nserver to keep checking all links in its resources, and update those that\nare obsolete.  It would be more efficient (and more immediate) to update a\nlink the first time someone discovers it's obsolete, instead of checking\nall links periodically.  Sort of like interrupt-driven versus polling. \n\nIf you know a way to do this with the current HTTP standard, then I'm the\none being dense (and please let me know how) (I mean how to do it, now how\ndense). \n\nThe scheme I'm talking about involves adding or modifying an HTTP method\nto let any client SUGGEST to a server that a link in a resource needs\nupdating.  The server is then expected to test the link with a HEAD\nrequest, before actually updating the link.  This suggestion mechanism\nallows anyone to notify any server that a link needs updating, while\npreventing unauthorized changes (the security concern that Dave Morris (Hi\nDave!) brought up in his note). \n\nDoes this make more sense? \n\nCheers,\nJames Marshall\njsm@crl.com\n\n\n> I agree that updating broken links is a useful feature in a web server. \n> But why does this need to be addressed in the standard?  Is this really \n> anything more than a useful utility for webmasters  ( as opposed to a \n> client-server communication issue which must be addressed in the \n> standard )?\n> \n> Apologies in advance if I'm just being dense :-)\n> \n> ------------\n> Vance Huntley\n> vance@webgenesis.com --- 607.255.8499\n\n\n\n", "id": "lists-010-5184600"}, {"subject": "Re: Any interest in automatic linkupdating", "content": "   Date: Sat, 13 Jul 1996 15:02:33 -0700 (PDT)\n   From: \"David W. Morris\" <dwm@shell.portal.com>\n\n   b.  I have a shipping product which includes a built in reference to\n       an unrelated external site. This product will make thousands of\n       users into micro-web servers w/o any ability to deal with any form\n       of automatic notification. I only offer this as the penultimate\n       example of difficulty which needs to be considered (and makes me\n       think our maintenance release should have a way to deal with\n       broken imbeded links...)\n\n   Dave Morris\n\nI don't see link update as neccessarily part of the protocol, except\nperhaps for requiring more strict adherrence to providing certain\nheaders (e.g., the \"Last-Modified\" field).\n\nMy online product consists of a large database of URLs.  Part of my\noffering is alerting customers to movement or change in the link\ntargets.  To do this, I batch run some custom link-verification code.\nThis works pretty well.\n\nJoe\n\n----\nJoseph Arceneaux\nArceneaux Consulting\n\nhttp://www.arceneaux.com\njla@arceneaux.com\n+1 415 648 9988 (direct)\n+1 415 341 1395 (fax)\n+1 500 488 9308\n\n\n\n", "id": "lists-010-5194329"}, {"subject": "Re: Minor issues with HTTP 1.1 draft ", "content": "Regarding <draft-ietf-http-v11-spec-06.txt>, Ross Patterson mentions:\n\n> I can't find any trace of these in the http-wg archives, and they seem like\n> \"editing\" issues, so I hope they're appropriate to bring up at this point in\n> the RFC process.  There are a few small points in draft 6 of the HTTP 1.1\n> specification that seem to be incorrect:\n> \n>    1) On page 30, section 4.2 \"Message Headers\" states 'Applications SHOULD\n>       follow \"common form\" when generating HTTP constructs, ...'.  The draft\n>       does not contain any definition of \"common form\", although by context it\n>       appears to mean \"headers without RFC-822-style folding\".\n\nNo, common form is just that -- whatever is common.  Since that is a\ntime-dependent function, it can't be defined.  If we were to define it\nas \"today's common form\", then it would be no folding, no space between\nfield-name and \":\", and only a single space between the \":\" and the\nfield-value.  However, I think that might lead to more poor implementations\nin the future.\n\n>    2) On page 95, section 14.9 \"Cache-Control\" contains in the BNF for\n>       cache-request-directive, the alternative:\n> \n>          '| \"max-stale\" \"=\" [ delta-seconds ]'\n> \n>       It seems this should read:\n> \n>          '| \"max-stale\" [ \"=\" delta-seconds ]'\n> \n>       All other usages of \"thing=value\" where the value is optional take the\n>       form where the \"=\" is omitted with the value (i.e.  \"max-stale\", not\n>       \"max-stale=\").\n\nYes, that should be fixed -- it was correct in the changes proposed to the\nWG mailing list, but just got fudged in editing (it happens).\n\n>    3) On page 105, section 14.15 \"Content-Location\" contains the phrase '...\n>       it is only a statement.  of the location of the resource ...', where the\n>       period after 'statement' is a typo.\n\nYep.\n\n>    4) On pages 128-129, sections 14.45 \"Warning\", the references to\n>       \"ISO-8599-1\" are typos, they should read \"ISO-8859-1\".  They should\n>       probably also have pointers to reference [22] \"ISO-8859 ...\".\n\nYep.\n\n>    5) On page 134, section 15.8 \"DNS Spoofing\" states that 'The deployment of\n>       DNSSEC should help this situation.\" without citation of DNSSEC.  There\n>       doesn't appear to be any RFC to cite at this time, although there are\n>       several valid Internet Drafts.  Either a citation should be added (in\n>       possible violation of the proper use of Internet Drafts) or the\n>       statement should be removed.\n\nI think it should be removed -- we cannot know if it will help until it\nis no longer a draft.\n\n>    6) On pages 137-139, section 17 \"References\", items [5] and [12] are not\n>       cited anywhere within the draft, and item [27] is omitted from the list.\n\nItem [5] should be referenced at the end of first para of section 19.6.2.4.\nItem [12] can be removed or include a reference to it in the third para\nof section 3.3.1 (where RFC 850 is referenced).\n\nI should also note that item [30] currently has two references munged\ntogether, and that the entire list should be listed alphabetically by\nthe first author's last name.\n\nLarry, these can all be categorized as editorial changes, but should be\nmade before the document is frozen into RFC form.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5203316"}, {"subject": "Re: Language negotiation and robot", "content": "> I am interested in the interaction between this and web robots; \n> specifically, how to tell the robot that the document is available\n> in more than one language.\n\nUsing the Alternates header field, which has yet to have a well-defined\nsyntax.\n\n> Using the Apache type-map model, if no Accept-language\n> field is sent, one gets the first URL (assuming qs is not used or are equal).\n> Would it be valid to do something like this:\n> \n> URI: start; vary=\"language\"\n> \n> URI: unknown.html\n> Content-type: text/html\n> Content-language: en,fr,de\n> \n> URI: german.html\n> Content-type: text/html\n> Content-language: de\n> \n> URI: english.html\n> Content-type: text/html\n> Content-language: en\n> etc.\n> \n> so that a robot with language-accept unset would retrieve the \n> \"Content-language: en,fr,de\" header, and then retry with \n> Accept-language set to en, fr, de in turn?\n\nNo, that would not be valid.\n\n> As this multiple-language header seems to be valid in the current draft \n> spec., I presume it should not break (future) browsers.\n\nThe semantics are different -- \"Content-language: en,fr,de\" means that\nthe content of that *response* is intended for people who want to read\nEnglish, French, or German (e.g., a document containing three descriptions \nof the same thing in those three languages).  It does not indicate what\nother languages are available at that resource.\n\nWhat you want is Alternates.  What is currently available is\n\n   Vary: Accept-Language\n\nwhich does indeed give the browser a clue that other languages may be\navailable (but not an exact clue).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5214881"}, {"subject": "RE: Any interest in automatic linkupdating", "content": "nice thing about that is that the checking is done on somebody else's\nInternet time.  unfortunately, I wouldn't want my client to waste my\nhard earned money telling your server about some link.\n\na more reasonable standard might be\n\nA.\n\nSERVER RECIEVES REQUEST FOR UNAVAILABLE RESOURCE\n1. Checks for the \"Referer\" header\n2. If it is available it writes the URL of the request, as well as the\nReferer in a ADVISE FILE\n\nB.\nADVISE DAEMON BEGINS IT'S PROCESS\n\n1. Reads the ADVISE FILE...at a time when connection activity is low\n\n2. INITIATES A CONNECTION WITH THE REFERRING SERVER\n\n----begin session----\nADVISE /your-referring-resource HTTP/1.X\nInvalid-Resource: http://myserver/mybadurl\nValid-Resource: http://myserver/mygoodresource\n----end session----\n\n3. if Valid-Resource is not available ..it is left blank\n\n4. if the server replies with a \"405 -Method Not Available\" ...it is\nrecommended that your server keep an \"ADVISE N/A CACHE\" .... and wait\nsome time before bugging that out of date machine again.\n\n\n>----------\n>From: James Marshall[SMTP:jsm@crl.com]\n>Sent: Monday, July 15, 1996 4:56 PM\n>To: Vance Huntley\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: Any interest in automatic link-updating?\n>\n>I see I wasn't very clear in my first post-- let me apply some Windex: \n>\n>Given the current HTTP standard, I don't see how to update a link the\n>first time any user follows it; the best method I know is for the\n>owning\n>server to keep checking all links in its resources, and update those\n>that\n>are obsolete.  It would be more efficient (and more immediate) to\n>update a\n>link the first time someone discovers it's obsolete, instead of\n>checking\n>all links periodically.  Sort of like interrupt-driven versus polling. \n>\n>If you know a way to do this with the current HTTP standard, then I'm\n>the\n>one being dense (and please let me know how) (I mean how to do it, now\n>how\n>dense). \n>\n>The scheme I'm talking about involves adding or modifying an HTTP\n>method\n>to let any client SUGGEST to a server that a link in a resource needs\n>updating.  The server is then expected to test the link with a HEAD\n>request, before actually updating the link.  This suggestion mechanism\n>allows anyone to notify any server that a link needs updating, while\n>preventing unauthorized changes (the security concern that Dave Morris\n>(Hi\n>Dave!) brought up in his note). \n>\n>Does this make more sense? \n>\n>Cheers,\n>James Marshall\n>jsm@crl.com\n>\n>\n>> I agree that updating broken links is a useful feature in a web server. \n>> But why does this need to be addressed in the standard?  Is this really \n>> anything more than a useful utility for webmasters  ( as opposed to a \n>> client-server communication issue which must be addressed in the \n>> standard )?\n>> \n>> Apologies in advance if I'm just being dense :-)\n>> \n>> ------------\n>> Vance Huntley\n>> vance@webgenesis.com --- 607.255.8499\n>\n>\n\n\n\n", "id": "lists-010-5223980"}, {"subject": "RE: Is referer spelled wrong", "content": "Netscape spells it \"Referer\".\n\nWebster's insists it's Referrer not Referer....\n\nThe World-Wide Web Common Code Library version 2.16pre2\nproduces \"Referer\"\n\nalso spelled wrong\n\nNASA spells it \"Referrer\", and so does Netcape's JavaScript in their\n\"URL\" library.\n\nhttp://cgi.netscape.com/eng/mozilla/Gold/handbook/javascript/ref_h-l.html\n\n\n\"IE: referrer - reflects the URL of the calling document \"\n\n\n\n", "id": "lists-010-5236655"}, {"subject": "RE: Is referer spelled wrong", "content": "I have had to look into this question recently for my logfile analysis\nprogram, analog. \"Referrer\" is correct English in both US and British\nvarieties. Unfortunately, it's probably too late to change it now in HTTP.\nPersonally, I've just put it down as another example of illiteracy invading\nour lives. (FWIW, analog understands either spelling as input, but outputs\n\"referrer\" in its reports).\n\n-- \nStephen R. E. Turner\n  Stochastic Networks Group, Statistical Laboratory, University of Cambridge\n  e-mail: sret1@cam.ac.uk  WWW: http://www.statslab.cam.ac.uk/~sret1/home.html\n  \"You may notice that your Customer Reference Number has changed\" British Gas\n\n\n\n", "id": "lists-010-5245520"}, {"subject": "Re: Is referer spelled wrong", "content": "I think I remember Philip ;-) H-B taking credit for this.  It's\nin the spec so forget about changing it.  In the headers it's \"Referer\".\nThere is no requirement to propogate the spelling beyond the headers.\n\n\n> \n> Netscape spells it \"Referer\".\n> \n> Webster's insists it's Referrer not Referer....\n> \n> The World-Wide Web Common Code Library version 2.16pre2\n> produces \"Referer\"\n> \n> also spelled wrong\n> \n> NASA spells it \"Referrer\", and so does Netcape's JavaScript in their\n> \"URL\" library.\n> \n> http://cgi.netscape.com/eng/mozilla/Gold/handbook/javascript/ref_h-l.html\n> \n> \n> \"IE: referrer - reflects the URL of the calling document \"\n> \n> \n> \n\n\n\n", "id": "lists-010-5253174"}, {"subject": "Re: Is referer spelled wrong", "content": "'referer' is misspelled. It says so in the spec.\n\nMaybe I'm feeling overly grumpy this morning, but folks, this is not\nthe \"general discussion about HTTP without having read the spec\" list.\nIf you have any questions about HTTP, please at least look to see if\nyour question is answered in the draft document FIRST, and even then,\nonly post in 'http-wg' if it is something we actually need to consider\nfor the work items we have before us.\n================================================================\nfrom draft-ietf-http-v11-spec-06.txt\n================================================================\n14.37 Referer\n\nThe Referer[sic] request-header field allows the client to specify, for\nthe server's benefit, the address (URI) of the resource from which the\nRequest-URI was obtained (the \"referrer\", although the header field is\nmisspelled.) The Referer request-header allows a server to generate\nlists of back-links to resources for interest, logging, optimized\ncaching, etc. It also allows obsolete or mistyped links to be traced for\nmaintenance. The Referer field MUST NOT be sent if the Request-URI was\nobtained from a source that does not have its own URI, such as input\nfrom the user keyboard.\n\n\n\n", "id": "lists-010-5261607"}, {"subject": "Re: Any interest in automatic linkupdating", "content": "I do not believe that HTTP needs any kind of additional method for\nadvising the site that a URL has changed. If the world needs a\nstandard like this, I suggest a new media type\n\n      application/webmaster-advice\n\nand that such a media type could be delivered by email to\n      webmaster@host\n\n\nI suggest for the format of application/webmaster-advice:\n\n\nDear Webmaster:\nMy system has discovered the following link on\n        one of your pages is incorrect. Please correct it.\n\nLink from page: http://sloppy.host/referring-resource\nInvalid-Resource: http://myserver/mybadurl\nValid-Resource: http://myserver/mygoodresource\n\n\n\n", "id": "lists-010-5269782"}, {"subject": "Re: Is referer spelled wrong", "content": "Referer is only spelt \"wrong\" if one insists on petit borgeois\nnotions of spelling-correctness. This movement must be seen as\na reactionary and authoritarian attempt to impose an artificial\nand unnecessary conformity on language and is therefore a movement\nof political and social control and to be resisted as such.\n\nWhen I proposed the \"referer\" field to Tim I used the alternative\nspelling as a means of signalling the iminent need to deconstruct\nthe concept of spelling, in particular the notion that machines \nshould follow human models of communications in issues which are \nentirely conventional. From the point of view of the machine the\nsignifier \"referer\" is superior to that of \"referrer\" by virtue \nof being a byte shorter while referencing the same designatum. \nI interpret fact that this differentiation between signifier and\nsignified was brought to light by the \"referer\" field as an\nunconcious but nevertheless valid recapitulation of Goedel's\nincompleteness theorem.\n\nI suggest that future comment in this thread be transferred to\nalt.postmodern.\n\n\nPhill\n\n\n\n", "id": "lists-010-5277413"}, {"subject": "RE: Is referer spelled wrong", "content": "The quick answer:  you loose!\n\n\n\n", "id": "lists-010-5285818"}, {"subject": "RE: Is referer spelled wrong", "content": "Shel Kaphan writes:\n > \n > The quick answer:  you loose!\n > \n > \n\nUhhhh, before one more person tells me I misspelled this,\nI guess I have to specify that this was intended as an ironic\nutterance.\n\n\n\n", "id": "lists-010-5293130"}, {"subject": "RE: Is referer spelled wrong", "content": "chill out. Smoke a refer.\n\nOn Tue, 16 Jul 1996, Shel Kaphan wrote:\n\n> Shel Kaphan writes:\n>  > \n>  > The quick answer:  you loose!\n>  > \n>  > \n> \n> Uhhhh, before one more person tells me I misspelled this,\n> I guess I have to specify that this was intended as an ironic\n> utterance.\n> \n> \n\n---\nCause maybe  (maybe)      | In my mind I'm going to Carolina\nyou're gonna be the one that saves me | - back in Chapel Hill May 16th.\nAnd after all      | Email address remains unchanged\nYou're my firewall -          | ........First in Usenet.........\n\n\n\n", "id": "lists-010-5300231"}, {"subject": "Minor update to the status code table (section 6.1.1", "content": "I was trying to update libwww-perl to the latest HTTP spec and found\nthe following inconsistencies between the table in section 6.1.1 and\nhow the status codes are named in the corresponding sections.\n\n\n--- draft-ietf-http-v11-spec-06.txt.orig        Wed Jul 17 13:01:52 1996\n+++ draft-ietf-http-v11-spec-06.txt     Wed Jul 17 13:21:58 1996\n@@ -2177,7 +2177,7 @@\n                       | \"405\"   ; Method Not Allowed\n                       | \"406\"   ; Not Acceptable\n                       | \"407\"   ; Proxy Authentication Required\n-                      | \"408\"   ; Request Time-out\n+                      | \"408\"   ; Request Timeout\n                       | \"409\"   ; Conflict\n                       | \"410\"   ; Gone\n                       | \"411\"   ; Length Required\n@@ -2189,8 +2189,8 @@\n                       | \"501\"   ; Not Implemented\n                       | \"502\"   ; Bad Gateway\n                       | \"503\"   ; Service Unavailable\n-                      | \"504\"   ; Gateway Time-out\n-                      | \"505\"   ; HTTP Version not supported\n+                      | \"504\"   ; Gateway Timeout\n+                      | \"505\"   ; HTTP Version Not Supported\n                       | extension-code\n \n        extension-code = 3DIGIT\n\nRegards,\nGisle Aas.\n\n\n\n", "id": "lists-010-5307972"}, {"subject": "The Title heade", "content": "Why and when did the Title: header disappear? (I find it in\nv11-spec-01, but not in v11-spec-06)\n\nLibwww-perl generates this header internally for HTML documents by\nlooking at the <title>...</title> element.\n\nRegards,\nGisle Aas.\n\n\n\n", "id": "lists-010-5315706"}, {"subject": "Re: The Title heade", "content": "Hi Gisle,\n\n> Why and when did the Title: header disappear? (I find it in\n> v11-spec-01, but not in v11-spec-06)\n\nSometime between those drafts.  It was removed because it wasn't being\nused much, because it duplicates the purpose of Content-Description\nin MIME, and because, like any Entity-Header, it can be used freely\nwithout the need for standardization.\n\n> Libwww-perl generates this header internally for HTML documents by\n> looking at the <title>...</title> element.\n\nThat's okay -- what we do internally is our own business (e.g., my use\nof 600 level response codes to indicate client failures).\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5322548"}, {"subject": "RE: The Title heade", "content": ">----------\n>From: Roy T. Fielding[SMTP:fielding@liege.ICS.UCI.EDU]\n>Sent: Friday, July 19, 1996 3:36 AM\n>To: Gisle Aas\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: The Title header \n>\n>Hi Gisle,\n>\n>> Why and when did the Title: header disappear? (I find it in\n>> v11-spec-01, but not in v11-spec-06)\n>\n>Sometime between those drafts.  It was removed because it wasn't being\n>used much, because it duplicates the purpose of Content-Description\n>in MIME, and because, like any Entity-Header, it can be used freely\n>without the need for standardization.\n\nDon't you mean \"no standardization in HTTP\"? If it is to be of more than\nprivate use, then it is extremely useful to have a spec somewhere that\nsays what it means, so that everyone uses it in the same way to mean the\nsame thing. Said spec could even be a standard.\n\nPaul\n\n\n\n", "id": "lists-010-5330461"}, {"subject": "Re: The Title heade", "content": "I made an editorial decision as editor that it should go (on discussion with\nRoy), as duplications of Mime, with different names, is Evil.\n\nIn some future version of HTTP, one could note what headers in HTTP are adopted\ndirectly from MIME.\n- Jim\n\n\n\n", "id": "lists-010-5340178"}, {"subject": "Re: The Title heade", "content": "On Fri, 19 Jul 1996 jg@zorch.w3.org wrote:\n\n> I made an editorial decision as editor that it should go (on discussion with\n> Roy), as duplications of Mime, with different names, is Evil.\n> \n> In some future version of HTTP, one could note what headers in HTTP are adopted\n> directly from MIME.\n\nIs current practice relevant?  I can cite you hundreds of servers using it.\nTrue this is a tiny fraction of all servers.\n\nI don't think it is fair to call this decision an editorial decision.\nIf you had decided to change the name to the corresponding MIME header\nthat could be called editorial.  \n\nAs it is, a decision was made that prevents its use in other than\nad hoc ways.  This is a functionality which HTTP used to have and\nno longer does.  Don't some search engines use it?\n\nIn fact, I would find an argument to make it mandatory persuasive.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-5348187"}, {"subject": "Re: The Title heade", "content": "Re removing \"Title:\"\n\n> I don't think it is fair to call this decision an editorial decision.\n> If you had decided to change the name to the corresponding MIME header\n> that could be called editorial.  \n\nYes, you're right: this was not an 'editorial' decision. The accepted\nprocedure was that any feature not explicitly HTTP/1.0 and for which\nthere was no consensus to add to HTTP/1.1 would be removed from\nHTTP/1.1, and it was on those grounds that Title was dropped.\n\nA number of items that appear in appendix D (\"Additional Features\") of\nRFC 1945 (HTTP/1.0) did not make it into 1.1 for similar reasons.\n\nRegards,\n\nLarry\n\n\n\n", "id": "lists-010-5358382"}, {"subject": "Re: The Title heade", "content": "On Fri, 19 Jul 1996, John Franks wrote:\n\n> In fact, I would find an argument to make it mandatory persuasive.\n\nI wouldn't be happy about any change to http which made analysis of the\ncontent of a file mandatory. We should not demand that servers \nanalyze HTML.\n\nAnd in current practice, my experience as the designer of a search \ntool is that titles are used to provide meaningful captions on browser\nscreens not sumarize the document content which would be useful for\nsearch engines.\n\nI haven't looked at the mime spec so it may already agree with me ... but\nif HTTP were to include a header relating to title or document content\nit should be clearly a document description which might be provided by\na server and might (NOT mandatory) be provided by content analysis or\nmeta data stored by the server external to the document. It should not\nbe directly specified as being a particular element of a particular \ncontent type.\n\nDave Morris\n\n\n\n", "id": "lists-010-5367191"}, {"subject": "Re: The Title heade", "content": "On Fri, 19 Jul 1996, David W. Morris wrote:\n> \n> On Fri, 19 Jul 1996, John Franks wrote:\n> \n> > In fact, I would find an argument to make it mandatory persuasive.\n> \n> I wouldn't be happy about any change to http which made analysis of the\n> content of a file mandatory. We should not demand that servers \n> analyze HTML.\n> \n> And in current practice, my experience as the designer of a search \n> tool is that titles are used to provide meaningful captions on browser\n> screens not sumarize the document content which would be useful for\n> search engines.\n> \n> I haven't looked at the mime spec so it may already agree with me ... but\n> if HTTP were to include a header relating to title or document content\n> it should be clearly a document description which might be provided by\n> a server and might (NOT mandatory) be provided by content analysis or\n> meta data stored by the server external to the document. It should not\n> be directly specified as being a particular element of a particular \n> content type.\n> \n\nI wouldn't really argue that it be mandatory.  I was venting my\npique that it was removed (even from an appendix) without discussion.\n\nI agree with everything you say and I hope that a header with this\nfunctionality can make its way back into HTTP/1.2. \n\nThe Title header (its name is not important) as implemented in the\nWN server has many of the aspects you suggest.  It is provided from\nmeta data stored external to the document which can be set by the\nmaintainer, but is by default automatically extracted from HTML\ndocuments if not set (this extraction is done once when the meta\ndata is \"compiled\" not every time the document is served).\n\nNon HTML documents which have no title set get the filename by default.\n\nI would really like to see not only Title, but also Keywords.  Abstract\nwould be nice too, but perhaps that could be done with \"Link\".\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-5377452"}, {"subject": "Typo Correctio", "content": "I don't remember if this one has been mentioned but on\npage 70 lines 1 and 2:\n\n  cache-request-directive =\n                  | \"no-cache\" [ \"=\" ,\". 1#field-name <\"> ]\n\nshould be:\n\n  cache-request-directive =\n                   \"no-cache\" [ \"=\" ,\". 1#field-name <\"> ]\n\n\nPaul\n\n\n\n\nPaul Hethmon\nphethmon@utk.edu\n----------------------------------------------------------\nComputerman -- Agricultural Policy Analysis Center\n----------------------------------------------------------\nNeoLogic Ftp & Mail Servers\n----------------------------------------------------------\nKnoxville Warp User's: http://apacweb.ag.utk.edu/os2\n----------------------------------------------------------\n\n\n\n", "id": "lists-010-5388559"}, {"subject": "Another Typo", "content": "This one seems (to me) to be another typo. Page 70:\n\ncache-request-directive =\n                  \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n                  | \"no-store\"\n                  | \"max-age\" \"=\" delta-seconds\n                  | \"max-stale\" \"=\" [ delta-seconds ]\n                                ^^^^^^\nShould the max-stale line be:\n\n                  | \"max-stale\" [ \"=\" delta-seconds ]\n                               ^^^^^^^\n\nPage 72 states \"If no value is assigned to max-stale, then the\nclient is willing to accept a stale response of any age\". Does\nthis then mean the header should be:\n\n  Cache-Control: max-stale=\n\nor\n\n  Cache-Control: max-stale\n\nIt seems to be a question of whether no value is explicit by no\nvalue to the right of the equal or by just not having the parameter\nand equal sign. The latter seems more in line with the other\ndirectives.\n\nthanks,\n\nPaul\n\n\n\n\nPaul Hethmon\nphethmon@utk.edu\n----------------------------------------------------------\nComputerman -- Agricultural Policy Analysis Center\n----------------------------------------------------------\nNeoLogic Ftp & Mail Servers\n----------------------------------------------------------\nKnoxville Warp User's: http://apacweb.ag.utk.edu/os2\n----------------------------------------------------------\n\n\n\n", "id": "lists-010-5395531"}, {"subject": "automatic retrying of request", "content": "  While trying to implement persistent connections in a client I've run\ninto a small question concerning the automatic retrying of requests\n(this is based on draft-ietf-http-v11-spec-06, section 8). Basically\nthe question I'm asking myself is 'how often should/may I automatically\nretry the request if it keeps failing due to a network error'. My\nuncertainty stems from the following parts in the spec:\n\npage 44, top, in the context of asynchronous close events, says\n\n> However, this automatic retry SHOULD NOT be repeated if the second\n> request fails.\n\nI assume that 'fails' refers to a network error, and not to a returned\nstatus code >= 400? So this says a given request should be retried at\nmost once.\n\nBut then, on page 45, an algorithm is presented for HTTP/1.0 persistent\nconnections that basically says 'retry until you get a valid status\n(success or error) or the user aborts the process'. Isn't this in\nconflict with page 44, as it implies you should automatically retry\nas often as necessary?\n\nFurthermore for HTTP/1.1 the last paragraph in section 8, page 46, says\n\n> An HTTP/1.1 (or later) client that sees the connection close after\n> receiving a 100 (Continue) but before receiving any other status SHOULD\n> retry the request, and need not wait for 100 (Continue) response (but\n> MAY do so if this simplifies the implementation).\n\nI assume this is only talking about the first attempt to send a request,\nand does not hold for retries? Otherwise it would again be in conflict\nwith page 44.\n\n\n  Confused,\n\n  Ronald\n\n\n\n", "id": "lists-010-5403207"}, {"subject": "I-D ACTION:draft-ietf-http-state-mgmt03.txt, .p", "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Proposed HTTP State Management Mechanism                \n       Author(s) : D. Kristol, L. Montulli\n       Filename  : draft-ietf-http-state-mgmt-03.txt, .ps\n       Pages     : 19\n       Date      : 07/19/1996\n\nThis document specifies a way to create a stateful session with HTTP \nrequests and responses.  It describes two new headers, Cookie and requests \nand responses.  It describes two new headers, Cookie and Set-Cookie, which \ncarry state information between participating origin servers and user \nagents.  The method described here differs from Netscape's Cookie proposal,\nbut it can interoperate with HTTP/1.0 user agents that use Netscape's \nmethod.  (See the HISTORICAL section.)                                     \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-state-mgmt-03.txt\".\n Or \n     \"get draft-ietf-http-state-mgmt-03.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-mgmt-03.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-state-mgmt-03.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-state-mgmt-03.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@ietf.org\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-5410950"}, {"subject": "Vary and AcceptLanguag", "content": "In HTTP 1.1:\nIf the server sends Vary: Content-Language must it also\nsend a Content-Language header ?  (I would think so)\n\nIf the server has more than one language available, but the user agent \ndoes not send Accept-Language, need the server send \nVary: Content-Language at all ? If it does, what is the \naction of a proxy server on getting a request for that URL without\nan Accept-Language header ?\n\nHow about:\n\nURI=index.var, server preferred language is English\n\nUser Agent sends        Server sends           Proxy Server caches\n -                       english.html          URI=index.var\n                         Content-Language: en  Content=<english.html>\n \nAccept-Language: en      english.html          URI=index.var\n                         Content-Language: en  Content=<english.html>\n                         Vary: Accept-Language Content-Language=en\n\nAccept-Language: fr      french.html           URI=index.var\n                         Content-Language: fr  Content=<french.html>\n                         Vary: Accept-Language Content-Language=fr\n\nThis way, user agents who fail to set Accept-Language will\nget the default, proxy caches will be used, and agents such as\nrobots will know the language of the default document.\n\n\nCurrently (Netscape, Apache, HTTPD 1.0) the ordering of languages\nin Accept-Language indicates the user's desired weighting, but I\nbelieve the ordering in Accept (type) is not significant. In 1.1,\ndoes the ordering have any defined significance, or should people\nuse the qs scheme to indicate preference? \n\n\n(As regards my earlier question about robots, I don't think \nVary: Accept Language is very useful on its own (for a robot), so \nI will have to wait for a definition of the Alternates header).\n\nAndrew Daviel\n\nandrew@vancouver-webpages.com \nhttp://vancouver-webpages.com  : home of searchBC\n\n\n\n", "id": "lists-010-5422363"}, {"subject": "Re: Vary and AcceptLanguag", "content": "> In HTTP 1.1:\n> If the server sends Vary: Content-Language must it also\n> send a Content-Language header ?  (I would think so)\n\nIt's clear from the example later, you meant \"Vary:\nAccept-Language\". And no, there is no requirement that\nContent-Language be provided even though the response\nincludes \"Vary: accept-language\".\n\n> If the server has more than one language available, but the user agent\n> does not send Accept-Language, need the server send Vary:\n> Content-Language at all ?\n\nThe server must send \"Vary: accept-language\" if and only if the\nresponse would be different if a different accept-language would be\nsent.\n\n> If it does, what is the action of a proxy server on getting a request\n> for that URL without an Accept-Language header ?\n\n\"Accept-language not supplied\" is a different request from\n\"Accept-Language: en\" if the response says \"Vary: accept-language\".\n\n> How about:\n> URI=index.var, server preferred language is English\n> User Agent sends        Server sends           Proxy Server caches\n>  -                       english.html          URI=index.var\n>                          Content-Language: en  Content=<english.html>\n> Accept-Language: en      english.html          URI=index.var\n>                         Content-Language: en  Content=<english.html>\n>                         Vary: Accept-Language Content-Language=en\n> Accept-Language: fr      french.html           URI=index.var\n>                          Content-Language: fr  Content=<french.html>\n>                          Vary: Accept-Language Content-Language=fr\n\nNo, this is wrong. It's much simpler than this. If there's more than\none language, and the response would be different if you send a\ndifferent Accept-language, then the response must say \"Vary:\nAccept-Language\". It's also a good idea to send Content-Language to\nidentify the language you actually sent, but there's no requirement\nthat you send it, that the Content-Language actually match (any of\nthe) Accept-Languages sent, etc.\n\nIt is my belief that my responses are the only reasonable\ninterpretation of what draft-ietf-http-v11-spec-06 actually says.  Do\nyou disagree?\n\nLarry\n\n\n\n", "id": "lists-010-5431765"}, {"subject": "Re: Vary and AcceptLanguag", "content": "    In HTTP 1.1:\n    If the server sends Vary: Content-Language must it also\n    send a Content-Language header ?  (I would think so)\n\nThis question reflects a subtle misunderstanding of the Vary\nmechanism, although you are certainly not the first person\nto have made this mistake.\n\n\"Vary: Content-Language\" makes little sense, because\n(1) Fields named by \"Vary\" are expected to be *request*\nheaders, not response headers.  The description of \"Vary\" was\nrewritten several times during the last few weeks of the\ndrafting process, and it looks like we neglected to stress this\npoint emphatically enough.\n(2) Content-Language is normally a *response* header, at least\nfor the GET method.\n\nSo the question makes very little sense, since the premise is\nwrong: servers would not normally send \"Vary: Content-Language\".\n\n    If the server has more than one language available, but the user agent \n    does not send Accept-Language, need the server send \n    Vary: Content-Language at all?\n\nIn this case, the server would *still* send \"Vary: Accept-language\",\nsince any cache involved might receive future requests from other\nclients that do send Accept-Language.\n\n    If it does, what is the \n    action of a proxy server on getting a request for that URL without\n    an Accept-Language header?\n\nIf the cache sees \"Vary: Accept-language\" in a response and then\na subsequent request for the same resource without an Accept-Language\nheader, it must forward the request (perhaps as a conditional GET).\nSection 14.43 says this quite explicitly:\n    the cache MUST NOT use such a cache entry to construct a response\n    to the new request unless all of the headers named in the cached\n    Vary header are present in the new request\nNote the word \"all\" in that phrase.\n\n-Jeff\n\n\n\n", "id": "lists-010-5441277"}, {"subject": "http-state-mgmt03.txt change", "content": "Larry Masinter asked me to summarize the differences between the last\nstate management draft and this one.  Here goes...\n\n- removed the word \"proposed\" in various places.\n- fixed grammar for av-pair\n- clarified that Set-Cookie can accompany any response and Cookie, any\nrequest\n- added Comment attribute to Set-Cookie\n- reworded section about caching cookies in the face of HTTP/1.0 caches\nand HTTP/1.1 caches.\n\nIf you want to see the changes marked in context, go to\nhttp://www.research.att.com/~dmk/cookie.html\nand find the \"With diff marks\" entry near the top.\n\nDave Kristol\n\n\n\n", "id": "lists-010-5449885"}, {"subject": "Please publish draft-ietf-http-state-mgmt03.txt as an Internet Standar", "content": "The HTTP working group has achieved consensus that the document\n\n     draft-ietf-http-state-mgmt-03.txt\n\nis ready to be published as a Proposed Standard.\n\nThanks,\n\nLarry\n\n\n\n", "id": "lists-010-5456598"}, {"subject": "Re: Please publish draft-ietf-http-state-mgmt03.txt as an Internet Standar", "content": "Larry Masinter wrote:\n>\n> The HTTP working group has achieved consensus that the document\n> \n>      draft-ietf-http-state-mgmt-03.txt\n> \n> is ready to be published as a Proposed Standard.\n\nWhoa! That is awfully quick since the notice of its release is barely 24\nhours old.\n\nHow about giving us a chance to look it over first, eh?\n\n-Robert\n-- \nr-lentz@nwu.edu                       http://www.astro.nwu.edu/lentz/plan.html\n      \"The intellectual level of the schools can be no higher than the\n       intellectual level of the culture in which they float.\"\n                                                     -Richard Gibboney\n\n\n\n", "id": "lists-010-5465290"}, {"subject": "draft-ietf-http-v11-spec0", "content": "I was just going throught the draft and I ``think'' there may be a small\ntypo right in the beginning on the expiry date. \n\n-Vikram\n\n\n\n", "id": "lists-010-5473821"}, {"subject": "HTTP 1.1, rev 6 comment", "content": "I've been reading the draft HTTP/1.1 specification\n(draft-ietf-http-v11-spec-06) and have some comments.\n\n\nSection 3.2.1 General Syntax, page 16\n\nsince <segment> is *<pchar>, it can be empty, implying that\n\"foo/////bar\" is a valid <rel_path>.  Is this correct?\n\n\nSection 3.2.3 URI comparison, page 17\n\nWhat is the intended behavior of the comparison for two\ndifferent host identifiers (names or addresses) which refer to\nthe same host?  From the spec, one could conclude any of the\nfollowing:  (1) this issue was neglected; (2) the comparison\nalgorithm is spared the effort and expense of doing DNS\naccesses; (3) the comparison algorithm is prohibited from\ndoing DNS accesses.  The comparison algorithm's behavior\ntoward synonomous host identifiers, and the reason for that\nbehavior, should be explicitly stated.\n\n\n\n", "id": "lists-010-5480109"}, {"subject": "Re: I-D ACTION:draft-ietf-http-state-mgmt03.txt, .p", "content": "Greetings,\n\nThe current cookie proposal appears insufficient to assure a secure\nenvironment for providing state management in an authenticated system\nwhere multiple users have access to the same single-user machine.\n\nTo be specific, and provide an example, I will use the environment,\nand application, I am trying to use:\n\nThere will be a cookie as an identifier for an authenticated session during\nwhich the student will conduct online course work, possibly from a public\ncomputer lab. What I want to guard against is the possibility of subsequent\nusers of the same machine from being able to \"work\" as the previous student.\n\nRelying upon the default Max-Age behavior of not saving the cookie is\nnot an option. I use Max-Age to limit the validity of a session to guard\nagainst a student just walking away from their computer, leaving it\nunattended (much like auto-locking screen savers or idle timeouts on\nvarious shells, or kerberos tickets).\n\nYet I would also like for the cookie to disappear after one person's\n\"use\" of the client, whether this be signified by an actual quitting of\nthe client program, closing the browsing window, switching user environment,\netc.\n\nWhat I would propose is another standard attribute \"Single-user\".\nThis attribute would indicate not only that the cookie is not to be kept\nacross client invocations, but also that the cookie should be discarded\nafter any indication that the user has closed the session, such as closing\nthe window, switching user environments, etc. (And perhaps the cookie should\nnot be shared by multiple windows of the user agent unless the other windows\nare opened from the originating session?)\n\nthank you,\n-Robert\n-- \nr-lentz@nwu.edu                       http://www.astro.nwu.edu/lentz/plan.html\n      \"The intellectual level of the schools can be no higher than the\n       intellectual level of the culture in which they float.\"\n                                                     -Richard Gibboney\n\n\n\n", "id": "lists-010-5487945"}, {"subject": "RE: I-D ACTION:draft-ietf-http-state-mgmt03.txt, .p", "content": "It would seem to me that you're just up the creek with no paddle. From\nfirst principles, if the OS you are using is \"single user\", there's no\nway that multiple users can use it securely (even serially), and nothing\nany *protocol* can do to fix it.\n\nI'd say that you need a browser that will encrypt all of a user's\ncookies under a key known only to that user, never stores them in the\nclear, never leaves them in main memory in the clear for more than the\ntime required to send them to the server, and exits or demands the user\ntype a password if idle for more than a few minutes.\n\nThis approach basically uses cryptographic techniques to turn a \"single\nuser\" workstation into a serially resuable secure multiple user one (at\nleast as far as cookies go).  I don't think anything less will do.\n\nAll of which has nothing to do with the cookie protocol.\n\n\n>----------\n>From: Robert A. Lentz[SMTP:lentz@annie.astro.nwu.edu]\n>Sent: Tuesday, July 23, 1996 5:44 PM\n>To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: I-D ACTION:draft-ietf-http-state-mgmt-03.txt, .ps\n>\n>Greetings,\n>\n>The current cookie proposal appears insufficient to assure a secure\n>environment for providing state management in an authenticated system\n>where multiple users have access to the same single-user machine.\n>\n>To be specific, and provide an example, I will use the environment,\n>and application, I am trying to use:\n>\n>There will be a cookie as an identifier for an authenticated session during\n>which the student will conduct online course work, possibly from a public\n>computer lab. What I want to guard against is the possibility of subsequent\n>users of the same machine from being able to \"work\" as the previous student.\n>\n>Relying upon the default Max-Age behavior of not saving the cookie is\n>not an option. I use Max-Age to limit the validity of a session to guard\n>against a student just walking away from their computer, leaving it\n>unattended (much like auto-locking screen savers or idle timeouts on\n>various shells, or kerberos tickets).\n>\n>Yet I would also like for the cookie to disappear after one person's\n>\"use\" of the client, whether this be signified by an actual quitting of\n>the client program, closing the browsing window, switching user environment,\n>etc.\n>\n>What I would propose is another standard attribute \"Single-user\".\n>This attribute would indicate not only that the cookie is not to be kept\n>across client invocations, but also that the cookie should be discarded\n>after any indication that the user has closed the session, such as closing\n>the window, switching user environments, etc. (And perhaps the cookie should\n>not be shared by multiple windows of the user agent unless the other windows\n>are opened from the originating session?)\n>\n>thank you,\n>-Robert\n>-- \n>r-lentz@nwu.edu\n>http://www.astro.nwu.edu/lentz/plan.html\n>      \"The intellectual level of the schools can be no higher than the\n>       intellectual level of the culture in which they float.\"\n>                                                     -Richard Gibboney\n>\n>\n\n\n\n", "id": "lists-010-5497434"}, {"subject": "Re: HTTP 1.1, rev 6 comment", "content": "> I've been reading the draft HTTP/1.1 specification\n> (draft-ietf-http-v11-spec-06) and have some comments.\n> \n> \n> Section 3.2.1 General Syntax, page 16\n> \n> since <segment> is *<pchar>, it can be empty, implying that\n> \"foo/////bar\" is a valid <rel_path>.  Is this correct?\n\nYes.\n\n> Section 3.2.3 URI comparison, page 17\n> \n> What is the intended behavior of the comparison for two\n> different host identifiers (names or addresses) which refer to\n> the same host?  From the spec, one could conclude any of the\n> following:  (1) this issue was neglected; (2) the comparison\n> algorithm is spared the effort and expense of doing DNS\n> accesses; (3) the comparison algorithm is prohibited from\n> doing DNS accesses.  The comparison algorithm's behavior\n> toward synonomous host identifiers, and the reason for that\n> behavior, should be explicitly stated.\n\nDNS accesses are irrelevant -- there is no such thing as synonomous host\nidentifiers because the hostname may affect the resource chosen.  The\ncomparison is therefore limited to the contents of the URLs.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5510293"}, {"subject": "Re: I-D ACTION:draft-ietf-http-state-mgmt03.txt, .p", "content": "   From: Robert A. Lentz <lentz@annie.astro.nwu.edu>\n   Date: Tue, 23 Jul 1996 19:44:44 -0500 (CDT)\n\n   Greetings,\n\n   The current cookie proposal appears insufficient to assure a secure\n   environment for providing state management in an authenticated system\n   where multiple users have access to the same single-user machine.\n   ...\n\nI believe there are a number of solutions to this problem, none of\nwhich require changes to the protocol.\n\nOne example would be to store the user's password as part of the\nsession info on the server, and use it to encrypt/decrypt the cookie.\nWhen the first student ends their session (or it times out), the\ncookie stored on the browser side becomes meaningless until replaced\nwith a cookie for a new session.\n\nJoe\n\n----\nJoseph Arceneaux\nArceneaux Consulting\n\nhttp://www.arceneaux.com\njla@arceneaux.com\n+1 415 648 9988 (direct)\n+1 415 341 1395 (fax)\n+1 500 488 9308\n\n\n\n", "id": "lists-010-5518806"}, {"subject": "Re: Please publish draft-ietf-http-state-mgmt03.txt as an Internet Standar", "content": "> Whoa! That is awfully quick since the notice of its release is\n> barely 24 hours old.\n\nIt was my belief that all of the changes that were made were the\nresult of discussion in the group and agreed to.\n\nHowever, please _do_ review the document; if there are any changes\nthat are actually necessary, we'll deal with them, even if IESG's\nalready sent out the IESG \"last call\".\n\nLarry\n\n\n\n", "id": "lists-010-5527673"}, {"subject": "Re: HTTP/1.2 topics and beyon", "content": "\"Roy T. Fielding\" writes:\n\n| An implementation guide should be just that -- go through the steps\n| of typical and non-typical scenarios and describe how one might implement\n| each step according to the standard.  When that is done and approved by\n| actual implemeters of the protocol, go through the RFC (hopefully it will\n| be one by then) and decide what has been duplicated.\n\nHere's another way of thinking about the problem...  Over on the\nwww-proxy list we were discussing how to make the Web a bit more\nresilient, in the proxy sense at least.  This seems to mean\nthings like browsers supporting fallback proxy servers and the\nproxying of new protocol schemes, HTTP clients in general making\nuse of multiple A records and SRV when available, and honouring\nTTLs in cached DNS lookups.  As an organization, we could really\nuse HTTP implementations which were smart enough to do at least\nsome of the more elementary things.  Heck, support for multiple\nA records would be a *big* step forward for certain vendors'\nsoftware...\n\nSo, does this sort of thing belong in the HTTP specification ?\nThe spec has lots of implementation related info in it already,\nso there is precedent.  Dilemma: it's already quite big.  Is the\nnew stuff really important enough that it should be included ?\nJust another five pages or so ?  OK, maybe ten :-)\n\nMartin\n\n\n\n", "id": "lists-010-5535664"}, {"subject": "HTTP/1.1 draft 6, minor EDI", "content": "Section 13.1.4, says: \"...the user agent might habitually add\n'Cache-Control: max-stale=3600' or 'Cache-Control: reload'...\"\n\nLooking at 14.9 (definition if cache-control header) I realize that\nthe reload directive isn't define. I beleive 13.1.4 should rather say\n'Cache-Control: must-revalidate' (hum, as reload is not defined, it's\npretty difficult to guess the intended behavior).\n\nAnselm.\n\n\n\n", "id": "lists-010-5544184"}, {"subject": "Re: I-D ACTION:draft-ietf-http-state-mgmt03.txt, .p", "content": "lentz@annie.astro.nwu.edu wrote:\n|Yet I would also like for the cookie to disappear after one person's\n|\"use\" of the client, whether this be signified by an actual quitting of\n|the client program, closing the browsing window, switching user environment,\n|etc.\n\nThis is not a protocol issue, rather an implementation issue.  I'd recommended\nthe following language regarding the tossing of cookies that didn't make it\ninto the draft:\n\n+If a browser has a kiosk mode for use as a public terminal, and is configured\n+to accept cookies, then the user agent should be configurable to clear its\n+cookie cache (and any other per-user authentication data), either by an\n+explicit user \"log out\" command or by a timeout mechanism.\n\nThere is enough complexity to the unaddressed issues of cookie privacy that\naren't appropriate to the protocol specification that an informational document\non cookie practice is probably a good idea.\n\n-marc\n\n-- \n\n\n\n", "id": "lists-010-5550725"}, {"subject": "Re: HTTP/1.2 topics and beyon", "content": "> | An implementation guide should be just that -- go through the steps\n> | of typical and non-typical scenarios and describe how one might implement\n> | each step according to the standard.  When that is done and approved by\n> | actual implemeters of the protocol, go through the RFC (hopefully it will\n> | be one by then) and decide what has been duplicated.\n> \n> Here's another way of thinking about the problem...  Over on the\n> www-proxy list we were discussing how to make the Web a bit more\n> resilient, in the proxy sense at least.  This seems to mean\n> things like browsers supporting fallback proxy servers and the\n> proxying of new protocol schemes, HTTP clients in general making\n> use of multiple A records and SRV when available, and honouring\n> TTLs in cached DNS lookups.  As an organization, we could really\n> use HTTP implementations which were smart enough to do at least\n> some of the more elementary things.  Heck, support for multiple\n> A records would be a *big* step forward for certain vendors'\n> software...\n> \n> So, does this sort of thing belong in the HTTP specification ?\n\nNo, but it does belong in an implementation guide.  In fact, one\nof the big advantages of writing an implementation guide is that\nyou don't have to restrict the discussion to HTTP, nor do you have\nto consider all possible underlying transport mechanisms for HTTP.\n\n> The spec has lots of implementation related info in it already,\n> so there is precedent.  Dilemma: it's already quite big.  Is the\n> new stuff really important enough that it should be included ?\n> Just another five pages or so ?  OK, maybe ten :-)\n\nThe HTTP specification defines HTTP (not DNS, bind, TCP/IP, etc.).\nThe only times that the draft discusses things outside HTTP is when they\nare part of the design rationale of the protocol or security considerations.\nSome of those descriptions are more verbose than they would need to be\nif there was a separate implementation guide to reference, but they\nwould still be referenced from the spec.  Other things, like the discussion\nof handling broken connections, could be moved to an impementation guide\nwithout affecting the definition of HTTP.  However, it is important to\nnote that they are only part of the HTTP specification if they appear\ninside the specification or some other RFC that updates the specification\n(the set of standards-track documents associated with the protocol).\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5558962"}, {"subject": "Re: HTTP/1.2 topics and beyon", "content": "\"Roy T. Fielding\" writes:\n\n| The HTTP specification defines HTTP (not DNS, bind, TCP/IP, etc.).\n| The only times that the draft discusses things outside HTTP is when they\n| are part of the design rationale of the protocol or security considerations.\n| Some of those descriptions are more verbose than they would need to be\n| if there was a separate implementation guide to reference, but they\n| would still be referenced from the spec.  Other things, like the discussion\n| of handling broken connections, could be moved to an impementation guide\n| without affecting the definition of HTTP.  However, it is important to\n| note that they are only part of the HTTP specification if they appear\n| inside the specification or some other RFC that updates the specification\n| (the set of standards-track documents associated with the protocol).\n\nDefinitely!  My concern is twofold - firstly the specification has \nbecome nearly three times larger than HTTP 1.0 (which had problems, \nadmittedly), whilst only adding (IMHO) a small amount of extra \nfunctionality.  Secondly, the number of MUSTs and SHOULDs (another \n\"entry cost\") has gotten quite alarming -\n\n  % grep MUST draft-ietf-http-v11-spec-06.txt | wc -l\n       258\n\n  % grep SHOULD draft-ietf-http-v11-spec-06.txt | wc -l\n       189\n\nI'd argue that some of these do not, strictly speaking, belong in the \nspecification but have accumulated there over time because of the lack \nof anywhere else that was particularly appropriate for them.  These are \nthe MUSTs and SHOULDs which are really guidelines for implementors \nrather than basic requirements of the protocol.  It would be nice to \nhive these off into a separate document, but this may turn out to be \nunrealistic given the timescales involved...  have to wait and see.\n\nOne meta-comment: given that HTTP 1.1 requires so much from its \nimplementors, some sort of quick reference might be in order?  e.g. \nlisting the status of the various headers in clients' requests and \nservers' responses (MUST? SHOULD?  MAY?) would be a good start.  To \nsave on the page count, this could perhaps be worked into the table of \ncontents?\n\nSince I haven't received any other private comments on that list of \ncandidate sections, apart from a note along similar lines to Roy's from \nJeff Mogul, here's what I'm fixing on doing next...\n\n(1) Create a new document consisting (for now) of just those sections I \nreferred to in my earlier message\n\n(2) Mark sections/paragraphs/... which I believe ought to belong in the \nspecification\n\n(3) Post a URL for this document (it'll be too long to post the whole \nthing) to the list and invite comments\n\nBarring accidents I'd expect to have a first draft ready in about a \nweek's time.  If in the course of (2) I decide that the specification \nand implementation details are too tightly intertwined for me to \nseparate out, I'll post a \"bail out\" note to the list!\n\nHow does that sound ?\n\nMartin\n\n\n\n", "id": "lists-010-5569266"}, {"subject": "Re: HTTP/1.2 topics and beyon", "content": "Martin Hamilton:\n> Secondly, the number of MUSTs and SHOULDs (another \n>\"entry cost\") has gotten quite alarming -\n>\n>  % grep MUST draft-ietf-http-v11-spec-06.txt | wc -l\n>       258\n>\n>  % grep SHOULD draft-ietf-http-v11-spec-06.txt | wc -l\n>       189\n>\n>I'd argue that some of these do not, strictly speaking, belong in the \n>specification but have accumulated there over time because of the lack \n>of anywhere else that was particularly appropriate for them.  These are \n>the MUSTs and SHOULDs which are really guidelines for implementors \n>rather than basic requirements of the protocol.  It would be nice to \n>hive these off into a separate document, but this may turn out to be \n>unrealistic given the timescales involved...  have to wait and see.\n\nI don't think that many MUST and SHOULDs are really guidelines for\nimplementors, so moving them into a separate document would not gain\nyou that much.  All in all, I don't think that the 1.1 spec can be\nmade much smaller by creating an implementation guide.  It can be made\nmore accessible, but not much smaller.  The 1.1 spec is a bit like a\nmonolithic OS kernel: there are so many interdependencies in the text\nthat you can't easily remove parts without breaking the whole.\n\nMy take on the MUSTs and SHOULDs in 1.1 is that most of them are of\nthe form `if you do this, only then MUST/SHOULD you ...'.  I estimate\nthat about half of the MUSTs and SHOULDs in 1.1 only apply to the\nauthors of proxy caches, not to user agent and origin server authors.\n\n>One meta-comment: given that HTTP 1.1 requires so much from its \n>implementors, some sort of quick reference might be in order? \n\nYes, I think a quick reference is definitely in order.  What we need\nis a table that could serve as a reading guide: the table should give\ninformation like `if you are a user agent author, do not bother to\nread/understand section X.Y'.\n\nAt this point, I think that implementors wanting to upgrade their\nsoftware to 1.1 need a guide to deconstructing the HTTP/1.1 spec much\nmore than they need a guide to constructing HTTP/1.1 implementations.\n\n> e.g. \n>listing the status of the various headers in clients' requests and \n>servers' responses (MUST? SHOULD?  MAY?) would be a good start.\n\nYes. I've been thinking about tables like the ones below.  I filled\nthese tables from memory, so re-use with caution.  `MUST IF' means\n`MUST if particular conditions apply.'\n\nRequest header  generation  handling   generation  handling\n                by user     by origin  by caching  by caching\n(GET requests)  agent       server     proxy       proxy\n---------------|-----------|----------|-----------|----------|\n\nAccept            MAY        MAY          -          -\nAccept-Charset    MAY        MAY          -          -\nAccept-Encoding   MAY        MAY          -          MAY\nAccept-Language   MAY        MAY          -          -\nAuthorization     MAY        MAY          -          -\nCache-Control     MAY        MUST         MAY        MUST\nConnection        MAY        -            MAY        MUST?\nrom              MUST IF    -            -          -\nHost              MUST       MAY          MUST       -\nIf-Modified-Since MAY        MAY          MAY        MAY\nIf-Match          MAY        MAY          MAY        MAY\nIf-None-Match     MAY        MAY          MAY        MAY\nIf-Range          MAY        MAY          MAY        MAY\nIf-Unmodified-Since MAY      MAY          MAY        MAY\nMax-Forwards      -          -            -          -\nPragma            MAY        -            MAY        MUST?\nProxy-Authorization MAY      -            MAY        MAY\n    ..etc..\n\n\nResponse header handling    generation handling    generation\n                by user     by origin  by caching  by caching\n(GET responses) agent       server     proxy       proxy\n---------------|-----------|----------|-----------|----------|\n\nAccept-Ranges     MAY        MAY          MAY        MAY\nAge               MAY        -            MUST       MUST\nAllow             MAY        MAY          -          -\nCache-Control     MAY        MAY          MUST       -\n   ..etc..\nVary              MAY        MUST IF      MUST       -\n   ..etc..\n\n\n>Martin\n\nKoen.\n\n\n\n", "id": "lists-010-5579280"}, {"subject": "New content negotiation draft availabl", "content": "Hi,\n\nI have made available text and HTML versions of a new transparent\ncontent negotiation draft at:\n\n   http://gewis.win.tue.nl/~koen/conneg/\n\nThe new draft contains the first complete specification of HTTP\ntransparent content negotiation, so this is a good time to read and\nreview it all.  Also, this is the first draft detailed enough to allow\nthe building of test implementations.\n\nI am not planning any further big rewrites, my future work on this\ndraft will mostly be limited to processing comments from the wg.\n\nPlease post questions and comments directly to the list.  I am sure\nthat there are at least some mistakes in the draft...\n\nABSTRACT\n\n     HTTP/1.1 allows one to put multiple versions of the same\n     information under a single URL. Transparent content negotiation\n     is a mechanism, layered on top of HTTP/1.1, for automatically\n     selecting the best version when the URL is accessed. This enables\n     the smooth deployment of new web data formats and markup tags.\n\n     Design goals for transparent content negotiation include low\n     overhead on the request message size, downwards compatibility,\n     extensibility, enabling the rapid introduction of new areas of\n     negotiation, scalability, low cost of minimal support, end user\n     control, and good cachability.\n\nHappy reading!\n\nKoen.\n\n\n\n", "id": "lists-010-5590973"}, {"subject": "draft-ietf-http-state-mgmt03.tx", "content": "The \"security considerations\" section of the draft does not include\nany text regarding privacy concerns.\n\nHere's some suggested text:\n\nPRIVACY CONCERNS:\n\nThe protocol described in this draft can be used to keep track of the\nbrowsing habits of a user without the user's knowledge or permission.\nMany people consider this to be an unethical invasion of privacy.\n\nAny HTTP client implementing this protocol MUST provide at least three\noptions for the user:\n1) disable cookies entirely.\n2) ask the user before setting a cookie.\n3) set cookies without asking the user.\n\nThe default \"out of the box\" behavior of the client MUST NOT be #3.\n\nAny HTTP client should provide a way for the user to know which\ncookies are associated with a given page.\n\n\n\n", "id": "lists-010-5599053"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "On Wed, 31 Jul 1996, Bill Sommerfeld wrote:\n\n> The \"security considerations\" section of the draft does not include\n> any text regarding privacy concerns.\n> \n> Here's some suggested text:\n> \n> PRIVACY CONCERNS:\n> \n> The protocol described in this draft can be used to keep track of the\n> browsing habits of a user without the user's knowledge or permission.\n> Many people consider this to be an unethical invasion of privacy.\n> \n> Any HTTP client implementing this protocol MUST provide at least three\n> options for the user:\n> 1) disable cookies entirely.\n> 2) ask the user before setting a cookie.\n> 3) set cookies without asking the user.\n> \n> The default \"out of the box\" behavior of the client MUST NOT be #3.\n> \n> Any HTTP client should provide a way for the user to know which\n> cookies are associated with a given page.\n\nImplementation issue and *IMPOSSIBLE* to enforce. It clearly is not a\nprotocal item and so is beyond the scope of the workgroup. I can't imagine\na MUST NOT statement like that getting by the IETF. It is like specifying\nthat newsreaders MUST NOT present Usenet articles unthreaded by default. \nYou are treading on the toes of the implementers. While it is certainly *a\ngood idea* not to do 3 by default - it is not something that can be\nwritten into the protocal as a MUST NOT: SHOULD NOT, perhaps.\n\nYou are also running head on into a issue regarding privacy that I have\nalready shown is long out of the bag. Even *without* any kind of HTTP\nlevel cookie, I can already, to a quite high degree of accuracy, track\nindividual users.  Even between seperate cooperating sites, without\nletting them know I am doing so, if I so desire. Cookies raise no new\nprivacy concerns in that regard. It is, and has been, a red herring for a\nlong time. \n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-5607926"}, {"subject": "New document on &quot;Simple hitmetering for HTTP&quot", "content": "At the Montreal IETF, Paul Leach and I volunteered to write up a draft\nof our design for a simple hit-metering mechanism for HTTP.  We said\nwe'd have it done by the end of July; Larry Masinter said \"does that\nmean August 1?\", and I said \"no, it means July 31.\"\n\nSo (with a few hours to spare), we managed to get something done that\nwe think should work fairly well.\n\nOur goal was NOT to solve the general problem of collecting demographic\ninformation; it was to reduce the incentive for origin servers to\ndefeat caching merely so that they could collect simple hit-counts, of\na sort that the caches could just as easily collect for them.  However,\nwe believe that our design solves a large chunk of the problem, without\nsignificant complexity in the protocol or the implementations.\n\nI believe that the design we described originated with me; however,\nPaul was the one who had to explain to me how and why my design\nactually worked.  He also did all of the word-processing and a lot of\nthe drafting.  The two of us have generated at least seven drafts\nbefore subjecting the rest of you to it.\n\nYou can find a copy at\n\n    http://ftp.digital.com/~mogul/draft-ietf-http-hit-metering-00.txt\n\nWe invite constructive criticism of the draft; we don't expect it to be\nflawless.  But we'd appreciate that critics recognize that we are not\ntrying to solve every aspect of the demographics problem, but are\nproposing a design that does a decent job with minimal complexity.\n\n-Jeff\n\n\n\n", "id": "lists-010-5618550"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "sommerfeld@apollo.hp.com wrote\n> Any HTTP client implementing this protocol MUST provide at least three\n> options for the user:\n>  1) disable cookies entirely.\n>  2) ask the user before setting a cookie.\n>  3) set cookies without asking the user.\n>\n> The default \"out of the box\" behavior of the client MUST NOT be #3.\n\nsnowhare@netimages.com responded\n|Even between seperate cooperating sites, without\n|letting them know I am doing so, if I so desire. Cookies raise no new\n|privacy concerns in that regard. It is, and has been, a red herring for a\n|long time.\n\nThe issue here is not one of tracking, but resource control.  If a user is\nto create or modify a local persistent resource at the behest of a remote \nserver, then the user must have complete control over that process.  If the \nuser is expected to spend bandwidth transmitting what a remote server considers\nimportant, then knowledge of the state of that process should not require\ngymnastics on the part of the (perhaps non-technical) party that is paying \nfor the bandwidth and perhaps the content.  \n\nThe changing states of the state mechanism WILL be known to the server and \navailable to only the most vigilant users (following the path of least \nresistence, the way Navigator doesn't conform to this specificied proposal \nright now) unless ideal privacy guidelines are specified, with SHOULD or \nperhaps in a separate document, if necessary.\n\n-marc\n\n\n\n", "id": "lists-010-5630199"}, {"subject": "New document on &quot;HTTP/1.2 extension protocol&quot; SOO", "content": "Henrik similarly committed that we would have a new draft out by the\nend of July. We have substantially revised (simplified) our proposal\nfor PEP, but the spec has not been prepared to a point we are ready to\nsubmit to this list today. It will take an additional 2-4 business\ndays to collect internal comments and one-on-one review from the\ncontributors whom we have been working with throughout July.\n\nThe teaser slug: We have removed most of the complicated and\nunderspecified \"negotiation\" and \"compatibility\" mechanisms of draft\n-01 in favor of a simplified set of headers for using an extension\n(Protocol:), requesting if an extension will be acceptable\n(Protocol-Query:), advertising that a protocol is available\n(Protocol-Info:).\n\nMy apologies,\nRohit Khare\n\nRohit Khare -- World Wide Web Consortium -- Technical Staff\nw: 617/253-5884  --   f: 617/258-5999   --  h: 617/491-5030\nNE43-354,  MIT LCS,  545 Tech Square,  Cambridge,  MA 02139\n\n\n\n", "id": "lists-010-5640109"}, {"subject": "Re: HTTP/1.2 topics and beyon", "content": "I don't consider anything in the HTTP specification to be optional\nreading.  Any implemenenter of any client or server software, cache or\nno cache, must understand the entire document in order to implement\nHTTP correctly.  I know this may be difficult to do, but it is certainly\npossible to do and a hell of a lot easier than fixing the constant\nstream of bugs generated by people who don't.\n\nThe reason HTTP/1.1 is much bigger (in specification) than HTTP/1.0\nis simply because HTTP/1.0 does not work in the presence of caches\n(UA or proxy or server-based) or proxies.  It may seem like it works,\nsimply because they do exist, but the vast majority of perceived transfer\nproblems with the Web are due to the interaction between caching\nand intermediary applications that don't behave like browsers, and \nbrowser implementions that can't even parse the basic message syntax\nlet alone determine if a message has been truncated in transit.\n\nAll implementers must be aware of those interactions, regardless of what\nthey are implementing with HTTP, because they won't know what they need\nto implement (and why) until they do understand all those interactions.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-5647514"}, {"subject": "Serverside rendering engine", "content": "A recent question from Andrew Mutz prompted me to discuss a weakness\nof transparent content negotiation\n(http://gewis.win.tue.nl/~koen/conneg/) and a possible extension to\nfix it.  As this discussion is of more general interest, I'll repost\nit here.\n\n-snip--\n\n\nTransparent content negotiation is not really a mechanism to give a\nserver-side .gif scaler your exact screen width.  Many people find\nserver-side scaling evil (because of caching issues), and the draft\nmore or less reflects this sentiment by not trying to provide for it.\n\nHowever, maybe some applications we have not yet explored\n(high-quality printing?) will *require* exact dimensions to be\ncommunicated to a server-side rendering engine, no matter what the\ncaching issues.  If this is the case, then transparent content\nnegotiation could be extended to allow this.  For example, one could\ndefine a `send-tags' attribute which would cause the user agent to\nsend certain feature tag values when making a request on a\nvariant.\n\nThis would allow a (short) variant list like this:\n\n {\"paper.html\" 0.8 {type text/html}},\n {\"paper.tex\" 1.0 {type application/x-tex} \n        {features client-typeset-tex}},\n {\"paper.server-typeset.ps\" 1.0 {type application/postscript}\n        {features !client-typeset-tex}\n        {send-tags paper-size_* dpi printcolor print-gamma}}\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nin which the third variant really hides an infinite number of variants\nwhich could be generated by the server-side typesetting engine.\n\nWhen retrieving paper.server-typeset.ps, the presence of the send-tags\nattribute would cause the user agent to extend the\npaper.server-typeset.ps URI with the tags it supports (recognizes) and\ndo a get request:\n\n GET paper.server-typeset.ps?paper-size_a4&dpi=600 HTTP/1.1\n\nThis way, the exact tag values end up at the server side typesetting\nengine.\n\nThe form submission syntax is used for downwards compatibility\nreasons: this way list responses can contain a form giving access to\nthe server side typesetting engine.\n\nNote that the chances for caches to optimize such requests on\ntypesetting engines are small.  But this may not hurt that much if, as\nabove, the paper.server-typeset.ps variant is only a fallback for\nclients which do not have a client side typesetting engine.\n\n\nKoen.\n\n\n\n", "id": "lists-010-5655982"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "Bill Sommerfeld <sommerfeld@apollo.hp.com> writes:\n\n  > The \"security considerations\" section of the draft does not include\n  > any text regarding privacy concerns.\n\nThat's hardly true.  Section 7 is entitled PRIVACY; its first subsection,\n7.1 is entitled \"User Agent Control\".\n\n  > \n  > Here's some suggested text:\n  > \n  > PRIVACY CONCERNS:\n  > \n  > The protocol described in this draft can be used to keep track of the\n  > browsing habits of a user without the user's knowledge or permission.\n  > Many people consider this to be an unethical invasion of privacy.\n  > \n  > Any HTTP client implementing this protocol MUST provide at least three\n  > options for the user:\n  > 1) disable cookies entirely.\n  > 2) ask the user before setting a cookie.\n  > 3) set cookies without asking the user.\n\nThese are enumerated, in slightly different terms, in 7.1.\n  > \n  > The default \"out of the box\" behavior of the client MUST NOT be #3.\n  > \n  > Any HTTP client should provide a way for the user to know which\n  > cookies are associated with a given page.\n\nThe thrust of the privacy considerations throughout the document is\nto give the user control.  But I have to agree with Benjamin Franz\n(<snowhare@netimages.com>) that user agent behavior is outside the\nIETF's purview.  IETF can specify protocols, and they/we do that to\nensure interoperation.  I believe user agent behavior can be suggested\n(we do), and then we have to rely on public (and, dare I say it,\nmarketplace) pressure to shine a spotlight on implementations that\npose a risk to users' privacy.\n\nDave Kristol\n\n\n\n", "id": "lists-010-5664986"}, {"subject": "Authentication issu", "content": "My company is building an intranet product for the financial industry. Our\nfinancial exchanges requires the product to pass some strict rules which prompted\nme to ask the following question:\n\n-How to properly authenticate the user? A user name/password is not enough. If\n we provide a cookie machanism, what stops the user to pass along the cookie\n file to another user?\n\n-What is the best method to limit users to a single login, that is, if a user is\n logged in once to access our service, how to reject a second attempt from \n another machine? In a proxy world, identifying by the IP/Socket address will \n not help.\n \nAny help will be highly appreciated.\n----------------------------------------------------------------------\nSam Narang            ILX Systems Inc.            212-720-3140\nemail: samn@ilx.com   111 Fulton Street  212-312-2983 (fax)\n                      New York, NY  10038\n----------------------------------------------------------------------\n\n\n\n", "id": "lists-010-5674161"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "> Implementation issue and *IMPOSSIBLE* to enforce. \n\nTake a look at the RFC1123 (host requirements) some time; you'll see\nsubsections there like \"TELNET/USER INTERFACE\" \"FTP/USER INTERFACE\".\nThere are a fair number of MUSTs in there..\n\nOr the ipsec rfc's; they specify similar issues (e.g., the user MUST\nbe able to do manual keying).\n\nI missed the PRIVACY section entirely; it probably be moved into the\nsecurity considerations section, or a backpointer with\n`Privacy Issues: see section 7' added.\n\n- Bill\n\n\n\n", "id": "lists-010-5681449"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "> Implementation issue and *IMPOSSIBLE* to enforce. \n\nTake a look at the RFC1123 (host requirements) some time; you'll see\nsubsections there like \"TELNET/USER INTERFACE\" \"FTP/USER INTERFACE\".\nThere are a fair number of MUSTs in there..\n\nOr the ipsec rfc's; they specify similar issues (e.g., the user MUST\nbe able to do manual keying).\n\nI missed the PRIVACY section entirely; it probably be moved into the\nsecurity considerations section, or a backpointer with\n`Privacy Issues: see section 7' added.\n\n- Bill\n\n\n\n", "id": "lists-010-5689969"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "On Thu, 1 Aug 1996, Bill Sommerfeld wrote:\n\n> > Implementation issue and *IMPOSSIBLE* to enforce. \n> \n> Take a look at the RFC1123 (host requirements) some time; you'll see\n> subsections there like \"TELNET/USER INTERFACE\" \"FTP/USER INTERFACE\".\n> There are a fair number of MUSTs in there..\n\nAll of the MUSTs and MUST NOTs in RFC1123 are protocal issues which have\nthe clear potential to break interoperability if not addressed. Even the\nNVT issue is interoperability - not user display.  Failure to display to\nthe user when a cookie is set will not break the *operability* of the\nprotocal at any level. Failure to negotiate a minumum terminal type in\ntelnet would result in option negotiation breaking down fatally -\nsomething that would clearly break interoperability. RFC1123 is *not* an\nexample of implementation issues coded into the MUSTs and MUST NOTs. \n\n> Or the ipsec rfc's; they specify similar issues (e.g., the user MUST\n> be able to do manual keying).\n\nYou are going to have to be a lot more specific about which RFC you are\nreferring to. Context is everything and there are a LOT of RFCs.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-5698552"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "Please look at RFC1123.\n\nSee section 4.1.2.4 (the QUOTE command), which is a MUST.\n\nSee also 6.1.4.2, (DNS user interface), 6.1.4.3 (DNS abbreviation\nexpansion), etc., etc.,\n\nAll of these concern themselves exclusively with how host software\nforms protocol messages based on user input.  \n\n6.1.4.3 seems quite analagous to cookies, since it places restrictions\non how much searching a resolver can do:\n\n There is danger that a search-list mechanism will\n                 generate excessive queries to the root servers while\n                 testing whether user input is a complete domain name,\n                 lacking a final period to mark it as complete.  A\n                 search-list mechanism MUST have one of, and SHOULD have\n                 both of, the following two provisions to prevent this:\n\n                 (a)  The local resolver/name server can implement\n                      caching  of negative responses (see Section\n                      6.1.3.3).\n\n                 (b)  The search list expander can require two or more\n                      interior dots in a generated domain name before it\n                      tries using the name in a query to non-local\n                      domain servers, such as the root.\n\n- Bill\n\n\n\n", "id": "lists-010-5708451"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "Please look at RFC1123.\n\nSee section 4.1.2.4 (the QUOTE command), which is a MUST.\n\nSee also 6.1.4.2, (DNS user interface), 6.1.4.3 (DNS abbreviation\nexpansion), etc., etc.,\n\nAll of these concern themselves exclusively with how host software\nforms protocol messages based on user input.  \n\n6.1.4.3 seems quite analagous to cookies, since it places restrictions\non how much searching a resolver can do:\n\n There is danger that a search-list mechanism will\n                 generate excessive queries to the root servers while\n                 testing whether user input is a complete domain name,\n                 lacking a final period to mark it as complete.  A\n                 search-list mechanism MUST have one of, and SHOULD have\n                 both of, the following two provisions to prevent this:\n\n                 (a)  The local resolver/name server can implement\n                      caching  of negative responses (see Section\n                      6.1.3.3).\n\n                 (b)  The search list expander can require two or more\n                      interior dots in a generated domain name before it\n                      tries using the name in a query to non-local\n                      domain servers, such as the root.\n\n- Bill\n\n\n\n", "id": "lists-010-5717750"}, {"subject": "Re: draft-ietf-http-state-mgmt03.tx", "content": "On Thu, 1 Aug 1996, Bill Sommerfeld wrote:\n\n> Please look at RFC1123.\n> \n> See section 4.1.2.4 (the QUOTE command), which is a MUST.\n\n<quote>\nThe \"QUOTE\" command is essential to allow the user to\naccess servers that require system-specific commands\n(e.g., SITE or ALLO), or to invoke new or optional\nfeatures that are not implemented by the User-FTP.\n</quote>\n\nInteroperability issue - NOT user interface. It is saying you have to have\na protocal backdoor to handle versioning (it *is* a poor way to do\nversioning, however).\n\nTry again. \n\n> See also 6.1.4.2, (DNS user interface), \n\n<quote>\n6.1.4.2 DNS User Interface\n\nHosts MUST provide an interface to the DNS for all \napplication programs running on the host.  \n</quote>\n\nNot even a user interface issue. This is nothing more than saying a host\nhas to have a way to resolve DNS addresses and to report errors *to\nprograms*. NOTHING about users here.\n\n>6.1.4.3 (DNS abbreviation expansion), etc., etc.,\n\n<quote> \n6.1.4.3 Interface Abbreviation Facilities \n\nUser interfaces MAY provide a method for users to enter abbreviations for\ncommonly-used names. Although the definition of such methods is outside of\nthe scope of the DNS specification, certain rules are necessary to insure\nthat these methods allow access to the entire DNS name space and to\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nprevent excessive use of Internet resources. \n</quote>\n\nOperability again. They are saying if you don't make sure that aliases\ncan't be confused with actual DNS namespace - it ain't gonna work\nreliably.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-5727029"}, {"subject": "RE: draft-ietf-http-state-mgmt03.tx", "content": "The whole concept of cookies is marginal...since a good user-tracking\ndatabase can implement all of these things in a secure and robust\nmanner.\n\nThe problem?  \"good user-tracking databases\" are hard to come by and\n\"bad HTTP standards\" are easy.\n\nSeriously, everyone knows that cookies can be dangerous when implemented\non operating systems/browsers with out decent user authentication.\n\nI would rather propose:\n\nIt is recommended that Cookies not be used if your data is sensitive\nand/or password controlled.  Use a user tracking system ... and you will\nthank yourself in the end.  (SICK)\n\n\n\n>----------\n>From: marc@ckm.ucsf.edu[SMTP:marc@ckm.ucsf.edu]\n>Sent: Wednesday, July 31, 1996 9:17 PM\n>To: snowhare@netimages.com; sommerfeld%apollo.hp.com@hplb.hpl.hp.com\n>Cc: dmk@bell-labs.com; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com;\n>montulli@netscape.com\n>Subject: Re: draft-ietf-http-state-mgmt-03.txt\n>\n>\n>sommerfeld@apollo.hp.com wrote\n>> Any HTTP client implementing this protocol MUST provide at least three\n>> options for the user:\n>>  1) disable cookies entirely.\n>>  2) ask the user before setting a cookie.\n>>  3) set cookies without asking the user.\n>>\n>> The default \"out of the box\" behavior of the client MUST NOT be #3.\n>\n>snowhare@netimages.com responded\n>|Even between seperate cooperating sites, without\n>|letting them know I am doing so, if I so desire. Cookies raise no new\n>|privacy concerns in that regard. It is, and has been, a red herring\n>for a\n>|long time.\n>\n>The issue here is not one of tracking, but resource control.  If a user\n>is\n>to create or modify a local persistent resource at the behest of a\n>remote \n>server, then the user must have complete control over that process.  If\n>the \n>user is expected to spend bandwidth transmitting what a remote server\n>considers\n>important, then knowledge of the state of that process should not\n>require\n>gymnastics on the part of the (perhaps non-technical) party that is\n>paying \n>for the bandwidth and perhaps the content.  \n>\n>The changing states of the state mechanism WILL be known to the server\n>and \n>available to only the most vigilant users (following the path of least \n>resistence, the way Navigator doesn't conform to this specificied\n>proposal \n>right now) unless ideal privacy guidelines are specified, with SHOULD\n>or \n>perhaps in a separate document, if necessary.\n>\n>-marc\n>\n>\n\n\n\n", "id": "lists-010-5737019"}, {"subject": "HTTP working group statu", "content": "Here's my take on the current status and tasks for HTTP-WG. There's a\nlot going on, so I'm sure I've missed something. Please correct me if\nyou see anything.\n\nPrevious work:\n - HTTP/1.1:  draft-ietf-http-v11-spec-06.*\n   In IESG last call. Some typos and clarifications may happen as a\n   result of last call comments. I'm waiting to hear back from Jim\n   Gettys and also one of the area directors on process.\n   Some messages to WG have not been answered and need to be.\n\n - digest:   draft-ietf-http-digest-aa-04.txt\n   In IESG last call. No comments; we're waiting for IESG action.\n   \n - cookies:  draft-ietf-http-state-mgmt-03.*\n   Submitted for IESG last call, but last call has not been\n   issued. Some comments on \"privacy\" issues, but comments addressed\n   on list.\n\nRelated items:\n - SHTTP: draft-ietf-wts-shttp-03.txt\n   In IESG Last Call, new version was issued August 1.\n   No visible discussion in either WG.\n\nNew items:\nWe have to decide as a group whether HTTP needs each of these; that\nevaluation should happen over the next several weeks. It's important\nfor you to express your opinions.\n\n 1. \"simple hit-metering\" \n    http://ftp.digital.com/~mogul/draft-ietf-http-hit-metering-00.txt\n    recently released; will be Internet Draft soon.\n    Need working group comments.\n\n 2. Revised PEP draft\n    Rohit promises \"soon\".\n\n 3. content negotiation: http://gewis.win.tue.nl/~koen/conneg/\n    Recently released to WG, to be an Internet Draft.\n    Needs WG comments. Needs volunteer to help with editing.\n\n 4. User Agent characteristics: draft-mutz-http-attributes-01.txt\n    New draft will be available very soon.\n    Will need WG comments.\n\n 5. sticky headers\n    draft expected \"today\".\n\n 6. referrals proposal\n    draft expected \"early next week\"\n\n\n\n", "id": "lists-010-5750002"}, {"subject": "caching idea", "content": "Is there a proposal to add globally unique id's to HTTP?\nThere is a noticeable lack of unique object identifiers.  \n\nLarge objects are often duplicated on the Web....and UID's \nassigned to largish objects would reduce traffic.\n\nThese UID's  could comes through as response-header fields.\n\n\"Content-Origin: url | some-other-distinguished-name\"\n\"Content-Identifier: md5-coded-hash | some-other-authenticatable-id\"\n\nso when an indexer sees that....it can say \n\"well...i already have that...so i'll store it as an alternate location\nof the same thing\"\n\nor whatever....\n\n------------------------------------\n\nor maybe even a little dumb authentication protocol:\n\n-client uses a HEAD method\n\n-server X's response contains the header:\n\n\"Content-Identifier: purportedly-unique-content-id\"\n\n-client already has a copy of this document given to him by Y\n-....asks if server X if Y's copy is to be trusted\n\nOR some such nonsense.........\n\n\n\n", "id": "lists-010-5758232"}, {"subject": "Re: caching idea", "content": "> Is there a proposal to add globally unique id's to HTTP?\n> There is a noticeable lack of unique object identifiers.  \n> \n> Large objects are often duplicated on the Web....and UID's \n> assigned to largish objects would reduce traffic.\n\nWhile the idea is sound, some time ago I did some measure on our\nproxy cache: out of ~300 MB of files in the cache, only about 7MB\nwere duplicates with a different name. I only considered files with\nthe same size (after stripping metadata), so I might have missed\nsomething, say text files with different end-of-line conventions;\nalso, this test should really be repeated on a larger data set.\nAnyways, I am not very convinced that the saving are worth the\neffort of handling multiple headers for the same object (while I\nwas *before* doing this test).\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n", "id": "lists-010-5766507"}, {"subject": "Sticky header perlim draf", "content": "Here is a preliminary version of an Internet-draft about sticky headers\nand header name compression that I committed to write up at the Montreal\nIETF.\n\nComments are welcome, and will be incorporated before sending it in as\nan official draft.\n\n===============HTTP Working Group                              Paul J.\nLeach, Microsoft\nINTERNET-DRAFT\n<draft-ietf-http-sticky-00.txt>\nExpires January 11, 1996                                  July 11, 1996\n                Sticky Headers and Header Name Compression\n                           Preliminary Draft\nSTATUS OF THIS MEMO\nThis preliminary draft is submitted in response to a work item of the\nHTTP Working Group for HTTP/1.2, but does not at this time represent its\nconsensus.\nThis document is an Internet-Draft. Internet-Drafts are working\ndocuments of the Internet Engineering Task Force (IETF), its areas, and\nits working groups. Note that other groups may also distribute working\ndocuments as Internet-Drafts.\nInternet-Drafts are draft documents valid for a maximum of six months\nand may be updated, replaced, or obsoleted by other documents at any\ntime. It is inappropriate to use Internet-Drafts as reference material\nor to cite them other than as \"work in progress\".\nWARNING: The specification in this document is subject to change, and\nwill certainly change.  It is inappropriate AND STUPID to implement to\nthe proposed specification in this document.  In particular, anyone who\nimplements to this specification and then complains when it changes will\nbe properly viewed as an idiot, and any such complaints shall be\nignored. YOU HAVE BEEN WARNED.\nTo learn the current status of any Internet-Draft, please check the\n\"1id-abstracts.txt\" listing contained in the Internet-Drafts Shadow\nDirectories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\nmunnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\nftp.isi.edu (US West Coast).\nDistribution of this document is unlimited.  Please send comments to the\nHTTP working group at <http-wg@cuckoo.hpl.hp.com>.  Discussions of the\nworking group are archived at\n<URL:http://www.ics.uci.edu/pub/ietf/http/>.  General discussions about\nHTTP and the applications which use HTTP should take place on the <www-\ntalk@w3.org> mailing list.\nDO NOT IMPLEMENT TO THIS DOCUMENT                   Page [1]\nInternet-Draft         Sticky Headers               08/03/96\nABSTRACT\nThis draft defines two mechanisms to reduce the size of HTTP requests\nand responses. This will result in fewer bytes transmitted over the net,\nwhich is especially desirable for clients attached to the network via\nslow links, although it may also help increase efficiency of congested\nlinks, such as the intercontinental ones, as well. The mechanisms flow\nfrom the following two observations.\nThe first is that a sequence of HTTP requests and responses between a\ngiven client and server typically contains many headers whose values are\nthe same for each request and/or response. This draft proposed a way to\noptionally omit sending repetitive headers, instead allowing the sender\nto indicate to the receiver that the values from a previous message\nshould be used. Such headers are called \"sticky\". Special provisions are\nmade for proxies so that messages from many clients to a single server\ncan be multiplexed over a single connection and still take advantage of\nsticky headers.\nThe second is that HTTP header names are usually chosen to be indicative\nof their purpose, not for their short length. This draft proposes a way\nto compress header names using tersely encoded abbreviations.\nThe intent is that this document be incorporated into the HTTP/1.2\nspecification [1], if the HTTP working group accepts the design.\nHowever, the only dependencies in the design are that the client and\nserver support persistent connections, and that any proxies in the\ncommunication chain correctly implement the Connection header, so that\nit could be implemented in applications speaking earlier versions of\nHTTP.\nTable of Contents\n1. Introduction............................................3\n2. Overall Operation.......................................3\n 2.1 Basic operation ......................................4\n 2.2 Contexts .............................................4\n 2.3 Changing the sticky-header set .......................5\n3. Specification...........................................5\n 3.1 Sticky connection-token ..............................6\n 3.2 Sticky header ........................................6\n4. Examples................................................6\nDO NOT IMPLEMENT TO THIS DOCUMENT                   [Page 2]\nInternet-Draft         Sticky Headers               08/03/96\n5. Header name compression.................................8\n6. Security Considerations.................................9\n7. Acknowledgments.........................................9\n8. References.............................................10\n9. Author's address.......................................10\n1. Introduction\nThis draft introduces sticky headers and header name compression. Sticky\nheaders require two simple extensions to HTTP -- a new connection-token\nfor the Connection header, and a new header, the Sticky header. Header\nname compression merely introduces short equivalents for existing header\nnames, and describes a methodology for creating short names for headers\nadded to HTTP in the future.\nBy default, HTTP/1.1 connections are persistent, and there is a fairly\nwidely implemented extension to HTTP/1.0 (the \"Connection: Keep-Alive\"\nextension) which provides the same functionality when clients are\ncommunicating directly with origin servers.\nSticky headers operate over persistent connections, and their must be\nnegotiated.\nThis draft is being proposed as a component of HTTP version 1.2. It is\nbased on work initially done by the persistent connection committee of\nthe HTTP working group of the IETF.\nThe organization of this draft is as follows. In section 2 we describe\nthe overall operation of sticky headers,  then section 3 specifies the\nextension to the Connection header and the Sticky header, and section 4\nprovides some examples. In section 5 we specify header name compression.\n2. Overall Operation\nWe describe the overall operation in stages:\n  .  basic operation -- suitable for most user-agent to server\n     communication\n  .  contexts -- an optimization for proxy to server operation\n  .  changing the default sticky headers\nDO NOT IMPLEMENT TO THIS DOCUMENT                   [Page 3]\nInternet-Draft         Sticky Headers               08/03/96\n  .  backwards compatibility considerations\n2.1 Basic operation\nThe negotiation of the sticky headers option may take place on any\nrequest sent over a persistent connection. The client may add the\nconnection-token \"Sticky\" to the Connection header in a request; if the\nserver accepts the use of sticky headers, it responds with the same\ntoken in the Connection header of its response.\nOnce the use of sticky headers has been negotiated, specified message-\nheaders (see section 4.1 and 4.2 of [1]) are remembered from message to\nmessage, so that they need to be transmitted in a message only if they\nhave changed since the last message. The use of sticky headers continues\nuntil the connection is closed, without further need for the \"Sticky\"\nconnection-token on each request.\nAn examination of HTTP/1.1 header fields shows that the message-headers\nwith the following field-names are reasonably likely to have the same\nfield-values in consecutive requests from a user-agent to a particular\nserver:\n     Accept, Accept-Charset, Accept-Language, Accept-Encoding,\n     Authorization, Proxy-Authorization, From, Host, User-Agent\n     Note: Because there do not appear to be any response headers\n     whose field-values are likely to be repeated on consecutive\n     responses, other than the Server header, it was not deemed\n     useful to support sticky headers in the server to client\n     direction.\nBy default, message-headers with these field-names are the ones that\nmust be remembered, and they comprise the \"sticky-header set\" for a\nconnection. If any of them are not present in a message received by a\nserver, they are taken from the remembered set and added to the message\nbefore processing it. If any of them are present in a message, they\nreplace the instance in the remembered set. To send a message without\none of the remembered message-headers, send a message-header line\nconsisting of just the field-name and a colon, and no field-value; upon\nreception of such a line, the message-header in the remembered set with\nthat field-name is deleted.\nSection 2.3 provides a way to change the default sticky-header set.\n2.2 Contexts\nProxies can operate just like user-agents if they want. However, they\ntypically act on behalf of many clients, multiplexing a single\nconnection to a server across messages from many clients. Such\nDO NOT IMPLEMENT TO THIS DOCUMENT                   [Page 4]\nInternet-Draft         Sticky Headers               08/03/96\nmultiplexing will likely destroy the correlation between consecutive\nmessages that makes sticky headers an effective compression technique.\nTo better support multiplexed connections, we allow for the creation of\nmultiple sticky header \"contexts\" on a single persistent connection.\nOnce a context has been set up, each context operates with respect to\nsticky-headers just as if it were a separate persistent connection.\nI.e., there is a sticky-header set and a remembered set of message-\nheaders for each context.\nTo create a context, the Sticky header is sent by the client:\n     Sticky:   NNNN\nwhere NNNN is the context number. If the server response contains a\nSticky header with the same context number, then the context is set up,\notherwise it is refused. Once a context is created on an persistent\nconnection, subsequent messages that want to use the context include a\nSticky header with the context number used to create it, and it remains\nin effect until the connection is closed.\nThe context in use when no Sticky header is present is context 0 (zero).\n2.3 Changing the sticky-header set\nTo change the sticky-header set from the default, extra header field-\nnames can be listed in the Sticky header when a context is created. For\nexample, the following message-headers would negotiate the use of sticky\nheaders and add the Foo and Bar message-headers to the sticky header set\nfor the default context:\n     Connection: Sticky\n     Sticky: 0 Foo, Bar\nThe main purpose for this facility is to allow yet-to-be-invented\nheaders to be added to the sticky header set. There is no way to remove\nheaders from the sticky header set, or to add more headers to the set\nafter a context is created.\n2.4 Backwards compatibility\nA client may safely attempt to negotiate sticky headers with any server\nto which it may legally set up a persistent connection.\nIf negotiation is attempted to an origin-server that does not support\nsticky headers, it will fail, because the server will not include\nConnection: sticky in its response.\nDO NOT IMPLEMENT TO THIS DOCUMENT                   [Page 5]\nInternet-Draft         Sticky Headers               08/03/96\nIf negotiation is attempted to an HTTP/1.1 proxy that does not support\nit, the Connection and Sticky headers will be removed by the proxy and\nso the negotiation will fail because neither the proxy nor any inbound\nserver will include Connection: sticky in its response.\nNegotiation will not be attempted to HTTP/1.0 proxies, because it is not\nlegal to set up persistent connections with them.\n3. Specification\n3.1 Sticky connection-token\nThe Sticky connection-token is used as part of a Connection header to\nnegotiate the use of sticky headers. If a request contains a Connection\nheader with the \"sticky\" connection-token, the client is requesting the\nuse of sticky headers. If the server's response contains a Connection\nheader with the \"sticky\" connection-token, then it accepts the use of\nsticky headers.\n3.2 Sticky header\nThe Sticky general header may be used to create a sticky header context,\nto indicate that a message belongs to an existing sticky header context,\nor to add headers to the sticky header set.\n     Sticky    = \"Sticky\" \":\" context-id 0#field-name\n     context-id     = *DIGIT\n     field-name     = token        ; see section 4.2 of [1]\nThe context-id either requests the creation of a new context with that\nID, or indicates that this message belongs to an existing context with\nthat ID.\nField-names may be present only when creating a new context. If present,\nthen the client requests that message-headers with the given field-names\nbe made part of the sticky header set for the context.\nIf the creation of a new context is requested, the server indicates\nacceptance of the new context by returning a Sticky header with the same\ncontext-id in its response message.\n4. Examples\n     client:\n     (requesting first object, negotiating sticky headers)\n     GET / HTTP/1.1<CRLF>\n     Accept: text/html<CRLF>\nDO NOT IMPLEMENT TO THIS DOCUMENT                   [Page 6]\nInternet-Draft         Sticky Headers               08/03/96\n     Accept-Language: en<CRLF>\n     Connection: sticky<CRLF>\n     <CRLF>\n     server:\n     HTTP/1.1 200 OK<CRLF>\n     MIME-Version: 1.0<CRLF>\n     Connection: sticky<CRLF>\n     Content-Type: text/html<CRLF>\n     Content-Length:94<CRLF>\n     <CRLF>\n     <94 bytes body data here>\n     client:\n     (client makes a request changing the Accept value but maintaining\n     the value of the Accept-Language: field from the previous request)\n     GET /myimg.jpg HTTP/1.1<CRLF>\n     Accept: image/jpeg<CRLF>\n     <CRLF>\n     server:\n     HTTP/1.1 200 OK<CRLF>\n     MIME-Version: 1.0<CRLF>\n     Content-Type: image/jpeg<CRLF>\n     Content-Length:4000<CRLF>\n     <CRLF>\n     <4000 bytes body data here>\n     client:\n     (requests a second context)\n     GET /myimg.jpg HTTP/1.1<CRLF>\n     Accept: image/jpeg<CRLF>\n     Accept-Language: fr<CRLF>\n     Connection: sticky<CRLF>\n     Sticky: 1<CRLF>\n     <CRLF>\n     server:\n     (accepts second context)\n     HTTP/1.1 200 OK<CRLF>\n     MIME-Version: 1.0<CRLF>\n     Content-Type: image/jpeg<CRLF>\n     Content-Length:4000<CRLF>\n     Connection: sticky<CRLF>\n     Sticky: 1<CRLF>\n     <CRLF>\n     <4000 bytes body data here>\n     client:\n     (making a request in second context -- Accept-Language is \"fr\")\n     GET / HTTP/1.1<CRLF>\nDO NOT IMPLEMENT TO THIS DOCUMENT                   [Page 7]\nInternet-Draft         Sticky Headers               08/03/96\n     Accept: text/html<CRLF>\n     Connection: sticky<CRLF>\n     Sticky: 1<CRLF>\n     <CRLF>\n     server:\n     HTTP/1.1 200 OK<CRLF>\n     MIME-Version: 1.0<CRLF>\n     Content-Type: text/html<CRLF>\n     Content-Length:94<CRLF>\n     <CRLF>\n     <94 bytes body data here>\n5. Header name compression\nOnce sticky headers are negotiated, then either client or server can\nreplace a header name in the left hand column with its abbreviation in\nthe right hand column of the following table.\nAccept               #A\nAccept-Charset       #B\nAccept-Encoding      #C\nAccept-Language      #D\nAccept-Ranges        #E\nAge                  #F\nAllow                #G\nAuthorization        #H\nCache-Control        #I\nConnection           #J\nContent-Base         #K\nContent-Encoding     #L\nContent-Language     #M\nContent-Length       #N\nContent-Location     #O\nContent-MD5          #P\nContent-Range        #Q\nContent-Type         #R\nDate                 #S\nETag                 #T\nExpires              #U\nrom                 #V\nHost                 #W\nIf-Modified-Since    #X\nIf-Match             #Y\nIf-None-Match        #Z\nIf-Range             #a\nIf-Unmodified-Since  #b\nLast-Modified        #c\nLocation             #d\nMax-Forwards         #e\nDO NOT IMPLEMENT TO THIS DOCUMENT                   [Page 8]\nInternet-Draft         Sticky Headers               08/03/96\nPragma               #f\nProxy-Authenticate   #g\nProxy-Authorization  #h\nPublic               #I\nRange                #j\nReferer              #k\nRetry-After          #l\nServer               #m\nTransfer-Encoding    #n\nUpgrade              #o\nUser-Agent           #p\nVary                 #q\nVia                  #r\nWarning              #s\nWWW-Authenticate     #t\nThe character after the \"#\" is a base64 \"digit\" as per RFC 1521. For the\nheaders defined in HTTP/1.1 it comes from the range 0-45 decimal. There\ncan be 18 more headers added to HTTP and still only require one digit;\nat that point if more are added a second digit can handle up to 4096\nheaders.\nA static analysis of the effectiveness of this compression method shows\nthat the lengths of the uncompressed headers total 480 bytes, whereas\ncompressed, they total 92 bytes -- an 81% savings. The actual savings\nwill depend on the relative frequency of use of the different headers.\n6. Security Considerations\nThis HTTP extension has two indirect effects on security. Using sticky\nheaders can reduce the performance penalty of authentication, since a\nclient can send one authentication header and maintain that\nauthentication for a period of time. On the other hand an attacker could\ntheoretically intercept a previously initiated communication channel and\nsubstitute itself, gaining the authentication attributes of the client\nthat initiated the communication. This latter attack could only work\nwith the non-secure authentication methods anyway so it is not\nconsidered to be a serious concern.\n7. Acknowledgments\nAn earlier version of this design resulted from discussion of the HTTP\npersistent connection sub-working group, and was written up in an\nInternet-Draft by Alex Hopmann.\nDO NOT IMPLEMENT TO THIS DOCUMENT                   [Page 9]\nInternet-Draft         Sticky Headers               08/03/96\n8. References\n  [1]  Roy T. Fielding, et. al., Hypertext Transfer Protocol --\n     HTTP/1.1.  Internet-Draft draft-ietf-http-v11-spec-06.txt, HTTP\n     Working Group, July 4, 1996.\n9. Author's address\nPaul J. Leach\nMicrosoft\n1 Microsoft Way\nRedmond, Washington, 98052, U.S.A.\nEmail: paulle@microsoft.com\nDO NOT IMPLEMENT TO THIS DOCUMENT                  [Page 10]\n\n\n\n", "id": "lists-010-5775127"}, {"subject": "Re: caching idea", "content": "Luigi Rizzo writes:\n\n| While the idea is sound, some time ago I did some measure on our\n| proxy cache: out of ~300 MB of files in the cache, only about 7MB\n| were duplicates with a different name. I only considered files with\n| the same size (after stripping metadata), so I might have missed\n| something, say text files with different end-of-line conventions;\n| also, this test should really be repeated on a larger data set.\n| Anyways, I am not very convinced that the saving are worth the\n| effort of handling multiple headers for the same object (while I\n| was *before* doing this test).\n\nI got curious about this a little while back, and wrote a little Perl \nprogram to calculate MD5 checksums of the objects in our \n(local/regional ?) cache, so we could see how many were dups.  The \nresults weren't very encouraging...  <URL:http://www.roads.lut.ac.uk/lis\nts/ircache/0202.html>\n\nMartin\n\n\n\n", "id": "lists-010-5801411"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Jeffrey Mogul:\n>\n[...]\n>Our goal was NOT to solve the general problem of collecting demographic\n>information; it was to reduce the incentive for origin servers to\n>defeat caching merely so that they could collect simple hit-counts, of\n>a sort that the caches could just as easily collect for them.\n[...]\n>You can find a copy at\n>\n>    http://ftp.digital.com/~mogul/draft-ietf-http-hit-metering-00.txt\n[...]\n\nI have several comments.\n\n1. Cascaded proxy caches.\n\nAt first glance, there seem to be counting problems in a cascaded\nproxy cache situation.  If we have the arrangement\n\n   origin server ---- proxy 1 ------ proxy 2 ---- user agent\n\nand the user agent requests and uncached page, section 5.1 seems to\nsay that both proxy 1 and proxy 2 must set the use count to 1 when\nrelaying the page.  This results in a count of 2 being reported in the\nend, though the page is only viewed once.  It seems like there needs\nto be a special case for proxy 1: a proxy should not count if it is\nrelaying the response to another proxy.  (Under HTTP/1.1, the Via:\nheader in the request would tell you that you are talking to another\nproxy.)\n\n2. Number of unconditional GETs = number of times read???\n\nYou argue that the number of unconditional GETs, rather than the total\nnumber of GETs, more accurately reflects the number of times a page is\nread.  I don't know if this is true; I would like to see section 4\ndiscuss user agents on shared machines, and situations in which user\nagent disk caches are disabled entirely because there is a central\nproxy (like on our local sun cluster).\n\n3. A `hit' being an *un*conditional GET\n\nIn the current (classic) meaning of the word,\n\n  1 hit-classic = 1 request on an origin server.\n\nYour draft defines a new kind of hit:\n\n  1 hit-new = 1 200/203/206 response returned to a user agent.\n\nNow, if I am an origin server which uses cache busting, and if most\ncaches play by the rules, then for my server I will measure:\n\n  hit-new < hit-classic .\n\nAssuming that I get payed by the hit, I have absolutely no incentive\nto start measuring hit-news instead of hit-classics.  To stop using\nthe cache-busting based hit-classics would be economic suicide.\n\nSo even if hit-new is a better metric than hit-classic, I fear it\nwon't be effective at reducing cache busting.\n\nThe nicest solution to this problem seems to be for proxies to count\nboth hit-new and a second metric:\n\n  1 touch-new = 1 response returned to a user agent\n\nfor which it is guaranteed that\n\n   hit-classic <= touch-new .\n\n(Note: `hit' and `touch' would *not* be my proposals for adequate\nnames for these metrics.)\n\n4. Interaction with Vary\n\nI don't like the extra complexity and inefficiency introduced by the\nVary counting rules in section 3. (See second-to-last sentence of\nSection 5.1.)\n\nI think the proposal would be better if the Vary special case were\nremoved entirely.\n\n5. Overhead in proxy efficiency\n\nI'm wondering if the counting mechanisms in the draft won't cause an\nunacceptable overhead for high-performance cache implementations.  I\nthink we definitely need the opinions of proxy cache implementers on\nthis issue.\n\nOne possible hit counting alternative, post-processing proxy logfiles\nand delivering the results to the servers, seems to have less\noverhead.\n\n6. Max-uses mechanism\n\nThe max-uses mechanism seems to be a way for origin servers to specify\nan upper bound to the inaccuracy of their information.  \n\nBut to allocate max-uses values to proxies an efficient way, an origin\nserver seems to have to keeping per-proxy database of\n`max-use-qouta-use-speed' (last two paragraphs of Section 2), which\nadds some overhead to every request.  Reading these paragraphs, the\ngoal of the max-uses allocation heuristics seem to be to ensure that\nall counts are reported `soon enough'.\n\nIt seems that a max-time-to-wait-before-reporting-hits mechanism, can\nachieve the same goal without the same computational overhead in\norigin servers.  This mechanism would also eliminate the need for\nimplementing difficult max-use distribution heuristics in proxy\ncaches: a cache could simply subtract the age of the response from\nthe max-time value.\n\nEven better, we *already have* a\nmax-time-to-wait-before-reporting-hits mechanism in the form of\ncache-control: max-age.\n\nI conclude that the max-use mechanism is unnecessary and propose that\nit is removed, and that a section about using cache-control: max-age\nis added.\n\n7. Hit-counts for 302 responses\n\nSection 7 talks about hit counts of 302 responses, but the definitions\nin section 5.1 do not allow such counting.  This can be easily fixed\nby rewriting the definitions, they should probably enumerate the 2xx\nand 3xx class response codes which should *not* be counted (as\nhit-news).\n\n8. How big is the cache busting problem anyway?\n\nAbout a year ago, I tried to measure cache busting for the web content\naccessed through our local proxy.  Contrary to my expectations, I\ncould not find any definite signs of it.  I could find several\nresources and even whole servers which never sent Last-Modified\nheaders, but I accounted this to bad CGI programming more than to\n`malicious' intent.\n\nNow, a lot can happen in a year, and maybe the cache busting sites\nwhich did exist a year ago were not sites which would get accessed\n(often) from a Dutch university.  But I would like to see some\nstatistics/stories to indicate how big the cache busting problem\nreally is, since cache busting (not improving your site through better\nstatistics) seems to be the sole motivation the draft has for\nintroducing the counting mechanisms at all.\n\n\nKoen.\n\n\n\n", "id": "lists-010-5809398"}, {"subject": "Sticky header draft &ndash;&ndash; as an attachmen", "content": "I can never predict what the outbound mail gateway will do -- looks like\nit stripped all blank lines from the sticky header draft I included\ninline with my previous message. So, here it is as an attachment, which\nhopefully will be more readable.\n\nComments are still welcome.\n\n \n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\nbegin 600 sticky2.txt\nM#0H*\"@H*\"DA45% @5V]R:VEN9R!'<F]U<\" @(\" @(\" @(\" @(\" @(\" @(\" @\nM(\" @(\" @(\" @(%!A=6P@2BX@3&5A8V@L($UI8W)O<V]F= T*24Y415).150M\nM1%)!1E0-\"CQD<F%F=\"UI971F+6AT=' M<W1I8VMY+3 P+G1X=#X-\"D5X<&ER\nM97,@2F%N=6%R>2 Q,2P@,3DY-B @(\" @(\" @(\" @(\" @(\" @(\" @(\" @(\" @\nM(\" @(\" @(\"!*=6QY(#$Q+\" Q.3DV#0H*\"@H*(\" @(\" @(\" @(\" @(\" @(%-T\nM:6-K>2!(96%D97)S(&%N9\"!(96%D97(@3F%M92!#;VUP<F5S<VEO;@T*(\" @\nM(\" @(\" @(\" @(\" @(\" @(\" @(\" @(\" @4')E;&EM:6YA<GD@1')A9G0-\"@H*\nM\"@I35$%455,@3T8@5$A)4R!-14U/#0H*5&AI<R!P<F5L:6UI;F%R>2!D<F%F\nM=\"!I<R!S=6)M:71T960@:6X@<F5S<&]N<V4@=&\\@82!W;W)K(&ET96T@;V8@\nM=&AE#0I(5%10(%=O<FMI;F<@1W)O=7 @9F]R($A45% O,2XR+\"!B=70@9&]E\nM<R!N;W0@870@=&AI<R!T:6UE(')E<')E<V5N=\"!I=',-\"F-O;G-E;G-U<RX-\nM\"@I4:&ES(&1O8W5M96YT(&ES(&%N($EN=&5R;F5T+41R869T+B!);G1E<FYE\nM=\"U$<F%F=',@87)E('=O<FMI;F<-\"F1O8W5M96YT<R!O9B!T:&4@26YT97)N\nM970@16YG:6YE97)I;F<@5&%S:R!&;W)C92 H24541BDL(&ET<R!A<F5A<RP@\nM86YD#0II=',@=V]R:VEN9R!G<F]U<',N($YO=&4@=&AA=\"!O=&AE<B!G<F]U\nM<',@;6%Y(&%L<V\\@9&ES=')I8G5T92!W;W)K:6YG#0ID;V-U;65N=',@87,@\nM26YT97)N970M1')A9G1S+@T*\"DEN=&5R;F5T+41R869T<R!A<F4@9')A9G0@\nM9&]C=6UE;G1S('9A;&ED(&9O<B!A(&UA>&EM=6T@;V8@<VEX(&UO;G1H<PT*\nM86YD(&UA>2!B92!U<&1A=&5D+\"!R97!L86-E9\"P@;W(@;V)S;VQE=&5D(&)Y\nM(&]T:&5R(&1O8W5M96YT<R!A=\"!A;GD-\"G1I;64N($ET(&ES(&EN87!P<F]P\nM<FEA=&4@=&\\@=7-E($EN=&5R;F5T+41R869T<R!A<R!R969E<F5N8V4@;6%T\nM97)I86P-\"F]R('1O(&-I=&4@=&AE;2!O=&AE<B!T:&%N(&%S(\")W;W)K(&EN\nM('!R;V=R97-S(BX-\"@I705).24Y'.B!4:&4@<W!E8VEF:6-A=&EO;B!I;B!T\nM:&ES(&1O8W5M96YT(&ES('-U8FIE8W0@=&\\@8VAA;F=E+\"!A;F0-\"G=I;&P@\nM8V5R=&%I;FQY(&-H86YG92X@($ET(&ES(&EN87!P<F]P<FEA=&4@04Y$(%-4\nM55!)1\"!T;R!I;7!L96UE;G0@=&\\-\"G1H92!P<F]P;W-E9\"!S<&5C:69I8V%T\nM:6]N(&EN('1H:7,@9&]C=6UE;G0N(\"!);B!P87)T:6-U;&%R+\"!A;GEO;F4@\nM=VAO#0II;7!L96UE;G1S('1O('1H:7,@<W!E8VEF:6-A=&EO;B!A;F0@=&AE\nM;B!C;VUP;&%I;G,@=VAE;B!I=\"!C:&%N9V5S('=I;&P-\"F)E('!R;W!E<FQY\nM('9I97=E9\"!A<R!A;B!I9&EO=\"P@86YD(&%N>2!S=6-H(&-O;7!L86EN=',@\nM<VAA;&P@8F4-\"FEG;F]R960N(%E/52!(059%($)%14X@5T%23D5$+@T*\"E1O\nM(&QE87)N('1H92!C=7)R96YT('-T871U<R!O9B!A;GD@26YT97)N970M1')A\nM9G0L('!L96%S92!C:&5C:R!T:&4-\"B(Q:60M86)S=')A8W1S+G1X=\"(@;&ES\nM=&EN9R!C;VYT86EN960@:6X@=&AE($EN=&5R;F5T+41R869T<R!3:&%D;W<-\nM\"D1I<F5C=&]R:65S(&]N(&9T<\"YI<RYC;RYZ82 H069R:6-A*2P@;FEC+FYO\nM<F1U+FYE=\" H175R;W!E*2P-\"FUU;FYA<FDN;WHN874@*%!A8VEF:6,@4FEM\nM*2P@9',N:6YT97)N:6,N;F5T(\"A54R!%87-T($-O87-T*2P@;W(-\"F9T<\"YI\nM<VDN961U(\"A54R!797-T($-O87-T*2X-\"@I$:7-T<FEB=71I;VX@;V8@=&AI\nM<R!D;V-U;65N=\"!I<R!U;FQI;6ET960N(\"!0;&5A<V4@<V5N9\"!C;VUM96YT\nM<R!T;R!T:&4-\"DA45% @=V]R:VEN9R!G<F]U<\"!A=\" \\:'1T<\"UW9T!C=6-K\nM;V\\N:'!L+FAP+F-O;3XN(\"!$:7-C=7-S:6]N<R!O9B!T:&4-\"G=O<FMI;F<@\nM9W)O=7 @87)E(&%R8VAI=F5D(&%T#0H\\55),.FAT=' Z+R]W=W<N:6-S+G5C\nM:2YE9'4O<'5B+VEE=&8O:'1T<\"\\^+B @1V5N97)A;\"!D:7-C=7-S:6]N<R!A\nM8F]U= T*2%144\"!A;F0@=&AE(&%P<&QI8V%T:6]N<R!W:&EC:\"!U<V4@2%14\nM4\"!S:&]U;&0@=&%K92!P;&%C92!O;B!T:&4@/'=W=RT-\"G1A;&M =S,N;W)G\nM/B!M86EL:6YG(&QI<W0N#0H*\"@I$3R!.3U0@24U03$5-14Y4(%1/(%1(25,@\nM1$]#54U%3E0@(\" @(\" @(\" @(\" @(\" @(\" @4&%G92!;,5T-# T*\"@I);G1E\nM<FYE=\"U$<F%F=\" @(\" @(\" @(%-T:6-K>2!(96%D97)S(\" @(\" @(\" @(\" @\nM(\" @,#@O,#,O.38-\"@H*04)35%)!0U0-\"@I4:&ES(&1R869T(&1E9FEN97,@\nM='=O(&UE8VAA;FES;7,@=&\\@<F5D=6-E('1H92!S:7IE(&]F($A45% @<F5Q\nM=65S=',-\"F%N9\"!R97-P;VYS97,N(%1H:7,@=VEL;\"!R97-U;'0@:6X@9F5W\nM97(@8GET97,@=')A;G-M:71T960@;W9E<B!T:&4@;F5T+ T*=VAI8V@@:7,@\nM97-P96-I86QL>2!D97-I<F%B;&4@9F]R(&-L:65N=',@871T86-H960@=&\\@\nM=&AE(&YE='=O<FL@=FEA#0IS;&]W(&QI;FMS+\"!A;'1H;W5G:\"!I=\"!M87D@\nM86QS;R!H96QP(&EN8W)E87-E(&5F9FEC:65N8WD@;V8@8V]N9V5S=&5D#0IL\nM:6YK<RP@<W5C:\"!A<R!T:&4@:6YT97)C;VYT:6YE;G1A;\"!O;F5S+\"!A<R!W\nM96QL+B!4:&4@;65C:&%N:7-M<R!F;&]W#0IF<F]M('1H92!F;VQL;W=I;F<@\nM='=O(&]B<V5R=F%T:6]N<RX-\"@I4:&4@9FER<W0@:7,@=&AA=\"!A('-E<75E\nM;F-E(&]F($A45% @<F5Q=65S=',@86YD(')E<W!O;G-E<R!B971W965N(&$-\nM\"F=I=F5N(&-L:65N=\"!A;F0@<V5R=F5R('1Y<&EC86QL>2!C;VYT86EN<R!M\nM86YY(&AE861E<G,@=VAO<V4@=F%L=65S(&%R90T*=&AE('-A;64@9F]R(&5A\nM8V@@<F5Q=65S=\"!A;F0O;W(@<F5S<&]N<V4N(%1H:7,@9')A9G0@<')O<&]S\nM960@82!W87D@=&\\-\"F]P=&EO;F%L;'D@;VUI=\"!S96YD:6YG(')E<&5T:71I\nM=F4@:&5A9&5R<RP@:6YS=&5A9\"!A;&QO=VEN9R!T:&4@<V5N9&5R#0IT;R!I\nM;F1I8V%T92!T;R!T:&4@<F5C96EV97(@=&AA=\"!T:&4@=F%L=65S(&9R;VT@\nM82!P<F5V:6]U<R!M97-S86=E#0IS:&]U;&0@8F4@=7-E9\"X@4W5C:\"!H96%D\nM97)S(&%R92!C86QL960@(G-T:6-K>2(N(%-P96-I86P@<')O=FES:6]N<R!A\nM<F4-\"FUA9&4@9F]R('!R;WAI97,@<V\\@=&AA=\"!M97-S86=E<R!F<F]M(&UA\nM;GD@8VQI96YT<R!T;R!A('-I;F=L92!S97)V97(-\"F-A;B!B92!M=6QT:7!L\nM97AE9\"!O=F5R(&$@<VEN9VQE(&-O;FYE8W1I;VX@86YD('-T:6QL('1A:V4@\nM861V86YT86=E(&]F#0IS=&EC:WD@:&5A9&5R<RX-\"@I4:&4@<V5C;VYD(&ES\nM('1H870@2%144\"!H96%D97(@;F%M97,@87)E('5S=6%L;'D@8VAO<V5N('1O\nM(&)E(&EN9&EC871I=F4-\"F]F('1H96ER('!U<G!O<V4L(&YO=\"!F;W(@=&AE\nM:7(@<VAO<G0@;&5N9W1H+B!4:&ES(&1R869T('!R;W!O<V5S(&$@=V%Y#0IT\nM;R!C;VUP<F5S<R!H96%D97(@;F%M97,@=7-I;F<@=&5R<V5L>2!E;F-O9&5D\nM(&%B8G)E=FEA=&EO;G,N#0H*5&AE(&EN=&5N=\"!I<R!T:&%T('1H:7,@9&]C\nM=6UE;G0@8F4@:6YC;W)P;W)A=&5D(&EN=&\\@=&AE($A45% O,2XR#0IS<&5C\nM:69I8V%T:6]N(%LQ72P@:68@=&AE($A45% @=V]R:VEN9R!G<F]U<\"!A8V-E\nM<'1S('1H92!D97-I9VXN#0I(;W=E=F5R+\"!T:&4@;VYL>2!D97!E;F1E;F-I\nM97,@:6X@=&AE(&1E<VEG;B!A<F4@=&AA=\"!T:&4@8VQI96YT(&%N9 T*<V5R\nM=F5R('-U<'!O<G0@<&5R<VES=&5N=\"!C;VYN96-T:6]N<RP@86YD('1H870@\nM86YY('!R;WAI97,@:6X@=&AE#0IC;VUM=6YI8V%T:6]N(&-H86EN(&-O<G)E\nM8W1L>2!I;7!L96UE;G0@=&AE($-O;FYE8W1I;VX@:&5A9&5R+\"!S;R!T:&%T\nM#0II=\"!C;W5L9\"!B92!I;7!L96UE;G1E9\"!I;B!A<'!L:6-A=&EO;G,@<W!E\nM86MI;F<@96%R;&EE<B!V97)S:6]N<R!O9@T*2%144\"X-\"@H*5&%B;&4@;V8@\nM0V]N=&5N=',-\"@HQ+B!);G1R;V1U8W1I;VXN+BXN+BXN+BXN+BXN+BXN+BXN\nM+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+C,-\"@HR+B!/=F5R86QL($]P97)A\nM=&EO;BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+C,-\nM\"@H@,BXQ($)A<VEC(&]P97)A=&EO;B N+BXN+BXN+BXN+BXN+BXN+BXN+BXN\nM+BXN+BXN+BXN+BXN+BXN+C0-\"@H@,BXR($-O;G1E>'1S(\"XN+BXN+BXN+BXN\nM+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+C0-\"@H@,BXS($-H\nM86YG:6YG('1H92!S=&EC:WDM:&5A9&5R('-E=\" N+BXN+BXN+BXN+BXN+BXN\nM+BXN+BXN+C4-\"@HS+B!3<&5C:69I8V%T:6]N+BXN+BXN+BXN+BXN+BXN+BXN\nM+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+C4-\"@H@,RXQ(%-T:6-K>2!C;VYN\nM96-T:6]N+71O:V5N(\"XN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+C8-\nM\"@H@,RXR(%-T:6-K>2!H96%D97(@+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN\nM+BXN+BXN+BXN+BXN+BXN+C8-\"@HT+B!%>&%M<&QE<RXN+BXN+BXN+BXN+BXN\nM+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+C8-\"@I$3R!.3U0@\nM24U03$5-14Y4(%1/(%1(25,@1$]#54U%3E0@(\" @(\" @(\" @(\" @(\" @(\" @\nM6U!A9V4@,ET-# T*\"@I);G1E<FYE=\"U$<F%F=\" @(\" @(\" @(%-T:6-K>2!(\nM96%D97)S(\" @(\" @(\" @(\" @(\" @,#@O,#,O.38-\"@H*-2X@2&5A9&5R(&YA\nM;64@8V]M<')E<W-I;VXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN\nM+BXX#0H*-BX@4V5C=7)I='D@0V]N<VED97)A=&EO;G,N+BXN+BXN+BXN+BXN\nM+BXN+BXN+BXN+BXN+BXN+BXN+BXY#0H*-RX@06-K;F]W;&5D9VUE;G1S+BXN\nM+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXY#0H*.\"X@\nM4F5F97)E;F-E<RXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN\nM+BXN+BXN+BXN+C$P#0H*.2X@075T:&]R)W,@861D<F5S<RXN+BXN+BXN+BXN\nM+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+BXN+C$P#0H*\"@H*,2X@26YT<F]D\nM=6-T:6]N#0H*5&AI<R!D<F%F=\"!I;G1R;V1U8V5S('-T:6-K>2!H96%D97)S\nM(&%N9\"!H96%D97(@;F%M92!C;VUP<F5S<VEO;BX@4W1I8VMY#0IH96%D97)S\nM(')E<75I<F4@='=O('-I;7!L92!E>'1E;G-I;VYS('1O($A45% @+2T@82!N\nM97<@8V]N;F5C=&EO;BUT;VME;@T*9F]R('1H92!#;VYN96-T:6]N(&AE861E\nM<BP@86YD(&$@;F5W(&AE861E<BP@=&AE(%-T:6-K>2!H96%D97(N($AE861E\nM<@T*;F%M92!C;VUP<F5S<VEO;B!M97)E;'D@:6YT<F]D=6-E<R!S:&]R=\"!E\nM<75I=F%L96YT<R!F;W(@97AI<W1I;F<@:&5A9&5R#0IN86UE<RP@86YD(&1E\nM<V-R:6)E<R!A(&UE=&AO9&]L;V=Y(&9O<B!C<F5A=&EN9R!S:&]R=\"!N86UE\nM<R!F;W(@:&5A9&5R<PT*861D960@=&\\@2%144\"!I;B!T:&4@9G5T=7)E+@T*\nM\"D)Y(&1E9F%U;'0L($A45% O,2XQ(&-O;FYE8W1I;VYS(&%R92!P97)S:7-T\nM96YT+\"!A;F0@=&AE<F4@:7,@82!F86ER;'D-\"G=I9&5L>2!I;7!L96UE;G1E\nM9\"!E>'1E;G-I;VX@=&\\@2%144\"\\Q+C @*'1H92 B0V]N;F5C=&EO;CH@2V5E\nM<\"U!;&EV92(-\"F5X=&5N<VEO;BD@=VAI8V@@<')O=FED97,@=&AE('-A;64@\nM9G5N8W1I;VYA;&ET>2!W:&5N(&-L:65N=',@87)E#0IC;VUM=6YI8V%T:6YG\nM(&1I<F5C=&QY('=I=&@@;W)I9VEN('-E<G9E<G,N#0H*4W1I8VMY(&AE861E\nM<G,@;W!E<F%T92!O=F5R('!E<G-I<W1E;G0@8V]N;F5C=&EO;G,L(&%N9\"!T\nM:&5I<B!M=7-T(&)E#0IN96=O=&EA=&5D+@T*\"E1H:7,@9')A9G0@:7,@8F5I\nM;F<@<')O<&]S960@87,@82!C;VUP;VYE;G0@;V8@2%144\"!V97)S:6]N(#$N\nM,BX@270@:7,-\"F)A<V5D(&]N('=O<FL@:6YI=&EA;&QY(&1O;F4@8GD@=&AE\nM('!E<G-I<W1E;G0@8V]N;F5C=&EO;B!C;VUM:71T964@;V8-\"G1H92!(5%10\nM('=O<FMI;F<@9W)O=7 @;V8@=&AE($E%5$8N#0H*5&AE(&]R9V%N:7IA=&EO\nM;B!O9B!T:&ES(&1R869T(&ES(&%S(&9O;&QO=W,N($EN('-E8W1I;VX@,B!W\nM92!D97-C<FEB90T*=&AE(&]V97)A;&P@;W!E<F%T:6]N(&]F('-T:6-K>2!H\nM96%D97)S+\" @=&AE;B!S96-T:6]N(#,@<W!E8VEF:65S('1H90T*97AT96YS\nM:6]N('1O('1H92!#;VYN96-T:6]N(&AE861E<B!A;F0@=&AE(%-T:6-K>2!H\nM96%D97(L(&%N9\"!S96-T:6]N(#0-\"G!R;W9I9&5S('-O;64@97AA;7!L97,N\nM($EN('-E8W1I;VX@-2!W92!S<&5C:69Y(&AE861E<B!N86UE(&-O;7!R97-S\nM:6]N+@T*\"@HR+B!/=F5R86QL($]P97)A=&EO;@T*\"E=E(&1E<V-R:6)E('1H\nM92!O=F5R86QL(&]P97)A=&EO;B!I;B!S=&%G97,Z#0H*(\" N(\"!B87-I8R!O\nM<&5R871I;VX@+2T@<W5I=&%B;&4@9F]R(&UO<W0@=7-E<BUA9V5N=\"!T;R!S\nM97)V97(-\"B @(\" @8V]M;75N:6-A=&EO;@T*\"B @+B @8V]N=&5X=',@+2T@\nM86X@;W!T:6UI>F%T:6]N(&9O<B!P<F]X>2!T;R!S97)V97(@;W!E<F%T:6]N\nM#0H*(\" N(\"!C:&%N9VEN9R!T:&4@9&5F875L=\"!S=&EC:WD@:&5A9&5R<PT*\nM\"@I$3R!.3U0@24U03$5-14Y4(%1/(%1(25,@1$]#54U%3E0@(\" @(\" @(\" @\nM(\" @(\" @(\" @6U!A9V4@,UT-# T*\"@I);G1E<FYE=\"U$<F%F=\" @(\" @(\" @\nM(%-T:6-K>2!(96%D97)S(\" @(\" @(\" @(\" @(\" @,#@O,#,O.38-\"@H*(\" N\nM(\"!B86-K=V%R9',@8V]M<&%T:6)I;&ET>2!C;VYS:61E<F%T:6]N<PT*\"@HR\nM+C$@0F%S:6,@;W!E<F%T:6]N#0H*5&AE(&YE9V]T:6%T:6]N(&]F('1H92!S\nM=&EC:WD@:&5A9&5R<R!O<'1I;VX@;6%Y('1A:V4@<&QA8V4@;VX@86YY#0IR\nM97%U97-T('-E;G0@;W9E<B!A('!E<G-I<W1E;G0@8V]N;F5C=&EO;BX@5&AE\nM(&-L:65N=\"!M87D@861D('1H90T*8V]N;F5C=&EO;BUT;VME;B B4W1I8VMY\nM(B!T;R!T:&4@0V]N;F5C=&EO;B!H96%D97(@:6X@82!R97%U97-T.R!I9B!T\nM:&4-\"G-E<G9E<B!A8V-E<'1S('1H92!U<V4@;V8@<W1I8VMY(&AE861E<G,L\nM(&ET(')E<W!O;F1S('=I=&@@=&AE('-A;64-\"G1O:V5N(&EN('1H92!#;VYN\nM96-T:6]N(&AE861E<B!O9B!I=',@<F5S<&]N<V4N#0H*3VYC92!T:&4@=7-E\nM(&]F('-T:6-K>2!H96%D97)S(&AA<R!B965N(&YE9V]T:6%T960L('-P96-I\nM9FEE9\"!M97-S86=E+0T*:&5A9&5R<R H<V5E('-E8W1I;VX@-\"XQ(&%N9\" T\nM+C(@;V8@6S%=*2!A<F4@<F5M96UB97)E9\"!F<F]M(&UE<W-A9V4@=&\\-\"FUE\nM<W-A9V4L('-O('1H870@=&AE>2!N965D('1O(&)E('1R86YS;6ET=&5D(&EN\nM(&$@;65S<V%G92!O;FQY(&EF('1H97D-\"FAA=F4@8VAA;F=E9\"!S:6YC92!T\nM:&4@;&%S=\"!M97-S86=E+B!4:&4@=7-E(&]F('-T:6-K>2!H96%D97)S(&-O\nM;G1I;G5E<PT*=6YT:6P@=&AE(&-O;FYE8W1I;VX@:7,@8VQO<V5D+\"!W:71H\nM;W5T(&9U<G1H97(@;F5E9\"!F;W(@=&AE(\")3=&EC:WDB#0IC;VYN96-T:6]N\nM+71O:V5N(&]N(&5A8V@@<F5Q=65S=\"X-\"@I!;B!E>&%M:6YA=&EO;B!O9B!(\nM5%10+S$N,2!H96%D97(@9FEE;&1S('-H;W=S('1H870@=&AE(&UE<W-A9V4M\nM:&5A9&5R<PT*=VET:\"!T:&4@9F]L;&]W:6YG(&9I96QD+6YA;65S(&%R92!R\nM96%S;VYA8FQY(&QI:V5L>2!T;R!H879E('1H92!S86UE#0IF:65L9\"UV86QU\nM97,@:6X@8V]N<V5C=71I=F4@<F5Q=65S=',@9G)O;2!A('5S97(M86=E;G0@\nM=&\\@82!P87)T:6-U;&%R#0IS97)V97(Z#0H*(\" @(\"!!8V-E<'0L($%C8V5P\nM=\"U#:&%R<V5T+\"!!8V-E<'0M3&%N9W5A9V4L($%C8V5P=\"U%;F-O9&EN9RP-\nM\"B @(\" @075T:&]R:7IA=&EO;BP@4')O>'DM075T:&]R:7IA=&EO;BP@1G)O\nM;2P@2&]S=\"P@57-E<BU!9V5N= T*\"B @(\" @3F]T93H@0F5C875S92!T:&5R\nM92!D;R!N;W0@87!P96%R('1O(&)E(&%N>2!R97-P;VYS92!H96%D97)S#0H@\nM(\" @('=H;W-E(&9I96QD+79A;'5E<R!A<F4@;&EK96QY('1O(&)E(')E<&5A\nM=&5D(&]N(&-O;G-E8W5T:79E#0H@(\" @(')E<W!O;G-E<RP@;W1H97(@=&AA\nM;B!T:&4@4V5R=F5R(&AE861E<BP@:70@=V%S(&YO=\"!D965M960-\"B @(\" @\nM=7-E9G5L('1O('-U<'!O<G0@<W1I8VMY(&AE861E<G,@:6X@=&AE('-E<G9E\nM<B!T;R!C;&EE;G0-\"B @(\" @9&ER96-T:6]N+@T*\"D)Y(&1E9F%U;'0L(&UE\nM<W-A9V4M:&5A9&5R<R!W:71H('1H97-E(&9I96QD+6YA;65S(&%R92!T:&4@\nM;VYE<R!T:&%T#0IM=7-T(&)E(')E;65M8F5R960L(&%N9\"!T:&5Y(&-O;7!R\nM:7-E('1H92 B<W1I8VMY+6AE861E<B!S970B(&9O<B!A#0IC;VYN96-T:6]N\nM+B!)9B!A;GD@;V8@=&AE;2!A<F4@;F]T('!R97-E;G0@:6X@82!M97-S86=E\nM(')E8V5I=F5D(&)Y(&$-\"G-E<G9E<BP@=&AE>2!A<F4@=&%K96X@9G)O;2!T\nM:&4@<F5M96UB97)E9\"!S970@86YD(&%D9&5D('1O('1H92!M97-S86=E#0IB\nM969O<F4@<')O8V5S<VEN9R!I=\"X@268@86YY(&]F('1H96T@87)E('!R97-E\nM;G0@:6X@82!M97-S86=E+\"!T:&5Y#0IR97!L86-E('1H92!I;G-T86YC92!I\nM;B!T:&4@<F5M96UB97)E9\"!S970N(%1O('-E;F0@82!M97-S86=E('=I=&AO\nM=70-\"F]N92!O9B!T:&4@<F5M96UB97)E9\"!M97-S86=E+6AE861E<G,L('-E\nM;F0@82!M97-S86=E+6AE861E<B!L:6YE#0IC;VYS:7-T:6YG(&]F(&IU<W0@\nM=&AE(&9I96QD+6YA;64@86YD(&$@8V]L;VXL(&%N9\"!N;R!F:65L9\"UV86QU\nM93L@=7!O;@T*<F5C97!T:6]N(&]F('-U8V@@82!L:6YE+\"!T:&4@;65S<V%G\nM92UH96%D97(@:6X@=&AE(')E;65M8F5R960@<V5T('=I=&@-\"G1H870@9FEE\nM;&0M;F%M92!I<R!D96QE=&5D+@T*\"E-E8W1I;VX@,BXS('!R;W9I9&5S(&$@\nM=V%Y('1O(&-H86YG92!T:&4@9&5F875L=\"!S=&EC:WDM:&5A9&5R('-E=\"X-\nM\"@H*,BXR($-O;G1E>'1S#0H*4')O>&EE<R!C86X@;W!E<F%T92!J=7-T(&QI\nM:V4@=7-E<BUA9V5N=',@:68@=&AE>2!W86YT+B!(;W=E=F5R+\"!T:&5Y#0IT\nM>7!I8V%L;'D@86-T(&]N(&)E:&%L9B!O9B!M86YY(&-L:65N=',L(&UU;'1I\nM<&QE>&EN9R!A('-I;F=L90T*8V]N;F5C=&EO;B!T;R!A('-E<G9E<B!A8W)O\nM<W,@;65S<V%G97,@9G)O;2!M86YY(&-L:65N=',N(%-U8V@-\"@I$3R!.3U0@\nM24U03$5-14Y4(%1/(%1(25,@1$]#54U%3E0@(\" @(\" @(\" @(\" @(\" @(\" @\nM6U!A9V4@-%T-# T*\"@I);G1E<FYE=\"U$<F%F=\" @(\" @(\" @(%-T:6-K>2!(\nM96%D97)S(\" @(\" @(\" @(\" @(\" @,#@O,#,O.38-\"@H*;75L=&EP;&5X:6YG\nM('=I;&P@;&EK96QY(&1E<W1R;WD@=&AE(&-O<G)E;&%T:6]N(&)E='=E96X@\nM8V]N<V5C=71I=F4-\"FUE<W-A9V5S('1H870@;6%K97,@<W1I8VMY(&AE861E\nM<G,@86X@969F96-T:79E(&-O;7!R97-S:6]N('1E8VAN:7%U92X-\"@I4;R!B\nM971T97(@<W5P<&]R=\"!M=6QT:7!L97AE9\"!C;VYN96-T:6]N<RP@=V4@86QL\nM;W<@9F]R('1H92!C<F5A=&EO;B!O9@T*;75L=&EP;&4@<W1I8VMY(&AE861E\nM<B B8V]N=&5X=',B(&]N(&$@<VEN9VQE('!E<G-I<W1E;G0@8V]N;F5C=&EO\nM;BX-\"D]N8V4@82!C;VYT97AT(&AA<R!B965N('-E=\"!U<\"P@96%C:\"!C;VYT\nM97AT(&]P97)A=&5S('=I=&@@<F5S<&5C=\"!T;PT*<W1I8VMY+6AE861E<G,@\nM:G5S=\"!A<R!I9B!I=\"!W97)E(&$@<V5P87)A=&4@<&5R<VES=&5N=\"!C;VYN\nM96-T:6]N+@T*22YE+BP@=&AE<F4@:7,@82!S=&EC:WDM:&5A9&5R('-E=\"!A\nM;F0@82!R96UE;6)E<F5D('-E=\"!O9B!M97-S86=E+0T*:&5A9&5R<R!F;W(@\nM96%C:\"!C;VYT97AT+@T*\"E1O(&-R96%T92!A(&-O;G1E>'0L('1H92!3=&EC\nM:WD@:&5A9&5R(&ES('-E;G0@8GD@=&AE(&-L:65N=#H-\"@H@(\" @(%-T:6-K\nM>3H@(\"!.3DY.#0H*=VAE<F4@3DY.3B!I<R!T:&4@8V]N=&5X=\"!N=6UB97(N\nM($EF('1H92!S97)V97(@<F5S<&]N<V4@8V]N=&%I;G,@80T*4W1I8VMY(&AE\nM861E<B!W:71H('1H92!S86UE(&-O;G1E>'0@;G5M8F5R+\"!T:&5N('1H92!C\nM;VYT97AT(&ES('-E=\"!U<\"P-\"F]T:&5R=VES92!I=\"!I<R!R969U<V5D+B!/\nM;F-E(&$@8V]N=&5X=\"!I<R!C<F5A=&5D(&]N(&%N('!E<G-I<W1E;G0-\"F-O\nM;FYE8W1I;VXL('-U8G-E<75E;G0@;65S<V%G97,@=&AA=\"!W86YT('1O('5S\nM92!T:&4@8V]N=&5X=\"!I;F-L=61E(&$-\"E-T:6-K>2!H96%D97(@=VET:\"!T\nM:&4@8V]N=&5X=\"!N=6UB97(@=7-E9\"!T;R!C<F5A=&4@:70L(&%N9\"!I=\"!R\nM96UA:6YS#0II;B!E9F9E8W0@=6YT:6P@=&AE(&-O;FYE8W1I;VX@:7,@8VQO\nM<V5D+@T*\"E1H92!C;VYT97AT(&EN('5S92!W:&5N(&YO(%-T:6-K>2!H96%D\nM97(@:7,@<')E<V5N=\"!I<R!C;VYT97AT(# @*'IE<F\\I+@T*\"@HR+C,@0VAA\nM;F=I;F<@=&AE('-T:6-K>2UH96%D97(@<V5T#0H*5&\\@8VAA;F=E('1H92!S\nM=&EC:WDM:&5A9&5R('-E=\"!F<F]M('1H92!D969A=6QT+\"!E>'1R82!H96%D\nM97(@9FEE;&0M#0IN86UE<R!C86X@8F4@;&ES=&5D(&EN('1H92!3=&EC:WD@\nM:&5A9&5R('=H96X@82!C;VYT97AT(&ES(&-R96%T960N($9O<@T*97AA;7!L\nM92P@=&AE(&9O;&QO=VEN9R!M97-S86=E+6AE861E<G,@=V]U;&0@;F5G;W1I\nM871E('1H92!U<V4@;V8@<W1I8VMY#0IH96%D97)S(&%N9\"!A9&0@=&AE($9O\nM;R!A;F0@0F%R(&UE<W-A9V4M:&5A9&5R<R!T;R!T:&4@<W1I8VMY(&AE861E\nM<B!S970-\"F9O<B!T:&4@9&5F875L=\"!C;VYT97AT.@T*\"B @(\" @0V]N;F5C\nM=&EO;CH@4W1I8VMY#0H@(\" @(%-T:6-K>3H@,\"!&;V\\L($)A<@T*\"E1H92!M\nM86EN('!U<G!O<V4@9F]R('1H:7,@9F%C:6QI='D@:7,@=&\\@86QL;W<@>65T\nM+71O+6)E+6EN=F5N=&5D#0IH96%D97)S('1O(&)E(&%D9&5D('1O('1H92!S\nM=&EC:WD@:&5A9&5R('-E=\"X@5&AE<F4@:7,@;F\\@=V%Y('1O(')E;6]V90T*\nM:&5A9&5R<R!F<F]M('1H92!S=&EC:WD@:&5A9&5R('-E=\"P@;W(@=&\\@861D\nM(&UO<F4@:&5A9&5R<R!T;R!T:&4@<V5T#0IA9G1E<B!A(&-O;G1E>'0@:7,@\nM8W)E871E9\"X-\"@H*,BXT($)A8VMW87)D<R!C;VUP871I8FEL:71Y#0H*02!C\nM;&EE;G0@;6%Y('-A9F5L>2!A='1E;7!T('1O(&YE9V]T:6%T92!S=&EC:WD@\nM:&5A9&5R<R!W:71H(&%N>2!S97)V97(-\"G1O('=H:6-H(&ET(&UA>2!L96=A\nM;&QY('-E=\"!U<\"!A('!E<G-I<W1E;G0@8V]N;F5C=&EO;BX-\"@I)9B!N96=O\nM=&EA=&EO;B!I<R!A='1E;7!T960@=&\\@86X@;W)I9VEN+7-E<G9E<B!T:&%T\nM(&1O97,@;F]T('-U<'!O<G0-\"G-T:6-K>2!H96%D97)S+\"!I=\"!W:6QL(&9A\nM:6PL(&)E8V%U<V4@=&AE('-E<G9E<B!W:6QL(&YO=\"!I;F-L=61E#0I#;VYN\nM96-T:6]N.B!S=&EC:WD@:6X@:71S(')E<W!O;G-E+@T*\"@H*1$\\@3D]4($E-\nM4$Q%345.5\"!43R!42$E3($1/0U5-14Y4(\" @(\" @(\" @(\" @(\" @(\" @(%M0\nM86=E(#5=#0P-\"@H*26YT97)N970M1')A9G0@(\" @(\" @(\"!3=&EC:WD@2&5A\nM9&5R<R @(\" @(\" @(\" @(\" @(# X+S S+SDV#0H*\"DEF(&YE9V]T:6%T:6]N\nM(&ES(&%T=&5M<'1E9\"!T;R!A;B!(5%10+S$N,2!P<F]X>2!T:&%T(&1O97,@\nM;F]T('-U<'!O<G0-\"FET+\"!T:&4@0V]N;F5C=&EO;B!A;F0@4W1I8VMY(&AE\nM861E<G,@=VEL;\"!B92!R96UO=F5D(&)Y('1H92!P<F]X>2!A;F0-\"G-O('1H\nM92!N96=O=&EA=&EO;B!W:6QL(&9A:6P@8F5C875S92!N96ET:&5R('1H92!P\nM<F]X>2!N;W(@86YY(&EN8F]U;F0-\"G-E<G9E<B!W:6QL(&EN8VQU9&4@0V]N\nM;F5C=&EO;CH@<W1I8VMY(&EN(&ET<R!R97-P;VYS92X-\"@I.96=O=&EA=&EO\nM;B!W:6QL(&YO=\"!B92!A='1E;7!T960@=&\\@2%144\"\\Q+C @<')O>&EE<RP@\nM8F5C875S92!I=\"!I<R!N;W0-\"FQE9V%L('1O('-E=\"!U<\"!P97)S:7-T96YT\nM(&-O;FYE8W1I;VYS('=I=&@@=&AE;2X-\"@H*,RX@4W!E8VEF:6-A=&EO;@T*\nM\"@HS+C$@4W1I8VMY(&-O;FYE8W1I;VXM=&]K96X-\"@I4:&4@4W1I8VMY(&-O\nM;FYE8W1I;VXM=&]K96X@:7,@=7-E9\"!A<R!P87)T(&]F(&$@0V]N;F5C=&EO\nM;B!H96%D97(@=&\\-\"FYE9V]T:6%T92!T:&4@=7-E(&]F('-T:6-K>2!H96%D\nM97)S+B!)9B!A(')E<75E<W0@8V]N=&%I;G,@82!#;VYN96-T:6]N#0IH96%D\nM97(@=VET:\"!T:&4@(G-T:6-K>2(@8V]N;F5C=&EO;BUT;VME;BP@=&AE(&-L\nM:65N=\"!I<R!R97%U97-T:6YG('1H90T*=7-E(&]F('-T:6-K>2!H96%D97)S\nM+B!)9B!T:&4@<V5R=F5R)W,@<F5S<&]N<V4@8V]N=&%I;G,@82!#;VYN96-T\nM:6]N#0IH96%D97(@=VET:\"!T:&4@(G-T:6-K>2(@8V]N;F5C=&EO;BUT;VME\nM;BP@=&AE;B!I=\"!A8V-E<'1S('1H92!U<V4@;V8-\"G-T:6-K>2!H96%D97)S\nM+@T*\"@HS+C(@4W1I8VMY(&AE861E<@T*\"E1H92!3=&EC:WD@9V5N97)A;\"!H\nM96%D97(@;6%Y(&)E('5S960@=&\\@8W)E871E(&$@<W1I8VMY(&AE861E<B!C\nM;VYT97AT+ T*=&\\@:6YD:6-A=&4@=&AA=\"!A(&UE<W-A9V4@8F5L;VYG<R!T\nM;R!A;B!E>&ES=&EN9R!S=&EC:WD@:&5A9&5R(&-O;G1E>'0L#0IO<B!T;R!A\nM9&0@:&5A9&5R<R!T;R!T:&4@<W1I8VMY(&AE861E<B!S970N#0H*(\" @(\"!3\nM=&EC:WD@(\" @/2 B4W1I8VMY(B B.B(@8V]N=&5X=\"UI9\" P(V9I96QD+6YA\nM;64-\"B @(\" @8V]N=&5X=\"UI9\" @(\" @/2 J1$E'250-\"B @(\" @9FEE;&0M\nM;F%M92 @(\" @/2!T;VME;B @(\" @(\" @.R!S964@<V5C=&EO;B T+C(@;V8@\nM6S%=#0H*5&AE(&-O;G1E>'0M:60@96ET:&5R(')E<75E<W1S('1H92!C<F5A\nM=&EO;B!O9B!A(&YE=R!C;VYT97AT('=I=&@@=&AA= T*240L(&]R(&EN9&EC\nM871E<R!T:&%T('1H:7,@;65S<V%G92!B96QO;F=S('1O(&%N(&5X:7-T:6YG\nM(&-O;G1E>'0@=VET: T*=&AA=\"!)1\"X-\"@I&:65L9\"UN86UE<R!M87D@8F4@\nM<')E<V5N=\"!O;FQY('=H96X@8W)E871I;F<@82!N97<@8V]N=&5X=\"X@268@\nM<')E<V5N=\"P-\"G1H96X@=&AE(&-L:65N=\"!R97%U97-T<R!T:&%T(&UE<W-A\nM9V4M:&5A9&5R<R!W:71H('1H92!G:79E;B!F:65L9\"UN86UE<PT*8F4@;6%D\nM92!P87)T(&]F('1H92!S=&EC:WD@:&5A9&5R('-E=\"!F;W(@=&AE(&-O;G1E\nM>'0N#0H*268@=&AE(&-R96%T:6]N(&]F(&$@;F5W(&-O;G1E>'0@:7,@<F5Q\nM=65S=&5D+\"!T:&4@<V5R=F5R(&EN9&EC871E<PT*86-C97!T86YC92!O9B!T\nM:&4@;F5W(&-O;G1E>'0@8GD@<F5T=7)N:6YG(&$@4W1I8VMY(&AE861E<B!W\nM:71H('1H92!S86UE#0IC;VYT97AT+6ED(&EN(&ET<R!R97-P;VYS92!M97-S\nM86=E+@T*\"@HT+B!%>&%M<&QE<PT*\"B @(\" @8VQI96YT.@T*(\" @(\" H<F5Q\nM=65S=&EN9R!F:7)S=\"!O8FIE8W0L(&YE9V]T:6%T:6YG('-T:6-K>2!H96%D\nM97)S*0T*(\" @(\"!'150@+R!(5%10+S$N,3Q#4DQ&/@T*(\" @(\"!!8V-E<'0Z\nM('1E>'0O:'1M;#Q#4DQ&/@T*\"D1/($Y/5\"!)35!,14U%3E0@5$\\@5$A)4R!$\nM3T-5345.5\" @(\" @(\" @(\" @(\" @(\" @(\"!;4&%G92 V70T,#0H*\"DEN=&5R\nM;F5T+41R869T(\" @(\" @(\" @4W1I8VMY($AE861E<G,@(\" @(\" @(\" @(\" @\nM(\" P.\"\\P,R\\Y-@T*\"@H@(\" @($%C8V5P=\"U,86YG=6%G93H@96X\\0U),1CX-\nM\"B @(\" @0V]N;F5C=&EO;CH@<W1I8VMY/$-23$8^#0H@(\" @(#Q#4DQ&/@T*\nM\"B @(\" @<V5R=F5R.@T*(\" @(\"!(5%10+S$N,2 R,# @3TL\\0U),1CX-\"B @\nM(\" @34E-12U697)S:6]N.B Q+C \\0U),1CX-\"B @(\" @0V]N;F5C=&EO;CH@\nM<W1I8VMY/$-23$8^#0H@(\" @($-O;G1E;G0M5'EP93H@=&5X=\"]H=&UL/$-2\nM3$8^#0H@(\" @($-O;G1E;G0M3&5N9W1H.CDT/$-23$8^#0H@(\" @(#Q#4DQ&\nM/@T*(\" @(\" \\.30@8GET97,@8F]D>2!D871A(&AE<F4^#0H*(\" @(\"!C;&EE\nM;G0Z#0H@(\" @(\"AC;&EE;G0@;6%K97,@82!R97%U97-T(&-H86YG:6YG('1H\nM92!!8V-E<'0@=F%L=64@8G5T(&UA:6YT86EN:6YG#0H@(\" @('1H92!V86QU\nM92!O9B!T:&4@06-C97!T+4QA;F=U86=E.B!F:65L9\"!F<F]M('1H92!P<F5V\nM:6]U<R!R97%U97-T*0T*(\" @(\"!'150@+VUY:6UG+FIP9R!(5%10+S$N,3Q#\nM4DQ&/@T*(\" @(\"!!8V-E<'0Z(&EM86=E+VIP96<\\0U),1CX-\"B @(\" @/$-2\nM3$8^#0H*(\" @(\"!S97)V97(Z#0H@(\" @($A45% O,2XQ(#(P,\"!/2SQ#4DQ&\nM/@T*(\" @(\"!-24U%+59E<G-I;VXZ(#$N,#Q#4DQ&/@T*(\" @(\"!#;VYT96YT\nM+51Y<&4Z(&EM86=E+VIP96<\\0U),1CX-\"B @(\" @0V]N=&5N=\"U,96YG=&@Z\nM-# P,#Q#4DQ&/@T*(\" @(\" \\0U),1CX-\"B @(\" @/#0P,# @8GET97,@8F]D\nM>2!D871A(&AE<F4^#0H*(\" @(\"!C;&EE;G0Z#0H@(\" @(\"AR97%U97-T<R!A\nM('-E8V]N9\"!C;VYT97AT*0T*(\" @(\"!'150@+VUY:6UG+FIP9R!(5%10+S$N\nM,3Q#4DQ&/@T*(\" @(\"!!8V-E<'0Z(&EM86=E+VIP96<\\0U),1CX-\"B @(\" @\nM06-C97!T+4QA;F=U86=E.B!F<CQ#4DQ&/@T*(\" @(\"!#;VYN96-T:6]N.B!S\nM=&EC:WD\\0U),1CX-\"B @(\" @4W1I8VMY.B Q/$-23$8^#0H@(\" @(#Q#4DQ&\nM/@T*\"B @(\" @<V5R=F5R.@T*(\" @(\" H86-C97!T<R!S96-O;F0@8V]N=&5X\nM=\"D-\"B @(\" @2%144\"\\Q+C$@,C P($]+/$-23$8^#0H@(\" @($U)344M5F5R\nM<VEO;CH@,2XP/$-23$8^#0H@(\" @($-O;G1E;G0M5'EP93H@:6UA9V4O:G!E\nM9SQ#4DQ&/@T*(\" @(\"!#;VYT96YT+4QE;F=T:#HT,# P/$-23$8^#0H@(\" @\nM($-O;FYE8W1I;VXZ('-T:6-K>3Q#4DQ&/@T*(\" @(\"!3=&EC:WDZ(#$\\0U),\nM1CX-\"B @(\" @/$-23$8^#0H@(\" @(#PT,# P(&)Y=&5S(&)O9'D@9&%T82!H\nM97)E/@T*\"B @(\" @8VQI96YT.@T*(\" @(\" H;6%K:6YG(&$@<F5Q=65S=\"!I\nM;B!S96-O;F0@8V]N=&5X=\" M+2!!8V-E<'0M3&%N9W5A9V4@:7,@(F9R(BD-\nM\"B @(\" @1T54(\"\\@2%144\"\\Q+C$\\0U),1CX-\"@I$3R!.3U0@24U03$5-14Y4\nM(%1/(%1(25,@1$]#54U%3E0@(\" @(\" @(\" @(\" @(\" @(\" @6U!A9V4@-UT-\nM# T*\"@I);G1E<FYE=\"U$<F%F=\" @(\" @(\" @(%-T:6-K>2!(96%D97)S(\" @\nM(\" @(\" @(\" @(\" @,#@O,#,O.38-\"@H*(\" @(\"!!8V-E<'0Z('1E>'0O:'1M\nM;#Q#4DQ&/@T*(\" @(\"!#;VYN96-T:6]N.B!S=&EC:WD\\0U),1CX-\"B @(\" @\nM4W1I8VMY.B Q/$-23$8^#0H@(\" @(#Q#4DQ&/@T*\"B @(\" @<V5R=F5R.@T*\nM(\" @(\"!(5%10+S$N,2 R,# @3TL\\0U),1CX-\"B @(\" @34E-12U697)S:6]N\nM.B Q+C \\0U),1CX-\"B @(\" @0V]N=&5N=\"U4>7!E.B!T97AT+VAT;6P\\0U),\nM1CX-\"B @(\" @0V]N=&5N=\"U,96YG=&@Z.30\\0U),1CX-\"B @(\" @/$-23$8^\nM#0H@(\" @(#PY-\"!B>71E<R!B;V1Y(&1A=&$@:&5R93X-\"@H*-2X@2&5A9&5R\nM(&YA;64@8V]M<')E<W-I;VX-\"@I/;F-E('-T:6-K>2!H96%D97)S(&%R92!N\nM96=O=&EA=&5D+\"!T:&5N(&5I=&AE<B!C;&EE;G0@;W(@<V5R=F5R(&-A;@T*\nM<F5P;&%C92!A(&AE861E<B!N86UE(&EN('1H92!L969T(&AA;F0@8V]L=6UN\nM('=I=&@@:71S(&%B8G)E=FEA=&EO;B!I;@T*=&AE(')I9VAT(&AA;F0@8V]L\nM=6UN(&]F('1H92!F;VQL;W=I;F<@=&%B;&4N#0H*06-C97!T(\" @(\" @(\" @\nM(\" @(\" @(T$-\"D%C8V5P=\"U#:&%R<V5T(\" @(\" @(\"-\"#0I!8V-E<'0M16YC\nM;V1I;F<@(\" @(\" C0PT*06-C97!T+4QA;F=U86=E(\" @(\" @(T0-\"D%C8V5P\nM=\"U286YG97,@(\" @(\" @(\"-%#0I!9V4@(\" @(\" @(\" @(\" @(\" @(\" C1@T*\nM06QL;W<@(\" @(\" @(\" @(\" @(\" @(T<-\"D%U=&AO<FEZ871I;VX@(\" @(\" @\nM(\"-(#0I#86-H92U#;VYT<F]L(\" @(\" @(\" C20T*0V]N;F5C=&EO;B @(\" @\nM(\" @(\" @(TH-\"D-O;G1E;G0M0F%S92 @(\" @(\" @(\"-+#0I#;VYT96YT+45N\nM8V]D:6YG(\" @(\" C3 T*0V]N=&5N=\"U,86YG=6%G92 @(\" @(TT-\"D-O;G1E\nM;G0M3&5N9W1H(\" @(\" @(\"-.#0I#;VYT96YT+4QO8V%T:6]N(\" @(\" C3PT*\nM0V]N=&5N=\"U-1#4@(\" @(\" @(\" @(U -\"D-O;G1E;G0M4F%N9V4@(\" @(\" @\nM(\"-1#0I#;VYT96YT+51Y<&4@(\" @(\" @(\" C4@T*1&%T92 @(\" @(\" @(\" @\nM(\" @(\" @(U,-\"D5486<@(\" @(\" @(\" @(\" @(\" @(\"-4#0I%>'!I<F5S(\" @\nM(\" @(\" @(\" @(\" C50T*1G)O;2 @(\" @(\" @(\" @(\" @(\" @(U8-\"DAO<W0@\nM(\" @(\" @(\" @(\" @(\" @(\"-7#0I)9BU-;V1I9FEE9\"U3:6YC92 @(\" C6 T*\nM268M36%T8V@@(\" @(\" @(\" @(\" @(UD-\"DEF+4YO;F4M36%T8V@@(\" @(\" @\nM(\"-:#0I)9BU286YG92 @(\" @(\" @(\" @(\" C80T*268M56YM;V1I9FEE9\"U3\nM:6YC92 @(V(-\"DQA<W0M36]D:69I960@(\" @(\" @(\"-C#0I,;V-A=&EO;B @\nM(\" @(\" @(\" @(\" C9 T*36%X+49O<G=A<F1S(\" @(\" @(\" @(V4-\"@I$3R!.\nM3U0@24U03$5-14Y4(%1/(%1(25,@1$]#54U%3E0@(\" @(\" @(\" @(\" @(\" @\nM(\" @6U!A9V4@.%T-# T*\"@I);G1E<FYE=\"U$<F%F=\" @(\" @(\" @(%-T:6-K\nM>2!(96%D97)S(\" @(\" @(\" @(\" @(\" @,#@O,#,O.38-\"@H*4')A9VUA(\" @\nM(\" @(\" @(\" @(\" @(V8-\"E!R;WAY+4%U=&AE;G1I8V%T92 @(\"-G#0I0<F]X\nM>2U!=71H;W)I>F%T:6]N(\" C: T*4'5B;&EC(\" @(\" @(\" @(\" @(\" @(TD-\nM\"E)A;F=E(\" @(\" @(\" @(\" @(\" @(\"-J#0I2969E<F5R(\" @(\" @(\" @(\" @\nM(\" C:PT*4F5T<GDM069T97(@(\" @(\" @(\" @(VP-\"E-E<G9E<B @(\" @(\" @\nM(\" @(\" @(\"-M#0I4<F%N<V9E<BU%;F-O9&EN9R @(\" C;@T*57!G<F%D92 @\nM(\" @(\" @(\" @(\" @(V\\-\"E5S97(M06=E;G0@(\" @(\" @(\" @(\"-P#0I687)Y\nM(\" @(\" @(\" @(\" @(\" @(\" C<0T*5FEA(\" @(\" @(\" @(\" @(\" @(\" @(W(-\nM\"E=A<FYI;F<@(\" @(\" @(\" @(\" @(\"-S#0I75U<M075T:&5N=&EC871E(\" @\nM(\" C= T*\"@I4:&4@8VAA<F%C=&5R(&%F=&5R('1H92 B(R(@:7,@82!B87-E\nM-C0@(F1I9VET(B!A<R!P97(@4D9#(#$U,C$N($9O<B!T:&4-\"FAE861E<G,@\nM9&5F:6YE9\"!I;B!(5%10+S$N,2!I=\"!C;VUE<R!F<F]M('1H92!R86YG92 P\nM+30U(&1E8VEM86PN(%1H97)E#0IC86X@8F4@,3@@;6]R92!H96%D97)S(&%D\nM9&5D('1O($A45% @86YD('-T:6QL(&]N;'D@<F5Q=6ER92!O;F4@9&EG:70[\nM#0IA=\"!T:&%T('!O:6YT(&EF(&UO<F4@87)E(&%D9&5D(&$@<V5C;VYD(&1I\nM9VET(&-A;B!H86YD;&4@=7 @=&\\@-# Y-@T*:&5A9&5R<RX-\"@I!('-T871I\nM8R!A;F%L>7-I<R!O9B!T:&4@969F96-T:79E;F5S<R!O9B!T:&ES(&-O;7!R\nM97-S:6]N(&UE=&AO9\"!S:&]W<PT*=&AA=\"!T:&4@;&5N9W1H<R!O9B!T:&4@\nM=6YC;VUP<F5S<V5D(&AE861E<G,@=&]T86P@-#@P(&)Y=&5S+\"!W:&5R96%S\nM#0IC;VUP<F5S<V5D+\"!T:&5Y('1O=&%L(#DR(&)Y=&5S(\"TM(&%N(#@Q)2!S\nM879I;F=S+B!4:&4@86-T=6%L('-A=FEN9W,-\"G=I;&P@9&5P96YD(&]N('1H\nM92!R96QA=&EV92!F<F5Q=65N8WD@;V8@=7-E(&]F('1H92!D:69F97)E;G0@\nM:&5A9&5R<RX-\"@H*-BX@4V5C=7)I='D@0V]N<VED97)A=&EO;G,-\"@I4:&ES\nM($A45% @97AT96YS:6]N(&AA<R!T=V\\@:6YD:7)E8W0@969F96-T<R!O;B!S\nM96-U<FET>2X@57-I;F<@<W1I8VMY#0IH96%D97)S(&-A;B!R961U8V4@=&AE\nM('!E<F9O<FUA;F-E('!E;F%L='D@;V8@875T:&5N=&EC871I;VXL('-I;F-E\nM(&$-\"F-L:65N=\"!C86X@<V5N9\"!O;F4@875T:&5N=&EC871I;VX@:&5A9&5R\nM(&%N9\"!M86EN=&%I;B!T:&%T#0IA=71H96YT:6-A=&EO;B!F;W(@82!P97)I\nM;V0@;V8@=&EM92X@3VX@=&AE(&]T:&5R(&AA;F0@86X@871T86-K97(@8V]U\nM;&0-\"G1H96]R971I8V%L;'D@:6YT97)C97!T(&$@<')E=FEO=7-L>2!I;FET\nM:6%T960@8V]M;75N:6-A=&EO;B!C:&%N;F5L(&%N9 T*<W5B<W1I='5T92!I\nM='-E;&8L(&=A:6YI;F<@=&AE(&%U=&AE;G1I8V%T:6]N(&%T=')I8G5T97,@\nM;V8@=&AE(&-L:65N= T*=&AA=\"!I;FET:6%T960@=&AE(&-O;6UU;FEC871I\nM;VXN(%1H:7,@;&%T=&5R(&%T=&%C:R!C;W5L9\"!O;FQY('=O<FL-\"G=I=&@@\nM=&AE(&YO;BUS96-U<F4@875T:&5N=&EC871I;VX@;65T:&]D<R!A;GEW87D@\nM<V\\@:70@:7,@;F]T#0IC;VYS:61E<F5D('1O(&)E(&$@<V5R:6]U<R!C;VYC\nM97)N+@T*\"@HW+B!!8VMN;W=L961G;65N=',-\"@I!;B!E87)L:65R('9E<G-I\nM;VX@;V8@=&AI<R!D97-I9VX@<F5S=6QT960@9G)O;2!D:7-C=7-S:6]N(&]F\nM('1H92!(5%10#0IP97)S:7-T96YT(&-O;FYE8W1I;VX@<W5B+7=O<FMI;F<@\nM9W)O=7 L(&%N9\"!W87,@=W)I='1E;B!U<\"!I;B!A;@T*26YT97)N970M1')A\nM9G0@8GD@06QE>\"!(;W!M86YN+@T*\"@H*\"@I$3R!.3U0@24U03$5-14Y4(%1/\nM(%1(25,@1$]#54U%3E0@(\" @(\" @(\" @(\" @(\" @(\" @6U!A9V4@.5T-# T*\nM\"@I);G1E<FYE=\"U$<F%F=\" @(\" @(\" @(%-T:6-K>2!(96%D97)S(\" @(\" @\nM(\" @(\" @(\" @,#@O,#,O.38-\"@H*.\"X@4F5F97)E;F-E<PT*\"B @6S%=(\"!2\nM;WD@5\"X@1FEE;&1I;F<L(&5T+B!A;\"XL($AY<&5R=&5X=\"!4<F%N<V9E<B!0\nM<F]T;V-O;\" M+0T*(\" @(\"!(5%10+S$N,2X@($EN=&5R;F5T+41R869T(&1R\nM869T+6EE=&8M:'1T<\"UV,3$M<W!E8RTP-BYT>'0L($A45% -\"B @(\" @5V]R\nM:VEN9R!'<F]U<\"P@2G5L>2 T+\" Q.3DV+@T*\"@HY+B!!=71H;W(G<R!A9&1R\nM97-S#0H*4&%U;\"!*+B!,96%C: T*36EC<F]S;V9T#0HQ($UI8W)O<V]F=\"!7\nM87D-\"E)E9&UO;F0L(%=A<VAI;F=T;VXL(#DX,#4R+\"!5+E,N02X-\"D5M86EL\nM.B!P875L;&5 ;6EC<F]S;V9T+F-O;0T*\"@H*\"@H*\"@H*\"@H*\"@H*\"@H*\"@H*\nM\"@H*\"@H*\"@H*\"@H*\"@H*\"@I$3R!.3U0@24U03$5-14Y4(%1/(%1(25,@1$]#\nB54U%3E0@(\" @(\" @(\" @(\" @(\" @(\"!;4&%G92 Q,%T-#%1/\n`\nend\n\n\n\n", "id": "lists-010-5822942"}, {"subject": "Re: Sticky header draft &ndash;&ndash; as an attachmen", "content": "Your draft says:\n\n    The negotiation of the sticky headers option may take place on any\n    request sent over a persistent connection. The client may add the\n    connection-token \"Sticky\" to the Connection header in a request; if the\n    server accepts the use of sticky headers, it responds with the same\n    token in the Connection header of its response.\n    \n    Once the use of sticky headers has been negotiated, specified message-\n    headers (see section 4.1 and 4.2 of [1]) are remembered from message to\n    message, so that they need to be transmitted in a message only if they\n    have changed since the last message. The use of sticky headers continues\n    until the connection is closed, without further need for the \"Sticky\"\n    connection-token on each request.\n\nI think this leads to an ambiguous situation when the client is\npipelining requests.  We identified this ambiguity at the meeting\nwe had in January of the persistent-connections subgroup.\n\nConsider this **somewhat contrived** example\n\n     client:\n     ## requesting first object, negotiating sticky headers ##\n     GET / HTTP/1.1<CRLF>\n     Accept: text/html<CRLF>\n     Accept-Language: en<CRLF>\n     Connection: sticky<CRLF>\n     <CRLF>\n\n     server:\n     HTTP/1.1 200 OK<CRLF>\n     MIME-Version: 1.0<CRLF>\n     Connection: sticky<CRLF>\n     Content-Type: text/html<CRLF>\n     Content-Length:94<CRLF>\n     <CRLF>\n     <94 bytes body data here>\n\n     client:\n     ## requesting second object, changing one of the to-be-sticky headers ##\n     GET /a.gif HTTP/1.1<CRLF>\n     Accept: image/gif<CRLF>\n     <CRLF>\n     ## requesting 3rd object, again changing 1 of the to-be-sticky headers ##\n     GET /b.jpeg HTTP/1.1<CRLF>\n     Accept: image/jpeg<CRLF>\n     ## requesting 4th object, not sending one of the to-be-sticky headers ##\n     GET /c.gif HTTP/1.1<CRLF>\n     <CRLF>\n\nOK, so what are the effective request headers for the 4th request?\nYour design implicitly disallows the client from assuming \"stickiness\"\nuntil it has received\n     Connection: sticky<CRLF>\nfrom the server.  However, because there is no way for the server\nto tell whether the client received that header before or after\nsending the fourth request, the server cannot unambiguously know\nwhat the client means (i.e., should the fourth request be interpreted\nas having \"Accept: image/jpeg\" or not?)\n\nOne could argue that this kind of thing could never happen in practice,\nand so my contrived example (which admittedly is pretty foolish on the\npart of the client) is not worth discussing.  But the race condition\nseems to be intrinsic in the current draft design, and I'd feel more\nconfident if there were either a proof that it is not really a problem,\nor a design modification that prevented the ambiguity.\n\nI can see several possible ways to solve this:\n(1) Explicitly declare that the client, after having sent\n\"Connection: sticky\", may start omitting headers as soon as it\nreceives the server's \"Connection: sticky\" response, but MUST\nNOT change any such headers before receiving a response to the\nfirst request sent *after* it has received that \"Connection:\nsticky\" response from the server.  This rule has to be\ninterpreted \"per connection\".  (If you draw out the timing\ndiagram, you can see that this will always avoid the race\ncondition.)\n\n(2) Change the proposed protocol so that the client explicitly\nsignals to the server that negotiation is over and it is\nstarting to use the sticky-header mechanism.  This is\nsimilar to how Telnet negotiation works.  E.g.,\n\nclient sends\nConnection: want-sticky ->\nserver replies\n<- Connection: sticky-OK\nclient sends\nConnection: doing-sticky ->\n\nimmediately after which the client can start taking full\nadvantage of the sticky-header mechanism, one RTT sooner than\nwith option #1.  You could still get by with a single \"sticky\"\ntoken, by adopting the rule that the client must send it a\nsecond time, but only after receiving it from the server.\n\nRegarding:\n    2.2 Contexts\n    \n    Proxies can operate just like user-agents if they want. However, they\n    typically act on behalf of many clients, multiplexing a single\n    connection to a server across messages from many clients. Such\n    multiplexing will likely destroy the correlation between consecutive\n    messages that makes sticky headers an effective compression technique.\n\nIt might be a good idea for you to provide a means for negotiating\nmultiple contexts over a single proxy-server connection, but I am\nnot sure it's wise to implicitly bless the multiplexing of request\nstreams from several clients.  This can lead to something akin to\nthe \"head of line blocking\" problem seen in network switches:\n\n    Head of line blocking occurs when a packet at the head of an input\n    queue blocks, thereby preventing a packet behind it from using an\n    available output port. The problem is common to networks which\n    employ input FIFOs which prevent packets from passing one another.\n    The solution to head of line blocking is to use random access\n    buffers which permit packets to be forwarded out of order.\n\n    quoted from \"High Performance Communication for Distributed Systems\"\n    by William Stasior,\n    http://www.tns.lcs.mit.edu/~wstasior/distrib_sys_comm/distrib_sys_comm.html\n\nConsider the case where proxy P is combining request streams from\nclient C1 and client C2 over the same TCP connection to server S.\nSince we don't allow reordering of requests (i.e., we use \"input FIFOs\"\nat the server side), if C2 makes a request that takes a long time to\nanswer, C1 may have to wait even though there is no intrinsic reason\nfor this.  The situation is especially bad if S is actually proxy\nas well, and C1 and C2 are really making their requests to unrelated\norigin servers S1 and S2.  If S2 is down, the C1->S1 request is stalled\nuntil the C2->S2 request times out.\n\nThis is why draft-ietf-http-v11-spec-06.txt says\n\n8.1.4 Practical Considerations\n\n[...] A proxy SHOULD use up to 2*N connections to another\nserver or proxy, where N is the number of simultaneously active\nusers. These guidelines are intended to improve HTTP response\ntimes and avoid congestion of the Internet or other networks.\n\n-Jeff\n\n\n\n", "id": "lists-010-5871470"}, {"subject": "Re: Sticky header draft &ndash;&ndash; as an attachmen", "content": "On Mon, 5 Aug 1996, Jeffrey Mogul wrote:\n\n> I think this leads to an ambiguous situation when the client is\n> pipelining requests.  We identified this ambiguity at the meeting\n> we had in January of the persistent-connections subgroup.\n\nI think the ambiguity is also resolved if we change HTTP/1.1 to not\nallow pipelining UNTIL the first server response is received accepting\nthe persistent connection and hence pipelining. That response will\neither include the server acceptance of the STICKY more or it won't.\nBut there will be no ambiguity for the server to interpret.\n\nDave Morris\n\n\n\n", "id": "lists-010-5885554"}, {"subject": "Sitcky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": ">----------\n>From: David W. Morris[SMTP:dwm@shell.portal.com]\n>Subject: Re: Sticky header \n>\n>On Mon, 5 Aug 1996, Jeffrey Mogul wrote:\n>\n>> I think this leads to an ambiguous situation when the client is\n>> pipelining requests.  We identified this ambiguity at the meeting\n>> we had in January of the persistent-connections subgroup.\n>\n>I think the ambiguity is also resolved if we change HTTP/1.1 to not\n>allow pipelining UNTIL the first server response is received accepting\n>the persistent connection and hence pipelining.\n\nI thought that this was already the rule. Did it get lost when we made\npersistence the default?\n\n> That response will\n>either include the server acceptance of the STICKY more or it won't.\n>But there will be no ambiguity for the server to interpret.\n>\n>Dave Morris\n>\n>\n\n\n\n", "id": "lists-010-5894632"}, {"subject": "Content negotiation implementations availabl", "content": "In the past few days, I have coded some experimental implementations\nto go along with the new transparent content negotiation draft.  (I\nhad almost forgotten what fun coding is!)  Both the draft and the\nimplementations are available under\nhttp://gewis.win.tue.nl/~koen/conneg/ .\n\nI implemented two things: \n - a transparently negotiated resource\n - conneg-uax, the content negotiating user agent extension.\n\n1) a transparently negotiated resource.\n\nThis is a very minimal implementation (as minimal as the draft allows)\nconsisting of a CGI script short enough to repeat here:\n\n--------snip--------\n#!/bin/sh\n\ncat - <<'blex'\nStatus: 300 Multiple Choices\nAlternates: {\"stats.tables.html\" 1.0 {type text/html} {features tables}},\n{\"stats.html\" 0.8 {type text/html}}, {\"stats.ps\" 0.95 {type\napplication/postscript}}\nVary: *\nContent-Type: text/html\n\n<title>Multiple Choices for Web Statistics</title>\n<h2>Multiple Choices for Web Statistics:</h2>\n<ul>\n<li><a href=stats.tables.html>Version with HTML tables</a>\n<p>\n<li><a href=stats.html>Version without HTML tables</a>\n<p>\n<li><a href=stats.ps>Postscript version</a>\n</ul>\nblex\n\n--------snip--------\n\nThis resource (and its variants) can be found at\nhttp://gewis:81/conneg-bin/stats .  Note that, as mentioned in the\ndraft, some browsers (notably lynx) can't handle the 300 response\nwithout a Location header generated by this script.\n\n\n2) conneg-uax, the content negotiating user agent extension\n\nThis is a software package which can be used to extend existing user\nagents with transparent content negotiation capabilities.  The package\ncontains a complete implementation of the user agent version of the\nnetwork negotiation algorithm.  The package is quite small: about 750\nlines of perl and awk.\n\nrom the INSTALLATION file:\n\n1. To run conneg-uax, you need a unix box with the following software\ninstalled:\n\n - perl 4\n - awk     (a newer awk implementation which understands func())\n - libwww-perl-0.4\n - (optional) a user agent which is able to use a http proxy.\n\nYou can get libwww-perl-0.4 at\n\n   http://www.ics.uci.edu/pub/websoft/libwww-perl/\n\nrom the README file:\n\nThe conneg-uax program is a HTTP/1.0 user agent which supports\ntransparent content negotiation.  It can be run from the command\nline, but it is mainly intended to work as a wrapper around a normal\nHTTP/1.0 user agent:\n\n    -----------------------------\n   |Transparently negotiating    |\n   |user agent                 |\n   |                             |\n   |   ----------------------    |\n   |  | Normal user agent    |   |\n   |   ----------------------    |\n   |         ^                   |\n   |         | HTTP/1.0          |\n   |         V                   |\n   |   ----------------------    |    \n   |  | conneg-uax           |<------- preferences/capabilities database\n   |   ----------------------    |     (capdb file)\n   |         ^                   |\n    ---------|-------------------\n             |\n             | HTTP/1.0 + transparent content negotiation\n             |\n             V\n       Proxy caches, if present\n             ^\n             |\n             | \n             V\n       Origin server, maybe with transparently negotiable resources\n\n\nSeen from the normal user agent, conneg-uax acts as a HTTP/1.0 proxy.\n\nSeen from the origin server, conneg-uax looks like a monolithic user\nagent system capable of transparent content negotiation.\n\nA user agent extended with conneg-uax is typically used for\ntransparent content negotiation experiments, but it can also access\nnormal, un-negotiated resources.\n\nWhile conneg-uax is running, the file capdb, which contains the\ncapabilities and preferences database, can be edited to try out the\nresults of different user agent profiles.  The file is read whenever a\nnegotiated resource which generates a 300 (multiple choices) response\nis accessed.  Common use is\n\n  1. access a negotiated resource with the user agent\n  2. view the result\n  3. edit and save the capdb file\n  4. press reload in the user agent and view the new result\n  5. goto 3.\n\n--snip--\n\nI mainly made these implementations to verify that the price of\nadmission for transparent content negotiation is indeed low, and to\nprovide a framework for future experiments.  I have no immediate plans\nto extend these implementations.\n\nAgain, all code is available under\nhttp://gewis.win.tue.nl/~koen/conneg/ .\n\nPlease send bug reports and questions about the implementations\ndirectly to me, and not to the http-wg mailing list.\n\nHave fun!\n\nKoen.\n\n\n\n", "id": "lists-010-5905242"}, {"subject": "Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": ">----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Subject: Re: Sticky header draft -- as an attachment \n>\n>Your draft says:\n>\n>    The negotiation of the sticky headers option may take place on any\n>    request sent over a persistent connection. The client may add the\n>    connection-token \"Sticky\" to the Connection header in a request; if the\n>    server accepts the use of sticky headers, it responds with the same\n>    token in the Connection header of its response.\n>    \n>    Once the use of sticky headers has been negotiated, specified message-\n>    headers (see section 4.1 and 4.2 of [1]) are remembered from message to\n>    message, so that they need to be transmitted in a message only if they\n>    have changed since the last message. The use of sticky headers continues\n>    until the connection is closed, without further need for the \"Sticky\"\n>    connection-token on each request.\n>\n>I think this leads to an ambiguous situation when the client is\n>pipelining requests.\n\n>  We identified this ambiguity at the meeting\n>we had in January of the persistent-connections subgroup.\n>\n>Consider this **somewhat contrived** example\n>\n>     client:\n>     ## requesting first object, negotiating sticky headers ##\n>     GET / HTTP/1.1<CRLF>\n>     Accept: text/html<CRLF>\n>     Accept-Language: en<CRLF>\n>     Connection: sticky<CRLF>\n>     <CRLF>\n>\n>     server:\n>     HTTP/1.1 200 OK<CRLF>\n>     MIME-Version: 1.0<CRLF>\n>     Connection: sticky<CRLF>\n>     Content-Type: text/html<CRLF>\n>     Content-Length:94<CRLF>\n>     <CRLF>\n>     <94 bytes body data here>\n>\n>     client:\n>     ## requesting second object, changing one of the to-be-sticky headers ##\n>     GET /a.gif HTTP/1.1<CRLF>\n>     Accept: image/gif<CRLF>\n>     <CRLF>\n>     ## requesting 3rd object, again changing 1 of the to-be-sticky headers\n>##\n>     GET /b.jpeg HTTP/1.1<CRLF>\n>     Accept: image/jpeg<CRLF>\n>     ## requesting 4th object, not sending one of the to-be-sticky headers ##\n>     GET /c.gif HTTP/1.1<CRLF>\n>     <CRLF>\n>\n>OK, so what are the effective request headers for the 4th request?\n>Your design implicitly disallows the client from assuming \"stickiness\"\n>until it has received\n>     Connection: sticky<CRLF>\n>from the server.  However, because there is no way for the server\n>to tell whether the client received that header before or after\n>sending the fourth request, the server cannot unambiguously know\n>what the client means (i.e., should the fourth request be interpreted\n>as having \"Accept: image/jpeg\" or not?)\n\nI don't see the ambiguity in this example. The fourth request should be\ninterpreted as having\nAccept: image/jpeg\nand the second, third and fourh requests all should be interpreted as\nhaving\nAccept-Language: en\nThe server doesn't need to know when the client received\nConnection: sticky\ninstead, the client has to send requests that are unambiguous. I.e.,\nuntil it receives the\nConnection: sticky\nit sends ALL headers that have ever been sent, which is unambiguous --\neven if the server thinks that the client *could* be omitting headers,\nnone are actually omitted, so the server won't be confused. The server\nshould remember the headers, regardless of what it thinks the client\nknows, so that when the client starts omitting headers, the server has\nthe previous value to add to the message.\n\nI think maybe your example was just buggy (or not clear to me). If the\nclient had sent the second request _before_ it got the reply from the\nserver, then it would have been ambiguous as to whether\n\"Accept-Language: en\" was omitted from the request. So, in such a case,\nit should either send\nAccept-Language:\nif there wasn't supposed to be any Accept-Language header, or\nAccept-Language: en\nif it was supposed to have remained the same as the previous message.\n\n>\n>One could argue that this kind of thing could never happen in practice,\n>and so my contrived example (which admittedly is pretty foolish on the\n>part of the client) is not worth discussing.  But the race condition\n>seems to be intrinsic in the current draft design, and I'd feel more\n>confident if there were either a proof that it is not really a problem,\n>or a design modification that prevented the ambiguity.\n\nI agree that something exlicit should be said about pipelining.\n>\n>I can see several possible ways to solve this:\n>(1) Explicitly declare that the client, after having sent\n>\"Connection: sticky\", may start omitting headers as soon as it\n>receives the server's \"Connection: sticky\" response, but MUST\n>NOT change any such headers before receiving a response to the\n>first request sent *after* it has received that \"Connection:\n>sticky\" response from the server.  This rule has to be\n>interpreted \"per connection\".  (If you draw out the timing\n>diagram, you can see that this will always avoid the race\n>condition.)\n\nI agree it needs to be per connection. I don't see why it has to wait\nthe extra round trip. I think the modification above solves the problem.\nIf a client were _required_ to omit headers that were the same as in the\nlast message once it had received the \"Connection: sticky\" from the\nserver, then I think there would need to be tighter synchorization.\n>\n\n\n\n", "id": "lists-010-5917002"}, {"subject": "Sticky headers and multiplexing (was: Sticky header draft &ndash;&ndash; as an attachment", "content": ">----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Subject: Re: Sticky header draft -- as an attachment \n>\n>\n>Regarding:\n>    2.2 Contexts\n>    \n>    Proxies can operate just like user-agents if they want. However, they\n>    typically act on behalf of many clients, multiplexing a single\n>    connection to a server across messages from many clients. Such\n>    multiplexing will likely destroy the correlation between consecutive\n>    messages that makes sticky headers an effective compression technique.\n>\n>It might be a good idea for you to provide a means for negotiating\n>multiple contexts over a single proxy-server connection, but I am\n>not sure it's wise to implicitly bless the multiplexing of request\n>streams from several clients.  This can lead to something akin to\n>the \"head of line blocking\" problem seen in network switches:\n>\n>    Head of line blocking occurs when a packet at the head of an input\n>    queue blocks, thereby preventing a packet behind it from using an\n>    available output port. The problem is common to networks which\n>    employ input FIFOs which prevent packets from passing one another.\n>    The solution to head of line blocking is to use random access\n>    buffers which permit packets to be forwarded out of order.\n>\n>    quoted from \"High Performance Communication for Distributed Systems\"\n>    by William Stasior,\n>    http://www.tns.lcs.mit.edu/~wstasior/distrib_sys_comm/distrib_sys_comm.ht\n>ml\n>\n>Consider the case where proxy P is combining request streams from\n>client C1 and client C2 over the same TCP connection to server S.\n>Since we don't allow reordering of requests (i.e., we use \"input FIFOs\"\n>at the server side), if C2 makes a request that takes a long time to\n>answer, C1 may have to wait even though there is no intrinsic reason\n>for this.  The situation is especially bad if S is actually proxy\n>as well, and C1 and C2 are really making their requests to unrelated\n>origin servers S1 and S2.  If S2 is down, the C1->S1 request is stalled\n>until the C2->S2 request times out.\n>\n>This is why draft-ietf-http-v11-spec-06.txt says\n>\n>8.1.4 Practical Considerations\n>\n>[...] A proxy SHOULD use up to 2*N connections to another\n>server or proxy, where N is the number of simultaneously active\n>users. These guidelines are intended to improve HTTP response\n>times and avoid congestion of the Internet or other networks.\n\nI think we're talking about different time frames for multiplexing. I\nthink you're thinking about multiplexing outstanding requests over one\nconnection using pipelining, and I'm thinking about switching the\nconnection from one client to another, when there are no outstanding\nrequests on the connection. The \"head of line blocking problem\" can't\noccur when there is never anything ahead of you in the line.\nTo give a concrete example that incorporates both points of view:\nsuppose that there is a proxy that, in a typical one hour period,\nservices requests for 1000 clients to a particular server, and that, due\nto the request interarrival times and service times, in this period as\nmany as 100 of these clients had a pipelined burst of requests\nsimultaneously outstanding to that server. Section 8.1.4 says that the\nproxy should use 200 connections to service these 100 clients (instead\nof trying to multiplex them across a smaller number of connections using\npipelining).  The sticky header context mechanism was designed to allow\nthe proxy to have (in this example) roughly 10 contexts on each\nconnection, so that an incoming request could be assigned to a\nconnection that already has a context for it, if it isn't busy;\notherwise, a new connection should be opened (and a context created for\nit. (If a new connection weren't opened, then the static assignment of\nclients to connections could lead to a blocking problem.)\n\nI think that 8.1.4 is ambiguous -- in my example, are there 100 or 1000\nsimultaneuously active users? I think it's 100, for the purpose for\nwhich 8.1.4 is intended, and 1000, for the purpose for which sticky\nheader contexts are intended.\n\nNevertheless:\nMaybe the word \"connection multiplexing\" too strongly implies the finer\ngrained sharing -- I'm open to using a less loaded term; \"connection\nreuse\", perhaps?\n\n\n\n", "id": "lists-010-5931792"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Sent: Sunday, August 04, 1996 6:11 AM\n>To: mogul@pa.dec.com\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: New document on \"Simple hit-metering for HTTP\"\n>\n>Jeffrey Mogul:\n>>\n>[...]\n>>Our goal was NOT to solve the general problem of collecting demographic\n>>information; it was to reduce the incentive for origin servers to\n>>defeat caching merely so that they could collect simple hit-counts, of\n>>a sort that the caches could just as easily collect for them.\n>[...]\n>>You can find a copy at\n>>\n>>    http://ftp.digital.com/~mogul/draft-ietf-http-hit-metering-00.txt\n>[...]\n>\n>I have several comments.\n>\n>1. Cascaded proxy caches.\n>\n>At first glance, there seem to be counting problems in a cascaded\n>proxy cache situation.  If we have the arrangement\n>\n>   origin server ---- proxy 1 ------ proxy 2 ---- user agent\n>\n>and the user agent requests and uncached page, section 5.1 seems to\n>say that both proxy 1 and proxy 2 must set the use count to 1 when\n>relaying the page.  This results in a count of 2 being reported in the\n>end, though the page is only viewed once.  It seems like there needs\n>to be a special case for proxy 1: a proxy should not count if it is\n>relaying the response to another proxy.  (Under HTTP/1.1, the Via:\n>header in the request would tell you that you are talking to another\n>proxy.)\n\nSounds like a bug. The fix sounds yucky, though. WE'll try to dream up a\ncleaner one.\n\n>2. Number of unconditional GETs = number of times read???\n>\n>You argue that the number of unconditional GETs, rather than the total\n>number of GETs, more accurately reflects the number of times a page is\n>read.  I don't know if this is true; I would like to see section 4\n>discuss user agents on shared machines, and situations in which user\n>agent disk caches are disabled entirely because there is a central\n>proxy (like on our local sun cluster).\n\n>We didn't really intend the claim to be that strong. The \"number of\nunconditional GETs = number of times read\" only holds (approximately)\ntrue for interior, shared caches. (By interior, I mean that the client\nof the cache itself has a cache.) A single user leaf cache doesn't have\nto do anything in the hit-metering design (it can ignore max-uses and\nnot send use-count) but a shared leaf cache would need to remember\nrequestors' From or IP address to try to relay a meaningful use-count.\n>\n>3. A `hit' being an *un*conditional GET\n>\n>In the current (classic) meaning of the word,\n>\n>  1 hit-classic = 1 request on an origin server.\n>\n>Your draft defines a new kind of hit:\n>\n>  1 hit-new = 1 200/203/206 response returned to a user agent.\n>\n>Now, if I am an origin server which uses cache busting, and if most\n>caches play by the rules, then for my server I will measure:\n>\n>  hit-new < hit-classic .\n>\n>Assuming that I get payed by the hit, I have absolutely no incentive\n>to start measuring hit-news instead of hit-classics.  To stop using\n>the cache-busting based hit-classics would be economic suicide.\n\nNo, the  payment per hit-new would just be higher than for per\nhit-classic.\n>Maybe the content providers only switch to hit-new when an ad contract runs\nout and the advertiser demands the better counting method, but other\n>than that I see no real barrier.\n>\n>So even if hit-new is a better metric than hit-classic, I fear it\n>won't be effective at reducing cache busting.\n>\n>The nicest solution to this problem seems to be for proxies to count\n>both hit-new and a second metric:\n>\n>  1 touch-new = 1 response returned to a user agent\n>\n>for which it is guaranteed that\n>\n>   hit-classic <= touch-new .\n>\n>(Note: `hit' and `touch' would *not* be my proposals for adequate\n>names for these metrics.)\n\n\nThere is no need to have a new way count hit-classic -- people already\ndo that today.\n\n>4. Interaction with Vary\n>\n>I don't like the extra complexity and inefficiency introduced by the\n>Vary counting rules in section 3. (See second-to-last sentence of\n>Section 5.1.)\n>\n>I think the proposal would be better if the Vary special case were\n>removed entirely.\n\nDon't you think that providers of multilingual content want to count the\nhits of the French, German, Dutch, and English (etc.) version\nseparately? And that, if they insist on not losing counts when a page is\nflushed from the cache, they'll want to do that on multilingual pages\ntoo?\n\nHowever, I think I could be convinced that there only needs to be one\ncount per Etag. The idea was that even if there were only an English\nversion of a page, having the server set \"Vary: Accept-Language\" would\ncause separate counting of hits for users with \"fr\" \"de\", etc, so that\nthe potential for these languages could be gauged without creating pages\nfor all of them. Being able to count them is, I think, a GOOD THING; but\nmaybe one could create a separate Etag without having to create a\nseparate physical copy of the page. We'll think about it.\n\n>5. Overhead in proxy efficiency\n>\n>I'm wondering if the counting mechanisms in the draft won't cause an\n>unacceptable overhead for high-performance cache implementations.  I\n>think we definitely need the opinions of proxy cache implementers on\n>this issue.\n\nThe design is supposed to just require addition of a counter to a data\nstructure that needs to be around anyway, and to add a few bytes to a\nmessage you needed to send anyway.  The alternative is that pages for\nwhich demographic info is required aren't cached at all, which is\nplainly MUCH worse.\n>\n>One possible hit counting alternative, post-processing proxy logfiles\n>and delivering the results to the servers, seems to have less\n>overhead.\n\nBut way more complicated to spec and implement.\n>\nMore replies to the other comments later.\n\n>Paul \n\n\n\n", "id": "lists-010-5944274"}, {"subject": "these results sound very encouragin", "content": "Server-side document hashing can be a viable way to reduce \ntraffic....but only for documents that are commonly copied...or \nfor those documents that are known to be \"mirrors\" of other \ndocuments.  It is certainly within the scope of the  \nspecification of HTTP, an \"object-oriented protocol\", to add a facility\nfor \nidentification of these objects.\n\nA suitable proposal would allow one or more \ncontent-identification headers to be reported by the server\n\n*1. the identifier\n*2. the method of identification (secure MD5 hash, \nregistration authority, etc.)\n 3. the scope of the content-identifier (eg: *.com)\n\nAlso a \"content-origin\" header would prove useful for \nindexing facilities and intelligent browsers making use\nof content-identification for space/time efficiency.\n\n>----------\n>From: Martin Hamilton[SMTP:martin@mrrl.lut.ac.uk]\n>Sent: Saturday, August 03, 1996 1:22 PM\n>I got curious about this a little while back, and wrote a little Perl \n>program to calculate MD5 checksums of the objects in our \n>(local/regional ?) cache, so we could see how many were dups.  The \n>results weren't very encouraging...  \n\n><URL:http://www.roads.lut.ac.uk/lists/ircache/0202.html>\n\nA significant number of hits for certain documents \ncould have been reduced if the your proxy had reported a \ndocument-hash to the client in the header.\n\n------------------------------------------------------\n\nAnyone who has experience with commercial document\nhandling services knows that object identification is \ncritical to the efficient functioning of his server (eg: \nLotus Notes, Microsoft Exchange).\n\n>\n\n\n\n", "id": "lists-010-5962104"}, {"subject": "Re: these results sound very encouragin", "content": "I suggest using \"Content-ID\" for a globally unique originator-supplied\nidentifier.\n\n> A suitable proposal would allow one or more \n> content-identification headers to be reported by the server\n\n> *1. the identifier\n\nyes\n\n> *2. the method of identification (secure MD5 hash, \n>registration authority, etc.)\n\nunnecessary\n\n> 3. the scope of the content-identifier (eg: *.com)\n\nalways global\n\n\n\n", "id": "lists-010-5972055"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Paul Leach:\n>[Koen Holtman:]\n>>\n>>I have several comments.\n>>\n>>1. Cascaded proxy caches.\n>>\n>>At first glance, there seem to be counting problems in a cascaded\n>>proxy cache situation.  If we have the arrangement\n>>\n>>   origin server ---- proxy 1 ------ proxy 2 ---- user agent\n>>\n>>and the user agent requests and uncached page,\n[....]\n>\n>Sounds like a bug. The fix sounds yucky, though. WE'll try to dream up a\n>cleaner one.\n\nI thought up another fix some hours after I sent my message: adopt the\nrule that a server which is only relaying, not generating, a response\nmust not count it.  In the above example, the origin server itself\nwould count the hit, not any upstream proxy cache.\n\nThis way, when serving a request the origin server can immediately add\n1 to its total hit count.  With my earlier fix, it would have to wait\nfor proxy 2 to report the hit in a future request.\n\n\n>>3. A `hit' being an *un*conditional GET\n>>\n>>In the current (classic) meaning of the word,\n>>\n>>  1 hit-classic = 1 request on an origin server.\n>>\n>>Your draft defines a new kind of hit:\n>>\n>>  1 hit-new = 1 200/203/206 response returned to a user agent.\n>>\n>>Now, if I am an origin server which uses cache busting, and if most\n>>caches play by the rules, then for my server I will measure:\n>>\n>>  hit-new < hit-classic .\n>>\n>>Assuming that I get payed by the hit, I have absolutely no incentive\n>>to start measuring hit-news instead of hit-classics.  To stop using\n>>the cache-busting based hit-classics would be economic suicide.\n>\n>No, the  payment per hit-new would just be higher than for per\n>hit-classic.\n\nYour answer assumes a level of maturity in the payer which simply is\nnot present yet.  This market is far too young, and far too dominated\nby customers who respond to the `your company will be obsolete if you\ndo not get on the web *now*' hype.\n\nLook, if all hit count customers were sophisticated enough to pay more\nfor `high quality' hits, advertising sites could not make more money\nby using cache busting.  So if your assumed level customer of maturity\nwas indeed present, there would not be a cache busting problem to\nsolve in the first place.\n\nI can't see the level of maturity you assume being present now\nanywhere except maybe in a few very big web advertisers.\n\nFor the companies who responded to the `your company will be obsolete\nif you do not get a home page *now*' hype by getting some noname\nstartup to create and maintain a homepage (usually containing at least\n200Kb of images) for them, this level of maturity will not appear for\nsome time.\n\nI guess we could argue at length about present and predicted levels of\nmaturity (both in the US and in Europe), but the bottom line is this:\n\n I feel very strongly that it would be a huge mistake to make a scheme\n to eliminate cache busting dependent on something, sufficient\n customer sophistication in this case, the existence of which is\n questionable at best.\n\nThis dependence on customer sophistication can easily be removed by\ncounting hits in a way which gives *higher* counts even to sites which\nnow use cache-busting.  By generating a higher counts even for these\nsites, the scheme will work no matter how (un)sophisticated the\ncustomers are.\n\nAgain, there is no need to count only one thing.  You could count both\nthe `meaningful' type of hits you have defined, and my `inflated'\nhits.  That would even let you measure cache efficiency in a neat way.\n\n>>4. Interaction with Vary\n>>\n>>I don't like the extra complexity and inefficiency introduced by the\n>>Vary counting rules in section 3. (See second-to-last sentence of\n>>Section 5.1.)\n>>\n>>I think the proposal would be better if the Vary special case were\n>>removed entirely.\n>\n>Don't you think that providers of multilingual content want to count the\n>hits of the French, German, Dutch, and English (etc.) version\n>separately?\n\nSure!  There will also be providers who want the referer info, the\ne-mail addres, and a credit rating of the user for every hit on a\npage.\n\nThe question here is where to strike the balance between\nsimplicity/efficiency and the need for statistics.\n\nI feel that your vary scheme adds to much complexity.  Making it\nEtag-based would help, but it would still be a bit too complex for my\ntaste.  I guess the WG as a whole will have to decide on where to\nstrike the balance.\n\nAlso, multilingual pages which use transparent content negotiation\nwill not need a Vary-based hit counting scheme: you can quite\nnaturally count hits on the variant URLs.\n\n[...]\n>>5. Overhead in proxy efficiency\n>>\n>>I'm wondering if the counting mechanisms in the draft won't cause an\n>>unacceptable overhead for high-performance cache implementations.  I\n>>think we definitely need the opinions of proxy cache implementers on\n>>this issue.\n>\n>The design is supposed to just require addition of a counter to a data\n>structure that needs to be around anyway, and to add a few bytes to a\n>message you needed to send anyway.\n\nI'm primarily worried about the filesystem read/write overhead needed\nto maintain the counters.  I would like to hear an implementer say\nthat this is not a problem.\n\n>  The alternative is that pages for\n>which demographic info is required aren't cached at all, which is\n>plainly MUCH worse.\n\nNo, it is not necessarily worse.  It depends on how much cache-busting\nthere is now, and on how much your proposal would eliminate.  We are\ncomparing an overhead for *every* page served with an overhead due to\nextra *conditional* requests for *some* pages.  This all boils down to\nthe last comment in my previous message: how much cache busting is\nthere anyway?\n\n>Paul \n\nKoen.\n\n\n\n", "id": "lists-010-5979967"}, {"subject": "draft-mutz-http-attributes0", "content": "The shorter length of most of the names is good.\n\nI really don't understand why x and y size for the display were\nseparated. I can't really see caching using Vary based only on x or y.\nSo, I'd suggest:\nUA-pix: <x-size> x <y-size>\nbecause it's shorter and no significant loss of functionlity.\n\nI also don't understand the rationale behind\nUA-media-<type>\nintroducing many extra header names, instead of\nUA-media: <type>\nwhich seems like it works just as well conceptually, and fits perfectly\ninto Vary based caching.\n\n\nBottom line -- I'd like it to be more like it was before, but with the\nshorter names.\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-5994416"}, {"subject": "(no subject", "content": "list\n\n\n\n", "id": "lists-010-6002655"}, {"subject": "Last Call: Proposed HTTP State Management Mechanism to Proposed Standar", "content": " The IESG has received a request from the HyperText Transfer Protocol\n Working Group to consider \"Proposed HTTP State Management Mechanism\"\n <draft-ietf-http-state-mgmt-03.txt, .ps> for the status of Proposed\n Standard.\n\n The IESG plans to make a decision in the next few weeks, and solicits\n final comments on this action.  Please send any comments to the\n iesg@ietf.org or ietf@ietf.org mailing lists by August 20, 1996.\n\n\n\n", "id": "lists-010-6008699"}, {"subject": "hitmetering pol", "content": "I need data from any content providers on this list!\n\nWhen you do hit counting:\n\nDo you count HEADs and GETs as hits?\n\nDo you count conditional GETs that return 304 (Not modified) as hits?\n(I.e., where the requester already had a copy of the page, and was just\nchecking to see if it was up-to-date.)\n\nIf you are not a content provider, but have information about their\npractices, I'd love it if you contributed it as well. (It would be nice\nif you indicated it was second hand...)\n\nThanks!\n----------------------------------------------------\nPaul J. Leach            Email: paulle@microsoft.com\nMicrosoft                Phone: 1-206-882-8080\n1 Microsoft Way          Fax:   1-206-936-7329\nRedmond, WA 98052\n\n\n\n", "id": "lists-010-6017191"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Quick poll for cache implementers (read below for context):\n\nIf you were going to implement hit-counting in some form akin to what's\nin the draft, how amenable would you be to implementing both \"use-count\"\n(counts unconditional GETs) and \"get-count\" (counts all accesses, even\nif they don't cause a new copy of the page to be sent), instead of just\n\"use-count\".\n\n>----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Subject: Re: New document on \"Simple hit-metering for HTTP\"\n>\n>\n>I thought up another fix some hours after I sent my message: adopt the\n>rule that a server which is only relaying, not generating, a response\n>must not count it.  In the above example, the origin server itself\n>would count the hit, not any upstream proxy cache.\n>\n>This way, when serving a request the origin server can immediately add\n>1 to its total hit count.  With my earlier fix, it would have to wait\n>for proxy 2 to report the hit in a future request.\n\nThanks. Sounds cleaner.\n>\n>>>3. A `hit' being an *un*conditional GET\n>>>\n>>>In the current (classic) meaning of the word,\n>>>\n>>>  1 hit-classic = 1 request on an origin server.\n>>>\n>>>Your draft defines a new kind of hit:\n>>>\n>>>  1 hit-new = 1 200/203/206 response returned to a user agent.\n>>>\n>>>Now, if I am an origin server which uses cache busting, and if most\n>>>caches play by the rules, then for my server I will measure:\n>>>\n>>>  hit-new < hit-classic .\n>>>\n>>>Assuming that I get payed by the hit, I have absolutely no incentive\n>>>to start measuring hit-news instead of hit-classics.  To stop using\n>>>the cache-busting based hit-classics would be economic suicide.\n>>\n\nActually, it just occurred to me -- you're definition of \"hit-classic\"\nis bogus. We (at least I) do not know how sites count hits. All we know\nis that when they bust the cache, they see all requests. They could\ncount both conditional and unconditional GETs as hits, or they could\ncount just unconditional ones. They could count HEADs, or not. Etc.\n\nSo I've sent out a poll to ask any content providers how they count\nhits.\n\n>>No, the  payment per hit-new would just be higher than for per\n>>hit-classic.\n>\n>Your answer assumes a level of maturity in the payer which simply is\n>not present yet.  This market is far too young, and far too dominated\n>by customers who respond to the `your company will be obsolete if you\n>do not get on the web *now*' hype.\n>\n>Look, if all hit count customers were sophisticated enough to pay more\n>for `high quality' hits, advertising sites could not make more money\n>by using cache busting.  So if your assumed level customer of maturity\n>was indeed present, there would not be a cache busting problem to\n>solve in the first place.\n\nHuh? Until we deploy something in caches, \"cache-busting\" is the only\nway they have to get demographic data. It seems to me that the people\ncollecting demographic data have been pretty clever in how they do it --\nI've heard stories about creating funny URLS to track your identity\nacross sessions, and your trick for figuring out who the referring site\nwas, etc.\n>\n>I can't see the level of maturity you assume being present now\n>anywhere except maybe in a few very big web advertisers.\n\nGotta start somewhere. Remember, a content provider that stops cache\nbusting will reduce load on its site and decrease response time to see\nthe content (and the ad). These will be powerful incentives.\n>\n>For the companies who responded to the `your company will be obsolete\n>if you do not get a home page *now*' hype by getting some noname\n>startup to create and maintain a homepage (usually containing at least\n>200Kb of images) for them, this level of maturity will not appear for\n>some time.\n>\n>I guess we could argue at length about present and predicted levels of\n>maturity (both in the US and in Europe), but the bottom line is this:\n>\n> I feel very strongly that it would be a huge mistake to make a scheme\n> to eliminate cache busting dependent on something, sufficient\n> customer sophistication in this case, the existence of which is\n> questionable at best.\n\nFirst, this isn't a scheme to \"eliminate\" cache busting, it is only\ntrying to reduce it.\n\nSecond, really unsophisticated customers just won't use it -- they'll\ncontinue cache-busting.\n\nModerately unsophisticated existing content providers who use it won't\neven know that the kind of hits that are counted are a little different.\n\nWithin a year of deployment, more than half of the content providers\nwill be new. They also won't know that the kind of hits that are counted\nare different.\n\n>\n>This dependence on customer sophistication can easily be removed by\n>counting hits in a way which gives *higher* counts even to sites which\n>now use cache-busting.  By generating a higher counts even for these\n>sites, the scheme will work no matter how (un)sophisticated the\n>customers are.\n>\n>Again, there is no need to count only one thing.  You could count both\n>the `meaningful' type of hits you have defined, and my `inflated'\n>hits.  That would even let you measure cache efficiency in a neat way.\n\nAt the cost of increased complexity in the cache implementation and\nhence longer until it gets deployed.\nSo, I put the quick poll at the top of this message.\n>\n>>>4. Interaction with Vary\n>>>\n>>>I don't like the extra complexity and inefficiency introduced by the\n>>>Vary counting rules in section 3. (See second-to-last sentence of\n>>>Section 5.1.)\n>>>\n>>>I think the proposal would be better if the Vary special case were\n>>>removed entirely.\n>>\n>>Don't you think that providers of multilingual content want to count the\n>>hits of the French, German, Dutch, and English (etc.) version\n>>separately?\n>\n>Sure!  There will also be providers who want the referer info, the\n>e-mail addres, and a credit rating of the user for every hit on a\n>page.\n>\n>The question here is where to strike the balance between\n>simplicity/efficiency and the need for statistics.\n>\n>I feel that your vary scheme adds to much complexity.  Making it\n>Etag-based would help, but it would still be a bit too complex for my\n>taste.  I guess the WG as a whole will have to decide on where to\n>strike the balance.\n\nI don't feel you made a strong enough argument that keeping a counter\nper cache entry is too complicated. Or even that, in order to be\n(completely optionally) \"cooperative\", sending a HEAD (without waiting\nfor a response) for each cache entry before it is purged is too\ncomplicated.\n\nNow, maybe the spec is unclear, but that's all it requires above what's\nneeded to do Vary.\n>\n>Also, multilingual pages which use transparent content negotiation\n>will not need a Vary-based hit counting scheme: you can quite\n>naturally count hits on the variant URLs.\n>\n>[...]\n>>>5. Overhead in proxy efficiency\n>>>\n>>>I'm wondering if the counting mechanisms in the draft won't cause an\n>>>unacceptable overhead for high-performance cache implementations.  I\n>>>think we definitely need the opinions of proxy cache implementers on\n>>>this issue.\n>>\n>>The design is supposed to just require addition of a counter to a data\n>>structure that needs to be around anyway, and to add a few bytes to a\n>>message you needed to send anyway.\n>\n>I'm primarily worried about the filesystem read/write overhead needed\n>to maintain the counters.  I would like to hear an implementer say\n>that this is not a problem.\n\nMine say it isn't. They say that they have to update some info\nassociated with a cache entry on every hit, just to maintain LRU-type\ninfo. So, keeping the counter is not a big deal.\n>\n>>  The alternative is that pages for\n>>which demographic info is required aren't cached at all, which is\n>>plainly MUCH worse.\n>\n>No, it is not necessarily worse.  It depends on how much cache-busting\n>there is now, and on how much your proposal would eliminate.  We are\n>comparing an overhead for *every* page served with an overhead due to\n>extra *conditional* requests for *some* pages.\n\n>This all boils down to\n>the last comment in my previous message: how much cache busting is\n>there anyway?\n\nI'm not woried about it. If there are not enough customers demanding\nthis feature, it won't get implemented very much. But I can say that\nthere's enough demand that at least one vendor puts it very high on\ntheir list. Of course, they could do something non-standard, but they\nactually *really* don't want to.\n\n\n\n", "id": "lists-010-6025783"}, {"subject": "Re: hitmetering pol", "content": "On Tue, 6 Aug 1996, Paul Leach wrote:\n\n> I need data from any content providers on this list!\n\nIt isn't content providers you need to ask - it is the people who write\nthe log analysis programs. Few people roll their own. I am one of those\npeople who have written a log analysis package for public use\n(<URL:http://www.netimages.com/~snowhare/utilities/>) so....\n\n> When you do hit counting:\n> \n> Do you count HEADs and GETs as hits?\n\nI count whatever the server writers put in their log: GET, HEAD, POST -\nwhatever.\n\n> Do you count conditional GETs that return 304 (Not modified) as hits?\n> (I.e., where the requester already had a copy of the page, and was just\n> checking to see if it was up-to-date.)\n\nYes. I also account for them seperately to allow measuring how significant\ncode 304's are on a particular server. I *don't* count codes 302, 400,\n401, 402, 500 or 501, except in seperate categories just for those codes. \n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-6045121"}, {"subject": "Re: Last Call: Proposed HTTP State Management Mechanism to Proposed  Standar", "content": "A nit, but it seems strange to call the 'rfc' \"Proposed HTTP State\nManagement Mechanism\" ... isn't \"HTTP State Management Mechanism\"\ncloser to the correct \"message\"?\n\nDave Morris\n\n\n\n", "id": "lists-010-6053851"}, {"subject": "Re: hitmetering pol", "content": ">>>>> \"Paul\" == Paul Leach <paulle@microsoft.com> writes:\n\n    Paul> I need data from any content providers on this list!  When\n    Paul> you do hit counting:\n\nThere is an effort underway to define a MIB for http and web servers\nin general.  There was a BOF on this at the 35th IETF in LA.  The \nwork is summarized on the web at http://http-mib.onramp.net/\n\nCurrently the latest draft is http://http-mib.onramp.net/draft_2.my\n\nMost probably this MIB development will become part of the charter of\nthe Applications MIB Working Group.\n\n    Paul> Do you count HEADs and GETs as hits?\n\nThe MIB counts each request type individually (for each virtual server).\n\n    Paul> Do you count conditional GETs that return 304 (Not modified)\n    Paul> as hits?  (I.e., where the requester already had a copy of\n    Paul> the page, and was just checking to see if it was\n    Paul> up-to-date.)\n\nIt also counts each response type individually (for each virtual server).\n\nCarl\n\n--\nCarl W. Kalbfleisch            This is the day which the LORD hath made; \nhttp://www.onramp.net/~cwk/       we will rejoice and be glad in it. \ncwk@onramp.net                              -- Psalm 118:24\n\n\n\n", "id": "lists-010-6061753"}, {"subject": "another contentnegotiation scrip", "content": "(written before I'd seen Koen's one)\n\nhttp://vancouver-webpages.com/multilingual/select-lang.pl\nhttp://vancouver-webpages.com/cgi-bin/select-lang/multilingual/try.lang\nhttp://vancouver-webpages.com:8070/multilingual/try.lang (maybe)\n\nI left the status of the multiple choice page as 200 (it should be 300),\nas it causes strange behaviour with X-Mosaic as well as Lynx (Mosaic-l10n\nbeing the browser I can most easily change Accept-Language on).\n\nI have mostly tested this with LWP4/GET (modified to send Accept-Language,\nPragma, etc.).\n\n\nAndrew Daviel\n\nandrew@vancouver-webpages.com \nhttp://vancouver-webpages.com  : home of searchBC\n\n\n\n", "id": "lists-010-6070632"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Koen:\n>>>>In the current (classic) meaning of the word,\n>>>>\n>>>>  1 hit-classic = 1 request on an origin server.\n\nPaul:\n>Actually, it just occurred to me -- you're definition of \"hit-classic\"\n>is bogus. We (at least I) do not know how sites count hits.\n\nYou may not know that, but I am pretty sure that when people say `my\nserver gets 10K hits per day', they mean request messages per day.\n\nMaybe some statistics packages use other definitions of `hits', but I\nam sure that cache-busting advertising sites who get payed by the hit\nuse this definition.  This definition yields the highest count, and\nhigh counts is what they want.  Even an image transfer aborted at 10%\nwould be a hit; it yields a line in the access log file, after all.\n(Which reminds me: you probably have to add something to your draft\nabout how to count aborted transfers).\n\nKoen:\n>>Look, if all hit count customers were sophisticated enough to pay more\n>>for `high quality' hits, advertising sites could not make more money\n>>by using cache busting.  So if your assumed level customer of maturity\n>>was indeed present, there would not be a cache busting problem to\n>>solve in the first place.\n\nPaul:\n>Huh? Until we deploy something in caches, \"cache-busting\" is the only\n>way they have to get demographic data. \n\nNope. You can collect demographic data fine without cache busting; you\njust get less of it.  There will still be plenty of stuff in your\nlogfile.\n\nAlso, you really need to distinguish here between two kinds of\ndemographic data:\n\n1) Hit counts\n\n2) User's Referer field, IP address, User-Agent field, ...\n\nYour hit counting mechanism can only reduce the cache busting done to\nget more of 1), not the cache busting done to get more of 2).  \n\nThat is why I focus on by how successful your scheme is at getting\nmore of 1).  If most advertising sites are actually doing cache\nbusting to get more of 2), (and they might, I don't know) then your\nproposal will not reduce cache busting anyway.\n\n[...]\n>>I guess we could argue at length about present and predicted levels of\n>>maturity (both in the US and in Europe), but the bottom line is this:\n>>\n>> I feel very strongly that it would be a huge mistake to make a scheme\n>> to eliminate cache busting dependent on something, sufficient\n>> customer sophistication in this case, the existence of which is\n>> questionable at best.\n>\n>First, this isn't a scheme to \"eliminate\" cache busting, it is only\n>trying to reduce it.\n>\n>Second, really unsophisticated customers just won't use it -- they'll\n>continue cache-busting.\n\nYou don't understand my argument: the customers above are the ones who\npay for each the hit.  They are not the ones who do the cache busting:\nit is the web advertising sites they pay who do the cache busting.\nThese advertising sites are sophisticated enough to pick and deploy\nthe mechanism which gets them the highest hit counts, which is cache\nbusting until something better comes along.\n\n[...]\n>>I'm primarily worried about the filesystem read/write overhead needed\n>>to maintain the counters.  I would like to hear an implementer say\n>>that this is not a problem.\n>\n>Mine say it isn't.\n\nOK, that is good enough for me.\n\n\nKoen.\n\n\n\n", "id": "lists-010-6078491"}, {"subject": "Re: Last Call: Proposed HTTP State Management Mechanism to Proposed Standar", "content": "\"David W. Morris\" <dwm@shell.portal.com> wrote:\n  > A nit, but it seems strange to call the 'rfc' \"Proposed HTTP State\n  > Management Mechanism\" ... isn't \"HTTP State Management Mechanism\"\n  > closer to the correct \"message\"?\n\nThat's an administrative error on my part.  The actual document (draft 03)\nhas elided \"Proposed\", but I forgot to tell the IETF staff to change their\nrecord of the title.  Because previous drafts said \"Proposed...\", that's\nwhat the announcement for this one said.\n\nI'll tell IETF today.\nDave Kristol\n\n\n\n", "id": "lists-010-6090208"}, {"subject": "Re: draft-mutz-http-attributes0", "content": "attached mail follows:\nThe new draft of draft-mutz-http-attributes is now available at:\nftp://ds.internic.net/internet-drafts/draft-mutz-http-attributes-01.txt\n\nMost of the syntax changes in the draft are to provide compliance\nwith the transparent content negotiation method for features described\nin: draft-holtman-pre02-19.txt.  It's not possible to supply more than\none numeric argument, or any non-numeric argument to features.  UA-pix\nwas broken into two numeric arguments, and UA-media was split into \na set of unique feature tags.  I would like to see non-numeric values\nfor feature tags allowed.                                          \n\nAndy Mutz.\n--------------------------------------------------------------\nAndrew Mutz                          |   mutz@hplabs.hp.com \nHewlett Packard Laboratories         |   Phone:  +1 415 857 6122\n1501 Page Mill Road 3U-3             |   Fax:    +1 415 857 4691\nPalo Alto, California, USA.          | \n---------------------------------------------------------------\n\n\n\n", "id": "lists-010-6098015"}, {"subject": "Re: draft-mutz-http-attributes0", "content": "> The new draft of draft-mutz-http-attributes is now available at:\n> ftp://ds.internic.net/internet-drafts/draft-mutz-http-attributes-01.txt\n\nUA-media-screen is inadequate to distinguish between scrolling and\npaged UI presentations. This is needed to obtain different versions\nof documents or style sheets etc. according to the presentation type.\n\nI suggest that the list of media tokens is extended to allow\nUA-media-screen-paged with UA-media-screen being taken to imply\na scrolling presentation.\n\n-- Dave Raggett <dsr@w3.org> tel: +1 (617) 258 5741 fax: +1 (617) 258 5999\n   World Wide Web Consortium, 545 Technology Square, Cambridge, MA 02139\n   url = http://www.w3.org/People/Raggett\n\n\n\n", "id": "lists-010-6107419"}, {"subject": "Apache Cookie Log Analyzer (was: Re: hitmetering poll", "content": "On Tue, 6 Aug 1996, Paul Leach wrote:\n\n> I need data from any content providers on this list!\n> \n> When you do hit counting:\n> \n> Do you count HEADs and GETs as hits?\n> \n> Do you count conditional GETs that return 304 (Not modified) as hits?\n> (I.e., where the requester already had a copy of the page, and was just\n> checking to see if it was up-to-date.)\n> \n> If you are not a content provider, but have information about their\n> practices, I'd love it if you contributed it as well. (It would be nice\n> if you indicated it was second hand...)\n\nIn a kind of addendum to what I said in my other reply, I have just\nreleased a free basic log analyser for the Apache cookie_log. This (at\nleast attempts to) measures the number of unique session accessing pages\non a site. The perl script can be found at\n<URL:http://www.netimages.com/~snowhare/utilities/cookies.html>. This\nmeasurement is, of course, completely different than those of traditional\naccess_log analyzers because it only counts unique sessions and not total\nhits. The script is not a full featured analyzer, but a basic overview\ntool. \n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-6115645"}, {"subject": "Re: Apache Cookie Log Analyzer (was: Re: hitmetering poll", "content": "On Wed, 7 Aug 1996, Benjamin Franz wrote:\n\n> On Tue, 6 Aug 1996, Paul Leach wrote:\n> \n> > I need data from any content providers on this list!\n> > \n> > When you do hit counting:\n> > \n> > Do you count HEADs and GETs as hits?\n> > \n> > Do you count conditional GETs that return 304 (Not modified) as hits?\n> > (I.e., where the requester already had a copy of the page, and was just\n> > checking to see if it was up-to-date.)\n> > \n> > If you are not a content provider, but have information about their\n> > practices, I'd love it if you contributed it as well. (It would be nice\n> > if you indicated it was second hand...)\n> \n> In a kind of addendum to what I said in my other reply, I have just\n> released a free basic log analyser for the Apache cookie_log. This (at\n> least attempts to) measures the number of unique session accessing pages\n> on a site. The perl script can be found at\n> <URL:http://www.netimages.com/~snowhare/utilities/cookies.html>. This\n\nDuh. <URL:http://www.netimages.com/~snowhare/utilities/cookiestats.html>\nis the correct URL.\n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-6124879"}, {"subject": "Questions about i18n support in HTT", "content": "Hi folks,\n\nI apologize in advance if the following question shows my lack of\nunderstanding of your latest spec.  :-)\n\nI am reading your draft-ietf-http-v11-spec-01.txt document which\ndescribes the HTTP 1.1 specification.  \n\nI have the following questions regarding HTTP support for\ninternationalization.\n\n1. Does the HTTP spec stipulate that both request and response headers\nmust always be sent in ASCII?\n\n2. If the answer to 1 above is \"no\", how can a client and server\nnegotiate \"Accept-encoding\"?  How else can such negotiation take\nplace?  Who will standarize the method of negotiation?  E.g., a\nUnicode encoded client request is unreadable by an HTTP server running\non an EUC machine, regardless of the locale in which either the client\nor server is running.\n\n3. What is the difference between the \"Content-language\" and\n\"Accept-Language\" fields?  Which one implies the runtime locale\nsetting of the client?\n\n\nI would greatly appreciate any insight into the above questions.\nYou may email or call me.\n\nThank you in advance.\n\nRegards,\n\nVartan Piroumian\n\nphone: 415.786.4431 \nemail: vartan.piroumian@sun.com\n\n\n\n", "id": "lists-010-6134357"}, {"subject": "RE: Sticky header draft &ndash;&ndash; as an attachmen", "content": "Here are my comments, in line number order:\n\n1) Line 76, \"ABSTRACT\":  \"proposed\" should be \"proposes\".\n\n2) Lines 85-86, \"ABSTRACT\":\n>This draft proposes a way\n>to compress header names using tersely encoded abbreviations.\nEach time I read this, I expect to see an algorithm at the end of the draft \nfor automatically deriving the compressed names for new headers.  So I am \ndisappointed when I don't :(.  Although it is more wordy, I would propose:\n This draft proposes a set of tersely encoded abbreviations for\n header names, along with recommendations for compressing the names of\n headers that may be added to HTTP in the future.\n\n3) Line 140, \"1. Introduction\":  To avoid a forward reference, I would \nsuggest:\n>for the Connection header (\"sticky\"),\n                           ^^^^^^^^^^\n\n4) Lines 150-151, \"1. Introduction\":  Missing word:\n>and their use must be negotiated\n           ^^^\n5) Lines 218-222, \"2.1 Basic operation\":\n>To send a message without\n>one of the remembered message-headers, send a message-header line\n>consisting of just the field-name and a colon, and no field-value; upon\n>reception of such a line, the message-header in the remembered set with\n>that field-name is deleted.\nAdding \"for the duration of this message\" at the end of this sentence would \nclear up any potential ambiguity as to whether the message-header is \npermanently deleted (it sounded like that to me on initial reading).\n\n6) Lines 275-277, \"2.3 Changing the sticky-header set\":\n>There is no way to remove\n>headers from the sticky header set, or to add more headers to the set\n>after a context is created.\nSince headers can be removed temporarily, but no headers can be added (even \ntemporarily), I would suggest instead:\n There is no way to add more headers to the sticky header set after\n a context is created.  Headers can be removed from the sticky header\n set only on a per-message basis.\n\n7) Lines 489-492, \"5. Header name compression\":\n>There\n>can be 18 more headers added to HTTP and still only require one digit;\n>at that point if more are added a second digit can handle up to 4096\n>headers.\nThis phrasing feels awkward.  Would:\n Up to 18 more headers can be added to HTTP while still requiring\n only a single digit (+ a '#') for their compressed representation.\n Adding a second digit would handle up to 4096 total HTTP headers.\nflow better?\n\n8) Lines 505-510, \"6. Security Considerations\":\n>This latter attack could only work\n>with the non-secure authentication methods anyway so it is not\n>considered to be a serious concern.\nAs I understand it, what I think you were trying to say was:\n This latter attack could only work\n with unencrypted communications methods anyway so it is not\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n considered to be a serious concern.\nWhether HTTP is sent over an encrypted channel is the concern, as\nan unencrypted channel is vulnerable to man-in-the-middle attacks\nif re-authentication is not done on each message.\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-6142970"}, {"subject": "Meta data in anchors.", "content": "When following discussions on content negociation and similiar issues,\nI often reflect as to whether it would be a good idea to allow meta data\nto be embedded in the anchor when a document is retrieved so that if a \nlink is to a docuemnt is several formats, the user could xxx-click on the\nlink and get a menu displayed of the available formats.\n\nMost requests must be made by users clicking on a link. Having meta data\navailable as part of the link could help to negate some of the\ncomplexity of negociation.\n\nMy limited use of Gopher+ has shown this to be very useful. You get to\nspecify your general preferences but still have the option of easily\nfetching an alternate representation. \n\n\nPete.\n-- \nThe TIS Network Security Products Group has moved again!\nvoice: 301-527-9500x111  fax: 301-527-0482\nRoom 334, 15204 Omega Drive, Rockville, MD 20850\n\n\n\n", "id": "lists-010-6153435"}, {"subject": "Re: hitmetering pol", "content": "On Tue, 6 Aug 1996, Paul Leach wrote:\n\n> When you do hit counting:\n>\n> Do you count HEADs and GETs as hits?\n\n        The log analysis software that we license to our customers whose\nsites we develop (TrafficWatch) counts the HEAD, GET and POST queries\ntogether. TrafficWatch is configurable to allow customized profiling of the\nstandard logfile format records, but no customer has requested to\ndifferentiate between the request types. Our own profiles across all sites\nwe host show very few HEAD queries (less than .01%), so the magnitude of\npossible inflation of the totals is of little concern to them. They are\nmostly interested in configuring various graphical representations of the\nvarious hit populations according to content or  client characteristics.\n\n--\nMatthew Rubenstein                     North American Media Engines\nToronto, Ontario   *finger matt for public key*       (416)943-1010\n\n               They also surf who only stand on waves.\n\n\n\n", "id": "lists-010-6161102"}, {"subject": "RE: hitmetering pol", "content": ">Do you count differentiate between conditional and unconditional GETs?\n>Seems like the answer is no, if they don't even discriminate between\n>HEAD, GET, and POST.\n\n        No. Our customers are concerned with the relative interest shown by\ntheir audience in their various online resources. The marketing logic that\nis being applied currently is not comprehensive enough to weight\nconditional vs unconditional, HEAD vs GET etc, in determining where they're\ngetting their value. TrafficWatch is reconfigurable to allow for the\ndevelopment of useful principles for evaluating any of the values recorded\nin the log. Once the marketing principles for qualifying the various kinds\nof Web-mediated impressions are established, the analysis will be able to\nproduce meaningful data. Until then, the scarcity of non-GET/POST queries\nand the lack of a context in which to interpret them statistically renders\nthat kind of report nothing more than a distraction. However, TrafficWatch\ncould be used to analyze the logs in order to identify trends in the\nappearance of such data.\n\n\n>>----------\n>>From:  ruby@name.net[SMTP:ruby@name.net]\n>>Sent:  Wednesday, August 07, 1996 12:50 PM\n>>To:    Paul Leach\n>>Cc:    http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; len@name.net; rens@name.net\n>>Subject:       Re: hit-metering poll\n>>\n>>On Tue, 6 Aug 1996, Paul Leach wrote:\n>>\n>>> When you do hit counting:\n>>>\n>>> Do you count HEADs and GETs as hits?\n>>\n>>        The log analysis software that we license to our customers whose\n>>sites we develop (TrafficWatch) counts the HEAD, GET and POST queries\n>>together. TrafficWatch is configurable to allow customized profiling of the\n>>standard logfile format records, but no customer has requested to\n>>differentiate between the request types. Our own profiles across all sites\n>>we host show very few HEAD queries (less than .01%), so the magnitude of\n>>possible inflation of the totals is of little concern to them. They are\n>>mostly interested in configuring various graphical representations of the\n>>various hit populations according to content or  client characteristics.\n>>\n>>--\n>>Matthew Rubenstein                     North American Media Engines\n\n--\nMatthew Rubenstein                     North American Media Engines\nToronto, Ontario   *finger matt for public key*       (416)943-1010\n\n               They also surf who only stand on waves.\n\n\n\n", "id": "lists-010-6169452"}, {"subject": "Re: Sitcky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": "    >From: David W. Morris[SMTP:dwm@shell.portal.com]\n    >On Mon, 5 Aug 1996, Jeffrey Mogul wrote:\n    >> I think this leads to an ambiguous situation when the client is\n    >> pipelining requests.  We identified this ambiguity at the meeting\n    >> we had in January of the persistent-connections subgroup.\n    >\n    >I think the ambiguity is also resolved if we change HTTP/1.1 to not\n    >allow pipelining UNTIL the first server response is received accepting\n    >the persistent connection and hence pipelining.\n    \n    I thought that this was already the rule. Did it get lost when we made\n    persistence the default?\n\nNot lost, exactly.  My recollection of the situation is this:  The\ndraft-*-06 spec does not require the client to wait before pipelining.\nDave Morris asserted that this could cause problems; Roy Fielding\nasserted that it would not.  Larry Masinter asked some people to\nconsider the issue.\n\nMy position was that I was not convinced by either side.  I suggested\nthat if Dave (or someone else) could come up with a specific example of\nhow this would break, then I would be more sympathetic to his point of\nview.  As far as I remember, nobody has provided such an example.\nGiven that the \"Proposed Standard\" phase is the place to take this kind\nof risk, I felt it was not necessary to require the client to wait.\n\nNote that if waiting is actually unnecessary and yet the spec requires\nthe client to wait, we are wasting an entire RTT on many (although not\nall) HTTP connections.  I suggest that this would be a mistake.\n\nIn particular, if you agree with me that the existing sticky-header\nproposal has the race condition that I believe it has, it would be\npossible to eliminate it either by requiring a client to wait before\npipelining, or to use the three-way handshake that I described.  But\nbecause of the extra RTT delay the former option imposes on pipelining,\nit seems preferrable to avoid it if possible.\n\n-Jeff\n\n\n\n", "id": "lists-010-6180272"}, {"subject": "Re: Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": "I wrote:\n>Consider this **somewhat contrived** example\n>\n>     client:\n>     ## requesting first object, negotiating sticky headers ##\n>     GET / HTTP/1.1<CRLF>\n>     Accept: text/html<CRLF>\n>     Accept-Language: en<CRLF>\n>     Connection: sticky<CRLF>\n>     <CRLF>\n>\n>     server:\n>     HTTP/1.1 200 OK<CRLF>\n>     MIME-Version: 1.0<CRLF>\n>     Connection: sticky<CRLF>\n>     Content-Type: text/html<CRLF>\n>     Content-Length:94<CRLF>\n>     <CRLF>\n>     <94 bytes body data here>\n>\n>     client:\n>     ## requesting second object, changing one of the to-be-sticky headers ##\n>     GET /a.gif HTTP/1.1<CRLF>\n>     Accept: image/gif<CRLF>\n>     <CRLF>\n>     ## requesting 3rd object, again changing 1 of the to-be-sticky headers##\n>     GET /b.jpeg HTTP/1.1<CRLF>\n>     Accept: image/jpeg<CRLF>\n>     ## requesting 4th object, not sending one of the to-be-sticky headers ##\n>     GET /c.gif HTTP/1.1<CRLF>\n>     <CRLF>\n\nPaul replied (I've reordered things a little):\n    I don't see the ambiguity in this example. [...]\n    I think maybe your example was just buggy (or not clear to me). If the\n    client had sent the second request _before_ it got the reply from the\n    server, then it would have been ambiguous as to whether\n    \"Accept-Language: en\" was omitted from the request.\n\nI think my example wasn't expressed quite right.  Not exactly\n\"buggy\", but not clear enough.  I meant that the server would\ntransmit its \"Connection: sticky\" response at the point given,\nbut I did not mean to imply that the client necessarily received\nit before making its second request.\n\n    The server doesn't need to know when the client received\n    Connection: sticky\n    instead, the client has to send requests that are unambiguous. I.e.,\n    until it receives the\n    Connection: sticky\n    it sends ALL headers that have ever been sent, which is unambiguous --\n    even if the server thinks that the client *could* be omitting headers,\n    none are actually omitted, so the server won't be confused.\n\nI think this rule, which was not stated in your draft (as far as I\ncan tell) is almost right.  The problem is that, as stated, it might\nrequire the client to send inappropriate headers for certain requests\n(if, for example, the client's preferences varied based on some aspect\nof the request-URL).  I would much prefer a more cautious specification,\nthat would work correctly independently of any assumptions we might\nmake know about the semantics of the request headers.\n\n    >I can see several possible ways to solve this:\n    >(1) Explicitly declare that the client, after having sent\n    >\"Connection: sticky\", may start omitting headers as soon as it\n    >receives the server's \"Connection: sticky\" response, but MUST\n    >NOT change any such headers before receiving a response to the\n    >first request sent *after* it has received that \"Connection:\n    >sticky\" response from the server.  This rule has to be\n    >interpreted \"per connection\".  (If you draw out the timing\n    >diagram, you can see that this will always avoid the race\n    >condition.)\n    \n    I agree it needs to be per connection. I don't see why it has to\n    wait the extra round trip. I think the modification above solves\n    the problem.  If a client were _required_ to omit headers that were\n    the same as in the last message once it had received the\n    \"Connection: sticky\" from the server, then I think there would need\n    to be tighter synchorization.\n\nI'm not sure how to interpret that paragraph.  Does \"the modification\nabove\" refer to my #1, or your \"the client has to send requests that\nare unambiguous\"?  If the latter, I would appreciate a careful \ndefinition of \"unambiguous.\"   I'm not sure that \"ALL headers that\nhave ever been sent\" really works.\n\n-Jeff\n\n\n\n", "id": "lists-010-6190399"}, {"subject": "Re: draft-mutz-http-attributes0", "content": "attached mail follows:\nI don't believe I understand the issues a non-scrolling display\nraises when interacting with other attributes such as screen size.\n\nIf for example, the available vertical size of the URL is longer \nthan the size of the display, how is this resolved?  Is a paged\nresource with pages longer than requested better or worse than a\nvertically long resource?  How does one calculate the vertical\nlength of a resource when local treatment of fonts will cause the\nlength to vary?  \n\nI agree non-scrolling displays are relevant and the tag\nUA-media-screen-paged will be added to the draft.\n\nAndy Mutz.\n---------------------------------------------------------------\nAndrew Mutz                          |   mutz@hplabs.hp.com \nHewlett Packard Laboratories         |   Phone:  +1 415 857 6122\n1501 Page Mill Road 3U-3             |   Fax:    +1 415 857 4691\nPalo Alto, California, USA.          | \n---------------------------------------------------------------\n\n\n\n", "id": "lists-010-6202396"}, {"subject": "RE: Sitcky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": ">----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Subject: Re: Sitcky headers and pipelining (was: Sticky header draft -- as\n>an attachment) \n>\n>In particular, if you agree with me that the existing sticky-header\n>proposal has the race condition that I believe it has, it would be\n>possible to eliminate it either by requiring a client to wait before\n>pipelining, or to use the three-way handshake that I described.  But\n>because of the extra RTT delay the former option imposes on pipelining,\n>it seems preferrable to avoid it if possible.\n\nI agree that it has a race condition (but I beleive your example\npurporting to show it had a bug).\n\nI think the simplest fix is to require a client that wants to pipeline\nand use sticky headers to wait to start taking advantage of sticky\nheaders until it has seen a response from the server agreeing to use\nsticky headers. I think this extra RTT wait is OK because I think the\noccaisions where the client has any idea of what to fetch in a pipelined\nmanner when the connection is first opened are rare -- typically, it\nneeds to get the first HTML resource before it knows which GIFs to fetch\n(in a pipelined manner).\n\n\n\n", "id": "lists-010-6211148"}, {"subject": "useful document identification (was encouraging....", "content": ">I suggest using \"Content-ID\" for a globally unique originator-supplied\n>identifier.\n\nthis cannot be used in a secure and verifiable fashion....which is\nprobably why nobody who cares about the integrity of their data uses it.\n\n>yes\n\nyes what? an attempt at sarcasm?  the lack of any verifiable content\nidentification in HTTP is marked.\n>\n>> *2. the method of identification (secure MD5 hash, \n>>registration authority, etc.)\n>\n>unnecessary\n\nhow would you propose to validate the identity of a document without\nknowing the method of identification?\ni would reference the rfc on http-digests where the hash-method is...of\ncourse...critical to the process of password-authentication.  this\ninformation is also required for document authentication\n>\n>> 3. the scope of the content-identifier (eg: *.com)\n>\n>always global\n\n>in the rfc \"draft-ietf-http-digest-aa-04\" this \"useless\" feature of\n>scoping is addressed in reference to authentication digests.  the point\n>was to define a scope for password caching.  this also applies to\ndocument-level caching and for the same reasons detailed therein\n\nin order to perform accurate object identification, a usable\n>\"Content-ID\" header should be specified\n>\n\n\n\n", "id": "lists-010-6221448"}, {"subject": "RE: Sitcky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": "At 02:02 PM 8/7/96 -0700, Paul Leach wrote:\n>I think the simplest fix is to require a client that wants to pipeline\n>and use sticky headers to wait to start taking advantage of sticky\n>headers until it has seen a response from the server agreeing to use\n>sticky headers. I think this extra RTT wait is OK because I think the\n\nI agree that this is the best solution to the problem, if only because it's\nthe simplest.  I know it's the simplest, because it's the one I first\nthought of, and if I thought of it....\n\nThis rule probably should apply to any future Connection features/extensions\nas well.\n\n-----\nDaniel DuBois, Software Animal           http://www.spyglass.com/~ddubois/\n\nSubject: Sorry\nFrom: Dedi Shy <shayd@post.tau.ac.il>\nNewsgroups: sci.chem\n\nI'm afraid that if someone will do a query on me in Deja News research \nservice he'll think I'm a real shallow person cuase all the newsgroups \nI'm writting to begin with alt.tv. So I'm writting to all the \nintellectual newsgroups also, so they'll see I'm really smart. Sorry for \nthe disturbance!\n\n\n\n", "id": "lists-010-6230971"}, {"subject": "RE: Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": ">----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Sent: Wednesday, August 07, 1996 12:52 PM\n>To: Paul Leach\n>Cc: 'http-wg'\n>Subject: Re: Sticky headers and pipelining (was: Sticky header draft -- as\n>an attachment) \n>\n>I wrote:\n>>Consider this **somewhat contrived** example\n>>\n>>     client:\n>>     ## requesting first object, negotiating sticky headers ##\n>>     GET / HTTP/1.1<CRLF>\n>>     Accept: text/html<CRLF>\n>>     Accept-Language: en<CRLF>\n>>     Connection: sticky<CRLF>\n>>     <CRLF>\n>>\n>>     server:\n>>     HTTP/1.1 200 OK<CRLF>\n>>     MIME-Version: 1.0<CRLF>\n>>     Connection: sticky<CRLF>\n>>     Content-Type: text/html<CRLF>\n>>     Content-Length:94<CRLF>\n>>     <CRLF>\n>>     <94 bytes body data here>\n>>\n>>     client:\n>>     ## requesting second object, changing one of the to-be-sticky headers\n>>##\n>>     GET /a.gif HTTP/1.1<CRLF>\n>>     Accept: image/gif<CRLF>\n>>     <CRLF>\n>>     ## requesting 3rd object, again changing 1 of the to-be-sticky\n>>headers##\n>>     GET /b.jpeg HTTP/1.1<CRLF>\n>>     Accept: image/jpeg<CRLF>\n>>     ## requesting 4th object, not sending one of the to-be-sticky headers\n>>##\n>>     GET /c.gif HTTP/1.1<CRLF>\n>>     <CRLF>\n>\n>Paul replied (I've reordered things a little):\n>    I don't see the ambiguity in this example. [...]\n>    I think maybe your example was just buggy (or not clear to me). If the\n>    client had sent the second request _before_ it got the reply from the\n>    server, then it would have been ambiguous as to whether\n>    \"Accept-Language: en\" was omitted from the request.\n>\n>I think my example wasn't expressed quite right.  Not exactly\n>\"buggy\", but not clear enough.  I meant that the server would\n>transmit its \"Connection: sticky\" response at the point given,\n>but I did not mean to imply that the client necessarily received\n>it before making its second request.\n\nOK. That's what I assumed in my reply.\n>\n>    The server doesn't need to know when the client received\n>    Connection: sticky\n>    instead, the client has to send requests that are unambiguous. I.e.,\n>    until it receives the\n>    Connection: sticky\n>    it sends ALL headers that have ever been sent, which is unambiguous --\n>    even if the server thinks that the client *could* be omitting headers,\n>    none are actually omitted, so the server won't be confused.\n>\n>I think this rule, which was not stated in your draft (as far as I\n>can tell) is almost right.\n\nYou're right, it isn't in the draft; I wasn't focussed on pipelining\nwhen I wrote it.\n\n>  The problem is that, as stated, it might\n>require the client to send inappropriate headers for certain requests\n>(if, for example, the client's preferences varied based on some aspect\n>of the request-URL).  I would much prefer a more cautious specification,\n>that would work correctly independently of any assumptions we might\n>make know about the semantics of the request headers.\n\nThis time it is I who wasn't clear. \n\nRegardless of the pipelining issue, when using sticky headers, a client\nalways has to have a way to say\n\"you know that header I sent in the last message -- it just isn't in\nthis message\"\n\nThe mechanism for that in the draft is that the client sends the header\nname and an empty value, which means that, even though there was a value\nfor that header in the last message, this message should be interpreted\nas if the header with that field-name is not present in the message.\n\nThere must be some mechanism like this, independent of the semantics of\nany request header, in order for sticky headers to work at all.\n\n>Applying the above to your claim that \"it might require the client to send\ninappropriate headers for certain requests\" I hope it is not too hard to\nsee that the client can never be forced to send a header it doesn't want\nto, because it has to have (by the nature of sticky headers) to not send\nany given header.\n\nNow, you could object that the mechanism I chose to allow a client to\nnot send a header is a bad one -- it does make one assumption about the\nsemantics of all headers -- that an empty value is never legal.\nI.e. it must be the case, if Foo is to be used as a sticky header, that\nFoo:<CRLF>\nis unambiguous, and means that even if there was a Foo header in the\nprevious message, this message should be interpreted as if there were no\nFoo header, for all headers Foo.\n\nOne could invent a header with the property that (say)\nFoo:<CRLF>\nmeant one thing, and\nFoo: Yes\nmeant another. I don't believe that are currently any examples of this,\nhowever. Certainly, either we would either have to outlaw adding any\nsuch headers to HTTP, or outlaw their participation in the sticky header\nmechanism.\n\n>\n>    >I can see several possible ways to solve this:\n>    >(1) Explicitly declare that the client, after having sent\n>    >\"Connection: sticky\", may start omitting headers as soon as it\n>    >receives the server's \"Connection: sticky\" response, but MUST\n>    >NOT change any such headers before receiving a response to the\n>    >first request sent *after* it has received that \"Connection:\n>    >sticky\" response from the server.  This rule has to be\n>    >interpreted \"per connection\".  (If you draw out the timing\n>    >diagram, you can see that this will always avoid the race\n>    >condition.)\n>    \n>    I agree it needs to be per connection. I don't see why it has to\n>    wait the extra round trip. I think the modification above solves\n>    the problem.  If a client were _required_ to omit headers that were\n>    the same as in the last message once it had received the\n>    \"Connection: sticky\" from the server, then I think there would need\n>    to be tighter synchorization.\n>\n>I'm not sure how to interpret that paragraph.  Does \"the modification\n>above\" refer to my #1, or your \"the client has to send requests that\n>are unambiguous\"?  If the latter, I would appreciate a careful \n>definition of \"unambiguous.\"   I'm not sure that \"ALL headers that\n>have ever been sent\" really works.\n\nIt is the latter. What I meant by \"unambiguous\" is that the client's\nrequests are unambiguous if the server need not know when the client\nreceived the response that agreed to sticky headers in order to process\nthe requests properly. The rule \"ALL headers that have ever been sent\"\nwas not intended to be \"spec-quality\". To elaborate a bit, but still not\nspec quality (it takes me a while longer to write spec-quality prose),\nuntil the client receives the reponse from the server, in each message\nthat it sends, for each header in the message,\nif any previous message had a value for the header, this message has to\neither have a value for that header, or use the \"Foo:<CRLF>\" encoding to\nindicate that it is intended that the header not be present. In\nparticular, it can't omit headers whose value is the same as in the last\nmessage.\n\nThe first time I read your suggestion about the three way handshake, the\nabove seemed simpler, and didn't require the extra bytes of the second\nConnection: sticky (and I didn't understand #1 above). But I'm not so\nsure now that the handshake isn't easier to explain and implement.\n\nPaul \n\n\n\n", "id": "lists-010-6240604"}, {"subject": "RE: Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": "At 02:35 PM 8/7/96 -0700, Paul Leach wrote:\n>The mechanism for that in the draft is that the client sends the header\n>name and an empty value, which means that, even though there was a value\n>for that header in the last message, this message should be interpreted\n>as if the header with that field-name is not present in the message.\n>\n>There must be some mechanism like this, independent of the semantics of\n>any request header, in order for sticky headers to work at all.\n>[...]\n>One could invent a header with the property that (say)\n>Foo:<CRLF>\n>meant one thing, and\n>Foo: Yes\n>meant another. I don't believe that are currently any examples of this,\n>however. Certainly, either we would either have to outlaw adding any\n>such headers to HTTP, or outlaw their participation in the sticky header\n>mechanism.\n\nUh.. The semantics of an empty Accept-Encoding are mentioned in the 11-06\ndraft.  And I think the semantics of \"Accept:<CRLF>\" are both self-evident,\nand clearly not the same as a lack of the header.  Certainly the BNF allows\nnumerous headers to be NULL.\n\nI was presuming (not having read the draft, can I admit that in a discussion\non the draft?) that your intentions were that a sticking client would\nindicate turning off individual headers on subsequent requests by sending\nthe explicit format that was semantically equivalent to the default of those\nindividual headers.  As in, sending \"Accept-Encoding:<CRLF>\" since empty and\nnon-existant have the same meaning, or \"Accept: */*\" since that has the same\nmeaning as non-existant.  This would mean that all stick-able headers must\nhave a explict default-equivalent format (User-Agent:<CRLF> ??), or a that\nyou need a global way to wipe off all the stickyness (Here's where I propose\nthe Towel: header.), or, finally, that you have a\n\"Stuck-Headers-That-Slipped-Off:\" header.\n\n-----\nDaniel DuBois, Traveling Coderman      http://www.spyglass.com/~ddubois/\n\n  The difference between cats and dogs:\n  1) Dogs look at their owners, and say:  \"Hmmm . . . they feed me, and\n     shelter me, and take care of me . . . THEY MUST BE GOD!!!!!!!!!!\"\n  2) Cats look at their owners, and say:  \"Hmmm . . . they feed me, and\n     shelter me, and take care of me . . . I MUST BE GOD!!!!!!!!\"\n\n\n\n", "id": "lists-010-6257527"}, {"subject": "RE: Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash;as an attachment", "content": ">----------\n>From: Daniel DuBois[SMTP:dan@spyglass.com]\n>Sent: Wednesday, August 07, 1996 3:11 PM\n>To: Paul Leach; 'Jeffrey Mogul'\n>Cc: 'http-wg'\n>Subject: RE: Sticky headers and pipelining (was: Sticky header draft --as an\n>attachment) \n>\n>At 02:35 PM 8/7/96 -0700, Paul Leach wrote:\n>>The mechanism for that in the draft is that the client sends the header\n>>name and an empty value, which means that, even though there was a value\n>>for that header in the last message, this message should be interpreted\n>>as if the header with that field-name is not present in the message.\n>>\n>>There must be some mechanism like this, independent of the semantics of\n>>any request header, in order for sticky headers to work at all.\n>>[...]\n>>One could invent a header with the property that (say)\n>>Foo:<CRLF>\n>>meant one thing, and\n>>Foo: Yes\n>>meant another. I don't believe that are currently any examples of this,\n>>however. Certainly, either we would either have to outlaw adding any\n>>such headers to HTTP, or outlaw their participation in the sticky header\n>>mechanism.\n>\n>Uh.. The semantics of an empty Accept-Encoding are mentioned in the 11-06\n>draft.  And I think the semantics of \"Accept:<CRLF>\" are both self-evident,\n>and clearly not the same as a lack of the header.  Certainly the BNF allows\n>numerous headers to be NULL.\n\nOh, foo. Drat. Darn. Shucks. I'll have to look at them. Mumble.\n>\n>I was presuming (not having read the draft, can I admit that in a discussion\n>on the draft?) that your intentions were that a sticking client would\n>indicate turning off individual headers on subsequent requests by sending\n>the explicit format that was semantically equivalent to the default of those\n>individual headers.\n\nOne can always do this. But is there a default for all headers? And is\nit concise enough to be acceptable?\n\n>  As in, sending \"Accept-Encoding:<CRLF>\" since empty and\n>non-existant have the same meaning, or \"Accept: */*\" since that has the same\n>meaning as non-existant.  This would mean that all stick-able headers must\n>have a explict default-equivalent format (User-Agent:<CRLF> ??), or a that\n>you need a global way to wipe off all the stickyness (Here's where I propose\n>the Towel: header.), or, finally, that you have a\n>\"Stuck-Headers-That-Slipped-Off:\" header.\n\nI was trying to avoid either of the last two approaches. But maybe I\ncan't.\n\nPaul\n\n\n\n", "id": "lists-010-6268571"}, {"subject": "Re: draft-mutz-http-attributes0", "content": "Andy Mutz:\n>\n>The new draft of draft-mutz-http-attributes is now available at:\n>ftp://ds.internic.net/internet-drafts/draft-mutz-http-attributes-01.txt\n>\n>Most of the syntax changes in the draft are to provide compliance\n>with the transparent content negotiation method for features described\n>in: draft-holtman-pre02-19.txt.  \n[...]\n>  I would like to see non-numeric values\n>for feature tags allowed.                                          \n\nI'll put non-numeric values in the next transparent content\nnegotiation draft.  Your UA-media-* tag family is an excellent example\nof why they need to exist.  It is much nicer, and more efficient, to\nsay\n\n Accept-Features: UA-media=stationary\n\nthan it is to say\n\n Accept-Features: UA-media-stationary, !UA-media-screen\n\nif you are going to print the response on stationary paper.\n\nWhere appropriate, I'll rewrite the examples in my transparent content\nnegotiation draft to use your feature tags.  I plan to submit an\nupdated transparent negotiation draft as an internet draft soon.\n\n>Andy Mutz.\n\nKoen.\n\n\n\n", "id": "lists-010-6279576"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    However, I think I could be convinced that there only needs to be\n    one count per Etag. The idea was that even if there were only an\n    English version of a page, having the server set \"Vary:\n    Accept-Language\" would cause separate counting of hits for users\n    with \"fr\" \"de\", etc, so that the potential for these languages\n    could be gauged without creating pages for all of them. Being able\n    to count them is, I think, a GOOD THING; but maybe one could create\n    a separate Etag without having to create a separate physical copy\n    of the page.  We'll think about it.\n\nI don't believe this would be a good idea.  As far as I can tell, we\nhave no way to tell a cache \"these two etags are for the same\nentity-body\".  That is, there is no possible way to avoid transmitting\nthe entire entity body twice if it is associated with two different\netags.  So if we were to adopt a \"one count per Etag\" requirement,\nthen this would force extra body transmissions between servers and\nproxies ... just what we are trying to avoid!\n\nI don't see that the Vary-based scheme we proposed is so difficult\nto implement, and it seems to avoid unnecessary transmissions of\nbulk data.\n\n-Jeff\n\n\n\n", "id": "lists-010-6288556"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    But to allocate max-uses values to proxies an efficient way, an\n    origin server seems to have to keeping per-proxy database of\n    `max-use-qouta-use-speed' (last two paragraphs of Section 2), which\n    adds some overhead to every request.\n\nOnly if it really wants to be cautious about bounding the counting\nerror.  I would imagine that the simplest servers would have a global\nsetting for this value (e.g., always send \"max-uses=10\").  A somewhat\nmore sophisticated server might keep a moderate-sized cache of\n(proxy-address, max-use-setting) entries.  At about 8 bytes per\nentry (4 bytes IP address, 2 bytes max-use-setting, perhaps 2 bytes for\nan LRU counter), you could spend about a dime on RAM and keep 1000 such\nentries.\n\n    Reading these paragraphs, the\n    goal of the max-uses allocation heuristics seem to be to ensure\n    that all counts are reported `soon enough'.\n\n\"Soon\" in the sense of \"bounding the error in the final count\", not\nin the sense of \"within 3 hours\".\n\n    It seems that a max-time-to-wait-before-reporting-hits mechanism,\n    can achieve the same goal without the same computational overhead\n    in origin servers.  This mechanism would also eliminate the need\n    for implementing difficult max-use distribution heuristics in proxy\n    caches: a cache could simply subtract the age of the response from\n    the max-time value.\n\n    Even better, we *already have* a\n    max-time-to-wait-before-reporting-hits mechanism in the form of\n    cache-control: max-age.\n\n    I conclude that the max-use mechanism is unnecessary and propose\n    that it is removed, and that a section about using cache-control:\n    max-age is added.\n\nI think this would be a mistake, since it doesn't distinguish between\nvery busy caches and very lightly-used ones.  If you send a small\nmax-age value, then the lightly-used caches might report far more\noften than necessary.  If you send a large max-age value, then the\nheavily-used caches might report far too infrequently (in terms\nof number of uses between reports).\n\n-Jeff\n\n\n\n", "id": "lists-010-6297708"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    I thought up another fix some hours after I sent my message: adopt the\n    rule that a server which is only relaying, not generating, a response\n    must not count it.  In the above example, the origin server itself\n    would count the hit, not any upstream proxy cache.\n    \n    This way, when serving a request the origin server can immediately add\n    1 to its total hit count.  With my earlier fix, it would have to wait\n    for proxy 2 to report the hit in a future request.\n\nThis sounds reasonable, although I want to spend some time working\nout all of the cases before I'm sure.  It's also important to optimize\nthe \"1-hit\" case, since various cache studies have shown that most\nURLs (even the cachable ones) are not re-referenced before being\ndeleted from the proxy cache.  I.e., we want this mechanism to work\nin such a way that if a proxy only uses an entity once, then there\nare no extra messages sent.\n\nSo, for example, the sentence that says:\n  When a proxy first caches R, it should set U to 1.\nmight have to be changed to\n  When a proxy first caches R, it should set U to 0.\nand then the examples would also need to be changed.\n\n    I'm primarily worried about the filesystem read/write overhead needed\n    to maintain the counters.  I would like to hear an implementer say\n    that this is not a problem.\n\nWe say several times that this is a \"best-efforts\" mechanism.  I think\nat one point we discussed explicitly stating that a proxy does NOT\nhave to keep the counts in non-volatile (crash-proof) storage.\nUnless your proxy crashes frequently (which would make it rather\nunattractive in the first place), there is no reason to require\na file-system write to update a counter.  Certainly, it need not\nbe a synchronous write.  (If asynchronous writes were such a problem,\nproxies wouldn't keep multiple log files the way they do now.)\n\n-Jeff\n\n\n\n", "id": "lists-010-6307611"}, {"subject": "Re: Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": "> The mechanism for that in the draft is that the client sends the header\n> name and an empty value, which means that, even though there was a value\n> for that header in the last message, this message should be interpreted\n> as if the header with that field-name is not present in the message.\n\nUmmm, I guess this is one of those things that should have been said\nin the section on message field parsing....\n\nAn empty header field value has a meaning which is distinct from\na nonexistant header field.  An analogy is a variable in Perl --\nno header field is equivalent to a value of undefined (a null pointer),\nwhereas a header field with an empty or whitespace-only value is equivalent\nto an empty value (a pointer to a zero-length data segment).  Whether or\nnot those two have equivalent semantics is dependent on the definition\nof the header field, just as\n\n   Accept:\n\nmeans accept nothing (if no other Accept headers are in the message)\nand no Accept header field means accept anything.\n\n> There must be some mechanism like this, independent of the semantics of\n> any request header, in order for sticky headers to work at all.\n\nTime to look for another mechanism...\n\n  Connection: sticky\n  Sticky: reset=\"Accept\"\n\nand assorted other yucky ideas come to mind.\n\nBTW, while we are on the topic, I would prefer that the two unrelated\nconcepts of sticky headers and short header names be in two separate\ndrafts.  They should be evaluated independently.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-6317116"}, {"subject": "Re: Questions about i18n support in HTT", "content": "> I apologize in advance if the following question shows my lack of\n> understanding of your latest spec.  :-)\n\n> I am reading your draft-ietf-http-v11-spec-01.txt document which\n> describes the HTTP 1.1 specification.  \n\nYou're reading the wrong version. The current draft is\ndraft-ietf-http-v11-spec-06, and Section 2.2 is explicit about the\ncharacter set of request and response headers (most are restricted to\nASCII except those that use TEXT; those can be encoded using RFC 1522\nrules), as well as your other questions. (\"content-language\" describes\nthe content, while accept-language describes the client's\npreferences).\n\nLarry\n\n\n\n", "id": "lists-010-6327078"}, {"subject": "Re: Meta data in anchors.", "content": "Peter J Churchyard:\n>\n>When following discussions on content negociation and similiar issues,\n>I often reflect as to whether it would be a good idea to allow meta data\n>to be embedded in the anchor when a document is retrieved so that if a \n>link is to a docuemnt is several formats, the user could xxx-click on the\n>link and get a menu displayed of the available formats.\n\nWell, the transparent content negotiation draft (see\nhttp://gewis.win.tue.nl/~koen/conneg/) defines a suitable meta data\nformat for this: the variant list.  Such lists could be included (with\nappropriate escapes for > and \") in anchors.\n\nHowever, transparent content negotiation already makes it possible to\nxxx-click on a link and get a menu of available formats, though it\nwill take a GET request to retrieve the list from the server.\n\n>Most requests must be made by users clicking on a link. Having meta data\n>available as part of the link could help to negate some of the\n>complexity of negociation.\n\nMeta data as part of the link would make content negotiation more\nefficient, and so could reduce complexity by eliminating the need for\nsome efficiency mechanisms elsewhere.\n\nHowever, I think that keeping the meta data in the links in sync with\nthe actually available variants will be a *very* complex task\n(especially for links to other peoples negotiated home pages).  So you\nend up pushing a lot of complexity to the content author, just to\neliminate a little complexity for server, proxy, and user agent\nauthors.\n\n>My limited use of Gopher+ has shown this to be very useful. You get to\n>specify your general preferences but still have the option of easily\n>fetching an alternate representation. \n\nYes, this functionality is extremely useful.  But transparent content\nnegotiation provides for it without requiring annotated links.\n\n>Pete.\n\nKoen.\n\n\n\n", "id": "lists-010-6335450"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">You don't understand my argument: the customers above are the ones who\n>pay for each the hit.  They are not the ones who do the cache busting:\n>it is the web advertising sites they pay who do the cache busting.\n>These advertising sites are sophisticated enough to pick and deploy\n>the mechanism which gets them the highest hit counts, which is cache\n>busting until something better comes along.\n\nI have to agree with Paul here....\n\nProfessionals (IE: Pathfinder) no longer report things like \"10K hits\nper day\" to clients who pay well.  They say \"we have a large\ninternational audience\" or \"we get 40% of our hits from browsers which\nsupport Java\".\n\nInformation such as \"User Agent\" and the clients ip address (for\ndemographics) are crucial to the log reporting in the sites I have\nworked on (albeit only 6 sites).  What companies want to do is leverage\ndemographic information and statistics by proving that their content is\nviewed for in a given region/language and OS.  (somebody who is selling\nMacintosh software won't want to advertise on a site whose viewers all\nuse Windows........etc.)\n\nPerhaps the hit-metering process should allow a proxy to forward some\nsort of a headers-only-summary during a period of relative inactivity. \nThe server should not care how long it has been since the proxy has last\nsent its summary.   The \"Expires\" header can then still be used to\naccurately reflect the duration of the validity of the document.\n\n\n\n>\n\n\n\n", "id": "lists-010-6344592"}, {"subject": "Call for compatibility tester", "content": "The transparent content negotiation draft\n(http://gewis.win.tue.nl/~koen/conneg/) currently specifies the 300\nresponse code for list responses.  \n\nIt turns out that this is not compatible with several existing\nbrowsers: lynx and some versions of Mosaic fail to display anything if\nthey get back a 3xx class response without a Location header.  As lynx\nin particular will be important for some people who want to offer\nnegotiated material, this rules out use of the 300 response code in\ntransparent content negotiation.\n\nI have done some checking, and it seems that the creation of a new\nresponse code:\n\n  416 List Response\n\nis the best way to get downwards compatibility.\n\nThe question now is: does 416 indeed cause compatible behavior with\nall existing browsers?  To find out, I have made a test page:\n\n  http://gewis.win.tue.nl/~koen/conneg/test-416.html\n\nand am calling for everyone who has a web browser other than NetScape,\nNCSA Mosaic, or Lynx to do the compatibility test on the page.  This\nwon't take longer than 20 seconds.  I'll report a summary of the\nresults back to the list.\n\nThanks,\n\nKoen.\n\n\n\n", "id": "lists-010-6355294"}, {"subject": "Beyond 1.", "content": "I agree with the discussion that the http 1.1 spec is getting a bit large.\n\nI would like to see the Transfer Protocol part of HTTP be split out as \na seperate standard from the HyperText part.\n\nThe transfer protocol spec would be about the 'wire' format, \nrequests, responses, persistance, chunking.\n\nThe HyperText part would be about content negociation, cacheing, \nhttp-attributes.\n\nThe transfer protocol spec would define the headers that are specific to \nit and say that other headers will also be added by the higher levels.\n\nI do beleive in the N layer model of communications protocols.\n\nPete.\n-- \nThe TIS Network Security Products Group has moved again!\nvoice: 301-527-9500x111  fax: 301-527-0482\nRoom 334, 15204 Omega Drive, Rockville, MD 20850\n\n\n\n", "id": "lists-010-6363656"}, {"subject": "Re: Beyond 1.", "content": "Peter J Churchyard:\n>\n>I agree with the discussion that the http 1.1 spec is getting a bit large.\n>\n>I would like to see the Transfer Protocol part of HTTP be split out as \n>a seperate standard from the HyperText part.\n[...]\n>I do beleive in the N layer model of communications protocols.\n\nI also believe that HTTP/1.1 can be defined as an N layer protocol,\nand that such a definition would be a good way to cope with the size\nof 1.1.\n\nHowever, making such an N layer model is no trivial task, especially\nif you are not allowed to make little changes to the protocol along\nthe way.  I don't think this group has the manpower needed to create\nan N-layer definition of 1.1 on short notice.\n\nI hope that HTTP/2.0 will end up having a layered definition, but even\nthere I have my doubts about manpower requirements.\n\n>Pete.\n\nKoen.\n\n\n\n", "id": "lists-010-6370884"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "> From:          koen@win.tue.nl (Koen Holtman)\n> Subject:       Re: New document on \"Simple hit-metering for HTTP\"\n> To:            paulle@microsoft.com (Paul Leach)\n> Date:          Wed, 7 Aug 1996 14:54:07 +0200 (MET DST)\n> Cc:            koen@win.tue.nl, mogul@pa.dec.com,\n>                http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n\n> \n> Koen:\n> >>>>In the current (classic) meaning of the word,\n> >>>>\n> >>>>  1 hit-classic = 1 request on an origin server.\n> \n> Paul:\n> >Actually, it just occurred to me -- you're definition of \"hit-classic\"\n> >is bogus. We (at least I) do not know how sites count hits.\n> \n> You may not know that, but I am pretty sure that when people say `my\n> server gets 10K hits per day', they mean request messages per day.\n> \n> Maybe some statistics packages use other definitions of `hits', but I\n> am sure that cache-busting advertising sites who get payed by the hit\n> use this definition.  This definition yields the highest count, and\n> high counts is what they want.  Even an image transfer aborted at 10%\n> would be a hit; it yields a line in the access log file, after all.\n> (Which reminds me: you probably have to add something to your draft\n> about how to count aborted transfers).\n> \n> Koen:\n> >>Look, if all hit count customers were sophisticated enough to pay more\n> >>for `high quality' hits, advertising sites could not make more money\n> >>by using cache busting.  So if your assumed level customer of maturity\n> >>was indeed present, there would not be a cache busting problem to\n> >>solve in the first place.\n> \n> Paul:\n> >Huh? Until we deploy something in caches, \"cache-busting\" is the only\n> >way they have to get demographic data. \n> \n> Nope. You can collect demographic data fine without cache busting; you\n> just get less of it.  There will still be plenty of stuff in your\n> logfile.\n> \n> Also, you really need to distinguish here between two kinds of\n> demographic data:\n> \n> 1) Hit counts\n> \n> 2) User's Referer field, IP address, User-Agent field, ...\n> \n> Your hit counting mechanism can only reduce the cache busting done to\n> get more of 1), not the cache busting done to get more of 2).  \n> \n> That is why I focus on by how successful your scheme is at getting\n> more of 1).  If most advertising sites are actually doing cache\n> busting to get more of 2), (and they might, I don't know) then your\n> proposal will not reduce cache busting anyway.\n> \n> [...]\n> >>I guess we could argue at length about present and predicted levels of\n> >>maturity (both in the US and in Europe), but the bottom line is this:\n> >>\n> >> I feel very strongly that it would be a huge mistake to make a scheme\n> >> to eliminate cache busting dependent on something, sufficient\n> >> customer sophistication in this case, the existence of which is\n> >> questionable at best.\n> >\n> >First, this isn't a scheme to \"eliminate\" cache busting, it is only\n> >trying to reduce it.\n> >\n> >Second, really unsophisticated customers just won't use it -- they'll\n> >continue cache-busting.\n> \n> You don't understand my argument: the customers above are the ones who\n> pay for each the hit.  They are not the ones who do the cache busting:\n> it is the web advertising sites they pay who do the cache busting.\n> These advertising sites are sophisticated enough to pick and deploy\n> the mechanism which gets them the highest hit counts, which is cache\n> busting until something better comes along.\n> \n> [...]\n> >>I'm primarily worried about the filesystem read/write overhead needed\n> >>to maintain the counters.  I would like to hear an implementer say\n> >>that this is not a problem.\n> >\n> >Mine say it isn't.\n> \n> OK, that is good enough for me.\n> \n> \n> Koen.\n>==========================================================\nASJzxc/;MKazsxc;/ljkZXDC /l;jkm  \n> \n> \n\n\n\n", "id": "lists-010-6378505"}, {"subject": "RE: Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": ">----------\n>From: Roy T. Fielding[SMTP:fielding@liege.ICS.UCI.EDU]\n>Subject: Re: Sticky headers and pipelining (was: Sticky header draft -- as\n>an attachment) \n>\n>BTW, while we are on the topic, I would prefer that the two unrelated\n>concepts of sticky headers and short header names be in two separate\n>drafts.  They should be evaluated independently.\n\nI thought about that. I agree that they should be evaluated separately,\nand can be adopted independently. However, if both are adopted, using\none mechanism (Connection: sticky) to say that you're using both saves\nsome bytes on the wire. Even if sticky headers are in use, the use of\nabbreviations is not required, so a client that only wants to do sticky\nis not forced to do extra work. The only drawback I can see is if the\nclient wants to do abbreviations but not do sticky headers. I don't know\nif that is likely.\n\nUnless there is some reason to believe that one will fly and the other\none won't, then I'd personally avoid the overhead of a separate draft.\n\n\n\n", "id": "lists-010-6392766"}, {"subject": "Size of the Spec Was:Re: Beyond 1.", "content": "I's like to take an unpopular position here. I don't think that the\nHTTP/1.1 spec is too large at all. It may be larger than the average\nIETF spec but I'm not all that impressed by many IETF specs. Many\nsmall specs are small because they stopped being developed once a\nminimum set of functionality was achieved. Every major application\nprotocol could do with a thorough rewrite, many such projects are \nin progress. Consider the following :-\n\nTelnet,\nBarely functional terminal protocol. Is claimed to contain\na negotiation facility yet that facility does not work. Result,\nthe user is prompted to type in the terminal type when connected.\nOther manufactures \"solve\" this problem by not providing\ncommand line editing.\n\nFTP,\nThe spec for FTP was written before the directory/subdirectory\nconcept was firmly established as the method of file organisation.\nAs a result FTP has somewhat ambiguous connection semantics making\nimplementation of the ftp:// URL somewhat of a trial and error\nprocess.\n\nSMTP,\nA spec which was betrayed by implementation. If SMTP mail\nservers actually implemented the protocol then email would\nbe much more reliable. As it is the email user receives \na host of spurious \"bounced mail\" messages.\n\nThe obvious solution here is to provide a return identifier\nfor bouncing an email so that these conditions can be handled\\\nautomatically.\n\nNNTP,\nA spec whose definition is determined more by the email \nsource code than by its RFC. Numerous threaded news\nextensions.\n\nThis is not to say that the above protocols are wrong or badly designed,\nnor that the moves to upgrade them are unimportant. The fact is however\nthat a full description of a completed and upgraded NNTP spec would\nprobably rival the HTTP spec in size. Comparing a completed HTTP spec\nto an incomplete NNTP spec is not a valid comparison.\n\nIt would be nice to strip down HTTP to its essentials and start again.\nWe don't have that opportunity. We have a deployed protocol and little\nchance to do a major upgrade. That chance probably went about the time\nNCSA released Mosaic.\n\nI'm somewhat unimpressed by the scheme approach to stripping down a \nsystem. You don't make a system simpler by restricting its\nfunctionality. You make it more complex to implement systems because \neach user must reinvent that missing functionality. To make a\nsystem simple you must apply more powerful ideas. Scheme looses \nbecause it rejects objects in favour of \"simplicity\". It would\nhave been better to have based the system arround objects and thrown\nout the old ideas rendered obsolete.\n\nIt would be nice to apply a layered model to HTTP but again I don't\nthink that that is necessarily appropriate. I don't think that HTTP \ndecomposes in a layered fashion. I think we need to look elsewhere.\n\n\nPhill\n\n\n\n", "id": "lists-010-6403069"}, {"subject": "Re: Size of the Spec Was:Re: Beyond 1.", "content": "> From http-wg-request@cuckoo.hpl.hp.com Thu Aug  8 10:22:58 1996\n> Resent-Date: Thu, 8 Aug 1996 18:21:40 +0100\n> To: Peter J Churchyard <pjc@trusted.com>,\n>         http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Cc: hallam@Etna.ai.mit.edu\n> Subject: Size of the Spec Was:Re: Beyond 1.1  \n> From: hallam@Etna.ai.mit.edu\n...\n> I's like to take an unpopular position here. I don't think that the\n> HTTP/1.1 spec is too large at all. It may be larger than the average\n\nThere are two distinct issues -\n- size\n- layers (or separable protocols)\n\nI would propose that the HTTP/1.1 spec should be split\nmore because of the latter than the former.\n\nThere are really distinct protocols:\n\n- object exchange (HTTP)\n\n- caching (MIME extensions for caching distributed objects)\n\n- non-protocol HTTP extensions (MIME extensions for HTML)\n\n- protocol extensions for persistence\nincluding extensions for chunking and muxing\n\n> It would be nice to strip down HTTP to its essentials and start again.\n> We don't have that opportunity. We have a deployed protocol and little\n> chance to do a major upgrade. That chance probably went about the time\n> NCSA released Mosaic.\n\nWe can simply spec a new version, and deal with backward compatibility \nor reduced-function translators until the phaseover is complete.\nThis is a poor argument for inertia, ever since version numbers have\nbeen included in protocol specs.\n\nJoe\n----------------------------------------------------------------------\nJoe Touch - touch@isi.edu    http://www.isi.edu/~touch/\nISI / Project Leader, ATOMIC-2, LSAM       http://www.isi.edu/atomic2/\nUSC / Research Assistant Prof.                http://www.isi.edu/lsam/\n\n\n\n", "id": "lists-010-6413476"}, {"subject": "RE: Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": "On Thu, 8 Aug 1996, Paul Leach wrote:\n\n> is not forced to do extra work. The only drawback I can see is if the\n> client wants to do abbreviations but not do sticky headers. I don't know\n> if that is likely.\n\nI can certainly see use of abbreviations w/o sticky headers. The\ncomplexity of implementing abbreviations is an order of magnitude or more\nless while still achieving a reduction in wire traffic.\n\nDave Morris\n\n\n\n", "id": "lists-010-6424450"}, {"subject": "Re: Size of the Spec Was:Re: Beyond 1.", "content": ">There are two distinct issues -\n>- size\n>- layers (or separable protocols)\n>\n>I would propose that the HTTP/1.1 spec should be split\n>more because of the latter than the former.\n\nWhile it would be nice to have better arrangement of the spec I think\nthat Jim et. al have been doing a very good job on this front. The\nHTTP/1.1 document is a lot clearer than earlier editions.\n\nI think this is a presentation and not a protocol design issue. The\nspec does have a clear separation of the various aspects of the \nprotocol. I don't think that separating it out into four separate\ndocuments would make it easier to implement or understand. I think\nthat the separations you describe have been addressed already.\n\nThe complaints I have seen have been about the size of the spec,\nnot its organisation. I don't think that the size is unreasonable \nfor the functionality the spec delivers, nor do I think that the \nspec is over complex. If I did I would have been complaining\nloudly at the time.\n\n\n>We can simply spec a new version, and deal with backward compatibility \n>or reduced-function translators until the phaseover is complete.\n>This is a poor argument for inertia, ever since version numbers have\n>been included in protocol specs.\n\n\nI don't think that such a move would simplify the spec at all. In fact\nI can think of few ways to make implementation more onerous and error\nprone. If you introduce a new basis for the protocol you will force\nimplementors to code both. Getting rid of HTTP/0.9 took long enough\ndespite there being less than 100 servers when it was superceeded.\n\nI think that we can implement syntactic changes such as Paul's \nheader compression hack. But the scope for such changes is limited.\nIf we attempt to change the semantics of HTTP then we will end up\nin the sendmail problem. Whatever extensions are proposed the spec\nwill be rooted in the past by an installed base of software whose\narchitecture is inadequate to extend it. \n\nLook at the various \"replacements\" for CGI. Much of the installed \ninfrastructure of the Web is now in plug in modules of one kind\nor another. Most API replacements for CGI provide parsed headers in \nsome sort of table indexed by the header name. Such implementations\ncan probably be adjusted to cope with Paul's proposal but I don't \nthink that they could be adjusted to cope with very much more.\n\nI think that at this point we should look to the functional \nimprovements possible in HTTP/1.2 and then look at any architectural\nchanges necessary. I don't think that it will be very fruitfull to \nlaunch an attempt to reduce the size of the spec. \n\n\nPhill\n\n\n\n", "id": "lists-010-6433999"}, {"subject": "Sticky stuff", "content": "Some comments on the sticky headers draft:-\n\n1) The asymmetry between the client/server responses may be due\nmore to the current limited way in which HTTP is used than a\nfeature of the problem itself. If we get servers which can implement\nthe PUT and POST mechanisms then I think that the situation might\nwell change.\n\n2) I'm not sure of the amount that these proceedures buy us. It\nwould be nice to have figures. Jim G. has made good points about\nthe importance of getting as much usefull information in\nthe first packet send out (i.e. before we hit the slow start\nthrottle). This mechanism appears to be aimed more at increasing\nthe efficiency of later packets.\n\nI suspect that the control data is not a substantial fraction of \nthe total message size. It may be more effective to push on\npeople to implement compression/decompression of message data\nrather than to worry overmuch on the size of the control data.\nOr at the very least point out this issue in the draft.\n \n3) Section 2.2 asserts that proxies typically multiplex server\nconnections across multiple clients. Is this in fact the case?\nWhat is the actual benefit of doing this? How often do two\npeople from the same site wish to connect to the same remote site\nsimultaneously? \n\nI am very skeptical about people having implemented such a \nfeature in a multi-process server where the interprocess communication\noverhead would be very large for the payoff. Certainly\nthe phrase \"typical\" does not seem appropriate. I could see it\nbeing possible in a single process, multi-thread server. I\nwould like to see figures showing how often this case came up\nbefore compromising other optimisations to adapt to this\ncomplication.\n\nI point out this problem because much of the complication of \nthe spec appears to be working arround this convergence of\nindependent sessions into a single stream. \n\n\nOn the other hand there may well be a number of proxies performing\nusefull work undoing the effects of simultaneous connections\nfor image downloads. In this situation combining 4 streams from\none anti-social browser into one is quite plausible, but note that\nin this case the headers will probably be compresssable!\n\n\n4) The sticky header and possibly the connection header should be \nexplicitly excluded from the set of sticky headers (!)\n\n5) Section 6.\n\nThis section should note that \"replay attack\" problem will always\nbe present whenever the compression technique is possible. If\nan authentication technique authenticates the message itself then\nit will have to be a function of the message body and hence not\n\"sticky-able\".\n\n6) Some mechanism for flushing the header cache would be usefull.\nThis would help in the multiplexing proxy server case. After \nit is finished receiving input from one source the proxy can send\na \"flush\" message and reset the stream.\n\n\nOverall I would like to see an awfull lot of numbers based on \nempirical measurement before deciding whether this is a \nworthwhile scheme or not. Although it looks OK to me I know\nfrom experience that without hard numbers it is very easy to\noveroptimise corner cases that almost never occur.\n\nPhill\n\n\n\n", "id": "lists-010-6444137"}, {"subject": "Re: Sticky stuff", "content": "hallam@etna.ai.mit.edu wrote:\n> \n> \n> \n> Some comments on the sticky headers draft:-\n> \n> 1) The asymmetry between the client/server responses may be due\n> more to the current limited way in which HTTP is used than a\n> feature of the problem itself. If we get servers which can implement\n> the PUT and POST mechanisms then I think that the situation might\n> well change.\n\nApache already implements PUT and POST, but I'm not sure I see why that changes\nthe situation?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-6454832"}, {"subject": "Re: Sticky stuff", "content": "    1) The asymmetry between the client/server responses may be due\n    more to the current limited way in which HTTP is used than a\n    feature of the problem itself. If we get servers which can implement\n    the PUT and POST mechanisms then I think that the situation might\n    well change.\n\nI basically agree.  If there is a straightforward way to make\nsticky headers work in both directions, it seems to make more sense\nto define that now, rather than assume that the current asymmetry\nwill be true for every.  If there is a true asymmetry of the complexity\nof supporting sticky headers, then maybe the request-only approach\nis best, but so far I haven't see any suggestion that this is the case.\n    \n    2) I'm not sure of the amount that these proceedures buy us. It\n    would be nice to have figures. Jim G. has made good points about\n    the importance of getting as much usefull information in\n    the first packet send out (i.e. before we hit the slow start\n    throttle). This mechanism appears to be aimed more at increasing\n    the efficiency of later packets.\n    \n    I suspect that the control data is not a substantial fraction of \n    the total message size. It may be more effective to push on\n    people to implement compression/decompression of message data\n    rather than to worry overmuch on the size of the control data.\n    Or at the very least point out this issue in the draft.\n     \nActually, headers are the predominant source of data bytes flowing\nfrom client to server (i.e., in the request direction), at least as\nfar as I am aware.  This is not such a significant fraction of the\nbytes on the Internet backbone, perhaps, but when people are using\nlow-bandwidth links, the request-header-transmission delays\ndirectly contribute to the user's perceived response time, and\nreducing them would seem valuable.  Also, if the home market becomes\nwidely served by asymmetric-bandwidth systems (such as Hybrid\nNetwork's product; see http://hybrid.com/), then request-header\nbytes become proportionally more expensive.\n\n    3) Section 2.2 asserts that proxies typically multiplex server\n    connections across multiple clients. Is this in fact the case?\n\n(Almost?) nobody yet uses proxies that support persistent connections.\nSo it's hard to provide data from experience.  At least, I have not\nseen any.\n    \n    Overall I would like to see an awfull lot of numbers based on \n    empirical measurement before deciding whether this is a \n    worthwhile scheme or not. Although it looks OK to me I know\n    from experience that without hard numbers it is very easy to\n    overoptimise corner cases that almost never occur.\n    \nI think it would be nice to get a trace of the actual bytes\ncarried by a real proxy (not just the URLs), and apply the\nproposed compression schemes to see how successful they are.\nOf course, this would have to be done carefully to avoid\nbreaching the privacy of the proxy's users.\n\n-Jeff\n\n\n\n", "id": "lists-010-6463136"}, {"subject": "RE: Sticky stuff", "content": ">----------\n>From: hallam@Etna.ai.mit.edu[SMTP:hallam@Etna.ai.mit.edu]\n>Sent: Thursday, August 08, 1996 1:07 PM\n>To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Sticky stuff. \n>\n>\n>\n>Some comments on the sticky headers draft:-\n>\n>1) The asymmetry between the client/server responses may be due\n>more to the current limited way in which HTTP is used than a\n>feature of the problem itself. If we get servers which can implement\n>the PUT and POST mechanisms then I think that the situation might\n>well change.\n\nI don't understand this. Accept-* headers (for example) are always\nrequest headers, and only clients ever send them, even on PUT and POST.\nSo I don't know how increased PUT and POST popularity would change the\nasymmetry.\n>\n>2) I'm not sure of the amount that these proceedures buy us. It\n>would be nice to have figures. Jim G. has made good points about\n>the importance of getting as much usefull information in\n>the first packet send out (i.e. before we hit the slow start\n>throttle). This mechanism appears to be aimed more at increasing\n>the efficiency of later packets.\n\nThere's not much that can be done about initial packets, unless somehow\nthe negotiation step can be skipped. On low speed dial-in lines,\ncompressing later packets still helps.\n>\n>I suspect that the control data is not a substantial fraction of \n>the total message size.\n\nIt is for GET requests -- there is no data.\n\n> It may be more effective to push on\n>people to implement compression/decompression of message data\n>rather than to worry overmuch on the size of the control data.\n>Or at the very least point out this issue in the draft.\n\nSure.\n> \n>3) Section 2.2 asserts that proxies typically multiplex server\n>connections across multiple clients. Is this in fact the case?\n\n>My wording was ambiguous. The \"typically\" referred mostly to the fact that\n>they acted on behalf of many clients. In practice today, probably not many\nproxies implement persistent connections, so they obviously can't\n>multiplex them. In the subWG meeting on persistent connections, I recall\n>comments to the effect that persistent connections would be good for proxies,\n>who could keep them open to servers longer because they would be merging\n>requests from many clients.\n\n>What is the actual benefit of doing this? \n\nThe connection does not have to be reopened.\n\n>How often do two\n>people from the same site wish to connect to the same remote site\n>simultaneously?\n\n It doesn't have to be simultaneuously -- just near enough in time to\n>make it worth keeping the connection open,\n>\n>I am very skeptical about people having implemented such a \n>feature in a multi-process server where the interprocess communication\n>overhead would be very large for the payoff. Certainly\n>the phrase \"typical\" does not seem appropriate. I could see it\n>being possible in a single process, multi-thread server. I\n>would like to see figures showing how often this case came up\n>before compromising other optimisations to adapt to this\n>complication.\n\nWhat other optimization was compromised?\n>\n>I point out this problem because much of the complication of \n>the spec appears to be working arround this convergence of\n>independent sessions into a single stream.\n\nHow so? A second context is created just like the first one, with the\naddition of specifying the context number.\n>\n>\n>On the other hand there may well be a number of proxies performing\n>usefull work undoing the effects of simultaneous connections\n>for image downloads. In this situation combining 4 streams from\n>one anti-social browser into one is quite plausible, but note that\n>in this case the headers will probably be compresssable!\n>\n>\n>4) The sticky header and possibly the connection header should be \n>explicitly excluded from the set of sticky headers (!)\n\nGood point.\n>\n>5) Section 6.\n>\n>This section should note that \"replay attack\" problem will always\n>be present whenever the compression technique is possible. If\n>an authentication technique authenticates the message itself then\n>it will have to be a function of the message body and hence not\n>\"sticky-able\".\n\nI don't think so -- the authentication should be applied before the\nsticky compression. Maybe I just don't understand this point.\n>\n>6) Some mechanism for flushing the header cache would be usefull.\n>This would help in the multiplexing proxy server case. After \n>it is finished receiving input from one source the proxy can send\n>a \"flush\" message and reset the stream.\n\nOK.\n>\n>Overall I would like to see an awfull lot of numbers based on \n>empirical measurement before deciding whether this is a \n>worthwhile scheme or not. Although it looks OK to me I know\n>from experience that without hard numbers it is very easy to\n>overoptimise corner cases that almost never occur.\n\nAnybody have traces? The usual log doesn't have enough data to see what\nthe improvement is, I suspect. Also, the presence of the mechanism may\nalter behavior -- instead of sending tailoring the Accept header value\nbased on the URL's extension (.html, .gif, etc) to save space, the\nclient could just create on big Accept that they send once and then omit\ntherever after.\n\nI have logs from our proxies that can measure the interval between\nconsecutive references to a single site to see it pays for a proxy to\nkeep a connection open to a server after a client is done with it.\n\nPaul\n>\n>\n>\n>\n>\n>\n>\n>\n>\n\n\n\n", "id": "lists-010-6473217"}, {"subject": "Statistics on reusing request headers in persistent connections (repost", "content": "   The benefits of reusing request headers in persistent\n   ----------------------------------------------------\n   HTTP connections: A statistical analysis.\n   -----------------------------------------\n\n                                           Oct 31, 1995\n                                           Koen Holtman, koen@win.tue.nl\n\n 1. INTRODUCTION\n ---------------\n\nWhen sending HTTP request over a persistent (keep-alive) HTTP\nconnection, it would be possible to re-use request headers from\nearlier requests in subsequent requests.  For example, if the\nUser-agent header for requests n and n+1 are the same, there would be\nno need to send the header twice, a special request header (using less\nbytes) could indicate that the User-agent header is to be reused.\n\nRoy Fielding recently proposed a mechanism allowing such reuse.  The\nquestion is whether designing and implementing such a mechanism would\nbe a good move.\n\nFor:  - less HTTP traffic\n      - faster browsing response time\n\nAgainst: - more software complexity\n         - time spent in design and implementation cannot be\n           used for making other improvements\n\nI have made some statistics about the size of the gains.\n\n\n 2. CONCLUSION\n -------------\n\nMy conclusion is that the gains are too small to bother about request\nheader reuse at this point:\n      - HTTP traffic savings would be about 1.3%\n      - speedup of browsing response time would be minimal:\n        page+inline loading times would be noticeably faster in\n        about 17% of all cases.\n\nMuch higher gain/effort ratios can be had by focusing on other\ndesirable features of future HTTP software, for example\n\n - (general) support for `Content-Encoding: gzip'\n - support for sending .jpg inlines instead of .gif inlines to all\n   browsers that can handle .jpg\n - reducing the amount of Accept headers generated by some browsers\n   (my Mosaic for X browser sends 822 bytes of accept headers, most of\n   them for MIME types I can't even view!), maybe introducing a\n   mechanism for reactive content negotiation at the same time.\n - proxies that change multiple Accept headers in a request into one\n   big Accept header when relaying the request\n\nI therefore propose to drop the subject of request header reuse on\nhttp-wg.\n\nHeader reuse mechanisms would only get interesting again if we find\nsome good reason to make the average request message much larger (say\n500 bytes) than it needs to be now (200 bytes).\n\n(End of conclusions.)\n\nYes, you can stop reading now!\n\nYou can also page to Section 6, which contains some statistics about\nthe number of requests done over persistent connections.\n\n\n 3. HOW LARGE DO REQUEST MESSAGES NEED TO BE?\n --------------------------------------------\n\n 3.1 CURRENT ACCEPT HEADER PRACTICE\n -----------------------------------\n\nI captured the request headers sent by the three browsers present on\nmy Linux box. \n\nA typical Mozilla/1.12 (X11) GET request message for a normal URL:\n\n  ---------------------------------------------------\n  GET /blah/blebber/blex.html HTTP/1.0\n  User-Agent: Mozilla/1.12 (X11; I; Linux 1.2.9 i486)\n  Referer: http://localhost/blah/blebber/wuxta.html\n  Accept: */*\n  Accept: image/gif\n  Accept: image/x-xbitmap\n  Accept: image/jpeg\n\n  ---------------------------------------------------\n\nWhen GETting URL contents for inline images, Mozilla omits the\n`Accept: */*' header above.\n\nNote that the four Accept headers above could be combined into a\nsingle Accept header:\n\n  Accept: */* image/gif image/x-xbitmap image/jpeg .\n\nNone of the three browsers on my Linux system do such combining,\nthough it would make the request message shorter (see also the table\nbelow).  Is there some ancient HTTP server, not supporting\nmulti-element Accept headers, they want to stay compatible to?\n\nHere is a table of typical GET request message sizes for the browsers\non my Linux system:\n\n  -----------------------+---+---+-----+----\n  Browser                 Len Acc (Ac1) Rest\n  -----------------------+---+---+-----+----\n  NCSA Mosaic for X/2.2   995 882 (299) 113\n  Lynx/2.3 BETA           349 248 (100) 101\n  Mozilla/1.12 (normal)   207  73  (36) 134\n  Mozilla/1.12 (inline)   194  61  (34) 133\n  -----------------------+---+---+-----+----\n\n  Len  : #bytes in request message\n  Acc  : #bytes in the Accept headers\n  (Ac1): #bytes that would be in an equivalent single-line Accept header\n  Rest : #bytes in non-Accept headers and first line of request\n\n\n 3.2 LACK OF NEED FOR LARGE ACCEPT HEADERS\n -----------------------------------------\n\nIn current practice on the Web, 99% of all URLs (if not more) only\nhave one content variant, so the Accept headers contained in a request\nare almost never used.  It is unlikely that this will change in the\nfuture.\n\nThus, there is no good reason for tacking large Accept headers onto a\nrequest, now or in the future.  An accept header larger than\n\n  Accept: */* image/gif image/x-xbitmap image/jpeg\n\nis wasteful, the small number of cases case not covered by the header\nabove could be solved by reactive content negotiation (300 and 406\nresponses).  Note that, if a browser discovers it is doing a lot of\nreactive content negotiation to a site, it could dynamically make its\nAccept headers to that site larger to reduce future reactive\nnegotiation.  So sending large Accept headers may be efficient\nsometimes, but not by default.\n\nI see the large default Accept header problem as a problem that will\ndisappear with browser upgrades in the near future, after a reactive\nnegotiation mechanism has been defined.\n\n\n 4. STATISTICS\n -------------\n\nTo make the statistics below, I took a set of proxy<->server HTTP\ntransactions between the www.win.tue.nl proxy and off-campus servers\n(18 days worth of traffic, approximately 150Mb in 14501 HTTP\ntransactions), and calculated what would happen if these\ntransactions were all done over persistent HTTP connections.\n\nIf a simulated persistent connection has been idle for 10 minutes, it\nis closed.\n\n\n 4.1 HEADER SIZES\n ----------------\n\nWorking from the reasoning above, I take the following request\nmessage, generated by Mozilla, as typical.\n\n  ---------------------------------------------------\n  GET /blah/blebber/blex.html HTTP/1.0\n  User-Agent: Mozilla/1.12 (X11; I; Linux 1.2.9 i486)\n  Referer: http://localhost/blah/blebber/wuxta.html\n  Accept: */*\n  Accept: image/gif\n  Accept: image/x-xbitmap\n  Accept: image/jpeg\n\n  ---------------------------------------------------\n\nEvery header in this message could potentially be reused in future\nrequests.  Only the `GET' line will always be different.\n\nI will use the following figures in the statistics below:\n\n- Without header reuse, the average request size is 200 bytes\n\n- With header reuse, the average request size is\n    - 200 bytes for the first request over a persistent connection\n    -  40 bytes for all subsequent requests over a persistent connection\n\n- The average size of the response headers is always 180 bytes.\n\n\n 4.2 RESULTS\n -----------\n\n 4.2.1 Size of HTTP traffic transmitted.\n\n                           in response  in\n                           bodies       headers   total \n    ---------------------+------------+---------+----------------\n\n    Without header reuse:     145 Mb     5.3 Mb   150.3 Mb (100.0%)\n    With header reuse:        145 Mb     3.3 Mb   148.3 Mb ( 98.7%)\n\n    Reuse saves:                         2.0 Mb            (  1.3%)\n\nCompared to other possible savings, 1.3% is too little to care about.\n\nBut traffic size counts are dominated by very large requests: maybe we\ncan get a noticeably faster response time on small requests?\n\n 4.2.2. Response time\n\nI use the following approximations for getting response time results:\n\n - The sequence of requests done over each persistent HTTP connection\n   is divided into `wait chains'.\n\n - Each subsequent request in a `wait chain' is no more than 20\n   seconds apart.\n\n - the idea is that the user does not perceive the speedup of\n   individual HTTP transactions in a `wait chain', but only the\n   average transaction speedup for the whole `wait chain'.\n\n - We want to determine the percentage of wait chains that get\n   noticeably faster after the introduction of header reuse.\n\n - We assume that for a wait chain to get noticeably faster, the\n   HTTP traffic size generated in that wait chain must decrease\n   with at least 10%.\n\nAmount of wait chains with a certain percentage of traffic decrease:\n\n           decrease %   amount\n           -----------+-------------\n                   0     1069    24%\n                 1-4     1763    39%\n                 5-9      926    21%\n               10-19      396     9%\n               20-49      278     6%\n               50-         70     2%\n\nThus, request header reuse will lead to a noticeable speedup for\n17% of all wait chains.\n\n\n\n 5. ALTERNATIVE 500 BYTE SCENARIO\n --------------------------------\n\nThe above statistics assume that \n\n- Without header reuse, the average request size is 200 bytes\n\n- With header reuse, the average request size is\n    - 200 bytes for the first request over a persistent connection\n    -  40 bytes for all subsequent requests over a persistent connection\n\nThe reasons for these assumptions are given in Section 3.\n\nOne could imagine an alternative scenario, in which we have a good (or\nbad) reason to make the requests much larger.  To see if introducing\nheader reuse is a good idea under such a scenario, I made the above\nstatistics again with the following assumptions:\n\n- Without header reuse, the average request size is 500 bytes\n\n- With header reuse, the average request size is\n    - 500 bytes for the first request over a persistent connection\n    -  40 bytes for all subsequent requests over a persistent connection\n\nThis gets us:\n\n 5.1.1 Size of HTTP traffic transmitted in 500 byte scenario\n\n                           in response  in\n                           bodies       headers   total \n    ---------------------+------------+---------+----------------\n    Without header reuse:     145 Mb     9.4 Mb   154.4 Mb (100.0%)\n    With header reuse:        145 Mb     3.7 Mb   148.7 Mb ( 96.3%)\n\n    Reuse saves:                         5.7 Mb            (  3.7%)\n\n\n 5.1.2 Response time in 500 byte scenario\n\nAmount of wait chains with a certain percentage of traffic decrease:\n\n           decrease %   amount\n           -----------+------------\n                   0     809    18%\n                 1-4     884    20%\n                 5-9     749    17%\n               10-19     980    22%\n               20-49     718    16%\n               50-       362     8%\n\nThus, request header reuse will lead to a noticeable speedup for 46% of\nall wait chains.\n\nI conclude that header reuse becomes moderately interesting _IF_ we\nfind a good reason use request messages which contain a large (>460\nbytes) amount of reusable headers.\n\n\n 5.1.2 Comparison between Section 4 and 500 byte scenario\n ---------------------------------------------------------\n\nTraffic generated:\n                                    in response  in\n                                    bodies       headers   \n    -------------------------------+------------+---------\n\n    Section 4 without header reuse:  145 Mb       5.3 Mb \n    Section 4 with header reuse:     145 Mb       3.3 Mb\n    500 byte without header reuse:   145 Mb       9.4 Mb\n    500 byte with header reuse:      145 Mb       3.7 Mb \n\n\nAmount of wait chains with a certain percentage of traffic decrease,\nwhen going from Section 4 _without_ reuse to 500 byte _with_ reuse:\n\n           decrease %   amount\n           -----------+------------\n               - -21     121     3%\n           -20 - -11     189     4%\n           -10 -  -6     198     4%\n            -5 -  -1     442    10%\n             0 -   4    2088    46%\n             5 -   9     784    17%\n            10 -  19     356     8%\n            20 -         324     7%\n\n(7% of wait chains get noticeably slower, 15% get noticeably faster)\n\n\n 6. RANDOM STATISTICS\n -------------------\n\nThe statistics below are not very relevant for deciding about reuse,\nbut they are nice to have anyway.\n\nAmount of proxy<->server responses with a certain response body size:\n\n    body size (bytes)  amount cumulative amount\n    ------------------+------+-----------------\n                0-99     4%     4%\n             100-199     4%     8%\n             200-499     8%    16%\n             500-999     9%    25%\n           1000-1999    19%    44%\n           2000-4999    25%    69%\n           5000-9999    16%    85%\n         10000-19999     7%    92%\n         20000-49999     6%    97%\n         50000-99999     2%    99%\n        100000-          1%   100%\n\n\nAmount of persistent proxy<->server connections over which a certain\nnumber of HTTP transactions are made (the connections have a timeout\nof 10 minutes):\n\n- on average, one persistent connection gets 9.2 transactions.\n\n    # of transactions   amount     cumulative amount\n    ------------------+-----------+-----------------         \n                   1    415   26%     26%\n                   2    214   14%     40%\n                   3    169   11%     50%\n                   4    118    7%     58%\n                 5-6    148    9%     67%\n                 7-9    134    8%     76%\n               10-19    198   12%     88%\n               20-49    139    9%     97%\n               50-       49    3%    100%\n\n\n(End of document.)\n\n\n\n", "id": "lists-010-6488176"}, {"subject": "Re: Sticky stuff", "content": "Paul Leach:\n>>From:  hallam@Etna.ai.mit.edu[SMTP:hallam@Etna.ai.mit.edu] \n  [...]\n>>2) I'm not sure of the amount that these proceedures buy us. It\n>>would be nice to have figures. \n\nI guess this is a good time for me to repost the figures I calculated\nalmost a year ago.  Look for a message `Statistics on reusing request\nheaders in persistent connections'.  The conclusion:\n\n My conclusion is that the gains are too small to bother about request\n header reuse at this point:\n      - HTTP traffic savings would be about 1.3%\n      - speedup of browsing response time would be minimal:\n        page+inline loading times would be noticeably faster in\n        about 17% of all cases.\n\n Much higher gain/effort ratios can be had by focusing on other\n desirable features of future HTTP software, for example [...]\n\nThe conclusion still seems to be valid.\n\n>>Jim G. has made good points about\n>>the importance of getting as much usefull information in\n>>the first packet send out (i.e. before we hit the slow start\n>>throttle). This mechanism appears to be aimed more at increasing\n>>the efficiency of later packets.\n>\n>There's not much that can be done about initial packets, unless somehow\n>the negotiation step can be skipped.\n\nIf you mean `unless somehow the Accept headers can be left out': you\ncan indeed leave them out under transparent content negotiation.  And\nin most requests, you will leave them out.  See Section 11.6\n(construction of short requests) in the conneg draft.  Does the\nrequest\n\n      GET /paper HTTP/1.1\n      Host: x.org\n      User-Agent: WuxtaWeb/2.4\n      Negotiate:\n\nfit into one packet?\n\nKoen.\n\n\n\n", "id": "lists-010-6509196"}, {"subject": "Re: Sticky stuff", "content": " \n>Actually, headers are the predominant source of data bytes flowing\n>from client to server (i.e., in the request direction), at least as\n>far as I am aware.  This is not such a significant fraction of the\n>bytes on the Internet backbone, perhaps, but when people are using\n>low-bandwidth links, the request-header-transmission delays\n>directly contribute to the user's perceived response time, and\n>reducing them would seem valuable.  Also, if the home market becomes\n>widely served by asymmetric-bandwidth systems (such as Hybrid\n>Network's product; see http://hybrid.com/), then request-header\n>bytes become proportionally more expensive.\n\nGranted, but is that the determining factor with respect to speed \nof response?\n\nBecause the number of bytes sent by the client to the server is a \nsmall proportion of the number comming the other way I'm \nwondering if we are optimising a feature that will not have\na significant impact.\n\nIf we have pipelining then the sequence of messages in time would \nbe something like:\n\n\nc->s Request#1\ns->c Reply-Headers#1 + 500bytes-entity#1\nc->s Request#2} Overlapping\ns->c 700bytes-entity#1}\nc->s Request#3\ns->c 700bytes-entity#1\ns->c Reply-Headers#2 + entity#2\ns->c Reply-Headers#3 + entity#3\n\nIn other words I don't think that the client headers are currently \non the critical path. \n\nAsymmetric bandwidth changes this but only slightly. Consider the\ntwo main asymmetric supply routes, Satelite and Cable. For satelite\nthere is a latency issue. The content arrives after a lag. I have \nto construct the Request#2 frame after I have started recieiving\nthe first entity body because I need to see the <IMG> tag before\nI know what to load. Now Request #2 will only be sent faster if the\ncompression is going to bring it bellow the IP frame size. \n\n[First piece of data required, plot of message size vs time to\ncomplete  transmission. I suspect that this has the following\nform (approximating the slow start factor for the moment) :\n\nt = a*[s/p] + b*s + c\n\nWhere:\n\ns = Size of the message\np = the packet size\n\na = routing delay parameter\nb = transmission bandwidth\nc = constant term dependent on connection establishment delay.\n[Slow start could be modelled by making this term\nmore complex]\n\nMy guess is that for all but connections from a dialup client to\na server on the dialup host that the a and c factors are the\ndominant terms. Ie compressing message size matters little unless\nyou can save a packet. That at least was the operating assumption\nfor HTTP in the early days. \n\nI haven't measured these recently and I think that empirical\nmeasurements of these parameters would be a very good thing\nto have if we are going to try optimisation.]\n\n\n>    3) Section 2.2 asserts that proxies typically multiplex server\n>    connections across multiple clients. Is this in fact the case?\n>\n>(Almost?) nobody yet uses proxies that support persistent connections.\n>So it's hard to provide data from experience.  At least, I have not\n>seen any.\n\nMy impression as well. I think that the idea that HTTP messages \nshould be interleavable in this manner is a very bad idea. It\nis a very large cost overhead for not such a great return - unless \nits done to prevent abusive clients.\n\nI would like to suggest that we consider making HTTP a non-idempotent\nprotocol and introduce methods to provide transaction semantics.\nE.g. :-\n\nSTART - Begin a transaction operation \nCOMMIT- Commit a transaction\nROLLBACK- Undo a transaction\n\nAlternatively we could provide a LOCK method which would acquire\na lock on a resource. There are obvious semantics that can then \nbe attached to connections - loose the connection before the \nCommit is recieved and you do a rollback. \n\nQuibbles about whether a client knows about whether a \nconnection has completed or not are not particularly relevant.\nEither the client can reconnect to the server and ask if a\ntransaction completed or the client is never going to find\nout - tough!\n\nThere are many databases that provide these facilities and \neven operating systems that provide them as low level \nprimitives. I think we should exploit them where possible.\n\nSo as I say I don't accept that we should allow the hypothetical\nmultiplexing proxy to limit the protocol. I think that it is \na corner optimisation that gives practically no benefit. It\nmakes significant restrictions on the future direction of HTTP.\n\nThe only place where I do see an argument for supporting \na muxing type proxy is that it might be handy for a proxy to be\nable to do connection reuse in the same way that clients\nroutinely do ftp and news connection reuse. this is easier\nto support because it simply requires a facility to flush the\nstate of the connection in its entirety. This is much easier to \nsupport than the synchronous case.\n\n>How so? A second context is created just like the first one, with the\n>addition of specifying the context number.\n\nIts easy for the client to support but much harder for the server\nwhich has to keep track of multiple sessions and match them in\nan efficient manner.\n\n>I don't think so -- the authentication should be applied before the\n>sticky compression. Maybe I just don't understand this point.\n\nThe point is that if the authentication is going to protect against\na replay attack it has to be dependent on the message and vary with \neach message and thus one has to send out a new one with each message.\n\n\nPhill\n\n\n\n", "id": "lists-010-6518644"}, {"subject": "Re: Sticky stuff", "content": ">     Overall I would like to see an awfull lot of numbers based on \n>     empirical measurement before deciding whether this is a \n>     worthwhile scheme or not. Although it looks OK to me I know\n>     from experience that without hard numbers it is very easy to\n>     overoptimise corner cases that almost never occur.\n>     \n> I think it would be nice to get a trace of the actual bytes\n> carried by a real proxy (not just the URLs), and apply the\n> proposed compression schemes to see how successful they are.\n> Of course, this would have to be done carefully to avoid\n> breaching the privacy of the proxy's users.\n\nYes, and I'd also like to see it compared to a complete tokenization\nof the protocol (also just a translation table to implement) and the\nperceived benefits as compared to a multiplexing of requests on a\nsingle connection. I doubt that there will be any perceptable performance\nimprovement when using modern user agents (ones without 1k worth of\nAccept headers) which do not generate GET requests larger than 2 packets,\nsince the multi-connection overhead is worse for any congested network.\nBut, I would like that confirmed (or even better, disproven) by actual\nperformance data over real TCP connections, and I have no time to do that.\n\nThe feedback I received while gathering design issues for HTTP/1.x\nis that no piddly performance improvements are worthwhile because\nof the added complexity to implementations.  The only changes that\nwere considered worthwhile were, in order: persistent connections\n(mostly to enable continuous connection to an organization's proxy),\nmultiplexing (mostly to enable non-jerky rendering of the visible\npage without opening extra connections), and tokenization of both\nheader fields and their contents (to reduce bandwidth consumption).\n\nThe design of HTTP/1.1 is purposely geared towards tokenization -- that\nwas the plan all along; once the semantics of HTTP were well-defined, it\nwould be very easy to make an extremely efficient HTTP/2.0.  Anything\nless is, IMHO, a waste of time.  I don't mind if people want to waste\ntheir own time, but I do mind when it is done under the banner of\nproposed standard (which ends up wasting everyone's time even when the\nidea is a great one). \n\nI don't see why this can't be done as an experiment first -- I created\nthe Upgrade header field specifically to allow such experiments to\noccur *outside* the standardization arena. Right now we are arguing over\nthe wording of a proposal which hasn't even been determined to do anything\nuseful, let alone something that should be standardized.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-6531387"}, {"subject": "Re: Sticky headers and pipelining (was: Sticky header draft &ndash;&ndash; as an attachment", "content": ">>BTW, while we are on the topic, I would prefer that the two unrelated\n>>concepts of sticky headers and short header names be in two separate\n>>drafts.  They should be evaluated independently.\n> \n> I thought about that. I agree that they should be evaluated separately,\n> and can be adopted independently. However, if both are adopted, using\n> one mechanism (Connection: sticky) to say that you're using both saves\n> some bytes on the wire. Even if sticky headers are in use, the use of\n> abbreviations is not required, so a client that only wants to do sticky\n> is not forced to do extra work. The only drawback I can see is if the\n> client wants to do abbreviations but not do sticky headers. I don't know\n> if that is likely.\n> \n> Unless there is some reason to believe that one will fly and the other\n> one won't, then I'd personally avoid the overhead of a separate draft.\n\nOkay.  Sticky headers adds statefulness to a protocol that is otherwise\nstateless.  This means that protocol applications must be aware of things\nlike how much state is being stored, whether or not to allow it to be\nstored, when to tell the client to pissoff because they have asked for\ntoo much, how to tell the client to pissoff given the possibility of\npipelined requests, how to avoid introducing security holes in the process,\nand how to do all the above in a way that is so efficient that it is\nworthwhile even though the first two requests are unlikely to gain from\nstickyness (due to the normal request profile of text, image, ...).\nAs such, I am adamantly opposed to the whole notion and have no intention\nof implementing it.\n\nCompressed header names (or, more accurately, tokenized header names) are\njust a weak form of a fully tokenized protocol.  Full tokenization is the\nway to go, and though I dislike halfway methods which will make\ninterpretation of things like the Connection header field ambiguous\n(a list of header field names to remove from the message), I do not\nadamantly oppose it.\n\nI'm not sure if those are good enough reasons to split the draft -- I am\npainfully aware of the editorial complications that arise when working\non more than one spec at the same time -- but I felt I should suggest it.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-6541675"}, {"subject": "Re: Statistics on reusing request headers in persistent connections (repost", "content": "> When sending HTTP request over a persistent (keep-alive) HTTP\n> connection, it would be possible to re-use request headers from\n> earlier requests in subsequent requests.  For example, if the\n> User-agent header for requests n and n+1 are the same, there would be\n> no need to send the header twice, a special request header (using less\n> bytes) could indicate that the User-agent header is to be reused.\n> \n> Roy Fielding recently proposed a mechanism allowing such reuse.  The\n> question is whether designing and implementing such a mechanism would\n> be a good move.\n\nI most certainly did not propose such a thing -- all I did was point\nout an alternative indicator for part of Paul's mechanism that could\nnot have worked, and said it was yucky at that.\n\nThe original idea came from Alex Hopmann and was improved upon in\nPaul's draft, neither of which involved my support (or even review).\n\nHowever, I do agree with your conclusions.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-6552040"}, {"subject": "Re: Call for compatibility tester", "content": "> The transparent content negotiation draft\n> (http://gewis.win.tue.nl/~koen/conneg/) currently specifies the 300\n> response code for list responses.  \n> \n> It turns out that this is not compatible with several existing\n> browsers: lynx and some versions of Mosaic fail to display anything if\n> they get back a 3xx class response without a Location header.  As lynx\n> in particular will be important for some people who want to offer\n> negotiated material, this rules out use of the 300 response code in\n> transparent content negotiation.\n\nRead the description of 300 in the HTTP/1.1 spec:\n\n    10.3.1 300 Multiple Choices\n \n    The requested resource corresponds to any one of a set of\n    representations, each with its own specific location, and agent-driven\n    negotiation information (section 12) is being provided so that the user\n    (or user agent) can select a preferred representation and redirect its\n    request to that location.\n \n    Unless it was a HEAD request, the response SHOULD include an entity\n    containing a list of resource characteristics and location(s) from which\n    the user or user agent can choose the one most appropriate. The entity\n    format is specified by the media type given in the Content-Type header\n    field. Depending upon the format and the capabilities of the user agent,\n    selection of the most appropriate choice may be performed automatically.\n    However, this specification does not define any standard for such\n    automatic selection.\n \n    If the server has a preferred choice of representation, it SHOULD\n    include the specific URL for that representation in the Location field;\n    user agents MAY use the Location field value for automatic redirection.\n    This response is cachable unless indicated otherwise.\n\nSo, please explain why you are sending a 3xx class response without a\nLocation header.\n\n> I have done some checking, and it seems that the creation of a new\n> response code:\n> \n>   416 List Response\n> \n> is the best way to get downwards compatibility.\n\nAnd is out of the question.  Bugwards compatibility is achieved by\nlooking at the User-Agent value, and sending 406 (or just 200 with\nan appropriate Vary and Alternates) is better than generating a\nsuccess message masked as a client error.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-6561619"}, {"subject": "RE: Sticky stuff", "content": "There were several problems with your analysis, some of them pointed out\nat the time it was first posted.\n\n1. It didn't include the effects of Accept-Language, Accept-Encoding,\nAccept-Charset, or From (or Host, but it didn't exist at the time).\nThese can add to the average request size. So could future headers added\nto HTTP.\n\n2. It was taken between proxy and server, where the effect of 304 (Not\nModified) is not seen. (The low % in headers in your because the average\nresponse entity-body was ~10,000 bytes. In a 304 response, the\nentity-body size is 0, so the savings in request header size is more\nimprtant). Between the user and the proxy, there could be a significant\nnumber of 304s. As user-agent caches get bigger, there would be even\nmore 304s (everything else being equal).\n\n3. It didn't consider asymmetric bandwidth situations. In the limit that\nthe downstream connection is infinite in speed and with low latency, the\nsavings of 80% in request header size that your study used would be\nsignificant. (The most likely early deployment of cable modems will use\nordinary telephone lines as the request channel. Phill points out that\nif other latencies are significant, then header size savings won't\nmatter much -- but these kind of cable modems are pretty close to the\ninfinite speed, low latency model.)\n\nI think a new study would be needed to take these effects into account\nbefore we can conclude that sticky headers aren't worth the effrort.\n\n>----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Sent: Thursday, August 08, 1996 4:12 PM\n>To: Paul Leach\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; hallam@Etna.ai.mit.edu\n>Subject: Re: Sticky stuff.\n>\n>Paul Leach:\n>>>From:  hallam@Etna.ai.mit.edu[SMTP:hallam@Etna.ai.mit.edu] \n>  [...]\n>>>2) I'm not sure of the amount that these proceedures buy us. It\n>>>would be nice to have figures. \n>\n>I guess this is a good time for me to repost the figures I calculated\n>almost a year ago.  Look for a message `Statistics on reusing request\n>headers in persistent connections'.  The conclusion:\n>\n> My conclusion is that the gains are too small to bother about request\n> header reuse at this point:\n>      - HTTP traffic savings would be about 1.3%\n>      - speedup of browsing response time would be minimal:\n>        page+inline loading times would be noticeably faster in\n>        about 17% of all cases.\n>\n> Much higher gain/effort ratios can be had by focusing on other\n> desirable features of future HTTP software, for example [...]\n>\n>The conclusion still seems to be valid.\n>\n>>>Jim G. has made good points about\n>>>the importance of getting as much usefull information in\n>>>the first packet send out (i.e. before we hit the slow start\n>>>throttle). This mechanism appears to be aimed more at increasing\n>>>the efficiency of later packets.\n>>\n>>There's not much that can be done about initial packets, unless somehow\n>>the negotiation step can be skipped.\n>\n>If you mean `unless somehow the Accept headers can be left out': you\n>can indeed leave them out under transparent content negotiation.  And\n>in most requests, you will leave them out.  See Section 11.6\n>(construction of short requests) in the conneg draft.  Does the\n>request\n>\n>      GET /paper HTTP/1.1\n>      Host: x.org\n>      User-Agent: WuxtaWeb/2.4\n>      Negotiate:\n>\n>fit into one packet?\n>\n>Koen.\n>\n\n\n\n", "id": "lists-010-6572007"}, {"subject": "Re: Sticky stuff", "content": ">>    3) Section 2.2 asserts that proxies typically multiplex server\n>>    connections across multiple clients. Is this in fact the case?\n>>\n>>(Almost?) nobody yet uses proxies that support persistent connections.\n>>So it's hard to provide data from experience.  At least, I have not\n>>seen any.\n> \n> My impression as well. I think that the idea that HTTP messages \n> should be interleavable in this manner is a very bad idea. It\n> is a very large cost overhead for not such a great return - unless \n> its done to prevent abusive clients.\n\nThe normal operation for a group-to-organizational proxy or a\nregional-to-national proxy will be one of interleaved requests on\na small number of persistent connections, with the possiblility of\nhigh performance requirements on those connections due to the bursty\nnature of web traffic.  I do not consider that to be \"typical\",\nbut it is part of the design of HTTP/1.1 and cannot be ignored without\nbreaking the protocol.  Such applications are not typical today, but\nthey will be typical in the future (the near future given the rate\nat which demand is approaching network bandwidth capacity).\n\nIt will happen as soon as the performance of an HTTP caching proxy\nexceeds that of using the network without a caching proxy. This is\nalready the case in most areas outside the US.\n\n> I would like to suggest that we consider making HTTP a non-idempotent\n> protocol and introduce methods to provide transaction semantics.\n\nHTTP is already non-idempotent (POST). Do you mean stateful?\n\n> E.g. :-\n> \n> START - Begin a transaction operation \n> COMMIT- Commit a transaction\n> ROLLBACK- Undo a transaction\n\nThose are resource transactions and do not require a stateful protocol\n(only stateful resources, which we already have). I do expect that they\nwill be necessary for eventual support of versioning and real distributed\nauthoring, though loss of the connection would not imply ROLLBACK -- it\nwould imply no COMMIT until the sequence is completed (and yes, each such\nrequest would have to carry a sequence number or the whole thing won't work).\nHowever, this is no longer relevant to the subject under discussion.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-6586307"}, {"subject": "Re: useful document identification (was encouraging....", "content": "I suggested using \"Content-ID\" as a globally unique\noriginator-supplied identifier. You replied in one part that \n\n\"this cannot be used in a secure and verifiable fashion\"\n\nwhich I interpret as implying that you'd somehow like the recipient to\nbe able to verify that the content actually corresponds to the\nidentifier, without having to take the word of the supplier of the\ncontent.\n\nBut later, you argued for content identifiers to have domains, rather\nthan global scope. I don't see why anyone would bother providing\nsomething that was both secure and verifiable and didn't have global\nuniqueness.\n\nMaybe you meant something else by \"the scope of the\ncontent-identifier\"?\n\nLarry\n\n\n\n", "id": "lists-010-6596133"}, {"subject": "Re: Call for compatibility tester", "content": "Roy T. Fielding:\n>\n>Read the description of 300 in the HTTP/1.1 spec:\n>\n>    10.3.1 300 Multiple Choices\n  [...]\n>    If the server has a preferred choice of representation, it SHOULD\n>    include the specific URL for that representation in the Location field;\n>    user agents MAY use the Location field value for automatic redirection.\n>    This response is cachable unless indicated otherwise.\n>\n>So, please explain why you are sending a 3xx class response without a\n>Location header.\n\nI'm sending it if the server DOES NOT have a preferred choice of\nrepresentation.\n\nSuppose I have a 5 minute movie in 3 different file formats.  Then I\ndon't want to have user agents not capable of transparent content\nnegotiation automatically retrieve one movie file.  I want to show a\nHTML list to the user.\n\n>> I have done some checking, and it seems that the creation of a new\n>> response code:\n>> \n>>   416 List Response\n>> \n>> is the best way to get downwards compatibility.\n>\n>And is out of the question.  Bugwards compatibility is achieved by\n>looking at the User-Agent value,  and sending 406 (or just 200 with\n>an appropriate Vary and Alternates) is better than generating a\n>success message masked as a client error.\n\nOK, you don't like 416.  But I'd rather be compatible without\nuser-agent tricks, because user-agent tricks increase the cost of\nminimal implementations and make caching *much* more difficult.\n\nUnder the current spec, an origin server can specify conditions under\nwhich a proxy can send a cached list response to a 1.0 user agent.\nThis seems to be important for efficiency and scalability, but it\ndepends on cached list responses having a format which all 1.0 user\nagents can handle.  That is why I'm seeking an alternative for using\nthe 300 code.\n\nI can think of a number of alternatives to using 416:\n\n1) Create a 2xx class response code for list responses\n\n2) Define a list response as `a response which has an Alternates\n   header but no Content-Location header'.  This allows you to pick\n   whichever status code you like, for example 302 if there is a\n   preferred choice, 200 if there is no preferred choice.  Or 300 if\n   you know the \n\nWhich one do you prefer?  All could be combined with language that\nallows you to send 300, but discourages it if you suspect that there\nmay be incompatible 1.0 clients on the other end of the cache.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-6604459"}, {"subject": "Re: Call for compatibility tester", "content": "Alexei Kosut:\n>\n>Okay... let me ask a silly question. Having read the various versions\n>of Koen's content negotiation drafts, I am at a loss to understand why\n>a server would send a 300 response to a user agent that did not\n>support it. \n\nYou would send a list response to a user agent not supporting\ntransparent content negotiation if you want the user to manually pick\nthe best variant.\n\nAn example would be if you have a 5 minute movie in 3 different file\nformats, or an executable for 5 different platforms.  \n\nBasically, whenever you put a `click here for the X version, click\nhere for the Y version' list on a html page now, you will want to send\na list response to un-negotiating clients (which might not support\n300) in future.  So these must be a way to send a list response\nwithout using the 300 code.\n\nOf course, for things like inline images, you would never send a list\nresponse, you just send a choice response with a .gif of something.\n\n[...]\n>If I was writing a server to implement transparent negotiation, I\n>would only send a 300 response if the user agent sent\n>Negotiate:. Otherwise, I would make the choice on behalf of the user\n>agent (based on the Accept: headers or some other indication), and\n>send that document.\n\nMy point is that you do not always want to make a choice.  But you do\nwant to get rid if the `click here for the X version, click here for\nthe Y version' stuff on your main pages.\n\n[...]\n>All of this combines to convince me that the intentions of the spec\n>(although it may not be explicitly stated - though it should be) are\n>that unless the Negotiate: header is sent, the server should never\n>send a 300 response, but should choose on behalf of the user agent,\n>and send the chosen resource.\n\nIf no Negotiate: is sent, choosing on behalf on the user agent will\nindeed be the normal case.\n\nBut the spec does recognize that there are situations in which you do\nnot want to return a list response, or have the proxy return a cached\nlist response on your behalf. (I don't know if the spec is explicit\nenough about this: you can see it in section 11.2, but that is a\nlittle late in the spec.)\n\nBy the way, I put mechanisms in the spec for letting proxies deal with\nnon-negotiating user agents, (like the min-q and forward attributes,\nsee Section 11.5.1), but I don't know if they do everything people\nwould want.  This is one area that needs careful review, in particular\nby people who use server-driven negotiation now.  My own experience in\nthis area is limited.\n\nOne thing is sure though: no mechanism for letting proxies deal with\nnon-negotiating user agents can be perfect, because you basically\nrevert to Accept header based server-driven negotiation, with all the\nproblems of not getting enough information in the Accept headers.\n\n>Alexei Kosut <akosut@nueva.pvt.k12.ca.us>      The Apache HTTP Server\n\nKoen.\n\n\n\n", "id": "lists-010-6614735"}, {"subject": "Re: Statistics on reusing request headers in persistent connections (repost", "content": "Roy T. Fielding:\n>\n   [Koen Holtman:]\n>> Roy Fielding recently proposed a mechanism allowing such reuse. The\n[...]\n>I most certainly did not propose such a thing \n\nThe `recently' above was written on Oct 31, 1995, and probably refers\nto http-wg mailing list messages posted in that time frame.  I did not\nknow this attribution was in the reposted message, and would have\nremoved it if I did.\n\nI apologize for any confusion caused.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-6625479"}, {"subject": "RE: useful document identification (was encouraging....", "content": "assuming that a hash-scheme is not feasible...for reasons unknown to\nme...\n...a scheme which uses trusted-hosts would require scope.....\n\ngranted a hashing algorithm is the most desirable solution.....\n\n\n>----------\n>From: Larry Masinter[SMTP:masinter@parc.xerox.com]\n>Sent: Friday, August 09, 1996 2:38 AM\n>To: Erik Aronesty\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: useful document identification (was encouraging....)\n>\n>I suggested using \"Content-ID\" as a globally unique\n>originator-supplied identifier. You replied in one part that \n>\n>\"this cannot be used in a secure and verifiable fashion\"\n>\n>which I interpret as implying that you'd somehow like the recipient to\n>be able to verify that the content actually corresponds to the\n>identifier, without having to take the word of the supplier of the\n>content.\n>\n>But later, you argued for content identifiers to have domains, rather\n>than global scope. I don't see why anyone would bother providing\n>something that was both secure and verifiable and didn't have global\n>uniqueness.\n>\n>Maybe you meant something else by \"the scope of the\n>content-identifier\"?\n>\n>Larry\n>\n>\n>\n>\n\n\n\n", "id": "lists-010-6633817"}, {"subject": "Next draft of HTTP/1.1 spec..", "content": "There have been a number of issues on the mailing list\nin the last month that have accumulated and make it worth\nanother draft; they are all minor, and the area directors\n(who have already seen this list) agree that none of these\nchanges are significant, and that issuing another draft will \ntherefore not delay such processing HTTP/1.1 by the IESG.\nThe IESG is happiest to approve things with zero changes \nfrom last draft to RFC form, so issuing a draft 07 is the right\nthing to do.\n\nSo that you can verify what changes I intend to make (and\nlet me know if Larry and I have missed anything in the mailing\nlist traffic that should be changed), what I currently intend\nto change are all below, along with the names of people who\nraised the issue.  If I've missed something, and you let me know\nby the end of Friday, I'll do what I can to deal with it.\nI'll make an annoucement when I've submitted draft 07, probably\non Monday.\n\nBelow are two items:\n1) an explanation of each change to Draft 06 I believe should be \nreflected in the RFC. I intend submit an updated draft 07 by Monday, \nAugust 12.\n2) context diffs of these changes to the text version of Draft 06 \nitself.\nAs you can see by the explanations below, the changes are all minor.\n- Jim Gettys\n\n\n\n=====================\nExplanation of changes:\n\n\nRoss Patterson, Roy Fielding,  Section 3.3.1\nAdded missing reference to RFC 850\n\nRonald Tschalaer, Section 8.2 Message Transmission Requirements\n\nAdded the words \"to this HTTP/1.0 server\" in the paragraph preceding \nthe binary exponential backoff algorithm, to clarify that this \nalgorithm only applies to HTTP/1.1 clients talking to HTTP/1.0 \nservers, which cannot otherwise reliably recieve a large entity \n(a known problem with HTTP/1.0 today with Post operations).\nThis resolves the appearent conflict with language above this, \non retry after connections are aborted, for 1.1 clients talking to \n1.1 servers.\n\nRoss Patterson, Roy Fielding,  Section 8.8.1 Purpose and Section 17 References\nSeparated references.\n\nAnselm Baird - Section 13.1.4 Explicit User Agent Warnings\nExample was incorrect; removed `Cache-Control:reload' example, as \nthere is no such directive in final HTTP/1.1\n\nRoss Patterson, Paul Hethmod, Section 14.9 Cache-Control\nBNF had minor error that would have required an '=' even if \ndelta-seconds not specified, so changed the line:\n          '| \"max-stale\" \"=\" [ delta-seconds ]'\nto\n          '| \"max-stale\" [ \"=\" delta-seconds ]'\nto correct this; this was my editing mistake in a previous draft \nwhen applying changes circulated to the working group list, where\nthe changes were applied incorrectly in this area.\nAlso in section 14.9 there was an excess '|' in the BNF\n        so the obvious fix is to change:\n cache-request-directive =\n                        | \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\nTo:\n cache-request-directive =\n                         \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n\nRoss Patterson, 14.15 Content-Location\nTypo of extra period in explanitory text.\n\nPaul Hethmon, Section 14.17, Content-Range\nThe example was incorrect in Content-Range, lacking the \nbytes-unit specifier.\n\nPaul Hethmon, Section 14.25 If-Match\nExample was incorrect; removed use of weak validators, since that is \nnot legal with If-Match.\n\nJeff Mogul, to clarify confusion on Vary - Section 14.43 \nAdded sentence \"Field-names listed in Vary headers are those of \nrequest-headers.\"\n\nRoss Patterson, 14.45 Warning\nTypographical error, spec said ISO-8599-1 where it meant ISO-8859-1 \n(2 occurances in this section; the document elsewhere stated 8859 \ncorrectly).\n\nRoss Patterson, Roy Fielding, 15.8 DNS Spoofing\nRemoved reference to DNSSEC work, as it isn't referenceable at this \ntime.\n\nV. Padmanabhan, Jeff Mogul, Section 17 References\nReference 26 was not the best reference to the work.  \nReplaced with correctedreference.\n\nRoss Patterson, Roy Fielding, 19.6.2.4\nAdded missing reference to HTML.\n\nThere was also a suggestion to alphebetize the references section, and\nrenumber.  I do not think this is worth doing for Proposed, but I will\ndo so for Draft standard.\n\n================\nContext Diffs\n================\n*** draft-ietf-http-v11-spec-06.txtTue Jul  9 14:41:17 1996\n--- draft-ietf-http-v11-spec-07.txtWed Aug  7 22:27:54 1996\n***************\n*** 1151,1157 ****\n  The first format is preferred as an Internet standard and represents a\n  fixed-length subset of that defined by RFC 1123  (an update to RFC 822).\n  The second format is in common use, but is based on the obsolete RFC\n! 850  date format and lacks a four-digit year. HTTP/1.1 clients and\n  servers that parse the date value MUST accept all three formats (for\n  compatibility with HTTP/1.0), though they MUST only generate the RFC\n  1123 format for representing HTTP-date values in header fields.\n--- 1151,1157 ----\n  The first format is preferred as an Internet standard and represents a\n  fixed-length subset of that defined by RFC 1123  (an update to RFC 822).\n  The second format is in common use, but is based on the obsolete RFC\n! 850 [12] date format and lacks a four-digit year. HTTP/1.1 clients and\n  servers that parse the date value MUST accept all three formats (for\n  compatibility with HTTP/1.0), though they MUST only generate the RFC\n  1123 format for representing HTTP-date values in header fields.\n***************\n*** 2355,2361 ****\n  causing congestion on the Internet. The use of inline images and other\n  associated data often requires a client to make multiple requests of the\n  same server in a short amount of time. Analyses of these performance\n! problems are available [30]; analysis and results from a prototype\n  implementation are in [26].\n  \n  Persistent HTTP connections have a number of advantages:\n--- 2355,2361 ----\n  causing congestion on the Internet. The use of inline images and other\n  associated data often requires a client to make multiple requests of the\n  same server in a short amount of time. Analyses of these performance\n! problems are available [30][27]; analysis and results from a prototype\n  implementation are in [26].\n  \n  Persistent HTTP connections have a number of advantages:\n***************\n*** 2578,2585 ****\n  older and will not use the 100 (Continue) response. If in this case the\n  client sees the connection close before receiving any status from the\n  server, the client SHOULD retry the request. If the client does retry\n! the request, it should use the following \"binary exponential backoff\"\n! algorithm to be assured of obtaining a reliable response:\n  \n    1. Initiate a new connection to the server\n  \n--- 2578,2585 ----\n  older and will not use the 100 (Continue) response. If in this case the\n  client sees the connection close before receiving any status from the\n  server, the client SHOULD retry the request. If the client does retry\n! the request to this HTTP/1.0 server, it should use the following \"binary\n! exponential backoff\" algorithm to be assured of obtaining a reliable response:\n  \n    1. Initiate a new connection to the server\n  \n***************\n*** 4031,4037 ****\n  caching mechanisms. For example, the user agent may allow the user to\n  specify that cached entities (even explicitly stale ones) are never\n  validated. Or the user agent might habitually add \"Cache-Control: max-\n! stale=3600\" or \"Cache-Control: reload\" to every request. The user should\n  have to explicitly request either non-transparent behavior, or behavior\n  that results in abnormally ineffective caching.\n  \n--- 4031,4037 ----\n  caching mechanisms. For example, the user agent may allow the user to\n  specify that cached entities (even explicitly stale ones) are never\n  validated. Or the user agent might habitually add \"Cache-Control: max-\n! stale=3600\" to every request. The user should\n  have to explicitly request either non-transparent behavior, or behavior\n  that results in abnormally ineffective caching.\n  \n***************\n*** 5502,5511 ****\n                         | cache-response-directive\n  \n         cache-request-directive =\n!                        | \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n                         | \"no-store\"\n                         | \"max-age\" \"=\" delta-seconds\n!                        | \"max-stale\" \"=\" [ delta-seconds ]\n                         | \"min-fresh\" \"=\" delta-seconds\n                         | \"only-if-cached\"\n                         | cache-extension\n--- 5502,5511 ----\n                         | cache-response-directive\n  \n         cache-request-directive =\n!                          \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n                         | \"no-store\"\n                         | \"max-age\" \"=\" delta-seconds\n!                        | \"max-stale\" [ \"=\" delta-seconds ]\n                         | \"min-fresh\" \"=\" delta-seconds\n                         | \"only-if-cached\"\n                         | cache-extension\n***************\n*** 6078,6084 ****\n  Location also defines the base URL for the entity (see section 14.11).\n  \n  The Content-Location value is not a replacement for the original\n! requested URI; it is only a statement. of the location of the resource\n  corresponding to this particular entity at the time of the request.\n  Future requests MAY use the Content-Location URI if the desire is to\n  identify the source of that particular entity.\n--- 6078,6084 ----\n  Location also defines the base URL for the entity (see section 14.11).\n  \n  The Content-Location value is not a replacement for the original\n! requested URI; it is only a statement of the location of the resource\n  corresponding to this particular entity at the time of the request.\n  Future requests MAY use the Content-Location URI if the desire is to\n  identify the source of that particular entity.\n***************\n*** 6233,6239 ****\n         HTTP/1.1 206 Partial content\n         Date: Wed, 15 Nov 1995 06:25:24 GMT\n         Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n!        Content-Range: 21010-47021/47022\n         Content-Length: 26012\n         Content-Type: image/gif\n  \n--- 6233,6239 ----\n         HTTP/1.1 206 Partial content\n         Date: Wed, 15 Nov 1995 06:25:24 GMT\n         Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n!        Content-Range: bytes 21010-47021/47022\n         Content-Length: 26012\n         Content-Type: image/gif\n  \n***************\n*** 6573,6581 ****\n  resource has been changed without their knowledge. Examples:\n  \n         If-Match: \"xyzzy\"\n-        If-Match: W/\"xyzzy\"\n         If-Match: \"xyzzy\", \"r2d2xxxx\", \"c3piozzzz\"\n-        If-Match: W/\"xyzzy\", W/\"r2d2xxxx\", W/\"c3piozzzz\"\n         If-Match: *\n  \n  \n--- 6573,6579 ----\n***************\n*** 6612,6617 ****\n--- 6610,6616 ----\n  \n  \n  \n+ \n  Fielding, et al                                   [Page 114]\n  \n  \n***************\n*** 7240,7246 ****\n  \n  The Vary response-header field is used by a server to signal that the\n  response entity was selected from the available representations of the\n! response using server-driven negotiation (section 12). The Vary field\n  value indicates either that the given set of header fields encompass the\n  dimensions over which the representation might vary, or that the\n  dimensions of variance are unspecified (\"*\") and thus may vary over any\n--- 7239,7246 ----\n  \n  The Vary response-header field is used by a server to signal that the\n  response entity was selected from the available representations of the\n! response using server-driven negotiation (section 12). Field-names listed\n! in Vary headers are those of request-headers. The Vary field\n  value indicates either that the given set of header fields encompass the\n  dimensions over which the representation might vary, or that the\n  dimensions of variance are unspecified (\"*\") and thus may vary over any\n***************\n*** 7249,7255 ****\n         Vary  = \"Vary\" \":\" ( \"*\" | 1#field-name )\n  \n  \n- \n  Fielding, et al                                   [Page 125]\n  \n  \n--- 7249,7254 ----\n***************\n*** 7422,7428 ****\n  This decision may be based on any available knowledge, such as the\n  location of the cache or user, the Accept-Language field in a request,\n  the Content-Language field in a response, etc. The default language is\n! English and the default character set is ISO-8599-1.\n  \n  Fielding, et al                                   [Page 128]\n  \n--- 7421,7427 ----\n  This decision may be based on any available knowledge, such as the\n  location of the cache or user, the Accept-Language field in a request,\n  the Content-Language field in a response, etc. The default language is\n! English and the default character set is ISO-8859-1.\n  \n  Fielding, et al                                   [Page 128]\n  \n***************\n*** 7430,7436 ****\n  INTERNET-DRAFT            HTTP/1.1     Monday, July 08, 1996\n  \n  \n! If a character set other than ISO-8599-1 is used, it MUST be encoded in\n  the warn-text using the method described in RFC 1522 [14].\n  \n  Any server or cache may add Warning headers to a response. New Warning\n--- 7429,7435 ----\n  INTERNET-DRAFT            HTTP/1.1     Monday, July 08, 1996\n  \n  \n! If a character set other than ISO-8859-1 is used, it MUST be encoded in\n  the warn-text using the method described in RFC 1522 [14].\n  \n  Any server or cache may add Warning headers to a response. New Warning\n***************\n*** 7762,7771 ****\n  \n  Clients using HTTP rely heavily on the Domain Name Service, and are thus\n  generally prone to security attacks based on the deliberate mis-\n! association of IP addresses and DNS names. The deployment of DNSSEC\n! should help this situation. In advance of this deployment, however,\n! clients need to be cautious in assuming the continuing validity of an IP\n! number/DNS name association.\n  \n  In particular, HTTP clients SHOULD rely on their name resolver for\n  confirmation of an IP number/DNS name association, rather than caching\n--- 7761,7768 ----\n  \n  Clients using HTTP rely heavily on the Domain Name Service, and are thus\n  generally prone to security attacks based on the deliberate mis-\n! association of IP addresses and DNS names. Clients need to be cautious\n! in assuming the continuing validity of an IP number/DNS name association.\n  \n  In particular, HTTP clients SHOULD rely on their name resolver for\n  confirmation of an IP number/DNS name association, rather than caching\n***************\n*** 7772,7777 ****\n--- 7769,7776 ----\n  the result of previous host name lookups. Many platforms already can\n  cache host name lookups locally when appropriate, and they SHOULD be\n  \n+ \n+ \n  Fielding, et al                                   [Page 134]\n  \n  \n***************\n*** 8016,8029 ****\n  [25]    P. Deutsch, \"GZIP file format specification version 4.3.\" RFC\n    1952, Aladdin Enterprises, May, 1996.\n  \n! [26]    Jeffrey C. Mogul. \"The Case for Persistent-Connection HTTP\". In\n!   Proc. SIGCOMM '95 Symposium on Communications Architectures and\n!   Protocols, pages 299-313. Cambridge, MA, August, 1995. A longer, more\n!   comprehensive version of this paper is available on line at\n!   <http://www.research.digital.com/wrl/techreports/abstracts/95.4.html>,\n!   Digital Equipment Corporation Western Research Laboratory Research\n!   Report 95/4, May, 1995.,\n  \n  [28]    Mills, D, \"Network Time Protocol, Version 3.\", Specification,\n    Implementation and Analysis RFC 1305, University of Delaware, March,\n    1992.\n--- 8015,8030 ----\n  [25]    P. Deutsch, \"GZIP file format specification version 4.3.\" RFC\n    1952, Aladdin Enterprises, May, 1996.\n  \n! [26]    Venkata N. Padmanabhan and Jeffrey C. Mogul. Improving HTTP\n!   Latency. Computer Networks and ISDN Systems, v. 28, pp. 25-35, Dec. 1995.\n!   Slightly revised version of paper in Proc. 2nd International WWW\n!   Conf. '94: Mosaic and the Web, Oct. 1994, which is available at\n!   http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html\n  \n+ [27]    Joe Touch, John Heidemann, and Katia Obraczka, \"Analysis of HTTP\n+   Performance\", <URL: http://www.isi.edu/lsam/ib/http-perf/>, USC/Information \n+   Sciences Institute, June 1996\n+ \n  [28]    Mills, D, \"Network Time Protocol, Version 3.\", Specification,\n    Implementation and Analysis RFC 1305, University of Delaware, March,\n    1992.\n***************\n*** 8032,8041 ****\n    1.3.\" RFC 1951, Aladdin Enterprises, May 1996.\n  \n  [30]    S. Spero. \"Analysis of HTTP Performance Problems\"\n!   <URL:http://sunsite.unc.edu/mdma-release/http-prob.html>, Joe Touch,\n!   John Heidemann, and Katia Obraczka, \"Analysis of HTTP Performance\",\n!   <URL: http://www.isi.edu/lsam/ib/http-perf/>, USC/Information Sciences\n!   Institute, June 1996\n  \n  [31]    P. Deutsch, J-L. Gailly, \"ZLIB Compressed Data Format Specification\n    version 3.3.\" RFC 1950, Aladdin Enterprises, Info-ZIP, May 1996.\n--- 8033,8039 ----\n    1.3.\" RFC 1951, Aladdin Enterprises, May 1996.\n  \n  [30]    S. Spero. \"Analysis of HTTP Performance Problems\"\n!   <URL:http://sunsite.unc.edu/mdma-release/http-prob.html>, \n  \n  [31]    P. Deutsch, J-L. Gailly, \"ZLIB Compressed Data Format Specification\n    version 3.3.\" RFC 1950, Aladdin Enterprises, Info-ZIP, May 1996.\n***************\n*** 8055,8061 ****\n  \n  \n  \n- \n  Fielding, et al                                   [Page 139]\n  \n  \n--- 8053,8058 ----\n***************\n*** 8609,8615 ****\n  resource and some other resource. An entity MAY include multiple Link\n  values. Links at the metainformation level typically indicate\n  relationships like hierarchical structure and navigation paths. The Link\n! field is semantically equivalent to the <LINK> element in HTML.\n  \n         Link           = \"Link\" \":\" #(\"<\" URI \">\" *( \";\" link-param )\n  \n--- 8606,8612 ----\n  resource and some other resource. An entity MAY include multiple Link\n  values. Links at the metainformation level typically indicate\n  relationships like hierarchical structure and navigation paths. The Link\n! field is semantically equivalent to the <LINK> element in HTML.[5]\n  \n         Link           = \"Link\" \":\" #(\"<\" URI \">\" *( \";\" link-param )\n  \n\n\n------- End of Forwarded Message\n\n\n\n", "id": "lists-010-6643608"}, {"subject": "Re: Sticky stuff", "content": "> 3. It didn't consider asymmetric bandwidth situations. In the limit that\n> the downstream connection is infinite in speed and with low latency, the\n> savings of 80% in request header size that your study used would be\n> significant. (The most likely early deployment of cable modems will use\n> ordinary telephone lines as the request channel. Phill points out that\n> if other latencies are significant, then header size savings won't\n> matter much -- but these kind of cable modems are pretty close to the\n> infinite speed, low latency model.)\n\njust as a datapoint, the cable modem installations in the toronto area\n(which are now actual production services, not beta tests) use two-way \ncommunication over the cable lines.  they're not assymetrical.\n\nmy opinion, not that you asked for it:   sticky headers would be pain to\nimplement, for little real benefit.  if the people involved really care, \ni would suggest someone go ahead and code it up before discussing\nit further on this list.\n\n-=- sfw\n\n                                                            stephen f. white\n                                                        sfwhite@incontext.ca\n                                    http://www.csclub.uwaterloo.ca/~sfwhite/\n                            \"\"information highway\" and \"virtual reality\" are\n               the farts at the end of the industrial burrito\"  -- ryan daum\n\n\n\n", "id": "lists-010-6669505"}, {"subject": "Re: Sticky stuff", "content": "Stephen White writes:\n> just as a datapoint, the cable modem installations in the toronto area\n> (which are now actual production services, not beta tests) use two-way \n> communication over the cable lines.  they're not assymetrical.\n> \n\nSeveral of the systems being tested in the U.S. use two-way\ncommunication over cable lines, but are still bandwidth-assymetric.\nAs I understand it, they use several frequency bands (each band being\nequivalent to what would deliver a standard cable channel) in the\ndownstream portion, but only a part of one in upstream portion.  The\nsystem in Toronto may,of course, be designed very differently.\n\n\nTed Hardie\nNASA Science Internet\n\n\n\n", "id": "lists-010-6678428"}, {"subject": "Re: Size of the Spec Was:Re: Beyond 1.", "content": "> From: touch@ISI.EDU\n> Subject:  \n> Cc:  touch@ISI.EDU\n> \n> > From http-wg-request@cuckoo.hpl.hp.com Thu Aug  8 10:22:58 1996\n> > Resent-Date: Thu, 8 Aug 1996 18:21:40 +0100\n> > To: Peter J Churchyard <pjc@trusted.com>,\n> >         http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> > Cc: hallam@Etna.ai.mit.edu\n> > Subject: Size of the Spec Was:Re: Beyond 1.1  \n> > From: hallam@Etna.ai.mit.edu\n> ...\n> > I's like to take an unpopular position here. I don't think that the\n> > HTTP/1.1 spec is too large at all. It may be larger than the average\n> \n> There are two distinct issues -\n> - size\n> - layers (or separable protocols)\n> \n> I would propose that the HTTP/1.1 spec should be split\n> more because of the latter than the former.\n> \n> There are really distinct protocols:\n> \n> - object exchange (HTTP)\n> \n> - caching (MIME extensions for caching distributed objects)\n> \n> - non-protocol HTTP extensions (MIME extensions for HTML)\n> \n> - protocol extensions for persistence\n> including extensions for chunking and muxing\n> \n\nFor any such proposal to be realistic, a number of issues need to\nbe overcome.  These include:\n1) who would do the editorial work (and coordinate a set of\ndocuments).  Doing separate documents is more work than one\ndocument, and I'm certainly not looking for more work to do.\n2) are the sections actually separable.  The current document\nhas many \"see section xx.xx\" cross references; I'm not sure\nsuch a spit is realistic, though I haven't examined it carefully.\nMy gut reaction it is that it is not...\n\nThe most sensible suggestions to date from where I sit would be to\nsplit along implementation advice and the strict protocol itself, but\nI haven't seen anyone volunteer to help do the actual work.  Without\nsuch editorial help, any such proposal to split the document, if it is\ndeemed desirable, is fated to be just so much hot air and merely\nwasting everyone's time, and I'd strongly suggest it be AFTER it is\nmoved to Proposed...\n\n- Jim\n\n\n\n", "id": "lists-010-6686399"}, {"subject": "Re: Sticky stuff", "content": ">The normal operation for a group-to-organizational proxy or a\n>regional-to-national proxy will be one of interleaved requests on\n>a small number of persistent connections, with the possiblility of\n>high performance requirements on those connections due to the bursty\n>nature of web traffic.  I do not consider that to be \"typical\",\n>but it is part of the design of HTTP/1.1 and cannot be ignored without\n>breaking the protocol.  Such applications are not typical today, but\n>they will be typical in the future (the near future given the rate\n>at which demand is approaching network bandwidth capacity).\n\nI really object to the use of this language. To my knowledge\nnobody has every produced such a server. I suspect that very\nfew such servers if any will ever be built.\n\nI don't regard it as part of the \"design\" of HTTP/1.1 it is\nsimply a side effect of other considerations. At this point I\nam interested in how far we can travel from our existing installed\nbase to where it would be nice to be. I don't think it is helpfull\nto start from a theoretical projection from the existing protocol\nand start complaining that software that nobody has written and\nmight never be written will break.\n\nIf we want to support such a mode then lets do it in a principled\nway. It would be a simple matter for such a proxy to add in stream\nids.\n\nEven more to the point such a convergence/multiplexing protocol\nwill suck eggs performance wise unless something like Jim 'n\nHenrik's MUX layer is implemented. The requests and responses\nwill have to be serialized. That does not appear to me to be a \ngood thing!\n\n\n>> I would like to suggest that we consider making HTTP a non-idempotent\n>> protocol and introduce methods to provide transaction semantics.\n>\n>HTTP is already non-idempotent (POST). Do you mean stateful?\n\nActually as implemented GET is non-idempotent. And idempotence is\nmathematically equivalent to statefull. I would like the protocol\nto admit what it has been for three years and drop the idempotence\nlanguage in favour of \"non-contractual\" which is closer to what\nTim actually meant. \n\n[transaction semantics stuff]\n\n>However, this is no longer relevant to the subject under discussion.\n\nI think the general topic we should be addressing is the space of \nTCP/IP improvements that we can make.\n\nI personally favour functional improvements over performance tweaks.\nThe Web lets me do very little of what it was originally meant to\nprovide. 1.1 was slated as the performance \"save the net edition\".\nI think that we should catch up on the backlog of making the Web\na usefull tool and address functional limitations.\n\n\nPhill\n\n\n\nPhill\n\n\n\n", "id": "lists-010-6697850"}, {"subject": "Re: Sticky stuff", "content": "The Gauntlet (TM) Internet firewall V3.2 has an http proxy/gateway that\ndoes maintain connections between the client and the proxy. But since this\nis over Ethernet or faster, sticky headers wouldn't be much of a win. The\nproxy does NOT do onward persistant connections yet. \n\nThe proxy is getting a bit big to be a firewall proxy at around 5000 lines\nof C. As features are added to HTTP it is going to become harder to \nproxy HTTP with a program that is small enough to trust.\n\nPete.\n-- \nThe TIS Network Security Products Group has moved again!\nvoice: 301-527-9500x111  fax: 301-527-0482\nRoom 334, 15204 Omega Drive, Rockville, MD 20850\n\n\n\n", "id": "lists-010-6707409"}, {"subject": "Re: Sticky stuff", "content": ">The Gauntlet (TM) Internet firewall V3.2 has an http proxy/gateway that\n>does maintain connections between the client and the proxy. But since this\n>is over Ethernet or faster, sticky headers wouldn't be much of a win. The\n>proxy does NOT do onward persistant connections yet. \n\nThis is not the case Roy is citing. Say you have two clients, Alice and\nBob. Alice goes to the CNN site, you establish a connection, Bob then comes \nand requests a page from CNN. The server re-uses the connection to CNN.\n\nIn the absence of any other reasons this would be a usefull \noptimization which might save a measurable amount of time but\nsomehow I doubt that the fact that Bob can only make his request\nafter Alice is finished makes this a very worthwhile optimisation.\n\nUnless you keep the TCP/IP socket open a while after Alice finishes the \nchance that you will reach the proxy in a timeslot when Alice is finished\nand the connection is still arround is very low. If you do hold\nthe socket open you and the remote server will both pay for that. \n\nI simply don't see any worthwhile benefit. Or at best I see more benefit\nin sticky headers than in the fan-in optimisation.\n\n\nI think we have to be realistic about what caching can and cannot \nachieve. Caching is only going to be effective if the response\nis insensitive to the request.\n\nI suggested a form of header compression a while ('93) in the\ndays when Mosaic spewed forth about 2K of Accept headers. The (not\nso good) idea then was to hash the headers with MD5 and then\nservers could quickly build up a database of Browser \"profiles\"\ndescribing their capabilities. In those days 2K was a worthwhile\nsaving because it brought the accept list down to a reasonable\nsize. I consider a multi circuit content negotiation on the \nSpero or Holtman model a better solution. Especially since these\ndays we have proxies that can fiddle with the headers (!).\n\n\nI think that before accepting any optimisation we should set \na fairly hefty threshold for it to cross before it is allowed\nto constrain the protocol. If we had adopted my compression hack\nin 93 then we would have been stuck when it came to proxies.\nI think that tokenising HTTP may well fit into the same \ncategory. We are in danger of missing out on major optimisations\nif we worry overmuch about minor ones. I think that numbers\nhave to rule here and Koen's numbers seem to rule out a\nsignificant advantage.\n\n\nPhill\n\n\n\n", "id": "lists-010-6714904"}, {"subject": "RE: Sticky stuff", "content": "In many cities, to \"build out\" (as they say in the cable TV biz) the\n\"cable plant\" to be two way is a major undertaking that will take years.\nWhereas, sending requests by modem over telephone line and receiving the\nresponse over the cable, is something that can be installed with no\nupgrade to the cable plant. The high profile tests often use the\nglitzier two-way technology, but cable companies are very attracted to\nthe other because it could be deployed more rapidly (this information\nfrom my stint in Interactive TV).\n\nSince nothing is ever simple, it is also true that many cable systems\nhave no spare channels in the downstream direction, and that when the\nsolve that problem they also make it two way; but most cable modems that\nI know of are still bandwidth asymmetric.\n\nAlso, the telco's are talking about \"ADSL\" which is typically something\nlike 64 kbits from the home, and 1.5 mbits to the home. (Other variants\nexist with higher bandwidth in both directions, but the degree of\nasymmetry is about the same.)\n\nPaul\n\n>----------\n>From: hardie@merlot.arc.nasa.gov[SMTP:hardie@merlot.arc.nasa.gov]\n>Subject: Re: Sticky stuff.\n>\n>Stephen White writes:\n>> just as a datapoint, the cable modem installations in the toronto area\n>> (which are now actual production services, not beta tests) use two-way \n>> communication over the cable lines.  they're not assymetrical.\n>> \n>\n>Several of the systems being tested in the U.S. use two-way\n>communication over cable lines, but are still bandwidth-assymetric.\n>As I understand it, they use several frequency bands (each band being\n>equivalent to what would deliver a standard cable channel) in the\n>downstream portion, but only a part of one in upstream portion.  The\n>system in Toronto may,of course, be designed very differently.\n>\n>\n>Ted Hardie\n>NASA Science Internet\n>\n>\n>\n>\n>\n\n\n\n", "id": "lists-010-6724357"}, {"subject": "RE: Sticky stuff", "content": "Phill -- you are talking about a different granularity of connection\nmultiplexing than the draft intended. It was a bad choice of\nterminology, and I'm going to change it in the next rev.\n\nA better analogy is \"serial reuse\" -- the proxy will keep a persistent\nconnection to a server, using for one client at a time, until it goes\nidle for \"long enough\" and the proxy closes it. It wouldn't actually\never use it for multiple outstanding requests for different clients.\n\n>----------\n>From: hallam@Etna.ai.mit.edu[SMTP:hallam@Etna.ai.mit.edu]\n>Sent: Friday, August 09, 1996 11:09 AM\n>To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Cc: hallam@Etna.ai.mit.edu\n>Subject: Re: Sticky stuff.  \n>\n>\n>>The normal operation for a group-to-organizational proxy or a\n>>regional-to-national proxy will be one of interleaved requests on\n>>a small number of persistent connections, with the possiblility of\n>>high performance requirements on those connections due to the bursty\n>>nature of web traffic.  I do not consider that to be \"typical\",\n>>but it is part of the design of HTTP/1.1 and cannot be ignored without\n>>breaking the protocol.  Such applications are not typical today, but\n>>they will be typical in the future (the near future given the rate\n>>at which demand is approaching network bandwidth capacity).\n>\n>I really object to the use of this language. To my knowledge\n>nobody has every produced such a server. I suspect that very\n>few such servers if any will ever be built.\n>\n>I don't regard it as part of the \"design\" of HTTP/1.1 it is\n>simply a side effect of other considerations. At this point I\n>am interested in how far we can travel from our existing installed\n>base to where it would be nice to be. I don't think it is helpfull\n>to start from a theoretical projection from the existing protocol\n>and start complaining that software that nobody has written and\n>might never be written will break.\n>\n>If we want to support such a mode then lets do it in a principled\n>way. It would be a simple matter for such a proxy to add in stream\n>ids.\n>\n>Even more to the point such a convergence/multiplexing protocol\n>will suck eggs performance wise unless something like Jim 'n\n>Henrik's MUX layer is implemented. The requests and responses\n>will have to be serialized. That does not appear to me to be a \n>good thing!\n>\n>\n>>> I would like to suggest that we consider making HTTP a non-idempotent\n>>> protocol and introduce methods to provide transaction semantics.\n>>\n>>HTTP is already non-idempotent (POST). Do you mean stateful?\n>\n>Actually as implemented GET is non-idempotent. And idempotence is\n>mathematically equivalent to statefull. I would like the protocol\n>to admit what it has been for three years and drop the idempotence\n>language in favour of \"non-contractual\" which is closer to what\n>Tim actually meant. \n>\n>[transaction semantics stuff]\n>\n>>However, this is no longer relevant to the subject under discussion.\n>\n>I think the general topic we should be addressing is the space of \n>TCP/IP improvements that we can make.\n>\n>I personally favour functional improvements over performance tweaks.\n>The Web lets me do very little of what it was originally meant to\n>provide. 1.1 was slated as the performance \"save the net edition\".\n>I think that we should catch up on the backlog of making the Web\n>a usefull tool and address functional limitations.\n>\n>\n>Phill\n>\n>\n>\n>Phill\n>\n>\n>\n>\n>\n\n\n\n", "id": "lists-010-6735412"}, {"subject": "Re: Sticky stuff", "content": "Paul Leach:\n>\n>There were several problems with your analysis, some of them pointed out\n>at the time it was first posted.\n>\n>1. It didn't include the effects of Accept-Language, Accept-Encoding,\n>Accept-Charset, or From (or Host, but it didn't exist at the time).\n>These can add to the average request size. So could future headers added\n>to HTTP.\n\nI calculated the `500 byte scenario' to take such things into account.\nEven this scenario showed no impressive improvements.  It would be\ngood to measure the average request size for today's clients, but I do\nnot expect large increases compared to a year ago.\n\n>2. It was taken between proxy and server, \n\nThis is because the internet backbone links are the bottleneck, not\nthe LAN between your proxy and your user agent.  Who cares if you get,\nsay, 30% savings in web traffic on the LAN?  In this game, it is\nbackbone savings that count.\n\n>where the effect of 304 (Not\n>Modified) is not seen. (The low % in headers in your because the average\n>response entity-body was ~10,000 bytes. In a 304 response, the\n>entity-body size is 0, so the savings in request header size is more\n>imprtant).\n\nI still have my original datasets, and just ran some new statistics on\nthem.  In proxy<->outside server traffic, 200 responses made up 82% of\nall responses, and 304 responses 4%.\n\n[...]\n>3. It didn't consider asymmetric bandwidth situations. In the limit that\n>the downstream connection is infinite in speed and with low latency, the\n>savings of 80% in request header size that your study used would be\n>significant. (The most likely early deployment of cable modems will use\n>ordinary telephone lines as the request channel.\n\nAnd the modem hooked to the ordinary telephone line will probably do\ndata compression, so you would gain little extra with sticky headers,\nI believe.\n\n[...]\n>I think a new study would be needed to take these effects into account\n>before we can conclude that sticky headers aren't worth the effrort.\n\nI agree.  To get something like a firm conclusion, at least one other\nstudy is needed.  My study was done a year ago, with a small sample\n(145 Mb of traffic), and by someone who is not a statistician.\n\nHowever, I think there is enough data to conclude that sticky headers\nare *unlikely* to be worth the effort.  I therefore see no reason for\nsticky headers to become a WG activity at this point.\n\nKoen.\n\n\n\n", "id": "lists-010-6748357"}, {"subject": "Re: Call for compatibility tester", "content": "On Thu, 8 Aug 1996, Roy T. Fielding wrote:\n\n> > The transparent content negotiation draft\n> > (http://gewis.win.tue.nl/~koen/conneg/) currently specifies the 300\n> > response code for list responses.  \n> > \n> > It turns out that this is not compatible with several existing\n> > browsers: lynx and some versions of Mosaic fail to display anything if\n> > they get back a 3xx class response without a Location header.  As lynx\n> > in particular will be important for some people who want to offer\n> > negotiated material, this rules out use of the 300 response code in\n> > transparent content negotiation.\n\n[...]\n\n> And is out of the question.  Bugwards compatibility is achieved by\n> looking at the User-Agent value, and sending 406 (or just 200 with\n> an appropriate Vary and Alternates) is better than generating a\n> success message masked as a client error.\n\nOkay... let me ask a silly question. Having read the various versions\nof Koen's content negotiation drafts, I am at a loss to understand why\na server would send a 300 response to a user agent that did not\nsupport it. According to version pre02-19 of the draft, support for\ntransparent negotiation is indicated by the presence of a Negotiate:\nheader. Presumably, any user agent that sent this header supports the\n300 response code (to do otherwie would be the height of idiocy - not\nto say that it might not happen, though).\n\nIf I was writing a server to implement transparent negotiation, I\nwould only send a 300 response if the user agent sent\nNegotiate:. Otherwise, I would make the choice on behalf of the user\nagent (based on the Accept: headers or some other indication), and\nsend that document.\n\nWhy? I know the user agent can't parse the Alternates: header; even if\nI sent a Location:, the user agent would just end up getting that\ndocument anyway (and note that popular browsers like Netscape\nNavigator do not do automatic redirection when they are given a 300\nresponse code and a Location: header - they only do that for 301 and\n302). Plus, I'd wager that the majority of negotiated content is (and\nwould be in the future) for inlined objects. And while if I'm\nnegotiating between text and postscript, it's acceptable to send an\nHTML representation that asks the user to choose by clicking on a\nlink, if I'm negotiating between GIF and PNG, odds are it's an inline\nrequest, in which case if I send HTML, the user agent will give the\nuser a broken image icon.\n\nAll of this combines to convince me that the intentions of the spec\n(although it may not be explicitly stated - though it should be) are\nthat unless the Negotiate: header is sent, the server should never\nsend a 300 response, but should choose on behalf of the user agent,\nand send the chosen resource.\n\nIf this principle is used, a user agent that does not support 300\nreponses will never receive one; even via a proxy cache, if the server\n(as recommended by the spec) sends \"Vary: Negotiate\", and an immediate\nexpires for HTTP/1.0 requests.\n\nSo, in short, I don't see the problem.\n\n-- \n________________________________________________________________________\nAlexei Kosut <akosut@nueva.pvt.k12.ca.us>      The Apache HTTP Server\nURL: http://www.nueva.pvt.k12.ca.us/~akosut/   http://www.apache.org/\n\n\n\n", "id": "lists-010-6758620"}, {"subject": "Re: Sticky stuff", "content": " > >I think a new study would be needed to take these effects into account\n > >before we can conclude that sticky headers aren't worth the effrort.\n > \n > I agree.  To get something like a firm conclusion, at least one other\n > study is needed.  My study was done a year ago, with a small sample\n > (145 Mb of traffic), and by someone who is not a statistician.\n > \n > However, I think there is enough data to conclude that sticky headers\n > are *unlikely* to be worth the effort.  I therefore see no reason for\n > sticky headers to become a WG activity at this point.\n\nAs a server implementer, I feel that one thing is missing from this\ndicussion: the (CPU) time to parse headers. Sticky header allows for\nreusing parsed header values, and at least in my Java web server, this\naccount for a non-negligible portion of the CPU time that is used to\nhandle a request (by optimizing this part of the server I got up to\n20% speed improvements). However, I don't know if this applies to C\nserver as well...\n\nOf course, I beleive that - still in my case - the CPU time to handle\nsticky headers would not be greater then the time to parse, say, a\nreal Accept header.\n\nAnselm.\n\n\n\n", "id": "lists-010-6770436"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Erik Aronesty:\n>\n>Professionals (IE: Pathfinder) no longer report things like \"10K hits\n>per day\" to clients who pay well.  \n\nI'm confused.  Do you mean that you no longer report hits, or that you\nno longer only report hits?\n\n>They say \"we have a large\n>international audience\" or \"we get 40% of our hits from browsers which\n>support Java\".\n>\n>Information such as \"User Agent\" and the clients ip address (for\n>demographics) are crucial to the log reporting in the sites I have\n>worked on (albeit only 6 sites). \n\nThis is very interesting...  I wrote earlier that we need to\ndistinguish between two kinds of demographic data:\n\n1) Hit counts\n\n2) User's Referer field, IP address, User-Agent field, ...\n\nThe proposed hit counting mechanism allows you to get 1) for all user\nagents without cache busting, but not 2).  You seem to predict that\nmost advertising sites will want to have 2) in future.  That would\nmake the the proposed hit counting mechanism pretty ineffective at\nreducing cache busting.\n\nOn the other hand, if you gather 2) without cache busting now, and do\nan extrapolation pass on the results by guessing the amount of hits\nhidden by certain proxies, then the hit counting data would allow more\naccurate extrapolations.  (In such an extrapolation pass, you would\nassume that, as far as the headers are concerned, the requests relayed\nby a proxy can be treated as a random sample of all requests made\nbehind the proxy.)\n\nSo my main question is: do you use cache busting to gather the 2)\nstatistics, and would you stop using it if the hit count proposal is\nimplemented?\n\nIf not, then the hit counting proposal won't reduce cache busting\nmuch, and we would be better off with a headers-summary mechanism like\nyou propose:\n\n>Perhaps the hit-metering process should allow a proxy to forward some\n>sort of a headers-only-summary during a period of relative inactivity. \n>The server should not care how long it has been since the proxy has last\n>sent its summary.   The \"Expires\" header can then still be used to\n>accurately reflect the duration of the validity of the document.\n\nKoen.\n\n\n\n", "id": "lists-010-6779124"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">I'm confused.  Do you mean that you no longer report hits, or that you\n>no longer only report hits?\n\nonly hits....i've never gotten a summary with \"just hits\"\n>\n>>They say \"we have a large\n>>international audience\" or \"we get 40% of our hits from browsers which\n>>support Java\".\n\n>So my main question is: do you use cache busting to gather the 2)\n>statistics, and would you stop using it if the hit count proposal is\n>implemented?\n\n>If not, then the hit counting proposal won't reduce cache busting\n>much, and we would be better off with a headers-summary mechanism like\n>you propose:\n>\n>how do you do an analysis on a logfile to see if caching would have\n>affected the results?\n\nperhaps the proposal should include such an analysis...\n\nstill...an incomplete demographics picture is ugly...and is useless in\nthe attempt\nto obtain statistical significance\n\n\n\n", "id": "lists-010-6789587"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    This is very interesting...  I wrote earlier that we need to\n    distinguish between two kinds of demographic data:\n    \n    1) Hit counts\n    \n    2) User's Referer field, IP address, User-Agent field, ...\n    \n    The proposed hit counting mechanism allows you to get 1) for all user\n    agents without cache busting, but not 2).  You seem to predict that\n    most advertising sites will want to have 2) in future.  That would\n    make the the proposed hit counting mechanism pretty ineffective at\n    reducing cache busting.\n\nThis is not true.  For any field that appears in the client's\nrequest headers (i.e., not \"IP address\" but definitely User-Agent),\nyou can obtain counts without cache-busting (which I would\ndefine as \"completely disabling caching\").\n\nE.g., suppose I (at an origin server) want to subdivide the count\nbased on User-Agent.  I would send \"Vary: User-Agent\" with my\nresponses, but I would still send the same Etag header with each\nresponse.\n\nSo the first time the cache does a GET on the URL, e.g.,\n\nGET /foo.html HTTP/1.1\nUser-Agent: Lynx\nCache-control: use-count=0\n\nI would reply\n\nHTTP/1.1 200 OK\nEtag: \"abc\"\nVary: User-Agent\nContent-Length: 10000\nCache-control: max-uses=1000\n[...]\n\nThen when the cache receives another request from a Mosaic user, it\nwould send\n\nGET /foo.html HTTP/1.1\nUser-Agent: Mosaic\nCache-control: use-count=0\nIf-None-Match: \"abc\"\n\nand I would reply\n\nHTTP/1.1 304 Not Modified\nEtag: \"abc\"\nCache-control: max-uses=1000\n\nI.e., the cache now knows that it can use the entity with Etag = \"abc\"\nfor both Mosaic and Lynx User-agents, and that I want it to keep\nseparate counts based on User-agent.  Only one copy of the body\nhas been transmitted from the server to the cache, only one copy\nis stored at the cache, and the cache need not do any more conditional\nGETs unless new User-Agents appear.\n\nIf the proxy is \"cooperative\" (as we define in our proposal), then\nwhen it finally removes this entity from its storage, it would have\nto send\n\nHEAD /foo.html HTTP/1.1\nUser-Agent: Lynx\nCache-control: use-count=97\n\nHEAD /foo.html HTTP/1.1\nUser-Agent: Mosaic\nCache-control: use-count=13\n\n(but of course these can be sent in one TCP packet, since we\nare presumably using persistent connections).\n\n-Jeff\n\n\n\n", "id": "lists-010-6798769"}, {"subject": "RE: Sticky stuff", "content": ">----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Subject: Re: Sticky stuff.\n>\n>\n>>2. It was taken between proxy and server, \n>\n>This is because the internet backbone links are the bottleneck, not\n>the LAN between your proxy and your user agent.  Who cares if you get,\n>say, 30% savings in web traffic on the LAN?  In this game, it is\n>backbone savings that count.\n\nNot everyone has a LAN link between user agent and proxy. Some small\nnumber of people dial in to their proxy, which is owned by  their\nservice provider. Say order 10 million or so, with providers like AOL\nand MSN.\n>\n>>where the effect of 304 (Not\n>>Modified) is not seen. (The low % in headers in your because the average\n>>response entity-body was ~10,000 bytes. In a 304 response, the\n>>entity-body size is 0, so the savings in request header size is more\n>>imprtant).\n>\n>I still have my original datasets, and just ran some new statistics on\n>them.  In proxy<->outside server traffic, 200 responses made up 82% of\n>all responses, and 304 responses 4%.\n\nAgain, I was talking about user-agent to proxy traffic, and the 304s\nthat would arise when the user-agent had something in its cache that it\nneeded to revalidate.\n>\n>[...]\n>>3. It didn't consider asymmetric bandwidth situations. In the limit that\n>>the downstream connection is infinite in speed and with low latency, the\n>>savings of 80% in request header size that your study used would be\n>>significant. (The most likely early deployment of cable modems will use\n>>ordinary telephone lines as the request channel.\n>\n>And the modem hooked to the ordinary telephone line will probably do\n>data compression, so you would gain little extra with sticky headers,\n>I believe.\n\nIn your model, a 200 byte request would be reduced by useing sticky\nheaders to 40 bytes, and then the 40 bytes would be compressed to (say)\n20 bytes (assuming a 2-1 compression ratio). Without sticky headers, and\nthe same compression ratio, requests would be 100 bytes. The savings\nfrom sticky headers is still the same 80%. \n>\n>[...]\n>>I think a new study would be needed to take these effects into account\n>>before we can conclude that sticky headers aren't worth the effrort.\n>\n>I agree.  To get something like a firm conclusion, at least one other\n>study is needed.  My study was done a year ago, with a small sample\n>(145 Mb of traffic), and by someone who is not a statistician.\n>\n>However, I think there is enough data to conclude that sticky headers\n>are *unlikely* to be worth the effort.\n\nSeems to me that there is enough data to show that it won't help much in\nyour environment.\nIn other environments, it's at least not so clear-cut.\n\nPaul\n\n\n\n", "id": "lists-010-6809126"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": "this is just selective cache-busting....the proxy has know way of\nknowing if\nthe document \"actually varies\" so the entire document gets sent for each\nvariation.\n\nperhaps the server should send\n\nCache-Control-Vary\n\nor some such nonsense\n\nBTW:\n\nit would be nice to be able to vary by *.com, *.edu, *.org ... and still\nhave everything work.\n\n\n\n\n\n>----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Sent: Friday, August 09, 1996 6:45 PM\n>To: koen@win.tue.nl\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; mogul@pa.dec.com\n>Subject: Re: New document on \"Simple hit-metering for HTTP\" \n>\n>    This is very interesting...  I wrote earlier that we need to\n>    distinguish between two kinds of demographic data:\n>    \n>    1) Hit counts\n>    \n>    2) User's Referer field, IP address, User-Agent field, ...\n>    \n>    The proposed hit counting mechanism allows you to get 1) for all\n>user\n>    agents without cache busting, but not 2).  You seem to predict that\n>    most advertising sites will want to have 2) in future.  That would\n>    make the the proposed hit counting mechanism pretty ineffective at\n>    reducing cache busting.\n>\n>This is not true.  For any field that appears in the client's\n>request headers (i.e., not \"IP address\" but definitely User-Agent),\n>you can obtain counts without cache-busting (which I would\n>define as \"completely disabling caching\").\n>\n>E.g., suppose I (at an origin server) want to subdivide the count\n>based on User-Agent.  I would send \"Vary: User-Agent\" with my\n>responses, but I would still send the same Etag header with each\n>response.\n>\n>So the first time the cache does a GET on the URL, e.g.,\n>\n>GET /foo.html HTTP/1.1\n>User-Agent: Lynx\n>Cache-control: use-count=0\n>\n>I would reply\n>\n>HTTP/1.1 200 OK\n>Etag: \"abc\"\n>Vary: User-Agent\n>Content-Length: 10000\n>Cache-control: max-uses=1000\n>[...]\n>\n>Then when the cache receives another request from a Mosaic user, it\n>would send\n>\n>GET /foo.html HTTP/1.1\n>User-Agent: Mosaic\n>Cache-control: use-count=0\n>If-None-Match: \"abc\"\n>\n>and I would reply\n>\n>HTTP/1.1 304 Not Modified\n>Etag: \"abc\"\n>Cache-control: max-uses=1000\n>\n>I.e., the cache now knows that it can use the entity with Etag = \"abc\"\n>for both Mosaic and Lynx User-agents, and that I want it to keep\n>separate counts based on User-agent.  Only one copy of the body\n>has been transmitted from the server to the cache, only one copy\n>is stored at the cache, and the cache need not do any more conditional\n>GETs unless new User-Agents appear.\n>\n>If the proxy is \"cooperative\" (as we define in our proposal), then\n>when it finally removes this entity from its storage, it would have\n>to send\n>\n>HEAD /foo.html HTTP/1.1\n>User-Agent: Lynx\n>Cache-control: use-count=97\n>\n>HEAD /foo.html HTTP/1.1\n>User-Agent: Mosaic\n>Cache-control: use-count=13\n>\n>(but of course these can be sent in one TCP packet, since we\n>are presumably using persistent connections).\n>\n>-Jeff\n>\n>\n>\n\n\n\n", "id": "lists-010-6821493"}, {"subject": "Re: Sticky stuff", "content": "    >This is because the internet backbone links are the bottleneck, not\n    >the LAN between your proxy and your user agent.  Who cares if you get,\n    >say, 30% savings in web traffic on the LAN?  In this game, it is\n    >backbone savings that count.\n    \n    Not everyone has a LAN link between user agent and proxy.\n\nJust to confirm this: we (at Digital's Palo Alto site) run the main\nproxy between the corporation (still almost 60K people) and the\nInternet.  We have something over 100 Mbits/sec of bandwidth\nbetween our proxy and the Internet, but several orders of magnitude\nless bandwidth to most of our internal sites.  In most cases,\nthe RTTs between the proxy and the users are on the order of\n100 msec or more.  We have numerous users, unfortunately, who\nhave internal connections at 56 kbits/sec and multi-hundred\nmsec RTTs.  Anything that saves lots of bits over these wires is\nGood.\n\n-Jeff\n\n\n\n", "id": "lists-010-6835819"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    this is just selective cache-busting....the proxy has know way of\n    knowing if the document \"actually varies\" so the entire document\n    gets sent for each variation.\n\nWrong.  Please read the proposed HTTP/1.1 specification; it\ndefinitely allows us to avoid sending \"the entire document\"\n\"for each variation.\"\n\n-Jeff\n\n\n\n", "id": "lists-010-6843691"}, {"subject": "Re: Sticky stuff", "content": "I'd like to agree with everyone (or disagree , but lets be positive...)\n\nWith current patterns of header usage, header reuse is not a \nparticularly significant fraction of traffic. Koen's study which was \nperformed about a year (?) ago shoed a rather small header set. \nEarlier samples had much larger header sizes (~1.2K). \n\nThe reason for the change is almost entirely due to the reduction in the \nnumber Accept headers in the typical profile.The reason for doing this \nwas that the headers were too big. \n\nThe lack of stickiness caused the header size to shrink - it's hard to \nseparate cause and effect here.\n\nSimon\n\n ---\nCause maybe  (maybe)      | In my mind I'm going to Carolina\nyou're gonna be the one that saves me | - back in Chapel Hill May 16th.\nAnd after all      | Email address remains unchanged\nYou're my firewall -          | ........First in Usenet.........\n\n\n\n", "id": "lists-010-6851501"}, {"subject": "Re: these results sound very encouragin", "content": "Erik Aronesty writes:\n\n| A significant number of hits for certain documents \n| could have been reduced if the your proxy had reported a \n| document-hash to the client in the header.\n\nYep!  But: not nearly as many as I'd expected.  What happened to all \nthose Cindy Crawford pictures ? ;-)\n\nMy take on this was to hack some very limited support for the \n\"Content-MD5:\" header into the Apache and NCSA HTTP servers, since the \nproxy doesn't really want to have to go to the trouble of calculating \nthis sort of thing itself ?  It already has quite a lot to do, and \nquickly!  Using MD5 (or whatever) to check that you got what you asked \nfor would require an MD5 calculation on the part of the proxy for each \nURL retrieved, which is likely to be a no-no for all but the most \nanally-retentive ?  There was a discussion which led up to this, but as \nI recall it was split across a number of participants, private and \npublic mail...\n\nUnfortunately, my feet haven't really touched the ground very much \nlately, and I haven't had the opportunity to sit down with the code \nagain and make it \"production strength\" - if you look at the sources \nyou'll see that it ships disabled by default.  Phew!  The world is \nsaved from my lame attempts at C programming :-)\n\nI think the next step, and what's required to make this really work, is \nfor the target HTTP servers themselves to generate and maintain a \n*cache* of checksums.  Being very lazy, I'm inclined to do this by \nputting them in a hash database.  A purpose-built in-memory cache would \nbe faster, but feels like it would be quite painful to code up.\n\nThere are a few nasties, like locking strategies on the cache when you \nhave a pool of servers, but it's doable and if I don't get around to \ndoing the extra work I'm sure somebody else will (eventually).  A \nlazier-than-thou first step would be to have a separate process which \nwent around generating the checksum cache periodically, so that the \nHTTP server itself doesn't need to be doing anything particularly \nclever.  Loosely consistent!\n\nObIETF:  Is \"Content-MD5:\" the right way to go about this ?  Should \nhttp-spec-v11-* note this use of MD5 ?  What about other algorithms ?\n\nMartin\n\nPS In case it's not obvious - the rationale is that over time the proxy \ncan automagically \"learn\" about replicated resources.  So, you can take \nyour URNs and stuff them up your...!\n\n\n\n", "id": "lists-010-6860548"}, {"subject": "Re: Meta data in anchors.", "content": "Peter J Churchyard writes:\n\n| When following discussions on content negociation and similiar issues,\n| I often reflect as to whether it would be a good idea to allow meta data\n| to be embedded in the anchor when a document is retrieved so that if a \n| link is to a docuemnt is several formats, the user could xxx-click on the\n| link and get a menu displayed of the available formats.\n\nAnother approach (and since this is the HTTP list...) might be to have \nsome way of saying *in HTTP* that what you want is the object's \n\"metadata\", or the metadata for a whole load of objects.  This could be \ndone perhaps even by a separate \"META\" method, or an extra header, and \nmight be implemented as (for instance) an Apache module.\n\nBy \"metadata\" I mean descriptive info which isn't the results of an \nHTTP HEAD request - it could be much more descriptive than that.  It \ncould also be hand generated, or extracted from the object - e.g. via \nHarvest's HTML summarizer, or embedded comments created by editors and \nconversion programs like LaTeX2HTML.\n\nAnyone who is interested in this sort of thing might want to check out \nthe discussions we've been having on the \"meta2\" mailing list - see \nrecent threads in the Hypermail archive at <URL:http://www.roads.lut.ac.\nuk/lists/meta2/>.  You're welcome to join the list if you can figure \nout how to.  This is my variation on Marshall Rose's great quip about \nsetting entrance exams :-)\n\nMartin\n\n\n\n", "id": "lists-010-6869658"}, {"subject": "Re: Size of the Spec Was:Re: Beyond 1.", "content": "jg@zorch.w3.org writes:\n\n| 2) are the sections actually separable.  The current document\n| has many \"see section xx.xx\" cross references; I'm not sure\n| such a spit is realistic, though I haven't examined it carefully.\n| My gut reaction it is that it is not...\n\n\"See [document y]\" perhaps ?  From the progress I've made so far (sorry \n- a week late already, sigh), this spl?it certainly seems to be more \npractical than the implementation/specification one.  Not surprising \nwhen you consider its origins ?\n\nOne possible advantage of splitting up would be that you could actually \nrevise (obsolete? supercede?) individual bits without having to re-do \nthe whole damn thing whenever you want to change part of it.  If you \nreally are only revising just one aspect (e.g. cache-control features), \nyou can have an effort which is focused on the problem, as opposed to a \nfree-for-all on what features/changes people would like to see in the \nprotocol ?\n\nI suppose this approach has been taken with DNS, SMTP and PPP, to name \nbut three.  Successfully ?  Hmm...\n\nMartin\n\n\n\n", "id": "lists-010-6877765"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Jeffrey Mogul:\n>\n     [Koen Holtman:]\n>    But to allocate max-uses values to proxies an efficient way, an\n>    origin server seems to have to keeping per-proxy database of\n>    `max-use-qouta-use-speed' (last two paragraphs of Section 2), which\n>    adds some overhead to every request.\n>\n>Only if it really wants to be cautious about bounding the counting\n>error.  I would imagine that the simplest servers would have a global\n>setting for this value (e.g., always send \"max-uses=10\").\n\nI just did some thinking about how good max-uses=10 would be at\nbounding the error, and the conclusions are not good.\n\nIf you send 80 immutable pages with max-uses=10 to an un-cooperative\nproxy with 500 web users behind it, then you will likely end up with a\nmeasured count of 80, all hits measued at the origin server itself,\nbecause the chance that more than 10 out of the 500 web users behind\nthe proxy will be interested in any one of your pages and generate a\nhit count report before the page disappears from the cache is small.\n\nHowever, you have handed out 80*10=1000 uses, which gives you 800 hits\nas the upper bound.  So all you know is:\n\n  80 <= actual hits <= 800\n\nThis is not what I call useful information.  Something like an\ninteresting upper bound would be\n\n  80 <= actual hits <= 100\n\nbut I see no way in which max-uses can provide such a bound.\n\nI suspect that max-uses counts higher than 3 will be disastrously\nineffective at yielding a useful upper bound if uncooperative caches\nare common.\n\nA proxy not being cooperative and only supporting max-uses seems about\nas bad as a proxy not supporting hit counts at all.\n\nI'd like to see *actual statistics* disprove my argument, but it seems\nto me that without statistics, there is no way you can justify the\ncomplexity of having max-uses.  It would be better to eliminate it and\ninstead add stuff to make being cooperative less costly (e.g. a method\nto piggy-back hit count reports for other URLs on a request you have to\nsend anyway).\n\nI am moderately interested in getting a working lightweight\ndemographics mechanism.  But I suspect that the current design won't\nwork well in practice.  It would require favorable actual statistics\nfor me to change my mind and support this design.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-6885833"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Jeffrey Mogul:\n>\n>    This is very interesting...  I wrote earlier that we need to\n>    distinguish between two kinds of demographic data:\n>    \n>    1) Hit counts\n>    \n>    2) User's Referer field, IP address, User-Agent field, ...\n>    \n>    The proposed hit counting mechanism allows you to get 1) for all user\n>    agents without cache busting, but not 2).  You seem to predict that\n>    most advertising sites will want to have 2) in future.  That would\n>    make the the proposed hit counting mechanism pretty ineffective at\n>    reducing cache busting.\n>\n>This is not true.  For any field that appears in the client's\n>request headers (i.e., not \"IP address\" but definitely User-Agent),\n>you can obtain counts without cache-busting (which I would\n>define as \"completely disabling caching\").\n\nThat is not my definition of cache busting.  The cache busting done\nfor demographics reasons will be of the `max-age=0' kind, which allows\nconditional GETs, rather than the `no-cache' kind.\n\nI can't see much difference, as far as efficiency is concerned,\nbetween your proposed use of the Vary header and max-age=0 cache\nbusting.  Both as are inefficient, but at least max-age=0 is\ninefficient in a non-complex way.\n\nTo do a good job at giving demographers the complete request data\n(including client IP address) they seem to want, a completely\ndifferent mechanism is needed.  Why don't you specify something that\nlets origin servers tell caches to log certain request characteristics\nand report them in a future request?  The logging overhead for such a\nmechanism seems to be about the same as for your Vary mechanism.\n\nKoen.\n\n\n\n", "id": "lists-010-6896568"}, {"subject": "Conventions for Sharing User Agent Profile", "content": "One way to reduce the number of accept headers, and user agent\ndescription headers is to leverage the existing user agent field.\nRight now its a pain for sites to build a database that associates\nuser agent values to feature sets. There is no agreed language for\ndescribing these sets and no way of exchanging them.\n\nWhat if we could define a simple file format for describing the\nfeature set for a given user agent, in analogy to the way robots.txt\nemerged as a useful convention? This would allow sites to exchange\ninformation easily. It would be an easy matter to create URLs for\naccess a database of such definitions. The information could be\nshared when needed or in advance as appropriate.\n\nWhen an HTTP request is seen for a previously unseen user agent,\nthe database could be consulted to determine the profile. The\nfirst person to access the site with a new user agent takes a\nhit but thereafter everyone wins. Note that this doesn't require\nany changes to HTTP.\n\nIt may also be useful for vendors to say that their user agent\nemulates, let's say Netscape Navigator 2.0. It would be a win if\nthe browser could send this as an additional HTTP header in the\nrequest, e.g. \"UA-Emulates:\". This would reduce the size of the\nUA profile database and reduce the hit taken by the first visitor\nwith the this ua.\n\nIs anyone interested in taking these ideas further?\n\nDave Raggett\n\n\n\n", "id": "lists-010-6906589"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "> It may also be useful for vendors to say that their user agent\n> emulates, let's say Netscape Navigator 2.0. It would be a win if\n> the browser could send this as an additional HTTP header in the\n> request, e.g. \"UA-Emulates:\". This would reduce the size of the\n> UA profile database and reduce the hit taken by the first visitor\n> with the this ua.\n\nThis hasn't worked doing matching on the user-agent. There are two\nfundamental problems:\n\n1) It requires a reasonable specification of what a working\n   \"emulation\" is. While MS has released parts of this for MSIE,\n   NetScape hasn't released one for NSN. A user-agent profile might\n   qualify for this.\n\n2) What part of the WWW mechanism is emulated? Supported MIME types?\n   Various HTTP headers? The mapping of HTML META elements to HTTP\n   headers? Or just the HTML? People have complained about emulations\n   that weren't \"good enough\" for their purpose because they didn't\n   support one or more of the non-HTML features.\n\nIf the user-agent profile provides descriptions of all those features\n(and there's no reason it shouldn't), then authors can build their own\nprofile that cover the areas they use. In this case, an \"emulates\"\nfacility is worthless, because the document author doesn't know what\nprofile the client author used.\n\nIf NetScape can be talked into releasing an a profile for their\nbrowsers, this problem goes away as well. But until they buy into\nthis, I suspect such work would be wasted.\n\n<mike\n\n   \n\n\n\n", "id": "lists-010-6915919"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Having been in the same meeting as Dave I agree that this is an\nimportant issue. It arose from a discussion with the managers of \na prominent Web sit in the DC area. Basically the complaint made\nwas that under the current regime content providers are forced\nto apply a lowest common denominator approach. Customising the\ncontent on the basis of the user agent field is very unsatisfactory:\n\n1) It limits competition, sites cannot track every browser's \ncapabilities so they tend to track the main browsers in use. This\nin turn means that people with non mainstream browsers get the\nlowest common denominator even when their site is capable of\nhandling much more.\n\n2) The User agent is likely to become a smaller part of the package \nin future. In a componentware situation a browser may well have the\nability to upgrade its redering code independently of its protocol\ncode. This may become an important consideration in future releases\nof O/S like Windows 97 where the ability to replace components in\nthe architecture would appear to be very much part of the idea.\n\n3) The user agent field is badly specified and badly implemented. \nIt is treated like a comment field by many browsers. It is not possible\nto make a simple extrapolation from the user agent ids passed about \nto the capabilities of the browser.\n\n\nI see this issue as one which must be addressed while the Web is\nstill in a relatively fluid stage. Users are still upgrading\nbrowsers, if only because so many out their at the moment crash\nevery 50 downloads or so.\n\n\nI see version number on MIME types as a part of a solution to this\nproblem. This allows a \"low watermark\" to be established in a cheap\nand convenient manner. If a browser advertises text/html; version=3.2\nthen it had better comply with the standard. The version numbering\nscheme could even be extended to provide for beta drafts etc, for\nexample text/html; version=3.2b24 would refer to W3 Consortium \ndraft number 24. Support for 3.2b24 would imply that all features of\n3.1 were supported, but not that features of 3.2b23 were necessarily\npresent - allowing specs to loose features rather than simply\naccumulate them.\n\nPrivate extensions are harder to cope with. One might posit that\neach vendor wishing to propose new tags could submit a draft and \nrequest a draft id number from the standards body for the mime type \nconcerned (eg W3C for HTML, Adobe for Postscript, \nBoeing for text/plane).\n\nAnother approach would be to introduce an opaque identifier on the\n\"lets think of a big number unlikely to collide by random chance\"\nprinciple and add it to the mime type as a variant :-\n\ntext/html; version=3.2c21384498123946123423148976\n\nThis is effectively the opaque tag that Dave describes.\n\n\nBesides this there is a parallel need for the browser to tell the\nserver about the features that that installation of the code\ncan support. Eg:-\n\nAre images turned off\nAre Animated Gifs disabled\n(Probably the number one item on my browser wish list.\nI have a script that obliterates animated gifs in\nthe cache each night but its clunky).\nWhat is the resolution of the screen\nWhat colour models are supported\nIs the device linear text or page mode\n\n\nThis is more in the area that Dave's proposal addresses. I think that\nboth aspect of the problem should be explored.\n\nI like the idea of emulates fields. Note that the objection that\npeople may not have a 100% emulation is bogus. It would be as \npointless for Netscape to claim that they supported a feature set\nof IE that they did not as for a terminal emulation company with a\nVT100 emulator to have it announce itself as VT320. Boasting about\nones abilitites in this area is self policing, the user will simply\nget a page of garbage.\n\nOne benefit of this approach may be to encourage the vendors to put\na bit more thought into some of the \"enhancements\" released with \n<blink>perhaps insufficient thought</blink>. Rather than dribble\nthem out there would be an interest in collecting together a\nreasonable package of enhancements and deliver them as a collection\nrather than the current trend of \"dribbling\" them out.\n\n\nPhill\n\n\n \n\n\n\n", "id": "lists-010-6924744"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "hallam@Etna.ai.mit.edu:\n>\n>Having been in the same meeting as Dave I agree that this is an\n>important issue.\n\nDave, Phill,\n\nI believe that the feature negotiation mechanism in transparent\ncontent negotiation solves many of the negotiation and registration\nproblems you have identified.\n\nPlease review the draft spec (see\nhttp://gewis.win.tue.nl/~koen/conneg/) and tell me what you think.\n\nI feel that feature negotiation is fundamentally more powerful than\nany user-agent header or version number dependent scheme can be,\nbecause a big limitation to the power of these latter schemes is not\npresent for feature negotiation.  The limitation in question is the\nneed to produce a small encoding, to be sent in request messages, for\n_all_ capabilities of the user agent and its components, and _all_\npreferences of the user.\n\nTransparent content negotiation does away with this limitation, and\nfeature negotiation exploits the absence of this limitation to the\nfullest extent, yielding a framework which allows an open,\nevolutionary approach to the problem of creating a shared language in\nwhich to express capabilities and preferences.\n\nKoen.\n\n\n\n", "id": "lists-010-6936450"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": ">Transparent content negotiation does away with this limitation, and\n>feature negotiation exploits the absence of this limitation to the\n>fullest extent, yielding a framework which allows an open,\n>evolutionary approach to the problem of creating a shared language in\n>which to express capabilities and preferences.\n\nAgreed that these are all related I think we have to get the\nconcept space right before we try to compress everything into\none unified scheme. One reason why I was skeptical of PEP was\nits \"all encompassing\" nature which never quite appeared to be\ngrounded.\n\nI think that there is a role for some king of multi-circuit \nexchange protocol. I think that needs to be more general\nthan simply content type or even feature negotiation. I\nam currently looking at assertion exchange based on common\nreference terms which are essentially pure URNs except much\nof the baggage associate with URNs has to be lost so I'm\nusing a new name.\n\n\nI think we need to kick the requirements space arround a bit,\nget a feel for the constraints on implementation and then look\nat architecture again.\n\n\nPhill\n \n\n\n\n", "id": "lists-010-6945696"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "hallam@Etna.ai.mit.edu:\n>\n [Koen Holtman:]\n>>Transparent content negotiation does away with this limitation, and\n>>feature negotiation exploits the absence of this limitation to the\n>>fullest extent, yielding a framework which allows an open,\n>>evolutionary approach to the problem of creating a shared language in\n>>which to express capabilities and preferences.\n>\n>Agreed that these are all related I think we have to get the\n>concept space right before we try to compress everything into\n>one unified scheme.  \n\nThe goal of the current draft spec is not to define the ultimate\nunified scheme.  I don't think it is possible right now to define a\nunified scheme that will last without changes for, say, the next 10\nyears.  Once we get more experience with content negotiation, we will\ndiscover social and technical problems we cannot even begin to imagine\nnow.\n\nQuoting from the draft spec:\n\n 19.3 Open issues in transparent content negotiation\n [...]\n   - Though it is expected that the feature negotiation framework will\n     solve many current and future negotiation problems, it is also\n     expected that there will remain current and future negotiation\n     problems not solved by feature negotiation.\n [...]\n\nFeature negotiation is my best attempt at extrapolating the mechanisms\nwe will need from current negotiation experience.  If you see something\nmissing in feature negotiation right now, by all means say so that we\ntry to think up a fix.  We already identified the need to add\nnon-numeric feature tags, and they will be added in the next version\nof the draft.\n\n>One reason why I was skeptical of PEP was\n>its \"all encompassing\" nature which never quite appeared to be\n>grounded.\n\nTransparent content negotiation makes no claim to be the ultimate\nmechanism.  Transparent content negotiation tries very hard to be\nextensible, because I fully expect it not to be the final answer.\n\n>I think that there is a role for some king of multi-circuit \n>exchange protocol. I think that needs to be more general\n>than simply content type or even feature negotiation.  I\n>am currently looking at assertion exchange based on common\n>reference terms which are essentially pure URNs except much\n>of the baggage associate with URNs has to be lost so I'm\n>using a new name.\n\nHm, assertion exchange sounds very much like what feature negotiation\ndoes.  For feature negotiation, the common reference terms are feature\ntags.\n\n>I think we need to kick the requirements space arround a bit,\n>get a feel for the constraints on implementation and then look\n>at architecture again.\n\nWe have been kicking the requirements space on this list, and on the\ncontent negotiation subgroup mailing list, for more than a year now.\nSome of the concepts in the conneg draft have been kicked around since\nat least 1993.  I don't think any more kicking can give us a much\nbetter feel.\n\nWe cannot look much further from where we stand now, and we definitely\ncannot see the final destination.  We have a draft spec that plots the\nnext few steps over the horizon.  Now is the time to finish the spec\nand start walking.\n\n>                Phill\n\nKoen.\n\n\n\n", "id": "lists-010-6954649"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Dave Raggett:\n% What if we could define a simple file format for describing the\n% feature set for a given user agent, in analogy to the way robots.txt\n% emerged as a useful convention? \n[...]\n% It may also be useful for vendors to say that their user agent\n% emulates, let's say Netscape Navigator 2.0. It would be a win if\n% the browser could send this as an additional HTTP header in the\n% request, e.g. \"UA-Emulates:\". This would reduce the size of the\n% UA profile database and reduce the hit taken by the first visitor\n% with the this ua.\n\nPhill:\n\n: I see version number on MIME types as a part of a solution to this\n: problem. This allows a \"low watermark\" to be established in a cheap\n: and convenient manner. If a browser advertises text/html; version=3.2\n: then it had better comply with the standard. The version numbering\n: scheme could even be extended to provide for beta drafts etc, for\n: example text/html; version=3.2b24 would refer to W3 Consortium \n: draft number 24. Support for 3.2b24 would imply that all features of\n: 3.1 were supported, but not that features of 3.2b23 were necessarily\n: present - allowing specs to loose features rather than simply\n: accumulate them.\n[...]\n: Rather than dribble\n: them out there would be an interest in collecting together a\n: reasonable package of enhancements and deliver them as a collection\n: rather than the current trend of \"dribbling\" them out.\n\nProbably the best thing, at least in a perfect world, is that vendors\nfreeze every now and then - probably more \"then\" than \"now\", in order \nnot to have too many versions... their extension, so that people can\nexploit them in a UA-Emulate: header, something like\n\nUA-Emulate: Netscape 3.0a, MSIE 3.0, W3C HTML 3.2\n\nIf the emulation is not perfect, the server should not care :-) It's up\nto the browser to let the user modify the Emulate string.\n\nThis would leave the User-Agent a pure informational header line, and\nallow people to write \"browser-enhanced\" pages if they want to.\n\nBut of course this would mean that specifications are to be released (not\nnecessarily through a DTD, but anyway in a usable way)\n\nciao, .mau.\n\n\n\n", "id": "lists-010-6965945"}, {"subject": "Re: Sticky stuff", "content": "Specialized compression schemes always beat general ones, usually hands\ndown, as they know much more about the content of messages than a general\none can.  The algorithms in modems can't use the best compression algorithms\nin any case, due to constraints on modem's behavior.\n\nFor example, many compression schemes only compress on a character by\ncharacter basis; in HTTP's case, you have a dictionary for long words,\nreducing a string in the 8-10 byte range down to a single byte.\n\nSo arguing that modem compression makes protocol compression meaningless\nis almost always incorrect (example; TCP header compression, that knows\nabout the details of TCP, gets about a 7:1 compression; compression in\nmodems is at best in the 2-3X range).\n\nHowever, I agree with the sentiment that getting real data is a good idea,\nthough it can be work to get the data.  To encourage real data, read on...\n\nI will assert that Paul's scheme over modems with data compression\nwill beat the modem alone by at least a factor of two in savings of bytes\nactualy transmitted...  These should be real requests, including Accept:\nand similar protocol features, to qualify.\nFirst person to prove me wrong with running code \nand real data gets $50.  At worst, I'll learn something about how HTTP\nworks...\n- Jim Gettys\n\n - Jim Gettys\n\n\n\n", "id": "lists-010-6975881"}, {"subject": "Entity Tag", "content": "I am in the process of designing an HTTP 1.1 server which  provides\n JPEG Images when a GET method with a certain URL is received, but am unsure\nabout the use of entity tags.  Should an entity of this type respond with an ETag\nfield ? In turn, should I support If-Match/If-None-Match? If so ,How?   \n\n\n\n\n", "id": "lists-010-6984625"}, {"subject": "RE: Entity Tag", "content": "Sorry , I sent this message to the wrong group.......\n\nI am in the process of designing an HTTP 1.1 server which  provides\n JPEG Images when a GET method with a certain URL is received, but am unsure\nabout the use of entity tags.  Should an entity of this type respond with an ETag\nfield ? In turn, should I support If-Match/If-None-Match? If so ,How?   \n\n\n\n\n", "id": "lists-010-6992010"}, {"subject": "Re: Sticky stuff", "content": ">Specialized compression schemes always beat general ones, usually hands\n>down, as they know much more about the content of messages than a general\n>one can.  The algorithms in modems can't use the best compression algorithms\n>in any case, due to constraints on modem's behavior.\n\nAgreed, however the question which I was raising was whether \ncompressing the body of the message was more relevant than\ncompressing the headers. Modem compression may well be OK(ish)\nfor HTML but most internet lines don't have compression.\n\nI think that the \"sticky headers\" issue should be considered\nin conjunction with the user-agent capabilities issue. It is\nthe capabilities of the user agent which represent the bulk\nof the headers - or rather did when Accept headers were 1.4K.\n\nAs noted in Jim's other message URLs etc are very compact \nmechanisms for including an arbitrary quantity of data by\nreference. I think that the security issue has to be considered.\nE.g consider that the Republicrat home page uses background\nMPEGs extensively if the Naviplorer browser is used. A \nDempublican attempts to sabotage this by connecting to the \nRepublicrat site and downloading a bogus set of capabilities\nfor Naviplorer.\n\nI favour mechanisms that include some deterministic mechanism\nfor forming a URI from the referent. E.g. using MD5/SHA to\nform the identifier. The recipient can then verify that the\nreferent is correct even if it is not received from an\nauthoratative source. This also neatly avoids various\nkludgey solutions such as requiring the browser capabilities to \nbe downloaded from the browser vendor's site - somethin that might\nnot be possible on an intranet and something that would\nlimit the number of tags possible.\n\n\nNote also that there is a degree of convergence between Paul's\n\"session\" concept and the \"referenced headers\" concept.\n\nThere may be a degree of unwillingness to accept this scheme\namongst people who have written multiple process servers on poorly\ndesigned operating systems with inefficient process-process\ncommunication.\n\n\nPhill\n\n\n\n", "id": "lists-010-6999264"}, {"subject": "Nonnumeric feature tag", "content": "The need for non-numeric feature tags has recently been discussed on\nthe list.  This is my proposal for adding them to the content\nnegotiation draft.\n\nThe mechanism below allows a feature tag to have multiple values,\nwhich need not be numeric.  By allowing for multiple values, you can\nsay something like\n\n Accept-features: paper=a4, paper=a5\n\nif you can handle two different paper sizes.  \n\nWhat follows are updated versions of Sections 6.2 and 6.3 of the\ncontent negotiation draft (see http://gewis.win.tue.nl/~koen/conneg/).\nChangebars were added by hand.\n\n----snip----\n\n6.2 Accept-Features header\n\n   The Accept-Features request header can be used by a client to give\n   information about the presence or absence of certain features.\n\n|      Accept-Features = \"Accept-Features\" : \n|                  #( [ \"!\" ] ftag [ \"=\" value ]\n|                   | ftag \"=\" \"{\" value \"}\"\n|                   | ftag \"<=\" number\n|                   | \"*\"\n|                   )\n|\n|      value  = token\n\n       number = 1*DIGIT\n\n|  Values are case-insensitive.   An example is:\n|\n|      Accept-Features: blex, !blebber, colordepth<=5, !screenwidth,\n|                 UA-media={stationary}, paper=a4, !paper=a0, *\n|\n|  The different syntactical forms have the following meaning:\n|\n|     ftag     ftag is present, but with no values\n|     \n|     !ftag    ftag is absent\n|\n|     ftag=V   ftag is present with the value V\n|\n|     !ftag=V  ftag is present, but not with the value V\n|   \n|     ftag={V} ftag is present with the value V, and not with any\n|              other values\n|\n|     ftag<=N  ftag is present with the numeric values from 0 up to\n|              and including N, and not with any other values\n|\n|     *        makes true all feature predicates (Section 6.3) which\n|              were not assigned truth values by other elements of the\n|              header\n\n   Absence of the Accept-Features header in a request is equivalent to\n   the inclusion of\n\n       Accept-Features: *\n\n\n6.3 Feature predicates\n\n   Feature predicates are used in the features attribute of a variant\n   description.\n\n|     fpred = [ \"!\" ] ftag [ \"=\" value ]\n\n   Examples feature predicates are\n\n|     blebber, !blebber, paper=a4, colordepth=5, !blex=54\n\n   The network negotiation algorithm can compute the truth value of a\n   feature predicate by using the contents of the Accept-Features\n   header of the current request.\n\n|  The truth value of a predicate is assigned as follows, depending on\n|  its form:\n|\n|     ftag     true if the feature is present, false if the feature is\n|              absent according to the Accept-Features header\n|\n|     !ftag    true if the feature is absent, false if the feature is\n|              present according to the Accept-Features header\n|\n|     ftag=V   true if the feature is present with the value V,\n|              false if the feature is not present with the value V\n|              according to the Accept-Features header\n|\n|     !ftag=V  true if the feature is not present with the value V,\n|              false if the feature is present with the value V\n|              according to the Accept-Features header\n|\n|  If the Accept-Features header does not contain sufficient\n|  information to assign a value using the above rules, then the value\n|  is true if there is a \"*\" in the Accept-Features header, false\n|  otherwise.\n\n   As an example, the header\n\n|      Accept-Features: blex, !blebber, colordepth<=5, !screenwidth,\n|                UA-media={stationary}, paper=a4, !paper=a0, *\n\n   makes the following predicates true:\n\n|      blex, colordepth=4, !colordepth=6, colordepth, !screenwidth,\n|      UA-media=stationary, paper=a4, !paper=a0, paper=a5\n|      frtnbf, !frtnbf, frtnbf>=4, frtnbf<4\n\n   and makes the following predicates false:\n\n|      !blex, blex=0, blebber, colordepth=6, !colordepth,\n|      screenwidth, screenwidth=640, !screenwidth=640\n|      !UA-media=screen, paper=a0\n\n\n----snip----\n\nNote that the mechanism above allows a smooth transition between use\nof `paper' as a single-value tag to its use as a multi-value tag.\nThis is crucial because, as a general rule, the first feature tags for\nan area of negotiation will be registered _before_ this area is well\nunderstood.  If there were no transition to use as a multi-value tag,\nthen an initial incorrect decision to create something as a\nsingle-value tag could block progress far to easily.\n\nAlso note that the `<=' has moved from the feature predicate syntax to\nthe Accept-Features header syntax.  With both '<=' and '=' being legal\nin the feature predicate syntax, I think that a CGI programmer could\ntoo easily make the error of writing\n\n  {features colordepth=8}\n\ninstead of the correct\n\n  {features colordepth<=8} .\n\nWith '<=' in the Accept-Feature header, it will be up to user agent\nauthors to make the mistake of generating\n\n  Accept-features: colordepth=8, *\n\ninstead of the correct\n\n  Accept-features: colordepth<=8, *\n\nand user agent authors are generally in a better position to find the\nerror through testing.\n\nA final note: I'm open for suggestions for improving the \"ftag={V}\"\nnotation.  It is a bit yucky, but at least has some mnemonic value\n(being the notation for a one-element set).  Other things I considered\nwere\n\n  ftag:=V  ftag@V  ftag==V  ftag:V  ftag=V!\n\n\nKoen.\n\n\n\n", "id": "lists-010-7008545"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "    If a user agent profile is a URL, rather than some silly static\n    database, there is no monopoly involved; a server would just\n    fetch the profile from the URL when first talking to the client,\n    and cache it.\n    \n    Unless such a cache of profiles were tiny, it is very likely that\n    any significant server will have profiles for almost any user agent\n    in use (and you can roll your own, anyone who has the ability to\n    provide a web page, and since this seems to be standard these days\n    with Internet accounts by ISP's, this means everyone).\n    \n    This is the scheme Simon Spero suggests in his NG work, and it looks\n    like a fine one to me.\n\nIt has some obvious advantages (indirection is often a Good Idea\nin computer systems design), but I can think of several problems\nthat could make it difficult to implement universally.\n\nHow would this work, for example, in an isolated intranet (one\nin which, by policy, no access to the Internet is allowed)?  Would\nthe intranet's operators have to mirror the set of user agent\nresources internally, and perhaps rebind the URLs for the browsers\nused internally?  Or build a translation table for the internal\nservers to use?\n\nHow would this work in a flakey Internet (supposing that, in\nspite of our current efforts, Internet reliability gets worse\nrather than better)?  I.e., a server has a less-than-100% chance\nof actually reaching the user-agent-profile URL?\n\nWhat are the security implications of trusting the Internet\nto deliver the correct user agent profile?  (Not that we necessarily\ndo any better today!)\n\nAnyway, I would guess that if we can come up with a standard\nencoding for the u-a-p resource pointed to by a u-a-p URL (and\nwhich would be a prerequisite for any such scheme) then with\na little more effort we might be able to come up with compressed\nencoding that could be transmitted in the request headers without\nmany more bytes that it would take to transmit the u-a-p URL.  And\ntransmitting it in-band does solve the problems that might arise\nfrom indirection.\n\nSo perhaps the first order of business is to think about what the\nu-a-p would actually encode, before thinking about what the most\nefficient way to transmit it might be.  After all, as you pointed\nout in another recent message, special-purpose encodings can yield\nimpressive compression ratios.\n\n-Jeff\n\n\n\n", "id": "lists-010-7020609"}, {"subject": "Re: Next draft of HTTP/1.1 spec..", "content": "I submitted Internet draft 07 today to the Internet drafts directory; \nyou can get text versions from:\n\nhttp://www.w3.org/pub/WWW/Protocols/\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-07.txt\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-07.txt.gz\n\nNote that Microsoft Word and Postscript are not yet available, (I'll\nmake them available later this week); I broke my toe late last week and\nhave been hobbling around between doctor's appointments and have to keep\nmy foot up, which has slowed me down a bit.\n\nI'll post another note when the Word and Postscript versions are available.\n\nAll changes are minor in nature addressing last call comments; \nI posted diffs last Friday of the changes.\n- Jim\n\n\n\n", "id": "lists-010-7030596"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">----------\n>From: Paul Leach\n>Subject: RE: New document on \"Simple hit-metering for HTTP\"\n>\n>There are, in fact, some of each kind of providers and adveritisers. We\n>hypothesize that our proposal is at a good place in the (simplicity,\n>deployability, demographic information). \n\nMeant to say:\n>We hypothesize that our proposal is at a good place in the (simplicity,\n>deployability, demographic information) space.\n                                                                        \n       ======\n> \n\nPaul\n\n\n\n", "id": "lists-010-7038097"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "jg@zorch.w3.org writes:\n > If a user agent profile is a URL, rather than some silly static\n > database, there is no monopoly involved; a server would just\n > fetch the profile from the URL when first talking to the client,\n > and cache it.\n > \n > Unless such a cache of profiles were tiny, it is very likely that\n > any significant server will have profiles for almost any user agent\n > in use (and you can roll your own, anyone who has the ability to\n > provide a web page, and since this seems to be standard these days\n > with Internet accounts by ISP's, this means everyone).\n > \n > This is the scheme Simon Spero suggests in his NG work, and it looks\n > like a fine one to me.\n > - Jim\n > \n > \n\nI definitely like the level of indirection implied by using a URL for\nthis, but I don't like the reliance it puts on a specific \"profile\nserver\" being available.  It doesn't sounds like a solution that\nscales all that well.  Let's not underestimate the number of servers\nthat may exist in the future.  Can we \"prove\" that the number of\nservers and number of different UAs is sufficiently small to allow\nthis kind of scheme to work?\n\nInstead of using a URL, could we not arrange to use the USER_AGENT\nstring as a key to databases maintained by replicated servers, using\nsome standardized URL-based addressing scheme to fetch the profile\ndocument associated with a specific user agent?  Then it would be a\nmatter of server configuration to tell it where to look for this database.\nIt has a bit of the flavor of DNS.\n\n--Shel\n\n\n\n", "id": "lists-010-7048898"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "The difficulty I believe is in the codification of features in a way that\nwill satisfy everybody (or mostbodies). I have two shipping products\nwhich are fairly complex interactive applications built using WWW\ntechnology. In each case I have discovered a need to know at a very\ndetailed level how a browser implements certain specific characteristics\nin order for my applications to interact correctly with the user.\n\nA specific concern is how the browser handles a selection list when \nthe HTML doesn't pre-select an option. One major browser follows the\nuser's markup the other follows the HTML spec and defaults the first.\nAnother 2nd tier browser ignores the selected attribute.\n\nGetting this level of detail coded is necessary but perhaps impossible\nto achieve. Certainly it becomes difficult to accept such detail from\nunknown sources since getting it wrong can have a very negative impact.\n\nI think any realistic database approach will need to incorporate chains\nof 'authority' and 'trust' much like security CAs do.\n\nSecondly, the coding format will need to be very flexible and easy to\nextend with minimal (or no) global registration required. Part of \neach database will need to be a description of which parameters the\nowner of the database certifies/tests/etc. Which were inherited from\nanother source, etc.\n\nMerging feature coding from multiple sources is a mandatory feature.\nFor example, I could envision an HTTP extention like the basic\nauthentication which would challenge a new unknown browser to identify\nits public feature set ... those features the published acknowledged/\nclaimed and later the local DB support person might need to modify the\nfeature set with additional parameters, etc.  Or a server might check\nthe unknown browser's features with a friendly cooperating server and\nlearn the base set but still need additional details of the nature\nI described above.\n\nFinally there are a whole bunch of characteristics which describe an\nindividual installation of a browser product which I believe Koen has\nalready noted should fit well under content negotiation.\n\nTo take this to the second order then, when we learn that a browser has\na particular plug-in installed (via content negotiation) we may then need\nuser agent like version identification for the plug-in so that our\ndatabase of extended characteristics can be checked.\n\nI think this is an important but difficult problem set.\n\nDave Morris\n\n\n\n", "id": "lists-010-7058374"}, {"subject": "RE: Conventions for Sharing User Agent Profile", "content": "definition of a u-a-p:\n\na readable and understandable file transmitted via. HTTP\nto a client .... it is similar in content\nif not identical to the equivalent HTTP headers which it purports to\nencapsulate\n\nthere should never be a NEED to use a u-a-p for a browser\n...although it will be more efficient\n\n??????????????????????????????????????????????????????\n\n>----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Sent: Monday, August 12, 1996 12:52 PM\n>To: jg@zorch.w3.org\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: Conventions for Sharing User Agent Profiles \n>\n>    If a user agent profile is a URL, rather than some silly static\n>    database, there is no monopoly involved; a server would just\n>    fetch the profile from the URL when first talking to the client,\n>    and cache it.\n>    \n>    Unless such a cache of profiles were tiny, it is very likely that\n>    any significant server will have profiles for almost any user agent\n>    in use (and you can roll your own, anyone who has the ability to\n>    provide a web page, and since this seems to be standard these days\n>    with Internet accounts by ISP's, this means everyone).\n>    \n>    This is the scheme Simon Spero suggests in his NG work, and it\n>looks\n>    like a fine one to me.\n>\n>It has some obvious advantages (indirection is often a Good Idea\n>in computer systems design), but I can think of several problems\n>that could make it difficult to implement universally.\n>\n>How would this work, for example, in an isolated intranet (one\n>in which, by policy, no access to the Internet is allowed)?  Would\n>the intranet's operators have to mirror the set of user agent\n>resources internally, and perhaps rebind the URLs for the browsers\n>used internally?  Or build a translation table for the internal\n>servers to use?\n>\n>How would this work in a flakey Internet (supposing that, in\n>spite of our current efforts, Internet reliability gets worse\n>rather than better)?  I.e., a server has a less-than-100% chance\n>of actually reaching the user-agent-profile URL?\n>\n>What are the security implications of trusting the Internet\n>to deliver the correct user agent profile?  (Not that we necessarily\n>do any better today!)\n>\n>Anyway, I would guess that if we can come up with a standard\n>encoding for the u-a-p resource pointed to by a u-a-p URL (and\n>which would be a prerequisite for any such scheme) then with\n>a little more effort we might be able to come up with compressed\n>encoding that could be transmitted in the request headers without\n>many more bytes that it would take to transmit the u-a-p URL.  And\n>transmitting it in-band does solve the problems that might arise\n>from indirection.\n>\n>So perhaps the first order of business is to think about what the\n>u-a-p would actually encode, before thinking about what the most\n>efficient way to transmit it might be.  After all, as you pointed\n>out in another recent message, special-purpose encodings can yield\n>impressive compression ratios.\n>\n>-Jeff\n>\n>\n\n\n\n", "id": "lists-010-7069110"}, {"subject": "HTTP State Management Mechanism draft questio", "content": "Gentlemen,\n\nIn my review of your document \"HTTP State Management Mechanism\" dated July\n19, 1996, I have come across two points that I find would impede commerce in\nweb usage by eliminating state retention mechanisms.\n\nIn the next to last paragraph of section 7.1, it is suggested that at the\nconclusion of a user agent session, the user agent inquire of the operator\nif the state information should be retained.  The proposed default value is\n\"No\".  This makes it more costly in user agent time and server resources to\nreestablish the user agent's state.  This raises privacy concerns as it\nmakes it mandatory for one web service to maintain a central database of\nusers' state information.  What is the reason for this?\n\nIn section 4.3.5 eliminating the ability of having \"unverifiable\"\nredirection impairs the ability of the web service (chosen by the user agent\noperator) to engage in using the services of a third party for advertising,\ncontent building, download specialized \"plugins\" or other usage.  This hurts\nweb commerce.  Why is this proposed?\n\nThe implementation of these two items would have two deleterious effects:\nFirst, they would decrease the privacy and anonymity of web users by storing\ndata about user agents / individuals on servers, not at the user agent,\nunder user agent / individual control.  Additionally, login and password\nexchanges would be required, allowing identity spoofing.\n\nSecond, they would increase the storage, processing and communication\nrequirements of web servers.  Re-establishing inter-session states would\nrequire server operators to send and receive extra messages (logins and\npasswords), extra disk space to keep the personal login, password, and state\ninformation, and extra processing to operate a state recover mechanism.\n\nI understand that the IESG plans to make a decision about the proposed\n\"HTTP State Management Mechanism\" document and has requested that all\ncomments be delivered by August 20th.  Please reply in as complete and\ntimely manner as possible so that I may most effectively deliver my\ncomments.\n\nThank you for your prompt attention.\n\nBest regards,\n\nPaul Grand\n\n_________________________________________________________________________\n Paul Grand                              e-mail: pgrand@netcount.com\n Chairman                                   WWW: http://www.netcount.com\n NetCount LLC                             Voice: (800) 700-0638\n 1645 N. Vine Street,             International: (213) 848-5700\n Los Angeles, CA 90028                      FAX: (213) 848-5499\n_________________________________________________________________________\n\n\n\n", "id": "lists-010-7081718"}, {"subject": "web directory listin", "content": "Hi everybody,\nI am having troube getting a web directory listing.  Sometimes I get \na default web page instead of a listing.  Is there a way of always \ngetting a full listing of the files in a web page's directory.  \nThank You,\n-Anwar\n\n\n\n", "id": "lists-010-7091597"}, {"subject": "Re: Sticky stuff", "content": "Paul Leach:\n>>From:  koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>>>2. It was taken between proxy and server, \n>>\n>>This is because the internet backbone links are the bottleneck, not\n>>the LAN between your proxy and your user agent.  Who cares if you get,\n>>say, 30% savings in web traffic on the LAN?  In this game, it is\n>>backbone savings that count.\n>\n>Not everyone has a LAN link between user agent and proxy. Some small\n>number of people dial in to their proxy, which is owned by  their\n>service provider. Say order 10 million or so, with providers like AOL\n>and MSN.\n\nI'm not a specialist in modem technology, but AFAIK modems offer you a\nconstant amount of bandwidth both ways.  So savings on the request\nstream will not get you more bandwidth on the response stream, which\nmeans that the savings matter even _less_ than on a LAN.  My wait\nchain calculations will _over_estimate the speedup in the modem case,\nassuming that the modem is the bottleneck.\n\nAs for the high RTT on some of Digital's internal links: RTTs are\nirrelevant for this discussion.  If anything, they will make the\nsavings less noticable.  If Digital's internal links were highly\n_saturated_, that would be another thing.\n\n[...]\n>>And the modem hooked to the ordinary telephone line will probably do\n>>data compression, so you would gain little extra with sticky headers,\n>>I believe.\n>\n>In your model, a 200 byte request would be reduced by useing sticky\n>headers to 40 bytes, and then the 40 bytes would be compressed to (say)\n>20 bytes (assuming a 2-1 compression ratio). Without sticky headers, and\n>the same compression ratio, requests would be 100 bytes. The savings\n>from sticky headers is still the same 80%. \n\nThe compression ratio varies depending on what you try to compress.\nHave you ever gzipped a httpd logfile?  If the same effect happens\nwith modem compression, Jim will learn something about how HTTP works\nvery soon.\n\n>Paul\n\nKoen.\n\n\n\n", "id": "lists-010-7098279"}, {"subject": "Re: Sticky stuff", "content": "hallam@Etna.ai.mit.edu:\n>[...] however the question which I was raising was whether \n>compressing the body of the message was more relevant than\n>compressing the headers.\n\nHere is a version of the answer:\n\n - sticky headers: 1-4% traffic savings\n - compressing text/html and text/plain entities in relayed\n   responses: 20-45% traffic savings\n\nNow, why is there so much fuss over sticky headers?  Ever since Accept\nheaders dropped below 1.4K, they are a solution in search of a\nproblem.  If this group were to produce a sticky header spec, the spec\nwould only distract vendors from implementing optimizations which are\nactually worth the effort.\n\nKoen.\n\n\n\n", "id": "lists-010-7108356"}, {"subject": "Re: Sticky stuff", "content": "Koen Holtman wrote:\n> \n> hallam@Etna.ai.mit.edu:\n> >[...] however the question which I was raising was whether\n> >compressing the body of the message was more relevant than\n> >compressing the headers.\n> \n> Here is a version of the answer:\n> \n>  - sticky headers: 1-4% traffic savings\n>  - compressing text/html and text/plain entities in relayed\n>    responses: 20-45% traffic savings\n> \n> Now, why is there so much fuss over sticky headers?  Ever since Accept\n> headers dropped below 1.4K, they are a solution in search of a\n> problem.  If this group were to produce a sticky header spec, the spec\n> would only distract vendors from implementing optimizations which are\n> actually worth the effort.\n\nI would like to chime in here and support Koen's position.\nAs an implementor I think my time would be much better\nspent on other optimizations than sticky headers.  Perhaps\nin the future, if HTTP still looks any thing similiar\nto what it does today, we should reconsider them.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n", "id": "lists-010-7116600"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": ">Anyway, I would guess that if we can come up with a standard\n>encoding for the u-a-p resource pointed to by a u-a-p URL (and\n>which would be a prerequisite for any such scheme) then with\n>a little more effort we might be able to come up with compressed\n>encoding that could be transmitted in the request headers without\n>many more bytes that it would take to transmit the u-a-p URL.  And\n>transmitting it in-band does solve the problems that might arise\n>from indirection.\n\n\nPulling together Jeff's points of security and downloading in \nan intranet I see the specific weakness of Jim's scheme being\nthat it uses URLs when in this particular case we could use\nsomething that has the properties of a URN.\n\nLet us start by defining for the sake of argument an Object\nIdentifier URI which has a prefix OID:. We will describe a\nspecific subcategory of these OIDs, specifically the set\nof identifiers which are a deterministic function of their\ndesignata, in this case via an unbiased one way function\nsuch as SHA-1.\n\nSo our \"URN\"-ish identifier which for the sake of avoiding\nconfusion with the URI requirements doc I will call a URS\n(Universal Resource Sign) is something like :-\n\nOID:SHA-khgq234o8t6quirgq2872135\n\nWhich refers to the headers h, where h = \n\n\"Accept: text/plain, text/html, image/gif, image/jpeg\nUser-Agent: Linemode/64.5\n\"\n\nand  BASE64(SHA(h)) =  \"khgq234o8t6quirgq2872135\"\n\nThe typical usage of this technique would be as follows :-\n\n\nGET http://foom.com/index.html http/1.2\nDate: <whatever>\nInclude: OID:SHA-khgq234o8t6quirgq2872135\n\n\nWhich would be equivalent to :-\n\nGET http://foom.com/index.html http/1.2\nDate: <whatever>\nAccept: text/plain, text/html, image/gif, image/jpeg\nUser-Agent: Linemode/64.5\n\n\nNow say that the server does not have the mapping for \nOID:SHA-khgq234o8t6quirgq2872135 avaliable. In that case it might\neither:-\n\n1) Return a default value anyway \n- especially usefull if the content is invariate wrt the\nheaders included by reference.\n\n2) Ask for the expansion of the headers.\n\n3) Say that it doesn't have time to play this game and that the\nclient should respond in traditional manner.\n\nIn the later case we might see :-\n\n3xx Don't Follow You\nServer: frob/54.3\n\n\nAnd the Client would then send some message to provide the\nappropriate binding or linkage, eg :-\n\n\nPOST OID:SHA-khgq234o8t6quirgq2872135 http/1.2\nDate: <whatever>\nContent-Type: application/http\n\nAccept: text/plain, text/html, image/gif, image/jpeg\nUser-Agent: Linemode/64.5\n\n\nPerhaps the method in this case needs to be adjusted \nsomewhat... Its not quite a POST, more a \"RETRY\" type\nof affair. \n\n\n\nNote that in all this we still need to have some way of avoiding the\ninitial loosing trip problem. Unless we know that we are dealing with \na 1.2 server we have to start talking 1.0 and tentatively upgrade.\n\nWho is interested in a DNS modification to provide for server profiles?\n\nI see that we could do some really interesting stuff with a very\nsimple scheme. Here I would suggest that we go just a little bit\nfurther than the current DNS hacks suggested and provide a WebX\nrecord analogous to the MX record which contains a mapping table\nfrom URIs to URIs :-\n\n[Attached to record www.w3.org]\n\nWEBX http://www.w3.org/* http://18.23.1.165 1 version=1.2\nWEBX http://www.w3.org/* http://18.23.1.120 2 interval=30sec version=1.2\nWEBX http://www.w3.org/* http://18.23.1.122 2 interval=1day version=1.2\nWEBX http://www.w3.org/* http://128.34.2.11 3 version=1.1\n\nThis means that there are four sites serving http:// type URIs.\nServer 18.23.1.165 has firstness quality and uses protocol\nhttp/1.2, Servers 18.23.1.120 and 18.23.1.122 have secondness\nand also serve 1.2, 128.34.2.11 has thirdness and is version 1.1.\n\nThe firstness, secondness and thirdness are inspired by the\ntrichotemy of Pierce. Basically this divides the relationship of\na sign to its designatum into three categories. In the first\ncategory there is a direct connection between the sign and the \nthing designated by the sign. In the second category there is \na conventional relationship between the sign and the designata,\nin the third category the sign relates to another sign. Loosely\ninterperting these abstract categories in terms of HTTP we arrive\nat three different qualities of cache scheme :-\n\nFirstness:The server is either the authoratative source\nfor the resources or it is guaranteed to behave in a\nsmantically equivalent manner (e.g. two servers operating\noff a mirrored filestore with exact semantics)\n[Alternatively the relationship of the resource to the\nURL is either static or otheriwse known in advance]\n\nSecondness:The server is guaranteed to behave as the \nauthoratative source would have done within a specified\ntime interval. This is equivalent to a server which recieves\nchange notifications from the originating host or some\nother host with quality firstness.\n\nThirdness:There was at some time in the past a relationship\nbetween the two. This is CERN proxy style caching.\n\nThe advantage of this division is that it maps very directly to\nthree classes of server implementation with a range of implementation\ncosts and functionality. It would be futile to try transaction\ncontrol on a server with thirdness quality. On the other hand data\nfrom a server with secondness quality and a time constant of 30sec\nis likely to be as good as a direct connection from most people's\npoint of view.\n\n\nI know that this is not the HTTP protocol per se but it is a method\nof addressing certain problems that are otherwise hard or impossible\nto avoid. I think we should think of the Web as a whole and not just\ntowards small pieces that happen to fit in particular pockets.\n\n\nPhill\n\n[P.S., Yes you could call this a URN scheme if you really\nwanted but note that the scheme still defines a proceedure for\nretreiving a resource and hence is in fact a URL. The main\nconcern of those asking for a naming scheme is addressed - the\nreferent is insensitive to the actual server used [This could be\nimproved upon by use of an \"index\" server offering redirections\nin response to requests\".] On the other hand certain requirements\nproposed for URNs which were cited as essential are not met. \nIndeed I suspect that no mechanism which involves a resolution\nmechanism can ever meet those requirements. A name is merely a\nsign that has a conventional assosiation with its designatum,\nthe binding occurs from the fact of common usage within a \ncommunity ]\n\n\n\n", "id": "lists-010-7125421"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "David W. Morris:\n>\n>The difficulty I believe is in the codification of features in a way that\n>will satisfy everybody (or mostbodies).\n\nThis is a very general problem.  Quoting from the Montreal minutes:\n\n Harald Alvestrand pointed out that the group does not have a unified\n model; we have a desire to create a language to describe what the user\n wants and a language to describe what the server has and we don't have\n a unified model for bringing those together; until that model comes\n together--neither is going forward.\n\nAs far as I know, the only complete specification of such unified\nlanguages is in the transparent content negotiation draft.  One of\nthese languages (Accept headers including Accept-Features) could be\nused for user agent profiles.\n\nI'm always wondering if (transparent) content negotiation can't be\nmade simpler, and the idea of using the power of indirection to\nsimplify the negotiation process is exciting.  But sadly I can't see a\nway to use this power to simplify the whole thing (yet).\n\n[....]\n>Finally there are a whole bunch of characteristics which describe an\n>individual installation of a browser product which I believe Koen has\n>already noted should fit well under content negotiation.\n\nYes, I see this as the main problem with UA profiles: they cannot\ncontain _all_ preferences and capabilities (which include things like\nhaving viewers for unusual MIME types, the quality factors of the MIME\ntypes, and the quality factors of the accepted languages), because if\nthey would contain everything, you would basically get a different\nprofile file for each user (except maybe on the LANs of boring\ncompanies), and the whole profile caching scheme breaks down horribly.\n\nAnd if UA profiles can't cover all preferences and capabilities, then\nyou still need a second negotiation mechanism to handle the things not\nin the profile.\n\nSo UA profiles won't solve all of our problems.  They could at best be\na short-term stopgap measure to reduce the pain before we have a\ngeneral mechanism.\n\nBut we already have a complete draft spec for a general mechanism,\ntransparent content negotiation, which will do much more than\nnegotiate on UA features alone.  The general mechanism could be\nfinished well before the stopgap!\n\nI estimate that to go from here to a complete internet draft for\nprofile sharing would take months at least.  The addition of a third\nparty in the negotiation process generates so many\nsecurity/reliability/authentication/privacy problems that the fast\nproduction of a simple draft seems out of the question.\n\n>Dave Morris\n\nKoen.\n\n\n\n", "id": "lists-010-7139550"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Paul Leach:\n>>From:  koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>>\n>>I can't see much difference, as far as efficiency is concerned,\n>>between your proposed use of the Vary header and max-age=0 cache\n>>busting.  Both as are inefficient, but at least max-age=0 is\n>>inefficient in a non-complex way.\n>\n>I think you need to read about how Vary works.\n\nI know how vary works.  I expect that service providers would send\n\n  Vary: Accept, Accept-Language, User-Agent, Referer, From\n\nor even\n\n  Vary: *\n\nunder your scheme.  The latter is equivalent to max-age=0. Even the\nformer this gives you so much variance that cachability is effectively\neliminated.  Also, the it is at least questionable whether cache\nimplementers will actually implement all optimisations allowed by\nVary.\n\n>Just to get terms straight: I define \"max-age=0\" as cache-busting.\n>Better than no-cache, but still cache busting.\n\nThen our terminology is in sync.\n\n>>\n>>To do a good job at giving demographers the complete request data\n>>(including client IP address) they seem to want, a completely\n>>different mechanism is needed.\n>\n>Aren't you arguing out of both sides of your mouth here.\n\nNo, I changed my mind after reading the messages from Erik Aronesty.\nI no longer think that providers who *only* want hit counts are in the\nmajority.\n\n> In the last\n>message, you claimed that providers are too unsophisticated to know the\n>difference between counts of conditional and unconditional GETs.\n\nFor the record, I claimed that the *customers* of providers would not\nknow the difference.\n\nI have outlined a number of problems in your proposal.  The bottom\nline is that I don't think it will work in its current form.  I'm\nstill moderately interested in getting a working lightweight\ndemographics mechanism, and will wait and see if the next draft\nimproves things.\n\nKoen.\n\n\n\n", "id": "lists-010-7149982"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "My actual proposal ended up not using URLs to fetch the data; instead, it\nused the obvious facts that (1) The client knows it's UA profile and (2)\nthe server is reachable from the client (otherwise the whole thing is\npretty pointless).\n\nThe conclusion - if the server doesn't have the profile, the client should\nsend it. Makes even more sense with NG or interleaved Persistent\nconnections...\n\nSimon\n\n\n\n", "id": "lists-010-7160763"}, {"subject": "Re: Sticky stuff", "content": "Yeah - sticky headers take roughly the same amount of work to implement on\nthe client side as SCP, and aren't nearly as big a win for http-1 class\nprotocols.\n\nThe real wins from this sort of thing come when you go balls out to keep\nrequest/responses under a packet, and in many of those situations you\ndon't have a connection going anyway, so you need to go to cached\nprofiles. \n\nSimon\n\n\n\n", "id": "lists-010-7169738"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Sent: Monday, August 12, 1996 3:57 PM\n>To: Paul Leach\n>Cc: mogul@pa.dec.com; koen@win.tue.nl;\n>http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: New document on \"Simple hit-metering for HTTP\"\n>\n>Paul Leach:\n>>>From:  koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>>>\n>>>I can't see much difference, as far as efficiency is concerned,\n>>>between your proposed use of the Vary header and max-age=0 cache\n>>>busting.  Both as are inefficient, but at least max-age=0 is\n>>>inefficient in a non-complex way.\n>>\n>>I think you need to read about how Vary works.\n>\n>I know how vary works.  I expect that service providers would send\n>\n>  Vary: Accept, Accept-Language, User-Agent, Referer, From\n>\n>or even\n>\n>  Vary: *\n>\n>under your scheme.  The latter is equivalent to max-age=0.\n\nI think it's obvious that if the origin-server wants to know everything\nabout every request, then it has to make sure it sees every request, and\nthat the most efficient and obvious way to do that is with max-age=0. I\nthink that Vary: * would only obfuscate things.\n\n(Of course, the other thing the content provider wants is to scale to\nlots of clients, which means that they have to allow caching. So maybe\nthey'll accept some tradeoffs.)\n\nOur whole proposal was addressed at providers who were willing to accept\nless than total information in exchange for more efficiency.\n\n> Even the\n>former this gives you so much variance that cachability is effectively\n>eliminated.\n\nAnd that's why it won't be sent. This whole line of argument is\nattacking a straw man. If the origin-server wants this much info,\nthey'll just set max-age=0.\n\nFor the cases that our proposal tries to address, I think a provider\nthat will accept less than total info would be satisfied with more like\none of these:\n1. no Vary at all -- just raw stats on how many users see the page\n2. Vary:  Accept-Language, User-Agent -- find out which language users\nprefer and which browser they are using\n3. Vary: User-Agent -- find out which broswer they are using.\n\nEach variation would add an entry of maybe 100-200 bytes to store the\nselecting headers and pointer to the entry containing the entity-body,\nwhich will average about 10k bytes -- so about a 1-2% overhead per\nvariation.\n\nThey can get referrer info without the Referer header using the\ntechnique in section 7 (due to a suggestion from you, which I forgot to\nacknowledge -- for which I apologize, and will rectify in the next\nrelease).\n\n>  Also, the it is at least questionable whether cache\n>implementers will actually implement all optimisations allowed by\n>Vary.\n\nI don't understand this. There seems to me to be two logical\nimplementations of Vary: treat it as if it were max-age=0, or implement\nit fully. If they do it fully, then adding a counter to each selecting\nheader entry seems trivial. If you can explain another logical\nintermediate implementation, then maybe I could buy this argument, but\nuntil then...\n>\n>\n>>>\n>>>To do a good job at giving demographers the complete request data\n>>>(including client IP address) they seem to want, a completely\n>>>different mechanism is needed.\n>>\n>>Aren't you arguing out of both sides of your mouth here.\n>\n>No, I changed my mind after reading the messages from Erik Aronesty.\n>I no longer think that providers who *only* want hit counts are in the\n>majority.\n\nOK. That wasn't clear. BTW: in a private exchange, Erik agreed that the\nproposal will still give good enough IP address info. (The caches are\ntypically geographically close enough to the client, and all you *ever*\nget, even when cache-busting, is the IP address of the caches.)\n>\n>> In the last\n>>message, you claimed that providers are too unsophisticated to know the\n>>difference between counts of conditional and unconditional GETs.\n>\n>For the record, I claimed that the *customers* of providers would not\n>know the difference.\n\nYou mean the advertisers?\n>\n>I have outlined a number of problems in your proposal.  The bottom\n>line is that I don't think it will work in its current form.\n\nThat doesn't follow from your objections. I summarize your objections\nas:\n1. Too complicated\n2. Advertisers want more or different data than it collects, so it won't\nget used\n3. It will lower \"hit counts\" so providers will have no incentive to\ndeploy it\n4. If used in certain ways, if will be inefficient\n\nI think I've addressed all of these. Certainly \"doesn't work\" wasn't one\nof them -- a correct implementation will do what we claimed it would.\n\n>Paul\n\n\n\n", "id": "lists-010-7178585"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Simon Spero writes:\n > My actual proposal ended up not using URLs to fetch the data; instead, it\n > used the obvious facts that (1) The client knows it's UA profile and (2)\n > the server is reachable from the client (otherwise the whole thing is\n > pretty pointless).\n > \n > The conclusion - if the server doesn't have the profile, the client should\n > send it. Makes even more sense with NG or interleaved Persistent\n > connections...\n > \n > Simon\n > \n > \n > \n\nThe problem with this approach is that oddities of particular browsers\nmay not exist in a hard coded database built into the browser itself\nat the time it is released, but may only be recognized by third\nparties, after the fact.  The UA-characteristics database should be\nindependent of the UA itself.  I'm assuming here that this database\nmay contain fine detail about UA behavior that may even include bugs\nnoticed long after the release.  \n\n\n\n", "id": "lists-010-7195182"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Simon Spero:\n>\n>My actual proposal ended up not using URLs to fetch the data; \n\nSimon,\n\nI was not aware that you had an actual proposal.  My thoughts on how\nlong it would take to go from here to deploying such a mechanism could\nhave been too pessimistic.\n\nCould you post your proposal, or post a pointer to it?  What is your\nguess on how long it would take to extend your proposal into a\ncomplete internet draft?\n\nI'm particularly interested in what you propose to include in the UA\nprofile.  There is a tradeoff between covering a larger area of\nnegotiation and getting more `profile cache misses' with your scheme,\nand I'm wondering where you stand on this.  I guess you won't be\nputting `dimensions of the UA window in pixels' in the profile, but\nwhere do you stop exactly?  How many `profile cache misses' do you\nestimate under your proposal?\n\nBTW, I recently did some statistics on an apache user agent log file,\nand I counted 1673 different user agent strings (after any `via proxy\n...' stuff has been stripped off, but _with_ comments like\n`(Macintosh; I; 68K)').\n\n>Simon\n\nKoen.\n\n\n\n", "id": "lists-010-7203403"}, {"subject": "I-D ACTION:draft-ietf-http-v11-spec07.tx", "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                \n\nNote: This revision reflects comments received during the last call period.\n\n       Title     : Hypertext Transfer Protocol -- HTTP/1.1                 \n       Author(s) : R. Fielding, J. Gettys, J. Mogul, \n                   H. Frystyk, T. Berners-Lee\n       Filename  : draft-ietf-http-v11-spec-07.txt\n       Pages     : 153\n       Date      : 08/12/1996\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol for\ndistributed, collaborative, hypermedia information systems. It is a \ngeneric, stateless, object-oriented protocol which can be used for many \ntasks, such as name servers and distributed object management systems, \nthrough extension of its request methods. A feature of HTTP is the typing \nand negotiation of data representation, allowing systems to be built \nindependently of the data being transferred.    \n                           \nHTTP has been in use by the World-Wide Web global information initiative \nsince 1990. This specification defines the protocol referred to as \n\"HTTP/1.1\".                                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-v11-spec-07.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-07.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-07.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@ietf.org\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-7212764"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "On Tue, 13 Aug 1996, Koen Holtman wrote:\n\n> Simon Spero:\n\n> I'm particularly interested in what you propose to include in the UA\n> profile.  There is a tradeoff between covering a larger area of\n> negotiation and getting more `profile cache misses' with your scheme,\n\nWith NG, there isn't quite a strong a division between the UA profile and \nthe individual user's profile; profiles can be modified dynamically; \nthe server has the option of either caching the whole modified profile, \nor just the 'base class' profile; the client just needs to know and note  \nwhat profile has been cached. \n\n> where do you stop exactly?  How many `profile cache misses' do you\n> estimate under your proposal?\n\nThe question of where you stop is up to the server; just caching the UA \nspecific base profiles wins big; spending  the extra effort to cache \nper-user profiles is an even bigger win - the tradeoff depends on how \nmuch perisistent storage you wan tto dedicate to the problem. \n\nI would expect to see a big win with a cache size of around 20 (enough for\nthe most popular Nevergethere, Exploder, and Slosaic versions to be safe\nfrom getting flushed by the small fry. There'd be a bigger win around \n2000, as even the small-fry get to stay put.\n\nFor a big site with a regular audience, it might be worth spending a \nhundred dollars or so on this and dedicating up to a gig or to profile \ncaching; this keeps things really fast for caching.\n\nSimon (more later)\n\n\n---\nCause maybe  (maybe)      | In my mind I'm going to Carolina\nyou're gonna be the one that saves me | - back in Chapel Hill May 16th.\nAnd after all      | Email address remains unchanged\nYou're my firewall -          | ........First in Usenet.........\n\n\n\n", "id": "lists-010-7223434"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Shel writes:\n    The problem with [Simon's] approach is that oddities of particular\n    browsers may not exist in a hard coded database built into the\n    browser itself at the time it is released, but may only be\n    recognized by third parties, after the fact.  The\n    UA-characteristics database should be independent of the UA\n    itself.  I'm assuming here that this database may contain fine\n    detail about UA behavior that may even include bugs noticed long\n    after the release.\n\nI know Shel has had to fight the real-world battle against the attack\nof the evil browsers, but I'm not sure I would be quite as gloomy\nabout a UA-provided UA-profile.\n\nEven if the UA vendor manages to botch the UA-profile setting when\nthe browser is released, it should not take a great deal of imagination\nto design in a mechanism for updating the UA's idea of its profile\nto reflect reality.  Simple applications of public-key signatures\nshould prevent any nasty security implications.  For example, whenever\nyour server believes that the UA-profile is wrong, it sends a Redirect\nto a URL that causes the browser to download and update its stored\nUA-profile, but only if the public-key signature is correct and only\nif the included timestamp is newer than the stored one.\n\nThis is just an off-the-cuff idea, but it seems to avoid the requirement\nfor complete connectivity (because the server could obtain a signed\nUA-profile from the UA vendor and mirror it locally).\n\n-Jeff\n\n\n\n", "id": "lists-010-7233734"}, {"subject": "Re: Sticky stuff", "content": "    As for the high RTT on some of Digital's internal links: RTTs are\n    irrelevant for this discussion.  If anything, they will make the\n    savings less noticable.  If Digital's internal links were highly\n    _saturated_, that would be another thing.\n\nPrecisely.  In the absence of high bit-error rates (such as on wireless\nlinks), high RTTs come from two sources: speed-of-light delays, and\nqueueing delays.  Although Digital is on a serious cost-cutting kick,\nit's not possible to pay less and get slower light, so we use the same\n300,000 KM/sec that everyone else has.  But extra bandwidth does cost\nmoney, so in some places we are undersupplied, which means queueing\ndelays, which means high RTTs.\n\nIn other words: high RTTs imply link saturation.\n\n-Jeff\n\n\n\n", "id": "lists-010-7242840"}, {"subject": "Re: Entity Tag", "content": "    I am in the process of designing an HTTP 1.1 server which  provides\n     JPEG Images when a GET method with a certain URL is received, but am\n     unsure about the use of entity tags.  Should an entity of this type\n    respond with an ETag field ?\n    \nGenerally, an HTTP/1.1 server would return Etag fields whenever\nthere is a straightforward way of generating them.  It has nothing\nto do with the precise data type; that is, it doesn't matter if the\nimages are JPEG, GIF, MPEG, or drawings made up of ASCII characters.\n\nIf your server does not have a way to generate an ETag field that\nmeets the specification, then don't send one.\n\n    In turn, should I support If-Match/If-None-Match?\n    \nIt's almost entirely pointless to send an ETag response header unless\nyou also support the request headers that use entity tags (i.e.,\nIf-Match, If-None-Match, and If-Range if you support subranges).\n\n    If so ,How?\n\nWe tried to describe this in the specification.  If there is some\npart of the specification (draft-ietf-http-v11-spec-07.txt) that\nis unclear to you, please describe the specific problem.  The\npurpose of this mailing list is to produce a good specification,\nand we need to know if people are confused after they have read \nit carefully.\n    \n-Jeff\n\n\n\n", "id": "lists-010-7250178"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Simon Spero:\n>\n>\n>With NG, there isn't quite a strong a division between the UA profile and \n>the individual user's profile; profiles can be modified dynamically; \n>the server has the option of either caching the whole modified profile, \n>or just the 'base class' profile; the client just needs to know and note  \n>what profile has been cached. \n\nSo if I understand you correctly, there are two profiles: the UA\nprofile and the user profile.  Or is there a complete inheritance\nlayer in there, which allows 5 layered profiles if you want to?  \n\nHow does the client know what profile has been cached?  If it knows\nthat the user profile is cached, will the client send the entire\nprofile in the request or will it omit any user profile info?\n\nIf I'm negotiating a bilingual homepage, how do I tell the client to\nsend me the user profile with the language preferences?\n\n>> where do you stop exactly?  How many `profile cache misses' do you\n>> estimate under your proposal?\n>\n>The question of where you stop is up to the server; just caching the UA \n>specific base profiles wins big; spending  the extra effort to cache \n>per-user profiles is an even bigger win - the tradeoff depends on how \n>much perisistent storage you wan tto dedicate to the problem. \n>\n>I would expect to see a big win with a cache size of around 20 (enough for\n>the most popular Nevergethere, Exploder, and Slosaic versions to be safe\n>from getting flushed by the small fry. There'd be a bigger win around \n>2000, as even the small-fry get to stay put.\n\nI think you are right about the UA profile (as long as UAs remain\nmonolithic systems, that is).  I have doubts about this working for\nuser profiles; read on.\n\n>For a big site with a regular audience, \n\nBut what if I'm a small site where random people drop by to read 2\npages?\n\n>it might be worth spending a \n>hundred dollars or so on this and dedicating up to a gig or to profile \n>caching; this keeps things really fast for caching.\n\nTaking 1K for a user profile, that would mean you have 1M users!\n\nSuppose I am a small site where random people drop by to read 2\nlanguage-negotiated pages.  Now, will user profile caching outperform\nsending large headers combined with a sticky header scheme in this\ncase?  That depends mainly on\n\n   P_same: the probability that two users have the same user profile\n\nif P_same is 1 in 1 million, and if you have already had 10.000\ndifferent users (OK, the site is not so small after all), the next\nuser will get a user profile cache hit in about 1% of all cases.  Not\nvery optimal.\n\nSo what is a realistic estimate of P_same?  That depends on the amount\nof variance in the user profile.  Making a table of things you want in\nyour user profile:\n\n   VERY CONSERVATIVE\n   estimate of number of           description\n   different settings\n\n        100      accepted languages with some q factors\n        10       accepted charsets with some q factors\n        2^8      presence of common file viewers (word processors,\n                 movie players, audio players)\n        3^4      q factors for the 4 file viewers you have\n        2^5      presence of common plug-ins\n        10       type of screen (colordepth, monochrome, ...)\n        4        size of screen\n        2        I hate frames\n        2        I hate animated gifs\n        2        Life is too short for large images\n     --------- * \n        2e11           \n\nSo a very conservative P_same is 1 in 2e11.  This reduces the chances\nof getting a user profile cache hit for a different user to zero.\n(Except for users who never change their default profiles, but how\nmany web users would be that boring?)\n\nAlso, with this P_same, the user profile cache key essentially becomes\na global user identifier.  Definitely not good for privacy.\n\nI don't see how you can ever get the numbers working for user profile\ncaching.  The introduction of user profile caching scheme seems to put\npenalties on sites seeking an irregular audience, even if such sites\nare very big.\n\nIt seems to me that only transparent content negotiation scales for\nP_same values of 1 in 1 million or more (while also still protecting\nprivacy).  And I think that for any moderately interesting collection\nof things to negotiate on (like the table above), you will get a\nP_same like this.\n\nYou can also look at this result in another way: if user profile\ncaching were effective, then P_same would be so low that we could\neasily encode all user profile information in a short request header.\n\nIt seems that profile caching is a good way to `translate' a\nuser-agent string into a large table of capabilities (assuming that\nuser agents stay monolithic), but that it has little general use\nbeyond that.\n\n>Simon (more later)\n\nKoen.\n\n\n\n", "id": "lists-010-7258233"}, {"subject": "Re: Entity Tag", "content": ">If your server does not have a way to generate an ETag field that\n>meets the specification, then don't send one.\n\nI'm having a moral dilemma with regards to my recommendation to the server\nteam about what we be a good means for generating an ETag:\n\nWarning:  Everything that follows is an implementor's issue, and not\ntechnically a protocol specification issue.\n\nInitially, I felt a hash of {the full pathname (to distinguish variants),\nthe size of the file, and the OS-reported last-modification time} would be\nsufficient to be a strong validator.  But, technically, that doesn't\nguarantee that a file that has changed twice in the same second has a\ndifferent ETag value, which the spec says is a must.\n\nSo now I have a dilemma.  On NT, I might have an easy solution, as\nsupposedly FILETIMES are stored as 100-nanosecond intervals.  On UNIX do I\nhave to use a W/ weak validator?  I'm strongly concerned that every time\nsomeone aborts a GIF download, and returns to the containing page, smart\nbrowsers of the future will be using If-Match with Range GET's, and the\nSpyglass Server will be left out in the cold with its wimpy weak validators.\n\nCertainly reading in the file and doing an MD5 or checksum of its contents\neach time we serve it is out of the question for performance reasons.\n\nI would think with our file-based web server, numerous changes per second\naren't as much of an issue as a database back-end that was generating\ncontent dynamically.  I'm thinking that said dynamic application was the\nfocus and intent of the restrictions on the strong validators, so maybe I\ncan slide by?\n\nHere's everyone's chance to express an opinion.\n\n-----\nDaniel DuBois, Traveling Coderman      http://www.spyglass.com/~ddubois/\n   A polar bear is a rectangular bear after a coordinate transform.\n\n\n\n", "id": "lists-010-7271439"}, {"subject": "Re: HTTP State Management Mechanism draft questio", "content": "Paul Grand <pgrand@netcount.com> wrote:\n\n  > In my review of your document \"HTTP State Management Mechanism\" dated July\n  > 19, 1996, I have come across two points that I find would impede commerce in\n  > web usage by eliminating state retention mechanisms.\n  > \n  > In the next to last paragraph of section 7.1, it is suggested that at the\n  > conclusion of a user agent session, the user agent inquire of the operator\n  > if the state information should be retained.  The proposed default value is\n  > \"No\".  This makes it more costly in user agent time and server resources to\n  > reestablish the user agent's state.  This raises privacy concerns as it\n  > makes it mandatory for one web service to maintain a central database of\n  > users' state information.  What is the reason for this?\n\nOne of the underlying themes of the State Management Mechanism (aka,\n\"cookies\") proposal is to provide a user with control over the\ninformation about them that accumulates with their Web accesses.  As\narticles in the popular and trade press have shown, users are surprised\nand disturbed that their Web accesses may be aggregated to form a\nprofile of what they're doing.  Many say they don't like it.\n\nOne way to grant users control is to alert them when things start to\nhappen, such as when a stateful session begins.  Another is to ask\nthem, when they exit a user agent, whether they want to retain the\nsession information or discard it.  The default is to protect the user.\n\nIf it's important to a service that the state information be retained\nlong-term (and section 3 of the I-D makes the point that sessions are\nassumed to be relatively short-lived), then it should make a point of\nexplaining to users why this is so and encourage them not to discard\nit.  Let the users decide.  If the user doesn't want the state to be\nretained, then issues of cost to the server and user agent are\nirrelevant.\n\nAs Harald Alvestrand said [in a private message], it's hard to see how\nthe proposal *reduces* privacy.\n  > \n  > In section 4.3.5 eliminating the ability of having \"unverifiable\"\n  > redirection impairs the ability of the web service (chosen by the user agent\n  > operator) to engage in using the services of a third party for advertising,\n  > content building, download specialized \"plugins\" or other usage.  This hurts\n  > web commerce.  Why is this proposed?\n\nAgain, the purpose is to reduce the ability to \"do things behind the\nuser's back.\" That section also says\n\n    User agents may offer configurable options that allow the user agent, or\n    any autonomous programs that the user agent executes, to ignore the\n    above rule, so long as these override options default to ``off.''\n\nOnce again, this reflects the theme of informing users and letting them\ndecide.  Users are not unconditionally prevented from doing anything;\nthey are given choice.  Recall, too, that web services may still use\nthird-parties for all the things you describe.  They are just prevented\nfrom surreptitiously creating sessions while doing so.\n\n  > \n  > The implementation of these two items would have two deleterious effects:\n  > First, they would decrease the privacy and anonymity of web users by storing\n  > data about user agents / individuals on servers, not at the user agent,\n\nThat sounds nice, but somehow I suspect they're already accumulating data\nabout user agents/individuals, with or without cookies.\n\n  > under user agent / individual control.  Additionally, login and password\n  > exchanges would be required, allowing identity spoofing.\n\nIdentity spoofing would seem to be neither easier nor harder than cookie\nspoofing.  They're both essentially clear-text.\n  > \n  > Second, they would increase the storage, processing and communication\n  > requirements of web servers.  Re-establishing inter-session states would\n  > require server operators to send and receive extra messages (logins and\n  > passwords), extra disk space to keep the personal login, password, and state\n  > information, and extra processing to operate a state recover mechanism.\n\nAs I said earlier, if users can be persuaded that the information will\nbe retained for what they consider a good cause, they will likely be\nhappy to do so, and there is no added burden.  But they must be given\nthe choice.  And if they choose not to retain the information, the\nservice operator is violating their trust by attempting to assemble it\nin spite of their wishes.\n\nDave Kristol\n\n\n\n", "id": "lists-010-7280972"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "On Tue, 13 Aug 1996, Jeffrey Mogul wrote:\n\n> Shel writes:\n>...\n>     itself.  I'm assuming here that this database may contain fine\n>     detail about UA behavior that may even include bugs noticed long\n>     after the release.\n> \n> I know Shel has had to fight the real-world battle against the attack\n> of the evil browsers, but I'm not sure I would be quite as gloomy\n> about a UA-provided UA-profile.\n\n\nEvil has nothing to do with it.  The fact is that my products have had\nto adjust themselves to browser behaviors which would never make it\ninto the feature list because the browser vendor does \ncontrol the characterization of their product. Those behavior\ndifferences may be bugs in my eyes and design intent in the developers.\nThey have been quite significant to the correct behavior of the \napplications but rather small in the overall scheme of things.\n\nHence, as was pointed out by someone, we need to define an extensible\nmechanism for recording and reporting browser characteristics. Some\ncharacteristics associate with the software version and some of those\nwould be based on vendor descriptions of their product. Others reflect\nuser configuration choices or platform charactistics.\n\nMany of the characteristics I'm concerned about involve HTML generated\non the fly and not cachable by a proxy anyway so I don't see these\n'feetures' as changing cachability.\n\nDaveR started this thread with a proposal that a mechanism be developed\nfor coding and sharing user agent characteristics. If we have a coding\nmechanism, it should be general enough to deal with the spectrum of\ndifferences and sources of the information. The coding scheme, should\nbe appropriate for negotiation based on characteristics the browser\nis willing to report as well as vendor supplied updates.\n\nAnd going back to where Dave started, it should support server operators\nwho want to collaborate and exchange characteristics. If the collaboration\nis to be more than mutually negotiated, then source verification will also\nneed to be dealt with. But I think it would be sufficient deal with \nthe coding issues and leave  sharing to be based on mutual agreement and\ntrust.  Very simple URL based requests could be used to exchange the \ndata. shttp/ssl and caching mechanisms are already defined to allow\nsecure cooperative management of data.\n\nDave Morris\n\n\n\n", "id": "lists-010-7292796"}, {"subject": "Re: Entity Tag", "content": "    Initially, I felt a hash of {the full pathname (to distinguish\n    variants), the size of the file, and the OS-reported\n    last-modification time} would be sufficient to be a strong\n    validator.  But, technically, that doesn't guarantee that a file\n    that has changed twice in the same second has a different ETag\n    value, which the spec says is a must.\n\n    So now I have a dilemma.  On NT, I might have an easy solution, as\n    supposedly FILETIMES are stored as 100-nanosecond intervals.  On\n    UNIX do I have to use a W/ weak validator?  I'm strongly concerned\n    that every time someone aborts a GIF download, and returns to the\n    containing page, smart browsers of the future will be using\n    If-Match with Range GET's, and the Spyglass Server will be left out\n    in the cold with its wimpy weak validators.\n\nFirst of all, on many UNIX systems, the file modification time\nactually has sub-second resolution.  Here's a little test program:\n    #include <sys/types.h>\n    #include <sys/stat.h>\n    main(int argc, char **argv) {\n    struct stat statbuf; static char buf[1000];\n    sprintf(buf, \"cat a.out > %s\", argv[1]);\n    system(buf);\n    stat(argv[1], &statbuf);\n    printf(\"%d.%06d\\n\", statbuf.st_mtime, statbuf.st_spare2);\n    system(buf);\n    stat(argv[1], &statbuf);\n    printf(\"%d.%06d\\n\", statbuf.st_mtime, statbuf.st_spare2);\n    }\nIf I compile this (as a.out) and run it on either ULTRIX or Digital\nUNIX, I get results like:\n% a.out foo\n839976508.783384\n839976508.904470\n%\ni.e., the st_spare2 field is really the tv_usec part of the file\nmodification date.\n\nNow, I'll immediately admit that this is not true for all UNIX\nvariants.  For example, the version of the 4.4BSD code that someone\nleft here a few years ago explicitly sets the st_spare2 field to zero.\nAnd even on DEC's UNIX systems, the resolution of this field is\nactually one clock tick (1 msec on Digital UNIX, 4 msec on ULTRIX)\nand so it is not 100% proof against collisions.  But it's pretty\nhard to update a file, hand out a copy via the HTTP server, and\nupdate it again all in the space of 1 msec - our best SPECweb96\nresults so far show only 809 ops/sec.\n\nI'm not a POSIX expert, so I don't know if the POSIX spec for\nstat() says anything about st_spare2.  Perhaps people blessed\nwith UNIX systems from other major vendors, or other POSIX-compliant\nsystems, can run my little test program and report what they find.\n(Please, not all at once!)\n\nMore to the point, the requirement in the HTTP/1.1 spec for strong\nvalidators does not preclude the use of 1-second timestamp resolutions\nunder the appropriate conditions.  For example, if your server is\nintegrated with some sort of authoring tool that simply delays its\nupdate if the new version is being written within a second of the\nold version, then you can make the appropriate guarantee.  The\nguarantee is not based solely on the resolution of the timestamp;\nit's based on what the server knows about how the timestamp has been\nused.\n\nEven more useful, probably, is the fact that a file modification\ntimestamp T of any resolution R is only \"weak\" during the\nperiod R+|E| after the file has been modified, where E is a bound\non clock skew.  To make this more specific, suppose that you know that\n(1) file F was modified at time T\n(2) the timestamp T has a resolution of R seconds\n(3) the clock used to generate the timestamp T is at most E\nseconds away from the clock used by the server to find out\nthe current time.\n\nThen, between time T and time T+R+|E|, you would have to generate\nan entity tag of the form\nW/\"encoding-of-T\"\nbut after time T+R+|E|, you could generate an entity tag of the form\n\"encoding-of-T\"\nbecause you are now sure that you have not given out that tag for\na previous version of the resource.\n\n[The slight rub here is that the server might have to lock the file\nagainst modification between the time that it reads the timestamp\nand the time that it finishes reading the data, so that these two\nvalues are effectively read in one atomic operation.  But this might\nbe necessary in any case, to avoid sending out a chimera with\npieces from two versions of the file.]\n\nThe rules in section 13.3 state that these two entity tags cannot\nbe treated as equal.  This prevents any errors on GETs, and if a\nclient does a conditional PUT using an entity tag, we've already\ninsisted that this only be done with non-weak tags.\n\nThe downside of this weak-and-then-strong approach is that if the file\nis changing rapidly *and* is being given out more frequently than it\nchanges, then one may miss a lot of opportunity for optimizing If-Match\nand Range GETs.  But then this is an incentive to use an operating\nsystem that minimizes T, and a system design that minimizes E, or makes\nE = 0 (e.g., don't have the server read its files via NFS unless you\nalso use NTP).\n\n    I would think with our file-based web server, numerous changes per\n    second aren't as much of an issue as a database back-end that was\n    generating content dynamically.  I'm thinking that said dynamic\n    application was the focus and intent of the restrictions on the\n    strong validators, so maybe I can slide by?\n\nI think it's not that difficult to abide by the rules even for\nfile-based servers.  And it would be sad if we discovered after a while\nthat we can't safely do conditional Range GETs, for example, because\nserver implementors didn't take this seriously enough.\n\n-Jeff\n\n\n\n", "id": "lists-010-7302512"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Jeffrey Mogul writes:\n\n > I know Shel has had to fight the real-world battle against the attack\n > of the evil browsers, but I'm not sure I would be quite as gloomy\n > about a UA-provided UA-profile.\n > \n...\n > This is just an off-the-cuff idea, but it seems to avoid the requirement\n > for complete connectivity (because the server could obtain a signed\n > UA-profile from the UA vendor and mirror it locally).\n > \n > -Jeff\n > \n > \n\nYour connectivity points are well taken.  At this point the\ncombination that sounds best to me is, as you suggest, a way to get\nthe server to return something to the browser that says, in effect,\n\"once more, with your profile\".  Then we'd need a DNS extension to\nidentify a set of servers on which browser profiles are kept, so that\nbrowsers could find a copy of their profile from some server that\ncaches profiles.  These servers might just as well be http servers.\n\nIs this just too much infrastructure for this to ever really work?\nHow else could the browser stay appraised of up-to-date versions of\nits own profile?  I think much of the point of the exercise would be\nlost if the browser just spits back a canned profile that was built in\nwhen the browser was released.\n\nIf things were set up in this manner, it would be possible for a fully\nconnected http server to query the profile server itself and not do\nthe redirection, if it was so configured, or to use the redirect\nmethod if it weren't.  It would be possible (and so, probably\ninevitable) for browsers to cheat and just send a canned profile if\nthey got such a redirection response.\n\nAlso, the bit about the signing of the profiles might need a little\nthought, because it isn't necessarily the vendor that you want to have\nbeing responsible for the profile.  Vendors might be less likely to note\nbugs or count them as worth including.\n\n--Shel\n\n\n\n", "id": "lists-010-7314883"}, {"subject": "Re: Call for compatibility tester", "content": "> OK, you don't like 416.  But I'd rather be compatible without\n> user-agent tricks, because user-agent tricks increase the cost of\n> minimal implementations and make caching *much* more difficult.\n\nThere is a difference between being compatible with older versions\nof HTTP and being compatible with old browsers that do not implement\nany version of HTTP correctly.  Any browser that does nothing upon receipt\nof a valid status response (valid being defined as anything that fits\nwithin the syntax of HTTP, not just the standard response codes) is not\nfit for use.  The only correct way to deal with such browsers is to\nexclude them on the basis of their User Agent field, or just let them\npuke and die.  The protocol itself will not be changed every time\nsome programmer screws up.\n\n> Under the current spec, an origin server can specify conditions under\n> which a proxy can send a cached list response to a 1.0 user agent.\n> This seems to be important for efficiency and scalability, but it\n> depends on cached list responses having a format which all 1.0 user\n> agents can handle.  That is why I'm seeking an alternative for using\n> the 300 code.\n\nSounds to me like you are making transparent negotiation incredibly\ndifficult to implement just to make a minor (and almost never effective)\nimprovement over including\n\n    Vary: User-Agent\n\nin the 300 response for those sites that care whether or not such\na broken browser would puke and die.  User agents that are known to\nbe broken could receive a 200 response instead of the 300.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7324542"}, {"subject": "drafts pleas", "content": "Hi folks,\n\nI don't like reviewing (on an IETF WG) drafts that are not yet in\nthe Internet Drafts repository and yet are intended to be IDs.\nUnless there are copyright problems, please submit them to the\nrepository as soon as they get in a shape fit for review by the WG,\nwith (unless Larry tells you otherwise) a file name beginning with\n\n    draft-author-http-\n\nsince that is how I decide what should be linked from my WG pages.\n\nIncluded below is the current draft guidelines [but I personally\nfeel that it is better not to include page breaks in text files\nuntil they are completed].\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n==========================================================================\nGuidelines to Authors of Internet-Drafts\n\nThe Internet-Drafts directories are available to provide authors with\nthe ability to distribute and solicit comments on documents they may\nsubmit to the IESG or RFC Editor for publication as an RFC. Submissions\nto the directories should be sent to ``internet-drafts@ietf.org''.\n\nInternet-Drafts are not an archival document series.  These documents\nshould not be cited or quoted in any formal document.  Unrevised\ndocuments placed in the Internet-Drafts directories have a maximum life\nof six months.  After that time, they must be updated, or they will be\ndeleted.  After a document becomes an RFC, it will be replaced in the\nInternet-Drafts Directories with an announcement to that effect.\n\nInternet-Drafts are generally in the format of an RFC, although they are\nexpected to be rough drafts.  This format is specified fully in RFC 1543.\nIn brief, an Internet-Draft must be submitted in ASCII text, limited to\n72 characters per line and 58 lines per page, followed by a formfeed\ncharacter.  Overstriking to achieve underlining is not acceptable.\n\nPostScript is acceptable, but only when submitted with a matching ASCII\nversion (even if figures must be deleted).  PostScript should be\nformatted for use on 8.5x11 inch paper.  If A4 paper is used, an image\narea less than 10 inches high should be used to avoid printing extra\npages when printed on 8.5x11 paper.\n\nThere are differences between the RFC and Internet-Draft format.  The\nInternet-Drafts are NOT RFCs and are NOT a numbered document series.\nThe words ``INTERNET-DRAFT'' should appear in the upper left hand corner\nof the first page.  The document should NOT refer to itself as an RFC or\na draft RFC.\n\nThe Internet-Draft should neither state nor imply that it has any\nstandards status; to do so conflicts with the role of the RFC Editor and\nthe IESG. The title of the document should not infer a status.  Avoid the\nuse of the terms Standard, Proposed, Draft, Experimental, Historic,\nRequired, Recommended, Elective, or Restricted in the title of the\nInternet-Draft.  All Internet-Drafts should include a section on the first\npage containing the following verbatim statement:\n\n     This document is an Internet-Draft.  Internet-Drafts are working\n     documents of the Internet Engineering Task Force (IETF), its\n     areas, and its working groups.  Note that other groups may also\n     distribute working documents as Internet-Drafts.\n\n     Internet-Drafts are draft documents valid for a maximum of six\n     months and may be updated, replaced, or obsoleted by other\n     documents at any time.  It is inappropriate to use Internet-\n     Drafts as reference material or to cite them other than as\n     ``work in progress.''\n\n     To learn the current status of any Internet-Draft, please check\n     the ``1id-abstracts.txt'' listing contained in the Internet-\n     Drafts Shadow Directories on ftp.is.co.za (Africa),\n     nic.nordu.net (Europe), munnari.oz.au (Pacific Rim),\n     ds.internic.net (US East Coast), or ftp.isi.edu (US West Coast).\n\nThe document should have an abstract section, containing a two-to-three\nparagraph description suitable for referencing, archiving, and\nannouncing the document.  This abstract will be used in the\n1id-abstracts.txt index, and in the announcement of the Internet-Draft.\nThe abstract should follow the ``Status of this Memo'' section.  In\naddition, the Internet-Draft should contain a section giving name and\ncontact information (postal mail, voice/fax number and/or e-mail) for\nthe authors.\n\nAll Internet-Drafts should contain the full filename (beginning with\ndraft- and including the version number) in the text of the document.\nThe filename information should, at a minimum, appear on the first page\n(possibly with the title).\n\nFor those authors submitting updates to existing Internet-Drafts, the\nchoice of the file name is easily determined (up the version by 1).  For\nnew documents, send a message to ``internet-drafts@ietf.org''\nwith the document title, if it is a product of a working group (and the\nname of the group), and an abstract.  The file name to be assigned will\nbe included in a response.  Simply add the filename text to the document\n(ASCII and PostScript versions) and submit the Internet-Draft.\n\nA document expiration date must appear on the first and last page of the\nInternet-Draft.  The expiration date is six months following the\nsubmission of the document as an Internet-Draft.  Authors can calculate\nthe six month period by adding five days to the date when the final\nversion is completed to cover processing time.\n\nIf the Internet-Draft is lengthy, please include, on the second page, a\ntable of contents to make the document easier to reference.\n\n\n\n", "id": "lists-010-7333918"}, {"subject": "Re: Call for compatibility tester", "content": "Roy T. Fielding:\n>\n   [Koen Holtman:]\n>> OK, you don't like 416.  But I'd rather be compatible without\n>> user-agent tricks, because user-agent tricks increase the cost of\n>> minimal implementations and make caching *much* more difficult.\n>\n>There is a difference between being compatible with older versions\n>of HTTP and being compatible with old browsers that do not implement\n>any version of HTTP correctly.\n\nHTTP/1.0 is an informational standard.  As far as I know, this means\nthat the IETF does not have an opinion on whether lynx implements\nHTTP/1.0 correctly.\n\nYou obviously have strong opinions about lynx not being a HTTP/1.0\napplication, but these are not universally held.  If the conneg draft\nwould continue to use the 300 code (without providing an adequate\nescape hatch) now that the lynx interoperability problems are known,\nthis would effectively encode your strong opinions in the spec.  I\ndon't think that would be appropriate.\n\n[...]\n>  The only correct way to deal with such browsers is to\n>exclude them on the basis of their User Agent field, \n\nUsing this compatibility hack would be very expensive; you would have\nto disallow proxies from ever sending a cached list (300) response to\na non-negotiating user agent, which means that you will have to handle\nall these requests yourself.  \n\nThe current draft does not allow you to use Vary to optimize here:\nproxies ignore the Vary header in cached list responses when using the\nnetwork negotiation algorithm.  The spec could be extended with more\nvary rules, but I don't think this is the way to go.\n\n[...]\n>Sounds to me like you are making transparent negotiation incredibly\n>difficult to implement just to make a minor (and almost never effective)\n>improvement over including\n>\n>    Vary: User-Agent\n>\n>in the 300 response for those sites that care whether or not such\n>a broken browser would puke and die.\n\nHuh? Sending list responses 416 instead of 300 would hardly make\ntransparent negotiation incredibly difficult to implement.  It is the\ncompatibility hacks you need if we keep using the 300 response without\na good escape which are difficult to implement.\n\n>  User agents that are known to\n>be broken could receive a 200 response instead of the 300.\n\nYes, but having a proxy downgrade a 300 into a 200 would be incredibly\nyucky.\n\nNewsflash: I just got a report that Arena 0.97h for Linux prints an\nerror message on stdout when getting a 416 response, without changing\nthe page on screen.  That seems to rule out using 416.\n\nOK, how about this proposal: we just forget about sending cached list\nresponses to non-negotiating clients altogether.  This allows us to\nkeep using the 300 code as originally intended.  The List_OS result of\nthe network negotiation algorithm disappears, and so does the\n`forward' directive in the alternates header, because `forward' is now\nthe only action possible.  We allow origin servers to generate\nwhatever response they want (including a 200 response with a list)\nwhen contacted by a non-negotiating user agent.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-7346241"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "    Also, the bit about the signing of the profiles might need a little\n    thought, because it isn't necessarily the vendor that you want to\n    have being responsible for the profile.  Vendors might be less\n    likely to note bugs or count them as worth including.\n    \nThe vendor is an obvious choice, both because the browser vendor\nhas an obvious incentive to keep the user relatively happy (at\nleast, until the browser market is back to being dominated by\na single vendor!) and because if you don't trust the vendor\nwho gave you the browser binary in the first place, you can't\nreally trust anything done with a browser profile.\n\nIt would not be too much of a stretch to imagine that a browser\nwould accept updates signed by either the original vendor or\nby one of a predetermined set of trustworthy parties, such as\nCERT or perhaps well-established support vendors (e.g., Cygnus\nor Digital).  There's no reason (except logistics) to limit this\nto a single signature authority.\n\n-Jeff\n\n\n\n", "id": "lists-010-7357111"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Jeffrey Mogul writes:\n >     Also, the bit about the signing of the profiles might need a little\n >     thought, because it isn't necessarily the vendor that you want to\n >     have being responsible for the profile.  Vendors might be less\n >     likely to note bugs or count them as worth including.\n >     \n > The vendor is an obvious choice, both because the browser vendor\n > has an obvious incentive to keep the user relatively happy (at\n > least, until the browser market is back to being dominated by\n > a single vendor!) and because if you don't trust the vendor\n > who gave you the browser binary in the first place, you can't\n > really trust anything done with a browser profile.\n > \n\nThere's trust, and then there's trust.  While I (browser user) may\ntrust a browser vendor enough to give me a browser I can use safely\nwithout trashing my filesystem (e.g.), I (service provider) may not\n*believe* everything a browser vendor says about the capability of\ntheir browser.  For instance, a browser vendor might want to advertise\nthey are fully compatible with the latest version of Netscape, when in\nfact, there are numerous niggly details about their rendering choices\nthat are not done in the same way, and that might not even be noticed by\nthe vendor themselves.  Signature authorities would certainly provide\na nice level of guarantee, but I think that \"relatively trustworthy\"\nDNS servers referring to relatively trustworthy profile servers\nwould be ok most of the time -- this seems to be very similar in\ncharacter to other kinds of DNS spoofing and so should be solved by\nsimilar methods (whatever they are).\n\n > It would not be too much of a stretch to imagine that a browser\n > would accept updates signed by either the original vendor or\n > by one of a predetermined set of trustworthy parties, such as\n > CERT or perhaps well-established support vendors (e.g., Cygnus\n > or Digital).  There's no reason (except logistics) to limit this\n > to a single signature authority.\n > \n > -Jeff\n > \n > \n\nI agree.\n--Shel\n\n\n\n", "id": "lists-010-7365764"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Shel Kaphan writes:\n> There's trust, and then there's trust.  While I (browser user) may\n> trust a browser vendor enough to give me a browser I can use safely\n> without trashing my filesystem (e.g.), I (service provider) may not\n> *believe* everything a browser vendor says about the capability of\n> their browser.  For instance, a browser vendor might want to advertise\n> they are fully compatible with the latest version of Netscape, when in\n> fact, there are numerous niggly details about their rendering choices\n> that are not done in the same way, and that might not even be noticed by\n> the vendor themselves.\n>...\n\nIt would seem that a validation program is, at heart, necessary. I am\nsurprised W3C is not pressing forward on this issue before any further\nHTML enhancements. I still find that current browsers cannot even handle\nsimple, non-trivial tables.\n\n-Robert\n-- \nr-lentz@nwu.edu                       http://www.astro.nwu.edu/lentz/plan.html\n      \"The intellectual level of the schools can be no higher than the\n       intellectual level of the culture in which they float.\"\n                                                     -Richard Gibboney\n\n\n\n", "id": "lists-010-7375901"}, {"subject": "Submission of draft-holtman-http-negotiation02.tx", "content": "At Roy's request, I submitted the transparent content negotiation\ndraft spec as an internet draft.\n\nThe submitted internet draft is *identical* to the `pre02-19' draft I\nmade available on June 30 at http://gewis.win.tue.nl/~koen/conneg/,\nexcept for the date, filename, and `STATUS OF THIS DOCUMENT' stuff at\nthe start, and the use of `HTTP' instead of `HTTP/1.1' in the abstract\nand introduction.\n\nIf you have not read the draft already, don't get the .txt version\nfrom an internet drafts ftp site, but get the HTML version at the web\npage above.\n\nKoen.\n\n\n\n", "id": "lists-010-7384328"}, {"subject": "Am I a ghost", "content": "Sorry, bothering the whole list but\nReceived headers says that I in list.\nRobot thinks other.\n\nReturn-Path: http-wg-request@cuckoo.hpl.hp.com\nReceived: from Draculina.nns.ru (Draculina.office.nns.ru [192.168.1.2]) by Alyona.office.nns.ru (8.7.3/8.7.3) with ESMTP id FAA19305 for <nms@alyona.office.nns.ru>; Wed, 7 Aug 1996 05:16:53 +0400 (MSD)\nReceived: from hplb.hpl.hp.com (hplb.hpl.hp.com [15.255.59.2]) by Draculina.nns.ru (8.7.3/8.7.3) with ESMTP id FAA23821 for <nms@nns.ru>; Wed, 7 Aug 1996 05:16:50 +0400 (MSD)\nReceived: from cuckoo.hpl.hp.com by hplb.hpl.hp.com; Wed, 7 Aug 1996 02:08:33 +0100\nReceived: by cuckoo.hpl.hp.com\n(1.37.109.16/15.6+ISC) id AA054460104; Wed, 7 Aug 1996 02:08:24 +0100\n\n------- Forwarded Message\n\nDate: Wed, 14 Aug 1996 22:21:26 +0100\nMessage-Id: <199608142121.AA242347686@cuckoo.hpl.hp.com>\nTo: nms@nns.ru\nSubject: Re: unsubscribe\nReferences: <5592.840057680@Alyona.office.nns.ru>\nIn-Reply-To: <5592.840057680@Alyona.office.nns.ru>\nX-Loop: http-wg@cuckoo.hpl.hp.com\nPrecedence: junk\n\nYou have not been removed, I couldn't find your name on the list.\n\nIf this wasn't your intention or you are having problems getting yourself\nunsubscribed, reply to this mail now (quoting it entirely (for diagnostic\npurposes), and of course adding any comments you see fit).\n\nTranscript of unsubscription request follows:\n- -- \nFrom nms@nns.ru Wed Aug 14 22:21:26 1996\n>Received: from otter.hpl.hp.com by cuckoo.hpl.hp.com with ESMTP\n>(1.37.109.16/15.6+ISC) id AA242237685; Wed, 14 Aug 1996 22:21:25 +0100\n>Return-Path: <nms@nns.ru>\n>Received: from hplb.hpl.hp.com by otter.hpl.hp.com with ESMTP\n>(1.37.109.16/15.6+ISC) id AA248887684; Wed, 14 Aug 1996 22:21:24 +0100\n>Received: from Draculina.nns.ru by hplb.hpl.hp.com; Wed, 14 Aug 1996 22:21:23 +0100\n>Received: from Alyona.office.nns.ru (Alyona.office.nns.ru [192.168.1.3]) by Draculina.nns.ru (8.7.3/8.7.3) with ESMTP id BAA25245 for <http-wg-request@cuckoo.hpl.hp.com>; Thu, 15 Aug 1996 01:21:21 +0400 (MSD)\n>Received: from Alyona.office.nns.ru (localhost [127.0.0.1]) by Alyona.office.nns.ru (8.7.3/8.7.3) with ESMTP id BAA05594 for <http-wg-request@cuckoo.hpl.hp.com>; Thu, 15 Aug 1996 01:21:21 +0400 (MSD)\n>To: http-wg-request@cuckoo.hpl.hp.com\n>Subject: unsubscribe\n>Mime-Version: 1.0\n>Content-Type: text/plain; charset=\"us-ascii\"\n>Content-Id: <5591.840057680.1@Alyona.office.nns.ru>\n>Date: Thu, 15 Aug 1996 01:21:20 +0400\n>Message-Id: <5592.840057680@Alyona.office.nns.ru>\n>From: Nickolay Saukh <nms@nns.ru>\n>\n>unsubscribe\n>\n\n------- End of Forwarded Message\n\n\n\n", "id": "lists-010-7391734"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Shel Kaphan:\n>\n>There's trust, and then there's trust.  While I (browser user) may\n>trust a browser vendor enough to give me a browser I can use safely\n>without trashing my filesystem (e.g.), I (service provider) may not\n>*believe* everything a browser vendor says about the capability of\n>their browser.  For instance, a browser vendor might want to advertise\n>they are fully compatible with the latest version of Netscape, when in\n>fact, there are numerous niggly details about their rendering choices\n>that are not done in the same way, and that might not even be noticed by\n>the vendor themselves.\n\nI fully agree.  If a niggly detail is added to a browser profile, this\naction will usually\n\n 1) contradict the marketing department of the browser vendor\n 2) make the programmer who got it subtly wrong unhappy.\n\nSo you can't really depend on the browser vendor to make such\nadditions, no matter if the company is marketing-driven or\ntechnology-driven.\n\nIt seems you need independent parties, and a system in which these\nparties do not have to care about browser vendors not liking them.\nHmmm.  Sociologically, this is beginning to sound like a rating\nproblem.  PICS anyone?\n\nIt seems that the biggest problem is not in the distribution of the\nprofiles, but in their creation: who would actually spend the\ntime/money gathering information about subtle incompatibilities?  I\ncan think of a number of answers:\n\n1) a consortium of content providers\n\n2) a third party which sells the info to individual content providers\n\n3) a company selling web content creation tools\n\nCould information about subtle incompatibilities flow freely over the\nweb in any of these cases?\n\nI think we first need to figure out a social/economical model in which\ninformation about subtle bugs would actually be created, not just for\nthe 5 most popular browser versions, but for the 100 most popular\nbrowser versions.  Building a distribution mechanism for this\ninformation seems to be a trivial matter in comparison.\n\nSome statistics to illustrate how much work you need to do to get\ndecent coverage:\n\n  the N most popular   account for\n  user agent versions  X% of all requests\n\n                   1    20.5% \n                   2    35.3% \n                   5    62.1% \n                  10    75.7% \n                  20    84.7% \n                  50    93.1% \n                  75    95.9%\n                 100    97.4% \n                 200    99.4% \n                 300    99.8% \n                 400   100.0% \n                 524   100.0%\n\n(Statistics based on the user agent strings in ~500K requests, 524\ndifferent agent versions found.  For a header like `User-Agent:\nMozilla/2.0 (Win16; I)', only `Mozilla/2.0' was significant in\ndetermining the user agent version.  Web Robots (accounting for an\nestimated 8% of requests) were not filtered out when making these\nstatistics, but I expect no big distortion in the general trend\nbecause of this.)\n\n[Maybe we should move this subthread to www-talk?]\n\n>--Shel\n\nKoen.\n\n\n\n", "id": "lists-010-7403963"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    However, you have handed out 80*10=1000 uses, which gives you 800 hits\n    as the upper bound.  So all you know is:\n    \n      80 <= actual hits <= 800\n    \n    This is not what I call useful information.  Something like an\n    interesting upper bound would be\n    \n      80 <= actual hits <= 100\n    \n    but I see no way in which max-uses can provide such a bound.\n    \n    I suspect that max-uses counts higher than 3 will be disastrously\n    ineffective at yielding a useful upper bound if uncooperative caches\n    are common.\n    \n    A proxy not being cooperative and only supporting max-uses seems about\n    as bad as a proxy not supporting hit counts at all.\n    \nIf I understand your argument, it is that in order to bound the\nsize of the error in the hit count to lie within a reasonable\nrange, the max-uses setting would have to be so small that it\nwould effectively disable caching.\n\n    I'd like to see *actual statistics* disprove my argument\n\nSo I got a day's worth of log entries from our proxy.  Here are\nsome statistics:\n\n589705total log entries\n529756after removing non-HTTP URLs with \"?\", \"cgi\", or \"htbin\"\n245481unique \"cachable\" URLs\n189723  \"cachable\" URLs referenced only once during the trace\n 55758\"cachable\" URLs referenced more than once\n\nThat's an effective cache hit rate of about 23%, not counting\nthings that can't be cached, and ignoring any misses that were\ncaused by modifications to the resources.\n\nSupposing that, for each of the \"cachable\" URLs referenced more than\nonce, the origin server sent max-uses=3.\n\nOf the\n 55758 \"cachable\" URLs referenced more than once\n 28951 (52%) were referenced exactly twice\n  9592 (17%) were referenced exactly 3 times\n\nOr in other words, of the\n340033 references to \"cachable\" URLs referenced more than once\n28951*2 + 9592*3 = 86678 of these references were to URLs\nreferenced 2 or 3 times \n    so\n340033 - 86678 = 253355 of these references were to URLs\nreferenced more than 3 times\n\nNow, assume that the servers had all sent max-uses=3 for these\nURLs.  Then the first use of each of these URLs (55758 uses)\nplus every 4th use of each of the URLs referenced more than\n3 times (roughly 253355/4 = 63339 uses) would have to be forwarded\nto the origin server.  This means that 340033 - (63339 + 55758)\n220936 uses would not have to be forwarded to the origin server,\nwhich comes out to about 37% of all the references logged.\n\nNow, it's quite true that not every server insists on demographics\ninformation, and so the actual number of references saved would\npresumably be lower.  But this should give some idea of the\nmagnitude of the possible savings, and I don't think it's insignificant.\n\n-Jeff\n\n\n\n", "id": "lists-010-7414976"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    It would be better to [...]\n    instead add stuff to make being cooperative less costly (e.g. a method\n    to piggy-back hit count reports for other URLs on a request you have to\n    send anyway).\n\nI don't think we need to add very much at all for this.  Remember\nthat the count report can be sent over the same persistent connection\nas other operations being sent to the server.  A typical count report\nwould be\nHEAD /foo.html HTTP/1.1\nHost: www.w3.org\nCache-control: use-count=37\nif no Vary was sent, or something like\nHEAD /foo.html HTTP/1.1\nHost: www.w3.org\nUser-agent: SuperBrowser 1.1\nCache-control: use-count=37\notherwise.  These are short enough that you can probably send 4 or 5\nof them without increasing the number of TCP packets (although the\nlength of certain packets would increase) in the typical case.\n\nClearly, some sort of header-abbreviation or tokenization would\nreduce the overhead even more, although you would have to at least\ntransmit the URL+host for each report.  It looks like the average length\nof possibly-cachable URLs in our proxy logs is about 48 bytes\n(including host names).\n\n-Jeff\n\n\n\n", "id": "lists-010-7425567"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "On Wed, 14 Aug 1996, Jeffrey Mogul wrote:\n\n>     Also, the bit about the signing of the profiles might need a little\n>     thought, because it isn't necessarily the vendor that you want to\n>     have being responsible for the profile.  Vendors might be less\n>     likely to note bugs or count them as worth including.\n>     \n> The vendor is an obvious choice, both because the browser vendor\n> has an obvious incentive to keep the user relatively happy (at\n\nI'll say it again... vendors have already demonstrated they don't rate\nthe trust to be designed in as the sole source of the profile information.\nOne well known vendor was statistically sampling bug reports 1-1/2 years\nago. What they are doing now, I haven't heard but as an ISV I can't\ntolerate a solution which leaves my customers wondering if the profile\nwill be maintained.\n\n> least, until the browser market is back to being dominated by\n> a single vendor!) and because if you don't trust the vendor\n> who gave you the browser binary in the first place, you can't\n> really trust anything done with a browser profile.\n\nThere is trust where intent is involved and there is trust where \nthe issue of things falling thru the cracks is involved. I am not\nsuggesting evil intent ... a bug is a bug.\n\nFurthermore, the original suggestion did not involved the browser at \nall and a solution only need depend on the browser correctly and\nuniquely identifying its identity. Some other collaborative repository\nowned by the content suppliers can provide the actual data base ...\nor the effective profile can be a compendium of multiple sources.\n\nIf we work on this issue, it needs to address coding and precedence for\nmerging first. Storage and exchange is a secondary issue.\n\nDave Morris\n\n\n\n", "id": "lists-010-7434679"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "HEAD is not a NOOP, OPTIONS is.  What is the rationale for\nusing HEAD to relay the final count instead of OPTIONS?\nDoes it make a difference when going through old proxies?\n\n....Roy [just curious]\n\n\n\n", "id": "lists-010-7443925"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    HEAD is not a NOOP, OPTIONS is.  What is the rationale for\n    using HEAD to relay the final count instead of OPTIONS?\n\nI guess I've never really understood what OPTIONS is supposed\nto do, nor does the existing spec language make it clear that\nthis is allowed to carry the various request-headers that\nare needed to report use-counts broken down by, say, User-agent.\n\nIn any event, while HEAD is not a \"no-op\", section 9.1.1 says\n\"the convention has been established that the GET and HEAD\nmethods should never have the significance of taking an action other\nthan retrieval.\"  So, from the point of view of the server's\nvisible state, doing a HEAD is indeed a no-op.  It does imply\nthat the server is going to access the meta-data of the resource\n(because the \"metainformation contained in the HTTP headers in response\nto a HEAD request SHOULD be identical to the information sent in\nresponse to a GET request\"), which does impose a cost on the server.\nBut no more than if the server had to do a conditional GET.\n\nFinally, OPTIONS responses include messages bodies, and HEAD\nresponses \"MUST NOT\", so I would expect the use of HEAD to\nrequire few bits transferred over the wire.\n\nActually, I never thought about using OPTIONS until now :-)\n\nMaybe we ought to have a true NOOP method?  But I'm not really\nthat worried about such slight changes in the load on a server.\n\n    Does it make a difference when going through old proxies?\n    \nYou mean, because HTTP/1.0 proxies don't understand OPTIONS and would\nreject such requests?  In theory, yes, although I'm not sure this would\nbe an issue, because a server that wants accurate use-counts would only\nbelieve them from a \"cooperative\" cache, which is identified by\n\"Connection: coop\" in the request, and we said\n\n     Note: a server might distrust such a request-header  when\n     received from an HTTP/1.0 client, which might have incorrectly\n     forwarded the Connection header.\n\nOne way for a server to deal with this case would be to send\nsomething like\nExpires: Sun, 06 Nov 1994 08:49:37 GMT\nCache-control: max-age=100000, max-uses=10\nwhich would tell the HTTP/1.0 proxy not to cache things, and the\n\"cooperative\" HTTP/1.1 proxy visible through that HTTP/1.0 proxy\nthat it could cache them and report its use-counts.  Then it\nwould indeed be useful to have the use-count relayed (naively)\nby the HTTP/1.0 cache, and so OPTIONS wouldn't work.  Of course,\nthis depends upon trusting the HTTP/1.0 cache to obey Expires,\nand I don't know if this is reasonable in practice.\n\n-Jeff\n\n\n\n", "id": "lists-010-7451984"}, {"subject": "Re: Entity Tag", "content": "On Tue, 13 Aug 1996, Daniel DuBois wrote:\n\n> Warning:  Everything that follows is an implementor's issue, and not\n> technically a protocol specification issue.\n> \n> Initially, I felt a hash of {the full pathname (to distinguish variants),\n> the size of the file, and the OS-reported last-modification time} would be\n> sufficient to be a strong validator.  But, technically, that doesn't\n> guarantee that a file that has changed twice in the same second has a\n> different ETag value, which the spec says is a must.\n\nHmm. I must admit, I never even *thought* of this. In implementing\nETag support for Apache 1.2 (which should be released sometime before\nthe turn of the century - I promise!), I used a hash of inode number,\nsize and last-modified date, which amounts to the same thing as yours,\nand I didn't see anything wrong with calling it a strong validator,\neither.\n\nBut... I dunno. What precisely are the odds that someone will modify a\nfile more than once per second, and leave the file the exact same\nlength? Pretty low, one would think.\n\nActually, it brings up an interesting point: when matching ETags for\nranges, one must use a strong validator comparison. But you can also\nuse dates. Namely, this is perfectly valid:\n\nGET /filename HTTP/1.1\nRange: bytes=30-500\nIf-Range: Thu, 15 Aug 1996 06:35:59 GMT\n\nSurely a date is a much weaker validator than the hashes that we are\nimplementing? Yet as I read the spec, it is valid in this\napplication.\n\n[...]\n\n> Certainly reading in the file and doing an MD5 or checksum of its contents\n> each time we serve it is out of the question for performance reasons.\n\nAgreed there.\n\n> I would think with our file-based web server, numerous changes per second\n> aren't as much of an issue as a database back-end that was generating\n> content dynamically.  I'm thinking that said dynamic application was the\n> focus and intent of the restrictions on the strong validators, so maybe I\n> can slide by?\n\nI would think that a database application would have to come up with a\nbetter way of making ETags, yes. But presumably that's something the\ndatabase is capable of doing. For a file-based web server such as\nSpyglass Server or Apache, I think that we can certainly \"slide\nby\".\n\nBut that's just my opinion.\n\n-- \n________________________________________________________________________\nAlexei Kosut <akosut@nueva.pvt.k12.ca.us>      The Apache HTTP Server\nURL: http://www.nueva.pvt.k12.ca.us/~akosut/   http://www.apache.org/\n\n\n\n", "id": "lists-010-7462372"}, {"subject": "I-D ACTION:draft-holtman-http-negotiation02.tx", "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Transparent Content Negotiation in HTTP                 \n       Author(s) : K. Holtman\n       Filename  : draft-holtman-http-negotiation-02.txt\n       Pages     : 38\n       Date      : 08/14/1996\n\nHTTP allows one to put multiple versions of the same information under a \nsingle URL.  Transparent content negotiation is a mechanism, layered on top\nof HTTP, for automatically selecting the best version when the URL is \naccessed.  This enables the smooth deployment of new web data formats \nand markup tags.              \n                                                 \nDesign goals for transparent content negotiation include low overhead on \nthe request message size, downwards compatibility, extensibility, enabling \nthe rapid introduction of new areas of negotiation, scalability, low cost \nof minimal support, end user control, and good cachability.                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-holtman-http-negotiation-02.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-holtman-http-negotiation-02.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-holtman-http-negotiation-02.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@ietf.org\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-7473442"}, {"subject": "POPUP WINDO", "content": "HI ! \n\n  This may be a simple query .  Please execuse.\n\n      I am writing an  Application in HTML .  Could somebody\n      tell me how do we generate a popup window when clicking\n      on an hyperlink.  (the TARGET=\"main\" directive generates \n      a new netscape session itself, which is rather clumsy).\n      I want something like the window which pops up when we\n      view the source .\n\nThanks in advance.\nRaj.\n\n\n\n", "id": "lists-010-7483861"}, {"subject": "HTTP 1.1 Server Available for Testin", "content": "Hi,\n\nWe have a web server implementing the HTTP 1.1 server MUSTS and most\neasy SHOULDs running at:\n\nhttp://wilson.ai.mit.edu/cl-http/cl-http.html\n\nThe server home page is at:\n\nhttp://www.ai.mit.edu/projects/iiip/doc/cl-http/home-page.html\n\nThe final prerelease is available for FTP complete with source code,\nwhich can be viewed over the web, but is most useful for advanced lisp\nprogrammers.\n\nAlthough the server implements a number of proxy-relevant headers, it\ndoes not provide 1.1 proxy service at the moment.\n\nA variety of computed pages are generated on the fly using chunked\ntransfer encoding (if you are a 1.1 client).  The particular machine on\nwhich this server is running has an adaptive TCP implementation, so you\nmay observe changes in the packet sizes as you interact with it.\n\nThe server makes efforts to keep connections open, as appropriate, when\nit returns non-success responses.\n\nThe server does not do content negotiation.\n\nIf anyone spots any bugs or shortcomings, please drop us a bug report at\nbug-cl-http@ai.mit.edu \n\nKindly treat the test machine with respect as it provides file service\nfor our research group as well.\n\nJohn Mallery\nhttp://www.ai.mit.edu/projects/people/jcma/jcma.html\n\n\n\n", "id": "lists-010-7490904"}, {"subject": "Re: POPUP WINDO", "content": "At 07:04 PM 15-08-96 IST, K.M.RAJAGOPAL wrote:\n>HI ! \n>\n>  This may be a simple query .  Please execuse.\n>\n>      I am writing an  Application in HTML .  Could somebody\n>      tell me how do we generate a popup window when clicking\n>      on an hyperlink.  (the TARGET=\"main\" directive generates \n>      a new netscape session itself, which is rather clumsy).\n>      I want something like the window which pops up when we\n>      view the source .\n>\n>Thanks in advance.\n>Raj.\n======================================================================\nHi!\n\nTry to targeted link to nonexisting frame. I think it will help you.\nBrowser will generate this frame. \n\nRagards, Maxxxx\n\n\n\n=================================================================\nReal name: Max Alexeev (Maxxxx), Global Ukraine-Kharkov. \n   webmaster@bbs.uanet.kharkov.ua (WWW related contacts)\n   max@bbs.uanet.kharkov.ua (personal contacts)\n   http://bbs.uanet.kharkov.ua/\n\n\n\n", "id": "lists-010-7499886"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "I wrote earlier in a reply to Simon Spero:\n>\n>I don't see how you can ever get the numbers working for user profile\n>caching.\n\nAfter reading Simon's HTTP-NG proposal, I must add something here.  It\nseems that HTTP-NG proposes a lot more than just user profile caching,\nand additional mechanisms in the proposal *will* make the numbers work\nfor HTTP-NG in the case I analyzed in my earlier message.\n\nSummarizing what I read Simon's text, it seems that HTTP-NG is not\nproposing plain profile caching as a negotiation scheme, but a\nPEP-like negotiation scheme, in which the initial request is kept\nsmall by allowing the server to go back and ask for more profile data.\nIn HTTP-NG, profile caching would only be an optional optimization on\ntop of the PEP-like scheme.  I thought that Simon's profiles would be\n5K chunks of data containing all information about a particular area\nof negotiation.  Instead they will usually only contain those\ncapabilities and preferences the server actually asked for in the\npast.\n\nI also wrote:\n>It seems to me that only transparent content negotiation scales for\n>P_same values of 1 in 1 million or more (while also still protecting\n>privacy).\n\nI now think that HTTP-NG negotiation will also scale for these P_same\nvalues, and that it will also allow privacy to be protected.  It is\nplain user profile caching, without any additional mechanisms, that\nwon't scale.\n\nI think that the HTTP-NG form of profile caching could also be\neffective as an optimization on top of transparent negotiation (in the\ncase where many requests are done on the same server).  The\noptimizations I have now in the transparent negotiation draft don't\nconcern themselves, like HTTP-NG does, with squeezing every last byte\nfrom the request message.  They instead cut RTT delays by using proxy\ncaches to respond to the request whenever possible.\n\nAs for the main problem discussed in this thread, how to share data\nfor negotiating around bugs the browser vendor does not care about: it\nseems that an internet draft to solve this problem would have to\ndefine a substantial amount of new stuff, beyond what is now in both\nHTTP-NG negotiation and transparent negotiation.\n\nKoen.\n\n\n\n", "id": "lists-010-7508390"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "Before people start getting jumpy ...\n\nThe Apache Group is not planning on releasing any server that uses\nthe native HTTP-version \"HTTP/1.1\" until after the IESG approves the\ndraft for RFC status (i.e., fixes that version in stone).  Since all\nof the HTTP/1.1 features can be tested as an HTTP/1.0 server (assumimg\nyou are careful not to send HTTP/1.1 features to HTTP/1.0 clients\nwhere it is noted in the spec that you MUST not do so), I strongly\nsuggest, for the sake of interoperability, that no application be shipped\nwith the \"HTTP/1.1\" label until after we all know that there will be\nno further requirements placed on HTTP/1.1 applications.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7519075"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "At 1:29 PM -0700 1996-08-15, Roy T. Fielding wrote:\n>Before people start getting jumpy ...\n>\n>The Apache Group is not planning on releasing any server that uses\n>the native HTTP-version \"HTTP/1.1\" until after the IESG approves the\n>draft for RFC status (i.e., fixes that version in stone).  Since all\n>of the HTTP/1.1 features can be tested as an HTTP/1.0 server (assumimg\n>you are careful not to send HTTP/1.1 features to HTTP/1.0 clients\n>where it is noted in the spec that you MUST not do so), I strongly\n>suggest, for the sake of interoperability, that no application be shipped\n>with the \"HTTP/1.1\" label until after we all know that there will be\n>no further requirements placed on HTTP/1.1 applications.\n>\n\nAre you saying that 1.0 clients do not ignore headers they don't understand?\nOr are you trying to say don't send chunked encoded stuff to 1.0 clients?\n\nI checked every occurence of MUST in the spec to make sure we were conforming,\nand I believe we are.\n\nCL-HTTP responds to 1.0 clients with 1.0 responses.  1.1 responses are\nreserved for clients or proxies that also advertise 1.1 functionality.\nMost of my user base is behind firewalls, with one prominent exception,\nand they like keeping their research on the cutting edge. Of course, since \nwe are in lisp, upgrades only take a couple of minutes. Servers don't\ngo down. Just load patches. No big deal.\n\n\n\n", "id": "lists-010-7527781"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Jeffrey Mogul:\n>\n     [Koen Holtman:]\n>    It would be better to [...]\n>    instead add stuff to make being cooperative less costly (e.g. a method\n>    to piggy-back hit count reports for other URLs on a request you have to\n>    send anyway).\n>\n>I don't think we need to add very much at all for this.  Remember\n>that the count report can be sent over the same persistent connection\n>as other operations being sent to the server.  A typical count report\n>would be\n>        HEAD /foo.html HTTP/1.1\n>        Host: www.w3.org\n>        Cache-control: use-count=37\n\nI guess you are right that this is small enough.  The thing that\nworries me more than the size of the request is your requirement on\nwhen to send it: a proxy must send it when removing the cache entry.\n\nBut when removing the cache entry, _the proxy does not necessarily\nhave an open persistent connection to the origin server in question_.\nI would be more happy if the draft explicitly allowed a proxy to\nreport the hit count of a removed cache entry within, say, one day of\nremoving it.  This would allow the proxy to batch the HEAD requests\nfor each server and send them at night, when the there is enough\nbandwidth left to make opening extra connections not hurt.\n\nNow, from a purely technical `we only specify the cache state\nobservable from the outside' point of view, you may be able to make a\ncase that the language in the spec already allows such batching, but\nI'd rather see the draft being explicit about this.\n\nI still think that max-uses=N for N>=2 does not work at getting any\nkind of reasonable upper bound for uncooperative caches, so that hit\ncounting can only work accurately if all caches are cooperative.\nTherefore, the draft had better make it very easy to be cooperative,\nand for some cache implementations it may only be easy if batching is\nallowed.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-7537412"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "On Thu, 15 Aug 1996, John C. Mallery wrote:\n\n> At 1:29 PM -0700 1996-08-15, Roy T. Fielding wrote:\n> >Before people start getting jumpy ...\n> >\n> >The Apache Group is not planning on releasing any server that uses\n> >the native HTTP-version \"HTTP/1.1\" until after the IESG approves the\n> >draft for RFC status (i.e., fixes that version in stone).  Since all\n> >of the HTTP/1.1 features can be tested as an HTTP/1.0 server (assumimg\n> >you are careful not to send HTTP/1.1 features to HTTP/1.0 clients\n> >where it is noted in the spec that you MUST not do so), I strongly\n> >suggest, for the sake of interoperability, that no application be shipped\n> >with the \"HTTP/1.1\" label until after we all know that there will be\n> >no further requirements placed on HTTP/1.1 applications.\n> >\n> \n> Are you saying that 1.0 clients do not ignore headers they don't understand?\n> Or are you trying to say don't send chunked encoded stuff to 1.0 clients?\n> \n> I checked every occurence of MUST in the spec to make sure we were conforming,\n> and I believe we are.\n\nNo, he's saying don't call yourself HTTP/1.1 until there is a HTTP/1.1\nspec to ahere to. See, the IESG could come back and say \"Whoa,\nfolks. We'll approve this spec, but only if you add a section that\nsays every time a request is made, the server MUST send to the\nPentagon, via overnight mail, a hot dog with lots of jalapeno\nmustard.\" Unlikely, but if it happened, you'd have a server that\ncalled itself HTTP/1.1, but was not HTTP/1.1 compliant. Which would be\na bad thing. So until the IESG approves the spec, you need to call\nyourself HTTP/1.0.\n\n-- \n________________________________________________________________________\nAlexei Kosut <akosut@nueva.pvt.k12.ca.us>      The Apache HTTP Server\nURL: http://www.nueva.pvt.k12.ca.us/~akosut/   http://www.apache.org/\n\n\n\n", "id": "lists-010-7547271"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "> Are you saying that 1.0 clients do not ignore headers they don't understand?\n> Or are you trying to say don't send chunked encoded stuff to 1.0 clients?\n\nThe latter.\n\n> I checked every occurence of MUST in the spec to make sure we were conforming,\n> and I believe we are.\n\nGreat -- but what I was saying is that you can't be sure you are conforming\nuntil the document stops changing, which means when the IESG recommends\nit for RFC status.  So, you need to wait for that before you can ship\nsoftware that responds with \"HTTP/1.1 200 OK\".\n\nI don't anticipate any more changes to the protocol, but stranger things\nhave happened.  I'd like to minimize the number of noncompliant servers\nadvertizing HTTP/1.1, and one way to do that is to encourage people to\nimplement the features within HTTP/1.0 first and only switch the version\nwhen it can be tested against a completed RFC.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7558398"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "On Thu, 15 Aug 1996, John C. Mallery wrote:\n\n> Are you saying that 1.0 clients do not ignore headers they don't understand?\n\nSome of those clients that advertise themselves as HTTP/1.0 will be\nclients that have started implementing some of the HTTP/1.1 features\n(but not all of them - so they still have to call themselves HTTP/1.0). \n\n> Or are you trying to say don't send chunked encoded stuff to 1.0 clients?\n> \n> I checked every occurence of MUST in the spec to make sure we were conforming,\n> and I believe we are.\n> \n> CL-HTTP responds to 1.0 clients with 1.0 responses.  1.1 responses are\n> reserved for clients or proxies that also advertise 1.1 functionality.\n\nThis brings up a question - How legitimate is it for the same server\n(identified ipaddress:port) to change personality between HTTP/1.1\nand HTTP/1.0?  If a server answers some requests with \"HTTP/1.1 200 ..\"\nand others with \"HTTP/1.0 200 ..\" (or any other response code), this\nmay confuse HTTP/1.1-aware clients.  \n\ndraft-ietf-http-v11-spec-07 says:\n\n3.1 HTTP Version\n\n... Applications sending Request or Response messages, as defined by this\nspecification, MUST include an HTTP-Version of \"HTTP/1.1\".\n\n\n8.2 Message Transmission Requirements\n\n...\nClients SHOULD remember the version number of at least the most recently\nused server...\n\n\n\n", "id": "lists-010-7567303"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "On Thu, 15 Aug 1996, Roy T. Fielding wrote:\n\n> Great -- but what I was saying is that you can't be sure you are conforming\n> until the document stops changing, which means when the IESG recommends\n> it for RFC status.  So, you need to wait for that before you can ship\n> software that responds with \"HTTP/1.1 200 OK\".\n\nC'mon Roy, they announced an experimental implemntation and solicited \ntesting against it with experimental HTTP/1.1 clients. Your response\nis quite harsh when you should be lauding their attempts to verify the\nspecification by attempting to implement it.\n\n> I don't anticipate any more changes to the protocol, but stranger things\n> have happened.  I'd like to minimize the number of noncompliant servers\n> advertizing HTTP/1.1, and one way to do that is to encourage people to\n> implement the features within HTTP/1.0 first and only switch the version\n> when it can be tested against a completed RFC.\n\nImpossible Roy ... many aspects of the new protocol depend on the version\nfor a experimental client to recognize the server as 1.1 and vice versa.\n\nIt is VERY IMPORTANT that such experimental servers not be released /\ndistributed as anything other than internet drafts until the RFC is\napproved. But lets focus on the issue that experiments be identified\nas such. Who knows, perhaps the protocol will change as a result of \nthis experiment.  \n\nDave Morris\n\n\n\n", "id": "lists-010-7576494"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Sure -- it already says it need not wait for the HEAD request to finish\nbefore removing the item; it wouldn't hurt to mention that it can defer\n(and batch) the requests as long as there is a high probability that it\nwill get made.\n\n>----------\n>From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n>Sent: Thursday, August 15, 1996 3:04 PM\n>To: mogul@pa.dec.com\n>Cc: koen@win.tue.nl; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: New document on \"Simple hit-metering for HTTP\"\n>\n>Jeffrey Mogul:\n>>\n>     [Koen Holtman:]\n>>    It would be better to [...]\n>>    instead add stuff to make being cooperative less costly (e.g. a method\n>>    to piggy-back hit count reports for other URLs on a request you have to\n>>    send anyway).\n>>\n>>I don't think we need to add very much at all for this.  Remember\n>>that the count report can be sent over the same persistent connection\n>>as other operations being sent to the server.  A typical count report\n>>would be\n>>        HEAD /foo.html HTTP/1.1\n>>        Host: www.w3.org\n>>        Cache-control: use-count=37\n>\n>I guess you are right that this is small enough.  The thing that\n>worries me more than the size of the request is your requirement on\n>when to send it: a proxy must send it when removing the cache entry.\n>\n>But when removing the cache entry, _the proxy does not necessarily\n>have an open persistent connection to the origin server in question_.\n>I would be more happy if the draft explicitly allowed a proxy to\n>report the hit count of a removed cache entry within, say, one day of\n>removing it.  This would allow the proxy to batch the HEAD requests\n>for each server and send them at night, when the there is enough\n>bandwidth left to make opening extra connections not hurt.\n>\n>Now, from a purely technical `we only specify the cache state\n>observable from the outside' point of view, you may be able to make a\n>case that the language in the spec already allows such batching, but\n>I'd rather see the draft being explicit about this.\n>\n>I still think that max-uses=N for N>=2 does not work at getting any\n>kind of reasonable upper bound for uncooperative caches, so that hit\n>counting can only work accurately if all caches are cooperative.\n>Therefore, the draft had better make it very easy to be cooperative,\n>and for some cache implementations it may only be easy if batching is\n>allowed.\n>\n>>-Jeff\n>\n>Koen.\n>\n>\n\n\n\n", "id": "lists-010-7586723"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": ">> CL-HTTP responds to 1.0 clients with 1.0 responses.  1.1 responses are\n>> reserved for clients or proxies that also advertise 1.1 functionality.\n> \n> This brings up a question - How legitimate is it for the same server\n> (identified ipaddress:port) to change personality between HTTP/1.1\n> and HTTP/1.0?  If a server answers some requests with \"HTTP/1.1 200 ..\"\n> and others with \"HTTP/1.0 200 ..\" (or any other response code), this\n> may confuse HTTP/1.1-aware clients.  \n\nWell, there's no requirement against it (because that particular confusion\nis harmless -- if it ever understands HTTP/1.1 then it is safe to send\nHTTP/1.1-only features to it).\n\nThe intention of the protocol is that the server should always respond\nwith the highest minor version of the protocol it understands within\nthe same major version of the client's request message.  The restriction\nis that the server cannot use those optional features of the\nhigher-level protocol which are forbidden to be sent to such\nan older-version client.  [BTW, there are no required features of\na protocol that cannot be used with all other minor versions within\nthat major version, since that would be an incompatible change and\nthus require a change in the major version.]\n\nIt sounds confusing, but it works in terms of providing a safe mechanism\nof indicating and deploying future protocol changes.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7600807"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "> On Thu, 15 Aug 1996, Roy T. Fielding wrote:\n>> Great -- but what I was saying is that you can't be sure you are conforming\n>> until the document stops changing, which means when the IESG recommends\n>> it for RFC status.  So, you need to wait for that before you can ship\n>> software that responds with \"HTTP/1.1 200 OK\".\n> \n> C'mon Roy, they announced an experimental implemntation and solicited \n> testing against it with experimental HTTP/1.1 clients. Your response\n> is quite harsh when you should be lauding their attempts to verify the\n> specification by attempting to implement it.\n\nI said \"ship\" -- as in letting it go outside your area of control where\nyou can personally ensure that changes to the protocol can be fixed\nin the server code and thus not result in long-term faulty applications\nin the hands of users that might not want to upgrade them.\n\nIf John hadn't said he was planning to ship it soon, I would not have\nbothered.  In any case, these messages are directed at the entire WG\nand not just his server (in fact, his server is the least of my worries,\nsince he has better control over his user base than other servers do).\n\n>> I don't anticipate any more changes to the protocol, but stranger things\n>> have happened.  I'd like to minimize the number of noncompliant servers\n>> advertizing HTTP/1.1, and one way to do that is to encourage people to\n>> implement the features within HTTP/1.0 first and only switch the version\n>> when it can be tested against a completed RFC.\n> \n> Impossible Roy ... many aspects of the new protocol depend on the version\n> for a experimental client to recognize the server as 1.1 and vice versa.\n\nThere is nothing in HTTP/1.1 that an HTTP/1.0 server cannot send to\nan experimental HTTP/1.1 client -- the only necessity is that the client \nnot have the normal attribute of disregarding HTTP/1.1 features received\nfrom an HTTP/1.0 server.  Nor is there any requirement anywhere that\nforbids an HTTP/1.0 server from properly interpreting HTTP/1.1 requests.\n\nBeing able to use HTTP/1.1 features within HTTP/1.0 is one of the design\nrequirements that Henrik and I laid down at the beginning.  It would be\nnice if people would take advantage of it.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7610256"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "On Thu, 15 Aug 1996, Roy T. Fielding wrote:\n\n[implementor of experimental 1.1 server:]\n> >> CL-HTTP responds to 1.0 clients with 1.0 responses.  1.1 responses are\n> >> reserved for clients or proxies that also advertise 1.1 functionality.\n> > \n[kw:]\n> > This brings up a question - How legitimate is it for the same server\n> > (identified ipaddress:port) to change personality between HTTP/1.1\n> > and HTTP/1.0?  If a server answers some requests with \"HTTP/1.1 200 ..\"\n> > and others with \"HTTP/1.0 200 ..\" (or any other response code), this\n> > may confuse HTTP/1.1-aware clients.  \n> \n> Well, there's no requirement against it (because that particular confusion\n> is harmless -- if it ever understands HTTP/1.1 then it is safe to send\n> HTTP/1.1-only features to it).\n\nOk.  But I assume that it should never be necessary for an HTTP/1.1\nserver to \"masquerade\" as HTTP/1.0 on its Status-Line, even whan the\nserver is talking to a 1.0 client?  In fact it may be better if an\nexperimental server never does this, if only to discover whether there\nare any clients out there which refuse responses starting with \n\"HTTP/1.1 \" (or treat them as 0.9...).\n\nI note that the sentence \"CL-HTTP responds to 1.0 clients with 1.0\nresponses\" is ambiguous (to me).  \"...responds ... with 1.0 responses\"\ncould mean\n\na) returns \"HTTP/1.0\" Status-Line, or\nb) returns \"HTTP/1.1\" Status-Line, but uses only 1.0 features.\n\nI think b) would be the better choice.  \n\n   Klaus\n\n> The intention of the protocol is that the server should always respond\n> with the highest minor version of the protocol it understands within\n> the same major version of the client's request message.  The restriction\n> is that the server cannot use those optional features of the\n> higher-level protocol which are forbidden to be sent to such\n> an older-version client.  [BTW, there are no required features of\n> a protocol that cannot be used with all other minor versions within\n> that major version, since that would be an incompatible change and\n> thus require a change in the major version.]\n> \n> It sounds confusing, but it works in terms of providing a safe mechanism\n> of indicating and deploying future protocol changes.\n\n\n\n", "id": "lists-010-7620653"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "Hi folks, could people stop getting all bent out of shape over this?\n\nRoy did not exactly put his case well, he made it sound like he was asking \nfor a chance for the Apache group to catch up rather than making a point\nabout version control issues. I wouldn;t mind less of the \"X. and I \nset out the requirements ages ago\" attitude. One of the things I have\nalways found very attractive about Tim is that he doesn't try to throw\nhis weight arround. \n\nThe 1.1 draft has not changed materially since the great cullback in\nMay. The wording has changed somewhat but the protocol itself has been\nstable. \n\nNow the IESG could in theory reject the draft but they would be very \nill advised to do so. In the first place there is no substantive\ndebate in the working group over 1.1, there are no outstanding \ncontroversies that concern it.\n\nThe Internet is not quite the place it was five years ago when returning\na draft to a working group for further consideration was of little\nimport. Today there is half the goddam stock market and an election\nriding on the Web. The HTTP/1.1 draft is already late and more than one\nvery senior and very influential person has made it very clear that\nif the IESG does not play ball it will be taken away from them. \n\nIf the IESG reject the draft they will have to have one hell of a good\nreason for it.\n\n\nNow lets consider what the draft is being forwarded as consideration as.\nRFC - how many people remember what those words mean? REQUEST for COMMENTS.\nThe draft is submitted as a proposed standard. There was a time when \nhaving an implementation of a standard before it was proposed was considered\nsomething of a good thing.\n\nNow Roy may be correct in his statement of the IETF rules but hes entirely\nout of line concerning their spirit. The idea is to develop good protocols\nwhich have well defined specifications which ensure that applications \ninteroperate. I don't think that the lawyerly attitude helps. If people\nthink thats how they want to work then they better expect to make room\nfor some real lawyers to join the group.\n\nPhill\n\n\n\n", "id": "lists-010-7630931"}, {"subject": "Problems with draft-ietf-http-v11-spec0", "content": "I realize that this is not the best time to air my complaints with\nthe http-v11 draft, but I hope better now than never...\nSo here it goes.\n\nOn Wed, 7 Aug 1996, Larry Masinter wrote:\n[in response to somebody's question regarding character sets]\n>                           ... and Section 2.2 is explicit about the\n> character set of request and response headers (most are restricted to\n> ASCII except those that use TEXT; those can be encoded using RFC 1522\n> rules), ...\n\nI have to disagree that Section 2.2 (or the draft as a whole) is clear\nabout character sets.  What is included in parentheses in the quote \nabove may be the intention, but it is not explicit.\n\nI tried to answer for myself the question \"Where, if at all, does\nHTTP 1.1 allow non-US-ASCII characters (other than within message-\nbody)?\", according to the latest draft.  I ran into several problems\nwith figuring out the answer.\n\nFirst, I would prefer if the answer to that question was clearly\nand explicitly stated somewhere in the draft.  As it is now, one\nhas to work one's way through several layers of BNF.\n\nOn to the details:  Unencoded non-US-ASCII octets (octets with the\nmost significant bit set, simply called eightbit chars in the following)\ncome in to the BNF in two ways:\n\n2.2\n       OCTET          = <any 8-bit sequence of data>\n\nwhence TEXT, comment and ctext, quoted-string and qdstring\nare all allowed to have eightbit chars.  (but not quoted-pair.)\n\nNote that this allows eightbit chars in lots of places, for example\nEtags or MIME parameters (including boundaries).\n\n\n3.2 Uniform Resource Identifiers\n3.2.1\n       national       = <any OCTET excluding ALPHA, DIGIT,\n                        reserved, extra, safe, and unsafe>\n\nwhence unreserved, uchar and pchar, and therefore nearly all parts\nof a URI (apart from the scheme) are allowed to have eightbit chars.\n\n3.2.1:\n    \"The BNF above includes national characters not\nallowed in valid URLs as specified by RFC 1738, since HTTP servers are\nnot restricted in the set of unreserved characters allowed to represent\nthe rel_path part of addresses, and HTTP proxies may receive requests\nfor URIs not defined by RFC 1738.\"\n\nI read that as meaning that HTTP application which handle such URIs\nwith eightbit chars can conform to the HTTP/1.1 spec, even if those\nURIs don't conform to RFC 1738.\n\nAlthough the quoted sentence doesn not explicitly speak of generating\nsuch URIs, there is nothing forbidding it.  (And it seems logical\nthat a server accepting requests for such URI's should also be allowed\nto generate Location: headers etc. containing them.)\n\nOn the other hand, in\n4.2 Message Headers\n\n       message-header = field-name \":\" [ field-value ] CRLF\n\n       field-name     = token\n       field-value    = *( field-content | LWS )\n \n       field-content = <the OCTETs making up the field-value\n                        and consisting of either *TEXT or combinations\n                        of token, tspecials, and quoted-string>\n\nNote that it doesn't say\n\n       field-content = <the OCTETs making up the field-value\n                        and consisting of either *TEXT or combinations\n                        of token, tspecials, URI, and quoted-string>\n\nnor does it say\n\n       field-content = <the OCTETs making up the field-value\n                        and consisting of either *TEXT or combinations\n                        of token, tspecials, quoted-string etc.>\n\nrom this I conclude that\n(a)  an URI in a field-content (which is not within a quoted-string),\n     since it is not defined as arbitrary *TEXT, has to be understood \n     as being comprised of component tokens, and\n(b)  (since eightbit chars are not allowed in tokens) an URI in a\n     field-content cannot contain unencoded eightbit chars.\n\nBut the BNF for specific headers uses rules which seem to allow\neightbit chars, for example\n\n14.30\n       Location       = \"Location\" \":\" absoluteURI\n\nI conclude that the draft is far from clear.\n\n                          *   *   *\n\nSome other (mostly BNF related) weirdnesses:\n\nSInce URI is comprised of tokens (see (a) above), the following\nseems to apply:\n\n2.1 Augmented BNF\n...\nimplied *LWS\n     The grammar described by this specification is word-based. Except\n     where noted otherwise, linear whitespace (LWS) can be included\n     between any two adjacent words (token or quoted-string), and\n     between adjacent tokens and delimiters (tspecials), without\n     changing the interpretation of a field. At least one delimiter\n     (tspecials) must exist between any two tokens, since they would\n     otherwise be interpreted as a single token.\n\nThat is, \n  http   :   / / host.dom.ain / etc. ? blah\nwould be a valid way to write \n  http://host.dom.ain/etc.?blah\nin HTTP headers.\n\nThe proviso of \"except where noted otherwise\" is not used anywhere\nin the description of URIs.  In fact, the only place where it is\nused is in \n3.7 Media Types\n\n   \"Linear white space (LWS) MUST NOT be used between the type and\nsubtype, nor between an attribute and its value.\"\n\nBut note that e.g. 14.1 Accept does not refer to the definition\nof media-type from 3.7, but defines media-range without explicitly\ndisallowing LWSP.  (nor does it disallow e.g. \"; q = 0.5\")\n\n3.8 Product Tokens is another place where LWS should be explicitly\ndisallowed.  \n\n                          *   *   *\n\nComments (within parentheses) should probably allowed in more\nplaces - at least, in 19.4.7\n       MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT\nshould probably be\n       MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT *comment\n\nAlso,\n      Via =  \"Via\" \":\" 1#( received-protocol received-by [ comment ] )\nin 14.44 should maybe become\n      Via =  \"Via\" \":\" 1#( received-protocol received-by [ *comment ] )\n\n                          *   *   *\n\n10.3.6 305 Use Proxy\n\nThe requested resource MUST be accessed through the proxy given by the\nLocation field. The Location field gives the URL of the proxy. The\nrecipient is expected to repeat the request via the proxy.\n\nHow exactly does is a proxy \"given\" by a Location field?\nLocation normally contains an URI, and URIs point to resources but\nnot (normally) applications (the proxy).  Does the URI have to be\na http_URL, does the abs_path have to be empty (or is it required\nto be \"/\"), and what if not?\n\n                          *   *   *\n\n3.6 Transfer Codings\n...\n       hex-no-zero    = <HEX excluding \"0\">\n\n       chunk-size     = hex-no-zero *HEX\n...\n       chunk-data     = chunk-size(OCTET)\n\nWhy does this rule use HEX and not DIGIT?  Does this mean the\nchunk-size is hexadecimally encoded?\n\n                          *   *   *\n\nA remark regarding 14.1 Accept:\nIt's a pity there is a \"q=\", but not a \"mxb=\".  An oversight or\nintentional?\n\n                          *   *   *\n\nOkay that's all.  \n\n   Klaus\n\n\n\n", "id": "lists-010-7640627"}, {"subject": "Re: HTTP 1.1 Server Available for Testin", "content": "David W. Morris:\n>On Thu, 15 Aug 1996, Roy T. Fielding wrote:\n>>\n>> I don't anticipate any more changes to the protocol, but stranger things\n>> have happened.  I'd like to minimize the number of noncompliant servers\n>> advertizing HTTP/1.1, and one way to do that is to encourage people to\n>> implement the features within HTTP/1.0 first and only switch the version\n>> when it can be tested against a completed RFC.\n>\n>Impossible Roy ... many aspects of the new protocol depend on the version\n>for a experimental client to recognize the server as 1.1 and vice versa.\n\nI agree.  For a start, the persistent connections default case depends\non the version number.  So does the `Message Transmission\nRequirements' stuff.\n\n>It is VERY IMPORTANT that such experimental servers not be released /\n>distributed as anything other than internet drafts until the RFC is\n>approved. But lets focus on the issue that experiments be identified\n>as such. Who knows, perhaps the protocol will change as a result of \n>this experiment.\n\nI think that Roy is attacking a non-problem here.  We have anticipated\nall along that it might be necessary to change the 1.1 version number\ninto 1.23723 if too much non-compliant stuff happens under the name of\n1.1.\n\nThe IESG could change the version number the day before the draft gets\nRFC status if too much non-compliant stuff happens, irrespective of\nwhether the non-compliant stuff was caused by the draft changing or by\nthe actions of some marketing department.\n\nI don't see the need to aggressively protect the 1.1 `brand name'\nbefore the draft makes RFC status; in fact I think it would be *very\ndangerous* to do so.\n\nI think we absolutely need large scale compatibility experiments with\n1.1 draft compliant software actually calling itself HTTP/1.1\ncompliant.  Only large scale experiments with sending the 1.1 version\nnumber will make it clear whether existing 1.0 proxies and gateways\ncorrectly downgrade request and response message versions, and whether\nthere are browsers that choke on seeing `HTTP/1.1 200 OK'.  The HTTP\nminor version number being insignificant might be a nice and\ntime-honored design goal for 1.1, but it it as yet unknown whether\nthis design goal is actually met *by all 1.0 implementations out\nthere*.  We cannot afford to discourage testing in this area.  It is\nvastly preferable to get nasty surprises *before* the draft is frozen\nas an RFC.\n\nShipping 1.1 draft compatible software which calls itself 1.1 by\ndefault would also be a bit too much for my taste, but I would\ndefinitely encourage shipping such software with a `send version\nnumber 1.0 or 1.1' toggle defaulting to 1.0.\n\nFinally, the slogan\n\n  `Save the Internet! Implement HTTP/1.1 *now*! Don't say you\n  implement HTTP/1.1!'\n\nsounds like something out of a Dilbert cartoon.  This is not the\nmessage I would want to send to the development community.\n\n>Dave Morris\n\nKoen.\n\n\n\n", "id": "lists-010-7656237"}, {"subject": "I-D ACTION:draft-ietf-http-v11-spec07.txt, .p", "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\nNote: This revision reflects comments received during the last call period.\n\n       Title     : Hypertext Transfer Protocol -- HTTP/1.1                 \n       Author(s) : R. Fielding, J. Gettys, J. Mogul, \n                   H. Frystyk, T. Berners-Lee\n       Filename  : draft-ietf-http-v11-spec-07.txt, .ps\n       Pages     : 153\n       Date      : 08/15/1996\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol for\ndistributed, collaborative, hypermedia information systems. It is a \ngeneric, stateless, object-oriented protocol which can be used for many \ntasks, such as name servers and distributed object management systems, \nthrough extension of its request methods. A feature of HTTP is the typing \nand negotiation of data representation, allowing systems to be built \nindependently of the data being transferred.  \n                             \nHTTP has been in use by the World-Wide Web global information initiative \nsince 1990. This specification defines the protocol referred to as \n\"HTTP/1.1\".                                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-v11-spec-07.txt\".\n Or \n     \"get draft-ietf-http-v11-spec-07.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-07.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-07.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-07.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@ietf.org\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-7667197"}, {"subject": "Re: Problems with draft-ietf-http-v11-spec0", "content": "Klaus Weide:\n[...]\n>A remark regarding 14.1 Accept:\n>It's a pity there is a \"q=\", but not a \"mxb=\".  An oversight or\n>intentional?\n\nIntentional: there is no known use of mxb= in requests, and you can\nusually get the same effect by using Range:, so mxb= was removed to\nmake the protocol a bit smaller/cleaner.\n\n>   Klaus\n\nKoen.\n\n\n\n", "id": "lists-010-7678057"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "On Fri, 16 Aug 1996, Kevin Kenny wrote:\n\n> I can't speak for David, but I maintain a data base of browser capabilities\n> to fight browser bugs.  Among others are\n> \n> - a number of browsers get confused if there are multiple\n>   <INPUT TYPE=IMAGE> with the same SRC= attribute.\n> \n> - a number of browsers get various mysterious failures and\n>   rendering errors on anchors in tables.\n> \n> Those are the two big ones on my site.  There are probably some others.\n> Neither of these is a presentation issue.\n\nOne that has risen more than once in my projects is the browser's \nhanding of the following <form> fragment:\n\n   <select name=x>\n     <option>Choice 1\n     <option>Choice 2\n     <option>Choice 3\n   </select>\n\nOne browser does what I prefer (default is nothing selected) but doesn't\nfollow the HTML standard as I read it and the other follows \nthe standard (default is first is selected). Yet another browser ignores\nselected in <option selected>.\n\nDave Morris\n\n\n\n", "id": "lists-010-7686407"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Koen writes:\n    But when removing the cache entry, _the proxy does not necessarily\n    have an open persistent connection to the origin server in question_.\n    I would be more happy if the draft explicitly allowed a proxy to\n    report the hit count of a removed cache entry within, say, one day of\n    removing it.  This would allow the proxy to batch the HEAD requests\n    for each server and send them at night, when the there is enough\n    bandwidth left to make opening extra connections not hurt.\n    \nPaul writes:\n    Sure -- it already says it need not wait for the HEAD request to finish\n    before removing the item; it wouldn't hurt to mention that it can defer\n    (and batch) the requests as long as there is a high probability that it\n    will get made.\n\nI see no problem with saying that the proxy can defer and batch\nthe final-report HEAD requests but I do not believe that it makes\nsense to have this delayed until \"at night\" (which is a problematic\nconcept, since it assumes that the proxy actually knows when more\nbandwidth will be available).\n\nI'm worried that long delays in updating this information will\ndiscourage origin servers from using the mechanism, and I'm also\nworried that any mechanism that causes all of the proxies within\na particular time zone to spontaneously generate a large stream\nof requests at a particular point in time will cause the nasty\nsynchronized congestion that Floyd and Jacobson warned about in\nProc. SIGCOMM '93.  (One could avoid this by some randomization,\nbut they point out that this is much harder than one might suppose.)\n\nSo my suggestion is that the spec says something like:\n\nA proxy MAY defer the final-report HEAD request to an origin\nserver, and send a batch of these later, either piggybacked on\nits next request to the server or when sufficient resources are\navailable.  A proxy SHOULD NOT delay these reports longer than\nabsolutely necessary, to avoid discouraging the use of\nhit-metering.  A proxy SHOULD NOT delay these reports if there\nis a significant probability that it will be unable to deliver\nthem after the delay.  A proxy MUST NOT delay these reports for\ndelivery at a non-randomly-chosen specific time of day; this is\nnecessary to prevent unnecessary congestion.\n\n-Jeff\n\n\n\n", "id": "lists-010-7696261"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "On Fri, 16 Aug 1996, Jeffrey Mogul wrote:\n\n> necessary to prevent unnecessary congestion.\n                             ^^^^^^^^^^^\n\nI think it would be sufficient to replace unnecessary with 'network'.\nOr add network after unnecessary would be a second choice but \nthe double use necessary to refer to two different activities is\nawkward and perhaps confusing.\n\n\nDave\n\n\n\n", "id": "lists-010-7706379"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": "I'll make the changes Jeff suggests to the next revision.\n\n>----------\n>From: Jeffrey Mogul[SMTP:mogul@pa.dec.com]\n>Sent: Friday, August 16, 1996 12:50 PM\n>To: Paul Leach\n>Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Subject: Re: New document on \"Simple hit-metering for HTTP\" \n>\n>Koen writes:\n>    But when removing the cache entry, _the proxy does not necessarily\n>    have an open persistent connection to the origin server in question_.\n>    I would be more happy if the draft explicitly allowed a proxy to\n>    report the hit count of a removed cache entry within, say, one day of\n>    removing it.  This would allow the proxy to batch the HEAD requests\n>    for each server and send them at night, when the there is enough\n>    bandwidth left to make opening extra connections not hurt.\n>    \n>Paul writes:\n>    Sure -- it already says it need not wait for the HEAD request to finish\n>    before removing the item; it wouldn't hurt to mention that it can defer\n>    (and batch) the requests as long as there is a high probability that it\n>    will get made.\n>\n>I see no problem with saying that the proxy can defer and batch\n>the final-report HEAD requests but I do not believe that it makes\n>sense to have this delayed until \"at night\" (which is a problematic\n>concept, since it assumes that the proxy actually knows when more\n>bandwidth will be available).\n>\n>I'm worried that long delays in updating this information will\n>discourage origin servers from using the mechanism, and I'm also\n>worried that any mechanism that causes all of the proxies within\n>a particular time zone to spontaneously generate a large stream\n>of requests at a particular point in time will cause the nasty\n>synchronized congestion that Floyd and Jacobson warned about in\n>Proc. SIGCOMM '93.  (One could avoid this by some randomization,\n>but they point out that this is much harder than one might suppose.)\n>\n>So my suggestion is that the spec says something like:\n>\n>A proxy MAY defer the final-report HEAD request to an origin\n>server, and send a batch of these later, either piggybacked on\n>its next request to the server or when sufficient resources are\n>available.  A proxy SHOULD NOT delay these reports longer than\n>absolutely necessary, to avoid discouraging the use of\n>hit-metering.  A proxy SHOULD NOT delay these reports if there\n>is a significant probability that it will be unable to deliver\n>them after the delay.  A proxy MUST NOT delay these reports for\n>delivery at a non-randomly-chosen specific time of day; this is\n>necessary to prevent unnecessary congestion.\n>\n>-Jeff\n>\n>\n\n\n\n", "id": "lists-010-7715502"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "With the latest suggested addition to the \"simple\" hit metering\ndraft it has become as complex as my original proposal. The\nonly difference being that in my proxy notification draft I \nwas also considering a possible generalisation of the\nmechanism to cover other types of notification such as updates.\n\nDo people really believe that using the HEAD (or whatever) method\nto communicate hit counts is really much simpler than the server\nperiodically requesting log files? I originally started with a \nscheme very close to the \"simple\" proposal. I had to expand the\nscheme after talking to people from Nielssen, Gallup and co.\n\nIn the notification draft defered loading of logfiles was handled\nvia a 3 byte, uuencoded string of a 24 bit mask corresponding\nto prefered download periods (in GMT).\n\nNote that on the compresssion side log file exchange is a lot \nbetter than \"simple\" hit count. Each logfile entry is a lot more \ncompact than a simulated hit.\n\nPhill\n\n\n\n", "id": "lists-010-7727506"}, {"subject": "Re: Conventions for Sharing User Agent Profile", "content": "Fisher Mark:\n>Koen Holtman:\n>>As for the main problem discussed in this thread, how to share data\n>>for negotiating around bugs the browser vendor does not care about: it\n>>seems that an internet draft to solve this problem would have to\n>>define a substantial amount of new stuff, beyond what is now in both\n>>HTTP-NG negotiation and transparent negotiation.\n>\n>Since user agent negotiation is just a special case of content negotiation, \n>we should definitely merge the two together for now.\n\nYes.  There are actually two ways to layer browser bug negotiation on\ntop of transparent content negotiation.\n\nSuppose browser PQ has `5dtables' in its profile (I support 5\ndimensional tables), but that you know from your browser bug database\nthat the support for 5dtables in PQ2.3 is buggy.  Knowing this, you can\n\n a) filter incoming Accept-Features headers from the PQ2.3 browser\n    to change \n\n         Accept-Features: frames, 5dtables, *\n\n    into\n\n         Accept-Features: frames, !5dtables, *\n\n b) filter or patch your variant lists to change the list\n\n     {\"home.html.5d\" 1.0 {features frames 5dtables}}\n     {\"home.html.2d\" 0.8 {features frames}}\n     {\"home.html.1d\" 0.7}\n\n   into\n\n     {\"home.html.5d\" 1.0 {features frames 5dtables !user-agent=PQ2.3}}\n     {\"home.html.2d\" 0.8 {features frames}}\n     {\"home.html.0d\" 0.7}\n\n   so that the first variant won't ever be chosen for PQ2.3.  \n\nI think that b) is best, in particular because this allows caching\nproxies to do the browser bug negotiation, saving the RTT to the\norigin server.  With a collection of numeric and non-numeric feature\ntags containing the name, version, major+minor version, and platform\nof a browser, you would be able to do pretty good bug negotiation in a\ncachable way.  Provided of course that the user agent does not lie\nwhen sending values for these feature tags.\n\nTransparent content negotiation is suitable as a carrier mechanism for\nbug negotiation.  You can hand-edit your variant lists if you know\nabout certain browser bugs.  But in a real bug negotiation\ninfrastructure, there would be no need to hand-edit your variant\nlists: some standard software layer on your server would take care of\nthis automatically, using the latest knowledge about bugs.  To get to\nthis level of automation, you would still have to define a substantial\namount of new stuff.  You need to define a bug database format and a\nway to merge bug databases for a start.\n\n>Mark Leighton Fisher\n\nKoen.\n\n\n\n", "id": "lists-010-7736022"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": "At the most gross and simplistic way of measuring complexity (weight),\nyour proposal was three I-Ds, totalling 26 pages; ours is one draft of\n11 pages -- factoring out bolierplate, that's about 1/3 as much real\ntext.\n\nThe minimum implementation is not much more than to attach a counter to\nentries in the cache and report them every time the entry is\nrevalidated. Adding an implementation of max-uses to the minimum\nimplementation adds a new way of controlling cache expiration that may\nbe useful in contexts other than hot counting. The next level of\nimplementation is to initiate an asynchronous HEAD when purging an\nentry. If you're really masochistic, you can defer sending the HEAD\nhoping to batch a bunch of them together -- but that's completely\noptional (and to my mind of dubious value).\n\n>----------\n>From: hallam@ai.mit.edu[SMTP:hallam@ai.mit.edu]\n>Sent: Friday, August 16, 1996 2:17 PM\n>To: cuckoo.hpl.hp.com@http-wg.uucp\n>Cc: hallam@ai.mit.edu\n>Subject: Re: New document on \"Simple hit-metering for HTTP\" \n>\n>\n>\n>With the latest suggested addition to the \"simple\" hit metering\n>draft it has become as complex as my original proposal. The\n>only difference being that in my proxy notification draft I \n>was also considering a possible generalisation of the\n>mechanism to cover other types of notification such as updates.\n>\n>Do people really believe that using the HEAD (or whatever) method\n>to communicate hit counts is really much simpler than the server\n>periodically requesting log files? I originally started with a \n>scheme very close to the \"simple\" proposal. I had to expand the\n>scheme after talking to people from Nielssen, Gallup and co.\n>\n>In the notification draft defered loading of logfiles was handled\n>via a 3 byte, uuencoded string of a 24 bit mask corresponding\n>to prefered download periods (in GMT).\n>\n>Note that on the compresssion side log file exchange is a lot \n>better than \"simple\" hit count. Each logfile entry is a lot more \n>compact than a simulated hit.\n>\n>Phill\n>\n>\n>\n\n\n\n", "id": "lists-010-7746654"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Hang on a sec, of the 3 drafts only one was to do notification.\n\nI don't think that length of the draft is a good way of measuring\nthe complexity of the draft. I thought spliting drafts up was\nflavour of the month :-)\n\nThe others are linked insofar as they extend the functionality\nbut the core of the notification idea is the same size.\n\nWhat I am more worried about is that the people are increasing the\nrequirements beyond those originally met by \"simple\" spec. Or\nto put it another way, by the time you meet the demands of the\nvarious advertsing brokers you may well be looking at 26 rather than\n11 pages (closer to 40% than 1/3 :-).\n\n\nOne factor you will have to take account of is the need to forward\nsession id information. This means either trapping cookies or\nsome sort of explicit session id such as I proposed. The value of \nadvertising is very heavily dependent on the demographic profile.\nLook at doubleclick where demographic constraints increase the\ncost of a 100,000 hits from 9K to 29K. \n\nPersonally I prefer my session id scheme :-)\n\n\nPhill\n\n\n\n", "id": "lists-010-7758266"}, {"subject": "RE: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">----------\n>From: hallam@ai.mit.edu[SMTP:hallam@ai.mit.edu]\n>Sent: Friday, August 16, 1996 5:57 PM\n>To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>Cc: hallam@ai.mit.edu\n>Subject: Re: New document on \"Simple hit-metering for HTTP\" \n>\n>\n>Hang on a sec, of the 3 drafts only one was to do notification.\n>\n>I don't think that length of the draft is a good way of measuring\n>the complexity of the draft. I thought spliting drafts up was\n>flavour of the month :-)\n\nThat's why I said it was a crude measure.\n>\n>The others are linked insofar as they extend the functionality\n>but the core of the notification idea is the same size.\n\nBut just being notified isn't enough, right? The proxy has to implement\nthe extended log format in order to transfer the information in a\nstandard way, and so on with session IDs.\n>\n>What I am more worried about is that the people are increasing the\n>requirements beyond those originally met by \"simple\" spec. Or\n>to put it another way, by the time you meet the demands of the\n>various advertsing brokers you may well be looking at 26 rather than\n>11 pages (closer to 40% than 1/3 :-).\n\nSorry -- that was a thinko -- if you assume 2 pages of boilerplate per\ndraft, your 3 have 20 pages of real text and ours has 9 -- closer to\n>50%, actually. Even at 1 page of boilerplate per draft, it's 10/23....\n>\n>One factor you will have to take account of is the need to forward\n>session id information. This means either trapping cookies or\n>some sort of explicit session id such as I proposed. The value of \n>advertising is very heavily dependent on the demographic profile.\n>Look at doubleclick where demographic constraints increase the\n>cost of a 100,000 hits from 9K to 29K.\n\nPart of our draft is just a note about how to get referer information\nwithout adding to the protocol.\n>\n>Personally I prefer my session id scheme :-)\n\nI'm not knocking your scheme -- its clearly the only approach to getting\nall the information one would get if the origin server saw all the\nrequests. I'd be quite happy if everyone said they were really going to\ndo it your way, and it got deployed really soon. I was only objecting to\nthe characterization that ours is just as complex. I don't really want\nto spend a lot of time trying to quantify their relative complexity  --\nit would be better spent getting both approaches through the\nstandardization process so that the decision can be made the way it\nusually is -- by implementors and customers voting where they want to be\nin cost/benefit/time-to-implement/time-to-deploy space.\n>\n>\n\n\n\n", "id": "lists-010-7766883"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">But just being notified isn't enough, right? The proxy has to implement\n>the extended log format in order to transfer the information in a\n>standard way, and so on with session IDs.\n\nNope, session ids are orthogonal but are needed if you want to do\nsession tracking behind a proxy. I think that this is necessary for\nboth your scheme and mine.\n\nThe extended log format can be implemented in an afternoon with\nlittle trouble provided the items logged are fixed. If you go for\nallowing a choice of the items to log it gets trickier but not\nmuch. \n\nThe pain for Xlog is for the analysis tool writers, not the server\nwriters :-)\n\nPhill\n\n\n\n", "id": "lists-010-7779782"}, {"subject": "Re: When to make objects uncacheable ", "content": "Andrew Daviel writes:\n\n| Supposing the above premises are reasonable, my question is whether\n| the output of more general search engines should be made cacheable?\n| It seems to me that certain queries are fairly popular, and that\n| some benefit might be had from cacheing the responses. On the other\n| hand, the number of possible URLs expands exponentially with the\n| length of the query string, so that cacheing every response would be\n| unreasonable, filling up caches with never-to-be-repeated requests.\n\nrom a quick scan through the logs, I don't see our proxies repeatedly \nservicing the same queries of the same search engines.  Does anyone \nelse ?  Our users seem to be coming up with quite sophisticated \nqueries, presumably because of the difficulty of actually finding \nanything on the Web... \n\nMartin\n\n\n\n", "id": "lists-010-7788473"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "Jeffrey Mogul:\n>\n>If I understand your argument, it is that in order to bound the\n>size of the error in the hit count to lie within a reasonable\n>range, the max-uses setting would have to be so small that it\n>would effectively disable caching.\n\nYes.  For uncooperative caches.\n\n     [Koen Holtman:]\n>    I'd like to see *actual statistics* disprove my argument\n\n>So I got a day's worth of log entries from our proxy.  Here are\n>some statistics:\n>\n>        589705  total log entries\n>        529756  after removing non-HTTP URLs with \"?\", \"cgi\", or \"htbin\"\n>        245481  unique \"cachable\" URLs\n>        189723  \"cachable\" URLs referenced only once during the trace\n>         55758  \"cachable\" URLs referenced more than once\n\nIt's very tricky to extrapolate from a day's worth of log entries: to\ndo these statistics right, you would have to count over the lifetime\nof a cache entry, which is presumably a lot longer than 1 day for your\ncache.  I find it difficult to guess in what direction your end\nresults would change if you calculate over log entry lifetimes.\n\n>That's an effective cache hit rate of about 23%, not counting\n>things that can't be cached, and ignoring any misses that were\n>caused by modifications to the resources.\n\nEek! I would calculate a ( 529756 - 245481 ) / 529756 * 100% = 54% hit\nrate for your figures, also ignoring misses due to modification\n(including the semi-modification known as cache busting!).  What is\nyour definition of hit rate?\n\n>Supposing that, for each of the \"cachable\" URLs referenced more than\n>once, the origin server sent max-uses=3.\n[...]\n>220936 uses would not have to be forwarded to the origin server,\n>which comes out to about 37% of all the references logged.\n\nSo if cache busting is replaced by max-uses=3, you expect a 37% cache\nhit rate (i.e. RTT savings in 37% of all cases) in an uncooperative\ncache, where it earlier had a 0% hit rate for the offending server\n\nThere are several factors to pollute this figure: 1 day sample, not\nfactoring out dynamic and authenticated content which is uncachable,\nnot counting the 8th, 12th, ... hits, but let's forget about those.\n\n>Now, it's quite true that not every server insists on demographics\n>information, and so the actual number of references saved would\n>presumably be lower.  But this should give some idea of the\n>magnitude of the possible savings, and I don't think it's insignificant.\n\nYour statistics don't answer the main question I have: does max-uses=3\n(or max-uses=2 for that matter) give a good enough upper bound to make\nsites switch from cache busting to max-uses=3?\n\nUsing figures from your post:\n\n         245481 unique \"cachable\" URLs\n         228240 of these were referenced 1, 2, or 3 times\n          17215 were referenced more than 3 times\n\n         529756 references on \"cachable\" URLs\n         276401 references to URLs referenced 1, 2, or 3 times.\n         253355 of these references were to URLs\n                referenced more than 3 times\n\nWe can calculate how good the upper bound is.  If we assume\noptimistically that all references to `more than 3 times' URLs are\nreported under max-uses=3, we have\n\n   228240 + 253355 = 481595 known uses.\n\nFor the 1,2,3 URLs, the server handed out 3 * 228240 = 684720 uses\nwhich never led to any reports.  481595 + 684720 = 11663135.  This\nmeans that the origin server knows\n\n  481595  <= actual uses <= 11663135 .\n\nBut this upper bound is a factor 2.4 higher, which makes it hardly\nuseful.  So max-uses=3 *still* gives you a useless upper bound, and\nyou can't expect that people will switch from using cache busting to\nusing max-uses=3.\n\n(Note that the real actual uses, 529756 uses, are only a factor 1.1\nhigher than the 481595 reported, but this good figure is caused for a\nlarge part by the optimistic assumption that all uses of `more than 3\ntimes' URLs are reported.)\n\nNow, to do all of the above statistics _right_, you would have to have\nfigures on how many times the contents of a cache slot are served\nduring the lifetime of the cache slot.  Unfortunately, I don't know of\nany data set with these figures.  But I feel safe in saying that we\ncan forget about the uncooperative cache option.  It won't work, and\nshould be removed from the draft to make it shorter.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-7796684"}, {"subject": "Re: Call for compatibility tester", "content": ">>There is a difference between being compatible with older versions\n>>of HTTP and being compatible with old browsers that do not implement\n>>any version of HTTP correctly.\n> \n> HTTP/1.0 is an informational standard.  As far as I know, this means\n> that the IETF does not have an opinion on whether lynx implements\n> HTTP/1.0 correctly.\n\nI need to correct this misunderstanding. As far as the IETF (and W3C)\nis concerned, RFC 1945 is the only definition of HTTP/1.0 -- there simply\nis no other reference for that protocol.  The fact that it is an\n\"informational\" specification rather than a \"proposed standard\" is because\nwe intend HTTP/1.1, a *different* but compatible protocol, to be the\nproposed standard for HTTP (a family of protocols).  A specification\ndoesn't need to be a proposed standard in order to be the definition\nof that protocol.\n\n> You obviously have strong opinions about lynx not being a HTTP/1.0\n> application, but these are not universally held.  If the conneg draft\n> would continue to use the 300 code (without providing an adequate\n> escape hatch) now that the lynx interoperability problems are known,\n> this would effectively encode your strong opinions in the spec.  I\n> don't think that would be appropriate.\n\nI don't see how any application which fails to interpret a response\nsimply because it doesn't understand a valid response code (a field which\nis and has always been extensible, and thus always intended to result in\nnew codes being unrecognized by older applications) can be considered a\nvalid implementation of HTTP/1.0.  It is a bug which needs to be fixed.\nHTTP cannot be concerned about such bugwards-compatibility simply because\nit is impossible to extend HTTP at all if we don't assume some minimal\nlevel of functionality of the part of the applications.  Implementations\ndo need to be concerned with such bugwards-compatibility, but they do\nso by means already established by the protocol (User-Agent and Server\nidentification and avoidance), not by changing the protocol every time\nsomebody implements it wrong.\n\n> [...]\n>>  The only correct way to deal with such browsers is to\n>>exclude them on the basis of their User Agent field, \n> \n> Using this compatibility hack would be very expensive; you would have\n> to disallow proxies from ever sending a cached list (300) response to\n> a non-negotiating user agent, which means that you will have to handle\n> all these requests yourself.  \n\nYes, in the short run.  However, we are concerned with the long run, and\nin the long run we would expect such clients to be fixed.  Adding hacks\nto an implementation is okay because all implementations are temporary;\nadding hacks to the protocol means that correctly implemented applications\nwill suffer forever.\n\n> The current draft does not allow you to use Vary to optimize here:\n> proxies ignore the Vary header in cached list responses when using the\n> network negotiation algorithm.  The spec could be extended with more\n> vary rules, but I don't think this is the way to go.\n\nI'll have to check your spec, but you need to be careful in considering\nthe overlapping purposes of Vary and Alternates -- Alternates does not\nact as a replacement for Vary.  Vary details the dimensions of variance\nfor the contents of that response; Alternates details alternative\nrepresentations of the response for 2xx, 4xx (except 406), and 5xx\nresponses, and alternative representations of the requested resource\nfor 1xx, 3xx and 406 responses (assuming you are expecting it to be used\nwith redirections as a complex form of the Location header field). \nTherefore, there is no reason for proxies to ignore Vary on 1xx, 3xx and 406\nresponses (nor can they do so if they wish to be compliant with HTTP/1.1).\n\nDoes that solve the problem?\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7809073"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">     HEAD is not a NOOP, OPTIONS is.  What is the rationale for\n>     using HEAD to relay the final count instead of OPTIONS?\n> \n> I guess I've never really understood what OPTIONS is supposed\n> to do, nor does the existing spec language make it clear that\n> this is allowed to carry the various request-headers that\n> are needed to report use-counts broken down by, say, User-agent.\n\nMy inclination is not to disallow anything unless it is known to be\nharmful, and I don't think that this usage of OPTIONS would be harmful.\n\n> In any event, while HEAD is not a \"no-op\", section 9.1.1 says\n> \"the convention has been established that the GET and HEAD\n> methods should never have the significance of taking an action other\n> than retrieval.\"  So, from the point of view of the server's\n> visible state, doing a HEAD is indeed a no-op.  It does imply\n> that the server is going to access the meta-data of the resource\n> (because the \"metainformation contained in the HTTP headers in response\n> to a HEAD request SHOULD be identical to the information sent in\n> response to a GET request\"), which does impose a cost on the server.\n> But no more than if the server had to do a conditional GET.\n\nActually, what made me think of it was the fact that a HEAD request\nis normally considered another \"hit\" on that resource, whereas an\nOPTIONS request should not be (speaking with my wwwstat hat on).\nThat begs the question of whether the proxy or origin would reduce\nthe reported count by one to avoid improperly counting the last HEAD\naction as a \"hit\"? Such a question is difficult to answer, since it\ndepends on how the server collects and analyzes such data.\n\n> Finally, OPTIONS responses include messages bodies, and HEAD\n> responses \"MUST NOT\", so I would expect the use of HEAD to\n> require few bits transferred over the wire.\n\nNope -- successful responses to OPTIONS must not include entity info.\n\n> Actually, I never thought about using OPTIONS until now :-)\n> \n> Maybe we ought to have a true NOOP method?  But I'm not really\n> that worried about such slight changes in the load on a server.\n\nOh, I wasn't thinking about server load, though it might indeed be\nless using OPTIONS.\n\n>     Does it make a difference when going through old proxies?\n>     \n> You mean, because HTTP/1.0 proxies don't understand OPTIONS and would\n> reject such requests?  In theory, yes, although I'm not sure this would\n> be an issue, because a server that wants accurate use-counts would only\n> believe them from a \"cooperative\" cache, which is identified by\n> \"Connection: coop\" in the request, and we said\n> \n>      Note: a server might distrust such a request-header  when\n>      received from an HTTP/1.0 client, which might have incorrectly\n>      forwarded the Connection header.\n\nI was going to mention this later (along with a real review of the draft),\nbut that is not an appropriate use of Connection.  For one thing, assuming\nthat all intermediaries have caches (and thus would assign any meaning to\nbeing cooperative) is wrong.  For another, it doesn't take advantage of\nthe extensibility mechanism inherent in cache-control.  Instead of all\nthe coop negotiation, an origin server should decide whether being\ncooperative is required or optional.  If it is required, then send\n\n    Cache-control: proxy-revalidate, coop\n\nwith coop (or something similar) being defined as a modifier on\nproxy-revalidate such that caches which obey the coop directive\n(whatever that may imply) may ignore the proxy-revalidate.\nIf cooperation is considered optional, then just send\n\n    Cache-control: coop\n\nThe advantage here is that you don't need to mess with Connection and\nthe directives will propagate to all recipients instead of just the\nnearest neighbor on the response chain.\n\nI still don't believe that such count-forwarding is appropriate for a\nproposed standard (experimental is okay), since I don't think that\npeople disable caching just to record hit-counts (which are already\nknown not to be an accurate measure of readers).  Most people disable\ncaching by accident, and those that do it on purpose are normally\nlooking for Referer and IP/hostname (more than just a request count).\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7821062"}, {"subject": "Re: Problems with draft-ietf-http-v11-spec0", "content": "> I have to disagree that Section 2.2 (or the draft as a whole) is clear\n> about character sets.  What is included in parentheses in the quote \n> above may be the intention, but it is not explicit.\n\nThe question of character sets/encoding is one that has been embroiled\nin controversy for every single draft that has gone through the applications\narea in the past three years.  For that reason, Henrik and I chose to\nbe inclusive of character set possibilities wherever it was safe\n(interoperable) to do so, and not add any explicit requirements that\nwould prevent future use of things like UTF-8 for some key areas of the\nprotocol.  This was all done with the understanding that HTTP is an\n8-bit clean protocol (but all protocol features do have 7-bit alternatives).\n\n> I tried to answer for myself the question \"Where, if at all, does\n> HTTP 1.1 allow non-US-ASCII characters (other than within message-\n> body)?\", according to the latest draft.  I ran into several problems\n> with figuring out the answer.\n> \n> First, I would prefer if the answer to that question was clearly\n> and explicitly stated somewhere in the draft.  As it is now, one\n> has to work one's way through several layers of BNF.\n\nYes.  My only defense for this is simply that we didn't want to answer\nthat question when we didn't need to.  That was a political choice, not\na technical one.\n\n> On to the details:  Unencoded non-US-ASCII octets (octets with the\n> most significant bit set, simply called eightbit chars in the following)\n> come in to the BNF in two ways:\n> \n> 2.2\n>        OCTET          = <any 8-bit sequence of data>\n> \n> whence TEXT, comment and ctext, quoted-string and qdstring\n> are all allowed to have eightbit chars.  (but not quoted-pair.)\n> \n> Note that this allows eightbit chars in lots of places, for example\n> Etags or MIME parameters (including boundaries).\n\nYep -- the keyword being \"allows\".\n\n> 3.2 Uniform Resource Identifiers\n> 3.2.1\n>        national       = <any OCTET excluding ALPHA, DIGIT,\n>                         reserved, extra, safe, and unsafe>\n> \n> whence unreserved, uchar and pchar, and therefore nearly all parts\n> of a URI (apart from the scheme) are allowed to have eightbit chars.\n\nYep, as explained below.\n\n> 3.2.1:\n>     \"The BNF above includes national characters not\n> allowed in valid URLs as specified by RFC 1738, since HTTP servers are\n> not restricted in the set of unreserved characters allowed to represent\n> the rel_path part of addresses, and HTTP proxies may receive requests\n> for URIs not defined by RFC 1738.\"\n> \n> I read that as meaning that HTTP application which handle such URIs\n> with eightbit chars can conform to the HTTP/1.1 spec, even if those\n> URIs don't conform to RFC 1738.\n\nThat is correct.  RFC 1738 did not define the syntax for URNs and there\nis no reason for HTTP to restrict URI's to the URL syntax exclusively.\n\n> Although the quoted sentence doesn not explicitly speak of generating\n> such URIs, there is nothing forbidding it.  (And it seems logical\n> that a server accepting requests for such URI's should also be allowed\n> to generate Location: headers etc. containing them.)\n\nThe spec doesn't say that.  Wherever possible, it leaves the issue of\nnaming resources in the hands of the origin server.\n\n> On the other hand, in\n> 4.2 Message Headers\n> \n>        message-header = field-name \":\" [ field-value ] CRLF\n> \n>        field-name     = token\n>        field-value    = *( field-content | LWS )\n>  \n>        field-content = <the OCTETs making up the field-value\n>                         and consisting of either *TEXT or combinations\n>                         of token, tspecials, and quoted-string>\n> \n> Note that it doesn't say\n> \n>        field-content = <the OCTETs making up the field-value\n>                         and consisting of either *TEXT or combinations\n>                         of token, tspecials, URI, and quoted-string>\n> \n> nor does it say\n> \n>        field-content = <the OCTETs making up the field-value\n>                         and consisting of either *TEXT or combinations\n>                         of token, tspecials, quoted-string etc.>\n> \n> From this I conclude that\n> (a)  an URI in a field-content (which is not within a quoted-string),\n>      since it is not defined as arbitrary *TEXT, has to be understood \n>      as being comprised of component tokens, and\n> (b)  (since eightbit chars are not allowed in tokens) an URI in a\n>      field-content cannot contain unencoded eightbit chars.\n\nNope -- that is an invalid extrapolation.  The generic syntax for\nparsing header fields considers a URI to be *TEXT.  This has no affect\non specific field definitions, because those specific definitions add their\nown requirements to the interpretation of the field-content of a specific\nfield *after* it has been extracted by the message parser.\n\nIn practice, you can put 8-bit text in any of the locations where the\nspec allows it -- current HTTP/1.0 applications work that way.\n\n> But the BNF for specific headers uses rules which seem to allow\n> eightbit chars, for example\n> \n> 14.30\n>        Location       = \"Location\" \":\" absoluteURI\n> \n> I conclude that the draft is far from clear.\n\nWell, look at how the BNF defines messages -- the only fields that are\ndefined in terms of <field-content> are the extension fields (those not\ndefined by the specification itself).\n\nSaying that the draft is \"far from clear\" is not useful.  Do you have\nspecific wording that could be added, and where it should be added, which\nwould help clarify the issue without harming the protocol's extensibility?\n\n> Some other (mostly BNF related) weirdnesses:\n> \n> SInce URI is comprised of tokens (see (a) above), the following\n> seems to apply:\n\nI'll pass on the rest, since the assumption is false.\n\n> Comments (within parentheses) should probably allowed in more\n> places - at least, in 19.4.7\n>        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT\n> should probably be\n>        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT *comment\n\nWhy? A comment in that location serves no useful purpose.\n\n> Also,\n>       Via =  \"Via\" \":\" 1#( received-protocol received-by [ comment ] )\n> in 14.44 should maybe become\n>       Via =  \"Via\" \":\" 1#( received-protocol received-by [ *comment ] )\n\nComments can be nested and we did not wish to encourage multiple\ncomments in a protocol that is normally only machine-read (unlike mail).\n\n> 10.3.6 305 Use Proxy\n> \n> The requested resource MUST be accessed through the proxy given by the\n> Location field. The Location field gives the URL of the proxy. The\n> recipient is expected to repeat the request via the proxy.\n> \n> How exactly does is a proxy \"given\" by a Location field?\n> Location normally contains an URI, and URIs point to resources but\n> not (normally) applications (the proxy).  Does the URI have to be\n> a http_URL, does the abs_path have to be empty (or is it required\n> to be \"/\"), and what if not?\n\nOn the contrary, proxies are normally identified by URL.  The URL\ndoes not need to be an http URL (though it would be in current practice)\nand the interpretation of the path (if any) would be dependent on\nthe method of proxying (http would not use any path).\n\nPlease keep in mind that the spec does not prevent people from doing\nthings that won't work -- it doesn't have to.\n\n> 3.6 Transfer Codings\n> ...\n>        hex-no-zero    = <HEX excluding \"0\">\n> \n>        chunk-size     = hex-no-zero *HEX\n> ...\n>        chunk-data     = chunk-size(OCTET)\n> \n> Why does this rule use HEX and not DIGIT?  Does this mean the\n> chunk-size is hexadecimally encoded?\n\nYes. It should have said that in the text as well, but fails to.\n\n>                           *   *   *\n> \n> A remark regarding 14.1 Accept:\n> It's a pity there is a \"q=\", but not a \"mxb=\".  An oversight or\n> intentional?\n\nThe conneg group decided it \"wasn't needed\" based on the observation\nthat browsers didn't implement it.  Koen is wrong in that the presence\nof Range does absolutely nothing to replace the functionality of mxb.\nThe only problem with mxb is that it adds complexity to the process\nof configuring a browser and there is no convenient way to adjust the\nmaximum based on the purpose of an individual request.  Given the\nlack of enthusiasm about Accept, and the growing complexity of content\nnegotiation in general, there was not enough reason to restore it to\nthe specification once it was removed.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7833932"}, {"subject": "When to send HTTP/1.", "content": "I guess I underestimated how jumpy people already are.  The only thing\nI did was repeat what we all agreed to at the LA IETF, which was that\nApache wouldn't start sending HTTP/1.1 in the HTTP-version field until\nafter the IESG approved the draft.\n\nIf the WG feels that the protocol is now stable enough to justify using\nit by name, then let's do so, but I think that should be a WG decision\nand not an individual decision.  My only concern is that the protocol\nwould change without a corresponding change in the version number.\nIf the WG agrees not to change the protocol (what goes on the wire, not\nthe wording in the spec) without also upping the version number, then\nthere won't be any interoperability problems caused by last-minute\nchanges.\n\nLarry, is this something you can call for consensus?\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7851004"}, {"subject": "201 created on PUT Method  et", "content": "Henrik and I found a problem with the 1.1 put when\ntesting against the linemode browser.\n\nWhen the document exists, I send back 204 with\nentity headers including the content-version.\n\nThis works fine because there is no body.\n\nHowever, when a new entity is created, I send\n201 but cannot send the entity headers on the new\nresource because there can be a separate entity body.\n\nThe result is that the client will have to do an additional\nhead to know the resulting entity version etc etc.\n\nIt seems to me that, in the first case, the \"200 or 204\" in the\nspec should be dropped the 200.\n\nIn the second case, it seem to me that either 201 should not\nreturn a body or a different return code like a 204 should be\nused, thus obviating the need to do an extra operation to collect\nthe information needed to handle versioning.\n\nAdditionally, the content-version and derived-from headers should\nbe moved into the spec body.\n\nWho wants to write file and not know if there is a version conflict? Hunh?\n\nNaturally, the 100 codes should be required in 1.1 so we don't have another\nbotch job on our hands.  Note that I close the connection whenever I reject\na 1.0 client on a put because I know the lossage is a'coming right behind.\nMost of the time, the client will have to authenticate.  (this may be fixed\nby now.)\n\nBTW, we getting some nice bug reports from testing.\n\n\n\n", "id": "lists-010-7858723"}, {"subject": "[johnh&#64;isi.edu: document about PHTTP/TCP interactions", "content": "I think this is interesting data for those implementing HTTP/1.1....\n\n------- Start of forwarded message -------\nX-Url: <http://www.isi.edu/~johnh/>\nTo: Larry Masinter <masinter@parc.xerox.com>\nSubject: document about P-HTTP/TCP interactions\nDate: Tue, 13 Aug 1996 10:31:58 PDT\nFrom: John Heidemann <johnh@isi.edu>\n\n\nI've been looking at some performance interactions between currently\nP-HTTP and TCP implementations and have prepared a brief web page\nsummarizing my findings:\n\n\n    title: Performance Interactions Between P-HTTP and TCP Implementations\n    url: <http://www.isi.edu/lsam/publications/phttp_tcp_interactions/>\n\n    Abstract:\n\n    This document summarizes several performance problems resulting from\n    interactions between the Apache-1.1b4 implementation of\n    persistent-HTTP (P-HTTP) and BSD-based implementations of TCP. Two of\n    these problems tie P-HTTP performance to TCP delayed-acknowledgments,\n    thus adding up to 200ms to each P-HTTP transaction. A third results in\n    multiple slow-starts per TCP connection. We discuss each problem and\n    summarize solutions.  Unresolved, these problems result in P-HTTP\n    transactions which are 10 times slower than standard HTTP and 20 times\n    slower than potential P-HTTP for machines connected by a 10 Mbps\n    Ethernet. After fixing these problems we observe that P-HTTP\n    successfully performs better than HTTP on a local Ethernet.\n\n\nI have forwarded a pointer to this document to the Apache team.\nBecause both the techniques used in Apache and BSD TCP implementations\nare common, I think that this document will be of interest to the\nhttp-wg mailing list and others doing P-HTTP development.\n\nI would be interested in any feedback you or the working group would\nhave about this work.\n\nThanks,\n   -John Heidemann\n    USC/ISI\n\n------- End of forwarded message -------\n\n\n\n", "id": "lists-010-7867048"}, {"subject": "Re: 201 created on PUT Method et", "content": "> Henrik and I found a problem with the 1.1 put when\n> testing against the linemode browser.\n> \n> When the document exists, I send back 204 with\n> entity headers including the content-version.\n> \n> This works fine because there is no body.\n\nYep.\n\n> However, when a new entity is created, I send\n> 201 but cannot send the entity headers on the new\n> resource because there can be a separate entity body.\n> \n> The result is that the client will have to do an additional\n> head to know the resulting entity version etc etc.\n> \n> It seems to me that, in the first case, the \"200 or 204\" in the\n> spec should be dropped the 200.\n\nNope -- the 200 is necessary for those cases where the server\nwants the client reload the resource before making more changes.\n\n> In the second case, it seem to me that either 201 should not\n> return a body or a different return code like a 204 should be\n> used, thus obviating the need to do an extra operation to collect\n> the information needed to handle versioning.\n\n201 provides the opportunity for the server to say: \"Okay, I did that,\nhere are pointers to what was done and perhaps something else you\nneed to do.\"  I can see why you might want 204 to be allowed as well,\nand I can't remember why only 201 is required in the spec.\n\n> Additionally, the content-version and derived-from headers should\n> be moved into the spec body.\n\nToo late -- that was postponed to HTTP/1.2.  I would have liked them\nin HTTP/1.1 as well.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7876262"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "On Sat, 17 Aug 1996, Roy T. Fielding wrote:\n\n> \n> I still don't believe that such count-forwarding is appropriate for a\n> proposed standard (experimental is okay), since I don't think that\n> people disable caching just to record hit-counts (which are already\n> known not to be an accurate measure of readers).  Most people disable\n> caching by accident, and those that do it on purpose are normally\n> looking for Referer and IP/hostname (more than just a request count).\n\nAs someone who has disabled caching on occasion - I couldn't care less\nabout exact hit counts, referer or IP/hostname for the most part. I just\nwant cache coherency - something many browsers are absolutely horrible \nat.\n\nCaching:\n           o Never check\n           x Check once per session\n           o Check everytime\n\nis my enemy. It means shopping basket applications and other applications\nwhere what the user sees *MUST* match the state kept on the server blow up\nbecause of some browsers poor caching behavior. At least one browser will\neven cache GET _and_ POST results with pre-expired dates, No-Cache\ndirectives and *different* parameter data. My *ONLY* reliable option is\ncache busting via random URL components since it can insure that a browser\n*never* sees the same URL twice. It is NOT acceptable that '1%' of\nbrowsers _could_ show a user that items a,b,c, and e are in their basket\nwhen in reality items a,b,c and d are in it. There is legal liability\ninvolved for site operators if they ship the wrong things. \n\nIOW: I think this discussion of hit counts is off base. It addresses a\nproblem that is minor (attempting to get *exact* hit counts - something\nthat is _also_ made a hash of by people setting their browsers to\n'non-standard compliant' caching to speed up their browse sessions - it\nsimply doesn't matter if the *proxy* could report hits when the browser\nnever makes the request in the second place) while the BIG problem of my\nbeing unable to insure that the user sees the *correct* information\nwithout cache busting URLs is still untouched. \n\n-- \nBenjamin Franz\n\n\n\n", "id": "lists-010-7885537"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "> IOW: I think this discussion of hit counts is off base. It addresses a\n> problem that is minor (attempting to get *exact* hit counts - something\n> that is _also_ made a hash of by people setting their browsers to\n> 'non-standard compliant' caching to speed up their browse sessions - it\n> simply doesn't matter if the *proxy* could report hits when the browser\n> never makes the request in the second place) while the BIG problem of my\n> being unable to insure that the user sees the *correct* information\n> without cache busting URLs is still untouched. \n\nThe \"must-revalidate\" cache-response-directive was created to fix that\nproblem and we have yet to see whether or not it will be sufficient.\nThe protocol is not capable of fixing the problem any better than that,\nsince the actual cause of your woes is simply that using state-marked URLs\nor cookies for storing and exchanging market-basket state is a poor\ndesign for a distributed client/server system.  If you want to do better,\nyou will need to convince the browser makers to define a standard for\nidentifying purchasable items in HTML and a client-side \"basket window\"\nfor manipulating items to be purchased via a POST request to one or more\n\"cashier\" resources (also identified in the HTML) which can verify the\nentire purchase (including cost/item and availability) before authorizing\nthe sale.  No changes to HTTP are necessary, and it is completely resistant \nto caching; in fact, it is cache-friendly.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-7896090"}, {"subject": "Re: Call for compatibility tester", "content": "   [Roy:]\n>>>There is a difference between being compatible with older versions\n>>>of HTTP and being compatible with old browsers that do not implement\n>>>any version of HTTP correctly.\n>> \n   [Koen:]\n>> HTTP/1.0 is an informational standard.  As far as I know, this means\n>> that the IETF does not have an opinion on whether lynx implements\n>> HTTP/1.0 correctly.\n\n[Roy:]\n>I need to correct this misunderstanding. As far as the IETF (and W3C)\n>is concerned, RFC 1945 is the only definition of HTTP/1.0 -- there simply\n>is no other reference for that protocol.\n\nYou misunderstand.  I was not claiming that there were other\ndefinitions of 1.0.  I was claiming that the IETF *has no opinion* on\nhow necessary it is to comply to every detail in the 1.0 definition.\nQuoting from rfc1602:\n\n           2.4.3  Informational\n\n              An \"Informational\" specification is published for the\n              general information of the Internet community, and does\n              not represent an Internet community consensus or\n              recommendation.\n\nIf you want to discuss how valid lynx is as an implementation of 1.0,\nthis WG is not the place to do it.\n\nThe conneg draft claims to be downwards compatible with 1.0, and as\nfar as I am concerned we need *running code* to prove this point, not\nany discussion which leads to WG rough consensus about well-known\n1.0 browsers being invalid 1.0 implementations.\n\nI am not very interested in keeping mechanisms which cause non-running\ncode with well-known 1.0 browsers, even if you can work around it\nusing Vary: user-agent.  I was too optimistic about the degree in\nwhich well-known 1.0 browsers implement features in the 1.0\ninformational spec, and I hope to correct for this in the next draft.\nThis is my current proposal for correcting it:\n\n We just forget about sending cached list responses to non-negotiating\n clients altogether.  This allows us to keep using the 300 code as\n originally intended.  The List_OS result of the network negotiation\n algorithm disappears, and so does the `forward' directive in the\n alternates header, because `forward' is now the only action possible.\n We allow origin servers to generate whatever response they want\n (including a 200 response with a list) when contacted by a\n non-negotiating user agent.\n\nWhat do you think?\n\nBy the way, I did some experiments with the Arena browser, made by the\nw3c, and the sun4 binary I tried does *nothing* when getting a 4xx\nclass response, it just keeps the current page on screen.\n\n[...]\n>adding hacks to the protocol means that correctly implemented applications\n>will suffer forever.\n\nMy proposal above (and also my earlier `use 416 instead of 300'\nproposal) do not add hacks to the protocol.  I'm just as much against\nadding hacks as you are.  By proposal above *cuts* a feature from the\ndraft, because it is now known that this feature is a premature\noptimization.\n\n[...]\n>> The current draft does not allow you to use Vary to optimize here:\n>> proxies ignore the Vary header in cached list responses when using the\n>> network negotiation algorithm.  The spec could be extended with more\n>> vary rules, but I don't think this is the way to go.\n>\n>I'll have to check your spec, but you need to be careful in considering\n>the overlapping purposes of Vary and Alternates --\n\nI was a careful as I could be.  You will have to check my work to see\nif I got everything right.\n\n> Alternates does not\n>act as a replacement for Vary.\n\nI'm painfully aware of that.  If Alternates: <something> would have\nimplied Vary: {accept-headers} for all 1.1 clients, as in my original\ndesign, the conneg draft would have been about 5 pages shorter.\n\n[...]\n>Alternates details alternative\n>representations of the response for 2xx, 4xx (except 406), and 5xx\n>responses, and alternative representations of the requested resource\n>for 1xx, 3xx and 406 responses\n\nNo it does not, at least not in my draft.  I have no intention of\nmaking my head explode by trying to define transparent content\nnegotiation for error responses.  You don't have conditional GETs and\nrange retrieval on error responses, and you won't have transparent\nnegotiation on them for the same reason: these mechanisms can change\nthe status code of the response, so they are always applied *before*\nthe status code is chosen.\n\n[...]\n>Therefore, there is no reason for proxies to ignore Vary on 1xx, 3xx and 406\n>responses (nor can they do so if they wish to be compliant with\n>HTTP/1.1).\n\nProxies *have to* ignore Vary if transparent content negotiation is to\nbe more efficient than plain vary-based negotiation.  And proxies can\ndo so while being 1.1 compliant.  From the 1.1 spec:\n\n This specification does not define any mechanism for transparent\n negotiation, though it also does not prevent any such mechanism from\n being developed as an extension and used within HTTP/1.1.\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^ \n\nThe rules on not ignoring Vary in the 1.1 spec apply to 1.1 proxies\nacting as a 1.1 caching proxy, not to 1.1 proxies which do transparent\nnegotiation on behalf of the origin server.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-7905751"}, {"subject": "Re: When to make objects uncacheable ", "content": "I just joined the ircache mail list, and thought I might have some\ninput.  I have been analyzing proxy logs to size a replacement proxy\ncache.  Background - Fidelity Investments firewall proxy cache,\nhandling 500,000 access (5 GB) per day, 4000 active users.\n\nThe CGI URLS at my proxy make up 11% of all accesses.  In terms of\nunique URLs, CGI URLs make up 15%.  Of these 56% are accessed more\nthan once within a week.   And the average number of times these URLs\nare accessed is 1.9.  This means that if I were to cache CGI URLs, I\nmight be able to get a 48% hit rate.\n\nWhat is more interesting is that only 16% of the CGI URLs are accessed\nmultiple times and might benefit from caching.  But these URLs are\neach accessed an average of 12 times within a week.  Scanning the raw\ndata, I see that the largest number of these is for stock quote\nlookups, where data more than 10 minutes old might be useless.  And\nI responding to a question related to search engines - :(\n\nHere's more data if anyone's interested - the raw numbers from a\nweek's access logs:\n\nTotal accesses (including client cache hits and failures)  3,586,096\nTotal successful transfers                                 2,502,142\nNumber of unique URLs                                        949,613\nNumber of unique URLs repeated                               234,906\nTotal successful CGI transfers                               274,809\nNumber of unique CGI URLs                                    143,543\nNumber of unique CGI URLs repeated                            23,033\n\nTotal bytes of data transferred                          25380996328\nTotal bytes of data transferred once only                10225694349\nTotal bytes of unique data transferred                   13097062787\nTotal bytes from repeated URLs                           12283933541\nTotal bytes from repeated CGI URLs                         988160134\n\nAverage transfer size                                          10143\nAverage CGI transfer                                            6404\n\n\nMartin Hamilton wrote\n>Andrew Daviel writes:\n>| Supposing the above premises are reasonable, my question is whether\n>| the output of more general search engines should be made cacheable?\n>| It seems to me that certain queries are fairly popular, and that\n>| some benefit might be had from cacheing the responses. On the other\n>| hand, the number of possible URLs expands exponentially with the\n>| length of the query string, so that cacheing every response would be\n>| unreasonable, filling up caches with never-to-be-repeated requests.\n>\nFrom a quick scan through the logs, I don't see our proxies repeatedly\n>servicing the same queries of the same search engines.  Does anyone\n>else ?  Our users seem to be coming up with quite sophisticated\n>queries, presumably because of the difficulty of actually finding\n>anything on the Web...\n\nChris Hull\n(The opinions expressed are my own)\n\n\n\n", "id": "lists-010-7918827"}, {"subject": "Re: Problems with draft-ietf-http-v11-spec0", "content": "On Sat, 17 Aug 1996, Roy T. Fielding wrote:\n\n[me, kw:] \n> > I have to disagree that Section 2.2 (or the draft as a whole) is clear\n> > about character sets.  What is included in parentheses in the quote \n> > above may be the intention, but it is not explicit.\n> \n[politics and the history of charset specificity avoidance...]\n\n[... lots snipped ]\n> > I conclude that the draft is far from clear.\n> \n> Well, look at how the BNF defines messages -- the only fields that are\n> defined in terms of <field-content> are the extension fields (those not\n> defined by the specification itself).\n\nTrue, if you strictly follow the BNF only.\n\nBut you hardly want to say that the whole of 14.2, including the\n#(values) rule whose textual description refers directly to the\nBNF-defined message-header, applies to extension headers only.\n\n> Saying that the draft is \"far from clear\" is not useful.  Do you have\n> specific wording that could be added, and where it should be added, which\n> would help clarify the issue without harming the protocol's extensibility?\n\nI'll try...\n\nReplace\n       field-content  = <the OCTETs making up the field-value\n                        and consisting of either *TEXT or combinations\n                        of token, tspecials, and quoted-string>\nwith\n       field-content  = <the OCTETs making up the field-value,\n                        either as defined by more specific rules\n                        (see 14, 19.6.2) or *TEXT for unrecognized\n                        message headers>\nReason:\nThe former is unnecessary specific; in fact it seems exactly equivalent\nto\n       field-content  = *(*TEXT | *(token | tspecial | quoted-string) )\nwhich is apparently not the intention.\n\nAlso add after the first paragraph of 3.2, before 3.2.1:\n\n  Whitespace is not allowed anywhere within an URI.  In other words,\n  the last paragraph of 2.1 (implied *LWS) does not apply to the BNF\n  rules in 3.2.1 and 3.2.2.\n\nIn 3.11 Entity Tags, add a sentence somewhere 'Whitspace is not\nallowed between the \"W/\" prefix and the opaque-tag' (if you think\nit should apply).\n\nIn 9.2 GET, make the last sentence (\"If the OPTIONS request passes\nthrough a proxy,...\") a separate paragraph, to make clear that it\napplies to both preceding pragraphs (which handle \"*\" and not-\"*\",\nrespectively).\n\n11 Access Authentication, 4th paragraph:\nA user agent that wishes to authenticate itself with a server--usually,\nbut not necessarily, after receiving a 401 or 411 response--MAY do so by\n                                              ^^^\nThe 411 seems to be in error here.\n\n> > Some other (mostly BNF related) weirdnesses:\n> > \n> > SInce URI is comprised of tokens (see (a) above), the following\n> > seems to apply:\n> \n> I'll pass on the rest, since the assumption is false.\n> \n> > Comments (within parentheses) should probably allowed in more\n> > places - at least, in 19.4.7\n> >        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT\n> > should probably be\n> >        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT *comment\n> \n> Why? A comment in that location serves no useful purpose.\n\nBecause I have seen headers like that in mail and news messages.\nApparently several MUAs/mime converters/emacs packages are creating \nthem.  If this is included in the HTTP spec at all -- since it has no\nsignificance to the HTTP protocol, why restrict the syntax unnecessarily.\nThere's even an explicit note in RFC1521 that the following are \nequivalent:\n                    MIME-Version: 1.0\n                    MIME-Version: 1.0 (Generated by GBD-killer 3.7)\n\nAnother place where trailing comments could be allowed would be the \nHTTP From header.  But I don't know how common that is in HTTP/1.0.\n \n\n> > 10.3.6 305 Use Proxy\n> > \n> > The requested resource MUST be accessed through the proxy given by the\n> > Location field. The Location field gives the URL of the proxy. The\n> > recipient is expected to repeat the request via the proxy.\n> > \n> > How exactly does is a proxy \"given\" by a Location field?\n> > Location normally contains an URI, and URIs point to resources but\n> > not (normally) applications (the proxy).  Does the URI have to be\n> > a http_URL, does the abs_path have to be empty (or is it required\n> > to be \"/\"), and what if not?\n> \n> On the contrary, proxies are normally identified by URL.  The URL\n> does not need to be an http URL (though it would be in current practice)\n> and the interpretation of the path (if any) would be dependent on\n> the method of proxying (http would not use any path).\n\nI don't understand what you mean with \"normally\".  I could not find\nany indication in the draft that proxies are identified by an URL.  If\nI have e.g. CERN httpd 3.0 running on my.dom.ain:80 with proxying\nenabled, then \"http://my.dom.ain/\" (with or without the trailing \"/\")\nwould refer to that server's home page, not to \"an intermediary\nprogram which acts as both a server and a client\".\n\nI understand that you want to keep the spec open for future extensions,\nbut since this draft defines (a version of) HTTP, it should at least\ndefine the use of this header with http_URLs.  Otherwise this response \ncode will be useless (different implementations use it differently,\nor nobody uses it), and it could just as well be defined as\n\"This code is reserved for future use\" (like 402).\n\nI suggest something like\n   The requested resource MUST be accessed through the proxy given by the\n   Location field.  If the Location field consists of a http_URL, it\n   MUST NOT contain an abs_path, and the recipient is expected to repeat \n   the request via the proxy given by the host and, if present, port \n   in the http_URL.\n\n   If the Location field contains an URL which is not an http_URL,\n   it indicates the proxy in a way not defined by this protocol.\n \n   If for some reason the recipient cannot repeat the request via the\n   proxy given in the response, this MUST be treated as an error. \n\nHowever, on closer inspection, this doesn't make much sense either...\nor is not nearly enough.  Some questions that are completely open:\nIs a proxy allowed to act on this, or does it have to forward the\nresponse, or are both allowed (dependent on configuration of the proxy)?  \nCan the request, when repeated, be forwarded through another proxy,\nor is direct connectivity required?\n\nAlso consider that this response is incompatible with clients which\ndon't understand it, since the location header here has a different\nmeaning from other cases.  Not even a compliant HTTP/1.1 client is\nrequired to understand that 305 is basically different from other\n3xx codes:  6.1.1: \"HTTP applications are not required to\nunderstand the meaning of all registered status codes, though such\nunderstanding is obviously desirable.\"[1]  If such a client treats\na 305 response as equivalent to 300 (as recommended in 6.1.1), \nit may redirect the request to the proxy machine's home page -\nnot at all what was intended.\n\nIt would be cleaner not to overload \"Location\" in this way and\ndefine a new \"Proxy-Location\" header instead.\n\n[1] I don't understand why the spec is so lenient here.  Shouldn't\nall HTTP/1.1 implementations be REQUIRED to recognize all the \nresponse codes in this spec?  There are several \"client MUST\"s\nin other parts for specific response codes, and it's not clear \nwhether \"not understanding\" a response code lets a client get away\nwith not following its MUSTs.  This includes 305.\n\n> Please keep in mind that the spec does not prevent people from doing\n> things that won't work -- it doesn't have to.\n\nOf course it's not possible to absolutely prevent people from such\na thing.  But it should be possible to say \"If you want to do X,\nthen better do it *this* way, because then it will also work with\nother peope's products who follow the same spec\".\n\n> >                           *   *   *\n> > \n> > A remark regarding 14.1 Accept:\n> > It's a pity there is a \"q=\", but not a \"mxb=\".  An oversight or\n> > intentional?\n> \n> The conneg group decided it \"wasn't needed\" based on the observation\n> that browsers didn't implement it.\n\nThere is a way in the latest lynx code to specify it via the .mailcap\nfile, and I understand that the Apache server can make use of it.\n\n> Koen is wrong in that the presence\n> of Range does absolutely nothing to replace the functionality of mxb.\n> The only problem with mxb is that it adds complexity to the process\n> of configuring a browser and there is no convenient way to adjust the\n> maximum based on the purpose of an individual request.\n\nI would like to be able to use it as a general maximum for all requests,\nas in \"Do not *ever* send my something bigger than...\".\n\n> Given the\n> lack of enthusiasm about Accept, and the growing complexity of content\n> negotiation in general, there was not enough reason to restore it to\n> the specification once it was removed.\n\nOkay, I guess it's not forbidden to use it...\n\n  Klaus\n\n\n\n", "id": "lists-010-7929124"}, {"subject": "Lynx as a HTTP/1.0 application (was Re: Call for compatibility testers", "content": "On Sat, 17 Aug 1996, Roy T. Fielding wrote:\n\n> >>There is a difference between being compatible with older versions\n> >>of HTTP and being compatible with old browsers that do not implement\n> >>any version of HTTP correctly.\n> > \n[Koen Holtman <koen@win.tue.nl>:]\n> > You obviously have strong opinions about lynx not being a HTTP/1.0\n> > application, but these are not universally held.  If the conneg draft\n> > would continue to use the 300 code (without providing an adequate\n> > escape hatch) now that the lynx interoperability problems are known,\n> > this would effectively encode your strong opinions in the spec.  I\n> > don't think that would be appropriate.\n> \n> I don't see how any application which fails to interpret a response\n> simply because it doesn't understand a valid response code (a field which\n> is and has always been extensible, and thus always intended to result in\n> new codes being unrecognized by older applications) can be considered a\n> valid implementation of HTTP/1.0.  It is a bug which needs to be fixed.\n> HTTP cannot be concerned about such bugwards-compatibility simply because\n> it is impossible to extend HTTP at all if we don't assume some minimal\n> level of functionality of the part of the applications.  Implementations\n> do need to be concerned with such bugwards-compatibility, but they do\n> so by means already established by the protocol (User-Agent and Server\n> identification and avoidance), not by changing the protocol every time\n> somebody implements it wrong.\n\nThis has very recently been fixed in the latest Lynx code.\nIt would be helpful if readers of this lists who think a browser\nis not \"a valid implemetation of HTTP/1.0\" would share that opinion\nwith the developers in the form of bug reports - in the case of Lynx,\nsend them to <URL:mailto:lynx-dev@sig.net>.  If it's not known, it can't\nbe fixed.\n \n> > [...]\n> >>  The only correct way to deal with such browsers is to\n> >>exclude them on the basis of their User Agent field, \n> > \n> > Using this compatibility hack would be very expensive; you would have\n> > to disallow proxies from ever sending a cached list (300) response to\n> > a non-negotiating user agent, which means that you will have to handle\n> > all these requests yourself.  \n\nNo server will have to special-case Lynx for this purpose, at least\nif the version in the User-Agent is Lynx/2.6 or greater...\n\n  Klaus\n\n\n\n", "id": "lists-010-7946503"}, {"subject": "Re: 201 created on PUT Method et", "content": "   Who wants to write file and not know if there is a version conflict?\n\nOne way to solve this in HTTP/1.1 is to do:\n\nPUT /foo HTTP/1.1\nIf-Match: \"1ad18937shjd\"\n\nif the previously known Etag: for the resource was \"1ad18937shjd\",\nor (if no Etag is available, but a Last-modified time is available)\n\nPUT /foo HTTP/1.1\nIf-Unmodified-Since: Sat, 29 Oct 1994 19:43:31 GMT\n\nThis latter approach is a little risky, since if the resource\nis modified more than once during a given second, a conflict\ncould arise.\n\n-Jeff\n\n\n\n", "id": "lists-010-7957603"}, {"subject": "(revised) HTTP working group statu", "content": "Here is my current understanding of the status of the various WG\nitems. However, there was a lot of mail last week, I was off-net, and\nI went through it quickly.  PLEASE send me any corrections. I'll send\nout a revised status note soon.\n\nPrevious work:\n\n- HTTP/1.1:  draft-ietf-http-v11-spec-07.*\n   IESG last call expired July 5. Jim issued a new draft with some\n   fixes based on last call comments. Some additional comments have\n   come in. (max-age, charset) Whether they'll require WG action is\n   still to be determined.\n\n- digest:   draft-ietf-http-digest-aa-04.txt\n   IESG last call expired July 22.  No comments; we're waiting for\n   IESG action. (Status unchanged).\n   \n- cookies:  draft-ietf-http-state-mgmt-03.*\n   In IESG last call, expires August 20. One IESG comment, which\n   has been responded to.\n\nNew items:\n\n- hit-metering\n    http://ftp.digital.com/~mogul/draft-ietf-http-hit-metering-00.txt\n    Active discussion over:\n     - should this be in HTTP/1.2 at all\n     - is it better/simpler than draft-hallam-http-*\n     - details about the form (HEAD vs OPTIONS, Vary: * vs max-age,\n        etc.)\n   Expect an internet draft \"soon\". (\"draft-mogul-http-hit-metering\"\n   probably).\n\n- Revised PEP draft\n  Rohit promises an I-D \"first thing Monday morning\". Monday morning\n  has come and gone, we're still waiting.\n\n- content negotiation: draft-holtman-http-negotiation-02.txt\n    Andy Mutz will co-edit with Koen Holtman.\n    Active discussion in the working group on details.\n    (I also have private mail suggesting that W3C may also\n    contribute, but no details or timetable.)\n\n- User Agent characteristics: draft-mutz-http-attributes-01.txt\n  Aug 1 draft ties in with content negotiation proposal; needs\n  non-numeric attributes.\n  \n- sticky headers: \n  Active discussion. Need Internet Draft.\n  Most people want \"real data\" before considering for standards track.\n  Some call to separate out 'sticky headers' from 'header compression'.\n\n- referrals proposal\n  draft was expected but didn't arrive.\n\n- User agent profiles\n  This was discussed on the list, but hasn't resulted in any\n  explicit proposal; it's relationship to content negotiation\n  is unclear.\n\nRelated items:\n\n - SHTTP: draft-ietf-wts-shttp-03.txt\n   In IESG Last Call, new version was issued August 1.\n   No visible discussion in either WG.\n\nCharter:\n\nWe actually didn't submit a revised charter; our tentative charter in\nthe minutes said:\n================================================================\n>  July 2: (Gettys) Revised HTTP/1.1 draft\n>  July 30: (Mogul, Leach): draft on 'hit count' additions\n>  Aug 1:   (Nielsen) revised PEP draft\n>  Aug 1:   (Mutz)    revised User Agent attributes draft\n>  Aug 1:   (Leach)   draft on sticky headers, short names for headers, and\n>      context identifiers\n>  undated:  revised content negotiation draft\n>  undated:  HTTP Implementation Guidelines draft\n> Dec 96: all remaining documents to Last Call\n> Target is to close working group by next IETF with work completed.  We\n> may not need meeting at the December IETF. Subsequent work (e.g., on\n> HTTP-NG) may happen by creating a new working group with a new mailing\n> list, etc.\n\nPersonal predictions:\n  I predict that the result of discussion will be:\n\n* Sticky Headers & Hit Counts will go to \"Experimental RFC\" status.\n  Too many people doubt the utility of these; some experimentation\n  and results would be useful, and releasing the drafts ASAP as\n  Experimental would encourage experimentation. Early results one\n  way or another could change this.\n\n* User Agent Attributes & Content Negotation will progress\n  together. With Andy Mutz as co-editor of both drafts, and\n  some focus in the group, we'll be able to make progress on this\n  and get a Proposed Standard out.\n\n* Progress on PEP depends on better responsiveness by PEP authors.\n  (We've yet to even see a draft we can evaluate.)\n\nWe might be able to handle the revision from \"Draft\" to \"Proposed\"\nwith only PEP and content/feature negotiation within the working group\nbefore shutting down.\n\nLarry\n\n\n\n", "id": "lists-010-7965024"}, {"subject": "Re: When to make objects uncacheable ", "content": "    When should one make objects uncacheable?\n\nAre you asking this question from the point of view of the origin\nserver, or of a proxy?\n\nIf your question is \"when should a proxy decide, on its own\ninitiative, that something is cachable?\", then the only safe\nanswer is \"never.\"\n\nBut you seem to be asking mostly about what an origin server\nshould do, e.g.:\n    \n    I would think that objects which change slowly over time should be\n    given Expires values commensurate with the rate of change, for\n    example a Webcam watching clouds go by might be given a lifetime of\n    10 minutes.  This would allow proxy caches to usefully save a\n    reasonably up-to-date image for popular views.\n\nThat's basically reasonable, but perhaps not quite definitive.\nOne way to look at this is that the origin server should set the\nlifetime so that the expected value of getting an incorrect response\nfrom a cache (that is, the probability multiplied by some sort of\n\"cost\" of a wrong answer) is lower than the expected \"cost\" of\nextra caches misses.\n\nThese costs aren't necessarily in the same units (milliseconds vs.\nlawsuits) and so it's not always easy to judge.  But \"commensurate with\nthe [expected] rate of change\" works nicely for some things (such as\nmost webcam pictures) and not at all for others (such as, for example,\na security-camera picture).\n    \n    A fish database with 1,000\n    entries might produce cacheable output when queried such as \n    \"/cgi-bin/query?salmon\" or \"/cgi-bin/query?trout\", but not\n    for \"/cgi-bin/query?anteater\". This would apply to any kind of\n    system creating HTML on-the-fly from (invariant) source.\n\nI'm not sure why the results would be any less cachable for the\nanteater query, if the HTTP response carries a \"200 (OK)\" status.\nThe critical question here is \"would I give a different response\nlater on?\", and I wouldn't expect it to change.\n    \n    Supposing the above premises are reasonable, my question is whether\n    the output of more general search engines should be made cacheable?\n    It seems to me that certain queries are fairly popular, and that\n    some benefit might be had from cacheing the responses. On the other\n    hand, the number of possible URLs expands exponentially with the\n    length of the query string, so that cacheing every response would be\n    unreasonable, filling up caches with never-to-be-repeated requests.\n\nI did a simple study a few months ago with about 1 day's worth of\nAltavista queries.  (This was back when we were doing something\nlike 4 million hits per day; we're now doing about 4 times as many,\nand so the statistics may have changed.)  I found that even if you\ncould cache the result of all of the queries for an entire 24-hour\nperiod, the best-case cache hit rate would be around 15%.  Not really\nworth the effort, I think.\n\nNote that the operators of AltaVista know how often they update the\ndatabase.  They could easily send a max-age value in their responses\nthat would allow caching without harming transparency (provided that\nproxies didn't view this as a license to choose their own expiration\ntimes!).\n    \nI don't think it makes sense to decide whether something is cachable\nbased on the number of hits; resources are cachable or not based on\ntheir essential nature, not how popular they are.\n\nIt's up to the cache (proxy or otherwise) to decide whether it wants\nto store a cachable response in its finite memory, and when to remove\nit.  But this is a separate decision.\n    \n    Currently, Apache 1.1.1 will cache anything with a Last-Modified\n    header, while Squid 1.0.x will not (as shipped) cache anything with\n    a query term.\n\nMy intuition is that Apache's policy will lead to misbehavior in\nsome cases, whether or not there is a query term.\n\n-Jeff\n\n\n\n", "id": "lists-010-7976216"}, {"subject": "HTTP 1.1: Horse out of the bar", "content": "http://www.macweek.com/mw_1032/gw_telefinder.html\n\n\n\n", "id": "lists-010-7987556"}, {"subject": "Re: HTTP 1.1: Horse out of the bar", "content": "The following is off topic for this group, but I had to followup.\n\nAt 11:00 AM 8/19/96 -0400, John C. Mallery wrote:\n>http://www.macweek.com/mw_1032/gw_telefinder.html\n\nIt's bad enough they claim HTTP/1.1 conformance before the spec is\nfinalized. The worst part is, it's a really crappy product.\n\n- It's support for persistent connections consists of sending \"Connection:\n  close\".\n- It spits out TWO response lines/codes on a HEAD (neither of which is\n  appropriate for the error entity body they send along with it).\n- For ANY method, you don't even have the opportunity to input HTTP headers\n  before it spits out a response.  How the heck it can support the Host header\n  is beyond me.  Maybe they expect all the headers to be in the first packet\n  (lame lame), or maybe they treat CRLF as CRCR (lame lame), which they can't\n  legally do anyway.\n\n----BEGIN\nrafiki SunOS5.5 users/ddubois 1150>telnet spiderisland.com 80\nTrying 199.35.3.99...\nConnected to spiderisland.com.\nEscape character is '^]'.\nHEAD / HTTP/1.0\nHTTP/1.0 501 Not Implemented\nHTTP/1.0 200 OK\nMIME-Version: 1.0\nServer: TeleFinder/5.1.0 b15\nDate: Mon, 19 Aug 1996 19:33:55 GMT\nLast-modified: Thu, 11 Jul 1996 21:35:11 GMT\nContent-type: text/html\nContent-Length: 841\n\n\n<HTML><HEAD><TITLE>404 Not Found</TITLE></HEAD>\nConnection closed by foreign host.m/~attention_design/</A><br>br>A><br>\nrafiki SunOS5.5 users/ddubois 1151>telnet spiderisland.com 80\nTrying 199.35.3.99...\nConnected to spiderisland.com.\nEscape character is '^]'.\nGET / HTTP/1.0\nHTTP/1.0 200 OK\nMIME-Version: 1.0\nServer: TeleFinder/5.1.0 b15\nDate: Mon, 19 Aug 1996 19:35:37 GMT\n---END\n\n\nI especially dislike the footer.  Mentioning competing and likely vastly\nsuperior product names in the comments 10 times each at the bottom of your\nweb page is in the poorest possible taste, embedded illigal characters,\ninvalid HTML, misspellings aside.\n\n---BEGIN\n<p>\n<i> July 25, 1996 -- ?Copyright 1996, Spider Island Software</i>\n<p>\nAll products mentioned are trademarks or registered trademarks of their\nrespective owners.<br>\n\n<!--Spider Island, BBS, TeleFinder, POP3, e-mai,l WWW, Web Server, \nOnline, FirstClass, QuickMail, Novalink, Mail Server,\nResNova, SoftArc, Chat, Conference, SMTP, WebSTAR, MacHTTP, Macintosh,\nWindows, Bulletin Board System\nSpider Island, BBS, TeleFinder, POP3, e-mai,l WWW, Web Server, \nOnline, FirstClass, QuickMail, Novalink, Mail Server,\nResNova, SoftArc, Chat, Conference, SMTP, WebSTAR, MacHTTP, Macintosh,\nWindows, Bulletin Board System\nSpider Island, BBS, TeleFinder, POP3, e-mai,l WWW, Web Server, \nOnline, FirstClass, QuickMail, Novalink, Mail Server,\nResNova, SoftArc, Chat, Conference, SMTP, WebSTAR, MacHTTP, Macintosh,\nWindows, Bulletin Board System\nSpider Island, BBS, TeleFinder, POP3, e-mai,l WWW, Web Server, \nOnline, FirstClass, QuickMail, Novalink, Mail Server,\nResNova, SoftArc, Chat, Conference, SMTP, WebSTAR, MacHTTP, Macintosh,\nWindows, Bulletin Board System\nSpider Island, BBS, TeleFinder, POP3, e-mai,l WWW, Web Server, \nOnline, FirstClass, QuickMail, Novalink, Mail Server,\nResNova, SoftArc, Chat, Conference, SMTP, WebSTAR, MacHTTP, Macintosh,\nWindows, Bulletin Board System\nSpider Island, BBS, TeleFinder, POP3, e-mai,l WWW, Web Server, \nOnline, FirstClass, QuickMail, Novalink, Mail Server,\nResNova, SoftArc, Chat, Conference, SMTP, WebSTAR, MacHTTP, Macintosh,\nWindows, Bulletin Board System\nSpider Island, BBS, TeleFinder, POP3, e-mai,l WWW, Web Server, \nOnline, FirstClass, QuickMail, Novalink, Mail Server,\nResNova, SoftArc, Chat, Conference, SMTP, WebSTAR, MacHTTP, Macintosh,\nWindows, Bulletin Board System\n-->\n---END\n\n-----\nDaniel DuBois, Traveling Coderman      http://www.spyglass.com/~ddubois/\n   A polar bear is a rectangular bear after a coordinate transform.\n\n\n\n", "id": "lists-010-7995147"}, {"subject": "draft-ietf-http-pep0", "content": "HTTP Working Group                                           Rohit Khare\nINTERNET-DRAFT                                 World Wide Web Consortium\n<draft-ietf-http-pep-02>           Massachusetts Institute of Technology\nExpires: February 19, 1997                               August 19, 1996\n\n                       HTTP/1.2 EXTENSION PROTOCOL (PEP)\n\nStatus of this memo\n\n   This document is an Internet-Draft. Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups. Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six months\n   and may be updated, replaced, or obsoleted by other documents at any\n   time. It is inappropriate to use Internet-Drafts as reference material\n   or to cite them other than as \"work in progress\".\n\n   To learn the current status of any Internet-Draft, please check the\n   \"1id-abstracts.txt\" listing contained in the Internet-Drafts Shadow\n   Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n   ftp.isi.edu (US West Coast).\n\n   Distribution of this document is unlimited. Please send comments to\n   the HTTP working group at <http-wg@cuckoo.hpl.hp.com>. Discussions of\n   the working group are archived at\n   http://www.ics.uci.edu/pub/ietf/http/.\n\n   This Internet Draft is also available as W3 Consortium Working Draft\n   WD-http-pep-960819. A current version is available at\n   http://www.w3.org/pub/WWW/TR/WD-http-pep.\n\n\nAbstract\n\n   PEP is a system for HTTP clients, servers, and proxies to reliably\n   reason about custom extensions to HTTP. Traditionally, HTTP agents\n   offer extended behavior by private agreement using additional message\n   headers. PEP has features for expressing the scope, strength, and\n   ordering of such extensions, as well as which extensions are\n   available.\n\n   PEP as described here is substantially simpler than previous drafts.\n   The title has also been changed, since PEP addresses a chartered HTTP\n   WG work item.\n\n\n1. Introduction\n\n   HTTP [3] messages, like most applications of RFC 822 [4], can be\n   extended with additional header fields and content formats.\n   Traditionally, the standards update process itself extends the\n   protocol for major, global new features. However, this approach is\n   problematic for deploying casual experiments which could scale up from\n   pairwise to universal deployment, especially as HTTP agents become\n   more modular, dynamic, and diverse. Casual extension of HTTP is\n   hindered by several challlenges:\n\n    1. Naming. To guarantee interoperability, header names, encoding\n       tokens, types, response codes, etc. must be centrally arbitrated\n       by appealing to IETF standards-track documents or the Internet\n       Assigned Numbers Authority (IANA). This requires more effort from\n       implementors than an extension may warrant.\n\n    2. Importance. There is no way to indicate the consequences of\n       ignoring an extension: can an agent silently ignore it, may it\n       warn the user, or must it return an error message?\n\n    3. Scoping. There is no reliable way to indicate who is to process an\n       extension: the next HTTP agent (proxy), or only the origin agent\n       (client or server)? 1.1's Connection: is only a partial solution,\n       since it cannot convey the criticality of stripped headers.\n\n    4. Ordering. There is no way to indicate in what order to process\n       extensions. If several are applied, it can be critical to 'unpack'\n       the message in the correct order.\n\n    5. Initiation. Short of unilaterally utilizing an extension, there is\n       no way to request the other party to initiate it.\n\n    6. Advertising. Though it is possible to enumerate content variants\n       and the headers they differ upon, it is not possible to describe\n       available extensions in detail.\n\n    7. Inquiring. There is no way to inquire if an extension is supported\n       before attempting to use it. This is critical for selecting among\n       related extensions and upgrading to new versions.\n\n   PEP introduces the concept of protocol extensions to systematically\n   address these issues at a level of abstraction above header names and\n   content-types (hence, `Protocol Extension Protocol'). With PEP, HTTP\n   agents can interoperate correctly with known and unknown protocol\n   extensions, select protocol extensions available to both sides, and\n   query partners for specific capabilities. PEP adds to HTTP the\n   extensibility lessons learned from ESMTP (extension naming) [10], IPv6\n   (unknown-option disposition) [5], and Telnet (option negotiation)\n   [12].\n\n   In addition to reliably describing statically extended HTTP servers\n   and clients, PEP will work with dynamically extended agents. Indeed,\n   the authors expect that PEP will drive the deployment of a new\n   generation of exensible agents (such as W3C's Jigsaw server and libWWW\n   reference library).\n\n   NOTE: PEP has been developed over the last year by many interested\n   parties, including the W3C/CommerceNet Joint Electronic Payments\n   Initiative (JEPI). Two strongly related drafts are also available from\n   the IETF and W3C: Don Eastlake's Universal Payment Preamble [6] and\n   ``Selecting Payment Mechanisms Over HTTP (or, seven examples of UPP\n   over PEP)'' [9].\n\n\n  1.1 EXAMPLE: PICS\n\n   Consider the example presented by the first deployed PEP application,\n   the Platform for Internet Content Selection (PICS) [11]. PICS\n   describes a format for labeling almost any content. Since it's\n   intended to be a fairly ubiquitous addition to various Internet\n   protocols, PICS appropriates the PICS-Label: header for 822\n   applications, including mail, news, and web content. The spec could\n   not use, say, Label:, since there's no way to effectively arbitrate\n   between conflicting uses of header names (PICS was not developed under\n   the IETF standards track).\n\n   Even with an eventual dispensation from IANA to use this new header,\n   new issues arise: If a server attaches this new header to label an\n   HTTP response, will a caching server pass it along? Will it act upon\n   the label at all? What kind of feedback will the end-user get when\n   this ``unknown'' header appears? How can the user specify which rating\n   systems to report?\n\n   The direct fix to the first few problems is to add a\n   machine-understandable statement about the extension and its new\n   headers. Protocol: headers describe the name of the extension, the\n   scope, importance, and any associated data headers. Moving further,\n   though, additional information about the protocol can be added to that\n   directive: which PICS services were employed, etc.\n\n   Using PEP in this way also solves the next logical problem: how can a\n   client request a server to initiate vending PICS labels? The same\n   structure for a Protocol: can be reused in a Protocol-Request: asking\n   the counterparty to start using the specified extension.\n\n   In addition, PEP affords additional capabilities, like using\n   Protocol-Query: to test if PICS is available and Protocol-Info: to\n   advertise it. In all cases, the PICS protocol extension can specify\n   additional parameters, such as which rating systems, services, which\n   kinds of labels, and which other URLs are labelled.\n\n\n2. The PEP Model\n\n   Protocol extensions can modify HTTP messages, most commonly by adding\n   headers, but also by modifying the contents, much like a proxy server\n   might do today. There are four points at which PEP processing occurs\n   in a basic HTTP transaction (see Section 3 for usage descriptions at\n   each step):\n\n    1. Sending an HTTP Request\n\n    2. Receiving an HTTP Request\n\n    3. Sending an HTTP Reply\n\n    4. Receiving an HTTP Reply\n\n   During a conversation, either side may invoke a Protocol:, issue a\n   Protocol-Request:, advertise some Protocol-Info:, or test a\n   Protocol-Query:. PEP-compliant recipients decode extended messages and\n   reply to each Protocol-Request: and Protocol-Query:.\n\n   Unlike some previous extension proposals [9], and unlike some existing\n   HTTP/1.1 features (Upgrade: and Connection: tokens), extensions are\n   attributes of a resource rather than a server. This means all PEP\n   statements are applied to the single resource mentioned in any HTTP\n   message (though there is a facility to hint at which other resources\n   the same statement applies to).\n\n   An HTTP agent can begin using a protocol extension (or query for one)\n   because:\n\n     * It is configured to do so by a user or webmaster\n\n     * It has been requested to do so by the other party\n\n     * The other party has hinted that an extension is available\n\n     * Someone else has hinted that an extension is available\n\n   Another aspect of the PEP model is that protocol extensions can have>\n   negotiable parameters. Unlike previous proposals which model\n   extensions as binary have/don't have features, PEP expects protocol\n   extensions will be able to handle different versions of a particular\n   protocol, invocation of ``compatible protocols'', and the selection of\n   compatible modes from several that might be advertised.\n\n   To summarize, PEP as proposed here will allow HTTP/1.2 implementors to\n   make the following kinds of statements:\n\n     * ``This agent has and is using (or does not have and isn't using)\n       the http://w3.org/DES protocol extension with mode parameter CFB\n       and header DES-Info:''\n\n     * ``This agent requires/requests/forbids the recipient to begin\n       using the http://w3.org/DES protocol extension with mode parameter\n       ECB''\n\n     * ``This agent has/requires/refuses the http://pics.org/PICS-1.1\n       protocol extension available for any URIs beginning with\n       http://www.my.site/homepages/''\n\n     * ``Does the recipient have the http://pics.org/PICS protocol\n       extension available for service parameter http://rsac.org/rsaci\n       ?''\n\n     * ``This agent has applied two protocol extensions to this message\n       in this order.''\n\n     * ``This agent is using the `generic' http://w3.org/UPP payment\n       negotiation extension via the payment protocol\n       http://cybercash.com/CCCP.''\n\n\n3. PEP Usage\n\n   Before presenting specifics about each of the four PEP directives and\n   the four points where they are processed, here are a few common\n   points:\n\n     * Attribute-value bags are used to represent all the data carried in\n       PEP headers.\n\n     * Each directive specifies which agent must process it and the\n       consequences of not processing it.\n\n     * Each directive is associated with a set of resources: either the\n       resource represented by the HTTP request or reply, or a list of\n       resource and resource prefixes it applies to.\n\n   A bag is an attribute-value list, where bags are allowed recursively\n   as values as well. In PEP, each directive begins with values\n   describing the protocol (making the extension name the top attribute).\n   Some of those values are common to all directives: the strength (str),\n   scope (scope), for list (for) , extension-specific parameters\n   (params), and other headers associated with this extension (headers).\n   The formal BNF is included in Section 3.5.1.\n\n   Strength and scope are important common concepts. Any directive can be\n   ignored, but strength specifies whether an error should be generated\n   as a result. Similarly, scope defines which agents should handle the\n   directive at all; either the immediate (conn) or final (origin)\n   recipient. The combination of the two concepts makes PEP powerful\n   enough to deploy features that in the past required a minor-version\n   number bump. With PEP, for example, Content-Transfer-Encoding: could\n   be described as {str req} {scope conn} for rapid experimentation with\n   compressed headers, sticky headers, etc.\n\n   A word about the for list: Since each PEP directive applies only to\n   the resource at hand (rather than, say, the entire server or all\n   resources of a given type), a hint as to which other resources the\n   directive applies to will prevent negotiation round-trips each time.\n   Thus, many PEP applications could include {for /*} to apply to an\n   entire server. Remember, there already are examples in HTTP/1.1 of\n   features which only apply to particular resources, such as Range:.\n\n   When fetching a URL, a client will typically search through its\n   database of hints to find all applicable directives before sending an\n   HTTP request. Of course, an agent which does not want to use hints can\n   ignore them: if the for list includes the URI of the current message,\n   just throw away the list; if it does not, just throw away the entire\n   directive.\n\n   Finally, note that the Protocol-Query: / Protocol-Info: pair describes\n   hypothetical capabilities of agents and resources but that the\n   Protocol-Request: / Protocol: pair has immediate consequences for the\n   current HTTP transaction. Each pair hangs together and must be\n   implemented together, though only the latter pair is mandatory.\n\n\n  3.1 PROTOCOL-QUERY\n\n   The first statement in a conversation might be to ask if the\n   counterparty supports an extension at all. The Protocol-Query: header\n   lets one agent ask the other if a particular extension is supported;\n   the response comes back in a subsequent Protocol-Info:.\n\n   A query directive specifies its scope, but not a strength, which\n   defaults to opt; that's because a query always asks optionally for\n   more information, but can neither demand an answer (since query\n   support is optional), nor refuse one.\n\n   An agent can query if an extension is supported for an entire list of\n   URIs. This means that an answer is expected when the replying agent\n   sends a message about any resource on the list; and that an answer\n   should reveal as much as can be said about extension support for that\n   URI set. This way, a server can ask for a client to be prepared to use\n   an extension in a part of his site (e.g. UPP in /Catalog/*); or a\n   client can ask a server if an extension applies to all its documents\n   of the same type (e.g. indexed range retreival for *.pdf).\n\n\n    3.1.1 Sending an HTTP Request\n\n   If a client believes an extension may be supported, it can attach a\n   Protocol-Query: to an outgoing request to test if an extension is\n   supported for that URI (empty for list), an entire server ({for /*})\n   or something in between.\n\n   A client can wait to piggyback its queries on other HTTP transactions\n   or it can explictly issue an OPTIONS request with its queries.\n\n   Note that a protocol extension might be defined to have side-effects\n   when it is queried. For example, the UPP payment selection protocol\n   defines that the answer to any queries for UPP should include\n   additional information about particular payment mechanisms. Thus, in a\n   dynamically extended agent an agent should call back the extension to\n   handle a query.\n\n\n    3.1.2 Receiving an HTTP Request\n\n   When an agent receives a query, it should respond whether the stated\n   configuration is acceptable or refused for any of the URI(s)\n   specified. Upon receipt of a query, an agent should test if the\n   extension is available and, if so, its response to the proposition.\n   The server should be prepared to send the resulting information in its\n   reply.\n\n\n    3.1.3 Sending an HTTP Reply\n\n   In preparing an HTTP reply, a server must respond to queries in the\n   HTTP request, as well as issue its own queries on the reply. For the\n   responses, see Section 3.2 below.\n\n\n    3.1.4 Receiving an HTTP Reply\n\n   The answers returned to the client are in the form of Protocol-Info:;\n   see Section 3.2. The server may have attached queries of its own; the\n   client should prepare an answer and attach it to the next HTTP request\n   it makes for a URI which matches the realm of the query. Here, too, a\n   dynamically extended client should immediately call back an extension,\n   since an extension's response to Protocol-Query: may result in\n   immediate side effects (e.g. a payment selection interface appearing\n   with highlighted possible selections).\n\n\n  3.2 PROTOCOL-INFO\n\n   The second statement in a conversation might be to declare that one\n   party requires/allows/forbids a given protocol extension, either\n   unilaterally or in response to an earlier query. The Protocol-Info:\n   header lets one agent tell the other if a particular extension is\n   supported; there is no required response. Remember, though, that\n   Protocol-Info: itself is the required response to Protocol-Query: (see\n   Section 3.1).\n\n   An informational response states if the protocol is required, allowed,\n   or forbidden for all matching URIs.\n\n\n    3.2.1 Sending an HTTP Request\n\n   A client will add Protocol-Info: directives if required by a previous\n   server query (Section 3.1.4) or if it has any unilateral declarations\n   to advertise.\n\n\n    3.2.2 Receiving an HTTP Request\n\n   A server should take note of any available information; knowledge of a\n   client's capabilities is important for step #3 (Section 3.3.3 and\n   3.4.3).\n\n\n    3.2.3 Sending an HTTP Reply\n\n   A server will add Protocol-Info: directives if required by a previous\n   client query (Section 3.1.1) or if it has any unilateral declarations\n   to make.\n\n\n    3.2.4 Receiving an HTTP Reply\n\n   A server should take note of any available information; knowledge of a\n   client's capabilities is important for step #1 (Section 3.3.1 and\n   3.4.1). The response can be to either ignore this information, request\n   initiation, or directly initiate the extension on the next request.\n\n\n  3.3 PROTOCOL-REQUEST\n\n   The third statement in a conversation might be to directly demand or\n   forbid that the counterparty begin using an extension.\n   Protocol-Request: lets one agent tell the other to initiate a\n   particular extension; the response requires a subsequent Protocol:\n   directive for each request.\n\n\n    3.3.1 Sending an HTTP Request\n\n   When a client wants to tell the server to begin using some extension,\n   it should check if it knows that the extension will work because of\n   stored Protocol-Info:. The Protocol-Request: describes the desired\n   protocol; it may include a range of parameters for the respondent to\n   pick and choose from.\n\n\n    3.3.2 Receiving an HTTP Request\n\n   A server must test if a request will succeed. If so, the server must\n   begin using the specified extension. If it cannot, the preferred\n   mechanism is to echo back the same Protocol-Request: but with {str\n   ref}. In addition, if the original request was {str req}, then\n   indicate that the request failed with a 42x client or 52x server error\n   status code (Section 3.5.2).\n\n\n    3.3.3 Sending an HTTP Reply\n\n   A server must reply to the client's request to require, accept or\n   forbid an extension by either initiating or preventing use of the\n   specified protocol or a compatible equivalent (Section 3.4.3). At this\n   point, a server may also attach its own requests for a client to\n   initiate extensions when accessing this URI or others.\n\n   Note that if a server Protocol-Request: later fails to elicit a\n   Protocol: response from the client, nothing can be assumed about the\n   client's ability to do PEP, since it may have ``forgotten'' the\n   request in the meantime.\n\n\n    3.3.4 Receiving an HTTP Reply\n\n   The client must evaluate each server Protocol-Request: and, if valid,\n   be prepared to use the specified protocol extension when accessing any\n   URI matching the request's for list (Section 3.4.1). If it is not\n   willing or able to use the specified protocol, it must echo back an\n   error on the next access as in Section 3.3.2.\n\n   Note that if a client Protocol-Request: fails to elicit a Protocol: or\n   PEP error response (assuming the response URI is in the request's\n   realm) at, the client must conclude the origin server is not\n   PEP-compliant.\n\n\n  3.4 PROTOCOL\n\n   The final statement in a conversation might be to use an extension\n   directly. Protocol: lets one agent declare that the particular\n   extension is in use for the current message; the response may be to\n   continue using the extension. An agent can apply an extension at any\n   time; the preceeding three conversational steps of testing and\n   requesting are only an example.\n\n   This directive packages together any added header lines and any\n   reprocessing of the message body. Since HTTP/1.1 eliminates many\n   earlier ambiguities about the ordering of headers, we can state\n   categorically that protocol directives must be dispatched in the order\n   they are listed in the Protocol: header. [The Content-Encoding:\n   pipelining mechanism of earlier proposals has been scrapped].\n\n   It is worth noting that the application of a protocol extension can\n   affect the cacheablity of the response. This document does not define\n   a cache control directive, but expects it to be feasible within the\n   1.1 model.\n\n   Connection-scope extensions can remain backward-compatible by listing\n   every associated header and the name of the extension in the\n   Connection: header.\n\n\n    3.4.1 Sending an HTTP Request\n\n   The client must decide which extensions to apply to the current\n   request: it must respond to each applicable Protocol-Request:, it can\n   continue using extensions the server has already used for the\n   request-URI, and it may initiate others. As each extension is applied,\n   added headers are referenced from the Protocol: line in order. If some\n   Protocol-Request: is satisfied by some other protocol extension\n   compatible with the one requested, the client will issue two protocol\n   directives, one for the extension used, and another indicating the\n   extension requested is being used via the other.\n\n\n    3.4.2 Receiving an HTTP Request\n\n   The server evaluates each protocol extension which is in scope\n   according to the order it is listed in the Protocol: header.\n\n   As the server evaluates each protocol, it can interrupt and return\n   HTTP error status codes: if a client encoding error is detected (42x),\n   the server can't or won't decode properly (52x), or if a protocol is\n   used inappropriately (it's refused, not allowed in conjunction with\n   certain other protocols, missing, etc.).\n\n   Proxy servers operate similarly: they can simply remove unknown\n   optional connection-scope directives but must report errors for\n   required directives.\n\n\n    3.4.3 Sending an HTTP Reply\n\n   The server must also choose protocols it will use from the sets of\n   those requested by the client, used by the client, and those it will\n   initiate independently. If it cannot construct a valid reply due to\n   contradictions among those requirements, it must return HTTP error\n   status 520 and enumerate the server's constraints in Protocol-Request:\n   headers of the error message.\n\n\n    3.4.4 Receiving an HTTP Reply\n\n   The client evaluates each protocol extension which is in scope\n   according to the order it is listed in the Protocol: header.\n\n   As the client evaluates each protocol, it could fail for many reasons.\n   One recovery technique is to reissue its request with the offending\n   protocol refused in a Protocol-Request: header. Errors from executing\n   required extensions should be reported to a user or log.\n\n\n  3.5 SYNTAX\n\n   PEP-related syntax is specified here relative to the definitions and\n   rules of the HTTP/1.0, HTTP/1.1, and the relative URL specification.\n\n\n    3.5.1 PEP Headers\n\n   PEP defines two new required general header fields, Protocol: and\n   Protocol-Request:, as well as two new optional general header fields,\n   Protocol-Query: and Protocol-Info:.\n\n       /* Added to General Header rule, Sec 4.3 of HTTP/1.1 */\n       Protocol         = \"Protocol\" \":\" 1#bag\n       Protocol-Request = \"Protocol-Request\" \":\" 1#bag\n       Protocol-Query   = \"Protocol-Query\" \":\" 1#bag\n       Protocol-Info    = \"Protocol-Info\" \":\" 1#bag\n\n       /* Following rules are copied from Section 2.2 of HTTP/1.1 */\n       bag              = \"{\" bagname 1*LWS *bagitem \"}\"\n       bagname          = token | URI\n       bagitem          = bag | token | quoted-string\n       word             = token | quoted-string\n       token            = 1*<any CHAR except CTLs or tspecials>\n       tspecials        = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n                        | \",\" | \";\" | \":\" | \"\\\" | <\">\n                        | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n                        | \"{\" | \"}\" | SP  | HT\n       quoted-string    = ( <\"> *(qdtext) <\"> )\n       qdtext           = <any CHAR except <\"> and CTLs but including LWS>\n\n   Each bag has several top-level reserved attributes defined:\n\n{<protocol-identifier>        {scope (origin | conn)}\n                              {str (opt | req | ref)}\n                              {headers *<token>}\n                              {params ...}\n                              {for *<uri>}\n                              {via <protocol-identifier>}}\n\n   protocol-identifier\n          The well-known protocol extension URI or an IANA-registered\n          token.\n\n   scope\n          scope is either of \"conn\" or \"origin\" ; the default is\n          \"origin\". It defines which agent must recognize this directive:\n          the immediate recipient or the origin client/server.\n\n   str\n          strength is one of \"req\", \"ref\" or \"opt\" ; the default is\n          \"opt\". In particular, a protocol directive only allows req or\n          opt; request and informational directives allow all three; and\n          queries only allow opt.\n\n   headers\n          A list of associated message headers; the default is the empty\n          set. If a header token ends in \"*\", it must be used to refer to\n          all message headers matching that prefix. Each directive\n          enumerates its own associated headers to disambiguate\n          `ownership'. This information is critical when 1) reporting an\n          error, 2) stripping an extension off of a message (if\n          possible), and 3) allowing multiple applications of an\n          extension.\n\n   params\n          A list of parameters configuring the extension, according to\n          the protocol specification; the default is empty.\n\n   for\n          a list of relative or absolute URIs [1,2,7] which a protocol\n          request is binding upon; the default when the for list is\n          empty, is the request-URI or Location: response header. If a\n          reference ends in \"*\", it should be understood as a wild-card\n          character (particular implementations may choose to only\n          recognize restricted applications of \"*\", e.g. `only in the\n          last component of http: URLs').\n\n   via\n          When responding to a request addressed to one protocol\n          extension (a query or request directive), an agent may answer\n          via another (more specific, upgraded, compatible) extension\n          (token or URI). The latter extension is expected to be used\n          elsewhere in the same message. This is a hint and only allowed\n          in the Protocol: and Protocol-Info: directives. The default is\n          empty.\n\n   Note: Multiple occurrences of PEP-defined attribute names in a list\n   (\"{str req}...{str ref}\") will yield undefined behavior.\n\n\n    3.5.2 PEP Status Codes\n\n   PEP defines several new status codes for HTTP replies. Note that the\n   HTTP/1.1 specification states in Section 6.1.1:\n\n     The first digit of the Status-Code defines the class of response.\n     The last two digits do not have any categorization role.\n\n   Informally, PEP distinguishes x2z response codes:\n\n   200 Class\n          220 Uses Protocol Extensions\n\n   400 Class\n          420 Bad Protocol Extension Request\n          421 Protocol Extension Unknown\n          422 Protocol Extension Refused\n          423 Bad Protocol Extension Parameters\n\n   500 Class\n          520 Protocol Extension Error\n          521 Protocol Extension Not Implemented\n          522 Protocol Extension Parameters Not Acceptable\n\n   Each of 400 and 500 class responses may include entity bodies with an\n   explanation of the error, and an indication of whether the problem is\n   temporary or permanent. A 220 response should be used only when some\n   PEP feature is required for correct handling -- it affects the\n   cacheability of the response.\n\n\n4. Conclusions\n\n   This is a discussion draft, not a complete specification. In the\n   meantime, this will be the basis for all further JEPI work.\n\n   For additional details and explorations of various issues (e.g.\n   security concerns), readers are encouraged to study the earlier\n   drafts, available at http://www.w3.org/pub/WWW/TR/WD-http-pep.\n\n\n5. References\n\n   [1]    Berners-Lee, T., \"Universal Resource Identifiers in WWW: A\n          Unifying Syntax for the Expression of Names and Addresses of\n          Objects on the Network as used in the World-Wide Web\", RFC\n          1630, CERN, June 1994.\n\n   [2]    Berners-Lee, T., Masinter, L., and M. McCahill, Editors,\n          \"Uniform Resource Locators (URL)\", RFC 1738, CERN, Xerox\n          Corporation, University of Minnesota, December 1994.\n\n\n   [3]    Jim Gettys, et al, \"Hypertext Transfer Protocol -- HTTP/1.1\".\n          Internet Draft W3 Consortium/MIT, August 1996 (Work in\n          Progress).\n\n   [4]    D. H. Crocker. \"Standard for the Format of ARPA Internet Text\n          Messages.\" STD 11, RFC 822, UDEL, August 1982.\n\n   [5]    S. Deering, R. Hinden, Editors, \"Internet Protocol, Version 6\n          (IPv6) Specification\", RFC 1883, December 1995.\n\n   [6]    D. Eastlake, \"Universal Payment Preamble\", Internet Draft\n          CyberCash, March 1996 (Work in Progress).\n\n   [7]    R. Fielding. \"Relative Uniform Resource Locators.\" RFC 1808 ,\n          UC Irvine, June 1995.\n\n   [8]    JEPI, \"Selecting Payment Mechanisms Over HTTP\", Internet Draft,\n          August 1996 (Work in Progress). [Also available as\n          http://www.w3.org/pub/WWW/Payments/JEPI/draft-jepi-uppflow-00\n          .html]\n\n   [9]    D. Kristol, \"A Proposed Extension Mechanism for HTTP\", Internet\n          Draft, January 1995 (Work in Progress, Expired).\n\n   [10]   J. Klensin, N. Freed, M. Rose, E. Stefferud, and D. Crocker.\n          \"SMTP Service Extensions.\" RFC 1869. MCI, Innosoft, Dover Beach\n          Consulting, Network Management Associates, Brandenburg\n          Consulting, November 1995.\n\n   [11]   J. Miller. \"Label Syntax and Communication Protocols (Version\n          1.1).\" W3C Proposed Recommendation PR-PICS-services , W3\n          Consortium/MIT, August 1996.\n\n   [12]   J. Postel, J. Reynolds, \"Telnet Protocol specification.\" STD 8,\n          RFC 854, USC/ISI, May 1983\n\n\n6. Author's Address\n\n   [Note: a great many people have participated heavily in the research,\n   design, development, and description of the ideas presented here.\n   Future revisions will sort out the contributions of various folks,\n   likely adding co-authors &c. In the meantime, I will be the single\n   contact point for this draft. Many, many thanks to these folks -- you\n   know who you are!]\n\n   Rohit Khare\n   Technical Staff, W3 Consortium\n   MIT Laboratory for Computer Science\n   545 Technology Square\n   Cambridge, MA 02139, U.S.A.\n   Tel: +1 (617) 253 5884\n   Fax: +1 (617) 258 5999\n   Email: khare@w3.org\n   Web: http://www.w3.org/People/Khare\n\n\n\n", "id": "lists-010-8007145"}, {"subject": "draft-jepi-uppflow0", "content": "INTERNET-DRAFT                      Joint Electronic Payments Initiative\n                                               World Wide Web Consortium\n<draft-jepi-uppflow>                                         CommerceNet\nExpires: February 1, 1997                                August 16, 1996\n\n                    SELECTING PAYMENT MECHANISMS OVER HTTP\n            Or, Seven Examples of UPP Over PEP (as used in JEPI)\n\nStatus of this memo\n\n    This document is an Internet-Draft. Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups. Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six months\n   and may be updated, replaced, or obsoleted by other documents at any\n   time. It is inappropriate to use Internet-Drafts as reference material\n   or to cite them other than as \"work in progress\".\n\n    To learn the current status of any Internet-Draft, please check the\n   \"1id-abstracts.txt\" listing contained in the Internet-Drafts Shadow\n   Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n   ftp.isi.edu (US West Coast).\n\n    Distribution of this document is unlimited. Please send comments to\n   the JEPI working group care of Jim Miller, W3C (jmiller@w3.org), Rohit\n   Khare (khare@w3.org), or Don Eastlake (dee@cybercash.com). This draft\n   is also available formatted as HTML at\n   http://www.w3.org/pub/WWW/Payments/JEPI/draft-jepi-uppflow\n\n   NOTE: This memo does not reflect the work of any current IETF Working\n   Groups. Discussion of this draft is intended to support the eventual\n   release of an IETF specification of the Universal Payment Preamble\n   (UPP) and the development of an HTTP Extension Protocol (PEP) in the\n   HTTP WG.\n\n\n1. Abstract\n\n   The Joint Electronics Payment Initiative aims to bring key industry\n   players together to assure that multiple payment protocols can operate\n   effectively in Web applications. The concrete goal is automatable\n   payment selection over HTTP.\n\n   The first step towards this was Don Eastlake's development of the\n   Universal Payment Preamble, which is also available as an\n   internet-draft (draft-eastlake-universal-payment). The second is the\n   development of an HTTP Extension Protocol to embed UPP in HTTP. The\n   latter proposal is part of the chartered activities of the IETF HTTP\n   WG (draft-ietf-http-pep-03).\n\n   This document describes how to use UPP over PEP to support payment\n   selection between clients and merchants. It explains basic operations:\n   requesting available payment choices, presenting multiple choices,\n   demanding a selection, making a selection, and accepting and rejecting\n   choices.\n\n2. Introduction\n\n   The JEPI project is using PEP as a vehicle for negotiating over\n   payment mechanism between a Web client and server. In order to\n   accomplish this, JEPI has adopted the Universal Payment Preamble (UPP)\n   proposed by Donald Eastlake as a particular protocol to be used over\n   PEP. This document describes a set of seven fundamental operations\n   that support payment mechanism negotiation, and shows how to use PEP\n   and UPP to accomplish each. In addition, it contains comments intended\n   for the implementation team for JEPI indicating the subset which are\n   actually needed for the JEPI demonstration.\n\n\n  2.1 THE SEVEN FUNDAMENTAL OPERATIONS OF PAYMENT MECHANISM NEGOTIATION\n\n    1. Request payment choices. Either end (client or server) should be\n       able to ask the other what forms of payment it supports. JEPI\n       implementors: In the demo, only the server will generate these\n       requests.\n\n    2. Present payment choices that it supports. Either end should be\n       able to list the forms of payment it supports. Notice that this\n       may not be a complete list, but rather a list of options that it\n       \"prefers\" at the current moment. This list may be presented in\n       response to a request (operation 1) or spontaneously. The latter\n       behavior is analogous to the use of \"logo stickers\" on a store\n       window or cash register. JEPI implementors: the demo will only\n       require this operation in response to a specific request.\n\n    3. Demand payment choice. The merchant (server) may demand that the\n       client choose a specific form of payment to be used to pay for\n       items. JEPI implementors: In the demo, this happens when the\n       \"invoice page\" is sent from the server to the client; the demand\n       indicates what components of the page will require a response with\n       payment choice (operation 4).\n\n    4. Make a payment choice. The cuustomer (client) can indicate the\n       payment method to be used to make a payment. This normally\n       indicates to the server that payment should actually begin, and\n       the response will either be to accept (operation 5) or reject\n       (operation 6) the chosen mechanism.\n\n    5. Accept a payment choice. The server, in response to a payment\n       choice (operation 4), may accept the choice and initiate an actual\n       payment operation. The payment operation itself is not part of the\n       JEPI project and may or may not use PEP to handle the payment.\n\n    6. Reject a payment choice. The server, in response to a payment\n       choice (operation 4), may reject the choice and request that\n       another choice be made. UPP specifies that a rejection can occur\n       either because the user canceled the transaction prior to\n       completion or because the transaction failed for other\n       (payment-system specific) reasons. They are distinguished and can\n       result in different client actions.\n\n    7. Do you accept payment by X? Either side can ask the other if it\n       supports payment by a particular payment mechanism. JEPI\n       implementors: This is not currently required for the\n       demonstration, but it might be a useful addition for the client to\n       ask the server this question prior to counter-offering with a\n       payment mechanism not mentioned by the server in its list of\n       supported mechanisms (operation 2).\n\n\n3. Notation\n\n   Amongst other things, UPP provides a uniform vocabulary for naming\n   options common to many payment systems, and a uniform syntax for each\n   such option. It is not clear at the current time what mechanism should\n   be used to allow independent payment system designers to name options\n   so that they will not collide with the UPP namespace of shared\n   options. We will use sub-bags to separate the name spaces. That is, we\n   will assume that a bag of the form {upp {upp-parameter-name\n   upp-parameter-value} ... } will be used to hold these parameters. A\n   complete list of these common parameters and their syntax is available\n   in the UPP specification.\n\n   In a complete implementation of UPP using PEP, it would be possible to\n   specify these common parameters in the PEP-specified header fields\n   Protocol:, Protocol-Request:, Protocol-Query:, and Protocol-Info: as\n   well as in any payment-system specific headers. In the JEPI\n   demonstration, however, we will not be using these parameters for the\n   generic UPP protocol (they may be used in payment-specific protocols).\n   In this document we will indicate where they are syntactically\n   permitted by using the notation \"upp-params.\" For the demonstration,\n   these will always be omitted in the examples shown here.\n\n   For clarity, we omit all of the HTTP headers and message body with the\n   exception of those parts directly related  to the operation being\n   demonstrated. The protocol-name URLs shown here are purely for\n   example, and will be determined by the participants at a later date.\n   The URLs for the various for lists will be determined by each merchant\n   application. Because we do not expect proxy servers to participate in\n   the payment negotiation shown during the JEPI demo, the scope\n   parameters of all PEP headers have been omitted: they are defined to\n   default to {scope origin} as required for the demonstration.\n   Similarly, the strength of PEP directives defaults to optional ({str\n   opt}), so it is only shown otherwise.\n\n\n4. Operation 1: Requesting Preferred Payment Choices\n\n     Either end (client or server) should be able to ask the other what\n     forms of payment it supports.\n\n\n  CLIENT ASKS SERVER\n\n   In order for the client to ask the server what payment choices are\n   available, the Protocol-Query: header is added to an HTTP request from\n   the client to the server. JEPI implementors: In the demo, only the\n   server will generate these requests.\n\nGET URL\nProtocol-Query: {http://www.w3.org/UPP upp-params}\n\n   This means \"do you have UPP available at the URL specified in the HTTP\n   request that contains this header.\" If any of the upp-params are\n   specified then they further restrict the meaning of the query (i.e. if\n   a {upp {amount {frf}}} were specified, the query would mean \"do you\n   have UPP available, for amounts denominated in French francs, at the\n   URL specified in the HTTP request that contains this header\").\n\n   In order to ask a more general question (such as \"what payment choices\n   are available for all URLs at your site\") the for option must be used:\n\nGET URL\nProtocol-Query: {http://www.w3.org/UPP {for /*} upp-params}\n\n   Notice that in a for, the \"URLs\" ending in * are actually prefix\n   strings. So the \"/*\" here means \"any URL at your server that starts\n   with '/',\" which in turn means all URLs aat your server.\n\n   The response to this HTTP message will fall into one of two\n   categories:\n     * No Protocol-Info: {http://www.w3.org/UPP A} header line. This\n       indicates that the server does not support all of PEP. [It is also\n       possible for a server to support PEP, but not UPP, in which case\n       it would send Protocol-Info: {http://www.w3.org/UPP {str ref}}]\n     * A header line of the form Protocol-Info: {http://www.w3.org/UPP A}\n       is included in the headers. This indicates that the server\n       supports PEP, and the response is in the form described below\n       under Operation 2. The header can also use a for list to hint\n       where on the server payments will be discussed.\n\n   A proper implementation of PEP requires that the protocol module\n   associated with the specified protocol will be invoked when a\n   Protocol-Query: line is encountered specifying that protocol. A proper\n   implementation of the UPP protocol module will supply one of the\n   responses indicated under Operation 2 (Present Payment Choices),\n   indicating the payment options that the server wishes to advertise.\n\n\n  SERVER ASKS CLIENT\n\n   In order for the server to ask the client what payment choices are\n   available, a similar mechanism is used. In this case, however, the\n   server should use the {for } to indicate the parts of its URL space\n   where payment might be discussed:\n\n200 OK\nProtocol-Query: {http://www.w3.org/UPP {for /PaymentPages/*}\n                                       upp-params}\n\n   Technically, this is a way for the server to ask the client to reveal\n   payments choices a user will consider for URLs that begin with\n   /PaymentPages/. The client will reply (at least) whether the\n   protocol can be used for the resource of the response, and\n   (optionally) whether it might be used elsewhere (the range the server\n   specified, anywhere on that server, etc).\n\n   A proper implementation of PEP requires that the protocol module\n   associated with the specified protocol will be invoked when a\n   Protocol-Query line is encountered specifying that protocol. A\n   proper implementation of the UPP protocol module will supply one of\n   the responses indicated under Operation 2 (Present Payment Choices),\n   indicating the payment options that the client wishes to advertise.\n\n   Then, the next time the client accesses any resource in the for list\n   from the query, it will include its answer(s) to the query.\n\n\n5. Operation 2: Present Payment Choices\n\n     Either end should be able to list the forms of payment it supports.\n     Notice that this may not be a complete list, but rather a list of\n     options that it \"prefers\" at the current moment. This list may be\n     presented in response to a request (operation 1) or spontaneously.\n     The latter behavior is analogous to the use of \"logo stickers\" on a\n     store window or cash register.\n\n     JEPI implementors: the demo will only require this operation in\n     response to a specific request.\n\n   This operation is performed by adding one or more Protocol-Info:\n   headers to the HTTP packet. If the list is being presented in response\n   to a request (operation 1), PEP requires that it include a header in\n   the following form:\n\n200 OK -or- GET ...\nProtocol-Info: {http://www.w3.org/UPP [for] [{str strength}]\n                                      upp-params}\n\n   where the for should be the same as the for clause in the request (or\n   omitted if it wasn't in the request); and the strength (if present)\n   must be ref, req, or opt. The strength can be opt (or omitted) in any\n   case; it may be ref only if payment won't be permitted at any of the\n   URLs specified by the for clause; it may be req only if payment is\n   required at all of the URLs specified by the for clause.\n\n   In addition, there should be Protocol-Info: headers for each of the\n   payment systems that are to be presented to the other end.. These will\n   have the form:\n\n200 OK -or- GET ...\nProtocol-Info: {http://...payment-system... [for] [{str strength}]\n                                            payment-params}\n\n   where payment-protocol is the URL for the specific payment protocol,\n   the for and strength are as discussed above, and the payment-params\n   are additional parameters (including the UPP parameters) that are\n   specific to the payment system.\n\n   For example, if a client receives the request:\n\n200 OK\nProtocol-Query: {http://www.w3.org/UPP {for /PaymentPages/*}\n                                       upp-params}\n\n   and wishes to indicate that it can pay using VISA over SET and via\n   CyberCash coins it might reply as follows (details of the\n   payment-specific lines are not finalized yet):\n\nHEAD ...\nProtocol-Info: {http://www.w3.org/UPP {for /PaymentPages/*}}\n   {http://www.SET.org/PEPSpec\n      {params {upp {instrument-brand VISA}}}\n      {for /PaymentPages/*}}\n   {http://www.CyberCash.com/PEPSpec\n      {params {upp {instrument-type ECASH}}}\n      {for /PaymentPages/*}}\n\n\n6. Operation 3: Demand Payment Choice\n\n     The merchant (server) may demand that the client choose a specific\n     form of payment to be used to pay for items.\n\n     JEPI implementors: In the demo, this happens when the \"invoice page\"\n     is sent from the server to the client; the demand indicates what\n     components of the page will require a response with payment choice\n     (operation 4). In the demonstration, this same invoice page will\n     carry both the operation 2 and operation 3 headers together: the\n     server will announce some of its payment options at the time it\n     issues the invoice and requires that payment be accompanied by a\n     particular payment choice.\n\n   As part of a standard server (successful) reply, it may deliver a page\n   that includes references that will require payment (i.e. a \"Pay\n   Button\" or \"Pay URL\"). These should be ideentified in the header of the\n   response packet by asking the client to respond by initiating a UPP\n   payment protocol sequence:\n\n200 OK\nProtocol-request: {http://www.w3.org/UPP {str req} {for /PayButton}}\n\n   Technically, this means that the server asks the client to use the UPP\n   protocol (operation 4) whenever it asks for retrieval of the exact URL\n   /PayButton from this same server. The {str req} is a hint to the\n   client that if it doesn't use the protocol, the request for that URL\n   will be refused. Thus, the client is not absolutely required to\n   remember that it should use UPP with the specified URL - but a network\n   roundtrip will be avoided if it does so.\n\n\n7. Operation 4: Make a Payment Choice\n\n     The customer (client) can indicate the payment method to be used to\n     make a payment. This normally indicates to the server that payment\n     should actually begin, and the response will either be to accept\n     (operation 5) or reject (operation 6) the chosen mechanism.\n\n   In practice, this will only happen when a client replies to an\n   operation 3 request for payment method. It must then respond with\n   two headers: one indicating that it is responding to a request to use\n   the UPP protocol by choosing a compatible payment protocol, and the\n   compatible protocol header itself. For example, if the payment choice\n   is to use VISA over SET, then we might expect a response as follows:\n\nGET ...\nProtocol: {http://www.w3.org/UPP {via http://www.SET.org/PEPSpec}}\n  {http://www.SET.org/PEPSpec\n     {str req}\n     {params {upp {instrument-type CREDIT} {instrument-brand VISA}}\n             other-SET-params}}\n\n   The expected response is either an operation 5 (server accepts the\n   choice of SET and VISA) or operation 6 (server refuses the choice).\n\n   It is expected that somewhere between receiving the operation 3 and\n   issuing the operation 4 the client application will have to decide on\n   the payment mechanism. Neither PEP nor UPP specifies how this happens.\n   For the JEPI demonstration, it is assumed that the browser will\n   intercept the request to access any specified payment URLs (from the\n   for list of the required challenge) and will engage in a dialog with\n   the user if necessary to produce the desired choice. This implies that\n   what merchants might typically describe as the \"Pay\" button becomes\n   the \"Choose a Payment Mechanism and Pay\" button.\n\n\n8. Operation 5: Accept a Payment Choice\n\n     The server, in response to a payment choice (operation 4), may\n     accept the choice and initiate an actual payment operation. The\n     payment operation itself is not part of the JEPI project and may or\n     may not use PEP to handle the payment.\n\n   At this point, operation 4 has provided enough information to the\n   server that it is willing to kick off the actual payment system. JEPI,\n   PEP, and UPP provide no information on precisely how to do this, but\n   there is one additional PEP/UPP header which can be optionally sent\n   back to the client. If a normal MIME-based helper application is\n   available to do the payment on the client side, then there is no need\n   for the following header. On the other hand, a better user interface\n   can often be produced if a helper application can be run while the\n   client (browser) waits for the application to complete. To support\n   this, UPP adds one final message from the server to the client. It\n   provides the URLs that should be shown in each of three cases:\n     * the payment is successfully completed\n     * the payment is canceled because of user intervention\n     * the payment is unable to complete because the computers are unable\n       to finish the transaction (network outage, over credit limit,\n       etc.)\n\n   The header is as follows:\n\n200 OK\nProtocol: {http://www.w3.org/UPP\n   {params {upp {abort abort-URL}\n                {cancel cancel-URL}\n                {success success-URL}}}\n\n\n9. Operation 6: Reject a Payment Choice\n\n     The server, in response to a payment choice (operation 4), may\n     reject the choice and request that another choice be made. UPP\n     specifies that a rejection can occur either because the user\n     canceled the transaction prior to completion or because the\n     transaction failed for other (payment-system specific) reasons. They\n     are distinguished and can result in different client actions.\n\n   If a client proposes a payment system that is not acceptable to the\n   server, the server responds with a 400- or 500-class PEP error\n   message. The body of the message should explain what went wrong as\n   well as possible, including any explanation that the requested payment\n   system may be able to supply. It should probably include a button to\n   go back to the invoice page, if possible, but the browser's BACK\n   button will work, too. The server should include one additional header\n   on this message to reduce the chance that the same payment system will\n   be tried a second time:\n\n420 Client PEP Error -or- 520 Server PEP Error\nProtocol-Info: {http://...payment-system...\n                  {str ref} payment-params}\nFollowed by an operation 3 header\n\n   where payment-system is the payment protocol that is being rejected,\n   payment-params are the parameters of the payment system which caused\n   the problem, and pay-URL is the URL of the item just requested (i.e.\n   the one that initiates the payment protocol on the server side).\n\n   The error code is distinguished mainly by whether the server has the\n   protocol and doesn't accept it and the client should know better (422\n   Protocol Extension Refused) or if the server does not have it (521\n   Protocol Extension Not Implemented) or cannot get it to work (520\n   Protocol Extension Error or 522 Protocol Extension Parameters Not\n   Acceptable). Other PEP error codes may be more specifically applicable\n   for particular payment systems.\n\n\n10. Operation 7: Do you accept payment by X?\n\n     Either side can ask the other if it supports payment by a particular\n     payment mechanism.\n\n     JEPI implementors: This is not currently required for the\n     demonstration, but it might be a useful addition for the client to\n     ask the server this question prior to counter-offering with a\n     payment mechanism not mentioned by the server in its list of\n     supported mechanisms (operation 2).\n\n   The PEP header Protocol-Query can be used by either party at any time\n   to ask this question. As with operation 1, there is a technical\n   meaning for the query that requires the other end to respond with a\n   Protocol-Info response that is specific to the particular URL being\n   queried, and the {for A} construct can be used to generalize the\n   query.\n\n   Also as with operation 1 and 2, a proper implementation of a payment\n   system module for use with UPP should provide additional information\n   about where and with which parameters the payment system will operate\n   when it is possible to do so. That is, a request for \"do you support\n   SET for VISA at URL /MerchantHomePage\" must be answered \"no\" (unless\n   payment happens on the home page), but a more thorough response will\n   volunteer the information that such a payment is permitted elsewhere\n   at the site.\n\n200 OK -or- GET ...\nProtocol-Query: {payment-system [payment-system-params]}\n\n   where payment-system is the URL of the payment system protocol, and\n   payment-system-params are parameters specific to that protocol\n   (including common UPP parameters).\n\n11. Security Considerations\n\n   None of these message headers have security protection. They should be\n   trusted only if received through a trusted medium (private channel,\n   etc). In addition, UPP makes no security claims about the contents of\n   the headers; ALL payment-related data should be recapitulated within\n   the particular (presumably cryptographically secure) payment protocol.\n\n   In short, this protocol only addresses payment selection in the clear.\n   Security of the overall payments process lies in other components.\n\n12. Authors' Addresses\n\nDonald E. Eastlake 3rd\nCyberCash, Inc.\n318 Acton Street\nCarlisle, MA 01741 USA\nTel: +1 (508) 287 4877 (+1 703 620 4200 main office, Reston, Virginia, USA)\nFax: +1 (508) 371 7148\nEmail: dee@cybercash.com\n\nRohit Khare\nTechnical Staff, W3 Consortium\nMIT Laboratory for Computer Science\n545 Technology Square\nCambridge, MA 02139, U.S.A.\nTel: +1 (617) 253 5884\nFax: +1 (617) 258 5999\nEmail: khare@w3.org\n\nJim Miller\nTechnology & Society Area Leader, W3 Consortium\nMIT Laboratory for Computer Science\n545 Technology Square\nCambridge, MA 02139, U.S.A.\nTel: +1 (617) 253 3194\nFax: +1 (617) 258 5999\nEmail: jmiller@w3.org\n\n\n\n", "id": "lists-010-8047603"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    > I guess I've never really understood what OPTIONS is supposed\n    > to do, nor does the existing spec language make it clear that\n    > this is allowed to carry the various request-headers that\n    > are needed to report use-counts broken down by, say, User-agent.\n    \n    My inclination is not to disallow anything unless it is known to be\n    harmful, and I don't think that this usage of OPTIONS would be harmful.\n\nOK, but it doesn't say this in the spec, so (although the robustness\nprinciple would certainly imply this) there is no way to insist that\nevery implementor of OPTIONS gets it right.\n    \n    Actually, what made me think of it was the fact that a HEAD request\n    is normally considered another \"hit\" on that resource, whereas an\n    OPTIONS request should not be (speaking with my wwwstat hat on).\n    That begs the question of whether the proxy or origin would reduce\n    the reported count by one to avoid improperly counting the last HEAD\n    action as a \"hit\"? Such a question is difficult to answer, since it\n    depends on how the server collects and analyzes such data.\n\nIf the recipient knows how to interpret the use-count information,\nthen it isn't ambiguous (the hit count is increment by the use-count\nvalue, period).  But perhaps we need a way to prevent the sending\nof use-counts in situations where they are not wanted or expected.\n    \n    > Finally, OPTIONS responses include messages bodies, and HEAD\n    > responses \"MUST NOT\", so I would expect the use of HEAD to\n    > require few bits transferred over the wire.\n    \n    Nope -- successful responses to OPTIONS must not include entity info.\n\nI guess I'm still confused.  I probably misread \"the response MUST NOT\ninclude entity information other than what can be considered as\ncommunication options\" as applying to the body, when it actually seems\nto apply to the header.  But unless you mean something different by\n\"entity info\", then there's an inconsistency between what you just said\nand what you wrote into the spec.\n    \n    >     Does it make a difference when going through old proxies?\n    >     \n    > You mean, because HTTP/1.0 proxies don't understand OPTIONS and would\n    > reject such requests?  In theory, yes, although I'm not sure this would\n    > be an issue, because a server that wants accurate use-counts would only\n    > believe them from a \"cooperative\" cache, which is identified by\n    > \"Connection: coop\" in the request, and we said\n    > \n    >      Note: a server might distrust such a request-header  when\n    >      received from an HTTP/1.0 client, which might have incorrectly\n    >      forwarded the Connection header.\n    \n    I was going to mention this later (along with a real review of the draft),\n    but that is not an appropriate use of Connection.  For one thing, assuming\n    that all intermediaries have caches (and thus would assign any meaning to\n    being cooperative) is wrong.  For another, it doesn't take advantage of\n    the extensibility mechanism inherent in cache-control.  Instead of all\n    the coop negotiation, an origin server should decide whether being\n    cooperative is required or optional.  If it is required, then send\n    \nCache-control: proxy-revalidate, coop\n    \n    with coop (or something similar) being defined as a modifier on\n    proxy-revalidate such that caches which obey the coop directive\n    (whatever that may imply) may ignore the proxy-revalidate.\n    \nThat's an interesting suggestion (and it's nice to know that the\nproxy-revalidate design we came up with seems to be the appropriate\nmechanism for this purpose).\n\nBut without some additional mechanism, I don't think this is going\nto work.  I don't see how it solves the problem of HTTP/1.0 proxy\nin the response chain.  Sad to say, but I don't think our hit-metering\ndesign would work if there is an HTTP/1.0 proxy in the chain; either\nit might respond to the use-count reports from its own cache, or\n(if we were to adopt the OPTIONS proposal) it wouldn't forward them.\nSo we need a foolproof way to prevent the use of hit-metering if\nthere are any pre-HTTP/1.1 proxies in the picture.\n\nUntil an origin server can determine with 99.999% reliability that its\n\"Cache-control: proxy-revalidate, coop\" will only let cooperative\nproxies cache the response, it's not going to allow any cache to do\nso.  And so nobody will use hit-metering, and we make no progress.\n\n-Jeff\n\n\n\n", "id": "lists-010-8080363"}, {"subject": "Re: draft-ietf-http-pep0", "content": "Rohit,\n\nJust for your information, I am currently working on your\nInternet-Draft <draft-ietf-http-pep-00.txt>.\n\nI'll probably send an announcement to the entire IETF\nregarding your I-D sometime tomorrow or the next day.\n       \nP.S.  As a normal procedure, please note that I've changed\n      the filename from \"draft-khare-http-pep-01.txt\" to\n      \"draft-ietf-http-pep-00.txt\" \n\nKind Regards,\n\nCynthia Clark\nInternet-Drafts Administrator\ncclark@ietf.org\n\n\n\n", "id": "lists-010-8092370"}, {"subject": "Re: (revised) HTTP working group statu", "content": "Larry Masinter:\n[...]\n>PLEASE send me any corrections.\n[...]\n\n>- content negotiation: draft-holtman-http-negotiation-02.txt\n>    Andy Mutz will co-edit with Koen Holtman.\n>    Active discussion in the working group on details.\n>    (I also have private mail suggesting that W3C may also\n>    contribute, but no details or timetable.)\n>\n>- User Agent characteristics: draft-mutz-http-attributes-01.txt\n>  Aug 1 draft ties in with content negotiation proposal; needs\n>  non-numeric attributes.\n\n`needs needs non-numeric attributes' is a but ambiguous.  More\naccurate is `future version will use non-numeric attributes'.  And at\nthe end of the `content negotiation' paragraph: `future version will\ndefine non-numeric attributes'.\n\n   - - -\n\nPlease also put these issues on the list:\n\n- there is a desire to make a shared recommendation on the use of the\n1.1 version number\n\n- when creating 1.1, we used the following rule a number of times:\n\n Any proposed HTTP/1.1 features not in HTTP/1.0 for which there is no\n consensus will revert to HTTP/1.0 status in 1.1 and be considered for\n inclusion in HTTP/1.2.\n\nThis rule also implies a promise that deferred proposals would not\ndisappear into the bit bucket.  I therefore propose an action item to\ngather such `not 1.1' things from the 1.1 issues list and record them\nin a document, to be passed on to any future 1.2 WG.  (By the way,\nhave we decided already that this WG would not do 1.2?)\n\n>Larry\n\nKoen.\n\n\n\n", "id": "lists-010-8100360"}, {"subject": "Re: Entity Tag", "content": "    But... I dunno. What precisely are the odds that someone will modify a\n    file more than once per second, and leave the file the exact same\n    length? Pretty low, one would think.\n    \nDanger! Danger!  I think it would be foolish to make this assumption.\nCertainly the part about leaving the file length the same, since many\nfrequently-modified files only change a few bytes.\n\nFor example, imagine a stock-quote page where the quoted price of,\nsay, EDS goes from \"52 7/8\" to \"51 1/8\".  That doesn't change the\nlength of the file at all.  OK, this is bad example; nobody in their\nright heads would cache stock-quote values, but my point is that\nrelying on file-length is a really bad idea.\n\n    Actually, it brings up an interesting point: when matching ETags for\n    ranges, one must use a strong validator comparison. But you can also\n    use dates. Namely, this is perfectly valid:\n    \n    GET /filename HTTP/1.1\n    Range: bytes=30-500\n    If-Range: Thu, 15 Aug 1996 06:35:59 GMT\n    \n    Surely a date is a much weaker validator than the hashes that we are\n    implementing? Yet as I read the spec, it is valid in this\n    application.\n\n\"Surely\" not.  If you are doing this comparison at a proxy cache,\nand you have the Date of the response as well as the Last-modified\ntime, then 13.3.3 explains rules for both the client and the proxy\nto follow so that there is no actual \"weakness.\"\n\nAnd, if the server is actually able to determine that it not handed\nout the same last-modified time for two different versions of the\nresource, then you can also support your example at the origin server.\n\nWhen you write, of your example, \"Namely, this is perfectly valid\"\nyou are only correct if you also know the Date: value for the cached\nresponses, and if that Date value is sufficiently later than the\nLast-Modified value.  Read the spec.\n\nOf course, if we discover that client implementors aren't reading\nsection 13.3.3, then we'll have to prohibit servers from doing this.\nBut 13.3.3 is cross-referenced from several places, so there is\nreally no excuse for not reading it.\n\n-Jeff\n\n\n\n", "id": "lists-010-8109370"}, {"subject": "Re: When to make objects uncacheable ", "content": "> The CGI URLS at my proxy make up 11% of all accesses.  In terms of\n> unique URLs, CGI URLs make up 15%.  Of these 56% are accessed more\n> than once within a week.   And the average number of times these URLs\n> are accessed is 1.9.  This means that if I were to cache CGI URLs, I\n> might be able to get a 48% hit rate.\n...\n> Total accesses (including client cache hits and failures)  3,586,096\n> Total successful transfers                                 2,502,142\n> Number of unique URLs                                        949,613\n> Number of unique URLs repeated                               234,906\n> Total successful CGI transfers                               274,809\n> Number of unique CGI URLs                                    143,543\n> Number of unique CGI URLs repeated                            23,033\n>\n> Total bytes of data transferred                          25380996328\n> Total bytes of data transferred once only                10225694349\n> Total bytes of unique data transferred                   13097062787\n> Total bytes from repeated URLs                           12283933541\n> Total bytes from repeated CGI URLs                         988160134\n>\n> Average transfer size                                          10143\n> Average CGI transfer                                            6404\n\nI'd be more interested in increasing the hit rate on cacheable URLs.\nI can't discern the hit rate from your data, but if you're getting a\n40% hit rate then, sure, you can try to squeeze the remaining 5% (48%\nof 11%) out of it.\n\nHere are my cache stats (for a small workgroup, data in megabytes) -\n\nTotal cacheable URLs:       64194/90.51\nTotal cacheable data:       427.2/91.98\nUnique cacheable URLs:      29799/46.42/42.01\nUnique cacheable data:      301.2/70.50/64.85\nURLs accessed only once:    23138/77.65/36.04/32.62\nData accessed only once:    242.2/80.41/56.69/52.15\nUnique non-cacheable URLs:   1204/17.88/ 1.70\nUnique non-cacheable data:    6.5/17.37/ 1.39\n\nI have similar numbers in terms of cacheable (91%) and non-cacheable\n(9%) URLs.  The percentage of URLs accessed only once is relatively\nhigh: 78% of unique cacheable URLs, 36% of total cacheable URLs, or\n33% of total URLs.  And the percentage of data accessed only once is\neven higher: 80% of unique cacheable, 57% of total cacheable, or 52%\nof total data.\n\nHIT/freq:       10721/16.76     96.4/20.76\nMISS/freq:      33777/52.81\nEXPIRED/freq:     697/ 1.09\nREFRESH/freq:    3196/ 5.00\nIMS/freq:       15564/24.34\nERR/freq:         239/ 0.37\n\nThe percentage of hits is relatively small (17% of requests and 21% of\ndata) and I'd like to increase this.  But it looks like the best I can\nhope for is about 40% of total data volume (+ 52% accessed once + 8%\nnon-cacheable = 100%).  Has anyone been able to do better than 40%?\nI'm wondering if that's the practical limit.\n\n...tai\n\n\n\n", "id": "lists-010-8118694"}, {"subject": "Re: (revised) HTTP working group statu", "content": "> - there is a desire to make a shared recommendation on the use of the\n> 1.1 version number\n\nAs far as I am concerned, this is out of scope of the HTTP working\ngroup. It's our job to come to consensus on Internet standards\ndocuments. We cannot control what various vendors marketing\ndepartments might do, and our recommendation on the use of the version\nnumber is already out there: draft-ietf-http-v11-spec-07.* Anyone can\nread RFC 1602 on the Internet Standards Process and come to their own\nconclusions about what they will or will not do with regard to\nemploying that specification or its designation.  As a working group,\nall WE can do is to change the version number of our specification. We\nmay yet threaten to do so, with decreasing credibility.\n\n> - when creating 1.1, we used the following rule a number of times:\n\n>  Any proposed HTTP/1.1 features not in HTTP/1.0 for which there is no\n>  consensus will revert to HTTP/1.0 status in 1.1 and be considered for\n>  inclusion in HTTP/1.2.\n\nWe have done so. Starting a month before the last IETF, we've called\nfor people to consider those issues remaining and decide which ones we\nshould actually include as future work. The result of that call was a\nlimited number of issues; those are the issues we're considering\nnow. I've not heard anyone mention any other issues, and I have no\ndesire to raise anything that isn't of concern at this point.\n\n> This rule also implies a promise that deferred proposals would not\n> disappear into the bit bucket.\n> I therefore propose an action item to gather such `not 1.1' things\n> from the 1.1 issues list and record them in a document, to be passed\n> on to any future 1.2 WG.  (By the way, have we decided already that\n> this WG would not do 1.2?)\n\nI think you misunderstood. There's been ample opportunity to do this\nalready.  This working group will deal with all remaining important\nissues in HTTP 1.x and then close. We're currently scheduled to do so\nby December. We may call the resulting protocol HTTP 1.2, if we need\nto increment the version number. (It's not clear to me at this point\nthat it will be necessary to increment the version number.)\n\nIf you think there are a list of issues that were raised before, are\nnot dealt with in HTTP/1.1 and need to be before the working group\ncloses, please name them. In lieu of WG consensus to process those\nitems, though, we should stop.\n\nPersonal opinion:\n\nI believe that HTTP 1.x is near the end of its evolutionary life as a\nprotocol.  Most of the things people want to do with the web either\ncan already be done with HTTP/1.1 or else are so completely beyond its\ncapabilities that some other protocol family is needed (RealAudio,\nInternet Telephony, etc.)  I think dramatically better performance,\nsecurity and reliability also require substantial incompatible\nprotocol changes.\n\nIt's often the tendency of working groups to thrash a while after\nthey're actually done; I think we should resist. There's no need for\nan endless stream of HTTP versions, and continually fiddling with the\nprotocol is counter-productive -- the world needs the standards to be\nstable, so that we can build real applications without \"continuous\nimprovement\".\n\nI think we've identified a few issues where it's worth our while to\npursue standardization, and we should continue investigation. However,\nI'm personally skeptical about most of the things that are _currently_\non our action list, and am not looking for more to add.\n\nLarry\n\n\n\n", "id": "lists-010-8129031"}, {"subject": "Re: Entity Tag", "content": "Isn't there some confusion about the issue of file modification dates\nas etags? The possibility that the file might be updated more quickly\nthan the clock resolution doesn't matter, as long as the server never\nsends a date that is the same as 'now'. As long as the modification\ndate is in the past, then you can use that date as a validator; even\nif the file gets modified 100 times in a nanosecond, it doesn't matter\nsince the new times will be different than the time you sent before.\n\nHere's how to write a safe server:\n   a) lock file from modifications\n   b) check last modification date compared to current time\n   c) if last modification date is the same as current time,\n      then wait one clock tick\n   d) send file with file modification date\n   e) unlock file\n\nIf you can't lock your files from modification while they're being\nsent, you're already in trouble. (What if it's updated in place while\nyou're sending it??!?). The only alternative would be to copy the file\nsomewhere else (into memory, for example):\n\n   a) note modification date of file.\n   b) copy file somewhere else.\n   c) check that the file wasn't modified while you were copying it.\n      If it was, go back to (a). (If you do this too many times,\n      you might send an error message, since there's no reliable\n      way to save a file that gets updated more quickly than you can \n      copy it.)\n   d) send copy of file with original modification date\n\n\n\n", "id": "lists-010-8140279"}, {"subject": "Re: (revised) HTTP working group statu", "content": ">I believe that HTTP 1.x is near the end of its evolutionary life as a\n>protocol.  Most of the things people want to do with the web either\n>can already be done with HTTP/1.1 or else are so completely beyond its\n>capabilities that some other protocol family is needed (RealAudio,\n>Internet Telephony, etc.)  I think dramatically better performance,\n>security and reliability also require substantial incompatible\n>protocol changes.\n\nI disagree, there are many extensions that are possible within the \nHTTP/1.x framework. The fact that non has been raised on the\nlist is hardly suprising, during the finalisation of the 1.1 draft \nit was made very plain to those of us who wanted to extend HTTP\nfunctionality that it was not a good time to bring it up.\n\nI think that rather than introducing new protocols we need to \nintroduce new classes of application that exploit the existing\ncapabilites of HTTP. There has been relatively little exploration\nof the editing browser concept. With the exception of NaviPress\nmost HTML editors have failed to implement POST for example.\n\nAdding security to HTTP need not involve enormous complexity.\nI'll not name the vendor who I discussed signing of documents with\nand got the respone \"Oh you are still using PKCS#1? we are up to \nPKCS#7!\", point is that some people seem to get caught up in\nmystification.\n\n\nWe can add signatures to HTTP very simply with one tag :-\n\nContent-Signature: <key-uri>; alg=DSA; value=<base64crud>\n\n\nContent signatures are really just assertions concerning an entity.\nIf one introduces a wrapper into the protocol one can make \nassertions about an entity and its meta-data.\n\nClearly this is a class of secuirty that SSL and other systems that\nattempt to hack the session layer can never provide. It is\nunnecessary to apply the excessive mechanism that S-HTTP involves\nsimply to provide non repudiation of an action.\n\n\nPhill\n\n\n\n", "id": "lists-010-8149299"}, {"subject": "Re: (revised) HTTP working group statu", "content": "Phill,\n\nYour example about possible extensions to HTTP was for security.\nI will point out (once again) that the SHTTP draft has now completed\n\"last call\" in the IESG with (as far as I can tell) no discussion or\ncomment. If the work of the WTS working group is inadequate for web\nsecurity, the appropriate forum for that discussion is on the WTS\nmailing list (or, at this point, to IESG) and not in HTTP-WG.\n\nLarry\n\n\n\n", "id": "lists-010-8158569"}, {"subject": "Re: Entity Tag", "content": "On Mon, 19 Aug 1996, Jeffrey Mogul wrote:\n\n> For example, imagine a stock-quote page where the quoted price of,\n> say, EDS goes from \"52 7/8\" to \"51 1/8\".  That doesn't change the\n> length of the file at all.  OK, this is bad example; nobody in their\n> right heads would cache stock-quote values, but my point is that\n> relying on file-length is a really bad idea.\n\nJust to put a real stake in the ground, I've received two *.dll-s from\na vendor. The second includes fixes/changes from the first, but the\nlength is identical (5xx120 bytes, I don't remember the xx). In this\ncase, the only improbability is generation of two 500K dlls within 1\nsecond ;-:)\n\nBottom line, you can't assume the length won't change.\n\nDave\n\n\n\n", "id": "lists-010-8166660"}, {"subject": "Re: draft-ietf-http-pep0", "content": "In general, this is a well-written draft and I very much appreciate\nthe early emphasis on how PEP differs from the normal extension process\nof adding header fields.  However, it does not consider the other extension\nmechanisms of methods, media-types, content-codings, transfer-codings,\nand status codes.   In particular, I think you should justify the use\nof ordered header fields instead of encapsulated media types and how\nthat affects the interpretation of an HTTP message [if it does].\n\nSpec-wise, please do not include the colon when referring to a\nheader field, e.g.,\n\n>    Using PEP in this way also solves the next logical problem: how can a\n>    client request a server to initiate vending PICS labels? The same\n>    structure for a Protocol: can be reused in a Protocol-Request: asking\n>    the counterparty to start using the specified extension.\n\nshould be either \"for a Protocol header field\", \"for a Protocol field\",\nor simply \"for Protocol\" (assuming you are defining Protocol in the BNF).\nThe reason is that the \":\" is not part of the field-name (it is the\nseparator) and people have a habit of misinterpreting the syntax when\nit is glommed together.\n\nAlso, I think it would improve readability if more of the PEP syntax\nwas introduced before the discussion of PEP usage, and then actual syntax\nexamples given with the PEP usage, but that is just my opinion.\n\n>    Unlike some previous extension proposals [9], and unlike some existing\n>    HTTP/1.1 features (Upgrade: and Connection: tokens), extensions are\n>    attributes of a resource rather than a server. This means all PEP\n>    statements are applied to the single resource mentioned in any HTTP\n>    message (though there is a facility to hint at which other resources\n>    the same statement applies to).\n\nThe above is awkward and a bit out of scope -- I would replace it with\n\n     PEP extensions are attributes of a resource rather than of the\n     client, server, or current connection; this is unlike the Upgrade\n     and Connection fields of HTTP/1.1, which apply only to the current\n     connection, and [which???] extensions in an earlier proposal [9].\n     PEP statements apply to the resource indicated by the HTTP message,\n     which is generally the requested resource. (However, there is a\n     facility to hint at other resources for which the same statement\n     applies).\n\n>    A client can wait to piggyback its queries on other HTTP transactions\n>    or it can explictly issue an OPTIONS request with its queries.\n\nThis would be clearer if you used OPTIONS in the examples (particularly\nin the JEPI draft where it improperly uses GET in places it would not be\nused in practice).\n\n>    This directive packages together any added header lines and any\n>    reprocessing of the message body. Since HTTP/1.1 eliminates many\n>    earlier ambiguities about the ordering of headers, we can state\n>    categorically that protocol directives must be dispatched in the order\n>    they are listed in the Protocol: header. [The Content-Encoding:\n>    pipelining mechanism of earlier proposals has been scrapped].\n\nTrue, but only if the extension does not affect the data type of the\nmessage body.  If it does, that must be indicated via a change in one of:\nContent-Type, Content-Encoding, or Transfer-Encoding, depending on how\nit is intended to be interpreted.  There are many possibilities in how\nto indicate such a change, but something must appear in one of those fields.\n\n>        word             = token | quoted-string\n>        token            = 1*<any CHAR except CTLs or tspecials>\n>        tspecials        = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n>                         | \",\" | \";\" | \":\" | \"\\\" | <\">\n>                         | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n>                         | \"{\" | \"}\" | SP  | HT\n>        quoted-string    = ( <\"> *(qdtext) <\"> )\n>        qdtext           = <any CHAR except <\"> and CTLs but including LWS>\n\nUnless you intend to change the above, they should just reference HTTP/1.1.\n\n>    scope\n>           scope is either of \"conn\" or \"origin\" ; the default is\n>           \"origin\". It defines which agent must recognize this directive:\n>           the immediate recipient or the origin client/server.\n\nUmm, what about a scope of \"all\"?  PEP cannot be used to express many\ncache-related extensions without the notion of a requirement that is\napplicable to all recipients.  The same applies to some encryption extensions.\nI know you had it before -- what happened to it?\n\n>    500 Class\n>           520 Protocol Extension Error\n>           521 Protocol Extension Not Implemented\n>           522 Protocol Extension Parameters Not Acceptable\n\nIs 522 intended for gateways?  It sounds more like a 4xx response.\nThe reason-phrase is not sufficient to divine the purpose of each code.\n\n>    [3]    Jim Gettys, et al, \"Hypertext Transfer Protocol -- HTTP/1.1\".\n>           Internet Draft W3 Consortium/MIT, August 1996 (Work in\n>           Progress).\n\nPlease include the correct reference (and a URL is appropriate for drafts).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-8174132"}, {"subject": "Re: (revised) HTTP working group statu", "content": "Larry Masinter:\n>\n>> - when creating 1.1, we used the following rule a number of times:\n>\n>>  Any proposed HTTP/1.1 features not in HTTP/1.0 for which there is no\n>>  consensus will revert to HTTP/1.0 status in 1.1 and be considered for\n>>  inclusion in HTTP/1.2.\n>\n>We have done so. Starting a month before the last IETF, we've called\n>for people to consider those issues remaining and decide which ones we\n>should actually include as future work.\n\nOK.  If you feel that these issues got adequate consideration, that is\ngood enough for me.  I agree that we should not add any additional\nissues for this WG.\n\n[....]\n> This working group will deal with all remaining important\n>issues in HTTP 1.x and then close. We're currently scheduled to do so\n>by December. We may call the resulting protocol HTTP 1.2, if we need\n>to increment the version number. (It's not clear to me at this point\n>that it will be necessary to increment the version number.)\n\nNone of the drafts before us seem to require incrementing the version\nnumber, they are all `extensions on top of 1.1'.  (They had better be\nfor editorial reasons alone: there is no way we can add N pages to\nthe 1.1 draft and call the result 1.2.)\n\n[...]\n>Personal opinion:\n>\n>I believe that HTTP 1.x is near the end of its evolutionary life as a\n>protocol.\n\nI think we will see lots of extensions, but they will be created\nwithin the feature negotiation and PEP frameworks, not in the IETF\nframework, where creating anything takes a lot more time.  I don't\nthink an IETF WG could add much value to the 1.x framework after we\nhave feature negotiation and PEP.\n\nPersonally, after this WG closes, I'd rather work in a WG chartered to\ndo 2.x.  This is where I can see some substantial added value which\ncan only be gotten with an IETF-like activity.\n\nBy the way, about closing this WG: If I'd have to choose between\n\n 1) closing in December\n 2) closing when transparent negotiation and PEP have\n    proposed status \n\nI'd choose 2).  I of course hope that 1) and 2) will coincide, and\nwill work to make this happen.  But if they do not coincide, I'd go\nfor 2).\n\n>Larry\n\nKoen.\n\n\n\n", "id": "lists-010-8187220"}, {"subject": "I-D ACTION:draft-ietf-http-pep00.tx", "content": "A New Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                \n\n       Title     : HTTP/1.2 EXTENSION PROTOCOL (PEP)                       \n       Author(s) : R. Khare\n       Filename  : draft-ietf-http-pep-00.txt\n       Pages     : 11\n       Date      : 08/19/1996\n\nPEP is a system for HTTP clients, servers, and proxies to reliably reason \nabout custom extensions to HTTP. Traditionally, HTTP agents offer extended \nbehavior by private agreement using additional message headers. PEP has \nfeatures for expressing the scope, strength, and ordering of such \nextensions, as well as which extensions are available.      \n               \nPEP as described here is substantially simpler than previous drafts.  \nThe title has also been changed, since PEP addresses a chartered \nHTTP WG work item.                                                                      \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-pep-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-pep-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (193.205.245.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-pep-00.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@ietf.org\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n", "id": "lists-010-8197273"}, {"subject": "Re: When to make objects uncacheable ", "content": "Tai Jin wrote:\n>> The CGI URLS at my proxy make up 11% of all accesses.  In terms of\n>> unique URLs, CGI URLs make up 15%.  Of these 56% are accessed more\n>>\n>>  ...\n>>\n>> Average CGI transfer                                        6404\n\n>I'd be more interested in increasing the hit rate on cacheable URLs.\n>I can't discern the hit rate from your data, but if you're getting a\n>40% hit rate then, sure, you can try to squeeze the remaining 5% (48%\n>of 11%) out of it.\n\nWell, actually the current hit rate is zero.  Due to sizing\nproblems we have had to turn off caching a few weeks ago.  That's\nwhy I found out about this list, and also why I wrote some code to\nanalyze 600MB of log files.  I'm looking to implement a multi-level\ncache, distributed over three sites, but today I am just collecting\ndata to model the sizing issues.\n\nTo answer another recent post, I am looking at implementing a central proxy \ncache of ~30 GB, using Netscape 2.0 (or 2.1 which Netscape claims will be \nperformance-enhanced).  We will start testing in a few weeks.\n\n>Here are my cache stats (for a small workgroup, data in megabytes) -\n>\n>Total cacheable URLs:       64194/90.51\n>Total cacheable data:       427.2/91.98\n>Unique cacheable URLs:      29799/46.42/42.01\n>Unique cacheable data:      301.2/70.50/64.85\n>URLs accessed only once:    23138/77.65/36.04/32.62\n>Data accessed only once:    242.2/80.41/56.69/52.15\n>Unique non-cacheable URLs:   1204/17.88/ 1.70\n>Unique non-cacheable data:    6.5/17.37/ 1.39\n>\n>I have similar numbers in terms of cacheable (91%) and non-cacheable\n>(9%) URLs.  The percentage of URLs accessed only once is relatively\n>high: 78% of unique cacheable URLs, 36% of total cacheable URLs, or\n>33% of total URLs.  And the percentage of data accessed only once is\n>even higher: 80% of unique cacheable, 57% of total cacheable, or 52%\n>of total data.\n>\n>HIT/freq:       10721/16.76     96.4/20.76\n>MISS/freq:      33777/52.81\n>EXPIRED/freq:     697/ 1.09\n>REFRESH/freq:    3196/ 5.00\n>IMS/freq:       15564/24.34\n>ERR/freq:         239/ 0.37\n>\n>The percentage of hits is relatively small (17% of requests and 21% of\n>data) and I'd like to increase this.  But it looks like the best I can\n>hope for is about 40% of total data volume (+ 52% accessed once + 8%\n>non-cacheable = 100%).  Has anyone been able to do better than 40%?\n>I'm wondering if that's the practical limit.\n\nThat's what I'm seeing as well.  The best I could hope for would be\n37% of the URLs, which would account for 48% of the data.  And this\nassumes that none of the pages have expired (which I can't easily\nsee from the logs) within the week.\n\nWhat I did notice that as the number of logs analyzed increase, the\npotential hit rates did get better.  When I looked at one day, I\ncalculated that I should only be able to get a 28% hit rate(URLs).\nWith two days data in cache, I should be able to get a 33% hit\nrate.  One week - 37%.\n\nThe relationship is not linear with respect to time, and I expect\nto see diminishing returns, but I imagine that if the cache is\nlarge enough to store all accesses for a month, the hit rate would\nincrease even higher.  I could try to analyze a month's worth of\naccess logs to calculate the potential hit rates, but my current\nprogram isn't up to the task.  However, the overhead of managing\nthe cache increases with the size. Or is it the square of the\nsize?\n\nChris\n\n\n\n", "id": "lists-010-8207538"}, {"subject": "Re: When to make objects uncacheable ", "content": "Hull, Chris wrote:\n> However, the overhead of managing\n> the cache increases with the size. Or is it the square of the\n> size?\n\nIt depends on how the cache is structured (and the OS it runs on). If stupidly\nstructured (i.e. flat) on a standard Unix then it is O(n^2), or worse. If\nsensibly structured, then it is O(n*log(n)) (probably).\n\nCheers,\n\nBen.\n\n> \n> Chris\n> \n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-8219540"}, {"subject": "Re: When to make objects uncacheable ", "content": "> That's what I'm seeing as well.  The best I could hope for would be\n> 37% of the URLs, which would account for 48% of the data.  And this\n> assumes that none of the pages have expired (which I can't easily\n> see from the logs) within the week.\n>\n> What I did notice that as the number of logs analyzed increase, the\n> potential hit rates did get better.  When I looked at one day, I\n> calculated that I should only be able to get a 28% hit rate(URLs).\n> With two days data in cache, I should be able to get a 33% hit\n> rate.  One week - 37%.\n>\n> The relationship is not linear with respect to time, and I expect\n> to see diminishing returns, but I imagine that if the cache is\n> large enough to store all accesses for a month, the hit rate would\n> increase even higher.  I could try to analyze a month's worth of\n\nOf course, it depends on how long you set your TTLs and how long ago\nthe modified times are.  I suspect that with longer TTLs you'll see\nmore IMS and REFRESH requests (due to users hitting the reload\nbutton), which is actually not a bad thing since you're still saving\nbandwidth over a clean MISS.\n\n...tai\n\n\n\n", "id": "lists-010-8228432"}, {"subject": "Re: draft-ietf-http-pep0", "content": "Thanks to Roy for his comments -- quick reaction time! -- here are some of my  \npublic responses...\n\nI have selected some of Roy's comments in > sections.\n\n>  In general, this is a well-written draft and I very much\n>  appreciate the early emphasis on how PEP differs from\n>  the normal extension process of adding header fields.\n>  ...\n>  In particular, I think you should justify the use of\n>  ordered header fields instead of encapsulated media types\n>  and how that affects the interpretation of an HTTP message\n\nFundamentally, this has to do with differing purposes in drafting. This  \ngo-round is explicitly a discussion draft about \"the extension problem\" and  \nless a formal specification in the form of language amending 1.x (like  \nprevious drafts, which did cover this arcana).\n\nThe underlying question behind this is, then: \"should extensions affect the  \ninterpretation of HTTP content or only HTTP protocol features?\". Namely, it is  \nin our power to say we *only* want extension to message headers, but  \nencryption, say, should be done the MOSS/MIME way and not to let a PEP  \nextension rejigger content bytes.\n\n>  Spec-wise, please do not include the colon when referring\n>  to a header field, e.g.,\n\nCool. The typewriter font in the HTML makes it clear anyway. This and several  \nother minor edits have been applied inW3C WD and IETF I-D format HTML and can  \nbe reached from our Technical Reports page:  \nhttp://www.w3.org/pub/WWW/TR/WD-http-pep.\n\nI have also incorporated rewording about previous proposals:\n\n]  Protocol extensions are attributes of a resource\n]  rather than an entire server or connection, which\n]  was the model in 1.1's Connection and Upgrade\n]  fields, and in previous extension proposals [9].\n]  This means each PEP statement is applied to the\n]  single resource mentioned in an HTTP message\n]  (though there is a facility to hint at which\n]  other resources the same statement applies for).\n\n>  This would be clearer if you used OPTIONS in the examples\n>  (particularly in the JEPI draft where it improperly uses\n>  GET in places it would not be used in practice).\n\n1) the JEPI project is being deployed atop 1.0 products, essentially, and can  \nnot rely on OPTIONS 2) From my reading the JEPI document uses GET and POST  \ncorrectly; detailed feedback is appreciated. In section 10, a query is  \npiggybacked on a GET, but could use OPTIONS if available.\n\n>  If it does [affect the body], that must be indicated via\n>  a change in one of:  Content-Type, Content-Encoding, or\n>  Transfer-Encoding, depending on how it is intended to be\n>  interpreted.  There are many possibilities in how to\n>  indicate such a change, but something must appear in one\n>  of those fields.\n\nMust? There are lines that should be drawn around MIME influence in this  \nprotocol. I have an open mind, all the way up to forbidding body-rewriting  \n(see above), but HTTP is a sovereign protocol which can define its own  \nextensions -- extensions which cannot be represented for MIME, anyway. Of  \ncourse, I can see that the MIME community might want to adopt parts of PEP,  \ntoo...\n\nAnother way to placate this need is to put in a blanket \"pep\" C-E: indicating  \nthat critical content modification was done by pep extensions.\n\n> [references to grammar rules]\n\nRoy! It says right above that section: these rules are copied from 2.2 of  \nHTTP/1.1. Grant the reader a *little* mercy in tracing back every reference in  \na spec... In any case, the spirit of this section is to normatively refer to  \n1.1.\n\n>  Umm, what about a scope of \"all\"?  PEP cannot be used to\n>  express many cache-related extensions without the notion\n>  of a requirement that is applicable to all recipients.\n>  The same applies to some encryption extensions.  I know\n>  you had it before -- what happened to it?\n\nThe \"route\" scope used to apply to all agents that handled a message; when  \nsome of us gamed it out, it was hard to imagine route extensions that weren't  \na-series-of-successive-connection-extensions, like a row of dominos. The  \ndifference between route and conn is in proxy handling of optional extensions:  \nconn/opt means strip always but route/opt means strip if necessary, but not  \nby default. route/req is a superset of conn/req. I admit, I haven't thought  \nmuch about caching scenarios -- but keep in mind one of my goals for this is  \nto make PEP much simpler though this review cycle.\n\n>  Is 522 intended for gateways?  It sounds more like a 4xx\n>  response.  The reason-phrase is not sufficient to divine\n>  the purpose of each code.\n\n522 was imagined more on the basis of \"hmm... you bumped into a server  \nimplementation limit -- yes, the spec allows you to ask me to do 8192-bit  \nsignatures, but it's my fault I can't accept above 256-bits\" -- or perhaps  \nsomething more temporary. Basically: a 423 indicates parameters bad prima  \nfacie (a string in place of a number), and 522 indicates run-time errors.  \nAgain, though, I'm quite open to rethinking the error list.\n\n>  Please include the correct reference (and a URL is\n>  appropriate for drafts).\n\nThe urls and so on are linked from the HTML versions cited above.\n\nRohit\n---\nRohit Khare -- World Wide Web Consortium -- Technical Staff\nw: 617/253-5884  --   f: 617/258-5999   --  h: 617/491-5030\nNE43-354,  MIT LCS,  545 Tech Square,  Cambridge,  MA 02139\n\n\n\n", "id": "lists-010-8237012"}, {"subject": "Re: When to make objects uncacheable ", "content": "> Well, actually the current hit rate is zero.  Due to sizing\n> problems we have had to turn off caching a few weeks ago.  That's\n> why I found out about this list, and also why I wrote some code to\n> analyze 600MB of log files.  I'm looking to implement a multi-level\n> cache, distributed over three sites, but today I am just collecting\n> data to model the sizing issues.\n\nWe have a techreport:\n\nhttp://www.iet.unipi.it/~luigi/caching.ps.gz\n\nwhich discusses some issues in the relation between cache size and\nhit rate.\n\nActually, we have also done some simulations on the performance of\nhierarchical and/or cooperating caches; a report on that is not\navailable yet, but the bottom line is the following:\n\nN cooperating caches (a la Harvest) with disk capacity D each work\nalmost as a single cache with N*D disk space (for N not too large,\nlet's say N < 10).  The communication between caches is a small\nfraction of the overall traffic (because it only affects documents\nwhich are accessed more than once).\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n", "id": "lists-010-8249572"}, {"subject": "Re: Entity Tag", "content": "    Isn't there some confusion about the issue of file modification dates\n    as etags? The possibility that the file might be updated more quickly\n    than the clock resolution doesn't matter, as long as the server never\n    sends a date that is the same as 'now'. As long as the modification\n    date is in the past, then you can use that date as a validator; even\n    if the file gets modified 100 times in a nanosecond, it doesn't matter\n    since the new times will be different than the time you sent before.\n\nYes, this is correct.  If the server is using the modification time\nto create Etags which are encodings of that timestamp, then it is\npossible for the server to tag them as \"weak\" or not based on your\nalgorithm.\n\nThat's why I wrote in \n    http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q3/0340.html\nthis:\n    a file modification timestamp T of any resolution R is only \"weak\"\n    during the period R+|E| after the file has been modified, where E\n    is a bound on clock skew.  To make this more specific, suppose that\n    you know that\n    (1) file F was modified at time T\n    (2) the timestamp T has a resolution of R seconds\n    (3) the clock used to generate the timestamp T is at most E\n    seconds away from the clock used by the server to find out\n    the current time.\n    \n    Then, between time T and time T+R+|E|, you would have to generate\n    an entity tag of the form\n    W/\"encoding-of-T\"\n    but after time T+R+|E|, you could generate an entity tag of the form\n    \"encoding-of-T\"\n    because you are now sure that you have not given out that tag for\n    a previous version of the resource.\n\nsee that URL for more details, including the issue of locking.\n\nBut a separate question arose regarding the use of Last-Modified\ndates (i.e., the strings passed in a Last-Modified header, not\nthe opaque strings passed in an Etag header) in context where\na strong validator is required.  And in that case, the server has\nno way to tag the Last-Modified value as weak or strong, so the\nrules in 13.3.3 are (probably) the best that we can do.\n    \n-Jeff\n\n\n\n", "id": "lists-010-8259387"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "    The extended log format can be implemented in an afternoon with\n    little trouble provided the items logged are fixed. If you go for\n    allowing a choice of the items to log it gets trickier but not\n    much. \n\n    The pain for Xlog is for the analysis tool writers, not the server\n    writers :-)\n\nHow does your technique (with the server asking the proxy to transmit\na large log) work when there are multiple levels of proxy involved,\nsome of which may be hidden behind firewalls?\n\n-Jeff\n\n\n\n", "id": "lists-010-8268495"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": ">How does your technique (with the server asking the proxy to transmit\n>a large log) work when there are multiple levels of proxy involved,\n>some of which may be hidden behind firewalls?\n\nThere are two parts to the question, first how do multiple levels of\nproxy work, second how does the scheme work through firewalls.\n\nTo support multiple levels of proxy the proxy must be coded to support\npassthrough fetching of logs. That is the proxy must be bidirectional\nso that if a server requests a logfile from the outermost proxy it must\ninterrogate any inferiors which offered log file exchange.\n\nSupport for firewalls is more complex and in this case not automatic.\nUnless the firewall allows the log files to be fetched they cannot\ncross the firewall. This is the behavior I would expect a firewall to \nenforce. After all the purpose of the firewall may be explicitly to\nprevent leakage of such information. \n\nPhill\n\n\n\n", "id": "lists-010-8276737"}, {"subject": "Hyphens and under bar", "content": "In the charset registry of the 12 August draft SHIFT_JIS uses an \nunderbar. The separator in all the other sets is a hyphen. \n\nWhy the difference ?\n\n-------------------------------------\nTim Greenwood        Open Market Inc\n617 679 0320         greenwd@openmarket.com\n\n\n\n", "id": "lists-010-8285413"}, {"subject": "Re: New document on &quot;Simple hitmetering for HTTP&quot", "content": "At 10:31 PM 8/16/96 -0400, hallam@ai.mit.edu wrote:\n>The extended log format can be implemented in an afternoon with\n>little trouble provided the items logged are fixed. If you go for\n>allowing a choice of the items to log it gets trickier but not\n>much. \n\nUh, Speaking form personal experience, I'd like to dispute this.  Unless by\n'not much' you mean an order of magnitude.\n\n-----\nDaniel DuBois, Traveling Coderman -- NEW! http://www.spyglass.com/~ddubois/\n         Roses are red, violets are blue, this .sig doesn't rhyme.\n\n\n\n", "id": "lists-010-8292780"}, {"subject": "Re: Hyphens and under bar", "content": "Tim Greenwood wrote:\n> \n> In the charset registry of the 12 August draft SHIFT_JIS uses an\n> underbar. The separator in all the other sets is a hyphen.\n> \n> Why the difference ?\n\nThat's how it's spelled in IANA's charset registry:\n\n  ftp://ftp.isi.edu/in-notes/iana/assignments/character-sets\n\n\nErik\n\n\n\n", "id": "lists-010-8301055"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "Roy and I were having a debate about the proper specification\nof the Age header, about 6 weeks ago.  I dropped the ball on\nthis for various reasons, but Larry Masinter has asked us to\nsettle this issue.\n\nRoy argues that the current language in section 14.6,\n  If a cache receives a value larger than the largest positive integer it\n  can represent, or if any of its age calculations overflows, it MUST\n  transmit an Age header with a value of 2147483648 (2^31). HTTP/1.1\n  caches MUST send an Age header in every response. [...]\n\nshould be changed so that the last sentence there says:\n  [An] HTTP/1.1\n  [cache] MUST send an Age header field in any response obtained from\n  its own cache.\n\nHe argues that this is simply removing an ambiguity, because we\ncan't possibly mean that a cache must include the Age header when\nit is simply passing along a response that it has just received\nfrom an inbound server, because that would lead to an overestimation\nof the Age value:\n\n    What you are saying is that every step along the way should add the amount\n    of time it took to satisfy the request EVEN IF THE REQUEST CAME DIRECTLY\n    FROM THE ORIGIN.  That means that if the request passes through three\n    caches (A, B, C) with each segment taking (a, b, c, d) amount of time to\n    satisfy the request\n    \n UA  ------->  A  ------->  B  --------->  C  ------->  OS\na           b              c             d\n    \n    then the Age will be calculated as follows:\n    \n At  C:  age=d\n     B:  age=d+(c+d)\n     A:  age=d+(c+d)+(b+c+d)\n    UA:  age=d+(c+d)+(b+c+d)+(a+b+c+d)\n    \n    when we all know that the REAL age at UA must be less than (a+b+c+d).\n    Note that (a+b+c+d) will always be added by UA.\n\n    The wording that I submitted would have corrected that error in the\n    specification.  Fortunately, the existing wording of the spec is only\n    ambiguous -- we'll just have to interpret \"cache\" to mean that it only\n    takes effect on retrieval from the caching mechanism.\n\nRoy is wrong; this is not an ambiguity: the current spec language\nmeans what it says, and this is the right thing for it to say.\n\nRoy is right that, as written, the spec causes the reported Age\nvalue to be larger than the actual time since the response was\ngenerated at the origin server.  THIS IS GOOD.  THIS IS INTENTIONAL.\nMoreover, because we are not relying on mandatory clock synchronization,\nTHIS IS THE ONLY SAFE APPROACH.  The alternative is a specification\nwhich would increase the chances of underestimating the Age of a\nresponse.  Underestimation is SERIOUSLY BAD, because it will lead\nto a cache believing that a response is fresh when it is, in fact,\nstale.\n\nOverestimation of the Age can lead to a cache treating a fresh\nresponse as stale, which can cause extra revalidation messages.\nThis is somewhat inefficient, but will never lead to a client\ninadvertently seeing an expired cache entry.  Underestimation\nis thus a much worse error than overestimation, and so the \nspec is designed to avoid underestimation as assiduously as\npossible.\n\nNow let's look at how badly the specified behavior can overestimate\nAge.  The algorithm in 13.2.3 allows (implicitly) the receiving\ncache to calculate the retrieval delay based on when the beginning\nof the response is received; it doesn't have to wait for the entire\nresponse.  Therefore, the magnitude of this delay is several RTTs\nthrough each hop of the network.  (It would be just one RTT if the\nconnections are already open.)  Roy's formulas:\n\n At  C:  age=d\n     B:  age=d+(c+d)\n     A:  age=d+(c+d)+(b+c+d)\n    UA:  age=d+(c+d)+(b+c+d)+(a+b+c+d)\n\ngeneralize to\nAge = Mean_RTT * N * (N + 1)/2\nfor N hops (N - 1 proxies)\n\nThe \"correct\" Age would be Mean_RTT * N, so the size of the overestimate\n(the \"error) is\nExcess_age = Mean_RTT * N * (N - 1)/2\n\nHere are some sample values for a Mean_RTT of 1 second (which\nis a relatively high value):\n\nNumber of proxiesNExcess_age (seconds)\n\n010\n121\n233\n346\n4510\n5615\n6721\n\nOK, so if the request chain includes 6 or more proxies, the overestimate\njust might start to change the caching behavior for responses with\nunusually short maximum ages.  (I'd be surprised if people sent\nmax-age values under 1 minute, but perhaps someone can provide a\ncounterexample).  Bottom line: this \"error\" is not really worth\ngetting excited about.\n\nLet's now look at what kinds of errors (underestimations of Age)\ncould arise if we followed Roy's proposal: a proxy only sends an\nAge header if the response comes from its own cache.\n\nConsider this not-very-contrived request-chain:\n\n   client C => HTTP/1.1 proxy P1 => HTTP/1.0 proxy P0 => Origin server S\n\nNow let us suppose that client C makes a request for resource\nhttp://S/foo.html, and that proxy P0 has had a copy of this\nresource in its cache for, say, 30 minutes.  Let's also suppose\nthat the clock on C (perhaps someone's PC) was set accidentally\nto the wrong time zone, and it's an hour slow.\n\nSo when the response arrives at C, under Roy's proposal, there\nis no Age header attached and so the client starts the Age\nticker running at that point.  I.e., the client will underestimate\nthe Age by the 30 minutes that the response was sitting in the\ncache at P0.  This might well exceed the max-age value sent by\nthe origin server, but C would be ignorant of the expiration\nof its cache entry for perhaps a significant amount of time.\n\nUnder the specification as it currently exists, P1 (the first\nHTTP/1.1 cache to see the response) would construct an Age\nvalue.  If its own clock is set wrong, we are no better off.\nBut if its clock is correct, then it will observe the discrepancy\nbetween its current clock and the Date value on the response,\nand will set the Age accordingly (to 30 minutes).\n\nThus, the specification as written will prevent Age underestimation\nresulting from end-client clock skew as long as at least one of\nthe HTTP/1.1 proxies in the chain has a nearly-correct clock.\n\nSummary: the specification, as written, does somewhat overestimate\nthe Age, but not by a tremendous amount, and is intended to reduce\nas much as possible the probability of inadvertently delivering a\nstale response to a user.  Roy's proposed change would give slightly\nmore accurate Age estimates, but could cause the undected delivery\nof stale responses in the presence of clock skew.\n\n-Jeff\n\n\n\n", "id": "lists-010-8308220"}, {"subject": "Re: (revised) HTTP working group statu", "content": ">  1) closing in December\n>  2) closing when transparent negotiation and PEP have\n>     proposed status \n\nAs long as we're making progress, we can continue working.\nBut if we can't finish the items in process by December, I'd seriously\nquestion whether we're \"making progress\".\n\nI'd like to encourage the authors of the current drafts to respond to\ncomments and release new drafts QUICKLY: turnaround measured in small\nnumbers of days rather than in weeks is really important.\n\nI know people have other jobs and other things to do, but just think:\nthis will be over soon.  Let's finish.\n\nLarry\n\n\n\n", "id": "lists-010-8322166"}, {"subject": "Feature tag registratio", "content": "The current content negotiation draft does not completely specify the\nfeature tag registration process, because when I wrote it, I did not\nknow what needed to be in such a specification.  Reading RFC 1602 (The\nInternet Standards Process -- Revision 2) and RFC 1341 (MIME\n(Multipurpose Internet Mail Extensions)) as an example, I gather that\nsuch a specification can consist of\n\n 1) a discussion of the purpose of feature tags in the content\n    negotiation draft\n 2) a form to register feature tags\n\nQuoting from RFC 1602:\n\n      Many protocol specifications include numbers, keywords, and other\n      parameters that must be uniquely assigned.  Examples include\n      version numbers, protocol numbers, port numbers, and MIB numbers.\n      The IAB has delegated to the Internet Assigned Numbers Authority\n      (IANA) the task of assigning such protocol parameters for the\n      Internet.\n      [....]\n      The IANA is expected to avoid frivolous assignments and to\n      distinguish different assignments uniquely.  The IANA accomplishes\n      both goals by requiring a technical description of each protocol\n      or service to which a value is to be assigned.  Judgment on the\n      adequacy of the description resides with the IANA.  In the case of\n      a standards track or Experimental protocol, the corresponding\n      technical specifications provide the required documentation for\n      IANA.\n\nIf I understand the above correctly, this WG will not have to specify\nany rules to distinguish `frivolous assignments' from real\nassignments, IANA will infer these rules from the conneg spec.\nFeature tag registration is intended as a very open process, and this\nhas some disadvantages, but it seems that IANA will act as a filter to\nstop weirdness like we have seen in the domain name registration rush\nof the last 2 years.\n\nBased on my current understanding of how IANA works, I propose to\nchange the start of Section 8 of the current draft to the following\ntext.\n\n------snip------\n\n8  Feature tag registration\n\n   Feature tags are registered with IANA using the form in Appendix\n   [TBA].  Feature negotiation intends to allow for the registration\n   of a feature tag before an area of negotiation is standardized or\n   even well-understood.  A requirement for registration is that the\n   tag name follows the syntax rules, and that a definition of the\n   meaning of the tag is supplied.  Registration will not require\n   actual implementation of a feature, and there will be no test on\n   whether the feature definition overlaps (partially) with another\n   feature definition.\n\n---snip---\n\nI don't see a great need to put some kind of `how to name your\nfeature' guidelines in the draft.  The collection of feature names in\na `core feature set' can act as a style guide.\n\nGetting the nature of the registration process right is crucial to the\nsuccess of feature negotiation.\n\nThe MIME type system has failed to deliver the kind of fine-grained\ninformation we need for the web not just because of technical\nlimitations, but also because the MIME type registration process was\ndesigned to resist expressing such fine-grained information as a MIME\ntype parameters.\n\nThe form below is at least as important as the technical details of\nfeature negotiation.  Many people who register feature tags will only\nread the form below, they will not read the conneg spec itself.\n\nMaking a good form is an exercise in applied psychology more than\nanything else.  For example, the use of BNF instead of plain English\nin the syntax instructions means to get the user into a\ntechnical/scientific mindset rather than a marketing-speak or `the\nbright idea I had this morning' mindset.  As a non-native speaker, I\nam not particularly qualified to play such language games, so please\nlook very carefully at the form and mail any improvements you can\nthink of.\n\nNote that in the end, this form will probably have a web-version and\nan e-mail version.  Imagine <input> and <textarea> elements below.\n\n---snip---\n\nAppendix [TBA]:\n\nFEATURE TAG REGISTRATION FORM\n\n | Instructions are preceded by `|'.\n\nFeature tag name:\n\n\n | The name is case-insensitive, may not start with \"x-\", and must fit\n | the ftag syntax rules:\n |\n |     ftag = 1*<any CHAR except CTLs or tspecials or \"!\">\n |\n |     tspecials      = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n |                    | \",\" | \";\" | \":\" | \"\\\" | <\">\n |                    | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n |                    | \"{\" | \"}\" | SP | HT\n |\n | Example: html_5dtables\n\nSummary:                                           \n\n\n | Supply one or two sentences.\n |\n | Example: The html 5 dimensional tables tag indicates the capability\n | to render 5 dimensional tables in text/html documents.\n\nKeywords:                                                       \n\n\n | Optional.\n\nIntends to label: [ ] a capability\n                  [ ] a preference\n                  [ ] a capability or preference\n\n | Check one.\n\nIntends to be used as: [ ] tag without a value\n                       [ ] tag with a single numeric value\n                       [ ] tag with a single value \n                       [ ] tag with one or more values\n\n | Check one or many.\n\nPresence of the tag indicates:\n\n\nTag value (if any) indicates:\n\n\nPredefined tag values (if any):                              \n\n\n | Tag values must fit the HTML/1.1 syntax rule for tokens:\n |\n |     token = 1*<any CHAR except CTLs or tspecials>\n\nAbsence (default case) of the tag indicates:                \n\n\n | Optional.\n\nIntended typical use:\n\n\n | Optional.\n |\n | Supply examples of Accept-Features headers, variant\n | descriptions, and/or variant lists.  Add comments if\n | necessary.\n\nDetailed description of indicated capability or preference:  [optional]\n\n\n | If more than 100 lines are needed, a reference to a related\n | standard or document is preferred.\n\nRelated standards or documents:\n\n\n | Optional.\n\nRelated media types (MIME types):\n\n\n | Optional.\n |\n | For example text/html if the tag indicates the capability\n | to handle some HTML extension.\n\nRelated markup tags:\n\n\n | Optional.\n |\n | For example <table>.  Note that the markup language does not\n | need to be text/html.  Add comments if necessary.\n\nRelated feature tags:\n\n\n | Optional.\n |\n | Add comments if necessary.\n\nNotes:                                                       \n\n\n | Optional.\n\nSee also:\n\n\n | Optional.\n\nRegistered on:                          \n\n\n | Date will be supplied by IANA.\n\nPerson & email address to contact for further information:\n\n\n--snip--\n\nKoen.\n\n\n\n", "id": "lists-010-8330170"}, {"subject": "HTTP/1.1, queries, and redirec", "content": "Suppose a search service moves from MachineA to MachineB.  MachineA\nredirects the requests.  Should MachineA affix the query string to\nthe Location header?  That is, if MachineA receives URL\n/search?query\nshould it return a header\nLocation: http://MachineB/search\nor\nLocation: http://MachineB/search?query\n?\n\nThe specification gives no guidance.  Empirically, user agents just go\nto wherever the Location header says to go.  So if the query string is\nomitted, the search doesn't happen at MachineB, because the user agent\ndoesn't concatenate the query string.  (And it probably shouldn't, in\ntruth.)\n\nMy server will append the query string on redirects that arise from the\nserver's configuration.  (CGIs can return whatever.)  What do others do?\n\nDave\n\n\n\n", "id": "lists-010-8343973"}, {"subject": "Re: HTTP/1.1, queries, and redirec", "content": "I'm not sure why this is even a question in your mind; of course the\nredirect should include the full URL that the old URL should be\nredirected to, no matter whether it is a query or non-query.\n\n\n\n", "id": "lists-010-8351309"}, {"subject": "Re: Feature tag registratio", "content": "Koen,\n\nMIME registration procedures are being updated. You should look at the\nappropriate Internet Drafts; I sent around the pointers a while ago,\nand can dig them up again.\n\nLarry\n\n\n\n", "id": "lists-010-8358522"}, {"subject": "Re: draft-ietf-http-pep0", "content": ">>  If it does [affect the body], that must be indicated via\n>>  a change in one of:  Content-Type, Content-Encoding, or\n>>  Transfer-Encoding, depending on how it is intended to be\n>>  interpreted.  There are many possibilities in how to\n>>  indicate such a change, but something must appear in one\n>>  of those fields.\n> \n> Must? There are lines that should be drawn around MIME influence in this  \n> protocol. I have an open mind, all the way up to forbidding body-rewriting  \n> (see above), but HTTP is a sovereign protocol which can define its own  \n> extensions -- extensions which cannot be represented for MIME, anyway. Of  \n> course, I can see that the MIME community might want to adopt parts of PEP,  \n> too...\n\nI am not referring to MIME -- I am referring to section 7.2.1 of HTTP/1.1,\nwhich is quite explicit about how to determine the data type transferred.\n\n> Another way to placate this need is to put in a blanket \"pep\" C-E: indicating\n> that critical content modification was done by pep extensions.\n\nYes, that is one way to do it, assuming you don't allow extensions to be\ninterleaved with normal encodings.\n\n>> [references to grammar rules]\n> \n> Roy! It says right above that section: these rules are copied from 2.2 of  \n> HTTP/1.1. Grant the reader a *little* mercy in tracing back every reference in\n> a spec... In any case, the spirit of this section is to normatively refer to  \n> 1.1.\n\nI saw it -- it is a C-style comment (odd in itself) above the definition\nof bag, which BTW was removed from the HTTP/1.1 spec.  The reason I mention it\nis that other specs (e.g. MIME) only define a BNF item when they intend\nto change it, so that only those aspects of the main protocol (HTTP in our\ncase, 822 for MIME) are highlighted by the updating spec.  Therefore, I think\nit is less readable to define things that you don't intend to change.\n\n>>  Umm, what about a scope of \"all\"?  PEP cannot be used to\n>>  express many cache-related extensions without the notion\n>>  of a requirement that is applicable to all recipients.\n>>  The same applies to some encryption extensions.  I know\n>>  you had it before -- what happened to it?\n> \n> The \"route\" scope used to apply to all agents that handled a message; when  \n> some of us gamed it out, it was hard to imagine route extensions that weren't\n> a-series-of-successive-connection-extensions, like a row of dominos. The  \n\nThe difference is that route placed a requirement on successive connections,\nwhereas conn only places the requirement on this connection.  If the\nrequirement has anything to do with security or robustness of the transport\npath, then conn just isn't sufficient to express the requirement.\n\n> 522 was imagined more on the basis of \"hmm... you bumped into a server  \n> implementation limit -- yes, the spec allows you to ask me to do 8192-bit  \n> signatures, but it's my fault I can't accept above 256-bits\" -- or perhaps  \n> something more temporary. Basically: a 423 indicates parameters bad prima  \n> facie (a string in place of a number), and 522 indicates run-time errors.  \n\nOkay -- it just needs the explanation then.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-8366399"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "> Roy is right that, as written, the spec causes the reported Age\n> value to be larger than the actual time since the response was\n> generated at the origin server.  THIS IS GOOD.  THIS IS INTENTIONAL.\n> Moreover, because we are not relying on mandatory clock synchronization,\n> THIS IS THE ONLY SAFE APPROACH.  The alternative is a specification\n> which would increase the chances of underestimating the Age of a\n> response.  Underestimation is SERIOUSLY BAD, because it will lead\n> to a cache believing that a response is fresh when it is, in fact,\n> stale.\n> \n> Overestimation of the Age can lead to a cache treating a fresh\n> response as stale, which can cause extra revalidation messages.\n> This is somewhat inefficient, but will never lead to a client\n> inadvertently seeing an expired cache entry.  Underestimation\n> is thus a much worse error than overestimation, and so the \n> spec is designed to avoid underestimation as assiduously as\n> possible.\n\nFirst off, this just isn't an accurate comparison of \"safe\",\nwhich by definition is the potential for harm (to people or property)\n[Leveson's paper on Software Safety has the complete definition].\nIt is absolutely and provably false that overestimation is safe.\nOverestimation creates the potential for cache misses where there should\nbe cache hits, and cache misses result in additional network requests,\nthat in turn cost some people real money and have the potential to\ncompletely block an already congested network at periods of peak usage\n(as witnessed with the USA-UK link last year).\n\nIn contrast, there is no direct correlation between presenting a stale\nentity to a user and causing harm. Any entity that is marked as cachable\nwill not be dependent on the user ALWAYS seeing it when fresh -- servers\ncannot make such critical entities cachable unless they know that there\nare no older caches in the loop, and your rationale is only applicable\nwhen there are older caches in the loop.\n\n> Now let's look at how badly the specified behavior can overestimate\n> Age.  The algorithm in 13.2.3 allows (implicitly) the receiving\n> cache to calculate the retrieval delay based on when the beginning\n> of the response is received; it doesn't have to wait for the entire\n> response.  Therefore, the magnitude of this delay is several RTTs\n> through each hop of the network.  (It would be just one RTT if the\n> connections are already open.)  Roy's formulas:\n> \n>  At  C:  age=d\n>      B:  age=d+(c+d)\n>      A:  age=d+(c+d)+(b+c+d)\n>     UA:  age=d+(c+d)+(b+c+d)+(a+b+c+d)\n> \n> generalize to\n> Age = Mean_RTT * N * (N + 1)/2\n> for N hops (N - 1 proxies)\n> \n> The \"correct\" Age would be Mean_RTT * N, so the size of the overestimate\n> (the \"error) is\n> Excess_age = Mean_RTT * N * (N - 1)/2\n> \n> Here are some sample values for a Mean_RTT of 1 second (which\n> is a relatively high value):\n> \n> Number of proxiesNExcess_age (seconds)\n> \n> 010\n> 121\n> 233\n> 346\n> 4510\n> 5615\n> 6721\n> \n> OK, so if the request chain includes 6 or more proxies, the overestimate\n> just might start to change the caching behavior for responses with\n> unusually short maximum ages.  (I'd be surprised if people sent\n> max-age values under 1 minute, but perhaps someone can provide a\n> counterexample).  Bottom line: this \"error\" is not really worth\n> getting excited about.\n\nBut you are completely overlooking what happens if ANY of the intermediaries\nhas a system clock which is out-of-sync with the origin.  If ANY of them\ndo have a bad clock, they will pass the additional bogus age on to every\nsingle request that passes through the proxy.  This bogus age will then\naffect the cache algorithms on the entire network of applications beneath\nthat proxy.  In other words, it completely ruins the purpose of Age which\nis to provide a reliable estimate regardless of clock synchronization.\n\n> Let's now look at what kinds of errors (underestimations of Age)\n> could arise if we followed Roy's proposal: a proxy only sends an\n> Age header if the response comes from its own cache.\n> \n> Consider this not-very-contrived request-chain:\n> \n>    client C => HTTP/1.1 proxy P1 => HTTP/1.0 proxy P0 => Origin server S\n> \n> Now let us suppose that client C makes a request for resource\n> http://S/foo.html, and that proxy P0 has had a copy of this\n> resource in its cache for, say, 30 minutes.  Let's also suppose\n> that the clock on C (perhaps someone's PC) was set accidentally\n> to the wrong time zone, and it's an hour slow.\n> \n> So when the response arrives at C, under Roy's proposal, there\n> is no Age header attached and so the client starts the Age\n> ticker running at that point.  I.e., the client will underestimate\n> the Age by the 30 minutes that the response was sitting in the\n> cache at P0.  This might well exceed the max-age value sent by\n> the origin server, but C would be ignorant of the expiration\n> of its cache entry for perhaps a significant amount of time.\n\nYes, there is a possibility that if one or more HTTP/1.0 caches\nexist in the chain AND the response came from one of those HTTP/1.0\ncaches in which it resided for some time AND the user agent's system\nclock is running behind the origin server's clock, that the user might\nview a stale (not necessarily invalid) entity and not know that it is\nstale for a period of time not exceeding the difference between the two\nclocks.  However,\n\n  1) this only affects one user\n  2) it is rarely true that viewing a stale document is \"unsafe\",\n     because people controlling safety-critical documents NEVER allow caching\n  3) it can be fixed by resetting the clock\n  4) it can only occur while HTTP/1.0 caches are still in the loop\n  5) large caches will be among the first to upgrade to HTTP/1.1\n\nIn other words, there is a slight possibility that there may result in\nsome staleness, but nothing compared to the other problems we live with\nwhen any old cache is present, and never occurring once people upgrade.\n\nContrast the above with this example\n\n    client C => HTTP/1.1 proxy P1 => HTTP/1.1 proxy P0 => Origin server S\n\nand someone mistakenly sets the date on P0 incorrectly.  These are all\nHTTP/1.1 applications and C, P1, and S all have exactly synchronized\nclocks, so there is no excuse for them to fail to operate as well as\nwe can design the protocol to allow them to operate in this situation.\n\nUnder Jeff's interpretation of the Age calculation, any message passing\nthrough proxy P0 will have X added to its age, where X is the amount of\ntime forward that P0 is running relative to S.  For times of a few seconds,\nthe problem will be negligible (it would just be added to the other error\nseconds that Jeff's calculation already adds to any response).  However,\nif the time is significant (say 24 hours), then every message passing\nthrough P0 is instantly aged by 24 hours, which in turn would cause most\nHTTP/1.1 content to be considered stale as soon as it arrives.  Thus,\nwith one mistake, an entire system of caches ceases to exist (they\nbehave like cacheless proxies) and will likely result in catastrophic\nnetwork failure (affecting not only HTTP traffic, but all network traffic).\nIn the case where C is my Aunt Nell, P1 is the Waikato regional proxy,\nand P0 is the North Island centre, we just designed a network failure\nfor the entire North Island of New Zealand.  Is that safe?  What do I tell\nmy Aunt when she receives the bill for unusual network usage?\n\nUnder my interpretation of the Age calculation, where the Age header\nfield is only added/increased when a response is retrieved from that\napplication's own cache, the only application affected by P0's clock\nfailure is P0 (it becomes a cacheless proxy until somebody fixes the\nbloody clock).  The other apps will lose the benefit of P0's cache,\nbut their own caches will remain unaffected. This is, after all, why\nwe invented Age instead of just relying on the cache and origin clocks\nbeing in sync.\n\nAND, all this is in addition to having an age calculation which doesn't\nadd superfluous age due to double-counting every connection round trip.\n\n> Summary: the specification, as written, does somewhat overestimate\n> the Age, but not by a tremendous amount, and is intended to reduce\n> as much as possible the probability of inadvertently delivering a\n> stale response to a user.  Roy's proposed change would give slightly\n> more accurate Age estimates, but could cause the undected delivery\n> of stale responses in the presence of clock skew.\n\nMy summary is that the spec of sections 13.2.3 and 14.6 is not only\nwrong (produces the wrong Age value on every request), but also has\nthe probability of causing catastrophic network failure for those\nnetworks which are becoming increasingly dependent on cache efficiency.\nMoreover, the overestimating algorithm is completely unnecessary if\nthere are no HTTP/1.0 caches in the loop, and that is our GOAL, right?\nWhy purposefully introduce a dangerous and obviously broken algorithm\nto account for a potential staleness that will cease to exist before\nanyone even begins using caching for critical applications?\n\nThat's it for my discussion -- it is now clear that we will need a new\ndraft that fixes this problem and the others already mentioned on and\noff the list.  I will be in Seattle (sister's wedding) til Monday, so\nplease forgive me if there are any further questions.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-8377414"}, {"subject": "Re: Problems with draft-ietf-http-v11-spec0", "content": "> Replace\n>        field-content  = <the OCTETs making up the field-value\n>                         and consisting of either *TEXT or combinations\n>                         of token, tspecials, and quoted-string>\n> with\n>        field-content  = <the OCTETs making up the field-value,\n>                         either as defined by more specific rules\n>                         (see 14, 19.6.2) or *TEXT for unrecognized\n>                         message headers>\n> Reason:\n> The former is unnecessary specific; in fact it seems exactly equivalent\n> to\n>        field-content  = *(*TEXT | *(token | tspecial | quoted-string) )\n> which is apparently not the intention.\n\nAgreed.  However, I think the more appropriate fix would be [in Section 4.2]\nto remove field-content (there is no other reference to it) and replace\nit with a more accurate definition of field-value, as in\n==========\n\n       message-header = field-name \":\" *LWS [ field-value ] *LWS CRLF\n\n       field-name     = token\n       field-value    = <*OCTET, up to but not including the next line\n                         which does not begin with SP or HT>\n\n   Additional requirements may apply to the syntax of each field-value\n   according to the definition of the associated field-name.  The common\n   HTTP/1.1 header fields are defined in section 14.\n\n   A message-header field with no field-value is considered to have an\n   empty or null value.  A header field which does not occur at all as a\n   message-header of the message is considered to have no value (a value\n   which is undefined) for that message. Depending on the definition of\n   a field's semantics, an empty value may or may not be equivalent to\n   an undefined value for that field.\n\n   The order in which header fields ...\n==========\n\n> Also add after the first paragraph of 3.2, before 3.2.1:\n> \n>   Whitespace is not allowed anywhere within an URI.  In other words,\n>   the last paragraph of 2.1 (implied *LWS) does not apply to the BNF\n>   rules in 3.2.1 and 3.2.2.\n\nI'm not sure if this is wise -- we may wish to allow URI-only header\nfields (Location, Content-Location, Content-Base) to be split over\nmore than one line for use in mail (i.e., ignore any whitespace),\nand thus we might be better off with just\n\n   No whitespace is allowed in the Request-URI.\n\nin section 5.1.2.\n\n> In 3.11 Entity Tags, add a sentence somewhere 'Whitspace is not\n> allowed between the \"W/\" prefix and the opaque-tag' (if you think\n> it should apply).\n\nYes.\n\n> In 9.2 [OPTIONS], make the last sentence (\"If the OPTIONS request passes\n> through a proxy,...\") a separate paragraph, to make clear that it\n> applies to both preceding pragraphs (which handle \"*\" and not-\"*\",\n> respectively).\n\nYes, good catch.  We should also change the second paragraph to say\n\n   Unless the server's response is an error, the response MUST NOT include\n   an entity, but MAY include those entity-header fields which describe\n   methods, restrictions, or requirements for access to the resource\n   (e.g., Allow is appropriate, but Content-Type is not). Responses to\n   this method are not cachable.\n \nsince the existing wording seems to be confusing people.\n\n> 11 Access Authentication, 4th paragraph:\n> A user agent that wishes to authenticate itself with a server--usually,\n> but not necessarily, after receiving a 401 or 411 response--MAY do so by\n>                                               ^^^\n> The 411 seems to be in error here.\n\nYes, that is an error, and it must be a year old.  Crikey.\n\n>> > Comments (within parentheses) should probably allowed in more\n>> > places - at least, in 19.4.7\n>> >        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT\n>> > should probably be\n>> >        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT *comment\n\nOh, okay.\n\n> Another place where trailing comments could be allowed would be the \n> HTTP From header.  But I don't know how common that is in HTTP/1.0.\n\nNo need -- comments are already allowed in the mailbox (from RFC 822).\nAt least I think that's the case (I can't see my copy of 822 right now).\n\n>> > 10.3.6 305 Use Proxy\n>> > \n>> > The requested resource MUST be accessed through the proxy given by the\n>> > Location field. The Location field gives the URL of the proxy. The\n>> > recipient is expected to repeat the request via the proxy.\n>> > \n>> > How exactly does is a proxy \"given\" by a Location field?\n>> > Location normally contains an URI, and URIs point to resources but\n>> > not (normally) applications (the proxy).  Does the URI have to be\n>> > a http_URL, does the abs_path have to be empty (or is it required\n>> > to be \"/\"), and what if not?\n>> \n>> On the contrary, proxies are normally identified by URL.  The URL\n>> does not need to be an http URL (though it would be in current practice)\n>> and the interpretation of the path (if any) would be dependent on\n>> the method of proxying (http would not use any path).\n> \n> I don't understand what you mean with \"normally\".  ...\n\nIt is how they were defined (in environment vars) for all WWW clients\nprior to Netscape [and any that still use libwww or libwww-perl].\n\n> I understand that you want to keep the spec open for future extensions,\n> but since this draft defines (a version of) HTTP, it should at least\n> define the use of this header with http_URLs.  Otherwise this response \n> code will be useless (different implementations use it differently,\n> or nobody uses it), and it could just as well be defined as\n> \"This code is reserved for future use\" (like 402).\n\nI felt it was self-evident -- you just demonstrated that to be wrong.\nWe can either change it to \"This code is reserved for future use\" now\nor someone can come up with a complete, yet not too restrictive, definition\nof 305 and (as you suggested) Proxy-Location before the editor decides\nto generate a new draft.  I won't be able to think about it til Tuesday.\nI do consider it *very* important that Proxy-Location not be restricted\nto http URLs.\n\n> [1] I don't understand why the spec is so lenient here.  Shouldn't\n> all HTTP/1.1 implementations be REQUIRED to recognize all the \n> response codes in this spec?  There are several \"client MUST\"s\n> in other parts for specific response codes, and it's not clear \n> whether \"not understanding\" a response code lets a client get away\n> with not following its MUSTs.  This includes 305.\n\nYes, the leniency is leftover from HTTP/1.0.  We should add\n\n    HTTP/1.1 clients MUST recognize all of the status codes defined\n    by this specification (above).\n\nto the end of the last paragraph in section 6.1.1.\n\n>> > A remark regarding 14.1 Accept:\n>> > It's a pity there is a \"q=\", but not a \"mxb=\".  An oversight or\n>> > intentional?\n>> \n>> The conneg group decided it \"wasn't needed\" based on the observation\n>> that browsers didn't implement it.\n> \n> There is a way in the latest lynx code to specify it via the .mailcap\n> file, and I understand that the Apache server can make use of it.\n\nYou should ask Larry if the issue can be reopened and mxb restored.\nI personally feel it was a mistake to remove it, and since we have\nalready missed the boat on getting HTTP/1.1 implemented in the most\nrecent wave of browsers, we might as well get it right.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-8395446"}, {"subject": "Re: Problems with draft-ietf-http-v11-spec0", "content": "Roy T. Fielding:\n>\n>>> > A remark regarding 14.1 Accept:\n>>> > It's a pity there is a \"q=\", but not a \"mxb=\".  An oversight or\n>>> > intentional?\n>>> \n>>> The conneg group decided it \"wasn't needed\" based on the observation\n>>> that browsers didn't implement it.\n>> \n>> There is a way in the latest lynx code to specify it via the .mailcap\n>> file, and I understand that the Apache server can make use of it.\n>\n>You should ask Larry if the issue can be reopened and mxb restored.\n>I personally feel it was a mistake to remove it, and since we have\n>already missed the boat on getting HTTP/1.1 implemented in the most\n>recent wave of browsers, we might as well get it right.\n\nIf we want to do it *right*, we should not re-introduce mxb, but\nintroduce an Accept-Length header.  \n\nI did not just remove mxb because it was unused, I removed it because\nit was was the kind of unused cruft that brings a protocol much closer\nto the point at which no extension is possible anymore because of\ninterference effects.\n\nPersonally, I think people vastly overestimate the good a `don't ever\nsend me something longer than X bytes' mechanism can do: if this\nactually was useful, browsers would have `abort all transfers longer\nthan X bytes' configuration options by now.\n\nBut if we are going to have such a mechanism, we at least need to make\nit orthogonal to the other negotiation headers.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-8411525"}, {"subject": "Re: Problems with draft-ietf-http-v11-spec0", "content": "This is an automatic reply.  Feel free to send additional\nmail, as only this one notice will be generated.  The following\nis a prerecorded message, sent for fielding\n\n\nI am off to Seattle for the weekend (for my sister's wedding)\nand will return Monday afternoon.  I expect no e-mail access\nfor the duration, and my fingers are *real* happy about that.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-8421038"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "Roy T. Fielding:\n[...]\n>My summary is that the spec of sections 13.2.3 and 14.6 is not only\n>wrong (produces the wrong Age value on every request), but also has\n>the probability of causing catastrophic network failure for those\n>networks which are becoming increasingly dependent on cache\n>efficiency.\n\nI too think that Jeff goes way to far in never trying to underestimate\nthe age, and that there could be problems because of this.\n\nMy main concern is that with the `every proxy cache must add an Age\nheader' interpretation, first-hand responses with max-age=0 or some\nother small max-age value may have been marked as aged and gotten a\nwarning:stale header by the time they have passed a few 1.1 proxy\ncaches.  This is extremely bad because\n\n  a) the next proxy might retry the request to get a fresh response,\n     and might even go into an infinite retry-loop when it keeps\n     getting responses with Age: 1.\n\n  b) the user will get a big *warning:stale* message from the browser\n     while in fact nothing is wrong (the response is firsthand, so it\n     is as fresh as it can be).  Such warnings might drown out the\n     actual warnings when responses *are* stale, might cause lots of\n     reload-pressing, might cause service authors to never use\n     max-age=0, etc.\n\nThere may be ways to carefully code around both problems, but all in\nall, I think that Jeff's interpretation leads to a very brittle\nsystem, in which `obvious' mechanisms added for robustness could\neasily cause catastrophic failure when deployed in a chain of proxies.\n\nI exchanged e-mail with Jeff and Jim about this before the 06 draft\nwas finished, and this led to Jeff writing the current ambiguous\nsentence, which I interpreted at the time as Roy does now: `must add\nan Age header only when serving a response from cache memory'.\n\nMy proposal to Jeff at the time was something like the following Age\nheader rule:\n\n - MUST add Age when serving from cache memory\n - MAY add Age when relaying a response from a 1.0 proxy\n - MUST NOT add Age when relaying a response from a 1.1 or higher proxy\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n", "id": "lists-010-8428541"}, {"subject": "Re: Feature tag registratio", "content": "Larry Masinter:\n>\n>Koen,\n>\n>MIME registration procedures are being updated. You should look at the\n>appropriate Internet Drafts; I sent around the pointers a while ago,\n>and can dig them up again.\n\nI know that the MIME procedures are being updated.  My comment on MIME\nwas about the current MIME situation, I did mean to imply that the\nfuture could not be better.\n\nIf your knowledge of recent MIME developments lets you infer problems\nwith the the proposed feature tag registration procedure, please post\ncorrections for the feature tag registration procedure to the list.\n\n>Larry\n\nKoen.\n\n\n\n", "id": "lists-010-8438601"}, {"subject": "Re: draft-ietf-http-pep0", "content": "A few comments:\n\n1. You might comment upon ordering issues when there are multiple \nProtocol-Info, Protocol-Request, Protocol-Query, and Protocol headers contained \nin the same message header.  For example, a JEPI server will likely do UPP \noperations 2 and 3 in the same message:\n\n   Protocol-Info:  {globeid ... {for \\pay}}  {CyberCash ... {for \\pay}}\n   Protocol-Request: {upp {str req} {for \\pay}\n\nFor UPP to work properly, the Protocol-Request has to be acted-upon after the \nProtocol-Info is stored.  This seems very natural given the semantics of \nProtocol-Info versus Protocol-Request.  I think the Internet Draft should make \nthis more explicit.\n\n2. Would it be useful to have Protocol explicitly express whether a Protocol \nwas done by a sender in response to a Request?  To put it another way, might \nthere be a \"{str resp}\" indicating that this Protocol line was inserted (and \nthe corresponding protocol run) in response to a request?  This would \ndifferentiate a Protocol line done ad-hoc by a sender.  I think this might make \nthe protocol more robust and help with problem diagnosis and error recovery.\n\n3. I (still) wish there were a matrix discussing the meaning of each parameter \n(str, scope, via, for, params) for each type of header (Protocol, \nProtocol-Request, Protocol-Info, and Protocol-Query).  I think some of the \ncells may be meaningless, and others may have ambiguous meanings.  I think the \nexercise of filling in such a matrix may bring clarity to this proposal.\n\n4. There used to be an \"implementation model\" in an appendix.  I hope you put \nit back in and expand upon it.\n\n5. In verbal discussions, you've made it clear that \"for\" is basically just a \nhint which implementations are (mostly) free to ignore.  You should explain \nthis concept somewhere.\n\n6. A comment about exposition: I suggest that the section on PEP Syntax in part \n3.5 be moved before the rest of part 3.  I think many readers don't appreciate \nforward references when reviewing this kind of document.  The discussion of PEP \nUsage at the start of part 3 is basically meaningless until you've read part \n3.5.  Also, I think you should greatly discuss the explanations of the reserved \ntop-level bags to make the semantics of each clearer.\n\n\n\n", "id": "lists-010-8446433"}, {"subject": "Re: draft-ietf-http-pep0", "content": "Rohit,\n\nOn a general note, I would like to have some information on how PEP\nwould work with the current state management proposals.  In\nparticular, I can see a fair number of situations in which sites would\nwant to use cookies to keep track of which clients had already\nnegotiated certain protocol extensions.  There has already been a fair\namount of discussion about how cookies and caches will interact; will\nthose interactions stay the same if part of the state being managed\nrelates to protocol extensions?\n\n\nMark writes: \n>3. I (still) wish there were a matrix discussing the\n>meaning of each parameter (str, scope, via, for, params) for each\n>type of header (Protocol, Protocol-Request, Protocol-Info, and\n>Protocol-Query).  I think some of the cells may be meaningless, and\n>others may have ambiguous meanings.  I think the exercise of filling\n>in such a matrix may bring clarity to this proposal.\n\n\nI, too, would like to see us develop a matrix, but a broader one,\nabout when we can expect to use which methods for what negotiations.\nMy current understanding is:\n\nUpgrade: Used to change versions of \"vanilla\" HTTP .\n\nTransparent Negotiation: Used to negotiate resource alternatives,\nbased on \"vanilla\" HTTP headers.\n\nPEP: Used to negotiate custom extensions to HTTP; these extensions\nmay transform a resource, alter delivery methods, and require\nspecific ordering of successive transformations/actions.\n\nObviously, each of these can interact with the others.  What do you\ndo, for example, when a client requests a resource which would\nproduce a List response, but where a PEP extension would allow the\ndelivery of a custom response?  My guess would be to return the list\nresponse with a Protocol-Info: header, but it would be nice to see\nsome of this worked out.\n\nIt would probably be a bit easier for the working group to see\nthese interactions if we had a specification draft, as well as\na discussion-oriented draft.  Do you have one in the works now,\nand if so when can we expect it?\n\n\nregards,\nTed Hardie\nNASA Science Internet\n\n\n\n", "id": "lists-010-8456229"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "    First off, this just isn't an accurate comparison of \"safe\",\n    which by definition is the potential for harm (to people or property)\n    [Leveson's paper on Software Safety has the complete definition].\n    It is absolutely and provably false that overestimation is safe.\n    Overestimation creates the potential for cache misses where there should\n    be cache hits, and cache misses result in additional network requests,\n    that in turn cost some people real money and have the potential to\n    completely block an already congested network at periods of peak usage\n    (as witnessed with the USA-UK link last year).\n    \n    In contrast, there is no direct correlation between presenting a stale\n    entity to a user and causing harm. Any entity that is marked as cachable\n    will not be dependent on the user ALWAYS seeing it when fresh -- servers\n    cannot make such critical entities cachable unless they know that there\n    are no older caches in the loop, and your rationale is only applicable\n    when there are older caches in the loop.\n\nRoy and I have disagreed in public for some time about what is \"safe\".\nBy now, it's clear that we will never agree.  And we both believe that\nthe other is using a completely bogus definition.\n\nMy definition \"safe\", as it applies to caching, is that there must\nnever be situation where a stale result is presented to a user as\nif it were fresh.\n\nRoy's definition of \"safe\" appears to be \"don't waste any opportunities\nfor caching, because this could cause additional network traffic.\nI'd question his use of the term \"catastrophic network failure\",\nespecially since we've seen ample evidence on this mailing list during\nthe past week or so that most HTTP caches would be hard-pressed to\nget hit rates above 40% to 50%, even if they were to cache EVERYTHING\nthat didn't have a question mark or \"cgi-bin\" in the URL.\n\nSince I'm not going to change Roy's mind, I'll limit myself to\ncorrecting the factual mistakes in his message, and I won't\ncontinue to rehash the \"safety\" argument.\n\n    But you are completely overlooking what happens if ANY of the\n    intermediaries has a system clock which is out-of-sync with the\n    origin.  If ANY of them do have a bad clock, they will pass the\n    additional bogus age on to every single request that passes through\n    the proxy.\n\nThe algorithm in section 13.2.3 is specifically designed to handle\nthe \"one bad clock\" (or even \"N bad clocks\") case.  It does this\nusing the following two steps:\n      apparent_age = max(0, response_time - date_value);\n      corrected_received_age = max(apparent_age, age_value);\n(see the draft for the definitions of the variables).  Because\nof the two max() operations, this will NEVER result in underestimating\nthe Age, as long as at least one of the clocks in the chain is correct.\n\nYes, it may overestimate the Age value, if one or more of the clocks is\nbadly out of sync.  If you agree with Roy that overestimation is evil,\nthen I suppose the word \"bogus\" is appropriate here.  Otherwise, it's\nexactly what the algorithm is intended to do.\n\n-Jeff\n\n\n\n", "id": "lists-010-8465946"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "I think the issue has been raised clearly enough for WG members to\ndecide whether the draft should stand \"as is\" or should change.\nWe've heard from Roy, Jeff, and Koen. What say others? I'd especially\nlike to hear from those folks who are currently implementing HTTP/1.1\nproxies.\n\nLarry\n\n\n\n", "id": "lists-010-8476631"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "    My main concern is that with the `every proxy cache must add an Age\n    header' interpretation, first-hand responses with max-age=0 or some\n    other small max-age value may have been marked as aged and gotten a\n    warning:stale header by the time they have passed a few 1.1 proxy\n    caches.  This is extremely bad because\n    \n      a) the next proxy might retry the request to get a fresh response,\n and might even go into an infinite retry-loop when it keeps\n getting responses with Age: 1.\n    \nI do not see anything in the draft specification that requires a\nproxy to retry a request until it gets a fresh response.  In fact,\nit seems ludicrous for an intermediate cache to do a retry purely\non this basis, since (both because of the case you observe, and\nthe possibility that an inbound cache is simply ignoring the\nCache-control headers) if you get a stale response on your first\ntry, an immediate retry is likely to result in the same error.\n\nThe only place that the draft currently suggests an automatic\nretry (if my memory is accurate) when an actual communication\nfailure occurs (e.g., when the server closes the connection\nbefore the client gets the entire response).\n\nIf people are confused by this, then maybe we need to add some\nclarifying language that makes it explict: a proxy SHOULD NOT\n(or perhaps MUST NOT) retry a request that completed with\na successful transmission of a response, whether or not that\nresponse is acceptable to the requesting client.\n\n      b) the user will get a big *warning:stale* message from the browser\n while in fact nothing is wrong (the response is firsthand, so it\n is as fresh as it can be).  Such warnings might drown out the\n actual warnings when responses *are* stale, might cause lots of\n reload-pressing, might cause service authors to never use\n max-age=0, etc.\n\nThe circumstances under which this could happen are:\n(1) hosts with badly incorrect clocks\n(2) origin servers that set very short max-age values\nand networks with relatively large RTTs\n\nEither one of these circumstances ought to be corrected (especially\nthe wrong-clock case) and so we ought to develop techniques for\ndiagnosing and reporting these problems, not hiding them.  The\nconsequences of inadvertently and silently delivering stale info to\nusers are far more likely to cause servers to use max-age=0 than\nthe \"cry wolf\" problem you think you see here.\n\n    My proposal to Jeff at the time was something like the following Age\n    header rule:\n    \n     - MUST add Age when serving from cache memory\n     - MAY add Age when relaying a response from a 1.0 proxy\n     - MUST NOT add Age when relaying a response from a 1.1 or higher proxy\n\nAfter thinking about the possible failure modes, I believe that Koen's\nproposal would be acceptable to me, but only with the following\nmodifications:\n\n     - MUST add Age when serving from cache memory\n     - MUST add Age when relaying a response from a pre-1.1 source\n     - SHOULD NOT add Age when relaying a response from a 1.1 or\n       higher source\n\nI replaced Koen's \"MAY\" with a \"MUST\" because that is necessary to\nprevent the wrong-clock failure mode that I described.\n\nI replaced his \"MUST NOT\" with a \"SHOULD NOT\" because I see no\ndemonstrably serious consequences here that would merit making this a\n\"MUST NOT\", and it's quite conceivable that future experience will\nreveal cases where a proxy might want to add Age when relaying from a\n1.1 proxy; we shouldn't ban this unnecessarily.\n\nI replaced \"proxy\" with \"source\" because otherwise there's no guidance\nfor what to do with responses received directly from an origin server.\n\nI replaced \"1.0\" with \"pre-1.1\" because there may still be some 0.9\nsystems out there.\n\n-Jeff\n\n\n\n", "id": "lists-010-8484986"}, {"subject": "ISOconformant Dates", "content": "Quick suggestion:\n\nThere's some ISO standard, don't know which, which suggests to\nuse this notation for dates:\n\nYEAR-MM-DD HH:MM:SS TZ\n\nor something like that.\n\nThis format is very handy as you can use lexical comparison\nto compare times.\n\nTherefore it would be a plus to have this in some future\nHTTP version.. Maybe one should introduce acceptance of\nthis style with HTTP/1.1 already so one can switch to\nthis syntax only sometime in the future..\n\nBest regards,\n-- \n___________\n mailto:LynX@impACT.pages.de    irc:symLynX   http://my.pages.de/\n mailto:LynX@you.might.aswell.use.this.as.my.mail.address.no.kidding.pages.dE\n\n\n\n", "id": "lists-010-8496251"}, {"subject": "Re: draft-ietf-http-pep0", "content": "Rohit,\n\nSome comments:\n\n1. I agree with you that the token/tspecials/... syntax definitions\nshould stay in the draft.  Readers should not have to trace references\nfor such things, and won't anyway.\n\n2. I agree with most of the other readability comments people have\nmade, and would like to mention in addition that I found it very\npainful to read the 3.[1234].[1234] sections.  I hope you can come up\nwith a more generalized description of this stuff.\n\n3. You talk a lot about `must begin using the specified extension'.\nWhat exactly does `begin using' mean?  Does the use end?  The JEPI\ndraft talks about `the protocol module associated with the specified\nprotocol will be invoked when ...'.  \n\nIt seems to me that you are setting up PEP to act as a generalized\nsystem by which a chain of HTTP agents can activate modules connected\nto them and mediate between these modules.  This seems to imply that\nthere can be state, both in the HTTP agent chain and in the connected\nmodules.  I think it is very important to clearly define which state\nis present where.  Maybe the ideal scheme never has state in the HTTP\nagents, or only has state in the agents as an optimization device.\n\nYou may want to consider a semantic model which does away with the\nconcept of `beginning to use'/`activating' altogether, this might\nsimplify state issues.\n\nYou may want to consider discussing the nature of protocol modules\nvery early in the draft, you have some stuff in there now but I'm\nmissing a discussion of state in modules and of how modules might do\nautonomous actions like sending the first PEP header in a negotiation\nsequence.  (See for example operation 3 in JEPI.  The NetScape ONE\nwhite paper I read recently makes a lot of fuss about triggering\nmodules through HTML elements.  There seems to be some `outside the\nscope of... boundary here that needs to be clearly defined.)\n\nI think that to determine if PEP is powerful enough, we will first\nneed to answer the question of what things we want modules to be\ncapable of doing.\n\n4. You write:\n>   Another aspect of the PEP model is that protocol extensions can have\n>   negotiable parameters.  Unlike previous proposals which model\n>   extensions as binary have/don't have features, PEP expects protocol\n>   extensions will be able to handle different versions of a particular\n>   protocol, invocation of ``compatible protocols'', and the selection of\n>   compatible modes from several that might be advertised.\n\nWho negotiates these negotiable parameters and who selects compatible\nmodes?  The HTTP agents? The protocol extensions themselves?  Both?\n\n5. The question of interaction with proxy caches is an interesting\none.  It seems that in general only the author of a server-side PEP\nmodule will be able to determine the cachability of the response if\nthe module is applied.  This seems to put cachability considerations\nbeyond the scope of PEP itself.  On the other hand, maybe generalized\nproxy mechanisms, like mechanisms to automatically downgrade responses\nbefore/after caching, are needed if module authors are to achieve good\ncachability.\n\n6. You write:\n>     * ``This agent has and is using (or does not have and isn't using)\n>       the http://w3.org/DES protocol extension with mode parameter CFB\n>       and header DES-Info:''\n\nThis example raises an interesting question: What happens if I define\na `Digital Epibration Service' protocol http://wuxta.com/DES with a\nheader DES-Info:?  It seems that PEP cannot prevent a header name\ncollision if both extensions are used at the same time.  So it seems\nthat PEP cannot support the safe use of unregistered HTTP headers by\npluggable extensions: the whole {headers *<token>} stuff could just as\nwell be removed.\n\n7. You write:\n>   Connection-scope extensions can remain backward-compatible by listing\n>   every associated header and the name of the extension in the\n>   Connection: header.\n\nI don't understand what you want to say here.\n\n8. You write:\n>          If a\n>          reference ends in \"*\", it should be understood as a wild-card\n>          character (particular implementations may choose to only\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n>          recognize restricted applications of \"*\", e.g. `only in the\n>          last component of http: URLs').\n\nI can't see how you can unambiguously negotiate if you don't know what\nwildcarding the implementation at the other side supports.\n\n9. The * is a legal character in URLs, but you also use it as a\nwildcard character in the {for ...} stuff.  Doesn't this raise various\nsticky ambiguity problems?\n\n10. It seems that there are also potential spoofing problems for\nmulti-user origin servers: author X could do a denial of service\nattack on the content of author Y by suggesting that Y's resources\nrequire or support some PEP extension that they do not require or\nsupport.\n\n11. I'm not happy with your use of the 220 code to indicate `uses\nprotocol extensions'.  I believe the use of extensions is already\nindicated by the presence of a Protocol header; cachability could be\nrestricted if necessary with the usual mechanisms.  Requiring 220 in\nsome cases seems to prevent use of PEP if I want to generate a not\nmodified/partial content/see other response.\n\n12. Could you comment on the state of mechanisms like JEPI which use\nPEP?  Is there already an installed PEP base we need to stay\ncompatible with (please say no)?  Are there publicly available test\nimplementations?\n\n13. Like Ted Hardie, I'd like to see a specification draft, preferably\na specification as complete as possible.  The devil is in the details,\nand I can't see all the details yet, in particular those concerning\nstate, after reading the discussion draft.  The previous\n(specification oriented) PEP draft left lots of details uncovered.\nOne often needs to consider the details to know how much power one can\nget before the complexity due to feature interaction starts exploding\nall over the place.\n\nKoen.\n\n\n\n", "id": "lists-010-8504309"}, {"subject": "Re: ISOconformant Dates", "content": "> Quick suggestion:\n\n> There's some ISO standard, don't know which, which suggests to\n> use this notation for dates:\n\n> YEAR-MM-DD HH:MM:SS TZ\n\n> or something like that.\n\n> This format is very handy as you can use lexical comparison\n> to compare times.\n\nFirst of all, your assertion that lexical ordering suffices to compare values\nof this sort is trivially shown to be false. Suppose I have the two values:\n\n   1990-04-04 07:00:00 -7000\n   1990-04-04 00:00:00 +0000\n\nA lexical comparison will find these values to be different when in fact they\nare the same. If you want to compare time values you have to either normalize\nthem with respect to zone or else figure out a way to rank times in different \nzones. This takes the problem out of the realm of simply lexical comparison\nfrom the outset, for the format you propose at least.\n\nSecond, ISO standards define a zillion different formats for time values, but\nas far as I know the one you describe isn't one of them. (It is, however, quite\npossible that such a format exists and I am unaware of it, as a complete set of\nISO specifications will easily fill a large bookcase completely.)\n\nThe ASN.1 specifications, for example, define universal time, which can be\neither YYMMDDHHMMZ, YYMMDDHHMMSSZ, YYMMDDHHMM+ZZZZ, or YYMMDDHHMMSS+ZZZZ.\n(There are also some broken implementations around that generate +ZZ instead of\n+ZZZZ. Also note the use of two digit years here, and the inability to tell the\ndifference between a four digit year and the presence/absence of seconds. ASN.1\nuniversal times are a monumentally poor way to represent time values in my\nopinion.)\n\nThere is also ASN.1 generalized time, which has a whole bunch of different\nforms, including YYYYMMDDHHMMSS.SSSZ and YYYYMMDDHHMMSS.SSS+ZZZZ and a bunch of\nothers. This is a much better way to represent time values, but it is\nunfortunately few if any applications use it.\n\nHowever, I suspect that the format you are thinking of is the one in ISO 8601.\nThe forms it allows are YYYYMMDDTHHMMSSZ and YYYYMMDDTHHMMSS (the latter is in\nlocal time. This format is used by the VCalendar specification (currently being\nconsidered by IETF Calendaring WG), for example. I think having either local\nvalues or GMT values is kinda strange, but there you go.\n\nIn any case, by restricting yourself to either GMT times or else to local time\nyou actually do end up with strings you can compare lexically. Needless to say,\nuse of local time is inherently ambiguous and hence not acceptable, but using\nGMT always would work and get you what you want.\n\nHowever, I question the need for any of this. While it is true that the formats\nwe tend to use for date/time values cannot be compared directly, I remain to be\nconvinced that this gives enough of a performance gain to warrant changing\nthings.\n\nNed\n\n\n\n", "id": "lists-010-8517660"}, {"subject": "[moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": "It is my belief that it is the intent of the working group that digest\nauthentication be part of HTTP/1.1. \n\nIf you disagree, would you please let me know ASAP?\n\nThanks,\n\nLarry\n\n------- Start of forwarded message -------\nX-URI: http://www.cs.utk.edu/~moore/\nTo: dsr@w3.org, masinter@parc.xerox.com\nSubject: http digest auth + http 1.1?\ncc: moore@cs.utk.edu, Harald Alvestrand <hta@uninett.no>\nFrom: Keith Moore <moore@cs.utk.edu>\nDate: Sun, 25 Aug 1996 22:13:43 PDT\n\n\nLast Thursday, the IESG approved both documents\n\ndraft-ietf-http-v11-spec-07.txt, .ps\ndraft-ietf-http-digest-aa-04.txt\n\nas Proposed Standards.\n\nIs it the intention of the working group that the support for the\ndigest authentication method should be included as part of http 1.1?\n\n(that is, should compliance with the http 1.1 spec require support for \nthe digest authentication method?)\n\nI want to make sure that we provide clear instructions to the RFC Editor\nwhich are consistent with the working group's wishes.\n\nthanks,\n\nKeith\n\n------- End of forwarded message -------\n\n\n\n", "id": "lists-010-8527543"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": "Larry Masinter:\n>\n>It is my belief that it is the intent of the working group that digest\n>authentication be part of HTTP/1.1. \n>\n>If you disagree, would you please let me know ASAP?\n\n[...]\n\n>Is it the intention of the working group that the support for the\n>digest authentication method should be included as part of http 1.1?\n>\n>(that is, should compliance with the http 1.1 spec require support for \n>the digest authentication method?)\n\nI feel that digest authentication is a `may support' feature, not a\n`must support' feature for HTTP/1.x applications.  I feel that\ncompliance with 1.1 must _not_ require support for digest\nauthentication: support for various authentication methods has always\nbeen optional in HTTP.  If support were required, this would greatly\nincrease the requirements on a minimal 1.1 application, which is a bad\nthing.\n\nI have no opinion on whether it is preferable to merge the digest\nauthentication draft into the main 1.1 draft.  As far as I am\nconcerned, this decision can be left to the RFC editor.\n\nKoen.\n\n\n\n", "id": "lists-010-8538007"}, {"subject": "Re: ISOconformant Dates", "content": "At 9:25 PM +0200 1996-08-25, Carl von Loesch wrote:\n>Quick suggestion:\n>\n>There's some ISO standard, don't know which, which suggests to\n>use this notation for dates:\n>\n>YEAR-MM-DD HH:MM:SS TZ\n>\n>or something like that.\n>\n>This format is very handy as you can use lexical comparison\n>to compare times.\n>\n>Therefore it would be a plus to have this in some future\n>HTTP version.. Maybe one should introduce acceptance of\n>this style with HTTP/1.1 already so one can switch to\n>this syntax only sometime in the future..\n\nYes, this date format is a definite winner, *and* for you bitheads,\nit saves a couple of characters in every date header.\n\nWe use an ISO hierarchy in our Persistent Document Identifier scheme.\nWorks nicely to produce chronologically sorted directories world wide\nas it provide hierarchy in an identifier.\n\nWe also use these for http logging.\n\n\n\n", "id": "lists-010-8546723"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": ">Larry Masinter:\n>>\n>>It is my belief that it is the intent of the working group that digest\n>>authentication be part of HTTP/1.1. \n>>\n>>If you disagree, would you please let me know ASAP?\n\nThat was my impression as well from following the mailing list; \nI wasn't present in Montreal, so don't know what was said there.\n\nPersoanlly, I think it's very desirable that digest authentication\nbe a mandatory part of of HTTP/1.1. \n\n\n--Michael Smith\n  ms@gf.org\n\n\n\n", "id": "lists-010-8554582"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": "koen@win.tue.nl (Koen Holtman) wrote:\n  > I feel that digest authentication is a `may support' feature, not a\n  > `must support' feature for HTTP/1.x applications.  I feel that\n  > compliance with 1.1 must _not_ require support for digest\n  > authentication: support for various authentication methods has always\n  > been optional in HTTP.  If support were required, this would greatly\n  > increase the requirements on a minimal 1.1 application, which is a bad\n  > thing.\n\nI would like to see it be mandatory.  Here's why.\n\n1) We would like Digest to supersede Basic.\n\n2) As long as there's uncertainty that Digest is widely supported by\nbrowsers, servers will of necessity ask for authentication by either.\n(That's assuming they support Digest themselves.)\n\n3) If servers can ask for both kinds of authentication, there's no\nincentive for browser vendors to support Digest.  So (I believe) they\nwon't.\n\nSo here's a proposal:  if an HTTP/1.1 agent (client or server) supports\nBasic, it must also support Digest.  Authentication support remains\noptional, but it's all or none.\n\nDave Kristol\n\n\n\n", "id": "lists-010-8562386"}, {"subject": "HTTP/1.1 + Diges", "content": "On Mon, 26 Aug 1996, Dave Kristol wrote:\n\n> koen@win.tue.nl (Koen Holtman) wrote:\n>   > I feel that digest authentication is a `may support' feature, not a\n>   > `must support' feature for HTTP/1.x applications.  I feel that\n>   > compliance with 1.1 must _not_ require support for digest\n>   > authentication: support for various authentication methods has always\n>   > been optional in HTTP.  If support were required, this would greatly\n>   > increase the requirements on a minimal 1.1 application, which is a bad\n>   > thing.\n> \n> I would like to see it be mandatory.  Here's why.\n> \n> 1) We would like Digest to supersede Basic.\n> \n> 2) As long as there's uncertainty that Digest is widely supported by\n> browsers, servers will of necessity ask for authentication by either.\n> (That's assuming they support Digest themselves.)\n> \n> 3) If servers can ask for both kinds of authentication, there's no\n> incentive for browser vendors to support Digest.  So (I believe) they\n> won't.\n> \n> So here's a proposal:  if an HTTP/1.1 agent (client or server) supports\n> Basic, it must also support Digest.  Authentication support remains\n> optional, but it's all or none.\n> \n\n\nI strongly agree with Dave.  I think his arguments are very sound.\nI would clarify one point, though.  It should be possible to support\nDigest and not support Basic.   But I like the requirement that\nif Basic is supported then Digest must be also.  I think Koen's \nconcerns about minimal implementations are met by the possibility of \nsupporting neither.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-8571683"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": ">So here's a proposal:  if an HTTP/1.1 agent (client or server) supports\n>Basic, it must also support Digest.  Authentication support remains\n>optional, but it's all or none.\n\nThis sounds right to me as well. If you support some authentication, you\nmust support good authentication, but you can also support Basic for\nobvious historical reasons.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-8582325"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": "Larry Masinter writes:\n> It is my belief that it is the intent of the working group that digest\n> authentication be part of HTTP/1.1. \n> \n> If you disagree, would you please let me know ASAP?\n\nI do agree that digest is essential but if both documents are already \napproved by the IESG then how can this be done without introducing \nsignificant delay in the process? Will IESG react different to a \"docked\" \ndigest document versus a simple reference from Chapter 11.2 or was the \napproval a \"blank check\" for what we decide on?\n\nHenrik\n\n\n-- \nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n", "id": "lists-010-8589932"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": ">Larry Masinter:\n>>\n>>It is my belief that it is the intent of the working group that digest\n>>authentication be part of HTTP/1.1. \n>>\n>>If you disagree, would you please let me know ASAP?\n\nThis was what it was originally proposed as.\n\nThe original idea was that DIGEST would be the preferred method\nand that BASIC be *VERY* strongly depreciated. That is why so\nmuch effort was made into making a scheme that was 100% plug\ncompatible with BASIC. If the idea was to produce the best\npossible digest authentication scheme we would have made a very\ndifferent proposal.\n\nAbout the only use for BASIC at present is in conjunction with \nSSL where it might possibly be preferable to DIGEST but not by \nmuch.\n\nI like Dave Kristol's proposal that suport for BASIC in 1.1\nimplies support for DIGEST.\n\n\nMy understanding was that we were hoping to have DIGEST docked\nand that only the adminstrivia issues would argue to not dock. \nIf the IESG are willing for docking to take place and any such \nmoves can be coordinated between the RFC edditor and the HTTP\neditor then fair enough.\n\n\nPhill\n\n\n\n", "id": "lists-010-8598551"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "Jeffrey Mogul:\n>\n[...]\n>If people are confused by this, then maybe we need to add some\n>clarifying language that makes it explict: a proxy SHOULD NOT\n>(or perhaps MUST NOT) retry a request that completed with\n>a successful transmission of a response, whether or not that\n>response is acceptable to the requesting client.\n\nClarifying language like this is already in the draft, see the end of\nSection 13.1.1.  The start of 13.1.2 could be interpreted to\ncontradict the clarification as far as the Warning header is\nconcerned, though.  My concerns are mainly about cache implementers\nwho just get it wrong, and add a Warning:stale when they should not.\nLike I said, `obvious' mechanisms added for robustness could easily\ncause catastrophic failure when deployed in a chain of proxies.\n\n[...]\n>After thinking about the possible failure modes, I believe that Koen's\n>proposal would be acceptable to me, but only with the following\n>modifications:\n>\n>     - MUST add Age when serving from cache memory\n>     - MUST add Age when relaying a response from a pre-1.1 source\n>     - SHOULD NOT add Age when relaying a response from a 1.1 or\n>       higher source\n\nThat would be acceptable to me.  I don't see the need to add Age when\nyou get something directly from a pre-1.1 _origin_ server, but I could\nlive with this requirement.\n\n>-Jeff\n\nKoen.\n\n\n\n", "id": "lists-010-8607389"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": "Dave Kristol:\n>\n>So here's a proposal:  if an HTTP/1.1 agent (client or server) supports\n>Basic, it must also support Digest.  Authentication support remains\n>optional, but it's all or none.\n\nI support this proposal.  It addresses my concerns about minimal\nimplementations, and I like the `if basic, then digest' requirement.\n\n>Dave Kristol\n\nKoen.\n\n\n\n", "id": "lists-010-8616758"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": "Folks,\n\nI'm sorry I raised this question without looking at the implications.\n\nThe HTTP document draft-ietf-http-v11-spec-07.txt has explicit\ninstructions:\n\n> 19.8.4 Possible Merge With Digest Authentication Draft\n\n> Note that the working group draft for Digest Authentication may be\n> processed by the IESG at the same time as this document; we leave it to\n> the RFC editor to decide whether to issue a single RFC containing both\n> drafts (see section 11.2 for where it would be put); in any case, the\n> reference in the reference list will need to be either deleted, or made\n> to the appropriate RFC (and section 11.2 deleted).\n\nand then in section 11.2:\n\n> 11.2 Digest Authentication Scheme\n\n> Note for the RFC editor: This section is reserved for including the\n> Digest Authentication specification, or if the RFC editor chooses to\n> issue a single RFC rather than two RFC's, this section should be\n> deleted.\n\nWe were asked for confirmation that it was our intent to merge the two\ndrafts. I don't think we have any other choices than either:\n\na) delete section 11.2, and ignore 19.8.4\nb) edit digest-aa in such a way that it is suitable for inserting into\n   v11-spec as a revised section 11.2.\n\nHowever, on looking over digest-aa, it seems to me that just inserting\nit as chapter 11.2 is unworkable; the results would be an unreadable\nmess. First, digest-aa repeats many of the definitions of v11-spec,\nand has an extensive security considerations section.\n\nI think what we should do is craft a replacement paragraph for section\n11.2. I suggest:\n\n\"The HTTP/1.1 protocol includes a Digest Authentication\nScheme, which is described in RFC xxxx.\"\n\nLarry\n\n\n\n", "id": "lists-010-8625301"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": "Basically, \"docking\" isn't practical. I don't think the HTTP editor\n(Jim) is prepared to do the rather major editing that would be\nnecessary to make the merged document readable, I know that the RFC\neditor isn't, and I don't think that we can do that much editing\nwithout recycling through \"last call\".\n\nI've heard there's currently a two-month backlog in getting things\nthrough the RFC editor, even when they're straightforward.\n\nLarry\n\n\n\n", "id": "lists-010-8635389"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "History has shown that getting bad data in a cache has had major\nheadaches associated with it.\n\nI believe we should stay with Jeff's conservative algorithm, and this is why\nI have not to date made any changes in the specification; I will of course\ngo with the consensus of the group, but to date, I've not seen\na good case made by Roy.  Not surprising users is a very good thing;\nif the web is unreliable, it will generate many more problems than whatever\nsmall performance gain might be available with a more optimistic algorithm.\n\n- Jim Gettys\n\n\n\n", "id": "lists-010-8644181"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": "I agree with Dave Krystol's position: if a client supports\nauthentication at all, it MUST support Digest.  This means that only\nthose supporting authentication must do work, keeping the simplest\nweb clients simple.\n\nWe have to get passwords in the clear out of use in the Web; naive people\ntend to put their regular passwords into password fields, not understanding\nthe lack of security.\n- Jim\n\n\n\n", "id": "lists-010-8652764"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "    >If people are confused by this, then maybe we need to add some\n    >clarifying language that makes it explict: a proxy SHOULD NOT\n    >(or perhaps MUST NOT) retry a request that completed with\n    >a successful transmission of a response, whether or not that\n    >response is acceptable to the requesting client.\n    \n    Clarifying language like this is already in the draft, see the end of\n    Section 13.1.1. \n\nRight, I had forgotten about that.\n\n    The start of 13.1.2 could be interpreted to\n    contradict the clarification as far as the Warning header is\n    concerned, though.\n\nI believe that the start of 13.1.2 does not contradict the no-retry\npolicy, but maybe that's not the part you are talking about anymore.\n\nIt says that a cache must (probably should be \"MUST\") return a\nWarning if the response is neither first-hand nor \"fresh enough\",\nand the end of section 13.1.1 says (in essence) \"don't add a warning\nif the response is being forwarded but is not fresh enough.\"\n\nI.e., the Warning should be added if the response is\n(1) taken from the cache\nand\n(2) is stale\nand should not be added otherwise.\n\nPerhaps this part needs a little rewording to make this rule explicit.\n\n    My concerns are mainly about cache implementers\n    who just get it wrong, and add a Warning:stale when they should not.\n    Like I said, `obvious' mechanisms added for robustness could easily\n    cause catastrophic failure when deployed in a chain of proxies.\n\nThis danger seems rather speculative.  I'd suggest deploying the\nspec, as written, with the understanding that it is easier to relax\nthis kind of rule in a latter draft (after some operation experience)\nthan it would be to start out loose and later make it stricter.\n\n-Jeff\n\n\n\n", "id": "lists-010-8660833"}, {"subject": "Re: [moore&#64;cs.utk.edu: http digest auth + http 1.1?", "content": ">Basically, \"docking\" isn't practical. I don't think the HTTP editor\n>(Jim) is prepared to do the rather major editing that would be\n>necessary to make the merged document readable, I know that the RFC\n>editor isn't, and I don't think that we can do that much editing\n>without recycling through \"last call\".\n\nAh, I misunderstood your question which sounded like you were\nasking about docking. My understanding of the wording in the spec \nwas that the group were generally agreed that incorporating\ndigest was a good idea but that it might be controvertial\nvis a vis the IESG and hence the conditional statement...\n\nAt all costs we must avoid going back to last call (IETF snakes\nand ladders, your draft has a misprint go back to square 1)\n\nPHILL\n\n\n\n", "id": "lists-010-8669950"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "On Mon, 26 Aug 1996, John Franks wrote:\n\n> I strongly agree with Dave.  I think his arguments are very sound.\n> I would clarify one point, though.  It should be possible to support\n> Digest and not support Basic.   But I like the requirement that\n> if Basic is supported then Digest must be also.  I think Koen's \n> concerns about minimal implementations are met by the possibility of \n> supporting neither.\n\nI disagree weakly ... SHOULD is strong enough ... I have an\nHTTP application\nwhich at the 99.9% level will be deployed in a single machine. A password\nin the clear would not be exposed outside of the machine. Of the remaining\n.1%, the bulk will be on an intranet LAN where exposure is not a large\nrisk. On that basis, we use basic authentication to restrict access\nfrom users outside the single machine. Hence, I believe it a reasonable\ndesign point to support BASIC w/o DIGEST. SHOULD support DIGEST provides\nan opportunity for carefully reasoned escape where other features are\nprobably worth more of the implementation effort. \n\nDave Morris\n\n\n\n", "id": "lists-010-8678544"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "Dave Morris describes HTTP application 99.9% certain to be on single\nmachine...\n\n\nI don't think that this is a convincing argument. The concern is\nto stop the password in the clear problem ASAP. There has been \nremarkably little progress on the part of the vendors here and\na SHOULD is not going to improve progress\n\n\nPhill\n\n\n\n", "id": "lists-010-8687120"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "At 01:31 AM 8/27/96 -0700, David W. Morris wrote:\n>design point to support BASIC w/o DIGEST. SHOULD support DIGEST provides\n>an opportunity for carefully reasoned escape where other features are\n>probably worth more of the implementation effort. \n\nWe all know exactly what we're talking about here:\n\n\"SHOULD\" is clearly not going to get Netscape to support Digest.  As long as\nNetscape doesn't support Digest, people will be shipping their passwords\naround in clear text for years to come.  The only thing that we can *hope*\nwill get Netscape to support Digest is the threat of slapping \"HTTP/1.1\nuncompliant\" on them publicly and hope it shames them into supporting it.\nSo that's what we're doing.  Bad reasoning?  Bad precedent?  Bad form?  All\nthese things and more I'm sure, but the end justifies the means.\n\n-----\nDaniel DuBois, Traveling Coderman -- NEW! http://www.spyglass.com/~ddubois/\n         Roses are red, violets are blue, this .sig doesn't rhyme.\n\n\n\n", "id": "lists-010-8694730"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "Phillip Hallam-Baker writes in <9608271501.AA01051@vesuvius.ai.mit.edu>:\n>Dave Morris describes HTTP application 99.9% certain to be on single\n>machine...\n>\n>\n>I don't think that this is a convincing argument. The concern is\n>to stop the password in the clear problem ASAP. There has been\n>remarkably little progress on the part of the vendors here and\n>a SHOULD is not going to improve progress\n\nUndoubtedly, there will be a very small minority of applications where \npasswords in the clear are not a serious problem.  But this is such a small \nfraction of the HTTP applications as to be negligible.\n\nIn my experience, applications have long lives with tortuous paths from \ntheir start point to their end -- witness the 1401 Autocoder payroll system \nI knew of running 10 or so years after IBM discontinued making the 1401. \n Dave, IMHO it is dangerous to assume that this application will forever and \never not be subject to someone, somewhere, wanting to break security on it \n -- if not on its current platform, then on the next platform, or the \nnext...  If the data is worth protecting, it is likely that the data \nsecurity is worth cracking.\n\nIf an HTTP 1.1 server supports Basic, they MUST support Digest.  This is the \nonly way to eventually eliminate passwords in the clear.\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-8703111"}, {"subject": "Netscape vs. Digest (?", "content": "Daniel DuBois <dan@spyglass.com> wrote:\n\n>\"SHOULD\" is clearly not going to get Netscape to support Digest. \n> The only thing that we can *hope*\n>will get Netscape to support Digest is the threat of slapping \"HTTP/1.1\n>uncompliant\" on them publicly and hope it shames them into supporting it.\n>\n\nIs it true that Netscape opposes Digest? Why? Is there anybody from Netscape \non the list who can comment on this? Failing that, is there anybody not from \nNetscape who's willing to offer a hypothesis?\n\n--Michael Smith\n  ms@gf.org\n\n\n\n", "id": "lists-010-8711545"}, {"subject": "Syntax and sticky/compressed headers", "content": "Hi,\n\nI've been thinking about Paul Leaches sticky headers\nnote. Basically the problem I see is a \"slippery slope\" one.\nIts not too difficult to do sticky headers, but then again its\nnot much more difficult (if at all) to go all the way to a \nbinary protocol.\n\nAs I see it each MIME header has a pretty uniform abstract\nsyntax, basically the data structure is :-\n\nmessage : struct\nheadersList(header)\nbodyArray(Octet)\nfootersList(header)\n\nheader : struct\ntagtoken\nparametersList(attribute_value)\n\nattribute_value : struct\ntagtoken\nvalueAlt (token | string)\n\n[The last alt being disambiguated by the specific tag token]\n\nBasically in MIME all tokens are strings so the distinction\nbetween tokens and strings is not apparent. The distinction I would\ndraw is that a token is simply a conventional symbol and could be\nexchanged for any other symbol without changing the meaning of \nthe message (ie Content-Type: could be \"MIMETYPE\" or 42 and \nthe protocol would still work identically. Some values (eg URLs)\ncannot be arbitrarily changed without the meaning of the message\nitself changing.\n\n\nRather than exchange Content-Type for #f as Paul suggests why not\ngo to a 100% binary protocol and represent tokens as numbers. \n[NB I'm not suggesting ASN.1 here or similar profanity, ASN.1 is\na bad implementation of a good idea and like COBOL deserves to\ndie regardless of which standards bodies tout it).\n\n\nThe scheme I would suggest would be to learn from ASN.1 and construct\nan unambiguous format that can be written out via a simple linear\ntraversal of the data structure (i.e. no BER nonsense). The MIME\ndata structure is simple enough to make this possible without \nexcessive complexity.\n\nThe traditional point raised against binary formats is that it\nis not possible to create new tags without danger of conflicts.\nIf a variable length encoding were used for identifiers this need\nnot be an issue. For example the high bit of a token could \ndetermine whether the token was a single byte (ie commonly used)\ntoken or a multi-byte token. If the token was a multibyte one the\nlow 7 bits could give the actual length. This would allow private\nextensions without a significant probability of collision.\n\n\nThe other major change that I would like to see which is linked to \nthis is to change HTTP from its present synchronous design to an \nasynchronous one. At present a client must send requests and recieve\nresponses in order. Pipelining means that the client can send \nmultiple requests before recieving a response but there is no \nallowance for out of order reciept of responses. Adding this \nfacility would make HTTP a general purpose object messaging \nprotocol. \n\nMore to the point it would mean that we could move to HTTP NG\nwithout obsoleting the vast installed base. Here the parts that\nmost concern me are CGI, NSAPI and ISAPI type scripts. It is\nall very well for Netscape to release a new server but the \ncontent providers won't be willing to support a new protocol \nunless it is transparent to their back end apps. I don't think\nthat the perl people are likely to want to support multiple\nprotocols unless it is entirely transparent. This leads me \nto think that there is no room to change the semantics of the \nHTTP protocol as Simon originally proposed in NG.\n\n\nAs I said, its a slippery slope which as far as I see it points\npretty clearly towards the territory that NG maps out. If one added\nin flow control and batched acknowledgements the feature set\nis completed.\n\n\nPhill\n\n\n\n", "id": "lists-010-8719874"}, {"subject": "Re: Netscape vs. Diges", "content": "On Tue, 27 Aug 1996, Michael Smith wrote:\n\n> Daniel DuBois <dan@spyglass.com> wrote:\n> \n> >\"SHOULD\" is clearly not going to get Netscape to support Digest. \n> > The only thing that we can *hope*\n> >will get Netscape to support Digest is the threat of slapping \"HTTP/1.1\n> >uncompliant\" on them publicly and hope it shames them into supporting it.\n> >\n> \n> Is it true that Netscape opposes Digest? Why? Is there anybody from Netscape \n> on the list who can comment on this? Failing that, is there anybody not from \n> Netscape who's willing to offer a hypothesis?\n> \n\nI am not sure this attack on Netscape is justified.  To the best of my\nknowledge Microsoft also does not support digest.  Some earlier\nversions of MSIE supported digest authentication, but the latest\nversion has apparently withdrawn that support.  (These comments are\nonly based on experience with the Mac version of MSIE so maybe things\nare different with Windows).\n\nAs one of the digest authentication spec authors, I would hope that\nboth Netscape and Microsoft will support it.  I don't know if either\nwill.  But, on the whole, I think Netscape has been more open and\n\"internet friendly\" than any other major corporation and singling them\nout in this kind of criticism is neither constructive nor fair.\n\nDaniel's company, Spyglass, deserves credit for much of the early\nwork on digest authentication and, I believe, they have been nearly\nthe only browser vendor to provide consistent support for it.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-8730070"}, {"subject": "Re: Netscape vs. Digest (?", "content": "At 02:22 PM 8/27/96 EDT, Michael Smith wrote:\n>Is it true that Netscape opposes Digest? Why? Is there anybody from Netscape \n>on the list who can comment on this? Failing that, is there anybody not from \n>Netscape who's willing to offer a hypothesis?\n\nThere was a marketing exec from Netscape at the last IETF meeting that said,\nand my paraphrasing isn't that far from a direct quote: \"Netscape is not\ndoing Digest.  Why bother doing Digest, when we have SSL.  You IETF guys\nwill take forever to get it standardized, and by then something better will\nbe around.\"\n\n-----\nDaniel DuBois, Traveling Coderman -- NEW! http://www.spyglass.com/~ddubois/\n         Roses are red, violets are blue, this .sig doesn't rhyme.\n\n\n\n", "id": "lists-010-8740107"}, {"subject": "Re: Netscape vs. Diges", "content": "At 01:42 PM 8/27/96 -0500, John Franks wrote:\n>I am not sure this attack on Netscape is justified.  To the best of my\n\nI didn't consider the statements I made to be an attack.  Just truths.\nIndependent of whether or not the statements were an 'attack' I think the\nstatements were justified.\n\n>knowledge Microsoft also does not support digest.  Some earlier\n>versions of MSIE supported digest authentication, but the latest\n>version has apparently withdrawn that support.  (These comments are\n\nGiven Paul Leach's involvement with discussion on Digest in his working\ngroup, I have faith that Microsoft would not ship a HTTP/1.1 product without\nDigest support (How's that for pressure Paul?) -- especially if we mandate\nit for HTTP/1.1 compliance.\n\n>both Netscape and Microsoft will support it.  I don't know if either\n>will.  But, on the whole, I think Netscape has been more open and\n>\"internet friendly\" than any other major corporation and singling them\n\nI have not said Netscape has been an unfriendly, un-open (I might be\nthinking it, but I haven't made available my opinions on Netscape in this\nforum) company...\n\n<sidetrack>\n...although I will say I'm quite irked when you say Netscape has been *more*\ninternet friendly than any other major corporation.  Maybe \"even\" at best.\nYou could have picked any number of large companies that have been even more\nstandards oriented, and I wouldn't have disagreed if you had picked\nSpyglass.  Netscape is much maligned, certainly overly so, but they haven't\nbeen poster children of IETF process either.  That cookie draft sure took a\nlong time to appear, and even then it wasn't without extraordinary efforts\nfrom Kristol.  Then there's all the HTML extensions that were never put into\nan internet draft, or didn't even show up quickly (yet?) in publicly\navailable DTDs.\n</sidetrack>\n\nI have only said that indications I've received imply Netscape needs to be\ncoaxed to support Digest, because they, as of recently, had no plans to\nsupport it.\n\n-----\nDaniel DuBois, Traveling Coderman -- NEW! http://www.spyglass.com/~ddubois/\n         Roses are red, violets are blue, this .sig doesn't rhyme.\n\n\n\n", "id": "lists-010-8748236"}, {"subject": "Re: Netscape vs. Digest (?", "content": ">Is it true that Netscape opposes Digest? Why? Is there anybody from Netscape \n>on the list who can comment on this? Failing that, is there anybody not from \n>Netscape who's willing to offer a hypothesis?\n\nThe story I heard blamed lack of cycles, but that tends to be\nthe way for every extension that they don't think up in house.\n\nFor a long time Netscape refused to contemplate tables claiming\nthey were unnecessary. They have dragged their feet on maths\nmarkup for years claiming \"nobody wants it\".\n\nWhat was somewhat ammusing was when Marc stood up to explain his\n\"discovery\" that the Web could be used as an Intranet collaboration\ntool, I think a guy in Geneva had a similar idea at some time:-)\n\n\nPhill\n\n\n\n", "id": "lists-010-8757897"}, {"subject": "Re: Netscape vs. Digest (?", "content": "Michael Smith wrote:\n> \n> Daniel DuBois <dan@spyglass.com> wrote:\n> \n> >\"SHOULD\" is clearly not going to get Netscape to support Digest.\n> > The only thing that we can *hope*\n> >will get Netscape to support Digest is the threat of slapping \"HTTP/1.1\n> >uncompliant\" on them publicly and hope it shames them into supporting it.\n> >\n> \n> Is it true that Netscape opposes Digest? Why? Is there anybody from Netscape\n> on the list who can comment on this? Failing that, is there anybody not from\n> Netscape who's willing to offer a hypothesis?\n\nWe had been adding support for digest auth back in 1.1 but\nthen some folks decided to keep changing the spec so we\ndropped the feature.\n\nAt this point there isn't any good reason to add such a weak\nauthorization scheme when certificates are available already.\n\nWhy would you ever want to use digest if you already have\ncertificate support?\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n", "id": "lists-010-8765956"}, {"subject": "Re: Netscape vs. Digest (?", "content": "> There was a marketing exec from Netscape at the last IETF meeting that said,\n> and my paraphrasing isn't that far from a direct quote: \"Netscape is not\n> doing Digest.  Why bother doing Digest, when we have SSL.  You IETF guys\n> will take forever to get it standardized, and by then something better will\n> be around.\"\n\nIn Netscape's defence, I implemented Digest for a late beta of\nNavigator 2.0 (I think that version was called SimpleMD5), but just\ndays before that beta was due to go out the spec submitted to the IETF\nchanged and became unstable judging by the amount of discussion that\nsuddenly bursted out in the respective mailing list.  Consequently, I\nhad to remove the support that I had added.  We had all due eagerness\nto support it asap but it backfired when spec kept changing.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n", "id": "lists-010-8774701"}, {"subject": "Re: Netscape vs. Digest (?", "content": "Lou Montulli wrote:\n> \n> Michael Smith wrote:\n> >\n> > Daniel DuBois <dan@spyglass.com> wrote:\n> >\n> > >\"SHOULD\" is clearly not going to get Netscape to support Digest.\n> > > The only thing that we can *hope*\n> > >will get Netscape to support Digest is the threat of slapping \"HTTP/1.1\n> > >uncompliant\" on them publicly and hope it shames them into supporting it.\n> > >\n> >\n> > Is it true that Netscape opposes Digest? Why? Is there anybody from Netscape\n> > on the list who can comment on this? Failing that, is there anybody not from\n> > Netscape who's willing to offer a hypothesis?\n> \n> We had been adding support for digest auth back in 1.1 but\n> then some folks decided to keep changing the spec so we\n> dropped the feature.\n> \n> At this point there isn't any good reason to add such a weak\n> authorization scheme when certificates are available already.\n> \n> Why would you ever want to use digest if you already have\n> certificate support?\n\nBefore everyone gets angry at me, allow me to refute my\nown message.\n\nWhile I still feel that certificates are *way* better when\nthey are used, realistically speaking, they are much \nharder to put into every users hands.\n\nOn the other hand Digest auth is *way*, *way* better than\nbasic auth and can be put into place fairly easily by\nadding support in new clients and servers.  For that\nreason they are worth implementing even given the\nexistance of certificates.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n", "id": "lists-010-8783527"}, {"subject": "Re: Netscape vs. Digest (?", "content": "Lou Montulli <montulli@netscape.com> wrote:\n\n>Why would you ever want to use digest if you already have\n>certificate support?\n\nI remember years ago thinking that IBM should replace \"think\" as its\ncorporate motto with \"Why would you ever want to do that,\" a phrase \noften heard in IBM's palmy days from their sales types, whenever you \nasked about some requirement they hadn't anticipated. Perhaps it's \nsomething of an occupational hazard in companies that attain a certain \ndegree of market dominance. \n\nClearly there is a belief among the community of people interested in HTTP\nand its future that a lightweight, non-proprietary mechanism is desirable, \neven though, as Lou says, \"certificates are available.\" Available is nice, \ncompulsory isn't. Incredible as it may seem, some of us like to have \nalternatives. \n\nIs it overly cynical to suspect that Netscape would like to _eliminate_ \nthe alternatives? Perhaps, but I don't think I'm the only one to whom \nthe suspicion will occur. A greater _appearance_ of respect for the \nstandards process would be good PR, at any rate -- unless, of course, the \nmarket dominance is thought to have reached such a point that good PR \ncan be dispensed with. \n\n--Michael Smith\n  ms@gf.org\n\n\n\n", "id": "lists-010-8792328"}, {"subject": "Re: Netscape vs. Diges", "content": "On Tue, 27 Aug 1996, Lou Montulli wrote:\n\n> > \n> > Daniel DuBois <dan@spyglass.com> wrote:\n> > \n> > >\"SHOULD\" is clearly not going to get Netscape to support Digest.\n> > > The only thing that we can *hope*\n> > >will get Netscape to support Digest is the threat of slapping \"HTTP/1.1\n> > >uncompliant\" on them publicly and hope it shames them into supporting it.\n> > >\n> > \n> \n> At this point there isn't any good reason to add such a weak\n> authorization scheme when certificates are available already.\n> \n> Why would you ever want to use digest if you already have\n> certificate support?\n> \n\n   1. It's freely exportable with no license restrictions.\n   2. There are no patent entanglements.\n   3. SSL has a significant performance cost.\n   4. Certificates don't work very well in environments where\n      users use many different computers (kiosks).\n\nActually, I like SSL and certificates a lot, and I think that Netscape\nshould be commended for making the spec and reference implementations\navailable.  There is no question that SSL is a \"good thing\" and I\nthink you deserve a lot of credit for creating it and contributing it\nto the net community.  But SSL doesn't solve all problems optimally.\n\nThe biggest problem is the continued widespread use of Basic\nAuthentication which results in transmission of unencrypted passwords.\nThe danger isn't so much sniffing -- it's that users have a strong\ntendency to use one password for everything.  This makes it easy for\nan unscrupulous person to ask for \"registration\" and collect\npasswords.\n\nDigest was never intended as strong authorization -- merely as something\nto get rid of Basic.\n\nFrankly, I would be happy even if Netscape doesn't support digest, if\nthey would also remove support of Basic Authentication.  I think this\nwould be HTTP/1.1 compliant and would also be consistent with your\nview that SSL meets all authentication needs.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-8800580"}, {"subject": "Re: Netscape vs. Diges", "content": "On Tue, 27 Aug 1996, Ari Luotonen wrote:\n\n> \n> \n> In Netscape's defence, I implemented Digest for a late beta of\n> Navigator 2.0 (I think that version was called SimpleMD5), but just\n> days before that beta was due to go out the spec submitted to the IETF\n> changed and became unstable judging by the amount of discussion that\n> suddenly bursted out in the respective mailing list.  Consequently, I\n> had to remove the support that I had added.  We had all due eagerness\n> to support it asap but it backfired when spec kept changing.\n> \n\nThis isn't quite fair.  Anyone can propose any crazy idea they want on\na newsgroup and many people did.  However, I don't believe that there\nhave been any drafts of the spec (the current one is 04) which would\nnot interoperate with \"SimpleMD5\".  There were some additions, but all\nwere optional. This was done despite the fact that (IMHO) there were\nsuggestions for (incompatible) changes which would have improved the\nspecification.\n\nThe motivation for this (at least on my part) was to preserve\ncompatibility with Spyglass client implementations (some of which also\nshowed up in some versions of MSIE).  People at Spyglass had done\nmuch of the early work on digest and I thought they deserved that\nconsideration -- even though when I took over as editor of the spec\nthey said do whatever is best and they would accomodate if need be.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-8810701"}, {"subject": "Re: Netscape vs. Digest (?", "content": "hallam@ai.mit.edu wrote:\n\n>For a long time Netscape refused to contemplate tables claiming\n>they were unnecessary. They have dragged their feet on maths\n>markup for years claiming \"nobody wants it\".\n\nNobody wants \"x-gzip\" or \"x-compress\" in the Mac/Windows world either.\n\nWhy else would they still refuse to add it, forcing users of those\nplatforms to jump through hoops to use compressed information they\ndownload from the web... the very people who will have the most trouble\nfinding a hoop to jump through.\n\nrob.\n\n\n\n", "id": "lists-010-8819986"}, {"subject": "image transfer", "content": "I have been studying the Internet Draft HTTP 1.0 dated September\n4, 1995, and trying to implement an HTTP server.  I can not find\nany mention of how the images in an HTML document are transmitted.\nHow are embedded images transmitted?\n\n\n\n", "id": "lists-010-8827162"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "Writing MUST instead of SHOULD in the specification is not any way to\nforce some vendor to either implement or not implement something. The\nspec should say what makes sense, not what is politically\nexpedient. We should write \"MUST\" if non-compliance causes systems to\nbreak.\n\nI think there are too many \"MUST\"s in HTTP/1.1, but agreed to wait\nuntil the review for \"Proposed\" -> \"Draft\" to review them. I don't see\nany reason to add one here, though.\n\nLarry\n\n\n\n", "id": "lists-010-8833482"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "Larry>\n\n>Writing MUST instead of SHOULD in the specification is not any way to\n>force some vendor to either implement or not implement something. The\n>spec should say what makes sense, not what is politically\n>expedient. We should write \"MUST\" if non-compliance causes systems to\n>break.\n\nThis is the case here. Sending passwords in the clear causes systems\nto be susceptible to security problems that they would not otherwise\nbe vulnerable to.\n\nHaving one's system hacked is a pretty extreeme form of having it break.\n\nPhill\n\n\n\n", "id": "lists-010-8841516"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "Servers can choose not to accept or request basic authentication. As\nhas been pointed out in many cases, Basic authentication is as safe as\nDigest if used in conjunction with some other one-time password system\n(SKey, SecurID, etc.).\n\nI think we're deluding ourselves if we think we can require \"MUST\nimplement\"; \"MUST implement\" doesn't belong in a protocol\nspecification: \"MUST send\", or \"MUST reply\" does.\n\nLarry\n\n\n\n", "id": "lists-010-8849762"}, {"subject": "Re: Netscape vs. Diges", "content": ">To the best of my\n>knowledge Microsoft also does not support digest.  Some earlier\n>versions of MSIE supported digest authentication, but the latest\n>version has apparently withdrawn that support.  (These comments are\n>only based on experience with the Mac version of MSIE so maybe things\n>are different with Windows).\n\nI was actually speaking with the MSIE folks about this just today- They\nremoved the digest support because they couldn't find any servers to test\nit against. They have assured me that it will be back in the next version.\n\n\n\nAlex Hopmann\nResNova Software\nhopmann@holonet.net\nhttp://www.resnova.com/\n\n\n\n", "id": "lists-010-8858152"}, {"subject": "Re: Netscape vs. Diges", "content": ">I was actually speaking with the MSIE folks about this just today- They\n>removed the digest support because they couldn't find any servers to test\n>it against. They have assured me that it will be back in the next version.\n\nThe common Lisp Web server has had it for some time now. Also Spyglass\nimplemented it in their server. Originally Jeff wanted digest because \nhe wanted to configure the server from the net and did not like the \nidea of BASIC.\n\nPhill\n\n\n\n", "id": "lists-010-8865995"}, {"subject": "Re: Netscape vs. Diges", "content": ">\n>>I was actually speaking with the MSIE folks about this just today- They\n>>removed the digest support because they couldn't find any servers to\n>test\n>>it against. They have assured me that it will be back in the next\nversion.\n>\n>The common Lisp Web server has had it for some time now. Also Spyglass\n>implemented it in their server. Originally Jeff wanted digest because \n>he wanted to configure the server from the net and did not like the \n>idea of BASIC.\n>\nOh yea, we have also been implementing it for some time now- I know there\nwere several implementations out there, just I guess it wasn't obvious how\nto find them...\n\nIn any case I am hearing from Lou and others that it should like noone is\nobjecting to digest authentication in 1.1, right? In other words, various\npeople may have things that are better (certificates, SSL, etc), but digest\nis easy so everyone can implement it as a minimum standard that is\nsignifigantly better than  basic.\n\nRight?\n\n\nAlex Hopmann\nResNova Software\nhopmann@holonet.net\nhttp://www.resnova.com/\n\n\n\n", "id": "lists-010-8873523"}, {"subject": "Re: Netscape vs. Diges", "content": "At 6:05 PM -0700 1996-08-27, Alex Hopmann wrote:\n>\n>>To the best of my\n>>knowledge Microsoft also does not support digest.  Some earlier\n>>versions of MSIE supported digest authentication, but the latest\n>>version has apparently withdrawn that support.  (These comments are\n>>only based on experience with the Mac version of MSIE so maybe things\n>>are different with Windows).\n>\n>I was actually speaking with the MSIE folks about this just today- They\n>removed the digest support because they couldn't find any servers to test\n>it against. They have assured me that it will be back in the next version.\n>\n\nActually, spyglass has such servers as do I. You can test the digest support\nfrom the examples/documentation at http://wilson.ai.mit.edu/cl-http/cl-http.html,\nalong with other 1.1 features. We tested implementation against theirs about\na yaer ago when we first did it.\n\nI vote for a *must* on digest authentication. We would be much happier using\nit rather than basic on a number of public web pages.  Of course, MD5 does have\nit shortcomings so it is important to implement the extensions that allow\nthe algorithm to be specified. \n\n\n\n", "id": "lists-010-8881667"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "It is completely against the principles of IETF standards that the\nprocess of arriving at \"rough consensus\" could result in threatening\nto slap \"uncompliant\" on someone publicly. It's a rather empty\npower-trip.\n\nBesides, there are ample examples of situations where a HTTP/1.1\ncompliant application (not necessarily a browser) might implement one\nkind of authentication and not another. There's no first-principled\nreason to brand THOSE applications uncompliant in a quest for some\nkind of marketing press release.\n\nNo thanks,\n\nLarry\n\n\n\n", "id": "lists-010-8889896"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "Now, now.. Let's just get the digest scheme into wider use by deprecating\nbasic in favor of digest.\n\nThere are a number of other uses of digest authentication, including\nauthenticating post transactions and http return codes so the client\nknows the data was received and actually processed by the server.\n\nWe are putting this into our system and phil should have a spec available\nsoon.  This allows one to implement transaction-controlled updates with post\nand have some confidence that everything is working properly.\n\nIt might be useful to extend the notion of digest authenticated transactions to any http transaction.\n\n\n\n", "id": "lists-010-8898058"}, {"subject": "Re: Syntax and sticky/compressed headers", "content": "On Tue, 27 Aug 1996 hallam@ai.mit.edu wrote:\n\n> not be an issue. For example the high bit of a token could \n> determine whether the token was a single byte (ie commonly used)\n> token or a multi-byte token. If the token was a multibyte one the\n> low 7 bits could give the actual length. This would allow private\n> extensions without a significant probability of collision.\n\nIt would be quite sensible to reserve some codes/tokens in both the\nsingle and extended ranges for private extensions.\n\nDave Morris\n\n\n\n", "id": "lists-010-8906182"}, {"subject": "Re: Netscape vs. Digest (?", "content": "Lou Montulli writes:\n> Why would you ever want to use digest if you already have\n> certificate support?\n\nI think at least one reason is clear:\nLack of export control hassles on hashing for authentication.\n\nThis means we can make it universal, and stop passwords in the clear\nworld-wide.  And as the #1 (and I think #2) servers on the Internet\nare Apache and NCSA, which have no solution to the export problem\navailable to them (as I understand it, the Apache folks had their arms\ntwisted to even remove hooks for stronger forms of encryption or\nauthentication), this is a Big Issue.  It is far from clear to me that\ncertificate support is universally available as a result of this\naction of the government.  Even if the code were available worldwide, it can't\njust get dropped into a server distribution.\n\n- Jim Gettys\n\n\n\n", "id": "lists-010-8913935"}, {"subject": "Re: Netscape vs. Digest (?", "content": "Lou Montulli <montulli@netscape.com> wrote:\n\n>Before everyone gets angry at me, allow me to refute my\n>own message.\n>[....]\n>On the other hand Digest auth is *way*, *way* better than\n>basic auth and can be put into place fairly easily by\n>adding support in new clients and servers.  For that\n>reason they are worth implementing even given the\n>existance of certificates.\n\nWell, I for one am *way* mollified by this statement. And\nI'll be not only mollified, but will do public penance for \nmy harsh words, if Mozilla and his seed declare unambiguously \nthat they _will_ implement Digest. \n\n--Michael Smith\n  ms@gf.org\n\n\n\n", "id": "lists-010-8922276"}, {"subject": "Re: Netscape vs. Digest (?", "content": "jg@zorch.w3.org wrote:\n> \n> \n> Lou Montulli writes:\n> > Why would you ever want to use digest if you already have\n> > certificate support?\n> \n> I think at least one reason is clear:\n> Lack of export control hassles on hashing for authentication.\n> \n> This means we can make it universal, and stop passwords in the clear\n> world-wide.  And as the #1 (and I think #2) servers on the Internet\n> are Apache and NCSA, which have no solution to the export problem\n> available to them\n\nIn fact, Apache does have a solution - originate the code outside the US and\nimport it. This has a major advantage over exported servers in that the\ncrypto is not crippled. This solution is, of course, Apache-SSL.\n\n> (as I understand it, the Apache folks had their arms\n> twisted to even remove hooks for stronger forms of encryption or\n> authentication),\n\nActually, it was the NCSA that had the arm-twist applied (by the NSA, I'm\ntold), and they advised the Apache Group to follow suit, which they did.\n\n> this is a Big Issue.  It is far from clear to me that\n> certificate support is universally available as a result of this\n> action of the government.  Even if the code were available worldwide, it can't\n> just get dropped into a server distribution.\n\nThis is true. If we were to drop crypto into the main Apache distribution, it\nwould prevent the distribution from being stored in the US (not really a big\ndeal) but worse, it would prevent US developers from working on Apache. This is\nwhy Apache-SSL is distributed separately, and developed entirely outside the\nUS by non-US nationals.\n\nBut even if there were no export hassles, certificate support is no substitute\nfor digest auth. The code is several orders of magnitude harder to write, and\nthe maintenance overhead for the user of certificates is also considerably\ngreater than maintaining digest auth.\n\nApache supports digest auth in a few hundred lines of code. Certificate support\ntakes tens of thousands of lines.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                  Phone: +44 (181) 994 6435\nFreelance Consultant and    Fax:   +44 (181) 994 6472\nTechnical Director          Email: ben@algroup.co.uk\nA.L. Digital Ltd,           URL: http://www.algroup.co.uk\nLondon, England.            Apache Group member (http://www.apache.org)\n\n\n\n", "id": "lists-010-8929911"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n>Servers can choose not to accept or request basic authentication. \n[....]\n>I think we're deluding ourselves if we think we can require \"MUST\n>implement\"; \"MUST implement\" doesn't belong in a protocol\n>specification: \"MUST send\", or \"MUST reply\" does.\n\nWith respect, this distinction seems a little labored. As I see it, \nBasic authentication is badly flawed from the point of view of the \n_function_ it is supposed to support. Protocols exist, and are specified, \nfor practical, functional reasons, not exercises in abstract logic, and \nit seems clear to me that for HTTP to achieve the purposes for which it \nis designed in a satisfactory way, we have to get away from Basic \nauthentication. So I strongly favor the MUST. \n\n--Michael Smith\n  ms@gf.org\n\n\n\n", "id": "lists-010-8940431"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "On Tue, 27 Aug 1996, Larry Masinter wrote:\n\n> Servers can choose not to accept or request basic authentication. As\n> has been pointed out in many cases, Basic authentication is as safe as\n> Digest if used in conjunction with some other one-time password system\n> (SKey, SecurID, etc.).\n> \n\nActually HTTP is very ill suited for use with one-time passwords,\nbecause every transaction is re-authenticated.  It is not impossible,\njust extremely cumbersome.  I seriously doubt that basic + one-time\npasswords will ever see widespread use.\n\nBasic authentication can be very useful under SSL, however.  In fact,\nthis is the one way that it is quite reasonable to tie authentication\nto a regular system password file.  That is maybe the only use of\nbasic authentication that digest authentication can't do better.\n\nThe drawbacks of basic + SSL are the export/patent hassles of SSL and\nthe overhead of SSL.  It has occurred to me that it wouldn't be hard\nto overcome the overhead problem with a kind of \"cookie\nauthentication\".  The initial transaction under SSL would do basic\nauthentication, issue a \"ticket\" in the form of a cookie and then\nredirect to the real (non-SSL) document.  The ticket would essentially\nbe an expiration time, the client's IP address, and perhaps the\n\"realm\", all signed by the server.\n\nThis would meet many of the objectives of digest, would be nearly as\nsecure.  Most importantly, it requires no new client support and no\nnew protocol support.  Don't get me wrong, I am still a strong\nadvocate of digest authentication -- we really need it.  I'm just\nthinking about a fall-back position.\n\nSpeaking of protocol change, one thing I would like to see at some\npoint is authenticating a *connection* rather than every transaction.\nOf course, we need per transaction authentication too.\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-8948170"}, {"subject": "Re: HTTP/1.1 + Diges", "content": ">The drawbacks of basic + SSL are the export/patent hassles of SSL and\n>the overhead of SSL.\n\nFor example, if I set up my Corporate Technical Memory document submission \nprocess to require SSL, it would (at least technically) be illegal for \nFrench TCE employees to submit documents, as they would be encrypted \nen-route and there is no general license to use SSL granted by the French \ngovernment.  This would be inside the TCE WAN -- not even out on the \nInternet.\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-8957593"}, {"subject": "Re: Netscape vs. Digest (?", "content": "Re Jim's note\n\nIts worth noting that the reason why BASIC made it into the\nspec in the first place was that Ari was worried by all the\nexport control hassles on crypto.\n\nPhill\n\n\n\n", "id": "lists-010-8965347"}, {"subject": "Re: digest vs basi", "content": "As larry has pointed out, basic for client / server non persistant requests\nis a poor choice.\n\nclient - proxy  with persistant connection between client and proxy \nwhen used with one time password systems ( as we do in our product) allows\nsites to authenticate strongly which of their users can do WEB stuff.\n\nBasic auth as a mechanism can very useful even if it is not what it was\nintended for.\n\nThere are no export restrictions for hash algs (MD4,5). crypto can be\nexported if used for authentication only and cannot be used for data\nencryption. (you ship binaries only.. no source.)\n\nPatent restrictions are a different matter...\n\nPete.\n-- \nThe TIS Network Security Products Group has moved again!\nvoice: 301-527-9500x111  fax: 301-527-0482\nRoom 334, 15204 Omega Drive, Rockville, MD 20850\n\n\n\n", "id": "lists-010-8973299"}, {"subject": "Re: digest vs basi", "content": "> As larry has pointed out, basic for client / server non persistant requests\n> is a poor choice.\n\nPlease, no. It has nothing to do with 'non-persistant' requests, it\njust has to do with multiple requests where state management is wanted\nbut the current mechanisms for state management (cookies, etc.) are\nnot used.\n\nLarry\n\n\n\n", "id": "lists-010-8980862"}, {"subject": "Re: digest vs basi", "content": "On Wed, 28 Aug 1996, Peter J Churchyard wrote:\n\n> As larry has pointed out, basic for client / server non persistant requests\n> is a poor choice.\n> \n> client - proxy  with persistant connection between client and proxy \n> when used with one time password systems ( as we do in our product) allows\n> sites to authenticate strongly which of their users can do WEB stuff.\n> \n\nThis sounds interesting.  But I am not sure whether you \n\n   (1) Authenticate a client only once for a persistent connection,\n\nor\n\n   (2) Authenticate each transaction (reusing the password), but use\n      a new password anytime there is a new connection.\n\nEither would seem possible.  \n\nIf it is (1) then strictly speaking you are probably not HTTP\ncompliant since you are essentially making the Proxy-Authorization\nheader \"sticky\".  But I see no reason that your proxy shouldn't\ninteroperate with HTTP clients.\n\nIf it is (2) then you aren't strictly using one-time passwords, as the\nsame password is re-used for each transaction, but you should have\nessentially all the benefits of one-time passwords.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-8988251"}, {"subject": "Digest Auth (fwd", "content": "Alexei Kosut (or someone doing a very good impression of him) asked me to\nforward this to the list, as he is having email problems:\n\nAlex Hopmann wrote:\n\n> I was actually speaking with the MSIE folks about this just today- They\n> removed the digest support because they couldn't find any servers to\n> test it against. They have assured me that it will be back in the next\n> version.\n\nBaloney - the latest versions of both Apache and NCSA's web servers (which\ntogether make up more than 40% of Internet web servers) support digest\nauth.\n\nThe problem here may be that no one actually *uses* digest auth. The\nproblem is that these servers don't let you use both together. This is\nbecause both servers (indeed, pretty much all Unix HTTP servers that I\nknow of) store Basic passwords crypted. This makes them unusable for\nDigest auth's purposes, which either needs the passwords in the clear or\nhashed. So the vast installed base of installed authentication cannot use\ndigest (except in specific, intranet-like cases, where you are assured\nthat the user is capable of supporting digest auth).\n\nIn addition, the architecture of both servers make it so that they cannot\nsupport more than one authentication scheme at the same time - so you\ncannot maintain seperate password files for each, one crypted and one\nhashed.\n\nThis may help to explain why it hasn't taken off, even though it's been in\na majority of WWW servers for several months. No one uses it on their\nservers, therefore no clients want to take the time to implement it.\n\n(FWIW, now that I've thought of it, I may make the upcoming Apache 1.2\nsupport both basic and digest auth at once (though not for existing\npassword databases, unfortunately, which would of course be ideal, but as\nI've mentioned, they're crypted), possibly easing the hopeful transition\nfrom digest to basic auth.)\n\n-- Alexei Kosut <akosut@organic.com>            The Apache HTTP Server \n   http://www.nueva.pvt.k12.ca.us/~akosut/      http://www.apache.org/\n\n\n\n", "id": "lists-010-8996616"}, {"subject": "Re: Digest Auth (fwd", "content": "On Thu, 29 Aug 1996, Ben Laurie (really Alexei Kosut) wrote:\n\n> \n> The problem here may be that no one actually *uses* digest auth. The\n> problem is that these servers don't let you use both together. This is\n> because both servers (indeed, pretty much all Unix HTTP servers that I\n> know of) store Basic passwords crypted. This makes them unusable for\n> Digest auth's purposes, which either needs the passwords in the clear or\n> hashed. So the vast installed base of installed authentication cannot use\n> digest (except in specific, intranet-like cases, where you are assured\n> that the user is capable of supporting digest auth).\n> \n\nI don't understand this.\n\nAs you observe server support for digest auth is widely available.\nThe reason no one uses it is because it is not supported by Netscape\nor MSIE -- period.  As long as this remains the case digest will never\nbe widely used.  All other pros and cons for digest are pretty much\nirrelevant at this point.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n", "id": "lists-010-9005734"}, {"subject": "Re: Digest Auth (fwd", "content": ">The problem here may be that no one actually *uses* digest auth. The\n>problem is that these servers don't let you use both together. This is\n>because both servers (indeed, pretty much all Unix HTTP servers that I\n>know of) store Basic passwords crypted. This makes them unusable for\n>Digest auth's purposes, which either needs the passwords in the clear or\n>hashed. So the vast installed base of installed authentication cannot use\n>digest (except in specific, intranet-like cases, where you are assured\n>that the user is capable of supporting digest auth).\n\nThis is unfortunate. The design of DIGEST deliberately made it possible\nto share a database for both purposes - if absolutely necessary. No \nserver should ever be storing the passwords used by DIGEST, all that is \nnecessary is the one way function hashed key.\n\nThe one way hash used by DIGEST is much stronger than that used by the\nUNIX password format. There is no cryptographic reason to prefer the\nUNIX format.\n\n\nThe reason why nobody is using DIGEST is because of clients which do \nnot.\n\nPhill\n\n\n\n", "id": "lists-010-9014464"}, {"subject": "Re: Digest Auth (fwd", "content": "On Thu, 29 Aug 1996, John Franks wrote:\n\n> As you observe server support for digest auth is widely available.\n> The reason no one uses it is because it is not supported by Netscape\n> or MSIE -- period.  As long as this remains the case digest will never\n\nI think we have a bit of a chicken and egg problem. Server software is\navailable which is claimed to support digest but it is not activated by\nreal installations because the majority of users don't have clients which\nwill support it *AND* apparently the server installation can't, at least\neasily, support basic and digest concurrently. On the otherhand, the\nclient authors can't find a set of easy places to test their code with\nso the clients don't include the code.\n\nIn my experience, it is very rare for a development organization to have\nthe resources to do everything the know to be right to do. I also find\nthat the QA groups are even more stressed. In particular, the QA group\nmay have learned how to test the browser but don't have the skills to\nset up many varieties of servers. I would expect that testing would\nfocus on a very few servers and then depend on WWW deployed servers for\nsurface verification with others. After all, if http is an interoperable\nprotocol, it should be sufficient to test with one server.\n\nThis group is a bit two faced. A couple weeks a go, a prominant member was\nchastising folks who might be publishing a server and calling it HTTP/1.1\nbefore the very stable draft is really approved by the IETF. Now we\nare complaining because one or more other software publishers chose not\nto deliver software matching a spec about which discussion had gotten\nvery hot and might be expected to be an unstable implementation target.\n\nC'mon folks we can't have it both ways!\n\nI think there is some obvious room here for W3C activity in the form of\nfacilitating the testing of client and server implementation of\ndigest.\n  1.  Bring up and make 'public' a copy of each of the servers which\n      claim to implement digest, with digest active of course.\n  2.  I believe there is at least one publicly available client which\n      also claims to support digest. Help any interested developers\n      install and use such clients.  If necessary because of platform\n      difficulties, use the client against a developers server.\n  3.  Do both of the above with appropriate diagnostic tools such as\n      sniffers available to facilitate diagnosis of the failure to\n      interoperate.\n  4.  Provide some level of consulting services to help with problem\n      determination.\n\nOf course, this is still difficult because I would expect that most\ndevelopers and QA groups are behind firewalls which might not be real\nfriendly to such testing.\n\nOne could hope that the server and client teams within a organization \nmight cooperate but I would guess that the product manager at one\npublisher for the server might have different priorities from the \nclient in the same organization. Surely the client needs the support\nat least a generation sooner than the server. And in any case, I \nwould worry that just because AAclient works with AAserver, they\nmay not operate with BBserver or BBclient.\n\nSo in summary, if we as a WG think deployment of digest is important, then\nI think we need to forget about the political implications of SHOULD\nvs MUST and somehow forcing publishers to do it our way and figure out\nhow to facilitate the environment needed for publishers to successfully\ndevelop, test, and ship digest enabled software.\n\nDave Morris\n\n\n\n", "id": "lists-010-9022101"}, {"subject": "Re: HTTP/1.1 + Diges", "content": "At 3:41 PM -0700 8/27/96, Larry Masinter wrote:\n>Writing MUST instead of SHOULD in the specification is not any way to\n>force some vendor to either implement or not implement something.\n\nI strongly disagree. Noncompliance to a spec is a very big marketing force.\n\n>The\n>spec should say what makes sense, not what is politically\n>expedient.\n\n-It makes sense to stop sending passwords in the clear.\n-It makes sense to have *a single way* to stop sending passwords in the clear.\n-It makes sense to require that any browser that allows a user to send a\npassword allow that user to send it securely.\n-It is not politically expedient to try to force the largest manufacturer\ndo anything. This argument is a (dare I say it?) red herring.\n\n>We should write \"MUST\" if non-compliance causes systems to\n>break.\n\nThe IESG has told us to consider security part of the system. Sending\npasswords in the clear breaks the security system.\n\n>I think there are too many \"MUST\"s in HTTP/1.1, but agreed to wait\n>until the review for \"Proposed\" -> \"Draft\" to review them. I don't see\n>any reason to add one here, though.\n\nI disagree, and it seems like the rough consensus so far is to use MUST.\n\n>As\n>has been pointed out in many cases, Basic authentication is as safe as\n>Digest if used in conjunction with some other one-time password system\n>(SKey, SecurID, etc.).\n\nHuh? It has been pointed out that Basic authentication does *not* work well\nwith OTP because every transaction is re-authenticated.\n\n>I think we're deluding ourselves if we think we can require \"MUST\n>implement\"\n\nI strongly disagree.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-9032973"}, {"subject": "FYI, resolution of &quot;Digest&quot; issu", "content": "I sent the following note to the area directors, as my reading of the\nintent of the working group and the current status:\n\n================================================================\n\nDate: Thu, 29 Aug 96 09:47:50 -0700\nTo: Harald.T.Alvestrand@uninett.no\nCC: moore@cs.utk.edu,dsr@w3.org,jg@w3.org\nIn-reply-to: <8467.841303987@domen.uninett.no>\n(Harald.T.Alvestrand@uninett.no)\nSubject: digest authentication summary\nFrom: Larry Masinter <masinter@parc.xerox.com>\n\nI missed the message, but saw some of the replies.\n\nI think the consensus of the working group is that Digest\nAuthentication is a part of HTTP/1.1. Fortunately, what people in the\nWG were talking about (\"Must implement digest if they implement\nbasic\") is actually moot, since \"implement\" can mean the \"null\nimplementation\", if you peer at draft-ietf-http-digest-aa-04.txt.\n\nSo, I think we should just go ahead with the two documents; it would\nbe convenient to include in section 11.2 a simple statement that\n\n\"The HTTP/1.1 protocol includes the Digest Access Authentication,\nwhich is described in RFC XXXX.\"\n\nI think this captures the intent of the working group and what it is\nthat got voted on in last call.\n\nI think we should instruct the RFC editor to change the title of\ndraft-ietf-http-digest-aa-04.txt from\n\n\"A Proposed Extension to HTTP: Digest Access Authentication\"\n\nto\n\n\"Digest Access Authentication for HTTP\"\n\nI'm sorry for causing a flap by forwarding the note to the working\ngroup instead of just responding directly. Actually, I think I may\nhave mainly been trying to SHARE THE GOOD NEWS that both had been\nvoted on by the IESG.\n\nLarry\n\n\n\n", "id": "lists-010-9041723"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": ">I think the consensus of the working group is that Digest\n>Authentication is a part of HTTP/1.1. Fortunately, what people in the\n>WG were talking about (\"Must implement digest if they implement\n>basic\") is actually moot, since \"implement\" can mean the \"null\n>implementation\", if you peer at draft-ietf-http-digest-aa-04.txt.\n\nLarry, I think you owe the HTTP WG an explanation of your \"null\nimplementation\", since it was never mentioned in the WG discussion. In my\n\"peering\" at the draft, I see nothing that has the word \"null\" in it and\ndo not understand what you mean.\n\nI'm not trying to be contentious, but I think it's inappropriate for you to\ndeclare a WG discussion moot in a message to the IESG when you as WG Chair\nnever said so on the WG list. Further, this kind of action is especially\ninappropriate when the WG chair is in the minority of the rough consensus,\nas I feel is the case here. Regardless of individuals' motivations, most\nfolks who spoke up (and there were plenty) agreed that\ndigest-authentication should be a MUST in the spec.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n", "id": "lists-010-9052499"}, {"subject": "Re: Netscape vs. Diges", "content": "Daniel DuBois:\n|I have only said that indications I've received imply Netscape needs to be\n|coaxed to support Digest, because they, as of recently, had no plans to\n|support it.\n\nPlans not to support it, rather.  And the statement was made by the NS\nstandards cop, not marketing exec (although I'm not sure the difference 'tween\nthe two is measurable).\n\nIf there's anything more unpleasant than advertising suspect claims of\nstandards conformance to sell products (We Support HTML 3.0!), its holding your\nbreath until blue in the face so that those standards to conform to planned\nnull implementations.\n\nThe technical case for (digest && basic) == MUST is overwhelming.  Requiring\nthis gaping BASIC security hole be plugged with (at least) digest does not\nprivilege any one product over another.  The political advance if digest is\n'musted' is on behalf of the users and ensuring that our privacy can be\nenhanced across interoperable implementations.\n\n-marc\n\n-- \n\n\n\n", "id": "lists-010-9062080"}, {"subject": "Re: Digest Auth (fwd", "content": "> This group is a bit two faced. A couple weeks a go, a prominant member was\n> chastising folks who might be publishing a server and calling it HTTP/1.1\n> before the very stable draft is really approved by the IETF. Now we\n> are complaining because one or more other software publishers chose not\n> to deliver software matching a spec about which discussion had gotten\n> very hot and might be expected to be an unstable implementation target.\n> \n> C'mon folks we can't have it both ways!\n\nYou are missing the point.  Digest can and should have been implemented\nin HTTP/1.0 as the experiment that it was -- whether or not it is stable\nonly affects the allocation of limited resources.  In contrast, we are\nusing the label \"HTTP/1.1\" to indicate minimum compliance to a specific\nproposed standard, and you cannot indicate minimum compliance to something\nwhich is still subject to change.  Any future change to HTTP's minimum\ncompliance will now require a change to the HTTP version, since that\nis how the version stuff is supposed to work. My concern is that the WG\nas a whole needs to understand the meaning of HTTP-version, and when\nthe version number should change, since that understanding is central\nto the protocol's extensibility.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-9069783"}, {"subject": "Re: Digest Auth (fwd", "content": "> > This group is a bit two faced. A couple weeks a go, a prominant\n> > member was chastising folks who might be publishing a server and\n> > calling it HTTP/1.1 before the very stable draft is really approved\n> > by the IETF. Now we are complaining because one or more other\n> > software publishers chose not to deliver software matching a spec\n> > about which discussion had gotten very hot and might be expected\n> > to be an unstable implementation target.\n> >\n> > C'mon folks we can't have it both ways!\n> \n> You are missing the point.  Digest can and should have been\n> implemented in HTTP/1.0 as the experiment that it was -- whether\n> or not it is stable only affects the allocation of limited resources. \n\nI disagree, because of the nature of \"experimental\" features.  The\nparticular case we're talking about (I believe) is the case where\nDigest was implemented, and pulled because the spec showed signs \nof destabilization.  With the final release of the servers in \nquestion rapidly approaching, we decided it would be better to play\nit safe and remove support until the spec was stable than to keep\nthe support in and saddle everyone with an experimental \nimplementation for a long time.  If a spec shows signs of\ninstability, and a product is scheduled to ship a final release,\nit is not prudent to release an experimental feature in a release\nproduct.  Haven't specs gotten bit by experimental features with\nlarge user bases before?  Currently I'm thinking of the Host:\nheader, which I believe was appending the port number in Navigator,\nand some discussion came up to remove the port number.  Even if\nthe spec did change, now experimental behavior has to be expected\nand dealt with because users will be sending it.  We decided to be\nprudent and wait for the spec to calm down rather than etch \nexperimental behavior into a final release.  (Note that I can only\nspeak for the server side.)\n\n> In contrast, we are using the label \"HTTP/1.1\" to indicate minimum\n> compliance to a specific proposed standard, and you cannot indicate\n> minimum compliance to something which is still subject to change.\n> Any future change to HTTP's minimum compliance will now require a\n> change to the HTTP version, since that is how the version stuff is\n> supposed to work. \n\nThe point I would like to make is that non-finalized specs are \nrisky for a final release of a product.  The situation does bear\nsome similarity in that this is a case where a released product would\nbe advertising that it does features involved in a non-finalized\nspec, and if the spec changes, the product becomes non-compliant.\nSimilarly, if we had released an implementation of digest auth\nat the time when we had it done, and the spec had changed, then\nwe would become non-compliant with the later drafts, and that\nbehavior would have to be dealt with for interoperability.  We\nchose not to risk being the black sheep.  As to why we haven't\nrevisited that decision after it happened, then it's more of a\nresource issue, which will undoubtedly be looked at again for\nthe next release.\n\n> My concern is that the WG as a whole needs to understand the\n> meaning of HTTP-version, and when the version number should change,\n> since that understanding is central to the protocol's extensibility.\n\nMy concern is simply that I think if Digest goes to a must, then\nit should be done for the right reasons.  People have brought up\nvalid reasons why it should be a must.  I don't think it's right\nto make it a must in order to force people who would not otherwise\nimplement digest auth to implement it.  Using that argument weakens\nan otherwise strong argument by making it look political.\n--MLM\n-- \n      ---- mlm@netscape.com * http://www.netscape.com/people/mlm/ ----\n\n\n\n", "id": "lists-010-9078702"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "> Roy and I have disagreed in public for some time about what is \"safe\".\n> By now, it's clear that we will never agree.  And we both believe that\n> the other is using a completely bogus definition.\n> \n> My definition \"safe\", as it applies to caching, is that there must\n> never be situation where a stale result is presented to a user as\n> if it were fresh.\n> \n> Roy's definition of \"safe\" appears to be \"don't waste any opportunities\n> for caching, because this could cause additional network traffic.\n\nNonsense -- that is a foolish trivialization of my position.  The term\n\"safe\", as it applies to software, is fully defined in the paper\n\n        Nancy G. Leveson, \"Software Safety: What, Why, and How\",\n        ACM Computing Surveys, 18(2):125-163, June 1986.\n\nand a host of other Software Engineering texts.  It is hardly my fault\nthat you are focusing on one potential hazard at the exclusion of all\nothers, and thus your use of the term \"safe\" to describe your\ninterpretation of Age is just plain wrong.\n\nTrying to be \"safe\" is an attempt to minimize the potential for harm given\nthe presence of any number of hazards.  Although in extremely rare cases\nit might be unsafe to view a stale document (the hazard being that the\ndocument on the origin server has changed and the user will be harmed\nby not seeing the difference between the two), it is almost always unsafe\nto disable caching on a congested network (the hazards being that users\nwho pay for bandwidth usage will be charged extra, and that when many\nsimultaneous requests are made on a line with limited bandwidth the\nconnection start-up overhead dominates the link and starves the requests\nfrom completion).\n\n> I'd question his use of the term \"catastrophic network failure\",\n> especially since we've seen ample evidence on this mailing list during\n> the past week or so that most HTTP caches would be hard-pressed to\n> get hit rates above 40% to 50%, even if they were to cache EVERYTHING\n> that didn't have a question mark or \"cgi-bin\" in the URL.\n\nOh please, caching is most effective during peak periods, and it is only\nduring peak periods that bandwidth is a concern.  Even at 30% hit rates,\ncaching can make the difference between a working network and one that\nis clogged with connection requests.  This isn't just theory -- we have\nseen it on the UK link!  HTTP/1.1 was designed with the anticipation that\nsuch situations will be more frequent in the future, which is why I spent\nso much time explaining hierarchical cache techniques to people who think\nevery HTTP connection goes straight to the origin.  In any case, judging\nthe effectiveness of HTTP/1.1 caching based on the limitations of HTTP/1.0\ncaches is not very conclusive.\n\n> Since I'm not going to change Roy's mind, I'll limit myself to\n> correcting the factual mistakes in his message, and I won't\n> continue to rehash the \"safety\" argument.\n> \n>     But you are completely overlooking what happens if ANY of the\n>     intermediaries has a system clock which is out-of-sync with the\n>     origin.  If ANY of them do have a bad clock, they will pass the\n>     additional bogus age on to every single request that passes through\n>     the proxy.\n> \n> The algorithm in section 13.2.3 is specifically designed to handle\n> the \"one bad clock\" (or even \"N bad clocks\") case.  It does this\n> using the following two steps:\n>       apparent_age = max(0, response_time - date_value);\n>       corrected_received_age = max(apparent_age, age_value);\n> (see the draft for the definitions of the variables).  Because\n> of the two max() operations, this will NEVER result in underestimating\n> the Age, as long as at least one of the clocks in the chain is correct.\n\nThe change that I suggested would also prevent underestimating\nthe age provided that either\n\n   a) The content did not come from an HTTP/1.0 proxy, or\n   b) The user agent and origin server have good clocks.\n\nAgain, the result will always be accurate if the proxies are HTTP/1.1.\nSince proxies will be the first to change to HTTP/1.1, my interpretation\nof the Age algorithm will eventually be completely safe.  In contrast,\nyour interpretation will always be unsafe, even when HTTP/1.0 is gone.\n\n> Yes, it may overestimate the Age value, if one or more of the clocks is\n> badly out of sync.  If you agree with Roy that overestimation is evil,\n> then I suppose the word \"bogus\" is appropriate here.  Otherwise, it's\n> exactly what the algorithm is intended to do.\n\nNo, the algorithm is intended to provide an accurate minimum of the\nage given the presence of any bad clocks.  Forcing all caches to add\nAge to an entity that hasn't been aged by them does result in the\nalgorithm being dependent on EVERY SINGLE CLOCK on the response path.\nThat breaks Age and introduces a security hole to any large network\nwhich depends on caching to reduce their bandwidth requirements.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-9089988"}, {"subject": "Re: Digest Auth (fwd", "content": "http://wilson.ai.mit.edu/cl-http/authentication/authentication.html\n\nThere are live examples of Digest authentication to test your client.  \nThese have been freely available for a year with reference implementation in a high-level language.\n\nWe haven't checked the lastest version of the digest draft to make sure\nwe handle all new additions, but we'll do so soon and drop in any\nneeded patches.  Any shortcomings are fixed in realtime when we have a minute\nor two.\n\nOur 1.1 testers have been very helpful and they seem to think we have\nas well.\n\nBTW, it would be desirable to select a better algorithm than MD5 like SHA\nat the earliest convenience..\n\n\n\n", "id": "lists-010-9102824"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "> History has shown that getting bad data in a cache has had major\n> headaches associated with it.\n\nUmmm, excuse me, but history has only shown that caches which cannot\nknow whether or not they have received the whole message, due to the\nlack of a content length or chunked encoding, are susceptible to storing\nincomplete responses (bad data).  What Jeff is talking about is stale\ndata (data which was known to be good at some prior time, and may simply\nbe out-of-date).\n\n> I believe we should stay with Jeff's conservative algorithm, and this is why\n> I have not to date made any changes in the specification; I will of course\n> go with the consensus of the group, but to date, I've not seen\n> a good case made by Roy.  Not surprising users is a very good thing;\n> if the web is unreliable, it will generate many more problems than whatever\n> small performance gain might be available with a more optimistic algorithm.\n\nI'm sorry, but this is incredible.  I demonstrated how the entire\nnetwork of New Zealand (3.6 million people and the highest tech ratio\nof any nation) can be completely disabled by Jeff's \"conservative algorithm\".\nWould it be more compelling if I explained how someone could do the same to\nDigital's corporate network by spoofing NTP responses to the proxy outside\nthe firewall?\n\nThere is no way that I will implement Jeff's notion of having every\nproxy add an Age header -- since I have documented the alternatives,\nimplementing Jeff's mechanism would subject me to liability every time\nsome idiot plays network ass-of-the-day.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-9110695"}, {"subject": "Re: Digest Auth (fwd", "content": ">> [...] Digest can and should have been\n>> implemented in HTTP/1.0 as the experiment that it was -- whether\n>> or not it is stable only affects the allocation of limited resources. \n> \n> I disagree, because of the nature of \"experimental\" features.  The\n> particular case we're talking about (I believe) is the case where\n> Digest was implemented, and pulled because the spec showed signs \n> of destabilization.  With the final release of the servers in \n> question rapidly approaching, we decided it would be better to play\n> it safe and remove support until the spec was stable than to keep\n> the support in and saddle everyone with an experimental \n> implementation for a long time.  If a spec shows signs of\n> instability, and a product is scheduled to ship a final release,\n> it is not prudent to release an experimental feature in a release\n> product.  Haven't specs gotten bit by experimental features with\n> large user bases before?  Currently I'm thinking of the Host:\n> header, which I believe was appending the port number in Navigator,\n> and some discussion came up to remove the port number.  Even if\n> the spec did change, now experimental behavior has to be expected\n> and dealt with because users will be sending it.  We decided to be\n> prudent and wait for the spec to calm down rather than etch \n> experimental behavior into a final release.  (Note that I can only\n> speak for the server side.)\n\nWe don't disagree -- that is what I meant by experiment.  I have no\nproblem with Netscape's decision not to include it in their released\nproducts until there is a stable spec.  However, I do hope that you\n(and everyone else) have continued to experiment and thus that you\nwill be ready to release a completed implementation, based on the\nnow final draft, as soon as possible.\n\nMy disagreement was with Mr. Morris' suggestion that this is the\nsame as the \"HTTP/1.1\" labelling issue; it is not.  Digest (as a draft)\nis now just as stable for HTTP/1.0 as it is for HTTP/1.1.\n\nI personally do not care whether Digest is a MUST or not -- people will\nimplement it because doing so results in a better product.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-9120536"}, {"subject": "Re: Confusion about Age: accuracy vs. safet", "content": "At 11:56 AM 8/23/96 PDT, Larry Masinter wrote:\n>We've heard from Roy, Jeff, and Koen. What say others? I'd especially\n>like to hear from those folks who are currently implementing HTTP/1.1\n>proxies.\n    \nWell, we're not working on a 1.1 proxy at the moment, but we will in a\nmatter of weeks.\n\nI vote:\n     - MUST add Age when serving from cache memory\n     - SHOULD add Age when relaying a response from a 1.0 proxy\n     - SHOULD NOT add Age when relaying a response directly from a 1.0 origin\n       server\n     - SHOULD NOT add Age when relaying a response from a 1.1 or\n       higher source\n\n-----\nDaniel \"Well, I haven't worked at Cambridge yet\" DuBois\nTraveling Coderman         -- NEW! http://www.spyglass.com/~ddubois/ -- \n\n\n\n", "id": "lists-010-9130500"}, {"subject": "Question on ChunkSize sect 3.6, 8/12/9", "content": "Clever implementations of chunked transfer encoding will do it\nby using the tcp buffer directly, thereby eliminating the need to\ncopy to another buffer and reducing the working set.\n\nThis is accomplished by reserving space at the beginning of a chunk for\nthe size and any chunk arguments, and then filling them in before transmitting\nthe buffer. Most simply, this requires using come padding characters before\nor after the hex chunk size.\n\nThis has been achieved under the following operating systems for\nCL-HTTP: MAC OS, UNIX, WindowsNT, Lisp Machine.\n\nWhereas I had the impression from reading earlier drafts that trailing \nwhitespace was a feasible for padding, one of our users has pointed out \nthat the current implementation does not match the description in section \n3.6 of the august, 12, 1996 draft.\n\nHowever, at first blush, the spec seems cleverly engineered to prevent \nusing any padding, front or back. \n\nBUT, in section 2.1 where implied *LWS is discussed, it suggests that\nwhitespace may appear between  and delimiters without changing the interpretation of a field.\n\nThus, hex-no-zero *lws CRLF would be legal. \n\nDoes this mean that padding the chunk-sized with spaces on the right is\nlegal in the current http 1.1 spec?\n\nIf so, section 3.6 should be updated to make this clear.\nIf not, section 3.6 should be amended to correct this mistake.\n\nAll the known 1.1 clients that have tested against the server (2 W3C clients\nand one from Switzerland) have been able to deal fine with trailing\nwhitespace before a \";\" or the CRLF\n\nI think this mistake should be corrected as quickly as possible.\n\nIn the event that this mistake cannot be corrected swiftly, we are\nprepared to employ the following workaround, which although legal,\nis quite likely to break current 1.1 clients that do not handle chunk-size\narguments.\n\nhex-no-zero \";\" *lws CRLF\n\nor one could imortalize it with\n\nhex-no-zero \";\"fyrCRLF\n\nConstructive suggestions? \n\n\n\n", "id": "lists-010-9138413"}, {"subject": "Small Detail on Chunked Encodin", "content": "For those who are interested, you will note that the chunk size\nemitted by the cl-http server varies.  This is because the lisp machine\nhas an adaptive TCP implementation that identifies optimal packet\nsizes based on TCP factors.  The chunk sizes vary because chunking is \nbuilt on the TCP buffer, which is the TCP packet to send. \nThe server selects chunk sizes 2 less than the packet size to allow\nthe CRLF to be appended at the end of the packet, thus delivering a chunk\nevery TCP packet.\n\nOne would imagine that this kind of implementation will achieve the\nbest throughput rates for a given platform and network route.\n\n\n\n", "id": "lists-010-9148100"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": "Paul, there is more here (and has been going on behind the scenes) than\nyou know.  I'll try to summarize the situation as I see it from the mail\nthat has been flowing between the area directors, Larry and myself.\n\nThe basic problem is a procedural one (that Keith Moore, Harald\nAlvestrand, Larry and I might be held responsible for): last calls\nwere issued on the documents independently, and they are not written\nto interdepend in appropriate ways.  With 20-20 hind sight, we should\nhave issued a single last call with covering explanation, and done the\nwordsmithing slightly differently.\n\nThe IETF and IESG get very upset if any normative requirements (i.e. MUSTs,\nSHOULDS, etc.) change between the final draft and the RFC; after all,\nthat is what is Last Call is all about.  Any changes should be entirely of\nan editorial nature.\n\nSo, we have to ways to fix this.\n\n1) Issue new drafts for both documents, and have to issue a new IETF\nlast call.  Once that is done, the IESG does its thing.  This would\ntake a month or two. (or three)....\n\n2) Have the IESG immediately issue the RFC's for both HTTP/1.1 and\nDigest.  Go ahead immediately with issue a separate (about 1 page) draft\nthat states the normative requirements we all intend(ed), forward to the\narea directors instantly, and do IETF last call on it.\nThis would take a month or two. (or three)....\n\nBoth 1) and 2) take the same length of elapsed time to the same\ndesired end state.  1) causes us to wait months more for any IESG\naction (i.e. we don't get RFC's immediately), and 2) gets us the announcement\nof the IESG action of last Thursday approving both our existing\ndocuments issued, and then the tiny RFC gets us the normative requirements\nwe want at the same elapsed time that 1 would have taken.\n\nSo the right outcome in our minds (Larry, myself, Keith, and Harald)\nis 2), so that more time does not elapse for proposed standard on HTTP/1.1. \n\nBy going ahead with the existing documents we get Proposed standard immediately,\nwithout taking significantly longer elapsed time for the requirements we've\nbeen discussing.\n\nLarry didn't make clear in his mail what our options are procedurally, in\nthe current process state.\n\nAnd we'll pull all this together into the Draft version of the document(s),\nso that this strange short RFC we'll end with won't matter thereafter.\n\nHopefully, this will clarify the situation to everyone.\n\nAren't standards processes fun? :-)\n- Jim\n\n\n\n", "id": "lists-010-9155696"}, {"subject": "Re: Question on ChunkSize sect 3.6, 8/12/9", "content": "> Clever implementations of chunked transfer encoding will do it\n> by using the tcp buffer directly, thereby eliminating the need to\n> copy to another buffer and reducing the working set.\n> \n> This is accomplished by reserving space at the beginning of a chunk for\n> the size and any chunk arguments, and then filling them in before transmitting\n> the buffer. Most simply, this requires using come padding characters before\n> or after the hex chunk size.\n\nWhen I proposed the chunked encoding to this WG (Dan Connolly originally\nsuggested it on www-talk more than a year before that), I wanted a fixed\nchunk size (and no CRLF's) for the same reason.\n\n> However, at first blush, the spec seems cleverly engineered to prevent \n> using any padding, front or back. \n> \n> BUT, in section 2.1 where implied *LWS is discussed, it suggests that\n> whitespace may appear between  and delimiters without changing the interpretation of a field.\n> \n> Thus, hex-no-zero *lws CRLF would be legal. \n\nUmmm, no, that would be bad because LWS may include CRLF.  Under\nthe circumstances, the spec could say\n\n       chunk          = chunk-size [ chunk-ext ] *(SP | HT) CRLF\n                        chunk-data CRLF\n\nand explicitly disallow any other LWS between chunk-size and chunk-data CRLF\n(as it does for the other cases where CRLF is acting as a delimiter).\n\n> Does this mean that padding the chunk-sized with spaces on the right is\n> legal in the current http 1.1 spec?\n\nWell, it isn't illegal, but that won't help interoperability any.\n\n> If so, section 3.6 should be updated to make this clear.\n> If not, section 3.6 should be amended to correct this mistake.\n\n*sigh*, I hope someone is making a list of these.\n\n> All the known 1.1 clients that have tested against the server (2 W3C clients\n> and one from Switzerland) have been able to deal fine with trailing\n> whitespace before a \";\" or the CRLF\n\nThat's how I'd implement it as well, just for the sake of robustness.\n\n> I think this mistake should be corrected as quickly as possible.\n\nNot possible without issuing another draft.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-9166610"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": "Do we presently have enough experience with 1.1 proxy implementations and how\nthey interoperate with the installed base to know that changes won't be required\nin the proxy related aspects of the current draft? \n\nI suppose additional patch/errata RFCs could be issued and later consolidated.\n\n\n\n", "id": "lists-010-9176871"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": "> Do we presently have enough experience with 1.1 proxy implementations and how\n> they interoperate with the installed base to know that changes won't be\n> required in the proxy related aspects of the current draft? \n\nNo.  But this is why the standards process requires additional review\nbefore a document can move from Proposed to Draft, or from Draft to Full \nStandard.  \n\nAssuming the document stays on the standards track, there will be at \nleast two opportunities to make changes in light of experience before\nit is fully approved.\n\nKeith\n\n\n\n", "id": "lists-010-9185947"}, {"subject": "resolution of digest issu", "content": "Okay, it looks like we have resolution.  We can't impose new requirements\non the protocol (beyond those already written in the documents) without \nanother Last Call and IESG ballot, and we don't want to delay HTTP 1.1 \nany longer.  Therefore:\n\n1. I'll ask the IESG Secretary to immediately issue the HTTP 1.1 and \ndigest protocol actions (declaring that each is approved for Proposed \nStandard status) and send the HTTP 1.1 and digest documents to the \nRFC Editor with the following note:\n\n   NOTE TO THE RFC EDITOR\n   \n   draft-ietf-http-v11-spec-07 was written without knowing\n   whether draft-ietf-http-digest-aa-* would be approved as\n   a Proposed Standard.   Since both documents were approved at\n   the same time, each may need modifications to make appropriate\n   references to the other.\n\n   Notes to the RFC Editor in the HTTP 1.1 draft (in section\n   11.2 and 19.8.4) leave the decision to the RFC Editor to\n   combine both drafts into one document or to keep them\n   separated.  Assuming the RFC Editor will prefer the latter,\n   it is suggested that section 11.2 of draft-ietf-http-v11-spec-07 \n   be replaced with the following text:\n\n   \"The Digest Authentication Scheme, described in RFC XXXX, may also\n   be used with HTTP/1.1.\"\n\n   where \"XXXX\" is the RFC number assigned to draft-ietf-http-digest-aa-04.txt\n\n2. The HTTP WG should decide whether to issue a brief applicability \nstatement which specifies requirements (and/or recommendations) of \nHTTP 1.1 implementations with respect to digest authentication.  \nIf and when the WG produces such a document, it will require a \nLast Call and IESG approval just like any other standards-track document.\n\nThanks for your patience,\n\nKeith\n\n\n\n", "id": "lists-010-9195464"}, {"subject": "Re: Question on ChunkSize sect 3.6, 8/12/9", "content": "At 7:56 PM -0700 1996-08-29, Roy T. Fielding wrote:\n>  Under the circumstances, the spec could say\n>\n> #1       chunk          = chunk-size [ chunk-ext ] *(SP | HT) CRLF\n>                        chunk-data CRLF\n>\n>and explicitly disallow any other LWS between chunk-size and chunk-data CRLF\n>(as it does for the other cases where CRLF is acting as a delimiter).\n>\n\nYour description would require the \";\" to immediately follow chunk-size \nand padding to appear between  the \";\" and CRLF.\n\nThe present known server and client implementations are handling this:\n\n#2       chunk          = chunk-size *(SP | HT) [ chunk-ext ] CRLF\n                        chunk-data CRLF\n\nAlthough one reference client doesn't parse [ chunk-ext ], and so\nmight need a small hack to detect \";\" and skip *(SP | HT) to the CRLF.\n\nAnother alternative could be:\n\n#3       chunk          = chunk-size *(SP | HT) [ chunk-ext ] *(SP | HT) CRLF\n                        chunk-data CRLF\n\nThis would seem most robust, if a little longer.\n\n\n\n", "id": "lists-010-9203875"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": "If there are two more opportunities for changes it would seem to\nme to be a good thing(TM) to keep a record of all the problems,\nperceived errors etc as we go along\n\nJust a thought... Would have raised it sooner but thought best\nget the royal assent before making the suggestion.\n\nPhill\n\n\n\n", "id": "lists-010-9212601"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": "> Larry, I think you owe the HTTP WG an explanation of your \"null\n> implementation\", since it was never mentioned in the WG discussion. In my\n> \"peering\" at the draft, I see nothing that has the word \"null\" in it and\n> do not understand what you mean.\n\nWell, I thought it was pretty self-explanatory, but I'll elaborate\npublicly:\n\nIf you look at draft-ietf-http-digest-aa-04.txt, it doesn't actually\nput many requirements on either clients or servers.  It says \"a server\nmay assume Digest support when a client identifies itself as HTTP/1.1\ncompliant.\" So servers assume digest support. It says \"a client is\nencouraged to fail gracefully if the server specifies any\nauthentication scheme it cannot handle.\"\n\nSince this is the language that we accepted through last call, it is\nprobably as strong a compliance statement we can make at this point,\nwithout going back through another\nrevision. draft-ietf-http-digest-aa-04.txt does NOT say that clients\nMUST prompt the user for authentication, or that clients MUST handle\ndigest authentication, all it says is that clients are encouraged to\nfail gracefully (whatever that means) if it doesn't understand\ndigest. It does say \"A server may assume Digest support\", but it's not\nclear what force that has on the clients. I suppose you could read \"a\nserver may assume X support\" as another way of specifying \"a client\nmust support X\".\n\nThis issue has arisen recently in the review of the MIME drafts of the\nquestion of whether any mail clients 'support'\nmultipart/parallel. There are a lot of clients that 'support'\nmultipart/parallel by just treating it as multipart/mixed. Is that\n'support'?\n\nIn any case, more tweaking of the language of the spec is not going to\nbe allowed without reopening one or both drafts in the working group.\nI didn't hear anyone wanting to do that, actually, just people wanting\nto vent their frustration.\n\nWhat *is* the case is that HTTP/1.1 is fine, as is, for a Proposed\nStandard, and that before we advance to Draft Standard and then\nStandard, every feature must be implemented. The IETF process is set\nup such that \"what gets implemented affects what's allowed to be a\nstandard\" rather than \"what's a standard affects what gets\nimplemented\". I think that's the strength of the process and that we\nshouldn't try to change that.\n\n> I'm not trying to be contentious, but I think it's inappropriate for you to\n> declare a WG discussion moot in a message to the IESG when you as WG Chair\n> never said so on the WG list. Further, this kind of action is especially\n> inappropriate when the WG chair is in the minority of the rough consensus,\n> as I feel is the case here. Regardless of individuals' motivations, most\n> folks who spoke up (and there were plenty) agreed that\n> digest-authentication should be a MUST in the spec.\n\nThis is really unfair, Paul. In the interest of getting on with\nthings, I sent a mail to the area directors, but I didn't respond\nprivately, I copied the list. If you actually disagree with my\nrecommendation, you have a chance to say so. I can't really tell from\nyour message whether you disagree. It's my understanding that we can't\nactually add more constraints than are in the documents as written\nwithout a reset to \"last call\" or (the alternative) writing a very\nshort RFC \"applicability statement\" and sending THAT into last call.\n\nIt sounds like you were reacting to my commentary rather than the\nactual recommendation, though.\n\nLarry\n\n\n\n", "id": "lists-010-9222346"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": "Personally, I don't see any point in a short RFC that says that you're\nrequired to implement what's in draft-http-digest-aa-04.txt, because\n*that* draft is pretty weak.\n\nIf working group members really want to REQUIRE that you MUST REALLY\nimplement digest authentication, I think we would have to revise\ndraft-http-digest-aa-04.txt so that it actually stated some\nrequirements where \"MUST IMPLEMENT\" actually meant something.\n\nPersonally, I think it's a waste of time. Little applicability\nstatements are the hobgoblins of ... well, you know how it goes.\n\nWe should focus on more important things, like the HTTP/1.2 documents.\nThe folks in distributed authoring and version management seem to also\nwant us to consider LINK and LOCK and UNLOCK and GET-VERSION and a\nbunch of other things, too. We have lots of work.\n\nI guess people can get excited about \"MUST implement digest\nauthentication\" but don't have time to actually make progress on\nthe _real_ problems.\n\nLarry\n\n\n\n", "id": "lists-010-9233886"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": "Phill,\nexcellent idea - the list of issues approach has done wonders for the\nDRUMS group, and I keep such a list of issues for the former NOTARY\ngroup's documents.\nAre you volunteering to keep such a list?\n\n       Harald A\n\n\n\n", "id": "lists-010-9243428"}, {"subject": "Protocol Action: Hypertext Transfer Protocol &ndash;&ndash; HTTP/1.1 to Proposed Standar", "content": "  The IESG has approved publication of Hypertext Transfer Protocol --\n  HTTP/1.1 <draft-ietf-http-v11-spec-07.txt, .ps> as a Proposed\n  Standard.\n\n  This document is the product of the HyperText Transfer Protocol\n  Working Group. The IESG contact persons are Keith Moore and Harald\n  Alvestrand.\n\n\nTechnical Summary\n\n This document changes the HTTP protocol in ways that we believe will\n make HTTP less harmful, and therefore more useful, to the Internet.\n In particular it:\n\n  - Clarifies cache control\n  - Includes information about hostname used to access a resource\n  - Introduces persistent TCP connections\n\n These changes from HTTP/1.0 are believed to be required, and\n sufficient to let the protocol enter the standards track.\n\nWorking Group Summary\n\n The working group was contentious, most of all when removing\n features.  There appears to be clear consensus on the current\n specification, both that the features are necessary and that no\n completely vital features are missing.\n\n Further work to add more features to the 153-page document is underway.\n\nProtocol Quality\n\n The protocol has been reviewed by a number of people.\n The reviewer for the IESG was Harald Alvestrand.\n\n\nNote to RFC Editor:\n\n   draft-ietf-http-v11-spec-07 was written without knowing\n   whether draft-ietf-http-digest-aa-* would be approved as\n   a Proposed Standard.   Since both documents were approved at\n   the same time, each may need modifications to make appropriate\n   references to the other.\n\n   Notes to the RFC Editor in the HTTP 1.1 draft (in section\n   11.2 and 19.8.4) leave the decision to the RFC Editor to\n   combine both drafts into one document or to keep them\n   separated.  Assuming the RFC Editor will prefer the latter,\n   it is suggested that section 11.2 of draft-ietf-http-v11-spec-07\n   be replaced with the following text:\n\n   \"The Digest Authentication Scheme, described in RFC XXXX, may also\n   be used with HTTP/1.1.\"\n\n   where \"XXXX\" is the RFC number assigned to\n   draft-ietf-http-digest-aa-04.txt\n\n\nThe HTTP WG will decide whether to issue an applicabilty statement\nwhich imposes requirements or makes recommendations to implement\ndigest authentication in HTTP/1.1 implementations.\n\n\n\n", "id": "lists-010-9252825"}, {"subject": "Re: Netscape vs. Digest (?", "content": "Lou Montulli wrote:\n> Why would you ever want to use digest if you already have\n> certificate support?\n\nI'll add some \"practical\" (non-technical) reasons:\n\n(1)  Browsers do not permit sharing of certificates. The user must purchase\n     a separate cert for each brand of browser they use.\n\n(2)  Browsers have no \"end-user\" support for carrying certs around between\n     computers. \n\n(3)  Corollary to (1/2): Server operator must be able to register and keep\n     multiple certs for at least some of their users\n\nUntil certs are as portable as passwords, I think there are good reasons\nfor having something like Digest.\n\n  -- Bob\n\n\n\n", "id": "lists-010-9262700"}, {"subject": "Protocol Action: A Proposed Extension to HTTP : Digest Access Authentication to Proposed Standar", "content": "  The IESG has approved the Internet-Draft \"A Proposed Extension to HTTP :\n  Digest Access Authentication\" <draft-ietf-http-digest-aa-04.txt> as a\n  Proposed Standard. This document is the product of the HyperText Transfer\n  Protocol Working Group. The IESG contact persons are Keith Moore and\n  Harald Alvestrand.\n\n\nTechnical Summary\n\n This protocol extension provides a method of HTTP client\n authentication using shared secrets.  Unlike the \"Basic\"\n authentication method defined by HTTP 1.0, the Digest Access\n Authentication method does not transmit the secret in unencrypted\n form.\n\n While not entirely immune to attack, this method appears to be\n significantly less vulnerable to passive attacks than the \"Basic\"\n authentication method.\n\nWorking Group Summary\n\n The extension has received extensive review in the HTTP working group,\n which has carefully considered the protocol for the extension itself,\n its effect on other features of the HTTP protocol, and the security\n considerations.  There is strong consensus in the working group that\n this extension is very desirable; a number of vendors have agreed to\n implement it.\n\nProtocol Quality\n\n Keith Moore reviewed the spec for IESG.\n\n\nNote to RFC Editor:\n\nPlease see RFC Editor note attached to\nHypertext Transfer Protocol -- HTTP/1.1 Protocol Action\nAnnouncemment.\n\n\n\n", "id": "lists-010-9269670"}, {"subject": "Re: FYI, resolution of &quot;Digest&quot; issu", "content": "I certainly have been keeping moderately careful note of mail messages\nthat go by that may want to affect the document.  And certainly an issues\nlist has been useful in 1.1 development (though we let it get out of\ndate toward the end of development of HTTP/1.1).\n\nI guess, for the moment, I'm probably happiest keeping track of this\nmyself for 1.1, though if someone wants to volunteer to help get\nwhat I have filed organized into an issues list again, I wouldn't mind.\n\n- Jim\n\n\n\n", "id": "lists-010-9278882"}, {"subject": "Re: Question on ChunkSize sect 3.6, 8/12/9", "content": "> Your description would require the \";\" to immediately follow chunk-size \n> and padding to appear between  the \";\" and CRLF.\n\nOops, right.  I forgot to add the chunk-ext change as well.\n\n> The present known server and client implementations are handling this:\n> \n> #2       chunk          = chunk-size *(SP | HT) [ chunk-ext ] CRLF\n>                         chunk-data CRLF\n> \n> Although one reference client doesn't parse [ chunk-ext ], and so\n> might need a small hack to detect \";\" and skip *(SP | HT) to the CRLF.\n\nWhat are these \"reference client\" implementations?  Amaya isn't quite\nready yet (last I checked) and Henrik's line-mode browser still has\na couple things to implement.  Is there another?  I'd like to do more\ntesting as well.\n\n> Another alternative could be:\n> \n> #3       chunk          = chunk-size *(SP | HT) [ chunk-ext ] *(SP | HT) CRLF\n>                         chunk-data CRLF\n> \n> This would seem most robust, if a little longer.\n\nActually, I would put the *(SP | HT) inside the definition of chunk-ext\n(before and after each \";\").\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n", "id": "lists-010-9288158"}, {"subject": "FW: REPOST (was: HTTP working group status &amp; issues", "content": ">What HTML mark-up would encode a\n>POST-style bookmark?\n\nMaybe I am picky, but the above should be \"what HTML-*like* mark-up would \nencode a POST-style bookmark?\".  Having just recently tried and failed to \nget HoTMetal Pro 3.0 to let me edit my Netscape 3.0 bookmark file, in my \nexperience you can't say that bookmark files are true HTML.  However, \nbecause they _could_ be valid HTML, the fact that bookmark files are not \ncurrently in valid HTML is not a valid argument.\n\nOffhand, I would think adding to HTML support for bookmark lists would be \nappropriate.  The DTD fragment could look something like:\n\n<!-- bookmark list                      -->\n<!ELEMENT BL        - -  (BL | BLE)+           >\n<!-- bookmark list entry                -->\n<!-- contains 1 anchor, possibly with POST request data     -->\n<!ELEMENT BLE       - -  (BL | ble.content)+   >\n<!ENTITY  % ble.content \"A, POSTDATA?\"              >\n<!-- data for a POST request, encoded as PCDATA        -->\n<!ELEMENT POSTDATA  - -  (#PCDATA)        >\n\nI can even see where you could use data entities in POSTDATA for binary \nfiles to be uploaded.\n\n(P.S. Please set flamethrowers on low; this is my first DTD fragment! :)\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n", "id": "lists-010-9924220"}, {"subject": "ContentMD5 heade", "content": "What is the (real) purpose of this header? \n \nThe client has no way to request it, it is not a security feature, and \nconnection oriented transport protocols (i.e. TCP/IP stream sockets) \nalready guarantee reliable data delivery. \n \nWhy would I want to ever generate it? \n \nI found no discussion in the archive on the rationale for this header, \nonly arguments about what it should be called.\n\nThank You, \nRichard L. Gray\n\n\n\n", "id": "lists-010-9933030"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "The reason for asking for a separate draft is because of a combination\nof factors, none of which depend on this being a 'big' idea: there is\nnot yet apparent consensus on the idea, and also, so far we are\nhandling proposed extensions to HTTP with separate drafts.\n\nThe proposal changes frequently (Friday it was 'redo-safe', today it\nis 'safe') and it won't be clear what we're asking for consensus on\nuntil there is an internet draft on which we do Last Call.\n\nIf we don't have a separate draft and apparent consensus on that\nseparate draft, then we won't be able to add this to the standard.\n\nYou might think it's unfair that proposals that were made before\nHTTP/1.1 became 'proposed standard' didn't require this extra level of\nnonsense, but this kind of inertia is necessary to achieve stability.\n\nLarry\n\n\n\n", "id": "lists-010-9939746"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n>The reason for asking for a separate draft is because of a combination\n>of factors, none of which depend on this being a 'big' idea: there is\n>not yet apparent consensus on the idea, and also, so far we are\n>handling proposed extensions to HTTP with separate drafts.\n>\n>The proposal changes frequently (Friday it was 'redo-safe', today it\n>is 'safe') and it won't be clear what we're asking for consensus on\n>until there is an internet draft on which we do Last Call.\n>\n>If we don't have a separate draft and apparent consensus on that\n>separate draft, then we won't be able to add this to the standard.\n>\n>You might think it's unfair that proposals that were made before\n>HTTP/1.1 became 'proposed standard' didn't require this extra level of\n>nonsense, but this kind of inertia is necessary to achieve stability.\n\nIt is not an issue of unfairness, but simply misperception of\nthe situation.  I can see now that a separate draft was required for\naddition of a header (idempotent->redo-safe->safe) or method (repost)\nfrom the outset of his discussion.\n\nIt might be better, then, if Roy and Koen finished their\ndiscussion of what we have term the \"rel=source alternative\", and\na qualifed group of people took on the issue of safe, private and\nsecure storage of post-content, and mechanisms or re-using it via\na bookmark-like facility, beyond the simple, \"first step\" I was\nsuggesting.  Ideally, this should be done in coordination with the\nW3C ERB.  It seems inappropriate to undertake an IETF ID that is\nlikely to involve new HTML markup or at least naming conventions\nunder circumstances where the IETF no longer has a clear role in\nthe development and standarization of HTML.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n", "id": "lists-010-9948238"}, {"subject": "Re: REPOS", "content": "% Larry Masinter <masinter@parc.xerox.com> wrote:\n% >The reason for asking for a separate draft is because of a combination\n% >of factors, none of which depend on this being a 'big' idea: there is\n% >not yet apparent consensus on the idea, and also, so far we are\n% >handling proposed extensions to HTTP with separate drafts.\n% >If we don't have a separate draft and apparent consensus on that\n% >separate draft, then we won't be able to add this to the standard.\n% It is not an issue of unfairness, but simply misperception of\n% the situation.  I can see now that a separate draft was required for\n% addition of a header (idempotent->redo-safe->safe) or method (repost)\n% from the outset of his discussion.\n\nI understand too that it's better to leave this feature in a separate\ndraft.\n\nI was however wondering if any addition to 1.1 should be lumped together\nand put in waiting list for 1.2 , or there is some shortcut, something\nlike tables' RFC in HTML. I know that probably it is more a political\nissue, but for \"simple\" modifications a preferential route would be\ngreat.\n\n.mau.\n\n\n\n", "id": "lists-010-9958771"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n|The proposal changes frequently (Friday it was 'redo-safe', today it\n|is 'safe') and it won't be clear what we're asking for consensus on\n|until there is an internet draft on which we do Last Call.\n\nI thought this would fit best using the Allow header as follows:\n\n     Allow: POST=resafe, GET, PUT\n\nfor a repostable or\n\n     Allow: POST=reunsafe, POST=foo, GET, PUT=bar\n\nfor a nonrepostable.\n\n-marc\n\n-- \n\n\n\n", "id": "lists-010-9967101"}, {"subject": "Feature tag registratio", "content": " Background and design considerations for feature tag registration\n =================================================================\n\nINTRODUCTION\n\n This document discusses considerations related to feature tag\n registration.  It intends to initiate a discussion of feature tag\n registration on the http-wg list, so all comments and additions are\n welcome.  Though partially based on remarks from other people, this\n document mainly represents my own limited understanding of the\n issues.\n\n The two big questions related to feature tag registration are:\n\n    1. How open should the registration process be?\n\n    2. How should the feature tag namespace be structured?\n\n These questions will have to be answered to some extent before we can\n write a feature tag registration internet draft.\n\n\n1 Purpose of feature negotiation\n\n The feature negotiation mechanism in the transparent content\n negotiation draft means to provide a better alternative to content\n negotiation based on the user-agent field.  User-agent negotiation\n has many disadvantages: it is cache-unfriendly, huge amounts of time\n are needed to keep up with new user agent releases, etc.\n\n An advantage of using user-agent based negotiation is that it can be\n used immediately after a user agent with a new feature is released,\n without waiting for any standardization action by the IETF.  This\n advantage will have to be retained in feature negotiation.  If the\n content creation community can't use feature negotiation to negotiate\n on the new hot feature of the week, feature negotiation will not meet\n its design goal.\n\n Therefore, feature registration needs to be a very fast process.\n\n It must be possible to register feature tags in parallel with the\n release of a new browser alpha version.  Maybe there even needs to\n be a mechanism by which browser vendors could reserve some tags for\n the next alpha version without having to make public the tag spec\n yet.\n\n Parties who would want to register feature tags are:\n\n   - browser vendors, when inventing new features\n\n   - browser component vendors, when creating new browser components\n\n   - the IETF or some other standards body, when creating a new\n     standard for a content format\n\n   - content authors who want to label their variants in a new way\n     (this presupposes that users can configure their browsers to make\n     present new feature tags to indicate a particular preference).\n\n\n2 The IANA\n\n With the MIME registration procedures being updated, there has been\n some confusion over what IANA will register.  John Postel recently\n told us that:\n\n   The IANA is pretty good at keeping lists.  It is not so good at\n   evaluating the merits (or lack thereof) of the requests to add\n   things to lists.   [...] \n   So, yes the IANA would keep the list of \"feature tags\" provided\n   that that there is either a very simple way to decide if requests\n   pass muster, or a designated someone else will be available to make\n   that decision.\n\n So from an IANA standpoint, this WG must either\n\n    a) create a feature tag review board, or\n\n    b) define very basic registration rules which do not take the\n       merit of the feature tag into account.  To quote\n       draft-ietf-822ext-mime-reg-04, this type of registration \"does\n       not imply endorsement, approval, or recommendation by IANA or\n       IETF or even certification that the specification is adequate.\"\n\n In Sections 3-4 below, we will see that we can't meet the design\n goals of feature negotiation if we go for a).  Going for b) implies\n that even feature tags without merit will be registered.  Sections 5\n and 7 explore the implications of this.\n\n\n3 Lack of manpower needed to review all features\n\n If feature negotiation is to replace user-agent negotiation, it must\n keep up with the development of new browser features in the\n marketplace.  As the marketplace is moving fast, significant manpower\n would be needed to review new tags quickly.  The IETF does not have\n this kind of manpower available.\n\n\n4 Market driven innovation does not wait for the IETF\n\n The development of new web features is currently driven by\n marketplace competition.  As history of the HTML 3.0 effort has\n shown, the market will not wait for IETF consensus.\n\n I feel that it would be a mistake for the IETF to attempt to control\n the introduction of new features by controlling the feature tag\n namespace.  The IETF cannot block the creation of a chaotic\n collection of features by the marketplace.  It _does_ have the power\n to block the creation of a matching chaotic collection of feature\n _tags_, but there is little sense in doing this.\n\n\n5 Filtering before registration vs. filtering after registration \n\n The considerations above lead to a fast feature registration process\n with extremely minimal requirements.   This inevitably leads to a\n feature tag namespace space which contains a lot of \n\n     a) tags without merit\n     b) tags indicating features without merit\n     c) tags indicating features which had great potential, but\n        which never gained widespread use\n\n Compare this to the situation in the USENET newsgroup namespace.\n\n A list of _all_ registered feature tags will generally be too long to\n be useful to any content author.  Third parties could filter the\n feature tag namespace and compile short lists of useful tags.  Web\n indexing robots could, while traversing the web, gather statistics\n about actual use of feature tags; these statistics could help in\n compiling lists.\n\n Note that this filtering after registration basically follows the\n HTML 3.2 model of creating order _after_ the marketplace battles have\n died down.\n\n\n6 Feature tag registration timeline\n\n To summarize the above, this is a timeline for the registration of a\n feature tag which ends up being generally used.\n\n  Time    Action \n (months)\n\n  t+0    Browser vendor A invents the new feature XY.\n\n  t+1    Vendor A starts implementing XY, and writes a\n         feature tag registration form for the `xy' tag.\n\n  t+2    Vendor A submits form and registers the `xy' feature tag.\n\n  t+2    Vendor A releases a new browser version, which\n          a) implements the feature XY\n          b) has the `xy' tag present when doing feature negotiation.\n\n  t+2.5  `Early adopter' content authors start making variants \n         which use XY.\n\n  t+3    Vendor B starts implementing XY in their own browser.\n\n  t+3    The `xy' tag appears in lists of useful tags maintained by\n         third parties.\n\n  t+3.5  Vendor B releases new browser version, which\n          a) implements the feature XY\n          b) has the `xy' tag present when doing feature negotiation.\n\n  t+3.5  Many content authors start making variants which use XY.\n\n  t+4    Vendor C starts implementing XY, and invents the extension\n         XY_COLOR.\n\n  t+4.5  Vendor C registers the `xy_color' feature tag.\n\n  t+4.5  Vendor C releases new browser version, which\n          a) implements the features XY and XY_COLOR\n          b) has the `xy' and `xy_color' tags present when doing\n             feature negotiation.\n\n  t+10   90% of all deployed browsers support XY, content authors\n         start using XY it without bothering to provide an alternate\n         representation.\n\n\n7 Potential problems of an open registration process\n\n Several potential problems connected to having an open registration\n process have been identified.  These are covered in the subsections\n below.\n\n\n7.1 Excessive registration, trademark fights\n\n One danger is excessive registration like we have seen in the\n internet domain namespace.\n\n I think that the various forces which contributed to the domain name\n registration problems are absent for feature tags: feature tags will\n not be very visible to end users, and registration of a feature tag\n does not mean you get exclusive use.\n\n Therefore, I do not expect that we will see excessive registration as\n with domain names.  Of course, it is always possible to change the\n registration procedure if excessive registration _does_ occur.  A\n part of the namespace could be reserved for use in a `plan B'.\n\n As a-priori safeguards, IANA could be allowed to reject registrations\n which are `obviously bogus', and be allowed to reject or delay the\n registration of `large collections of tags with questionable value'.\n Such decisions could be appealed to the IESG.\n\n Note however that the danger of excessive registration is also\n present in the vendor and personal spaces of\n draft-ietf-822ext-mime-reg-04.txt, and that\n draft-ietf-822ext-mime-reg-04.txt does not specify such safeguards.\n\n7.1.1 More closed registration procedures\n\n Of course, it is possible to invent more closed registration\n procedures,  which could limit excessive registration.\n\n To meet the design goals of feature negotiation, these closed\n procedures would however have to grant browser and browser component\n vendors the right to register feature tags quickly and without\n review, on release of the first alpha version which implements the\n feature.  Registration by people who are not browser vendors could be\n subject to higher barriers.\n\n Such closed procedures which grant special rights to some parties are\n however politically unattractive.  Also, there is the problem of\n identifying the actual browser and browser component vendors.  And\n with browsers increasingly becoming open systems with pluggable\n components, everybody with a C compiler would have some claim to\n being a browser component vendor.\n\n7.1.2 Separate registration spaces\n\n Another way to deal with the `noise' of possible excessive\n registration would be to have multiple registration spaces, for\n example\n    1 one for tags defined by IETF standards track documents\n    2 one in which browser (component) vendors may register tags\n      for new features\n    3 one for all other tags which are meant for general use\n    4 one for tags which are meant for limited use\n\n This way, `channel 1 and 2' are kept relatively noise-free.\n\n A problem with space 2 above is that IANA does not have the resources\n to verify if the checkbox `[X] I intend to add this feature to the\n next release of my browser/browser component' is telling the truth.\n Tags could however be stamped `obsolete' if it is brought to the\n attention of IANA that the promised implementation has not appeared\n after N months.\n\n\n7.2 Intentional misregistration\n\n Vendor X could try to pre-emptively block the development of a\n `walking gifs' feature by other vendors by registering a\n `walking_gifs' feature tag which indicates, for example, a preference\n for paragraphs separated by horizontal line .gifs in which little\n smurfs can be seen walking on the line.\n\n However, I feel that the English language is flexible enough to allow\n the invention of alternative tags to label the real `walking gifs'\n feature, if ever developed.\n\n Also, the registration rules could allow IANA to reject registration\n `if the tag name is clearly bogus or misleading'.  The rejection\n would have to include a suggestion for a tag name which _would_ be\n acceptable.\n\n\n8 Partitioning the feature tag namespace\n\n It has been suggested that a partitioning of the feature tag\n namespace (for example through the definition of a standard prefix\n naming scheme) could help to keep feature negotiation more manageable.\n The partitioning could be based on several criteria, or combinations\n thereof:\n\n     a) who registered the tag ( ietf / vendor / other )\n\n     b) the intended scope ( global / local / personal / experimental )\n\n     c) the area of negotiation:\n             - capability / preference\n             - related to HTML / not related to HTML\n             - specific to one MIME type / not MIME type specific\n             - for static content / for dynamic content\n             - etc.\n\n The options for partitioning have not been explored yet, much work\n still needs to be done in this area.  It is not clear whether\n partitioning will actually help to make feature negotiation more\n manageable.  Risks connected to partitioning are\n\n   - that the registration of otherwise useful feature tags could be\n     blocked because they do not fit in any predefined category\n\n   - that searching for tags which do not clearly fit into one category\n     could be difficult\n\n The first risk could be eliminated by having a `miscellaneous'\n category.\n\n\n\n", "id": "lists-010-9974945"}, {"subject": "Re: REPOST (was: HTTP working group status &amp; issues", "content": "Foteos Macrides:\n>\n[...]\n>        It might be better, then, if Roy and Koen finished their\n>discussion of what we have term the \"rel=source alternative\",\n\nI too have been waiting for Roy (or Larry) to comment on my objections\nto rel=source.  I have been thinking about writing a `safe: yes'\nmini-ID in case nobody else volunteers, but would like to see the\n`rel=source' issue settled first.\n\n> and\n>a qualifed group of people took on the issue of safe, private and\n>secure storage of post-content, and mechanisms or re-using it via\n>a bookmark-like facility, beyond the simple, \"first step\" I was\n>suggesting.\n\nI personally have little interest in taking the work beyond a `safe:\nyes' mini-ID.  I don't think a bookmarking standard would create much\nadded value.\n\n>                                Fote\n\nKoen.\n\n\n\n", "id": "lists-010-9994202"}]